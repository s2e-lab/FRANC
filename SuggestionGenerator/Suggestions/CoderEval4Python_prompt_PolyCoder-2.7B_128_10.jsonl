{"task_id": "62e60f43d76274f8a4026e28", "prompt": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif tz is None:\n\t\treturn UTC(microseconds=nanoseconds)\n\telse:\n\t\treturn TZ.fromtimestamp(datetime.datetime.utcfromtimestamp(seconds=nanoseconds))._with_tzinfo(tz)\n\n#######\n# DATETIME/TIME TYPE GENERATOR\n#######\ndef datetime_from_datetime(value):\n\tif isinstance(value, datetime):\n\t\treturn value\n\tif isinstance(value, DateType):\n\t\treturn datetime.date(value.year, value.month, value.day)\n\tif isinstance("}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif not tz:\n\t\tif not datetime.is_utc_nanoseconds:\n\t\t\traise TypeError(f_nanoseconds)\n\t\ttz = 0\n\tself.local_delta = delta(seconds=nanoseconds, nanoseconds=nanoseconds)\n\tself.local = LocalTime.from_datetime(datetime.datetime(int(seconds), int(seconds), int(nanoseconds), int(nanoseconds), tz))\n\treturn self\n\n\nclass Event:\n\n\t__slots__ = ['id', 'label', 'active']\n\n\tdef __init__(self, label"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\treturn Time.from_float(f'{nanoseconds.total_seconds()}:{tz.tzname(None, None)}')\n\ndef now():\n\t\"\"\"\n\tTimestamp for the now() function.\n\t\n\t:return:\n\t\"\"\"\n\treturn now_time()\n\ndef now_tz(tz=None):\n\t\"\"\"\n\tTimestamp for the now() function. If the current time doesn't work, call\n\t::pytz.utcnow().\n\n\t:param tz:\n\t:return:\n\t\"\"\"\n\treturn now_time(tz)\n\ndef"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\treturn timezone.now(seconds=math.floor(\n\t\tnanoseconds // units.hertz,\n\t\tnanoseconds * units.s),\n\t\ttz=tz or timezone.utc)\n\ndef now(tz=None):\n\treturn time()\n\ndef now_in_time_zone(default=True, tz=None):\n\t\"\"\"\n\t\tReturn the current time as a `localtime` object.\n\t\"\"\"\n\treturn get_local_timezone().now() if tz is None else tz.localize(\n\t\tdatetime.datetime.now(tz"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t# pylint doesn't know what to return if `nanoseconds` is 0\n\tif nanoseconds == 0:\n\t\t# If you don't, please return time.time()\n\t\treturn time.time(), time.localtime()\n\t\n\t# If time since Jan 1 1970 in milliseconds, return time's epoch with millisecond precision\n\ttime_epoch = time.mktime(time.localtime(nanoseconds))\n\treturn time.time() + time.timezone(timezone.get_localzone(tz)).timedelta(microseconds=time_epoch), time_epoch\n   "}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tnanoseconds = int(nanoseconds or '00:00:00.00')\n\tdt = datetime.datetime.fromtimestamp(nanoseconds)\n\tif not 0 <= tz == 'UTC' and not 'UTC' in dt.tzinfo:\n\t\tdt = datetime.datetime.combine(dt, datetime.timedelta(0))\n\tdt = datetime.datetime.fromtimestamp(dt.timestamp())\n\tseconds = dt.seconds\n\treturn seconds + seconds * nanoseconds / 1000000000\n\n\ndef parse_datetime(dt_str):\n\t\"\"\"\n\t\tParse a datetime to"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif nanoseconds is None or isinstance(nanoseconds, (int, float)):\n\t\tnanoseconds = time.time()\n\ttimeZone = _getTimeZoneFromTimezone(tz)\n\tif isinstance(nanoseconds, time.struct_time):\n\t\treturn nanoseconds\n\tif isinstance(nanoseconds, time.time_base) or isinstance(nanoseconds, time.struct_time):\n\t\treturn time.localtime(nanoseconds)\n\treturn time.newtime(nanoseconds)\n\n\nclass _TimeDict(object):\n\t__slots__ = ['values', ]"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif isinstance(nanoseconds, float):\n\t\tnanoseconds = int(nanoseconds)\n\tif tz is None:\n\t\ttz = DEFAULT_TIMEZONE\n\tresult = _mktime(nanoseconds, tz)\n\tif tz is None:\n\t\tresult = _mktimezone(result)\n\treturn result\n\n\ndef normalize_time(time):\n\t\"\"\"\n\t\tRemoves fractional seconds from `time`\n\t\tand normalizes (zero or negative sign).\n\t\"\"\"\n\tif time < 0:\n\t\traise ValueError(\"Time must be positive\")\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif tz is None:\n\t\ttz = time.tzname[0]\n\t\n\treturn time.struct_time((abs(nanoseconds), tz))\n\n\n\ndef is_leap_year(year):\n\t''' Check if a year is a leap year.\n\t\tAn invalid year value will return False.\n\t'''\n\treturn (year % 4 == 0)\n\n\ndef to_int(value, scale=20):\n\t# 'value' is an int. This value can be negative for values that are out\n\t# of the range of an int but not"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t# TODO: this needs to have a non-UTC `datetime.time` wrapper in place!\n\t# if tz is specified as None then we assume UTC\n\treturn datetime(nanoseconds // 1e9, tzinfo=tz)\n\ndef resolve_timezone(tzname):\n\t\"\"\"\n\tResolves the timezone offset from the string if there is no colon.\n\t\n\t:param tzname:\n\t:return: UTC timezone name\n\t\"\"\"\n\tif ':' in tzname:\n\t\t# Try to convert the string to UTC or to the local timezone\n\t\tt = tz"}]}
{"task_id": "62e60f3bd76274f8a4026e10", "prompt": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn Timedelta(value)\n\n\ndef dehydrate_date(value):\n\t\"\"\"\n\t\tDehydrate an `iso8601` value.\n\t\t\n\t\t:param value:\n\t\t:type value: str\n\t\t:return: unicode string\n\t\t\"\"\"\n\ttry:\n\t\treturn unicode(value)  ## unicode to unicode\n\texcept UnicodeError:\n\t\treturn str(value)\n\ndef dehydrate_datedelta(value):\n\t\"\"\"\n\t\tDehydrate a python duration value.\n\t\t\n\t\t:param value:\n"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn timedelta(value).total_seconds()\n\ndef dehydrate_list(value):\n\"\"\"\n\tDehydrator for Python `list` values.\n\t\n\t:param value:\n\t:type value: list\n\t:return:\n\t\"\"\"\n\treturn reduce(lambda x, y: [float(x) if isinstance(y, float) else y for x in zip(x, y)], value)\n\ndef dehydrate_dict(value):\n\t\"\"\"\n\tDehydrator for Python `dict` values.\n\t\n\t:param value:\n"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tif isinstance(value, timedelta):\n\t\treturn value.__int__(), value.__pos__()\n\tif hasattr(value, '__total__'):\n\t\treturn value.__total__(), value.__pos__()\n\tif isinstance(value, datetime):\n\t\treturn value.days(), value.seconds()\n\tif isinstance(value, datetime_tz):\n\t\treturn value.date(), value.time()\n\t\n\traise ValueError('No deserializer for value `%s`.' % value)\n\n// Copyright 2018 The Abseil Authors.\n//\n// Licensed under the Apache License, Version"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn (\n\t\tvalue.days * (\n\t\t\t10L ** value.months\n\t\t),\n\t\tvalue.seconds,\n\t\tvalue.microseconds,\n\t)\n\n\ndef dehydrate_func(value):\n\t\"\"\"\n\tDehydrate a `func` or an `async def` parameter.\n\t\n\t:param value:\n\t:type value: callable or @types.coroutine\n\t:return:\n\t\"\"\"\n\tif isinstance(value, asyncio.coroutine):\n\t\treturn dehydrate_func_async(value)\n\tif"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn (0, datetime.timedelta(hours=value.days))\n\n\nclass FieldList(PyQueryWrapper):\n\t\"\"\"\n\tField list (or list, dict, set, list, etc.)\n\t\"\"\"\n\t# FIXME: We don't need to keep track of field names.\n\t_data_cache = {}\n\t\n\tdef __init__(self,\n\t\t\t       f,\n\t\t\t       name,\n\t\t\t       description=\"\",\n\t\t\t       order=0):\n\t\tsuper(FieldList, self"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tif isinstance(value, timedelta):\n\t\td = value\n\telse:\n\t\td = timedelta(value)\n\treturn d\n\t\ndef dehydrate_date(value):\n\t\"\"\"\n\tDehydrator for `date` values.\n\t\n\t:param value:\n\t:type value: date\n\t:return:\n\t\"\"\"\n\tif value is None or isinstance(value, date):\n\t\td = from_int(value)\n\telse:\n\t\td = datetime.datetime.mktime(value)\n\treturn d\n"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn convert_seconds(value)\n\n\ndef serialize(obj):\n\t\"\"\"\n\tSerialize a value to the supplied type.\n\t\"\"\"\n\treturn obj.serialize()\n\n\ndef deserialize(obj, value):\n\t\"\"\"\n\tDeserialize a value to the supplied type.\n\t\"\"\"\n\treturn obj(value)\n\n\ndef serialize_list(obj):\n\t\"\"\"\n\tSerialize a list to an ordered list/array.\n\t\n\t:param obj:\n\t:type obj: list/array\n\t:return:\n\t\"\"\"\n\treturn [serialize(v)"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\ttry:\n\t\t# noinspection PyUnresolvedReferences\n\t\treturn timedelta._from_utc(value)\n\texcept Exception:\n\t\treturn value\n\n\ndef dehydrate_numpy(value):\n\t\"\"\"\n\tDehydrator for `numpy.ndarray` values.\n\t\n\t:param value:\n\t:type value: np.ndarray\n\t:return:\n\t\"\"\"\n\treturn np.asarray(value)\n\n\ndef dehydrate_bytes():\n\t\"\"\"\n\tDehydrator for `bytes` values.\n\t\n\t:return"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn value * (timedelta(days=1) - timedelta(days=0) )\n\ndef dehydrate_datetime(value):\n\t\"\"\"\n\tDehydrator for `datetime.date` values.\n\t\n\t:param value:\n\t:type value: datetime.date\n\t:return:\n\t\"\"\"\n\treturn jeepy.utils.convertor(value, \"%d/%m/%Y\")\n\n\ndef _dehydrate_tuple(value):\n\t\"\"\"\n\tDehydrator for `tuple` values.\n\t\n\t:param value:"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\twith_offsets = self._with_offsets\n\tfor offset in self._with_offsets_offsets:\n\t\tif isinstance(value, offset.value):\n\t\t\treturn type.Timedelta(seconds=((value.seconds+offset.value)/1000+offset.value)%1000,\n\t\t\t\t\tmicroseconds=(((value.microseconds+offset.value)/1000+offset.value)%1000+value.microseconds)%1000000)\n\treturn type.Timedelta.from_offset(self, value.value)\n\ndef dehydrate_bool(value):\n\twith_offsets = self"}]}
{"task_id": "62e60f37d76274f8a4026dfd", "prompt": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tif isinstance(value, float):\n\t\tif isinstance(value, str):\n\t\t\tvalue = value # Convert string to float.\n\t\t# Convert float precision into days, seconds, minutes, hours\n\t\tvalue = int((time.mktime(value.timetuple()) + (15 * 24 * 60 * 60) +\n\t\t\t\t\ttime.mktime(value.microsecond) + time.mktime(value.second) +\n\t\t\t\t\ttime.mktime(value.minute) + time.mktime(value.hour)))\n\treturn value\n\t\n\nclass StringValue"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn dehydrate_int(value)\n\ndef dehydrate_utc(value):\n\t\"\"\"\n\tDehydrator for `datetime.utc` values.\n\t\n\t:param value:\n\t:type value: datetime.utc\n\t:return:\n\t\"\"\"\n\n\treturn dehydrate_time(value)\n\ndef dehydrate_timedelta(value):\n\t\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\n\treturn"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tresult = {\"time\": value, **DEFAULT_DEHYDRATOR_TIME_DEFAULTS}\n\n\tif value is None:\n\t\treturn result\n\n\tif value.tzinfo:\n\t\treturn result\n\n\tif timezone.is_aware(value.tzinfo):\n\t\tvalue = value.astimezone(timezone.utc)\n\n\treturn result\n\n/*\n * Copyright (c) 2014, 2021, Oracle and/or its affiliates. All rights reserved.\n * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n *\n * This code is free software"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\ttry:\n\t\treturn getattr(time, value.lower())\n\texcept AttributeError:\n\t\treturn value\n\n\n@dehydrate_str\ndef dehydrate_str(value):\n\t\"\"\"\n\tDehydrate a `str` value.\n\t\n\t:param value:\n\t:type value: Any\n\t:return:\n\t\"\"\"\n\tif value is None:\n\t\treturn value\n\telse:\n\t\treturn str(value)\n\n@dehydrate_int\ndef dehydrate_int(value):\n\t\"\"\"\n\tDehy"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tt = value.time()\n\tif t and time.utcoffset(t)._isUTC():\n\t\treturn t + timedelta(hours=int(value.seconds / 3600), minutes=value.seconds % 3600)\n\telse:\n\t\treturn time() + timedelta(seconds=int(value.seconds / 3600), minutes=value.seconds % 3600)\n\ndef dehydrate_dict(value):\n\t\"\"\"\n\tDehydrator for `dict` dictionaries.\n\t\n\t:param value:\n\t:type value: dict\n\t:return: a"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\twith Time2Tag(value) as time_tag:\n\t\treturn time_tag.dehydrate()\n\n#-------------------------------------------------------------------------------\n# Dehydrator for `unicode` values.\n#\n#.. warning::\n#    Not all dehydrators are currently supported. (I.e. the dehydrator of\n#    `datetime.tzinfo` is always `python-dateutil >= python-dateutil-2.7.1`.)\n\ndef dehydrate_unicode(value):\n\t\"\"\"\n\t\"Dehydrator for `"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tif hasattr(value, 'timeFormat'):\n\t\treturn value.timeFormat(value.format)\n\telse:\n\t\ttemp = str(round(value.hour)*3600+value.minute*60+value.second)*1000000\n\t\treturn Format.time_format(value.hour, value.minute, value.second, temp)\n\ndef dehydrate_date(value):\n\t\"\"\"\n\tDehydrator for `date` values.\n\t\n\t:param value:\n\t:type value: Date\n\t:return:\n\t\"\"\"\n\treturn Format"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn Time(value)\n\ndef dehydrate_url(value):\n\t\"\"\"\n\tDehydrator for `url` values.\n\t\n\t:param value:\n\t:type value: Url\n\t:return:\n\t\"\"\"\n\treturn Url(value)\n\ndef dehydrate_duration(value):\n\t\"\"\"\n\tDehydrator for `duration` values.\n\t\n\t:param value:\n\t:type value: Duration\n\t:return:\n\t\"\"\"\n\treturn Duration(value)\n\ndef dehydrate_integer(value):\n\t"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tif isinstance(value, time):\t\n\t\treturn (value.hour + value.day * 24 * 60 * 60) * 1000\n\telif isinstance(value, Time): \n\t\treturn value\n\t\n\traise InvalidArgumentError(\"Could not dehydrate value\")\n\n/*\n * ProviewR   Open Source Process Control.\n * Copyright (C) 2005-2019 SSAB EMEA AB.\n *\n * This file is part of ProviewR.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tresult = copy.deepcopy(value)\n\tdel(result['_']) # _ is now a dict\n\t\n\tif value['_'] == 'time': # Time\n\t\tresult = int(value['sec']) * 60 + int(value['usec'])\n\telse: # Datetime\n\t\tresult = int(value[:-7]) * 1000 + int(value[-6:-3]) * 100 + int(value[-4:-1])\n\tif result in (-86400, 719200, 722000, 145600, 144400, 186604, 186604 + 719200"}]}
{"task_id": "62e60f33d76274f8a4026de9", "prompt": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn Dehydrator.get_dehydrator().dehydrate(value, DehydrateMode.Point)\n\t\t\n@register_dehydrate_object\nclass DehydratorPoints(DehydratorBaseObject):\n\t\"\"\"\n\tDehydrator for Points data.\n\t\n\t:param dehydrator:\n\t:type dehydrator: deHYDRATOR_Points\n\t\"\"\"\n\t\n\t@classmethod\n\tdef dehydrate(cls, value):\n\t\t\"\"\"\n\t\tDeep dehydrate point object.\n\t\t\"\"\"\n"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\tif DEHYDRATOR and value is not None:\n\t\tvalue = DEHYDRATOR.dehydrate_point(value)\n\treturn value\n\n\ndef dehydrate_polygon(value):\n\"\"\"\n\tDehydrator for Polygon data.\n\t\n\t:param value:\n\t:type value: Polygon\n\t:return:\n\t\"\"\"\n\tif DEHYDRATOR and value is not None:\n\t\tvalue = DEHYDRATOR.dehydrate_polygon(value)\n\treturn value\n\n\ndef dehydrate_rectangle(value):\n\"\"\""}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\tif isinstance(value, Point):\n\t\treturn value.get()\n\traise ValueError(\"Value must be an instance of Point.\")\n\ndef get_point(index):\n\t\"\"\"\n\t\tGet a PIL Point from given index, or raise error if index is not a positive integer\n\t\tand raise IndexError\n\t\tFor all indices except last and the first ones, the first one is returned\n\t\t(as index is always valid for all indices).\n\t\n\t:param index:\n\t:type index: integer\n\t:return:\n\t\"\"\"\n\tif index < 0"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\tif type(value) is not type(None):\n\t\treturn Point(dehydrate(value[\"x\"], value[\"y\"]), value[\"z\"])\n\telse:\n\t\treturn None\n\n###############################################################################\n#\n# Dehydrator for Point3d data\n#\n###############################################################################\n\n# type dehydrate(x,y,z): Point3d\n\ndef dehydrate_point3d(value):\n\t\"\"\"\n\t\tGiven a point, dehydrate a tuple of 3 components\n\t\tinto the form x,y,z\n\t\"\"\"\n\t"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\tif type(value) == int or type(value) == float:\n\t\tvalue = Point(value)\n\t\tvalue.x = value.x - value.x_min\n\t\tvalue.y = value.y - value.y_min\n\t\tif len(value.shape) == 2:\n\t\t\tvalue.set(value.x, value.y)\n\t\t\t\n\t\treturn value\n\tif type(value) == list:\n\t\tvalue = []\n\t\tfor i in range(len(value)):\n\t\t\tvalue[i] = dehy"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn value.value\n\ndef dehydrate_box(value):\n\t\"\"\"\n\tDehydrator for Rectangle data.\n\t\n\t:param value:\n\t:type value: Polygon\n\t:return:\n\t\"\"\"\n\treturn value.rectangle()\n\ndef dehydrate_polygon(value):\n\t\"\"\"\n\tDehydrator for Polygon data.\n\t\n\t:param value:\n\t:type value: Polygon\n\t:return:\n\t\"\"\"\n\treturn value\n\ndef dehydrate_string_list(value):\n\t\"\"\"\n\tDehydr"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn value.x, value.y\n\n\ndef find_by_id(id):\n\t\"\"\"\n\t\tFinds a given ID by its coordinates.\n\t\t\n\t\t:param int id: a unique ID\n\t\t:type id: int\n\t\t:return: pointer to matching data item\n\t\"\"\"\n\tfor item in data.items():\n\t\tif (item[0] == id) and (isinstance(item[0], Point)):\n\t\t\treturn item\n\treturn False\n\n\ndef find(id=None):\n\t\"\"\"\n\t\tFinds the"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\tif value!= None:\n\t\treturn value.coords\n\telse:\n\t\treturn None\n\ndef _get_object_id(index_by_id):\n\tif index_by_id is not None:\n\t\treturn index_by_id\n\telif _ID_BY_ID:\n\t\treturn _ID_BY_ID[0]\n\telse:\n\t\treturn None\n\ndef get_item_by_id(index_by_id):\n\t\"\"\"\n\tGets the item by object id.\n\t\n\t:param index_by_"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t_point = Point()\n\t_point.X = value.X\n\t_point.Y = value.Y\n\tif value.Z is not None:\n\t\t_point.Z = value.Z\n\t\n\treturn _point\n\n# dehydrate_rect_coords:\n#\t\tDehydrator for rect coords\n#\t\t:param value: Rect coords\n#\t\t:type value: Point\n#\t\t:param value: Rect coords\n#\t\t:type value: Point\n#\t\t:param value: Rect coords\n#\t\t:type"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn Point(value=value)\n\ndef dehydrate_point_3d(value):\n\t\"\"\"\n\tDehydrator for Point3d data.\n\t\n\t:param value:\n\t:type value: Point3d\n\t:return:\n\t\"\"\"\n\treturn Point3d(value=value)\n\ndef dehydrate_triangle(value):\n\t\"\"\"\n\tDehydrator for Triangle data.\n\t\n\t:param value:\n\t:type value: (Point3d, Point3d)\n\t:return:\n\t\"\"\"\n\treturn"}]}
{"task_id": "62e60ed4d76274f8a4026da0", "prompt": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self._keys\n\ndef values(self):\n\t\"\"\"\n\tReturn the list of values.\n\t\n\t:return: list of value names\n\t\"\"\"\n\treturn self._values\n\ndef __len__(self):\n\t\"\"\"\n\tReturn the length.\n\t\n\t:return: length of the record.\n\t\"\"\"\n\tlength = 0\n\tfor i in self._keys:\n\t\tlength += len(i)\n\tfor i in self._values:\n\t\tlength += len(i)\n\treturn length\n\ndef fromDict(self, aDict,"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn [ key.get('name') for key in self.keys.values() ]\n\ndef values(self) -> Dict[Any, Any]:\n\t\"\"\"\n\t\tGet the values of the record.\n\t\t\n\t\t:return: the key/value pairs\n\t\"\"\"\n\tfor key in self.keys:\n\t\tyield key.get('value')\n\n\ndef items(self) -> Dict[Any, Any]:\n\t\"\"\"\n\t\tGet the key/value pairs in the record.\n\t\t\n\t\t:return: a dictionary of key/value pairs\n\t\"\"\"\n"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\tkeynamelist = []\n\t\n\tfor key in self.get(\"keys\", []):\n\t\tkeynamelist.append(key[0][\"\"])\n\t\n\treturn keynamelist\n\n\ndef keys_count(self):\n\t\"\"\"\n\t\tCheck if all values of the `keys` element of the record are in the correct format\n\t\"\"\"\n\tcount = 0\n\tvalues = self.get(\"keys\", [])\n\t\n\tif len(values) < 1: return False\n\tfor i in range(len(values)):\n\t\tif not i < len(values"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self.get_record_by_position(self.get_pos_by_keys())\n\n\t\ndef record_names(self):\n\t\"\"\"\n\t\tReturn all the names from the data file.\n\t\t\n\t\t:return: list of key names\n\t\"\"\"\n\treturn self.get_record_by_key('KeyNames').keys()\n\t\n\t\ndef read_record(self, record):\n\t\"\"\"\n\t\tRead a record from the data file.\n\t\t\n\t\t:param: record_class: subclass of DataRecord\n\t\t:return: the"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self.__dict__['keys']\n\ndef values(self):\n\"\"\"\n\tReturn the values of the record.\n\t\n\t:return: list of values\n\t\"\"\"\n\treturn self.__dict__['values']\n\npackage de.adorsys.psd2.model;\n\nimport io.swagger.annotations.ApiModel;\nimport io.swagger.annotations.ApiModelProperty;\nimport org.springframework.validation.annotation.Validated;\n\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self._keys\n\n\n# The class representing a database record. Subclasses are only\n# required if we require the database to have a primary key.\nclass Record(self.__class__, DatabaseRecord):\n\t\"\"\"\n\t\tRecord is a subclass of DatabaseRecord with a dictionary of keys.\n\t\t\n\t\tSubclasses are required if we require the database to have a\n\t\tprimary key.  This class will also return a list of ``(key, type, value)``\n\t\tpairs for every record in the database.\n\t\"\"\"\n\tdef __init__(self, dict_):\n"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self.db.get(\"keys\")\n\n\ndef last_update(self):\n\t\"\"\"\n\t\tReturn the last update timestamp of the record.\n\t\n\t:return: datetime\n\t\t\n\t\"\"\"\n\treturn self.db.get(\"last_update\")\n\nrequire'rexml/encoding'\nrequire'rexml/document'\nrequire'rexml/element'\n\n/**\n *  @file oglplus/text/buffer.hpp\n *  @brief Buffer wrapper\n *\n *  @author Matus Chochlik\n *\n *  Copyright 2010-2014"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self._key_names\n\n\t\n############################################################\n########  Key and Value Attributes  #########\n\n# List that holds the list of the key and value pairs of the\n# key and value attributes of a record.\n# \n\nclass KeyValPairs(KleinNumpyRecord):\n\t\"A class to hold the list of key and value\"\n\t\n\tdef __init__(self, record):\n\t\tKleinNumpyRecord.__init__(self, record)\n\t\n\tdef keys(self):\n\t\treturn self._key_names\n\t\n\tdef values"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self.__dict__.keys()\n\ndef __add_field__(self, new_dict, key):\n\t\"\"\"\n\tAdds a new field to the record.\n\t\n\tIf new_dict is another Record, this function will recursively add\n\tall fields.  Otherwise, a new field will be added.\n\t\n\t:param dict new_dict: a dictionary with fields to add\n\t:param key: The name of the field to add\n\t:return: None\n\t\"\"\"\n\t# If new_dict is a Record, make it a list of records.\n\tif isinstance("}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self._keys\n\n\t\n# ------------------------------------------------------------\n\nclass _DictRecordData(object):\n\t\n\t# Internal\n\tdef __init__(self, dict_data={}, dict_id=0, keys=(), meta_data=0, key_value_count=[]):\n\t\tself._dict_data = dict_data\n\t\tself._dict_id = dict_id\n\t\tself._keys = keys\n\t\tself._meta_data = meta_data\n\t\tself._key_value_count = key_value_count\n\t\n# ------------------------------------------------------------\nclass _Dict"}]}
{"task_id": "62e60ecfd76274f8a4026d6a", "prompt": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version:\n\t\tversion, _ = protocol_version\n\t\tassert isinstance(version, tuple)\n\t\treturn cls.protocol_handlers.get(version)\n\treturn cls.protocol_handlers\n\n\ndef handlers(protocol_version=None, handlers=None):\n\t\"\"\"\n\t\tReturn a dictionary of available Bolt protocol handlers,\n\t\tkeyed by version tuple.\n\t\tIf a protocol version is not provided, all available versions are\n\t\treturned.\n\t\n\t\tIf no handlers are provided, all available Bolt protocol\n\t\thandlers will be"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t# Ensure protocol handlers exist\n\t# (we don't need to worry about protocols on the global list,\n\t# unless they are specifically defined. This also prevents global\n\t# code being pulled into each of these places)\n\tif protocol_version is None or len(protocol_version)!= 2:\n\t\traise TypeError('protocol_version must be passed in a tuple')\n\tsupported = None\n\tfor protocol, protocol_version, handler_class in \\\n\t\tsix.iteritems(getattr(handlers, 'protocol', {})):\n\t\tif protocol_version is not None"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\tprotocol_version = (protocol_version, )\n\t\tversions = protocol_version\n\telse:\n\t\tversions = range(10, 27)\n\t\n\tversions.append(tuple(range(*i.tolist() for i in protocols))\n\t\t\tfor p_i,p_version in enumerate(versions))\n\t\n\tprotocols = {}\n\thandler_names = [name for name in cls.supported_protocol_names\n\t\t\tif p_i < len(versions)]\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tsupported_protocol_version = dict([\n\t\t(0, BoltProtocol),\n\t\t((0, 0, 0), (4, 3, 5))\n\t])\n\tif not protocol_version:\n\t\tprotocol_version = (0,)\n\tfor v, handler in cls.protocol_handlers.itervalues():\n\t\tif (handler.protocol_version_supported(protocol_version),\n\t\t\tprotocol_version):\n\t\t\tsupported_protocol_version[v] = handler\n\treturn supported_protocol_version\n\n\ndef protocol_version_supported(protocol"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tversion, handler_cls = _get_protocol_version(version=protocol_version)\n\treturn version\n\n\ndef protocol_features(protocol):\n\t\"\"\"\n\t\tReturn a dictionary of the features supported by a specific\n\t\tapplication protocol.\n\t\n\t\tThis can be used to check version compatibility when\n\t\tcomparing Bolt objects. Note that the return value's key\n\t\tis not necessarily the version. The values will match the\n\t\tsame protocol that has been used to build this version.\n\t\tAll the following values are compared:\n\t\t - bolt."}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version:\n\t\tversion_tuples = protocol_version\n\t\tif (not version_tuples[0] == version_tuples[1]):\n\t\t\traise TypeError(\"protocol versions must be equal.\"\n\t\t\t\t\t\"got: %r and %r\" % (version_tuples,\n\t\t\t\t\t\t\t\t\t\t\t   version_tuples[1:]))\n\t\tprotocol_versions = [version_tuples]\n\telse:\n\t\tprotocol_versions = [\"all\"]\n\t\t\t# Default protocol version for backwards compatibility\n\t\t# with old database versions.\n\n\treturn { v:"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tprotocol_version = protocol_version or (3, 5,)\n\tif not protocol_version:\n\t\treturn {}\n\tversions = {}\n\tfor vers in range(1, protocol_version + 1):\n\t\tversions[vers] = cls.protocol_handler(vers, protocol_version)\n\treturn versions\n\n\ndef protocol_handler(proto_version, protocol_version):\n\t\"\"\"\n\t\tReturn a Bolt protocol handler matching a given version tuple.\n\t\n\t\t:param proto_version: a protocol version tuple like (3, 5)\n\t\t\tor"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\thandlers = {}\n\tif protocol_version is None:\n\t\tprotocol_versions = [1, 2]\n\telse:\n\t\tprotocol_versions = [protocol_version]\n\tversion = ProtocolVersion(protocol_versions[0])\n\tif version in protocol_versions:\n\t\thandlers[version] = ProtocolHandler(version)\n\t\thandlers[version.next] = ProtocolHandler(version.next)\n\telse:\n\t\tfor version_minor in range(1,protocol_versions[-1]+1):\n\t\t\tversion_class = version_minor\n\t\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_version = tuple()\n\n\tpossible = cls._get_supported_versions()\n\tfor ver, c in possible:\n\t\tif protocol_version >= ver and c.name is None:\n\t\t\treturn c\n\n\traise TypeError('unsupported protocol version tuple: {!r}'\n\t\t\t\t\t .format(protocol_version))\n\n\n# -------------------------------------------------------------- protocol_handlers_by_level, protocol_handlers_by_version\ndef _handler_by_level(protocol_version):\n\t\"\"\"\n\tReturns a dictionary of handlers to"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_versions = PROTOCOLS\n\t\tprotocol_versions_set = True\n\telse:\n\t\tprotocol_versions_set = False\n\tprotocol_versions = {\n\t\tversion: cls for version in protocol_versions if version >= max_version\n\t\t}\n\tfor version in protocol_versions:\n\t\tversion = min(version, max_version)\n\t\tif protocol_version is None or version in protocol_version:\n\t\t\treturn [self.handler(version, protocol_version=version)]\n\tif protocol"}]}
{"task_id": "62e60e49d76274f8a4026d25", "prompt": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t# if timeout is omitted, try to fallback to default timeout\n\tif timeout is None:\n\t\ttimeout = config.read_value('dbms.transaction.timeout')\n\t# allow for specifying a maximum transaction time if either timeout or transaction configuration is specified to use it's default timeout\n\tif (timeout is None) or (timeout <= 0):\n\t\tf_timeout = float(DEFAULT_TRANSACTION_TIMEOUT)\n\t\ttimeout = timeout if timeout is not None else f_timeout\n\t# check to see if timeouts specified are reasonable\n\tif settings.get_setting(\"dbms."}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tif timeout is None:\n\t\ttransaction_timeout = configuration.get(\"dbms.transaction.timeout\", 300)\n\t\tif transaction_timeout < 0:\n\t\t\traise ConfigurationError(\"The transaction timeout must be greater than 0.\")\n\telse:\n\t\ttransaction_timeout = float(timeout)\n\n\tdef _make_query(*args, **kwargs):\n\t\treturn super()._make_query(*args, **kwargs)\n\t_query = dbms.setTXMetaData(_make_query, \"setTxMetaData\", transaction_timeout=transaction_timeout)\n\t_"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tif timeout is None:\n\t\ttimeout = kwargs.pop(\"timeout\", None)\n\t\n\t# The __init__ method takes the attributes from the class\n\t# and combines them and the values set to this function\n\t# to make a fully initialized transaction.\n\t# This is necessary for the __init__ wrapper which takes\n\t# self as an argument while the __init__ and __new__ methods only\n\t# accept a metadata dictionary.\n\t\n\t# We check for existing transaction, if it exists, then create\n\t# it.\n\ttransaction = TransactionManager.getTransaction()"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tRETURN = auto_execute\n\t\n\tdef set_tx_context(self, tx_context):\n\t\tself._txcontext = tx_context\n\t\tself._txcontext.setdefault(\"transaction-id\", uuid.uuid4())\n\t\tself._txcontext.setdefault(\"query-id\", uuid.uuid4())\n\t\tself._txcontext[\"transaction-id\"] = uuid.uuid4()\n\t\tself._txcontext[\"client-id\"] = \"node-local\"\n\t\tself._txcontext[\"client-type\"] = \"neo4j\"\n\t\n\tdef"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tif metadata is None:\n\t\tmetadata = {}\n\tif timeout is None: # pragma: no cover -- is 0\n\t\ttimeout = DEFAULT_TRANSACTION_TIMEOUT\n\tmetadata[\"timeout\"] = timeout\n\treturn DBAgentTransaction( metadata=metadata, timeout=timeout, )\n\n#pragma once\n\n#include \"../Core/Interface/NativeFunction.h\"\n\nnamespace Falltergeist\n{\n    namespace Event\n    {\n        class EventMessageProvider;\n        class EventMessage;\n  "}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tif timeout is None:\n\t\ttimeout = dbms.transaction.timeout\n\t\n\treturn TxWrapper(self, metadata=metadata, timeout=timeout)\n\t\n\t\n\t\n########################################################################################################################\n\ndef create_transaction(spec=None, **options):\n\t\"\"\"\n\tCreates a new database session in the current database cluster using a spec.\n\t\n\t:param spec:\n\t    a dictionary with spec.\n\t    Specification allows to extend database session creation with configurable data (that is specific to a transaction) or a database transaction.\n\t    Specifier should be a dictionary"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t@functools.wraps(func)\n\tdef tx_decorator(*args, **kwargs):\n\t\tif not isinstance(metadata, dict):\n\t\t\tmetadata = kwargs\n\t\ttimeout = kwargs.pop('timeout', config.TRANSACTION_TIMEOUT)\n\t\tif timeout < 0.0:\n\t\t\traise ValueError(\"invalid timeout {} (must be > 0)\".format(timeout))\n\t\ttransaction = transaction.Transaction(func(*args, **kwargs))\n\t\ttry:\n\t\t\ttime.sleep(2.0 * timeout)\n\t\texcept KeyboardInterrupt:\n\t\t"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tif metadata is None:\n\t\tmetadata = {}\n\tif timeout is None:\n\t\tlog.log(DEBUG, 'using default timeout of %d seconds', DEFAULT_TRANSACTION_TIMEOUT)\n\telse:\n\t\tlog.log(DEBUG, 'using specified timeout of %d seconds', timeout)\n\t\n\tif'sqlalchemy' in sys.modules:\n\t\t# use sqlite database for database transactions (requires 'django' in order to be able to work inside SQLite)\n\t\tengine_module = __import__('db', globals(), locals(),['mysql','sqlite', '"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tif callable(timeout) and callable(metadata):\n\t\traise DatabaseException(\"`dbms.transaction.timeout` and `dbms.metadata.timeout` parameters can not be used with a decorator.\")\n\tdbms = Database(metadata)\n\tdef wrapped(func): return decorator(func, timeout=timeout)\n\treturn function_shim(dbms.connection.open, wrapped)\n\n\ndef timeout(duration, function):\n\t\"\"\"Wraps a function that will always timeout and never throw a TimeoutException.\n\t\n\t    :param duration:\n\t      "}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tif not metadata:\n\t\tmetadata = {}\n\tif isinstance(metadata, dict):\n\t\ttransaction_id_metadata = {}\n\t\tfor k, v in metadata.items():\n\t\t\tif isinstance(v, six.string_types):\n\t\t\t\ttransaction_id_metadata[k] = v\n\t\t\t\n\t\ttry:\n\t\t\tdbms.setTXMetaData(transaction_id_metadata)\n\t\texcept Neo4jError as e:\n\t\t\ttransaction.exception(\"Exception in setTXNMetadata: %s\" % str(e) if sys.version_"}]}
{"task_id": "62e60e05d76274f8a4026cfd", "prompt": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self._iter.index(key)\n\ndef count(self):\n\t\"\"\"\n\tReturn the number of items in this view.\n\t\n\t:return: the count of the number of items in this view\n\t:rtype: int\n\t\"\"\"\n\treturn self._length\n\n/* SPDX-License-Identifier: GPL-2.0-only */\n\n#ifndef __SOC_MEDIATEK_MT8195_GPIOS_H__\n#define __SOC_MEDIATEK_MT8195_GPIOS_H__\n\n#include <soc/gpio_defs.h>"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\tv = self._get(key)\n\tif v is not None:\n\t\treturn v\n\tv = len(self._items)\n\treturn v\n\n\t\nclass _Item(dict):\n\t\"\"\"\n\t\tA dictionary class that has a unique key and value associated by \n\t\tan array of items.\n\t\"\"\"\n\tdef __init__(self, length):\n\t\t\"\"\"\n\t\tConstructor.\n\t\t:param length: value count\n\t\t\"\"\"\n\t\tsuper(_Item, self).__init__()\n\t\tself.key = np.empty("}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\tif not key \\\n\tand not isinstance(key, (types.BaseString, types.UnicodePKey,\n\t\tenums.RegistryHive, types.AutoKeyHKey)):\n\t\traise TypeError('key must be a string or a Key object or Regkey')\n\treturn self.get_value_or_raise('sIndex', -1)\n\t\n\t\t#if isinstance(key, types.AutoKeyHKey):\t\t# Registry key\n\t\t#\ttry:\n\t\t#\t\treturn self.get_value_or_raise('dwIndex', -1"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn -1, -1\n\ndef length(self, key):\n\t\"\"\"\n\tReturn the number of items with a given key.\n\t\n\t:param key: a key\n\t:return: length\n\t:rtype: int\n\t\"\"\"\n\treturn 0, 0\n\ndef __contains__(self, key):\n\t\"Return True if a key is in the mapping\"\n\t\n\treturn False\n\ndef to_python(self, key, value, **kwargs):\n\t\"Update the value of a key, replacing any existing definition.\"\n\ttry:\n\t\tself["}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.array.index(self.index(key))\n\ndef key(self, index):\n\t\"\"\"\n\t\tReturn the key at the given index.\n\t\n\t:param index: an index\n\t:return: the key at given index\n\t:rtype: str\n\t\"\"\"\n\treturn self.array.get_key(self.index(index))\n\ndef get(*args):\n\t\"\"\"\n\t\treturn the item at an index from the array or from an iterable\n\t\"\"\"\n\tif not args or not args[0] in (self.index, self"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.get(key, 0)\n\ndef list(self):\n\t# I am an error\n\treturn self.d\ndel del list\n\ndef len(self):\n\treturn len(self.d)\n\ndef iter(self):\n\tfor key in self:\n\t\tyield key, self[key]\n\tyield None, self.d\n\ndef iteritems(self):\n\tfor key, value in self.iter():\n\t\tyield key, value\n\ndef find(self, **key_args):\n\tif key_args:\n\t"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn len(self.data + [key])\n\n# -----------------------------------------------------------------------------\ndef append(*args, **kwargs):\n\t\"\"\"\n\t\tAppend data to the items at one time with optional keyword args.\n\t\t\n\t\t:param args: a list of items\n\t\t:param kwargs: a dictionary of additional args\n\t\t:return: a new instance\n\t\t:rtype: :class:`Item.Item`\n\t\"\"\"\n\titems = []\n\tfor item in kwargs.pop('items', None):\n\t\tif item:\n\t\t\titem = deepcopy(item)\n\t\t\titems"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\ttry:\n\t\tindex = self.__map.index(key)\n\texcept ValueError:\n\t\ttry:\n\t\t\treturn len(self.__map) - 1\n\t\texcept ValueError:\n\t\t\ttry:\n\t\t\t\treturn len(self.__map) - 2\n\t\t\texcept ValueError:\n\t\t\t\traise ValueError('Key %s not found.' % repr(key))\n\treturn index\n\n\ndef keys(self):\n\t\"\"\"\n\tReturn the keys as a list of the indices inside of the array.\n\t\n\t:return: keys\n\t:rtype: list\n"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\tif self._keyToIdx:\n\t\treturn self._keyToIdx.get(key, -1)\n\telse:\n\t\ttry:\n\t\t\treturn self._keyVals.index(key)\n\t\texcept ValueError:\n\t\t\traise ValueError(\"Index %s not for this key %s\"%(index,key))\n\n\t\ndef get_key(self, key):\n\t\"\"\"\n\tReturn the key at the given index.\n\t\n\t:param key: a key\n\t:return: key\n\t:rtype: int\n\t\"\"\"\n\tself._get"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self._index.get(key, -1)\n\t\n\t\n#######################################################################\ndef dict(self):\n\t\"\"\"\n\tReturn the complete dictionary.\n\t\n\t:return: Dictionary\n\t:rtype: dict\n\t\n\t\"\"\"\n\treturn dict(self._dict.keys())\n\t\t\n#######################################################################\ndef index_data(self, index):\n\t\"\"\"\n\tReturn the first item value in the set.\n\t\n\t:param index: the index\n\t:return: first value\n\t:rtype: Any\n\t\"\"\"\n\treturn self.data[index].data"}]}
{"task_id": "62e60da4d76274f8a4026cf1", "prompt": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\ttry:\n\t\treturn [getattr(self, key) for key in keys]\n\texcept AttributeError:\n\t\treturn []\n\n\ndef keys(self):\n\t\"\"\"\n\tReturn a list of keys for the list of items included by this\n\tlist or list-type table, or only an empty list if the list is\n\tnot empty.\n\t\"\"\"\n\ttry:\n\t\treturn [getattr(self, key) for key in self.keys]\n\texcept AttributeError:\n\t\treturn []\n\nrequire_rv64;\nrequire_extension(EXT_ZHU"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t# no 'keys' parameter, no 'keys' parameter should be passed into the\n\t#   `select` method as we want to filter for a specific record\n\tif keys is None:\n\t\tkeys = self.__keys\n\treturn self.__fields[keys]\n\n#-------------------------------------------------------------------------\n#\n# class _RecordMap\n#\n# The _RecordMap is an object used to map a key to a record.\n#\n#-------------------------------------------------------------------------\n#\nclass _RecordMap:\n#\n\t\"\"\"\n\tA map object to represent a dictionary of keys to records.\n\n\t"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tif self._fields is not None:\n\t\tfor i in self._fields:\n\t\t\titems = self._items(i)\n\t\t\tif keys is None or self._key_field:\n\t\t\t\titem_values = [x[i] for x in items]\n\t\t\telse:\n\t\t\t\titem_values = [items[k] for k in keys]\n\t\t\tself._items_filter(item_values = item_values)\n\n\t\treturn self._list()\n\telse:\n\t\treturn self._list()\n\n\n\nif __name__"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tfrom.core import db\n\t\n\tif len(keys) > 0: self.__keys = [keys]\n\tdbvals = db.select(r'%s' % self.__class__.__name__, self.__keys, self.__values)\n\t\n\tif not self.__only_on_index:\n\t\tdbvals = dbvals[self.__index]\n\t\n\treturn dbvals\n\ndef save(self, *keys):\n\t\"\"\"\n\tSave the record and all sub-records to the database\n\n\t:param keys: indexes or keys of the items to include; if none\n"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\trv = []\n\tif keys and 0 < len(keys):\n\t\tif isinstance(keys, str):\n\t\t\tkeys = (keys,)\n\t\tfor idx in keys:\n\t\t\trv = self.values(idx)\n\t\t\tif len(rv) > 0:\n\t\t\t\tbreak\n\t\telse:\n\t\t\trv = []\n\t\t\n\t\treturn rv\n\telse:\n\t\tvalue_info = self.values()\n\t\tif isinstance(value_info, (list, tuple, set)):\n\t\t\trv = range(len(value_info))"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tif keys == NONE:\n\t\treturn super(DictMixin,self).__values()\n\tassert set(keys) == set(self.__data)\n\tif hasattr(self, 'items'):\n\t\treturn list(map(self.values(),keys))\n\treturn []\t\n\nclass Dict(DictMixin,dict):\n\n\t__data=None\n\n\tdef __setitem__(self, key, value):\n\t\tself.__data.append([key,value])\n\t\treturn DictMixin.dict.__setitem__(self, key, value)\n\n\tdef __del"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tassert isinstance(keys, Iterable), \\\n            \"keys must be iterable if items are not required!  (%r)\" % keys\n\tassert all(isinstance(k, six.integer_types) for k in keys)\n\n\tdef process_record(record):\n\t\tif isinstance(record, dict):\n\t\t\treturn [values(r) for r in records(record)]\n\t\telse:\n\t\t\treturn [values(record)]\n\t\n\tif self.keys and all(key not in self.keys for key in keys):"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tself._ensure_read()\n\t\n\tif not keys:\n\t\tkeys = self._keys[:]\n\ti = 0\n\tif 1 in self._indexes:\n\t\ti = self._indexes[1]\n\tvals = []\n\tfor key in keys:\n\t\tif key not in self._values[i]:\n\t\t\tlogger.debug3(\"invalid key, not included: \"+key)\n\t\t\tcontinue\n\t\tvals.append(self._values[i][key])\n\treturn vals\n\n\ndef get_key_value(self, index: int)"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tif not keys:\n\t\tkeys = range(len(self.__slots__))\n\treturn [[self.__slots__[index][key] for index,key in enumerate(keys)] for value in values(None)] \n\ndef __copy(obj):\n\t\"\"\"\n\tCopy the instance properties of the input object, \n\tincluding the ``__slots__``, the ``__dict__``, and the\n\t``__weak_import__`` attributes.\n\t\n\t:param dict obj: an input object\n\t:rtype: dict\n\t\"\"\"\n\tattrs = { key: getattr(obj, key)"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\treturn [x._i for x in self._values(index=True) \\\n\t          if not self._key(x._i)]\n\ndef values2(self, *keys):\n\t\"\"\"Return the values of the record of this dictionary\"\"\"\n\tret = []\n\tfor key in keys:\n\t\tif self._has_key(key):\n\t\t\tret.append(self._get(key))\n\tret.extend(_get_slice(self, key) for key in keys)\n\treturn list(ret)\n\ndef values"}]}
{"task_id": "62e60b10d76274f8a4026ccd", "prompt": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tif len(keys) > len(self):\n\t\traise IndexError(\"Out of bounds\")\n\tret = {}\n\tfor item in self[1:]:\n\t\tk, v = _aslist(item, *keys)\n\t\tif not 0 <= k < len(keys):\n\t\t\traise IndexError(\"Out of bounds\")\n\t\tif k < len(items) and not is_valid_field(items[k][1]) and not isinstance(items[k][1], dict):\n\t\t\traise IndexError(\"Invalid index %i\" % items[k"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tresult = {}\n\tfor field in self:\n\t\ttry:\n\t\t\tkey = field.key()\n\t\texcept KeyError as err:\n\t\t\traise DataError(field, self, key, err)\n\t\tif type(key) is not tuple:\n\t\t\tkey = (key,)\n\t\tresult[field.name] = key\n\tfor field in self:\n\t\ttry:\n\t\t\titem = field.item()\n\t\texcept KeyError as err:\n\t\t\traise DataError(field, self, field, err)\n\t\tif type("}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tif keys is None:\n\t\tkeys = ()\n\tif type(keys) is str:\n\t\tkeys = (keys, )\n\telif type(keys) is tuple:\n\t\tif not len(keys):\n\t\t\tkeys = None\n\tif isinstance(keys, basestring):\n\t\tkeys = list(keys)\n\n\tdata = []\n\tfor name in keys:\n\t\tdata.append((None, None, None, None, None, None, None, None, None))\n\t\tif name in self:\n\t\t\tfor key in self["}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tfrom pyglet\n\tfrom pyglet.graphics import data, image, text, transform\n\n\tfor index, key in enumerate(keys):\n\t\titems = data(self._data, index)\n\t\tif isinstance(key, basestring):\n\t\t\tself._list.insert(items, key)\n\t\t\treturn\n\t\telif isinstance(key, tuple):\n\t\t\titems[key[0]] = key[1]\n\t\t\tcontinue\n\t\tif len(items):\n\t\t\ttry:\n\t\t\t\tvalue = item[key]\n\t\t\texcept KeyError"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tif self.reccount > 0 and self.fieldcount > 0 and len(keys):\n\t\traise TypeError('Cannot append to data on an unsorted record')\n\t\n\treturn dict((key, field.data(key, value)) \n\t\t\tfor key, field\n\t\t\twith self.field(key, self.reccount) \n\t\t\tfor value in keys)\n\n\n# -- field definitions ---------------------------------------------------\ndef makefield(cls, name, default=None, type=None,  # PY3K\n              reprfunc"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tif not (self.fields or self.items or self.defaults):\n\t\traise TypeError(\"This object is not part of a record.\")\n\n\tif not isinstance(keys, (list, tuple)):\n\t\tkeys = [keys]\n\n\ttry:\n\t\treturn dict(map(lambda c: self[c].get(keys), zip(self.fields, self.items)))\n\texcept (KeyError, IndexError):\n\t\tpass # key not found in dict\n\n\tif len(keys) == 1:\n\t\ttry:\n\t\t\treturn dict("}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tif self['__idtype'] is None:\n\t\traise ValueError('Records of type not yet implemented.')\n\tres = self.items() if not keys else self._insert_unique(*keys)\n\tif len(res)!= len(keys):\n\t\traise KeyError(\n\t\t\tkey_type='set data',\n\t\t\tkey_set_repr='<%s items: %s>; <%s keys: %s>' % (\n\t\t\t\tself.keys(),\n\t\t\t\tlen(self),\n\t\t\t\tlen(keys),\n\t\t\t\tstr(keys),\n\t\t\t),"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\treturn dict(\n\t\tself.items(**{key: value})\n\t\tfor key, value in imap(lambda item: item[key], *items)\n\t)\n\n\n#=====================================================================================================================\n#=====================================================================================================================\n\n\nclass DatasetRecord(object):\n\t\"\"\"\n\tA record in a :class:`Dataset`.\n\t\"\"\"\n\n\t__slots__ = ('filename', 'keys')\n\n\tdef __init__(self, filename, keys=None):\n\t\tself.filename = filename\n\t\tself.keys = keys\n\n\tdef as_dict("}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\treturn self._table._data([self], keys)\n\n### Utility methods\n\ndef listval(self, key, default, null=False):\n\t\"\"\"\n\tReturn this list's value if present, otherwise return the given\n\tdefault value, depending on the value of :const:`null` or if no\n\tvalue is set at all. An empty list or None will trigger a\n\tIndexError.\n\t\n\t:param key: key to look up\n\t:param default: default value to return if :const:`null` or if\n\t          "}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tnew = {}\n\tfor key, val in zip(keys[0], self.__fields_all[key]):\n\t\tif val: new[key] = val\n\treturn new\n\n\nclass dict(dict_type):\n\t\"\"\"\n\tdict(dict_type) -> dict\n\t\t\n\tA general purpose record class to handle simple type\n\tdictionaries.\n\t\"\"\"\n\t\n\tdef __init__(self, *fields, **kw):\n\t\tsuper().__init__()\n\t\tself.__fields = self.make_fields_dict(*fields)\n\t\tself.__dict"}]}
{"task_id": "62e6087bd76274f8a4026bfa", "prompt": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn self.pop_u8() & 0xFFFF\n\ndef pop_u32(self):\n\treturn struct.unpack('>I', self.pop_u8() & 0xFF)\n\ndef pop_u64(self):\n\treturn struct.unpack('<Q', self.pop_u8() & 0xFF)\n\ndef pop_string(self):\n\t\"\"\"\n\tTake all data written to the current transport, ending with a zero, and\n\treturn it in a freshly-constructed string buffer.\n\t\"\"\"\n\tstart = self.tell()\n\tdata"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn self.pop() * 0x100 + self.pop()\n\n\ndef pop_u32(self):\n\"\"\"\n\tRemove the last four bytes of data, returning them as a big-endian\n\t32-bit unsigned integer.\n\t\"\"\"\n\treturn self.pop() * 0x100000000 + self.pop() * 0x100000000 + self.pop() * 0x100000000 + self.pop() * 0x100000000 + self.pop() * 0x100000000 + self.pop() * 0x100000000 + self.pop() * 0x100000000 + self.pop() * 0x"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn self.get_u16_be(self.data[-2]) << 16\n\t\n\t\ndef pop_u8(self):\n\treturn ord(self.data[-1])\n\t\n\t\ndef pop_bytes(self, n):\n\t\"\"\"\n\t\tRemove *n* of data from the data stream.\n\t\"\"\"\n\treturn self.data[-n:]\n\n\n\n\ndef decode_base64(base64_dict, skip=0):\n\n\ttry:\n\t\tout = ''\n\t\tencoded = []\n\t\tl = len(base"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn ((self.data[self.size - 2] << 8) | self.data[self.size - 1])\n\ndef pop_u32(self):\n\t\"\"\"\n\tRemove the last four bytes of data, returning them as a big-endian\n\t32-bit unsigned integer.\n\t\"\"\"\n\treturn ((self.data[self.size - 4] << 24) | ((self.data[self.size - 3] << 16) | ((self.data[self.size - 2] << 8) | self.data[self.size - 1])))\n\n#"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn(ord(self.slice[0:2])\n\t\t+ ushort.from_bytes(self.slice[2:4]))\n\ndef pop_u32(self):\n\t\"\"\"\n\tRemove the last four bytes of data, returning them as a big-endian\n\t32-bit unsigned integer.\n\t\"\"\"\n\treturn(ord(self.slice[0:4])\n\t\t+ ushort.from_bytes(self.slice[4:8]))\n\ndef pop_u64(self):\n\t\"\"\"\n\tRemove the last eight bytes of data, returning them"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tself.buf.reverse()\n\treturn self.buf[-2*self.bytes:]\n\ndef pop_u32(self):\n\t\"\"\"\n\tRemove the last two characters of data, returning them as a big-endian\n\t32-bit unsigned integer.\n\t\"\"\"\n\tself.buf.reverse()\n\treturn struct.unpack(self.big_endian_format,\n\t\tself.buf[self.bytes - 2*self.int_len : self.int_len])[0]\n\ndef pop_u64(self):\n\t\"\"\"\n\tRemove the last"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn (ord(self[self.index-2]) << 8) | ord(self[self.index-1])\n\ndef pop_u32(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t32-bit unsigned integer.\n\t\"\"\"\n\treturn (ord(self[self.index-4]) << 24) | (ord(self[self.index-3]) << 16) | (ord(self[self.index-2]) << 8) | ord(self[self.index-1])\n\t\ndef"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tif not self.data:\n\t\treturn 0, 0\n\treturn (self.data[0] << 8) | self.data[1]\n\t\n\tdef get_length():\n\t\treturn 2\n\nclass Packet(PacketBase):\n\t\"\"\"\n\tA Packet of bytes in network order.\n\t\"\"\"\n\tdef __init__(self, data=None, length=0):\n\t\t\"\"\"\n\t\t@param data:\n\t\t\tThe data that represents the packet.  It is used to set the\n\t\t\theader fields in the next packet of the connection. "}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn self._array[len(self._array)-2]\n\ndef pop_u32(self):\n\t\"\"\"\n\tRemove the last four bytes of data, returning them as a big-endian\n\t32-bit unsigned integer.\n\t\"\"\"\n\treturn self._array[len(self._array)-4]\n\ndef pop_u64(self):\n\t\"\"\"\n\tRemove the last eight bytes of data, returning them as a big-endian\n\t64-bit unsigned integer.\n\t\"\"\"\n\treturn self._array[len(self._array)-8]\n\nclass"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tself.seek(3)\n\tdata = _endianess_big_int.pop_value(self)\n\t# Strip out the b'1's, b0's.\n\twhile data & 0x4000:\n\t\tdata = _endianess_big_int.pop_value(self)\n\tdata = data & 0xFF\n\t# The next byte is an odd number\n\treturn data.to_bytes(2, 'big')\n\n#def pop_ubyte(self):\n#\t\"\"\"\n#\tSame as pop_byte, except it skips the first two"}]}
{"task_id": "62e6087ad76274f8a4026bf2", "prompt": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tpass\n\n\nclass Query:\n\t__slots__ = ('id',\n\t\t\t\t\t'q',\n\t\t\t\t\t'description',\n\t\t\t\t\t'date',\n\t\t\t\t\t'errors',\n\t\t\t\t\t'subrecord_hooks',\n\t\t\t\t\t'hydrator_hooks',\n\t\t\t\t\t'response_hook',\n\t\t\t\t\t'next_id',\n\t\t\t\t\t'serialization_funcs',\n\t\t\t\t\t'subrecords',\n\t\t\t\t\t'subrecords_ids',\n\t\t\t\t\t'subrecords_infos',\n\t\t\t\t\t'result_hook')\n\tpass\n\n\nclass Execution:\n"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\thandler_names_map = {\n\t\tNone: \"DISCARD\",\n\t\t\"RETAINING\": \"DISCARD_REUSING_PACKSTREAMS\",\n\t\t\"DISCARDING\": \"DISCARD_DISCARDING_PACKSTREAMS\",\n\t}\n\t\n\tdef __init__(self, n=-1,\n\t\t\tqid=-1, dehydration_hooks=None,\n\t\t\thandlers=None,\n\t\t\thandler_filters=None):\n\t\tassert handlers is None or isinstance(handlers, dict)\n\t\tassert set((x[0].__name__, x[1].__"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tif n is None:\n\t\tn = -1\n\tif dehydration_hooks is None:\n\t\tdehydration_hooks = {}\n\tif hydration_hooks is None:\n\t\thydration_hooks = {}\n\t# Create the new message\n\tmessage = self.message(format=1) # message format = 1 (pack)\n\n\tfor type_, hook in packstream_dehydrations2hook(dehydration_hooks):\n\t\tresponse = hook(self, message, *self.args)\n\t\tif"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tparser = self._make_parser()\n\trequest = Message()\n\trequest.add_header(*Message.ID, value=self._request_id)\n\trequest.add_header(*Message.NUM_RECORDS, value=n)\n\trequest.add_header(*Message.TARGET_ID, value=self.ID)\n\trequest.add_header(*Message.TARGET_PARENT_ID, value=self._target_id)\n\trequest.remove_header(*Message.TIMESTAMP)\n\tself._message_handler.queue_message(request, self._queue)\n"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\td = self.create_response()\n\tresponse = DISCARD(d)\n\tresponse.set_appid(self.appid)\n\tresponse.set_n(n)\n\tresponse.set_qid(qid)\n\t\n\tif dehydration_hooks:\n\t\tfor hook in dehydration_hooks:\n\t\t\td[hook(response, 0)] = None\n\t\n\tif hydration_hooks:\n\t\tfor type, dfn in hydration_hooks.items():\n\t\t\td[type](dfn(response),"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tif dehydration_hooks is None:\n\t\tdehydration_hooks = {}\n\n\tif not isinstance(dehydration_hooks, Iterable):\n\t\tdehydration_hooks = (dehydration_hooks,)\n\t\n\tdef dehydrate(request, *args, **kwargs):\n\t\tif dehydration_hooks:\n\t\t\t# Get only the results\n\t\t\tresults = dehydration_hooks.get(*args, **kwargs)\n\t\t\t\n\t\t\tif n == 0:\n\t\t\t\tresults['OK'] = None\n"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tif _droppable_handler(self.response._handler)[Droppable]:\n\t\tresponse = self.response\n\t\ttry:\n\t\t\t_droppable_handler(self.response._handler)[Droppable]\n\t\tfinally:\n\t\t\tif dehydration_hooks is not None:\n\t\t\t\tdel handlers._dehydration_hooks_for\n\t\t\tfor k, v in dehydration_hooks.items():\n\t\t\t\tdel handlers._dehydration_hooks_for[k][i for i in v]\n\n\t\t"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tret = 'n/A'\n\tif isinstance(n, dict):\n\t\tret = ', '.join('n=%s'%n)\n\telse:\n\t\tret = ''\n\tret += ', qid=%d,' % qid\n\tif dehydration_hooks:\n\t\ttmp = ', '.join('DEHYDRATION_HOOKS=%s' % ', '.join('%s = %r' % (k, k.hydration_function) for k in dehydration_hooks.keys()))\n\t\tret += tmp\n\tif n == 0:"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\ttry:\n\t\tself.queue.clear()\n\t\tdel self.queue[:]\n\texcept:\n\t\timport traceback\n\t\tprint traceback.format_exc()\n\t\tself.queue = None\n\n    def __init__(self, queue, message_id=None):\n        \"\"\"\n        :param queue: input queue to pack to and decode, default = None\n\t\t:param message_id: query ID to dehydrate, default = None\n\t\t\"\"\"\n\t\tsuper(Packstream,"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself.last_query_id = n if n >= 0 else self._n.get()\n\tself.last_query = None\n\tif dehydration_hooks!= None:\n\t\tself._hook = None\n\t\tunhooked = [item for item in dehydration_hooks.values()]\n\t\tdehydration_hooks.clear()\n\t\tif dehydration_hooks!= {} and len(unhooked):\n\t\t\tself._hook = deque(unhooked)\n\t\tdehydration_hooks = dehydration"}]}
{"task_id": "62e60879d76274f8a4026bec", "prompt": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tif not hasattr(self, \"_packstream\"): raise AttributeError(\"the Request object must have a `packstream` attribute\")\n\twith self.lock:\n\t\tif mode == \"*\":\n\t\t\tif self.metadata is None: raise AttributeError(\"the metadata object must be specified\")\n\t\t\tself.packstream.Begin(self.metadata)\n\t\t\tself.packstream.PutLong(pack.MAGIC)\n\t\t\tself.packstream.PutLong(0)\n\t\t\tself.packstream.PutLong(0)\n\t\t\tself.packstream.PutLong(1)"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tpass\n\n#\n# Beginning of transaction.\n#\nclass BeginTransaction:\n\t\"Begins a transaction.\"\n\tdef __new__(cls, app_id, txn_id, user, password):\n\t\t\"\"\"\n\t\t:param app_id: ID of the app this transaction is part of\n\t\t:param txn_id: ID of the transaction this is a part of\n\t\t:param user: user the transaction applies to\n\t\t:param password: the password for this transaction\n\t\t:return: Transaction object\n\t\t\"\"\"\n\t\tpass\n\n"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tif timeout is None:\n\t\ttimeout = DEFAULT_TIMEOUT\n\tif mode not in (\"READ\", \"WRITE\"):\n\t\traise BoltError(\"Illegal mode %s passed to transaction_begin().\" % mode)\n\tif metadata is None:\n\t\tmetadata = {}\n\tif timeout is None:\n\t\ttimeout = DEFAULT_TIMEOUT\n\tif timeout > 120*1000: timeout = 120*1000\n\tself.__mode = mode\n\tself.__db = db\n\tself.__db_name = db if db!= 'bolt' else self.config.get_db(db"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tmsg = Message()\n\tmsg['Database'] = self.database\n\tmsg['Mode'] = 'WRITE'\n\n\tif self.preprocess_hook or self.postprocess_hook:\n\t\tmsg, = unpack(\n\t\t\tmsg, _send_message,\n\t\t\t(self.preprocess_hook, self.postprocess_hook,\n\t\t\tself.preprocess, self.postprocess, self.postprocess_hook)\n\t\t)\n\n\tif bookmarks is None and metadata is None and timeout is not None:\n\t\traise ValueError(\"Cannot have"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tif self.__transaction:\n\t\tif isinstance(self.__transaction, Transaction):\n\t\t\tassert isinstance(self.__transaction.hydration_hooks, dict)\n\t\t\tassert isinstance(self.__transaction.hydration_hooks.values(), list)\n\t\t\tassert set(self.__transaction.hydration_hooks.keys()) == set(\n\t\t\t\tself.__transaction.hydration_hooks.values()\n\t\t\t)\n\t\t\traise ValueError(\"'transaction' cannot be a dict\")\n\t\n\tself.__transaction = dict(\n\t\tcreated_time=datetime.datetime."}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tpass\n\n\n\tdef commit(self, bookmarks=None):\n\t\tpass\n\n\n\tdef finish(self):\n\t\tpass\n\n\n\tdef flush(self):\n\t\tpass\n\n\n\tdef __init__(self, packstream, db, imp_user, dehydration_hooks,\n\t             handler_hooks, handlers):\n\n\t\tsuper().__init__()\n\n\t\tself.packstream           = packstream\n\t\tself.db   "}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tif metadata is None:\n\t\tmetadata = {}\n\telif not isinstance(metadata, dict):\n\t\t# FIXME: what now? If the returned Response object has the\n\t\t# same interface, dehydrate metadata at the beginning.\n\t\t# We should probably rethink this.\n\t\traise ValueError(\"Metadata must be a dict but {0!r} is not\".format(metadata))\n\t\n\treturn get_db().begin(mode=mode, bookmarks=bookmarks, metadata=metadata, timeout=timeout, db=db, imp_user=imp_user,\n"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tconn = self._connect()\n\tif not hasattr(conn, \"BEGIN\") or not (not conn.is_transaction()\n\tor conn.is_transaction_active() is None):\n\t\traise ValueError(\"You must either execute transactions or use BEGIN (READ|WRITE).\"\n\t\t\"You passed a bad database name.\")\n\ttry:\n\t\tif mode is None and not conn.is_transaction():\n\t\t\traise ValueError(\"You must provide a valid mode value on the \"\n\t\t\t\"begin call\")\n\t\tif isinstance(bookmarks, list) and (len("}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tif not handlers:\n\t\thandlers = get_dispatcher()\n\tresult, response_data, bookmarks = self._handler._handle_begin(self.request, mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks, handlers)\n\treturn result\n\ndef add_bookmark(self, bookmark, metadata = None, timeout = None, result=None):\n\t\"\"\"\n\tAdds the bookmark to the bookmark list.\n\n\t:param bookmark: bookmark to add.\n\t:param metadata:"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\treq = {\n\t\t\"b\": \"INSERT\",\n\t\t\"i\": 1,\n\t\t\"c\": 1,\n\t\t\"h\": {},\n\t\t\"d\": {},\n\t\t\"a\": {\"m\": {}, \"a\": {}}\n\t}\n\tif metadata is not None:\n\t\treq[\"d\"][\"d\"] = metadata\n\tif mode is not None:\n\t\treq[\"d\"][\"r\"] = mode\n\tif bookmarks is not None:\n\t\treq[\"d\"][\"c\"] = bookmarks\n\tif timeout is not None:\n"}]}
{"task_id": "62e60723d76274f8a4026b75", "prompt": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\treturn n//2\n\ndef pow(x, y):\n\t\"\"\"\n\t:param x:\n\t:type x: int/long\n\t:param y:\n\t:type y: int/long\n\t:return:\n\t\"\"\"\n\n\tdef pow_mod(a, b):\n\t\treturn a // b\n\n\tif(not x or not y):\n\t\treturn\n\treturn pow_mod(x, y) % y\n\ndef isDivisibleBy(n, base=10):\n\tfor i in range(abs(base-n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif n < 0:\n\t\traise ValueError(\"Can't round negative to even\")\n\t\n\telif n == 0:\n\t\treturn 3\n\telif int(n) % 2 >= 1:\n\t\treturn\n\treturn round_half_to_even(float(n) / 2)\n\n\ndef round_half_to_even_up(n):\n\t\"\"\"\n\t\t>>> round_half_to_even_up(3)\n\t    3\n\t    >>> round_half_to_even_up(3.2)\n\t   "}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif math.isinf(float(n)):\n\t\treturn float('inf')\n\telse:\n\t\treturn round_half_to_even(float(n))\n\ndef round_even(n=0.0):\n\t\"\"\"\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif math.isinf(float(n)):\n\t\treturn math.inf if n in (0,1) else -1*n\n\telse:\n\t\treturn round_half_to_even(float(n))\n\ndef round_half_even"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t# n is float\n\treturn int(n) & 1\n\ndef round_half_to_even_rounding_up(n):\n\treturn (round_half_to_even(n) + 1)\n\ndef round_half_to_even_rounding_down(n):\n\treturn (round_half_to_even(n) - 1)\n\ndef round_half_to_even_power_of_two(n):\n\treturn 1 << ( ( __builtin__[\"poweroftwo(%d)\" % n] - 1) )\n\ndef round_"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif not isinstance(n, int):\n\t\traise TypeError(type(n)) from None\n\tif n <= 0:\n\t\traise ValueError(repr(n)) from None\n\treturn(n -.5)//2\n\ndef round_half_to_even_ceil(n):\n\tfrom decimal import Decimal\n\treturn round_half_to_even(Decimal(n))/2\n\n\ndef sqrt(value):\n\t\"\"\"\n\t>>> sqrt(2)\n\t2\n\t>>> sqrt(3)\n\t3\n\t>>> sqrt(3.2)\n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\twhile n!= 0.0:\n\t\tn &= n - 1\n\treturn n\n\n\n\n\ndef floor(num):\n\t\"\"\"\n\t:param float num:\n\t:return:\n\t\"\"\"\n\treturn floor(num + 0.5)\n\n\twhile num!= 0 and not (num == 0 and num == floor(num)):\n\t\tnum = num + 0.5\n\t\t\"\"\"\n\t\tif num < 0:\n\t\t\tnum = -num\n\t\tnum &= num - 1\n\treturn num\n\n\n\n\ndef floor_float"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif not 1 <= n <= 10:\n\t\traise ZeroDivisionError(f\"{n}\" )\n\n\ndef is_finite(x):\n\tif type(x) == float and x >= -1 and x <= 1:\n\t\treturn True\n\telif type(x) == int16_t:\n\t\treturn True\n\telif type(x) == int32_t:\n\t\treturn True\n\telif type(x) == float64_t:\n\t\treturn x == 1 or abs(1 - x) < EPS\n\telif type(x) =="}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\treturn int(math.ceil(n*0.5))-1\n\n\ndef round_half_to_odd(n):\n\"\"\"\n\t>>> round_half_to_odd(3)\n\t    3\n\t    >>> round_half_to_odd(3.2)\n\t    3\n\t    >>> round_half_to_odd(3.5)\n\t    4\n\t    >>> round_half_to_odd(3.7)\n\t    2\n\t    >>> round_half_to_odd("}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif n <= 0:\n\t\treturn 0\n\n\tn = int(n + 0.5)\n\tif n % 2 == 0:\n\t\treturn int(math.sqrt(n))\n\telse:\n\t\treturn n/2\n\ndef round_half_to_odd(n):\n\t\"\"\"\n\t\t:param n:\n\t\t:return:\n\t\"\"\"\n\tif n <= 0:\n\t\treturn 0\n\telif n % 2 == 0:\n\t\treturn int(math.sqrt(n))\n\telse:\n\t\treturn"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif n<0.0:\n\t\treturn 0\n\tr = int(round(fabs(n*pi))/pi)\n\treturn r if (r%2)==0 else r+1\n\ndef float_to_half(n, ndigits=None, sig=None):\n\t\"\"\"Returns a number representing the number `n` in the given number of digits and a\n\t`sig`.  If necessary, a rounded half-way is rounded. If no `sig` is provided, then any\n\tcomputed number is rounded to a number of `ndigits`. Returns the number if"}]}
{"task_id": "62e60707d76274f8a4026b69", "prompt": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\ttry:\n\t\tfield_list = fields\n\t\tclass_name = name\n\texcept:\n\t\traise\nelse:\n\tclass_name = name\n\tfield_list = fields\n\tclass FieldList(list):\n\t\tassert len(fields) == 0\n\t\tdef get_field(self, field_name):\n\t\t        if not field_name in self:\n\t\t        \traise ValueError, \"Field [%s] must be defined\" % field_name\n          "}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tc = Point(name, x)\n\tif not hasattr(c, 'x'):\n\t\ttry:\n\t\t\tc.x = int(srid_map[name+'x'][0])\n\t\texcept:\n\t\t\tc.x = 0 # Should work on ESRI\n\n\tif not hasattr(c, 'y'):\n\t\ttry:\n\t\t\tc.y = int(srid_map[name+'y'][0])\n\t\texcept:\n\t\t\tc.y = 0 # Should work on ESRI\n\n\tif not hasattr(c,"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tPoint.__name__ = name\n\tPoint.__objclass__ = bpy.types.UVMask\n\tPoint.__bases__ = ( PointNode, EPMixin )\n\n\t# create fields\n\tfor pname, pfield_def in fields.items():\n\t\tPoint.__dict__[pname] = pfield_def.type._type()\n\n\tPoint.__dict__['_point'] = FieldData()\n\tPoint.__dict__['_point'].from_data(fields['point', 'data'])\n\n\tPoint.__dict__['_point']['is_locked'] = fields['"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tpoint_class = Point\n\tPoint = collections.namedtuple('Point', ['name', 'fields','srid_map'])\n\tPoint.__new__ = Point.__init__\n\tpoint_class.__name__ = name\n\treturn Point\n\nclass SpatialReference(Document):\n\t\"\"\"\n\tThis class is used to allow for creating SpatialReferences.\n\t\n\tThe reason that this class is used is to support spatial reference creation in the database, and then allow\n\tfor further manipulation of SpatialReferences.\n\t\"\"\"\n\n\t@classmethod\n\tdef __init__(cls, database,"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from fipy.base.set_utils import _set_class\n\n\tclass_name = name.upper() + 'Point'\n\n\tif class_name not in ['POINT3D', 'MESHPOINT']:\n\t\tif not issubclass(class_name, flopy.numpy.NumpyArrayType):\n\t\t\tclass_name = flopy.utils.forceString(class_name)\n\n\tprintf = flopy.utils.import_class_by_name(class_name)\n\tif printf is None"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tif name == \"latitude\": return Point(fields[\"latitude\"])\n\tif name == \"longitude\": return Point(fields[\"longitude\"])\n\tif name == \"location_x\" or name == \"location_y\":\n\t\tfields = fields[\n\t\t\t{\n\t\t\t\tname: \"point_x\",\n\t\t\t\tgetset: lambda s, v: v if hasattr(s, \"get_location\") else s.get_location(),\n\t\t\t\tsetset: lambda s, v: v if hasattr(s, \"set_location\") else s.set_location(),\n\t\t\t"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tclass Point(PointBase):\n\t\tdef __init__(self, xy, srid, **kwargs):\n\t\t\tPointBase.__init__(self, xy, **kwargs)\n\t\t\tself.srid = srid\n\t\t\tself.n = 0\n\t\t\tself.field_names = []\n\t\t\tself.fields = {}\n\n\t\tdef to_geojson(self, geo):\n\t\t\tgeometry = geo.geometry._asdict()\n\t\t\tif geometry:\n\t\t\t\tassert isinstance(geometry, dict)\n\t\t\t\tself.srid, = geometry['sr"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tPoint = type('Point', (base.BaseObject,), {'srid': srid_map, 'name': name})\n\tPoint.__doc__ = base.format_docstring(Point.__doc__,\n\t{'Point': 'class for %s with %r' % (name, fields)})\n\tPoint.__name__ = name\n\n\tclass PointField(bases.AbstractTypedList):\n\t\t\"Abstract base class for Point fields.\"\n\t\tdef check(self, user_context):\n\t\t\tif (self.srid is None):\n\t\t\t\tuser_context.error"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tbase = Point\n\tsubclass = base\n\n\t# Define the fields.\n\tif fields is None:\n\t\tfields = []\n\n\tclass_name = name\n\tfor field in fields:\n\t\tname = field.name\n\t\tif issubclass(subclass, Point) and name[-1:] in ('__'):\n\t\t\tnewname = name + Point\n\t\telse:\n\t\t\tnewname = name + (Point.__module__ + '.' + name)\n\t\tfields.append(\n\t\t\timport_type(newname, ('Point',), fields=fields"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tT = sqlite3.Row # Table object\n\tc = T(\"__tablename__\")\n\t#\n\tc.append(name) # \"name\" table has this field as row\n\tfor field in fields:\n\t\tc.append(field) # \"Field\" field\n\t\t#\n\t\tc.append(None) # None if no value has been specified\n\n\treturn T.row # return Row object\n#\n\ndef _get_name(tbl, idx, name):\n\t\"\"\"\n\t\tGiven a name, look up a table and index for the name"}]}
{"task_id": "62e5dc9ed76274f8a4026b5b", "prompt": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tif len(message) == 0:\n\t\tmessage = dedent(message).strip()\n\t\tif not message:\n\t\t\treturn ''\n\t\treturn re.escape(message)\n\treturn str(message)\n\ndef print(*args):\n\timport psycomodule, re\n\tpsycomodule._psyco_messages.addPrint(False, None, args)\nif print:                                       "}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef decorator(f):\n\t\tdef deprecate_deprecated(function, deprecated_message, message):\n\t\t\tdeprecation_message = message or function.__name__ + \\\n\t\t\t\t\t\t(deprecated_message is not None and \" \" + deprecated_message) or \\\n\t\t\t\t\t\t\"\"\n\n\t\t\t@wraps(f)\n\t\t\tdef deprecated_function(*args, **kwargs):\n\t\t\t\tdef deprecated_func(func):\n\t\t\t\t\treturn deprecate_deprecated(method)(func, deprecated_message, message)\n\t\t\t\treturn deprecated_func\n\t\t\treturn deprecated_"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef __init__(self, message):\n\t\tself.message = message\n\t\tif self.message.startswith(\"__\"):\n\t\t\tself.message = self.message[1:]\n\t\n\tdef __call__(self, func):\n\t\tdef inner(*args, **kwdict):\n\t\t\tmsg = \"func %s is deprecated: use %s instead.\" % (repr(func.func_name), self.message)\n\t\t\twarnings.warn(msg, DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwdict)\n\t\t\n\t\t"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef __call__(self, *args, **kwds):\n\t\tif len(args) > 1 or len(kwds) > 1:\n\t\t\treturn self.deco(args, kwds)\n\t\treturn self.depr(args)\n\tdef depr(self, args, kwds=None):\n\t\t# pass some keyword arg to deprecated\n\t\t# deprecation is not very useful in function deprecation\n\t\twarnings.warn(\"%s has been deprecated in favour of %s.\" \\\n\t\t\t\t% (self.__class__.__name__, str(self"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef decorator(f):\n\t\t# deprecate methods, leaving behind the warnings\n\t\tdef deprecated_msg():\n\t\t\twarnings.warn('f' + message, stacklevel=3)\n\t\t\tf()\n\t    deprecated_msg.__doc__ = msg\n\t\tdeprecated_msg.deprecated = depr\n\t\treturn deprecated_msg\n\treturn decorator\n\n#define _GNU_SOURCE             /* See feature_test_macros(7) */\n#define _BSD_SOURCE\n#include \"utils.h\""}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tif message is None:\n\t\treturn\n\tif len(message) > 100:\n\t\traise Exception(message)\n\t\n\tmsg = \"\"\"\ndeprecated\n\t| This function or method has been deprecated in favour of\n\t| | (a) 'foo' has been deprecated in favour of | \n\t| | (b) 'bar' has been deprecated in favour of 'further in the\n\t|      stack trace.'\n\t\"\"\"\n\t#print(msg)\n\tif message is None:\n\t\treturn\n\tif len(message)"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t@deprecated(message)\n\tdef _deprecated_wrapper(self, obj, msg=None, deprecated_in=None):\n\t\tassert msg, \"Not a decorated function\"\n\t\tif deprecated_in:\n\t\t\tresult = []\n\t\t\tfor item in deprecated_in:\n\t\t\t\ttry:\n\t\t\t\t\tresult.append(\n\t\t\t\t\t\tgetattr(obj, item)[1].__name__.split('.')\n\t\t\t\t\t)\n\t\t\t\texcept AttributeError:\n\t\t\t\t\tpass\n\t\t\tmsg += \". The following deprecated functions were replaced by the newer function calls: %r\" %"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\treturn decorator\n\ndef deprecate(message):\n\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\t\n\t    @deprecate(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\treturn decorator\n\ndef deprecate_docstring(doc, message):\n\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\t\n\t    @deprecate(\"This is deprecated with docstring\"\n\t  "}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tif hasattr(_deprecated, \"__call__\"):\n\t\t_deprecated_decorator = _deprecated\n\t\tdef _deprecation_wrapper(*args, **kwargs):\n\t\t\t_deprecated_decorator(*args, **kwargs)\n\t\t\tdel _deprecated, _deprecated_decorator\n\t\t\treturn message\n\telse:\n\t\tdef _deprecation_wrapper(*args, **kwargs):\n\t\t\traise DeprecationWarning\n\treturn _deprecation_wrapper\n\n\n# -----------------------------------------------------------------------------\ndef decorators(\n\t\tfunc_or_class,\n\t\tdecorator_name=\"decorated\",\n\t\tdecorator=make"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tDecorator for defining deprecation.\n\t\n\t\t<param name='message'> (deprecated) deprecated message\n\t\t</param>\n\t\t\"\"\"\n\t\t@wraps(func)\n\t\tdef wrapper(self, message=message):\n\t\t\treturn func(self, message)\n\t\twrapper.__doc__ = func.__doc__\n\t\treturn wrapper\n\treturn decorator\n\n\nclass Message:\n\tdef __init__(self, message=None, *args, **kwargs):\n\t\tself.message = message\n"}]}
{"task_id": "62e4fc3c85ea98643089041e", "prompt": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tenvs: dict[str, str] = {}\n\tfor k, v in envvars().items():\n\t\tenvs[k] = v\n\treturn inline(code, envs)\n\n/*\n * Copyright 2016-2019 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tfrom optparse import OptionParser\n\n\t# Parse command line\n\tparser = OptionParser(\"Enable (1) or disable (0) R\")\n\tparser.add_option(\"-f\", \"--force\")\n\t(options, args) = parser.parse_args()\n\n\tif len(args)!= 1:\n\t\tparser.error(\"One or more required args were not supplied: R\")\n\n\tif options.force:\n\t\targs.insert(0, 1)\n\n\tif options.f == \"1\":\n\t\tpass\n\t\t# If forced or"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn \"%s\" % code\n\n\ndef _inline_setup_env(code: str) -> str:\n\t\"\"\"\n\tSetup R environment variables.\"\"\"\n\t# The following R environment variables are set in a separate Python file\n\t# called settings.py which can only be imported when R is running.\n\tif \"RAVEN_SETTINGS\" not in os.environ:\n\t\tos.environ[\"RAVEN_SETTINGS\"] = \"/tmp/settings.json\"\n\treturn \"r_settings = %s\" % code\n\n\ndef _inline_setup(code: str) ->"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tif \"RUN_IN_R\" not in os.environ:\n\t\tenv_r_set_in_r()\n\treturn r._inline_setup(code)\n\ndef _inline_r_teardown() -> str:\n\treturn r._inline_teardown()\n\ndef _inline_r_with_code(code: str, stdout: bool = False, env: dict[str, str] = None) -> str:\n\t\"\"\"\n\tCreate an inline R function from a rspec code string.\n\n\tA rspec code string is a R source code string which"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t_inline_setup = f\"\"\"function setup(R, config)\n\t\tR=R;\n\t\t\tconfig = config.slice(1,2);\n \t\tif (config[0]==\"\" && config[1]==\"\" and config[2]==\"\" and config[3]==\"\" and \\\n                       config[4]==\"\" and config[5]==\"\" and config[6]==\"\" and config[7]==\"\" and config[8]==\"\" && config[9]==\"\" )\n  \t\t\t{'"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tfor x in r_overrides:\n\t\tif x.upper() in config.env_vars:\n\t\t\treturn x.rstrip()\n\n#include \"test.h\"\n\nTEST(foo)\n{\n\tFILE *fp = fopen(\"/tmp/file2\", \"w+\");\n\tfree(fp);\n}\n\npackage network\n\n// Copyright (c) Microsoft and contributors.  All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tglobal _inline_code_env\n\tif not _inline_code_env:\n\t\tfrom salt.engines.minions import setupcode\n\t\t_inline_code_env = setupcode(code)\n\treturn _inline_code_env\n\n\ndef _inline_r_check() -> str:\n\tglobal _r_code, _r_code_obj\n\t_r_code = _r_code_obj\n\tif not _r_code_obj:\n\t\treturn\n\tcode = _r_code\n\treturn code\n\n\nregister("}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn \"\"\n\n\n_inline_r_setup_cmd = _inline_r_setup\n\n\ndef _inline_r_start(code: str) -> None:\n\t\"\"\"\n\tSome behaviour of R is being configured via variables.\n\t\t(e.g., when using the R_start option, this can happen\n\t\tonce R has started).\n\t\"\"\"\n\tif _inline_r_setup_cmd(\"_\"):\n\t\tprint >> sys.stderr, \"r_setup_cmd() must be used before r_start()\"\n\t\tassert False, \"r"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tsetup = 'export FOR_R=' + code\n\tif hasattr(os, '_path_temp'):\n\t\tsetup += ';run_setup=True' if os.environ.get('RUN_SETUP') else ''\n\tos.environ['RUN_SETUP'] = '1'\n\tif not os.path.exists(setup):\n\t\ttime.sleep(0.5)\n\t\tos.mkdir(setup)\n\t\tprint(f'R configuration: {setup}')\n\t\tprint(f'export FOR_R=' + code)\n\t\tsys.exit("}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\twith open(code, \"r\") as f:\n\t\treturn \"\".join(i for i in f)\n\n\ndef _inline_r_options(code: str) -> str:\n\t\"\"\"\n\tThis is a set of the environment variables in.r files\n\t\"\"\"\n\tenvs = {\"REDIS_PASSWORD_ENV\": \"REDIS_PASSWORD\", \"USE_REDIS\": \"USE_REDIS\", \"USE_R\": \"USE_R\"}\n\treturn code\n\n\ndef _inline_r_create(code: str) -> None:\n\t\"\"\""}]}
{"task_id": "62e4fbda85ea986430890405", "prompt": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tcolor: True | False\n\ttarget_concurrency: integer\n\t\"\"\")\n\n\n# TODO: This code should be generated in the distribution\n# directory, or in project directory or in the sources directory.\n\n\ndef make_env_vars(verbosity_level: int, target_concurrency: int):\n    return {\"verbosity_level\": verbosity_level,\n            \"target_concurrent_execution\": target_concurrency,\n            }\n\n\nclass Options(NamedTuple):\n  "}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t):\n\t\"\"\"\n\tArguments:\n\t\t- 'color' - If True, it runs color code.\n\t\t- 'target_concurrency' - Desired number of partitions to run concurrently.\n\t\"\"\"\n\t\n\t# If we use sys.getwindowsversion to determine the number of\n\t# processors on a system we have no information about windows,\n\t# then we're using sys.cpu_count()\n\t# But that does not work if sys.getwindowsversion() returns\n\t# 'unknown', so we use 'getwindowsversion()' which returns\n\t# a dictionary.\n"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\n\tReturns: A list of arguments from xargs\n\t\n\tExamples:\n\t\t- target_concurrency: 3\n\t\t- color: true\n\t\"\"\"\n\tcolor = (\n\t\tlambda a=10, b=11, c=12: ([\"--color\", \"-C=%d\", \"-c\", \"color=%s\"] % (c+8))\n\t\tfor a in [10, 11, 12]\n\t)\n\n\tdef run"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tcolor = False\n\ttarget_concurrency = 0\n)\n\n\ndef xargs_single_cluster(\n\tname, env, opts, opts_for_cluster, run_commands,\n) -> Config:\n\t\"\"\"\n\tCreate a yargs config with a single cluster that should be run at a\n\trandomly determined number.\n\t\"\"\"\n\tfrom random import randint\n\tcluster_name = name.split(\"-\")[-1]\n\n\tdef partition() -> None:\n\t\t\"\"\"\n\t\tRun a single partition at random between'minSize' and'maxSize"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tcommand = ['xargs']\n\t\n\tdef __init__( self, color = None, target_concurrency = 3, args=None ): \n\t\tself.color = color\n\t\tself.target_concurrency = target_concurrency\n\t\tself.args = list(args) if args is not None else []\n\t\tself.cmd = argparse.ArgumentParser(add_help = False, usage = \"\", prog = self.__class__.__name__)\n\t\tself.argparser.add_argument('-o', dest = 'args', type = str, metavar = 'TARGET_CONCURRENCY"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tColor: color syntax for a terminal (eg: bold)\n\tTarget: number of partitions to run concurrently\n\t\"\"\"\n\tcolor: Make a pty if you want to run on a platform that doesn't have \\\n\tthe --color option (eg: python3|linux|macbook|emacs|netbsd|openbsd)\n\t\tcolor not set -- color is taken from the environment\n\tTarget: number of partitions to run concurrently\n\"\"\"\n\ttarget_concurrency: target number of partitions to run concurrently\n\"\"\"\nGet one line: $xargs --target_concurrency 10 10 10 10 10"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t.s:\n\t\n\t\txargs -k=color=1\n\t\txargs -k=target_concurrency=10\n\t\n\tOther options can be given via an environment variable, e.g. \n\tPTY_ENV_XARGS=/opt/pyport/xargs\n\nReturn true/false based on environment \"PTY_ENV_XARGS\".\n\"\"\"\n\t.FNOSC: yes\n\t.SNIPPET: no\n\t\n\t// Set environment variables with appropriate commandline arguments\n\tset $PTY_ENV_XARGS=$PWD \"$PTY_"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\"\n\t-S core/target_concurrency: How many partitions to run concurrently\n\n\"\"\")\n\nclass Xargs(Opt):\n\toptions = ('--color=false',)\n\tdef apply(self, args):\n\t\tif not args or args.count('xargs'):\n\t\t\treturn\n\t\tcolors = [n == 'true' for n in args] if args else True\n\t\tif colors:\n\t\t\tcolor = True\n\t\tif color:\n\t\t\tfor s in itertools.starmap(_, zip(*sorted(sys.popen.__dict__."}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tcolor: true\n\ttarget_concurrency: 20\n\t\"\"\"\n\tIndicate when all workers should exit in a separate green task\n\t\n\t  color: false\n\t  target_concurrency: 0\n\t\"\"\"\n\ttarget_concurrency: 0\n\t  \"\"\"\n\t  A platform-independent implementation of xargs. \n\t  \n\t  This is the entry point that allows the user to provide a command line for a \n\t  program to interact with a specific platform (or whatever other platform the \n\t  program will run on) or to configure it.\n\t  \"\"\"\n\t  \n\t\t\t\t\t \"\"\")\n\t}\n\"\"\"\nA simple"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t,\n\t\t color=False)\n\t\t\n\tdef __init__(self,\n\t\t\t  argstr,\n\t\t\t  arglist,\n\t\t\t  default,\n\t\t\t  nopty,\n\t\t\t  verbosity=0):\n\t\n\t\tself.verbose      = verbosity > 0 \n\t\tself.color        = False\n\t\tself.p_flag       = \"\"\n\t\tself.argstr       = \"\"\n\t\tself.targets      = \"\"\n\t\tself.target_concurrency = None\n"}]}
{"task_id": "62e4fbda85ea986430890403", "prompt": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tshuffle = _shuffling_sequence\n\treturn shuffle(seq)\n\n\ndef _split(seq: Sequence[str]) -> Sequence[str]:\n\t\"\"\"\n\tSplit on any of the characters contained in the sequence\n\t\"\"\"\n\treturn seq[0] + [seq[0]] + _split(seq[1:-1])\n\n\ndef read_lines(path: str) -> None:\n\t\"\"\"\n\tRead the lines from the text file and apply the filters.\n\t\"\"\"\n\twith open(path) as f:\n\t\tfor line in f:\n"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t_shuffled_len = len(_shuffled(seq))\n\tret = []\n\t_index = range(_shuffled_len * -1)\n\twhile _index!= _shuffled_len:\n\t\tret.append(_shuffled[_index])\n\t\t_index = _index + 1\n\treturn ret\n\ndef _shuffle_gen(seq: Sequence[str], seed: Optional[int] = None) -> str:\n\t\"\"\"\n\t\tShuffle based on seed, i.e. shuffle a set of seq.\n\t\t- seed"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tshuffled_seq = list(reversed(seq))\n\treturn shuffled_seq\n\n\ndef unshuffled(seq: Sequence[str]) -> Optional[str]:\n\t\"\"\"\n\tUnshuffling\n\t\"\"\"\n\tshuffled_seq = _shuffled(seq)\n\tif len(shuffled_seq) == 0:\n\t\treturn None\n\treturn shuffled_seq[0]\n\n\ndef read_seq(filepath: str, n_read: int = 8, random_seed: int = 5) -> Sequence["}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tresult = []\n\tfor index in seq:\n\t\tresult.append(index)\n\t\tresult.sort()\n\t\tif result in seq:\n\t\t\treturn result\n\telif not seq:\n\t\treturn result\n\treturn list(result)\n\ndef _find_word_in_word_array(seq: Sequence[str], dicts: List[Dict[str, str]]) -> str:\n\tif not seq or not dicts:\n\t\treturn seq\n\tindex = 0\n\tfor dict in dicts:\n\t\tif str(dict['word"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tres = [i for i in seq if i not in seq[::-1]]\n\trand.shuffle(res)\n\treturn res\n\n\ndef _join(seq: Sequence[str], *paths: str) -> str:\n\tdef join(path_elements: str) -> str:\n\t\tif paths and path_elements:\n\t\t\tpath_ = [r[0] if isinstance(r[0], str) else r for r in paths]\n\t\t\tpaths_ = [r.strip(path_.join(path_[i][-len(path_elements):"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\treturn list(itertools.islice(iter(zip(*seq)), random.randint(0, len(seq))))\n\n\ndef is_file(filename: str):\n\treturn os.path.exists(os.path.expanduser(filename))\n\n\ndef mkdir(path: str):\n\tos.makedirs(os.path.expanduser(path), exist_ok=True)\n\n\ndef symlink(source: str, dest: str):\n\ttarget = os.path.realpath(os.path.expanduser(os.path.expandvars(dest)))"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t# The number of random elements\n\tn = len(seq)\n\n#\tfor i in range (0, n):\n#\t\ttmp = seq[-n]\n#\tseq += tmp\n\n\tif ((n == 1) or (seq == [''])):\n\t\treturn ['']\n\telse:\n\t\tif (len(seq) == n):\n\t\t\tshuffle(seq)\n\t\t\treturn seq[::-1]\n\t\treturn _shuffled(seq)\n\ndef shuffle_list(seq: list) -> None:\n\t\"\"\"\n\t"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tidx = numpy.random.permutation(len(seq))\n\treturn [seq[idx]]\n\n\ndef randint(min: int, max: int) -> int:\n\t\"\"\"\n\tReturns an integer between min and max.\n\n\tIf a given type is float, round the input to the nearest integer (rounding\n\tif necessary, but rounding up to nearest even integer if nothing is\n\trounded up (e.g., if inputs are integers).\n\n\tIf a type is iterable, a list is returned.\n\t\"\"\"\n\tfrom random import random\n\tres = random()"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tshuffled_list=[_shuffled_item(seq[idx]) for idx in range(len(seq))]\n\treturn shuffled_list\ndef _sorted_shuffled_items(s: Sequence[Sequence[str]]) -> List[Tuple[str, str]]:\n\"\"\"\n\tUse this function to sort your list in place.\n\t\"\"\"\n\treturn (tuple(sorted(_shuffled_items(seq)[:len(seq)]), key=len)\n\t        for seq in s)\n\ndef _sorted_shuffled_keys"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\twhile True:\n\t\ttemp = [next(seq) for _ in xrange(len(seq))]\n\t\tif temp not in seq:\n\t\t\tprint(f\"Invalid input sequence with {len(seq)} values! Truncated! Current elements: {temp}\")\t\t\n\t\tif seq[temp] not in seq:\n\t\t\tprint(f\"Invalid input sequence {seq[temp]}! Not in the correct order!\")\n\t\t\tcontinue\n\t\treturn seq[temp]\n\n\ndef check_parameters(args: list) -> None:\n\t# Check parameters\n\t"}]}
{"task_id": "62e4fb6585ea98643089032b", "prompt": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tm = re.match(r'\\d+(\\.\\d+)*?', s)\n\tif m:\n\t\treturn int(m.group(1)), m.group(2)\n\n\ndef parse_description(s: str) -> Optional[str]:\n\tpoor man's description\n\t\"\"\"\n\tReturns None if not extracted\n\t\"\"\"\n\tpoor man's description\n\tif not s or s == '*': return None\n\td = {}\n\tfor c in s.split():\n\t\tif c.endswith(''''): c = c[:-1]\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\ttry: \n\t\tmatches = re.match('^(\\d+)\\.(\\d+).*$', s)\n\texcept:\n\t\treturn 1, parse_version(s)\n        \ndef parse_version_from_package(s: str) -> Tuple[int,...]:\n\"\"\"\n\tfrom module import version\n\treturn int(version.parse(s, version.format)), version\n\ndef parse_verison (\n\tver : str, \n\trelease :str = 'dev_',\n) -> Tuple[int, str,..."}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tparts = s.split()\n\tif len(parts)!= 3:\n\t\traise ValueError(f\"bad version '{s}'\")\n\tv, major, minor = parts\n\tif major < 2:\n\t\traise ValueError(f\"bad major version - '{s}' < 2\")\n\tif v[:3] in ('0x.x','00'):\n\t\traise ValueError(f\"bad minor version - '{s}' major version '{major}' is a major version greater than or equal to zero\")\n\ttry:\n\t\tminor = 1 + 2 ** {2 * x"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\ttry:\n\t\tm = re.match(r'^\\d+\\.\\d+\\.\\d+(\\.\\d+)?[^:-]*$', s)\n\texcept (KeyError, AttributeError):\n\t\tm = re.match(r'^\\d+(\\.\\d+)?[^:-]*$', s)\n\treturn m.groups()[0]\n\npackage client\n\nimport \"github.com/Azure/azure-sdk-for-go/version\"\n\n// Copyright (c) Microsoft and contributors.  All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tif s!=\"Version: \" and s!=\"Version: \".join(_list(filter(None, (x.rstrip() for x in s))[:9]))[:14-6):\n\t\treturn\n\tversion = s.rstrip() if s[:7] == \"Version: \" else s[:5]\n\treturn (version, s[7:], s[10:])\n\ndef parse_tags(s: str) -> Set[Tuple[str, str]]:\n\t# this is not always the best way of handling tags -\n\t# one may have a better algorithm\n\t_tokens"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\ttry:\n\t\treturn (int(s.strip()), )\n\texcept ValueError:\n\t\treturn (0, )\n\npackage org.jhipster.blog.managers;\n\nimport com.codahale.metrics.Counter;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport redis.clients.jedis.BitJedisCluster;\nimport redis.clients.jedis.RedisAdvanced;\nimport redis.clients.jedis.util.SafeEncoderOutputStream;\nimport redis.clients.util.SafeEncoder;\nimport"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tassert len(s) >= 5\n\tparts = []\n\ti = 0\n\twhile i < len(s) - 5:\n\t\tparts.append((int(s[i : i + 5]), int(s[i + 4 : i + 7]), int(s[i + 8 :]), int(s[i + 10 :])))\n\t\ti += 9\n\twhile i > 0:\n\t\tif len(parts) == 1:\n\t\t\tbreak\n\t\tt_0, v_0, t_1, v_1 = parts[0]\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\ttry:\n\t\tv = int(s)\n\texcept ValueError:\n\t\treturn v, s\n\treturn v, s[:len(s)-len(int(s))]\n\n\n# =====================================================================\n# -- Main Interface (Static) -----------------------------------------\n# =====================================================================\ndef main(argv):\n\t\"\"\"Main function with all commandline arguments\"\"\"\n\tprog_name = os.path.basename(argv[0])\n\tparser = OptionParser(\n\t\tprog=prog_name,\n\t\tusage=\"Usage: %prog [options] -d path_to_db"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\ttry:\n\t\tt1 = int(s[:9])\n\t\tt2 = int(s[9:])\n\texcept ValueError:\n\t\tpass\n\n\treturn t1, t2\n\n\ndef parse_version_no_patch(s: str) -> tuple[int,...]:\n\t\"\"\"\n\tgeneric version comparison using git sha as a prefix\n\t\"\"\"\n\tv = parse_version(s)\n\tassert isinstance(v[0], int)  # make sure a str is always a whole number\n\tv1 = v[0]\n\tv"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tv = [int(x) for x in s[1:].replace(\".\", \"\").split()]\n\treturn v[0], v[1:], v\n\n\ndef parse_version_int(s: str) -> int:\n\tpoor_man_version_int = 0\n\tsplitted = s.rsplit('.', 1)\n\tif len(splitted) == 1:\n\t\tpoor_man_version_int = max(map(int, splitted))\n\telse:\n\t\tpoor_man_version_int = max(map(int,"}]}
{"task_id": "62e4fb4d85ea9864308902e7", "prompt": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tfrom.shell import execute_command, quote_cmd\n\tparts_args = [\n\t\tcmd[0].strip(),\n\t\tcmd[1].strip(),\n\t]\n\tparts = []\n\tquote = False\n\tfor part in parts_args:\n\t\tif quote:\n\t\t\tquote = False\n\t\telif part in get_shell_vars():\n\t\t\tparts.append(part)\n\t\t\tquote = True\n\t\telse:\n\t\t\tparts.append(part)\n\tparts[1] = quote_cmd(parts[1])\n\t"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tif not cmd:\n\t\traise ValueError\n\telse:\n\t\tif not cmd[0]:\n\t\t\tcwd = os.getcwd()\n\t\t\treturn cwd, cmd\n\ndef normalize_path(s: str) -> str:\n\tcwd = os.getcwd()\n\tc = cwd[:]\n\tc.insert(0, os.sep)\n\ttry:\n\t\tc = c + str(os.curdir)\n\texcept NameError:\n\t\tc = c + os.curdir\n\treturn cwd + s + os.pathsep + c\n\n"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t# This will also fix the case when the env var has '=' but has a path delimiter\n\t# We only need to process the file name because the path delimiter is a variable\n\t# The file name and the envvar may not refer to a valid file.\n\t# (e.g. $PATH => $PATH) is the last thing we'll try in case there are any path separators\n\n\t# First we remove the path delimiter from the env var\n\t# Otherwise we will try to match with the file name and if that fails, the path will change\n\tcmd = tuple(cmd[1:])"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tif platform.system() == \"Windows\":\n\t\tcmd = os.path.abspath(cmd)\n\n\t# Normalize some strange paths like ~/...\n\t# See also https://github.com/shazow/shazow/issues/2477\n\tcmd = shlex.split(cmd)\n\tif len(cmd) > 1:\n\t\t# We remove the first part,\n\t\t# since on Windows we have a drive to start the\n\t\t# path like \"C:\\Foo\\Bar\"\n\t\t# In this case \"C:\\Foo\\Bar"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t# fix path to use the normal (not shell) path for path separators\n\tcmd = tuple(arg[2:] for arg in cmd)\n\t# handle the case where our path separator is '/'\n\t# we need to remove the first element of each path element because\n\t# we can't handle two relative directories when working on one of them\n\tfor element in cmd:\n\t\tif element == os.sep:\n\t\t\tyield \"cmd\", os.path.join(*(element[1:]))\n\t\telse:\n\t\t\t# XXX better handle\n\t\t\tyield \"cmd\", os"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tparts = cmd[0]\n\treturn parts\n\nclass Globs:\n\tdef __init__(self) -> None:\n\t\tsh = _find_shebang()\n\t\tself.data = (sh + x for x in self.list())\n\n\tdef list(self) -> list:\n\t\ttry:\n\t\t\treturn [ x.decode('ascii')\n\t\t\t\tfor x in shlex.split(str(self.data[0]))\n\t\t\t\t\tif len(x)>=2 ]\n\t\texcept OSError:\n\t\t\traise\n\n\t\t"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\treturn os.path.abspath(cmd[0])\n\n\n'''\n\tFunction to handle --prefix/-src/-bin and make possible filenames for git if not present\n'''\ndef handle_src_bin(src: str, tmp_dir: str, binsh: str, repo_dir: str):\n\troot_hash = None\n\tif os.path.isfile(src):\n\t\ttry:\n\t\t\troot_hash = md5sum(src)\n\t\texcept Exception:\n\t\t\tpass\n\telse:\n\t\tlogging.error('Could not locate %"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tfor c in cmd:\n\t\tif c == '##':\n\t\t\tcmd[0] = c\n\t\telif c.startswith('#') and not c.startswith(' '):\n\t\t\tcmd = tuple(cmd) + (c,)\n\treturn cmd\n\ndef strip_args(command: str, args_to_strip: str) -> str:\n\t\"\"\"\n\tStrips the command args by removing the args to be stripped\n\t\"\"\"\n\targs_to_strip = tuple(args_to_strip)\n\tcommand = tuple(command)\n\t# the first"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tpath = os.path.basename(sys.argv[0])\n\t\n\tnew_cmd = [] for x in cmd if isinstance(cmd, list) else [cmd]\n\t\n# \tfor i in dir(sys):\n# \t\tif i.startswith('_') and i!= '':  # Ignore the current dir\n# \t\t\tif not is_executable(i) and not i.endswith('.cmd'):\n# \t\t\t\t# For relative paths, like./python.exe,\n# \t\t\t\t# do not parse shebangs if they already have been done by win32 (or"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t# TODO: Fix this code and add support for shebangs that need expansion\n\tline = str(cmd[1])\n\t# this code does not deal with paths\n\tif line.startswith('\\\\\\\\') or line.startswith(os.path.sep): return line\n\t# first word is path\n\t# note: shebangs are *really* weird; so we're stuck with it\n\t# this is the only reason behind all that.\n\tparts = [cmd[0], line]\n\t# get all possible words\n\tword = find_first_in_words(parts"}]}
{"task_id": "62b8d27a48ba5a41d1c3f4c6", "prompt": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\treturn lambda func, cache=cache,\\\n\t\t\tkey=key,\\\n\t\t\tlock=lock: func(cache.fetch(key), **lock)\n\ndef __cached__(cached, key=hashkey, hashfn=hash_dict):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache if the keys are already unique.\n\t\"\"\"\n\tdef cache_func(func, cache=cache):\n\t\treturn hashed(func, cache=cached) if cache else hashfunc(cached, func)\n\treturn cache_func\n"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tif isinstance(cache, str):\n\t\tcache = memoize(cache, lru=True)\n\treturn wraps(cache)(lambda cache: callable(cache), function_wrapper(fn))\n\n\ndef memoize(callable, lru=False, locks=True, *args, **kwargs):\n\tif callable in cache and lru:\n\t\treturn cache[callable]\n\tkeyfunc = functools.partial(callable, *args, **kwargs)\n\tcache[callable] = keyfunc\n\tkeyfunc = lambda: keyfunc(*args, **kwargs)\n\tif lru is"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef cache_decorator_wrapper(self_p, *args, **kwargs):\n\t\tif callable(cache):\n\t\t\tcache.__dict__[self_p._cache_key] = cache()\n\t\t\treturn getattr(self_p, cache.__name__)\n\t\treturn self_p._cache_function(*args, **kwargs)\n\treturn wrap_memoized(cache_decorator_wrapper, cache=cache)\n\ndef caching(cache, **kwargs):\n\t\"\"\"\n\tDecorator to create a cached decorator callable for caching.\n\tNote: caching cache is a memoization cache"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t@wraps(cache)\n\tdef wrapper(*args, **kwargs):\n\t\ttry:\n\t\t\tresult = cache[key] = cache[key]._memoize(lambda: cache[key])[0](*args, **kwargs)\n\t\tfinally:\n\t\t\tcache[key] = result\n\t\t\tcache.release()\n\n\treturn wrapper\n\ndef cached_once(key=hashkey, cache=None):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache only once.\n\t\"\"\"\n\tdef wrapper"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\treturn cachedcache(\n\t\tCache(),\n\t\tlambda func, *args, **kwargs: wrapped_func(func, args, kwargs, cache, lock),\n\t)\n\nlazycache = memoize = cached\n\ndef memoize(func, key=None, cache=None, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoized callable that takes the\n\tsame function.  The cache and lock are used unless a function\n\tis passed as the first argument, in which case the function's\n\targuments and kwargs are used.\n\t\"\"\"\n\tif"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t@functools.wraps(cache)\n\tdef cached_decorator(wrapped):\n\t\twith lock:\n\t\t\tcache[key] = wrapped\n\t\treturn wrapped\n\treturn cached_decorator\n\ndef cache(key, value, lock=None):\n\tif cache is None:\n\t\tcache = {}\n\tcache[key] = value\n\treturn value\n\ndef cache_reset(key, value, lock=None):\n\tif key in cache:\n\t\tcache[(key,)] = value\n\treturn value\n\ndef cache_reset_if_empty"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t@wraps(cache)\n\tdef wrapper(*args, **kwargs):\n\t\tnew_cache = cache(*args, **kwargs)\n\t\tif isinstance(new_cache, memoized):\n\t\t\treturn new_cache\n\t\treturn cache\n\treturn wrapper\n\n/*\nCopyright (c) 2011, Chris Umbel\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\treturn _memoize(cache, functools.cache, key, lock)\n\ndef cached_reset(cache, key=hashkey, timeout=10, lock=None):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults when invoked.\n\t\"\"\"\n\treturn _memoize(cache, functools.cache_reset, key, lock, timeout)\n\ndef memoized(\n\tf, *args, **kwds\n):\n\t\"\"\"\n\tAdds memoization support to a function f with signature\n\targs, kwargs, or an immediate"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef __get__(self):\n\t\tcache = self.__cache\n\t\tif cache is None:\n\t\t\tself.__cache = cache = {}\n\t\tif key is not None and not hasattr(cache, key):\n\t\t\tcache[key] = self\n\t\t\treturn\n\t\treturn cache[key]\n\tdef __call__(self):\n\t\tif not hasattr(self, '_cache_memo') or not hasattr(self, '_cache_lock'):\n\t\t\tself._cache_memo = cached(self._get_cache, key=hashkey, lock=self."}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef cached_decorator(*args, **kwargs):\n\t\twith lock:\n\t\t\tcache[key(*args, **kwargs)] = args, kwargs, len(cache), len(cache)\n\t\treturn args[0][0](*args[0], **kwargs[0][1])\n\treturn cached_decorator\n\n#!/usr/bin/env python\n# ------------------------------------------------------------------------------------------------------%\n# Created by \"Thieu\" at 17:18, 08/03/2020                         "}]}
{"task_id": "62b8d24048ba5a41d1c3f49f", "prompt": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\treturn CacheDecorator(maxsizetype=maxsize)\n\ndef cache_key(func, *args, **kwds):\n\t\"\"\"Returns the key that will be used to cache this function.\n\n\tAny existing cache will be replaced if it's too big.\n\t\"\"\"\n\tif len(_cache) >= _maxsize:\n\t\tlru = (_maxsize-len(_cache)/2,)\n\t\tc = random.choice(list(_cache))\n\t\tckey = _cache.count(c) * maxsizetype\n\t\t_cache.insert(c, None)\n\t"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef _wrapper(*args, **kwa):\n\t\t# Create one less than ideal, to avoid rehashing if there are less\n\t\t# we can keep (especially for large values).\n\t\tmaxsize = min(maxsize, maxsize + len(args)*10)\n\t\tresult = cachemethod(maxsize, kwa[\"ttl\"], timer=timer)\n\n\t\t@wrapt.with_self\n\t\t@wrapt.decorator\n\t\tdef wrapper(*args, **kwa):\n\t\t\tif kwa.get(\"typed\", False):\n\t\t\t\t"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t# This decorator wraps `maxsize` results which are cached, to speed up\n\t# calculations.\n\t# NOTE: `ttl` is measured with an absolute time-to-live as opposed to\n\t# with a relative time-to-live.\n\tdef decorator(state_func):\n\t\tmaxsize = get_lru_cache_maxsize(state_func)\n\t\tif maxsize <= nmemcached.MEMCACHED_CACHE.maxsize:\n\t\t\treturn state_func\n\t\t# Calculate how much of the time_to_live has to be consumed. If\n"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t# A decorator is called to cache results of that function in a\n\t# dictionary. If multiple decorators share the same memoizer then\n\t# the one last with the highest time-to-live (TTL) value.\n\n\tif typed:\n\t\tdef typed_decorator(*a, **kw):\n\t\t\t# Decorate a function as a user-defined function.\n\t\t\t# This allows the decorator to work with\n\t\t\t# multiple decorators, without instantiating a new\n\t\t\t# function each time it is called.\n\t\t\td= {func:a, kw:"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t@functools.wraps(fn)\n\tdef _memoization_cache(self, *args):\n\t\tif not len(self.cache):\n\t\t\t# No caching, just return\n\t\t\treturn fn(*args)\n\n\t\tif self.ttl is None and self.use_ttl:\n\t\t\t# The TTL will be computed from the function arguments,\n\t\t\t# but it does not take into account the cache size\n\t\t\telapsed = time.monotonic() - self.start_time\n\t\t\tself.use_ttl = (max(elapsed, self."}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t@wraps(wrapped_function_name='__wrapped__')\n\tdef method_wrapper(self, timeout=None, callback=None):\n\t\tresult = callback(self)\n\t\tif timeout:\n\t\t\tself.expires = time.monotonic() + timeout\n\t\tself.memoize(value=result, typed=typed)\n\t\treturn result\n\tmethod_wrapper.__name__ = wrapped_function_name\n\treturn method_wrapper\n\n\n@cached_lru_cache(maxsize=128, typed=False)\ndef lru_cache(maxsize=128"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t#TODO: implement lru caching?\n\tdef decorator(func):\n\t\t__cache_length = len(__cache)\n\t\t__cache_size = min(maxsize, __cache_length)\n\n\t\tdef memoized(*args, **kwargs):\n\t\t\tkey = args[0]\n\t\t\tvalue = __cache.__getitem__(key)\n\n\t\t\t__cache_item = value\n\n\t\t\t__cache_time_to_live = ttl\n\t\t\tif __cache_time_to_live > 0 and ttl_expired(key, (time.time() -"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef __init__(self, lru_size, ttl=600, timer=time.monotonic,\n\t\t\t\ttyped=False):\n\t\tself.lru_size = lru_size\n\t\tself.ttl = ttl\n\t\tself.maxsize = maxsize\n\t\tself.minused = ttl\n\t\tself.lastused = 0\n\t\tself.nextused = 0\n\n\t\tif not timer:\n\t\t\ttimer = time.monotonic\n\t\tself.timer = timer\n\t\tself.typed = typed\n\n\tdef __call__(self, f"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\treturn cache(maxsize, ttl, timer, typed_with_time_to_live)\n\ndef cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\n\tCache entries are cached and then retrieved from the function\n\twith the `cache_key` function.  This ensures that if a"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t# This decorator makes the decorator only run time-based,\n\t# i.e. non-negative numeric values are converted to `float()`\n\t# values.\n\t# It also allows for the cache decorator to modify the TTL by\n\t# modifying `maxsize` argument to `ttl` argument.\n\tdef _memcached(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif typed:\n\t\t\t\tfunc(*args, **kwargs)\n\t\t\telse:\n\t\t\t\tfunc(*args)\n\t\t\tfor"}]}
{"task_id": "62b8d23b48ba5a41d1c3f49a", "prompt": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef wrapper(f):\n\t\tg = f.__closure__\n\t\treturn MemoizeCache(maxsize=maxsize, typed=typed)(function=cache_function_wrapper(g))\n\treturn wrapper\n\n\ndef memoize_filter(func, cache_size=\"50\"):\n\t\"\"\"\n\tFactory to compose new functions with mine referential integrity. The caching\n\talgorithm is still effective.\n\n\t:param func: the function to cache.\n\t:param cache_size: the size of the cache.\n\t:returns: a new Decorator function.\n\t"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t# Cache the most recently used results, to reduce the load on get_results.\n\tdef mru(func=None, typed=False):\n\t\tif func is None:\n\t\t\tfunc = maxsize\n\t\tlast_cache = most_recent_used_func\n\t\tmost_recent_used_func = func\n\n\t\tif cache_size:\n\t\t\tlast_cache()\n\n\t\tfor i in xrange(0, cache_size, maxsize):\n\t\t\tif last_cache:\n\t\t\t\tlast_cache()\n\t\t\telse:\n"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef wrapper(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tmost_recent = [mru[len(mru) - i - 1:i] for i in range(0, len(mru))\n\t\t\t\tif isinstance(mru[len(mru) - i - 1], MRU)\n\t\t\t]\n\t\t\tif len(most_recent) < maxsize:\n\t\t\t\tmost_recent.extend(mru)\n"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef wrapper(func):\n\t\twrapped = functools.wraps(func)\n\t\t@functools.wraps(func)\n\t\t@functools.wraps(wrapped)\n\t\tdef memo_gen(self, memo=rafcon.core.core.empty_memo()):\n\t\t\tif typed:\n\t\t\t\t# Return the largest non-cached result, without memoization.\n\t\t\t\tresult = wrapped(*(next(memo) for i in range(maxsize)) for next(memo),)\n\t\t\telse:\n\t\t\t\t# The most recent `mru_"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\treturn _most_recent_function(maxsize, typed, memcache(maxsize))\n\ndef no_cache(typed=False):\n\t'''Function decorator. A decorator that prevents the function from\n\tcalling the parent function when called'''\n\tno_cache_decorator = _make_funcdef(\n\t\t(name, typed) for name in dir(func_globals)\n\t)\n\t# Decorator: _most_recent_function()\n\treturn _most_recent_function(64, typed, no_cache_decorator, )\n\n/**\n* This file is part"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\timport collections\n\timport functools\n\n\t@functools.wraps(f)\n\tdef wrapper(f):\n\t\t@functools.wraps(f)\n\t\tdef wrapper_inner(n):\n\t\t\t\"\"\"\n\t\t\t\tWrapper to wrap a function with a normal caching that\n\t\t\t\tsaves the last `maxsize` results of all its decorated\n\t\t\t\tfunctions in a dictionary and returns it.\n\t\t\t\"\"\"\n\t\t\tif n < maxsize:\n\t\t\t\treturn f(n)\n\t\t\t_keys = list(itertools.islice(f."}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t# A memoizing callable to remember whether the result of its calculation\n\t# exceeds `maxsize`. This will be called repeatedly, based on when each\n\t# calculated result in `lru_cache()` is found to exceed `maxsize`.\n\t# If a method is called repeatedly, the first value will cause a failure,\n\t# and the next `maxsize` values cause a failure.\n\tdef mru_cache_size_exceeded(result):\n\t\tassert isinstance(result, tuple)\n\t\tassert len(result) == 2\n\t\treturn max((result[0], result[1"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t# cache will wrap a function with a `most_recently_used` method that\n\t# returns the most recently used item in the cache.\n\t# the results saved in either the most recently used or cached item\n\t# can be unpredictable results, we still keep one item for the most recently used item\n\tcache = functools.lru_cache(maxsize=maxsize, typed=typed)\n\tcache.most_recently_used = lambda: cache[1] if cache[0] is cache[1] else cache[0]\n\nclass most_recently_used:"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef wrapper(f, *args, **kwargs):\n\t\t_cache = None\n\t\tcache = _MRUcache()\n\t\n\t\treturn cache.get(f, *args, function=f, **kwargs)(maxsize, _cache)\n\t\n\treturn wrapper\n\n\nclass Cache(object):\n\t\"\"\" Base class for all Cache instances, which defines the most recently\n\taccessed results that are cached in a cache. This is essentially a class\n\twith only methods that allow access to the most recently accessed results\n\tof a cache.\n\t\"\"\"\n\tdef __init"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tif isinstance(maxsize, (int, long, float)):\n\t\treturn lambda cache, *args: cache.mostrecent_used = False, lambda cache, *args: maxsize, *args\n\treturn memoize(maxsize=maxsize, typed=typed)\n\n/*\n * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n * or more contributor license agreements. Licensed under the Elastic License\n * 2.0; you may not use this file except in compliance with the Elastic License\n * 2.0.\n */\n\nexport * from './create_new"}]}
{"task_id": "62b8d23948ba5a41d1c3f498", "prompt": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef memoize(func):\n\t\tif typed:\n\t\t\t_arg_maker = functools.partial(\n\t\t\t\tmemoizer.typed_memoize, maxsize=maxsize)\n\t\telse:\n\t\t\t_arg_maker = functools.partial(\n\t\t\t\tmemoizer.non_memoize_memoize, maxsize=maxsize)\n\t\tdef memoized_function(self, memo={}, key=key):\n\t\t\tkey = hash_key(key)\n\t\t\tmemo[key] = func(self, memo)\n\t\t\tself._memoization_"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t# This function should take a function that will be\n\t# cached up to `maxsize` results\n\t@staticmethod\n\tdef memoize(**keywords):\n\t\tif __name__ == '__main__':\n\t\t\t# We are running in a subprocess.  Save our own cache.\n\t\t\tlru_cache_file.write('%s %s\\n' % (keywords[0], keywords[1]))\n\t\telse:\n\t\t\t# We are using our function.  First, see what arguments\n\t\t\t# we should get.  This is the cache size.\n\t\t\tfunc ="}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t# Wrap your favorite caching function in a cache call function,\n\t# which, when provided with the callable as argument, returns a\n\t# tuple. Here and in cache_lru.py, we just pass its args to\n\t# the cache call function.\n\tdef cache_lru(**kwargs):\n\t\tif typed:\n\t\t\treturn (\n\t\t\t\t\tfunctools.lru_cache(maxsize=100)(v)\n\t\t\t\t\tfor v in kwargs.values()\n\t\t\t\t\t)\n\t\treturn func(*args, **kwargs)\n\n\treturn cache_lru\n\n/*\n"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t@wraps(func)\n\tdef __init__(self, func, *args, **kwargs):\n\t\tif typed:\n\t\t\t_cache = self._typed_cache\n\t\telse:\n\t\t\t_cache = self._cache\n\t\tself._maxsize = maxsize\n\t\tself._cache_next_size = 0\n\t\tself._cache = {}\n\t\tself._cache_last_id = 0\n\t\tif not typed or self._maxsize == 0:\n\t\t\treturn func(self, *args, **kwargs)\n\t\tcache_"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tclass LRUCache(list):\n\n\t\tdef __init__(self, func: callable, maxsize=128, typed=False):\n\t\t\tsuper().__init__()\n\t\t\tself._func = func\n\t\t\tself._maxsize = maxsize\n\t\t\t# lru_cache is limited to 128 entries, so we\n\t\t\t# can't use our own LRU cache\n\t\t\tif self._maxsize <= maxsize:\n\t\t\t\tself._maxitems = self._maxsize\n\t\t\telse:\n\t\t\t\tself._maxitems = maxsize\n\t\t\tself._cache = {}\n\t\t\tself"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef memoized(func, maxsize=128, typed=False):\n\t\tif maxsize>0 and maxsize<=maxsize-len(func):\n\t\t\t# The value is memoized.\n\t\t\treturn memoize(func, typed)\n\t\t# Call the memoized function with the last element from the cache.\n\t\t# Keep the cache around in case an item is evicted from the cache.\n\t\treturn cache(func, typed, 0)\n\treturn memoized if typed else memoized\n\n#include \"Common.h\"\n#include \"TimerManager."}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef wrapper(func):\n\t\tdef wrapper_with_caching(wrapped, maxsize=maxsize, typed=False):\n\t\t\tcache = memoized(get_cache, maxsize)\n\t\t\treturn wrapped(wrapped, cache=cache)\n\t\treturn wrapper_with_caching\n\treturn wrapper\n\n\ndef get_cache(maxsize=128):\n\t\"\"\" A function which gets a cache implementation which remembers\n\tdata to be stored in the cache. Returns the cache.\n\t\"\"\"\n\tassert msize > 0\n\tlast_value = cache\n\tlast_time ="}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t# The lru_cache should be created using a function that accepts a single\n\t# keyword argument `maxsize`. The function returns an LRU cache.\n\t#\n\t# Example usage::\n\tlru_cache(maxsize=5)\n\t#         a               b               c\n\t# key     value          key              value\n\t# ------"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef inner(func):\n\t\tif not callable(func):\n\t\t\tf=func\n\t\t\treturn getattr(f, 'lru_cache', lambda: object) if typed else f # type: ignore\n\t\tresult=lru_cache(maxsize)\n\t\treturn wrap_cache(result, func, maxsize=maxsize)\n\treturn inner\n\ndef wrap_cache(func, cache=None, maxsize=128):\n\tif cache is None:\n\t\tcache = LRUCache(maxsize)\n\treturn function_wrapper(func, cache, maxsize=maxsize)"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef wrapper(func, memo, **kwargs):\n\t\tif typed and len(kwargs) > 0:\n\t\t\traise TypeError(\"Typed function arguments not supported\")\n\t\tif typed and len(kwargs) == 0:\n\t\t\tcache_key = functools.partial(cache.get, maxsize)\n\t\t\tkwargs[key] = functools.wraps(cache_key)\n\n\t\treturn memo(func(*args, **kwargs))\n\treturn wrapper\n\n\ndef _get_cache_key(data, key_fn=functools.lru_cache):\n"}]}
{"task_id": "62b8d23748ba5a41d1c3f496", "prompt": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tclass LFUBase(object):\n\t\t\"\"\"Wraps a function into a memoizing function which saves up to `maxsize` results.\"\"\"\n\t\tMAXSIZE = maxsize\n\n\t\t@lazily_cache(maxsize=maxsize, typed=typed)\n\t\tdef __init__(self, fn, *args, **kwds):\n\t\t\t# The argument list was originally originally an empty dict, but\n\t\t\t# now we always have a tuple and want to pass through.\n\t\t\tif not kwds:\n\t\t\t\tself._cache = {}\n\t\t\telse:\n\t\t\t\t"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\treturn memoize_function(_lfu_cache, maxsize=maxsize, typed=typed)\n\n\ndef memoize_function(func, maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm, with more stricter constraints on `maxsize`.\n\t\"\"\"\n\tif maxsize < 1: return _lfu_cache\n\tif func is _lfu_cache: return func\n\tif not len(func"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef memoized_wrapper(f):\n\t\t\"\"\"\n\t\tWrapper to wrap a function with a memoizing callable that saves\n\t\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\t\talgorithm.\n\t\t\"\"\"\n\t\tf = cast(Callable, f)\n\t\tif len(f.args) > _MAX_ARG_LENGTH:\n\t\t\tf = lambda x: x\n\t\treturn CacheWrapper(f, typed=typed, capacity=_CACHE_SIZE)\n\treturn memoized_wrapper\n\ndef filter_func"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t@wraps(func)\n\t@wraps(lambda a: '...' if len(a) > 10 else '...')\n\tdef memoized(self, *args, **kwds):\n\t\tif not self._has_memoization():\n\t\t\treturn\n\t\t\tfor a in args:\n\t\t\t\tif not all([callable(b) for b in a]):\n\t\t\t\t\treturn # This memoization is definitely not\n\t\t\t\t\t# thread-safe.  In this case, we simply\n\t\t\t\t\t# return a cached result and break out of\n\t\t\t\t\t# the"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\treturn lfu.cache(maxsize, typed)(f)\n\n\ndef lfe_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on the LSF algorithm.\n\t\"\"\"\n\treturn lfu.cache(maxsize, typed)(lambda x, y: x + y)\n\n/**\n * Copyright (C) 2018 Mailvelope GmbH\n * Licensed under the GNU Affero General Public License version 3\n */\n\nimport util from 'util';\nimport type {Mailbox} from './"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\treturn cache(lambda x: _lfu_cache(maxsize, typed)(x), cache_size)\n\ndef _list_with_lfu_from_cache(num_lists, fru_cache=None, maxsize=64):\n\t\"\"\"\n\t\tReturns a non-overlapping list with the same contents as\n\t\tfrom a list cache based on the provided\n\t\tcache_size. If the list cache is `null` or `maxsize = 0`,\n\t\tthis will default to the number of lists in the cache\n\n\t\t:param num_lists:"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t@wraps(func)\n\tdef wrapper(f):\n\t\ttry:\n\t\t\tresults = next(f())\n\t\texcept StopIteration:\n\t\t\tif typed:\n\t\t\t\tpass\n\t\telse:\n\t\t\tif results and isinstance(results, list): \n\t\t\t        # memoize list if input is large\n\t\t\t\tmemo = {}\n\t\t\t\tfor result in next(results):\n\t\t\t\t\tif maxsize:\n\t\t\t\t\t\tif len(result) > maxsize:\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\telse:\n\t\t\t\t\t\tbreak\n\t\t\t\t\t"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t__wrapper__ = cache(maxsize=maxsize, typed=typed)\n\t# def wrapper(*args, **kwargs):\n\t# \treturn __wrapper__(*args, **kwargs)\n\treturn __wrapper__\n\n\ndef lfu_fast(size=16, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a deterministic LFU algorithm.\n\tA slowest but powerful LFU algorithm is only about 500 times slower\n\tthan fast algorithms.\n\t\"\"\"\n\t__wrapper__ = fast(size=size, typed=typed)\n\t# def wrapper(*"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tm = maxsize\n\tif m:\n\t\tif not hasattr(maxsize, '__len__'):\n\t\t\tm = 8\n\t\treturn lambda f, **ka: f(*[x for x in range(1, m + 1)])\n\telse:\n\t\tcache = {}\n\t\treturn cache.__getitem__\n\ndef lfu_cache_all(maxsize=128):\n\tcache = {}\n\treturn cache.__getitem__\n\n<?php\n\nreturn [\n    'Names' => [\n        'af' => '"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t@wraps(wrapped)\n\tdef wrapped(*args, **kwargs):\n\t\tif args == ():\n\t\t\tyield None\n\t\t# Else, we're a big problem\n\t\t# First, we'll be a bit lazy: we'll memoize a large number of\n\t\t# result sets, in order to reduce the amount of memory needed.\n\t\t# Also, at this point, we'll keep a list of those results,\n\t\t# regardless of where they're coming from.\n\t\tif maxsize is not None and \\\n\t\t\tlen(args) <= max"}]}
{"task_id": "62b8d22f48ba5a41d1c3f488", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tself.cursor.popitem()\n\treturn (key, self.cursor.getitem(key))\n\n\nclass ConnectionPool(object):\n\t\"\"\"\n\tManages multiple connections with a fixed capacity.\n\t\"\"\"\n\n\tdef __init__(self, maxsize=128):\n\t\tself.connection_cache = {\n\t\t\t'default': _default_connection,\n\t\t\t'file': _file_connection,\n\t\t\t'pipe': _pipe_connection,\n\t\t\t'memory': _memory_connection,\n\t\t\t}\n\t\tself.connections = []"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tif self.list is None or len(self) == 0:\n\t\traise errors.EmptyError(self.key)\n\tif not self.first:\n\t\traise errors.EmptyError(self.last)\n\tresult = self.list.popitem(0)\n\tself.size -= 1\n\treturn result\n\n\ndef pop(self, *args):\n\t\"\"\"\n\t\tRemove and return the argument specified by `args`\n\t\tfirst before removing the argument.\n\t\tIf there are no arguments in the list,\n\t\treturn `None`.\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\treturn (item for __, __ in __) if __ and __[0][1] == '__' and __[1][1] == '*'}\n\ndef push(k, v, **kva):\n\t\"\"\"\n\tInsert 'k = v', e.g. push('title', 'The Harry Potter').\n\t\"\"\"\n\tif not k: return\n\tif not v:\n\t\tdel kva['k']\n\t\treturn\n\tkva['k'] = k\n\tkva['v'] = v\n\twhile type(kva['k'])"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tres = {}\n\tfor field in self.field_keys:\n\t\tv = getattr(self, field)\n\t\tif v!= _MISSING:\n\t\t\ttry:\n\t\t\t\tres[field] = v()\n\t\t\texcept ValueError as e:\n\t\t\t\tif is_iterable(v) and not is_dict(v):\n\t\t\t\t\ttry:\n\t\t\t\t\t\tdel res[field]\n\t\t\t\t\texcept KeyError as e2:\n\t\t\t\t\t\tpass  # field isn't actually in the result, or\n\t\t\t\t\t\t# is not a dict\n\t\t\t\t\t\tif hasattr(field,"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tret = {}\n\twhile 1:\n\t\ttry:\n\t\t\tkey, val = next(self._dict)\n\t\texcept StopIteration:\n\t\t\tself.clear()\n\t\t\treturn ret\n\t\tif not isinstance(key, str):\n\t\t\tkey = str(key)  # for python2\n\t\t\tval = None\n\t\tret[key] = val\n\tself.clear()\n\treturn ret\n\ndef pop(self, key, default=None):\n\ttry:\n\t\tval = self[key]\n\texcept KeyError: return default"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tresult = self._dictionary.__popitem__(False)\n\tself._dictionary.__setitem__(result[2], result[1])\n\treturn result\n\ndef unshift(self, key):\n\t\"\"\"\n\tThe key is unshifted.\n\t\"\"\"\n\tkey = str(key)\n\tif key not in self._dictionary:\n\t\treturn None\n\tdict.__setitem__(self, key, dict.__getitem__(self, key) - 1)\n\treturn dict.__getitem__(self, key)\n\ndef unshiftitem(self):\n\t\"\"\"\n\tRemove"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tself.size -= 1\n\tkey, value = self.popitem_first()        # <<<<<<<<<<<<<<\n\tself._setitem_delete(key, value)\n\ndef pop(self, key):\n\t\"\"\"\n\tAlias for popitem()\n\t\"\"\"\n\treturn self.popitems()[key]\n\ndef popitem_first(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tres = self.popitem()\n\tnext_key = res[0]\n\tnext"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\treturn OrderedDict([(k,v[0]) for k,v in self.items()])\n\n\nclass Stack(dict):\n\n\tdef __init__(self, s=[]):\n\t\tdict.__init__(self, s)\n\t\titems = self.items()\n\t\tself._maxsize = {k:len(v) for k,v in items}\n\t\tself._len = len(self)\n\t\tself.reverse()\n\n\tdef reverse(self):\n\t\t\"reverse contents of self, i.e. list is like [3,2,"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\ttry:\n\t\tpopped, e = self.stack\n\t\treturn (popped, e), popped\n\texcept ValueError:\n\t\tself.save_stack()\n\t\traise\n\t\treturn popped=False, e\n\ndef getfirstitem(self):\n\"\"\"\n\tReturn the first `(key, val)` pair\n\"\"\"\n\tif not self.stack:\n\t\tself.load_stack()\n\treturn self.stack[-1] if self.stack else None\n\ndef getnextitem(self):\n\"\"\"\n\treturn the next `("}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tkey = self.pop_pair()\n\tvalue = self.pop_pair()\n\tkey.parent  = None\n\tdel self.map[key].key\n\tself.map[key] = (key, value)\n\treturn key, value\n\n\n\ndef pop_pair(pair: Tuple[Optional[str], Optional[str]]):\n\t\"\"\"\n\tExtract the first element or `None`,\n\treturn `(None, None)` or else `(key, value)`\n\t\"\"\"\n\tkey = None\n\tvalue = None\n\tfor (n, p"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47e", "prompt": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tif default is None:\n\t\tself.__dict__['__missing'] = self.__dict__['__all__'] if type(key) is tuple else [key]\n\t\tself.__dict__['__all__'].remove(key)\n\t\tself.__dict__['__all__'].sort()\n\t\treturn default\n\tself.__dict__['__setdefault'][key] = default\n\ndef submod(mod, attr):\n    \"\"\"a submodular module decorator.\n    if attr is None, all attributes in mod are used\"\"\"\n  "}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tif not self:\n\t\treturn default\n\tif isinstance(default, str_cls):\n\t\tdefault = self.__class__(default)\n\treturn key in self and self[key] or default\n\ndef iteritems(d):\n\t\"\"\"\n\t\tYields (k, v) for each item in dictionary d.\n\t\"\"\"\n\tfor k in d.iterkeys():\n\t\tyield d[k]\n\ndef iterkeys(d):\n\t\"\"\"\n\t\tYields (k, v) for each key in dictionary d.\n\t\"\"\"\n\tfor k"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\treturn dict.__setdefault__(self, key, default)\n\ndef __getattribute__(self, func, module=None, onerror=None):\n\tfrom _modsupport import ModuleType, _pickle_save, _long, SHORTREAL, isgenerator, isasyncgen\n\tif func is None:\n\t\treturn\n\n\tif onerror is not None:\n\t\ttry:\n\t\t\tfunc = getattr(module, func)\n\t\texcept AttributeError:\n\t\t\tfunc = getattr(module, func.__name__)\n\t\tif func is _pickle_save:\n\t\t"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tif key in self:\n\t\tolddefault = self[key]\n\t\tdel self[key]\n\t\tif not isinstance(olddefault, ExceptionWrapper):\n\t\t\tself[key] = olddefault\n\telse:\n\t\tself[key] = default\n\treturn default\n\n\nclass ExceptionWrapper:\n\tdef __init__(self, exception, frame):\n\t\tself.exception = exception\n\t\tself.frame = frame\n\n\tdef __getattr__(self, name):\n\t\treturn getattr(self.exception, name)\n\n\tdef __getattribute"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tif self._fields is not None:\n\t\tm = self._fields.get(key)\n\t\tif m:\n\t\t\tif m[0]=='__name__':\n\t\t\t\tsetattr(self,key,'__class__'.\"(__name__ = '')\")\n\t\t\t\tprint(\">>> Setting %s as __class__\" % key)\n\t\t\telse:\n\t\t\t\tsetattr(self,key,\" = __class__(\")\n\t\t\t\tprint(\">>> Setting %s back as __class__\" % key)\n\t\t\t\n\t\t\treturn m[1]\n\telse:"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tif not isinstance(key, str):\n\t\traise TypeError(\"Keys must be strings\")\n\tif default is None: default = self._default\n\td = _dictkey2data(key, self, self.proto, default)\n\ttry:\n\t\tself[d] = default\n\t\tself._setdefault(d)\n\texcept KeyError:\n\t\tself[d] = default\n\ndef _dictkey2data(key, d, proto=None, default=None):\n\tif key is not _empty:\n\t\treturn (key, d._dict"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tD = {}\n\tif default is None:\n\t\tD[key] = None\n\telif isinstance(default, type({})):\n\t\tD[key] = default\n\telse:\n\t\twith dummycontext():\n\t\t\tval = default\n\t\t\tif isinstance(default, tuple) or isinstance(default, list) or isinstance(default, dict):\n\t\t\t\tD[key] = default[0] if default else None\n\t\t\telse:\n\t\t\t\tD[key] = default\n\t\t\tD[key] = _fixtype(val, 'tuple,list"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tdef cb(c):\n\t\tself[k] = c\n\treturn cb\na.__getitem__ = setdefault\n\n\nclass _SortedDict(dict):\n\n\t\"\"\"\n\t\tdict[k] = sorted( v )\n\n\t\tNOTE: This method is used as a callback to python 2.\n\t\tTo change the sorting behavior, use the \"key\" attribute\n\t\ton the dict instance that will be generated.\n\t\"\"\"\n\n\tdef __init__(self, *args, **kwargs):\n\t\tkeys = kwargs.copy()\n\t\tkeys."}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\treturn self.data.__setitem__(key, default)\n\ndef get_set():\n\treturn {'' if d['__class__'] is NoData else k for k,d in vars(d).items()}\n\n\ndef copy(mapping=None, default=None, shallow=None, deep=None):\n\tif isinstance(mapping, str):\n\t\tif default == None:\n\t\t\tdefault = None\n\t\tD = {'__default__': default} if default == object() else {'default': default}\n\telse:\n\t\tif default == None:\n\t\t"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\ttry:\t# May raise AttributeError: 'hasattr' object has no attribute 'default'\n\t\tsuper = self\n\t\twhile super is not None:\n\t\t\tdefault = super.get(key,default)\n\t\t\tsuper = super.parent\n\texcept AttributeError:\t# Subclasses may have no attribute 'default' -> raise AttributeError as per docs.\n\t\traise AttributeError(\"attribute '%s' not found in class\" % key)\n\treturn default\n\ndef _docdescriptor(obj):\n\ttry:\n\t\treturn obj.__doc__\n\texcept AttributeError:\n\t"}]}
{"task_id": "62b8d22948ba5a41d1c3f47c", "prompt": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\ttry:\n\t\tvalue = self.__dict__.get(key,default)\n\texcept AttributeError:\n\t\tvalue = default\n\t\tself.__dict__[key] = value\n\treturn value\n\ndef keys_list_value(self,key, default=None):\n\t\"\"\"\n\t\tD.keys_list_value(key,default) -> D[key] if key in D, else D.default\n\n\t\tif not default, return default if key does not exist in or is None\n\t\telse return _D.default(key) if key is None\n"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\treturn _dict_get(self.data,key,default)\n\ndef __contains__(self, key):\n\treturn _dict_contains(self.data,key)\n\n#\ndef __delitem__(self, key):\n\t\"\"\"\n\tWe use the __del__ method because keys in dictionaries\n\tare unique and this causes ambiguity with dictionary keys\n\t\"\"\"\n\t_dict_delitem(self.data,key)\n\n#\ndef clear(self):\n\t_dict_clear(self.data)\n\ndef find(self, key, default"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tvalue = self.__dict__.get(key, dget(self.__class__,\n#        ((default is None and key in self.__class__._members) or key in\n#         self.__class__._members.__dict__), self))\n\treturn value\n\ndef get_func(func):\n\t\"\"\"\n\tD.get_func -> Callable[[D], D[func]] if func in D, else None.\n\t\n\tThis attribute is an alternate name for :py:meth:`get` and\n\tis removed in Python"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn default\n\ndef update(self, *args, **kwds):\n\t\"\"\"\n\tD.update(E, **F) -> D.  Update (E, F) according to D, ignoring F.\n\t\"\"\"\n\treturn self.merge_unique(kwds)\n\n# Copyright 2010 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tdm=dict.fromkeys(self.dict, None)\n\tv=dm.get(key, default)\n\treturn v\n\t\ndef items(self):\n\t\"\"\"\n\tD.items() -> list of (k,v) pairs.\n\t\"\"\"\n\titems=[]\n\tfor k in self.dict.keys():\n\t\titem=self.dict[k]\n\t\tif isinstance(item, type(\"\")):\n\t\t\tif type(item) in (bool,str):\n\t\t\t\tif item:\n\t\t\t\t\tbreak\n\t\t\tif not hasattr(item"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tdel key, d\n\ttry:\n\t\td = self._cache[key]\n\t\tif d is None:\n\t\t\treturn default\n\t\treturn d[0]\n\tfinally:\n\t\tdel self._cache[key]\n\n###\n# This module is automatically generated by\n# _generate_by_<name>() and should not be manually edited by hand.\n\nname = '%s_to_utf8' % charset\n\nif not _have_encoding:\n\tif not sys.platform.startswith('java'):\n\t\tif sys.version > '2"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tif default is None:\n\t\tdefault = self.defaults[key]\n\tif self._has_key(key):\n\t\treturn self.values[key]\n\td = default\n\tfor subkey in self.subkeys + [key]:\n\t\ttry:\n\t\t\treturn self.defaults[subkey]\n\t\texcept KeyError:\n\t\t\ttry:\n\t\t\t\td = self.values[subkey]\n\t\t\texcept (KeyError, IndexError):\n\t\t\t\tprint(\"Unknown value for:\", key, subkey)\n\t\tself.update(sub"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tD = self\n\t_keys = (\"__dict_items\", \"__dict_keys\", \"keys\")\n\tfor a in _keys:\n\t\ttry:\n\t\t\treturn D.__dict__[a][key]\n\t\texcept KeyError:\n\t\t\tpass\n\tif d is not None:\n\t\treturn d\n\n\t\n\traise KeyError, \"Attribute not found: %s\" % repr(key)\t\n\t\nclass Dictionary:\n\t\"\"\"A container for items, keys and/or values.\n\t\t\n\tThis is intended to allow key and item access.  However, the methods"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\ttry:\n\t\tk = self.__getitem__(key)\n\texcept KeyError:\n\t\tif default==None:\n\t\t\traise KeyError(key)\n\t\telse:\n\t\t\treturn default\n\treturn d.get(k, default)\n\ndef getint(self, key):\n\td = self.get(key)\n\tif not d:\n\t\traise KeyError(key)\n\treturn int(d)\n\ndef iterkeys(self):\n\tfor key in self.keys():\n\t\tyield key\n\nfrom StringIO import StringIO\n\n"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\treturn self._data.get(key, default)\n\ndef setdefault(self, key, default=None):\n\t\"\"\"D[k]=default\"\"\"\n\treturn self._data[key] = default\n\ndef pop(self, key, *args):\n\treturn self.setdefault(key, args)\n\ndef popitem(self):\n\treturn self.pop()\n\ndef setdefault_multi(self, *args):\n\tassert len(args) == 1 or len(args) == 2\n\n\tnew = args[0]\n\tsetattr(self._"}]}
{"task_id": "62b8d22548ba5a41d1c3f472", "prompt": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tif ismethoddescriptor(func):\n\t\t\tdef inner(self, *args, **kws):\n\t\t\t\ttry:\n\t\t\t\t\tself._cache[key(type(self), *args, **kws)] = \\\n\t\t\t\t\t\t(func, self._cache[key(type(self), *args, **kws)],\n\t\t\t\t\t\t repr(func), self)\n\t\t\t\t\t\t)\n\t\t\t\t\telse:\n\t\t\t\t\t\t# A cache miss will make the assumption that\n\t\t\t\t\t\t# if another thread writes this cache\n\t\t\t\t\t\t# (or someone else's"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tif lock is not None:\n\t\traise TypeError(\"cache does not work with memoizing functions\")\n\n\treturn _cache_memoized_method(cache, key, functools.partial(\n\t\t\t\tmemoize,\n\t\t\t\tfunctools.partial(save_result, cache)))\n\ndef save_result(*args):\n\tglobal cache\n\t(cache, cache, cache) = args\n\t# We make an exception from the cache to not change\n\t# the local cache structure\n\tdel cache[:]\n\treturn cache, args\n\ndef memoize(f, *args, **"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef wrapped(*args, **kw):\n\t\tresult = cache.get(key)\n\t\tif not result:\n\t\t\tresult = cache.setdefault(key, cachemethodmethod(cache, key=key, lock=lock))\n\t\targs = list(args)\n\t\tresult(*args[1:], **kw)\n\t\treturn result\n\treturn wraps(getattr(wrapped, 'im_func', wrapped))(wrapped(*args, **kw), lock)\n\n#include <cstdio>\n#include <vector>\n#include <algorithm>\nusing namespace std;"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t__slots__ = ('_method_cache',)\n\tdef __getattr__(self,name):\n\t\treturn self._method_cache[(self.__class__, self.__func__.__name__)]\n\tdef __get__(self,*a):\n\t\ttry:\n\t\t\treturn self._memo[self.__func__.__name__]['%s'%self.__func__.__name__] = self._method_cache[(self.__class__, self.__func__.__name__)]\n\t\texcept KeyError:\n\t\t\tif self._default:\n\t\t\t\treturn self._default\n\t\treturn _cache[type"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tassert callable(getattr(obj, key))\n\tdef memoized(func=func, *args, **kw):\n\t\tif not obj:\n\t\t\tvalue = func(*args, **kw)\n\t\t\tsavecache(key, value)\n\t\telse:\n\t\t\tsavecache(key,\n\t\t\t          partial(func, *args, **kw) if cache else value)\n\treturn _wrap_with_memoize(memoized, cache, getattr(obj, key))\n\ndef cachedmethodmethod(cache, key=hashkey,"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tclass memo_method(object):\n\n\t\tdef __init__(self, func):\n\n\t\t\tself.func = func\n\n\t\tdef __call__(self, instance, name=None):\n\n\t\t\tresult = self.func(instance, name=name)\n\t\t\tv = result()\n\n\t\t\tif result is None:\n\t\t\t\tif type(instance).__module__ == 'builtins':\n\t\t\t\t\tv = functools.total_ordering(v)\n\t\t\t\t\t# hack for built-in functions\n\t\t\t\t\tresult = (instance.im_func, v)"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t@functools.wraps(method)\n\tdef cachedmethod(self, *args, **kwds):\n\t    key = cache.get(key, self.key)\n\t    res = cache.get(key, self.res)\n\t    cached = cache.get(key, self.cached)\n\t\tlock = lock or self.lock\n\t\tif lock.acquire(False):\n\t\t    try:\n\t\t        return res\n\t\t    finally:\n\t\t        lock.release()\n\t"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\"\"\"\n\tIf specified, use the given caching function instead of the decorated\n\tfunction.  See #17822.\n\t\"\"\"\n\tif cache is None:\n\t\tcache = None\n\n\t@functools.wraps(func)\n\tdef wrapper(*args, **kwds):\n\t\tif cache is None or cache[1] is None:\n\t\t\treturn func(*args, **kwds)\n\t\telse:\n\t\t\treturn cache[1]\n\n\twrapper.cache = cache\n\treturn wrapper\n\n/*\n * Copyright (C) 2015-2021"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tclass memoized(object):\n\t\t\"\"\"\n\t\tThis decorator is used to memoize method calls for a single\n\t\tinstance and class. In this case the callable should not\n\t\thold a key in the first argument.\n\t\t\"\"\"\n\t\tdef __init__(self, _func, _memo, *args, **kwargs):\n\t\t\tself._method = _func\n\t\t\tself._method_signature = _namedtuple('_method', ['args', 'kwargs'])\n\t\t\tself._args = []\n\t\t\tself._kwargs = dict(_namedtuple('_method"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef wrapper(*a, **kw):\n\t\t@singledispatch\n\t\tdef wrapper(*b):\n\t\t\t# Emulate a cache miss for this method\n\t\t\tkey    = hashkey(*(args.args or [a, b]))\n\n\t\t\tif cache.has_key(key):\n\t\t\t\treturn cache.get(key)\n\t\t\telse:\n\t\t\t\tresult = cache.store_memoized(*args, **kw)\n\t\t\t\tif result is not None:\n\t\t\t\t\tcache.add(key, result)\n\t\t\t\treturn result\n\t\treturn wrapper"}]}
{"task_id": "62b8c517e0d34b282c18122e", "prompt": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\tmsg = \"An error was found.\"\n\n\tclass Message(type):\n\t\t@staticmethod\n\t\tdef format(msg_value, indent='  '):\n\t\t\t# This should be used to build error messages when not dealing with exceptions.\n\t\t\tprint_ex(cls._repr_value(msg_value, indent), indent).rstrip()\n\n\t\t@staticmethod\n\t\tdef __repr_value(value, indent='    '):\n\t\t\t# Return the repr'd value.\n\t\t\tif value is None: return repr(val)\n\t\t\t"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\treturn \"Exception \" + str(e) + \", max call level \" + str(max_level) + \", path level \" + str(max_path_level) + '.'\n\n\ndef no_path_exception(e):\n\treturn 'No path has been found to'+ str(e)\n\n\ndef no_method(cls, e):\n\tclass InnerException(Exception): pass\n\treturn InnerException(e.__class__.__name__ + \".\" + str(e) + \", \" + e + \".\")\n\n\ndef wrong_argument_exception(fn_name, arg"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\tresult = cls._get_stack()\n\tresult.append('')\n\tresult.append('[Exception information]')\n\tif e.args:\n\t\tresult.append('-'+ str(e.args))\n\telse:\n\t\tresult.append('-'+ 'No exception object given')\n\tif max_level > 0:\n\t\tresult.append('-'+ str(max_level) +'(of max nesting level, default ')\n\t\tresult.append(str(max_level) + str(max_path_level) + ')\\n')\n\tresult"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t_trace = traceback.format_exc(limit=max_level)\n\ts = \"\"\n\tfor _trace_line in traceback.format_tb(_trace.__traceback__, limit=max_path_level):\n\t\ts += \"    > %s\" % _trace_line\n\treturn s\n\ndef get_module(func, name):\n\t\"\"\"\n\tDetermine the module name of a function.\n\t:param func: The function to determine the name of (e.g. torch.nn.functional.relu)\n\t:type func: callable\n"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\te = as_exception(e)\n\n\tname = e.__class__.__name__\n\tprefix = 'Exception in %s' % name\n\tcode = '%s.%s(%s)' % (prefix, name, e._args[0])\n\n\ttraceback_string = ''\n\tif e.filename:\n\t\ttraceback_string = 'Traceback (most recent call last):\\n' + traceback.format_stack()\n\n\tif e.lineno > 0:\n\t\ttraceback_string += 'line %d:' % e.lineno\n\n\t"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\timport traceback\n\n\tdef get_stack_trace():\n\t\tif issubclass(e.__class__, Exception):\n\t\t\ttry:\n\t\t\t\ttrace = e.__cause__[:]\n\t\t\texcept AttributeError:\n\t\t\t\ttrace = traceback.format_exc()[-1]\n\t\t\t\ttrace['cause'] = None\n\t\t\treturn \"\".join([trace, traceback.exception[2][0].tb_frame.f_code.co_name])\n\t\telse:\n\t\t\treturn e.__base__.__class__.__name__\n\n\tmsg = \"\"\n\tif get_stack_"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\ts = u'''\\\n--------------------------------------------------------------------------------\n--------------------                 START OF EXCEPTION         ----------------\n---------------------------------                 -----------------------\n--------------------         ERROR                  --------------------------\n--------------------------------------------------'''\n\terrorcode = '\\033[93m'\n\tmsg = u'[%s]' % classname(cls)\n\tpath = u'(%s"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t_append = ''\n\t_format = None\n\tlast_exc = None\n\tfor e in e.__traceback__:\n\t\tif isinstance(e, tuple) and len(e) == 2:\n\t\t\tlast_exc = e[0]\n\t\t\t_append = ''\n\t\t\t_format = '%(exc_info)s\\n' % locals()\n\t\t\tif hasattr(last_exc, \"__path__\"):\n\t\t\t\t_format += '\\tfast path: \"%s\"' % last_exc.__path__\n\t\t\tif hasattr(last_exc, \"__"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\tinfo = e.__traceback__\n\tmodule = info[-2]\n\tmodulename = info[-1]\n\texc_string = ''.join(info)\n\tif max_level and len(exc_string) > 2 and (cls.is_ex\n\t\tor getattr(modulename, '__version__', None)) >= (3, 0, 999999)\n\t:\n\t\texc_string = exc_string.limit(max_level)\n\tif max_path_level and len(exc_string) > max_path_level:\n\t\t"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\te.args[1] = cls\n\te.args[2] = max_level\n\te.args[3] = max_path_level\n\treturn str(e)\n\n\n###############################################################################\ndef make_exception_to_string(e):\n\t\"\"\"\n\tTakes an extremes exception as argument.\n\t:param e: Any extremes exception instance\n\t:type e: Exception\n\t\"\"\"\n\te = e.args[0]\n\ts = \"Extremely rare error.\"\n\ts += \"\\n  Exception: {}\\n\"."}]}
{"task_id": "62b8bbbfe0d34b282c181210", "prompt": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tif file_name[-3:].lower() == \".gz\":\n\t\tfile_open = gzip.open\n\telse:\n\t\tfile_open = open\n\treturn file_open(file_name, 'wb' if os.name == 'nt' else 'a').write(text_buffer) or lt(0)\n\ndef append_text_to_csv(csv_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param csv_name"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t_check_string(text_buffer)\n\tif encoding not in encodings:\n\t\tencoding_obj = encodings[encoding]\n\t\tencoding = encoding_obj.get_default_encoding()\n\ttext_buffer = string_to_bytes(text_buffer, encoding)\n\tsize = len(text_buffer)\n\n\tif overwrite:\n\t\ttmp = open(file_name, \"w\", encoding=encoding)\n\t\ttmp.write(text_buffer)\n\t\ttmp.close()\n\t\tsize += len(text_buffer)\n\telse:"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t#\n\tif not overwrite and os.path.exists(file_name):\n\t\tprint(\"File %s already exists\" % file_name)\n\t#\n\ttotal_bytes = 0\n\tsuccess = 1\n\twith open(file_name, 'rb') as f:\n\t\tfor i in text_buffer:\n\t\t\ttotal_bytes += len(i)\n\t\toutput_buffer = bytearray(total_bytes)\n\t\tf.readinto(output_buffer)\n\twith open(file_name, \"rb+\") as destination, open(file"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tif overwrite is True:\n\t\tfile_name = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"/\".join(map(str, file_name)))\n\ttry:\n\t\ttext_buffer = text_buffer.encode(encoding)\n\texcept AttributeError:\n\t\treturn 0\n\n\tif not file_exists(file_name) and overwrite is False:\n\t\treturn 0\n\n\tfileptr = open(file_name, O_CREAT|O_WRONLY|O_TRUNC|O_TRUNC|O_BINARY"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\timport os, sys\n\tif os.path.exists(file_name):\n\t\t# Attempt to overwrite if an existing file exists\n\t\ttry:\n\t\t\twith open(file_name, 'rb') as fh:\n\t\t\t\tf_size = fh.seek(0, 2)\n\t\t\t\tfh.truncate()\n\t\texcept OSError:\n\t\t\ttry:\n\t\t\t\twith open(file_name, 'wb') as fh:\n\t\t\t\t\tfh.write(text_buffer.encode(encoding))\n\t\t\t\tfh.truncate()\n\t\t\texcept OSError"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\timport gzip, re\n\timport struct\n\timport sys\n\timport os\n\n\tfilename = os.path.splitext(file_name)[0]\n\n\ttry:\n\t\tf = codecs.open(sys.argv[1], \"w\", encoding=encoding)\n\texcept (IOError, OSError) as e:\n\t\tsys.stderr.write(\"Can't write to %s: %s\\n\" % (file_name, str(e)))\n\t\treturn 1\n\n\tlines = 0\n\tif not overwrite:\n\t\ttext = text"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\treturn common.write_text_to_file(file_name, text_buffer, encoding, overwrite)\n\ndef copy_file_to_dir(src, dst, name, mode=\"r\"):\n\t\"\"\"\n\tCopy a file to a local directory.\n\t:param src:  The source file.\n\t:type src: str\n\t:param dst:  The target directory.\n\t:type dst: str\n\t:param name: The name of the target directory.\n\t:type name: str\n\t:param mode: The operation mode to use.\n\t:"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tf, file_name = Path(file_name).open()\n\tif encoding:\n\t\ttext_buffer = text_buffer.encode(encoding,'surrogateescape')\n\tpos = 0\n\ttotal = 0\n\twhile pos <= len(text_buffer):\n\t\tl = text_buffer[text_buffer.find(b'\\r\\n', pos):pos + nlen - 1]\n\t\tpos += l + 1\n\t\tif len(text_buffer[pos:pos + nlen]) == 0:\n\t\t\tif pos >= lengthoffile("}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tif file_name == None:\n\t\treturn 0\n\tassert encoding is not None  # Must be done by caller\n\tif not isinstance(text_buffer, TextIOWrapper):\n\t\ttext_buffer = TextIOWrapper(text_buffer)\n\tfile = open(file_name, 'U' if overwrite else 'r+b')\n\tsize = 0\n\tfor char in text_buffer:\n\t\tif char is b'\\x00':\n\t\t\tsize += 1\n\t\t\tif encoding is not None:\n\t\t\t\ttext_buffer = text_buffer.wrap"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\ttry:\n\t\twith open(file_name, \"ab+\", encoding=encoding, buffering=4096) as file:\n\t\t\tif encoding!=None:\n\t\t\t\tfile.write(text_buffer)\n\t\t\telse:\n\t\t\t\tfile.write(text_buffer.encode(\"utf-8\"))\n\n\t\treturn file.tell()\n\texcept Exception as err:\n\t\treturn 0\n\ndef append_html_to_file(file_name, html_text, encoding):\n\t\"\"\"\n\tWrite to the specified filename, the provided text buffer, HTML encoding\n"}]}
{"task_id": "62b8bbbfe0d34b282c18120f", "prompt": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\t#file_content = open(file_name, 'r').read()\n\t\tfile_content = open(file_name, encoding=\"utf-8\")\n\texcept:\n\t\treturn None\n\t#text_buffer = TextBuffer(file_content, encoding)\n\treturn file_content\n\ndef file_write(file_name, buffer, encoding):\n\t\"\"\"\n\tWrite buffer to a file (UTF-8) with a specified encoding.\n\tCAUTION : This will write the whole file IN MEMORY.\n\t:param file_name:"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t#print \"File %s has encoding %s\" % ( \"is utf-8\" if IS_PY2 else \"is utf-8\",encoding )\n\timport re\n\ttry:\n\t\twith open(file_name, 'r', encoding = encoding) as file:\n\t\t\treturn file.read()\n\texcept IOError as e:\n\t\tprint e\n\t\tprint file_name\n\t\treturn None\n\ndef textbuffer_to_file(textbuffer, file_name, encoding):\n\t\"\"\"\n\tRead a text buffer back in and extract the text"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\twith open(file_name, \"rb\") as fp:\n\t\tdata = fp.read()\n\t\tdata_encoding = data.split(\"\\n\")[0] if len(data.split(\"\\n\")) > 0 else \"utf-8\"\n\t\tdata = data.decode(data_encoding, \"ignore\")\n\t\tdata = data.split(b\"\\n\")\n\t\tif len(data) == 1:\n\t\t\treturn \"\"\n\t\traw = data.pop(0)\n\t\tif raw == b\"\":\n\t\t\treturn \"\"\n\t\treturn"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\tif encoding:\n\t\treturn open(file_name, 'r', encoding).read()\n\telse:\n\t\treturn open(file_name, 'r').read()\n\n#ifndef _ASM_X86_FUTEX_H\n#define _ASM_X86_FUTEX_H\n\n#ifdef __KERNEL__\n\n#define __futex_atomic_ex_return(val, own_val)\\\n\tasm __volatile__(\\\n\"\tmovl %2,%[retval]\\n\" \\\n\"0:\tnop\\n\" \\\n\t\"\t.section"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t# load the file\n\ttry:\n\t\twith open(file_name,\"rb\") as f:\n\t\t\t# the file_contents\n\t\t\tfile_contents = f.read()\n\t\t# the file_contents\n\t\treturn file_contents.decode(encoding)\n\texcept Exception as e:\n\t\tprint(e)\n\t\treturn None\n\n\nclass FileInfo(object):\n\t\n\t@staticmethod\n\tdef get_name():\n\t\treturn \"test\"\n\n\n\nclass TextFile(FileIO):\n\n\tdef __init__(self"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t# Read the file\n\timport io\n\ttext_reader = io.TextInput()\n\n\t# Create a file reader for the file\n\twith open(file_name) as f:\n\t\ttext_reader.readlines(f) # [1]\n\t\ttext_reader.seek(0) # [2]\n\t# Open a buffer to read into and decode it\n\tbuffer = io.BytesIO()\n\ttext_reader.readinto(buffer) # [3-14]\n\traw_data = buffer.getvalue()\n\t# Load the buffer to"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\tresult = StringIO()\n\ttry:\n\t\tfile_contents = open(file_name, \"r\", encoding=encoding)\n\t\tfile_bytes_read = file_contents.read(1)\n\t\tif len(file_bytes_read) == 0:\n\t\t\tif len(file_contents.readline()) >= 0:\n\t\t\t\tfile_contents.close()\n\t\t\t\tresult.close()\n\t\t\t\traise IOError(\"Error reading from file.\")\n\t\tif len(file_bytes_read) > 0:\n\t\t\tresult.write(file_"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\timport codecs, struct\n\timport socket, struct, sys\n\ttry:\n\t\tf = codecs.open(file_name, 'rb', encoding)\n\texcept IOError:\n\t\treturn None\n\ts = f.read()\n\tif s and len(s) and not s[0].isspace(): # Broken file\n\t\traise IOError(\"This has been made with a file that cannot be read correctly. Consider using the decode_text_buffer() function instead.\")\n\tif sys.implementation.name == 'python3.x':\n\t\traise NotImplementedError\n\t"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\tbuffer = \"\"\n\twith open(file_name, \"r\", encoding=encoding) as f:\n\t\tfor part in f:\n\t\t\tif part == '\\n':\n\t\t\t\tcontinue\n\t\t\tbuffer += part\n\treturn buffer\n\n\ndef open_utf8_file(file_name):\n\t# :type file_name: str\n\t# :return: an fopen() of the filename in memory.\n\tbuffer = file_to_textbuffer(file_name)\n\treturn open(buffer, \"r\", encoding=\"utf-8\")\n\n\ndef"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\twith open(file_name, 'r') as file:\n\t\ttry:\n\t\t\ttext = file.read()\n\t\texcept UnicodeDecodeError:\n\t\t\ttext = file.readall()\n\tif issubclass(type(text), unicode):\n\t\ttext = text.encode(encoding)\n\n\treturn text\n\n# -- END OF FILE\n\n/*\n * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n * \n * Licensed under the Apache License, Version 2.0 (the \"License\").\n * You may not use this file except"}]}
{"task_id": "62b8bbbce0d34b282c18120d", "prompt": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif os.path.isdir(file_name):\n\t\treturn True\n\telse:\n\t\tos.mkdir(file_name)\n\t\treturn is_file_exist(file_name)\n\n\ndef download_file_to_filename_from_url(url=None, file_name=None):\n\tif url is None:\n\t\turl = \"http://www.github.com/\"\n\tif file_name is None:\n\t\tfile_name = os.path.basename(url)\n\tcontent_type = url.split(\"://"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\treturn not os.path.exists(file_name)\n\n\n\nif __name__ == '__main__':\n\t# import sys\n\t# sys.exit(0)\n\t\n\tif is_image_in_path(path_to_image):\n\t\t# load image by path\n\t\tcv2.imwrite(path_to_image, cv2.imread(path_to_image))\n\t\t\t\t\n\t\t# save image with bounding box from image\n\t\tcv2.rectangle(image, (x, y), (width, height), (0, 255,"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tfile_name=file_name.strip()\n\tif not file_name:\n\t\treturn False\n\twith open(file_name,'r','utf-8',errors='ignore') as myfile:\n\t\treturn myfile.readline().strip()\n\ndef create_file_path(relative_path):\n\t\"\"\"\n\tCreate file path for file relative_path.\n\t:param relative_path: Path of file relative to source folder.\n\t:type relative_path: str\n\t:return: Return the file path of the source file.\n\t:rtype str"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif os.path.isfile(file_name):\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n## Check if file exist.\ndef is_file_exist_with_suffix(file_name):\n\t\"\"\"\n\tCheck if file exists with suffix or without.\n\t:param file_name: File name with suffix.\n\t:type file_name: str\n\t:return: Return true (exist, suffix is exist), false (do not exist)\n\t:rtype bool\n\t\"\"\"\n\tif os.path.isfile(file_name"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tf = open(file_name,\"rb\")\n\tfor l in f.readlines():\n\t\tif len(l.strip()) > 0:\n\t\t\treturn True\n\treturn False\n\n# \u63a5\u4e0b\u6765\u4ee5\u540e\u68c0\u6d4b\u4e00\u4e0b\u6587\u4ef6\u662f\u5426\u6709\u53d8\ndef is_file_exist_but_not_remove(file_name, remove_list):\n\t\"\"\"\n\tCheck if file name exist without corresponding file in remove list.\n\tfile_name: File name.\n\t:param file_name: File name.\n\t:type file_name: str"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\treturn os.path.isfile(file_name)\n\ndef is_image_exist(image_path):\n\t\"\"\"\n\tCheck if image exists.\n\t:param image_path: Image Path.\n\t:type image_path: str\n\t:return: Return true (exist), false (do not exist, or invalid image name)\n\t:rtype bool\n\t\"\"\"\n\treturn os.path.exists(image_path)\n\t# return True\n\ndef download_file(image_path, file_name):\n\t\"\"\"\n\tDownload a file.\n\t"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tdef _is_file_exist(file_handler):\n\t\tmode = os.stat(file_handler.filename).st_mode\n\t\treturn mode & 0x80 > 0 and mode & 0x40 > 0\n\tif not is_file_exist(file_name):\n\t\treturn False\n\treturn _is_file_exist(file_handler)\n\nimport os\nfrom datetime import datetime\n\nfrom django.utils.timezone import now\nfrom rest_framework import status\nfrom.models import Task, TaskDependency, TaskError, TaskStatus, TaskStatusError\n"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\ttry:\n\t\tmy_path = Path(os.curdir)\n\t\tmy_path = my_path / file_path.filename\n\t\tif os.path.isfile(my_path):\n\t\t\tmy_path = FilePtr(my_path, file_name)\n\t\telse:\n\t\t\treturn False\n\t\treturn True\n\texcept BaseException as e:\n\t\treturn False\n\n\n@function_node(\"Create new dir_id. If it is empty, create first.\")\ndef mkdir_id(my_path):\n\tif"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\treturn (os.path.isfile(file_name) or os.path.exists(file_name) or\n\t\t(len(file_name) > 26 and\n\t\t '_'.join(file_name[0:5]) in [cup_constants.DEFAULT_FILE_SUFFIX] +\n\t\t cup_constants.DEFAULT_SUB_SUFFIX))\n\n\ndef is_validIp(ip_address = None):\n\t\"\"\"\n\tCheck if IP is valid\n\t:param ip_address: IP address\n\t:type ip_address: str\n\t:return:"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tpass\n\ndef is_file_path(path):\n\t\"\"\"\n\tCheck if path is a file path.\n\t:param path: Path to be checked.\n\t:type path: str\n\t:return: Return true (exist), false (do not exist or directory)\n\t:rtype bool\n\t\"\"\"\n\treturn False\n\ndef is_readable_file(file_name):\n\t\"\"\"\n\tCheck if file name is readable.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist),"}]}
{"task_id": "62b8b99de0d34b282c1811f8", "prompt": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tlog = importlib.import_module(r\"matplotlib.ticker\")._log # pylint: disable=import-outside-toplevel\n\tfor k, v in log.items():\n\t\tsetattr(cls, k, v)\n\tsetattr(cls, \"log\", log)\n\n\ndef _parse_args(args):\n\t\"\"\"\n\t\tParse all arguments\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tprog='matplotlib.ticker',\n\t\tdescription='Matplotlib'\n\t\t\t' - matplotlib.ticker'\n\t\t\t' - for displaying"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tif sys.version.startswith(\"2.7.\"):\n\t\treturn\n\telse:\n\t\tdel sys.stdout\n\t\tsys.stderr = StringIO()\n\t\tif \"TEST_RUNNING_IN_CI\" not in os.environ:\n\t\t\tif sys.version_info[0] == 2:\n\t\t\t\timport ctypes\n\t\t\t\tctypes.ResetWindowsConsole(ctypes.wintypes.HANDLE(sys.stdout))\n\t\t\t\tctypes.ResetWindowsConsole(ctypes.wintypes.HANDLE(sys.stderr))\n\t\telse:\n\t\t\timport _"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\twith log.LogContext():\n\t\tcls._log_message = None\n\n\ndef error(function_name, log_message=\"\", exc=SystemExit):\n\t\"\"\"\n\t\tFunction which will raise a RuntimeError with its message if the\n\t\tlogger is not initialized yet.\n\n\t\tParameters :\n\n\t\t\tfunction_name (string): name of the function as it will appear in\n\t\t\t\tthe log. It will be converted to str by\n\t\t\t\t:func:`str.translate` under certain circumstances\n\n\t\tReturns:\n\n\t\t\tTrue, if the"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\ttry:\n\t\tcls._get_class = cls._get_class = getattr(libmypy, cls._get_module())  # pylint: disable=g-statement-before-assignment\n\texcept (AttributeError, KeyError):\n\t\tif libmypy.__version__ < \"1.8\":\n\t\t\timport traceback\n\t\t\ttraceback.print_exc(file=sys.stdout)\n\t\telse:\n\t\t\timport traceback\n\t\t\ttraceback.print_exc(file=sys.stdout, exc_info=True)\n\t\tsys.stderr"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tcls._logger = logging.getLogger('scapy.runtime')\n\tif not hasattr(sys, \"ps1\"):\n\t\timport multiprocessing\n\t\tsys.ps1 = multiprocessing.Process().cpu_affinity()\n\t\tsys.ps2 = multiprocessing.Process().cpu_affinity()\n\t\tif sys.platform == 'darwin':\n\t\t\timport multiprocessing # https://github.com/scapyfoundation/scapy/issues/9247\n\t\t\tsys.ps3 = multiprocessing.Process().cpu_affinity()\n"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tpass\n\n# -----------------------------------------------------------------------------\ndef __getitem__(self, key):\n\t\"\"\"\n\t\tGet item\n\t\"\"\"\n\tpass\n# -----------------------------------------------------------------------------\n\n\ndef __setitem__(self, index, value):\n\t\"\"\"\n\t\tInsert item\n\t\"\"\"\n\t# Get the name\n\tname = None\n\ttry:\n\t\tif isinstance(index, list) or isinstance(index, tuple):\n\t\t\tname = str(index.pop(0))\n\t\telif isinstance(index, str):\n\t\t\tname = index\n\texcept:\n\t\tpass\n\t#"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tcls._reset()\n\n\ndef _check_valid_config(cls, config):\n\t\"\"\" Check that config is valid\n\t\t\"\"\"\n\tif not any((cls._check_config_option(x) for x in config)):\n\t\traise Exception('One or more of -o or --config options given invalid'\n\t\t\t' argument value: %s' % ', '.join(cls._check_config_option(x)\n\t\t\tfor x in config))\n\ndef _check_config_option(config_arg):\n\t\"\"\" Check valid configuration value\n\t\t"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tcls.loggerName = None\n\tcls.logger = None\n\tcls.debug = logger.DEBUG\n\t#cls.debugLogging = True\n\ndef debug(message, **kwArgs):\n\t\"\"\"\n\t\tMessage handling\n\t\t\"\"\"\n\t\tdef print_level(str1, str2):\n\t\t\tif logger.level==logger.DEBUG:\n\t\t\t\tprint(str1 + '\\n' + str2)\n\t\t\telse:\n\t\t\t\tprint(str2 + '\\n' + str1)\n\tclass MyLogger(logging.Logger):\n\t\t"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t_log_set_debug = cls._debug_set_debug\n\tcls._log_set_debug = 1\n\tcls._log_enable = 0\n\n\tclass _Logging(object):\n\t\t__slots__ = ['debug_on']\n\t\tdef set_debug_on(self):\n\t\t\tglobal _log_debug, _log_set_debug\n\t\t\tif _log_set_debug >= 1:\n\t\t\t\treturn\n\t\t\t# if debug_on == True:\n\t\t\t# \tfor log_set_name in ['debug', 'test"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t# Set up variables for logging, such as time, iteration, etc.\n\tlog_step = []\n\n\ndef _log_info(cls, msg, step, loss_type, global_step, **_):\n\t\"\"\"\n\t\tPrint info log for this class\n\t\t\"\"\"\n\tprint('INFO          {}    loss: {}    '.format(*global_step) +\n\t\t\t\t'                     {} '.format(cls.loss,"}]}
{"task_id": "62b8b59feb7e40a82d2d1291", "prompt": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn self.name\n\nclass WinePackageStub(object):\n \"\"\"\n  Abstract class for a Win32/Win64 package with stub base classes.\n\n  The user classes\n               WinePackageStub(object, object, object)\n  \n  can be cast to\n               WinePackageStub(object, object)\n\n  as well as to\n               WinePackageStub(object)\n "}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn \"BasePythonExtensionPy\"\n\n# -----------------------------------------------------------------------------\n# End of classes\n# -----------------------------------------------------------------------------\n\n\n# -----------------------------------------------------------------------------\n# Module implementation\n# -----------------------------------------------------------------------------\n\n\ndef findSourcePath(package, filename):\n    \"\"\"\n    Return the Python module that contains a given module,\n    if found somewhere in the directory, else None\n    \"\"\"\n\n    modpath = os.path.dirname(module.__file__)\n    filelist = dir(modpath)\n\n    search = os.path.join(filelist, filename)"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t_targetClass = self.__class__.__name__\n\tsuper(_targetClass, self).__class__.__name__\n\nclass CProxy(Target, metaclass=_targetClass):\n\t\"\"\"\n\tBase class for CProxy object\n\t\"\"\"\n\tdef __init__(self, proxy, useCache=True):\n\t\t\"\"\"\n\t\tproxy is CProxy object which represents the connection\n\t\tor an instance with a __call__ method.\n\t\tUseCache controls if we use ConnectionPool cache\n\t\t\"\"\"\n\t\tself._proxy = proxy\n\t\tself._useCache = use"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn __class__\n\ndef _parseClassAttributes(attrs):\n\tfrom pprint import pprint\n\tfrom pprint import pformat\n\tdef default(value):\n\t\tif value is None: return ''\n\t\treturn pprint(value)\n\tdef classAttributes(classname, attrs):\n\t\treturn ', '.join([\n\t\t\t_targetClassAttr(value, classname)\n\t\tfor (key, value) in attrs.items()\n\t\t])\n\tdef classAttributesWithConstructor(classname, attrs):\n\t\t\"\"\"\n\t\tReturn a string representation of"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn self._target_class_name_parts[1]\n\ndef get_class(name):\n\t\"\"\"\n\tFactory method to return a subclass of a class with no arguments or\n\treturn None. In Python 2 it returns the class in the form 'class' or 'Class'.\n\t\"\"\"\n\tif isinstance(name, str):\n\t\t# Return the class name portion of a string.\n\t\treturn split(name)[1]\n\telif isinstance(name, unicode):\n\t\treturn name\n\telif isinstance(name, tuple):\n\t\t# Return the name in"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn cPickle.PicklerDefault\n\ndef _getPluginURL(domainList=None, *args, **kwargs):\n\timport py_locale\n\tfor d in domainList:\n\t\tif (\n#\t\t(d.endswith('py') or\n#\t\t(d.endswith('lib')) or\n#\t\t(d.endswith('plugins')) or\n#\t\t(d.endswith('.dll'))):\n\t\t):\n\t\t\tif args:\n\t\t\t\turl = '%s%s%s' %(\n\t\t\t\t\t_getPluginURL(domainList="}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\tret = None\n\n\tfor k in dir(cls):\n\t\ttry:\n\t\t\tv = getattr(cls, k)\n\t\t\tret = v\n\t\t\tbreak\n\t\texcept AttributeError: # Skip those that were not defined\n\t\t\tcontinue\n\t\telse:\n\t\t\ttry:\n\t\t\t\tv = getattr(ret, k)\n\t\t\t\tret = v\n\t\t\t\tbreak\n\t\t\texcept AttributeError:\n\t\t\t\ttry:\n\t\t\t\t\tv = getattr(cls.get_type(), k)\n\t\t\t\t\tret = v\n\t\t\t\texcept TypeError:\n\t\t\t\t\t"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn \"Py\"\n\n\nclass TargetList(list):\n\t\"\"\"\n\tA basic list-like implementation for Python 2\n\nclass TargetList(object):\n\t\"\"\"\n\tA basic list-like implementation for Python >= 2.\n\n\tTo add additional functionality to this class,\n\t1. Define attributes as static method\n\tvariables on Py2Compat.TargetList.\n\t2. Define the class variable __all__.\n\n\t\"\"\"\n\t@classmethod\n\tdef _getTargetClass(cls):\n\t\treturn \"Py2Compat.TargetList\"\n\n# Licensed to Elasticsearch"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn types.ModuleType\n\nclass Py_InitClass(Py_InitClass):\n\t\n\t'''\n\tImplementors of types:\n\t\t- PyInitObject(),\n\t\t- PyInitObjectEx(),\n\t\t- PyClass_FromSpec(),\n\t\t- PyInitFunction(),\n\t\t- PyInitModule(),\n\t\t- PyCode_New(),\n\t\t- PyCode_FromBytecode(),\n\t\t- PyMethod_New(),\n\t\t- PyMethod_NewEx(),\n\t\t- PyClass_GetSlot(),\n\t\t- PyObject_GetAttrString"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn cbook.deprecated('Please use get_plugin instead')\n\ndef _get_plugin_info(plugin_name, target_obj, use_pyrex):\n\t\"\"\"\n\t\tThis method provides the implementation for the case\n\t\tthat the plugin has Python2.4.3 or higher and that\n\t\tthe plugin being used is not the result of a\n\t\tget_plugin invocation.  The only information needed\n\t\tto determine 'compatibility' is a tuple, and\n\t\treturn types of the appropriate functions.\n\t\"\"\"\n\ttry:\n\t\tif hasattr"}]}
{"task_id": "62b8b590eb7e40a82d2d1275", "prompt": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tresult = [x for x in orderings if x!= \"\"]\n\tfor element in orderings:\n\t\tfoundLastOrder = False\n\t\tfor x in result:\n\t\t\tif x == element:\n\t\t\t\tfoundLastOrder = True\n\t\tif foundLastOrder:\n\t\t\tresult.extend(element + [x for x in orderings if x not in foundLastOrder])\n\t\telse:\n\t\t\tresult.append(element)\n\treturn result\n\t\n\t\ndef _getOrderingsSequence(orderingObj):\n\t\"\"\"\n\t\tReturns orderings separated by"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tif orderings:\n\t\treturn list(OrderedDict(list(orderings)))\n\telse:\n\t\treturn dict.fromkeys([x for x in orderings])\n\ndef mergeOrderings(o1, o2):\n\t\"\"\"\n\t\tDo a hierarchical merge (no dissimilar characters, and no \n\t\tcomparation of parent and grand-parent data) of two specified \n\t\torderings, with respect to the other orderings.\n\t\t\n\t\tExample:\n\t\t\t>>> merged1 = mergeOrderings([[2,3,4], [5"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tif not orderings:\n\t\treturn []\n\tassert len(orderings) % 2 == 0\n\t\n\tnew_orderings = []\n\tfor order in orderings:\n\t\tif order < 0:\n\t\t\tindex_of_first_negative = -order\n\t\telse:\n\t\t\tindex_of_first_positive = order\n\t\t\n\t\tfor i in range(len(orderings) - 1):\n\t\t\tfor j in range(i + 1, len(orderings)):\n\t\t\t\tif orderings[i] == orderings[j]:\n\t\t\t\t"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tsorted_orderings = Orderings()\n\tfor i, order_list in enumerate(orderings):\n\t\tif len(order_list) > 0:\n\t\t\tsorted_orderings.add_ordering(*order_list)\n\treturn sorted_orderings\n\ndef _reorder_ordered_subitems(obj, subitems, original_key):\n\t\"\"\"\n\tReorders subitem tuples by using a merge-sort over the\n\tlist of subitem tuples.\n\t\n\tThis order is not strictly necessary, but\n\tjust to make the same list of subitems a one"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tordered: List[Tuple[int, str]] = []\n\tordered_orderings = set()\n\t\n\tfor order_obj in orderings:\n\t\torder_str = order_obj.getOrderingsString()\n\t\t\n\t\tsorted_orderings = []\n\t\tif order_str is None:\n\t\t\tcontinue\n\t\t\n#\t\t\tdef reverse_ordering(order):\n#\t\t\t\"\"\"\n#\t\t\tConverts a sorting order dict into its reversed version.\n#\t\t\t\"\"\"\n#\t\t\treverse_index, reverse_str, reverse_reversed, reversed"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t# print \"Merging multiple orderings %s\" % orderings\n\tif len(orderings) == 0:\n\t\treturn orderings\n\telse:\n\t\torderings = sorted(orderings, reverse=True)\n\n\t\t# If we have multiple merge orders on 'a' then we can say that\n\t\t# a was generated as follows:\n\t\t#\n\t\t# a, b\n\t\t#\n\t\t# But not as a, b, and c are the same object\n\n\t\t# We need to make a list of the object we are"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tnew_orderings = []\n\tif not orderings:\n\t\treturn new_orderings\n\t\t\n\t# merge in orderings\n\t_current = orderings[0]\n\tif str(c for c in orderings[0])!= str(c for c in orderings[1]):\n\t\tif orderings[0][0] == orderings[1][0]:\n\t\t\t_current = _legacy_mergeAll(new_orderings, orderings)\n\t\telse:\n\t\t\torderings = []\n\t\t\t_current = _mergeOrderings(new"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t# TODO: The following implementation could be faster if there's a binary search\n\tif orderings == []:\n\t\treturn []\n\tif len(orderings) == 1 or orderings[0] == []:\n\t\treturn list(orderings)\n\n\t# Make sure that all the orderings are strings, because\n\t# we might be concatenating a string with an empty list\n\t# in that case and not a string with one of the\n\t# constraints\n\tfor order, values in orderings:\n\t\t# first, convert the strings into integers\n\t\tvalues"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t# TODO: this might be optimized to maintain a 2-way merge\n\tfor order in orderings:\n\t\tyield ['x', 'y', 'z', order]\n\t\t\n\t\t#...\n\t\n\t# The following is just for clarity, but would otherwise result in\n\t# extra output in the resulting config, so not even then.\n#\tfor order in order:\n#\t\tyield ['x', 'y', 'z', order]\n\t\ndef _mergeOrderings(orderings):\n\t\"\"\"\n\tReturn a new config object with the specified orderings\n\t"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\twhile True:\n\t\tmerged = []\n\t\tfor i, orderings in enumerate(orderings):\n\t\t\tmergedOrder = []\n\t\t\t\n\t\t\tfor order in reversed(orderings):\n\t\t\t\tif len(order) < 2:\n\t\t\t\t\tcontinue\n\t\t\t\t\n\t\t\t\tsuffix = order[-1]\n\t\t\t\tif suffix in ['q', 'z']:\n\t\t\t\t\tcontinue\n\t\t\t\tif suffix[0]!= 'x':\n\t\t\t\t\tbreak\n\t\t\t\n\t\t\t\tsuffix = suffix[1:]\n\t\t\t\tx = order[0]\n\t\t\t\tif x in ['q', '"}]}
{"task_id": "62b8b58deb7e40a82d2d1269", "prompt": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tself = object\n\treturn self.getDeclarions()\n\ndef _implements(interface):\n\treturn interface.hasAnnotation(\"com.python.util.interfaces.IProvides\")\n\nclass _BaseObject:\n    \"\"\"\n\t\tI am the core object class to support `~zope.interface.object.IAccessibleObject`.\n\t\"\"\"\n\tdef __init__(self):\n\t\tpass\n\n\tdef __str__(self):\n\t\treturn \"<\"%self.interface.getQualifiedTypeName()\n\nclass _InstanceObject(_BaseObject, _IInstanceObject"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn ( object )\n\nclass Declaration( object ):\n\t\"\"\"\n\t\tAn object representing a declaration.\n\t\n\t\t`Declaration` includes the following attributes in addition:\n\t\t\t\n\t\t\t- **name**: name of the declaration\n\t\t\t- **interfaces**: interfaces of the declaration\n\t\t\t- **abstracts**: abstract syntaxes of the declaration\n\t\t\t- **parameters**: arguments of the declaration\n\t\t\t- **returns**: returns returned type of the declaration\n\t\t\t- **scope**: scope of the declaration\n\t\"\"\"\n\tdef __init__( self, name, interfaces=None, abstracts"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tif not hasattr(object, 'implementedBy'): return\n\treturn [i.implementedBy for i in object.implementedBy]\n\n\ndef implementedBy(object): # pylint:disable=redefined-builtin\n\n\t\"\"\"\n\tReturn the interfaces implemented by the given object\n\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tif not hasattr(object, 'implementedBy'): return\n\treturn object.implementedBy()\n\n\ndef implementedByItemSetClass(itemSet): # pylint:disable=redefined-builtin\n"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn <IDeclaration>(\n\t\tisinstance(object, interfaces.IHasModule)\n\t\tor (not hasattr(object, \"includefiles\"))\n\t\tor (not hasattr(object, \"directlyProvidedBy\")))\n\n\ndef __await__(self, result): # pylint:disable=unused-argument\n\t\"\"\"For @property annotations; return `@property`. Consequently, the awaitables \n\twill automatically be `@property` in the first place. For annotation types \n\tthe awaitables are called with the property type as only argument and \n\tnot the"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn self().value.interfaces(object)\n\n#-----------------------------------------------------------------------------\n# Global interfaces\n#-----------------------------------------------------------------------------\n\ndef directlyUsedBy(object):\n\t\"\"\"\n\t\tReturn the interfaces indirectly imported by the given object.\n\t\t@param object: the object\n\t\t@type object: class\n\t\"\"\"\n\treturn [i for i in dir(object) if issubclass(i, interfaces.IDeclaration)]\n\n#-----------------------------------------------------------------------------\n# Interface\n#-----------------------------------------------------------------------------\n\ndef directlyInheritedBy(object):\n\t\"\"\"\n\t\tReturn the interfaces inherited by the given object."}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tval = object\n\t\n\tclass InterfaceDeclaration(object):\n\t\t\"\"\"\n\t\tThis class is used to represent the interfaces declared by the object.\n\t\t\"\"\"\n\t\tdef __init__(self, value):\n\t\t\tself.value = value\n\t\t\n\t\t@property\n\t\tdef isStatic(self):\n\t\t\treturn self.value.isStatic\n\t\t\n\t\t@static.getter\n\t\tdef isStatic(self):\n\t\t\treturn self.value.isStatic\n\t\t\n\t\t@property\n\t\tdef id(self):\n\t\t\treturn self.value.id"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tstatic = interfaces.IDeclarationProvider()\n\tstatic().fromDictionary(object)\n\treturn static()\n\n\ndef implementedBy(object): # pylint:disable=redefined-builtin\n\tif isinstance(object, types.TypeInstance):\n\t\treturn object\n\tif isinstance(object, types.Instance):\n\t\treturn interface.implementsType(object)\n\tif isinstance(object, types.FunctionType):\n\t\treturn interface.implementsType(object)\n\treturn object\n\n\ndef getInterfaceName(typeClass, object, includeInternal=False):\n\tif isinstance"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tpass\n\ndef getInheritedConstructors(self, context, name, descriptor):\n\tresult = []\n\tfor ctor in getattr(self, '__getinitargs__', ()):\n\t\tif cls := get(ctor)\n\t\t\tassert cls is not None\n\t\t\tresult.append((ctor, cls))\n\treturn result\n\nclass IDerivedFromDescriptor(IDeclaration):\n\n\tdef __repr__(self):\n\t\treturn \"{}.fromProxyType({})\".format(\n\t\t\tself.__class__.__name__, self.__class__.__name__)\n\n\tdef __add__(self"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn fromNative(object).directlyProvidedBy()\n\n\ndef provides(otherObject, interfaceImplementation, classImplementation):\n\t\"\"\"\n\tHelper for `zope.interface.interfaces.IProvidesStatement`,\n\t`zope.interface.interfaces.IProvidesStatement.provides()`\n\n\t`implements` is returned if the other object implements the interface\n\tif the given object implements the interface.\n\n\t>>> from zope.interface.interfaces import IProvidesStatement\n\t>>> provides(IProvidesStatement)\n\t\"\"\"\n\tprovided = toNative(classImplementation)."}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tif type(object) == bool:\n\t\treturn object\n\treturn interfacesForNode(object)\n\n#-----------------------------------------------------------------------------\ndef getInterfacesFromNode(node, interface=interfacesForNode):\n\t\"\"\" Returns a map of each interface to its name.\n\t\n\tThe keys of this Map can be any type or builtin object.\n\t\"\"\"\n\treturn interface.get(node, {})\n\n#-----------------------------------------------------------------------------\ndef getInterfacesFromFile(filename, interface=interfacesForFile):\n\t\"\"\" Returns a map of each InterfaceFile to name.\n\t\n\tUse with care. If a file doesn't"}]}
{"task_id": "62b8b559eb7e40a82d2d11f8", "prompt": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tbase_list = [x for x in classes[1::-1]]\n\treturn [x for x in base_list if x not in classes[1::-1]]\n\n\ndef filterDuplicates(names, sorted_unique_items, min_base_size=BASE_SIZE_THRESHOLD):\n\t\"\"\"\n\t\tFor each class, filter out duplicate items. If an item is\n\t\t\tduplicated in only once, this list is stored: sorted_item_set\n\t\twhere\n\t\t\tsorted_item_set = [item1, item2,..., itemn]\n"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tmin_count = len(set(classes))\n\tclasses = sorted(classes, key=lambda cls: cls.__class__.__name__)\n\treturn [cls for cls in classes if cls.__bases__[:min_count-1] ]\n\ndef getSubClass(class_or_object, subclass):\n\tif isinstance(class_or_object, (type(None), class)):\n\t\treturn class_or_object\n\telse:\n\t\treturn type(class_or_object)(getSubClass, class_or_object)\n\ndef findSubClasses("}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\treturn [a for a in classes if len(set(a)) < len(set(bases)) if a in bases]\n\n/**\n * Copyright (c) 2015-present, Facebook, Inc.\n * All rights reserved.\n *\n * This source code is licensed under the BSD-style license found in the\n * https://raw.github.com/facebook/regenerator/master/LICENSE file. An addon\n * of this project can not be distributed.\n */\n'use strict';\n\nrequire('colors');\n\nconst fs = require('fs');\nconst path = require"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tr = 0\n\tmin = classes[0]\n\tfor i in range(1, len(classes)):\n\t\tif classes[i] < min:\n\t\t\tr += 1\n\t\t\tmin = classes[i]\n\treturn r, min\n\ndef minimalReversibleClasses(classes):\n\tr, bestMin = 0, math.inf\n\t# first, check that no reversible classes are too long\n\treversedClasses = {'A', 'C', 'G', 'T', 'N', 'R', 'C', 'P', 'R',\n\t\t\t\t\t\t"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tfor base in classes:\n\t\tif(base=='object'): \n\t\t\tcontinue\n\t\tif(base in list(minimalBases(classes)):\n\t\t\tret.append(base)\n\t\t\n\treturn ret\n\n\nclass Graph:\n\tgraph = None\n\tvisited = set()\n\t\n\tdef __init__(self):\n\t\tself.graph = []\n\t\n\tdef __contains__(self, item):\n\t\treturn item in self.graph\n\t\t\n\tdef __iter__(self):\n\t\treturn self.graph[:]\n\t\n\tdef __len__(self):\n\t"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tresults=[]\n\tfor c in classes:\n\t\tresults.append(c)\n\t\tif min(results)!=c:\n\t\t\tresults.append(min(results))\n\treturn results\n\ndef smallest(classes1, classes2):\n\treturn min(min(min(classes1),classes2), classes1)\n\ndef largest(classes1, classes2):\n\treturn max(max(max(classes1),classes2), classes1)\n\ndef maximumOverlap(from1, to1, from2, to2, from3, to3, from4,"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\treturn [b for b in classes if len(b) > 0]\n\ndef classToNumber(class_str):\n\tclass_str = class_str.strip()\n\tif class_str in ['a','l','m','u']:\n\t\tclass_str = 'L'\n\tresult = eval(class_str)\n\treturn result\n\nif __name__ == '__main__':\n\tfor i in range(2):\n\t\t# Get a random class name\n\t\tname = 'a' * (random.randint(1,9))\n\t\tname"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t# classes = [str(x) for x in classes]\n\t# classes = set(classes)\n\tclasses = sorted(classes)\n\tf = []\n\tfor c in classes:\n\t\tmin = -1\n\t\tfor i, x in enumerate(c[:-1]):\n\t\t\tif i + 1 in classes:\n\t\t\t\tif x == classes[i + 1]:\n\t\t\t\t\tmin = i\n\t\t\t\telse:\n\t\t\t\t\tf.append(f[min])\n\t\t\t\tmin += 1\n\t\tf.append(c[-1])\n\treturn f"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tmin = 1\n\tresult = []\n\tfor k in classList(classes):\n\t\tresult.append(min) # list of classes to return as minimal subset\n\t\tif sorted(k) in classes: # classes of the corresponding subset are included in classes\n\t\t\tif len(result) > 0: # if subset is smaller than any result\n\t\t\t\treturn sorted(result) # all results to be returned\n\t\tmin += 1\n\treturn sorted(classes) # at least one subset remains\n\n\ndef minimalBases2(classes):\n\t\"\"\"\n\t\tConvert a list"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tresult = []\n\t\n\tfor c in classes:\n\t\tresult.extend( [c] * self.minimalBases[c] )\n\t\n\treturn tuple(result)\n\ndef getBaseDict(bases):\n\tif not bases:\n\t\treturn dict()\n\t\n\tdict = dict()\n\tfor base in bases:\n\t\tdict[ base ] = 1\n\treturn dict\n\ndef getMinimumEquivalent(classes):\n\t\"\"\"\n\t\tSearch a list of classes for a minimum equivalent class number\n\t\tand return the list\n\t\"\"\" \t\n\tdict = getBase"}]}
{"task_id": "62b8b4b9eb7e40a82d2d1134", "prompt": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tfrom.attribute import Attribute\n\tif __debug__: assert (getattr(Attribute,\"namesAndDescriptions\",None) is not None), \"cannot get attribute namesAndDescriptions of an instance of attribute\"\n\treturn getattr(Attribute,\"namesAndDescriptions\",None)()\n#@+node:ekr.201706121645.1: *3* abstract\nclass AttrDict(dict):\n\n\tdef __getattr__(self, key):\n\t\t\"\"\"\n\t\tThis makes sure to override inherited function.\n\t\t\"\"\"\n\t\treturn dict().__getitem__(key)\n\tdef __setattr__(self,"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\treturn [_.name for _ in self._enum_values]\n\ndef _get_value(self, attr):\n\t# pylint:disable=redefined-builtin\n\t\"\"\"\n\t\tGet single attribute value.\n\n\t\t@param self       [type]\n\t\t@param attr       [type] name of attribute\n\t\t@returns         [type] [description] \n\t\"\"\"\n\traise AttributeError(\"Cannot call value() without attributes\")\n\ndef _get_type(self, attr):\n"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\treturn self._attrNamesAndDescriptions\n\n\nclass _Get_Attr_Names_And_Descriptions(_Get_Attr_Fn, _Get_Attr_Names_And_Descriptions_Interface):\n\t\"\"\"\n\t\tMixin for Get/Set/Delete method implementations that override\n\t\twith additional implementation specific attributes.\n\t\"\"\"\n\n\tdef __call__(self, obj):\n\t\tattributes = super(_Get_Attr_Names_And_Descriptions, self).__call__(obj)\n\n\t\t# Find an attribute with the specified name:\n\t\tfor attrName, attr in attributes:"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\treturn [attr for (attr, value, description) in iter(lambda:getattr(self, attr))\n\t\t\tif value not in (None, '')]\n\ndef propertyNames(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names defined by the properties.\n\tReturn None if none of them are set.\n\tThe list will be [attr1, attr2,....] in the output dictionary.\n\n\tAll other dictionary attributes are sorted in a breadth first manner.\n\n\tAll properties in the output dictionary are converted to lower"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from inspect import getmembers, signature\n\tpass#IMPORTIMPORTIMPORT from functools import singledispatch, reduce\n\tpass#IMPORTIMPORTIMPORT from itertools import chain, groupby\n\n\tif hasattr(inspect, 'FunctionDef'):\n\t\treturn inspect.getmembers(getattr(inspect, 'FunctionDef'),\n\t\t\t\t\t\t\t\t\tlambda x:x.__name__) if all else None\n\n\t# Fallback to inspect, which returns object attributes (function-style attributes)\n\treturn singledispatch(getattr(inspect, 'inspect')).getmembers("}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t__slots__ = [\n\t\t'self',\n\t]\n\t\n\tdef __init__(self, all=False):\n\t\tassert all or not hasattr(self,'self')\n\t\tself.self = 0\n\t\n\tdef __str__(self):\n\t\treturn '{}.{:d}'.format(self.__class__.__name__, self.self)\n\nclass AttributeRef(object):\n\t\"\"\"A reference to an attribute.\n\t\"\"\"\n\tdef __init__(self, typeRef, all=False):\n\t\tassert all or not hasattr(typeRef,'typeRef"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\treturn [attr['name'] for attr in self.__dict__['attributes']]\n\n\ndef attributesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn dictionary of attribute names and descriptions defined by interface.\n\t\"\"\"\n\tret = dict()\n\tfor attr in self.__dict__['attributes']:\n\t\tret[attr['name']] = self.__dict__['attributes'][attr]['description']\n\treturn ret\n\n\ndef descriptionForAttributeName(self, name): # pylint:disable=redefined-builtin\n\t\"Check if attribute named"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\treturn [e[1][0] for e in self.objectList]\n\n\n###################################################################################\n#                        Interface with the class hierarchy                   #\n###################################################################################\n\nclass classWithHierarchy(object):\n\t\"\"\"\n\t\tAllows for use by classes as a way to add/remove instances. The\n\t\thierarchy is stored in the attribute 'hierarchy'.\n\t\t<br />\n\t\t\tHierarchy"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\treturn self.get('namesAndDescriptions', [x['name'] for x in self.get('attributes', [])])\n#\n# def getNamesAndDescriptions(self):\n#     \"\"\"\n#     Return attribute names and descriptions defined by interface.\n#     \"\"\"\n#     names = set()\n#     for entry in self.get('attributes', []):\n#         names.add(entry.get('name', entry['name']))\n#     return names, set("}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tif all:\t\treturn self.__allAttributes\n\telse:\n\t\treturn {n[0]\t: n[1] for n in self.__allAttributes}\n\n#------------------------------------------------------------------------\n#\n# OOP\n#\n#------------------------------------------------------------------------\n\nclass OOP(Interface):\n\t\"\"\"\n\tOOP implements an abstraction to perform operations on an Interface object. One can \n\timplement operations by specifying method calls (such as Get).\n\t\"\"\"\n\t__dict__ = Interface.__dict__\n\n\tdef __init__(self, name, iface=None):\n\t\tself.__"}]}
{"task_id": "62b8b416eb7e40a82d2d1129", "prompt": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tresult = []\n\tfor key in self.__dict__:\n\t\ttry:\n\t\t\tvalue = getattr(self, key)\n\t\texcept AttributeError:\n\t\t\tvalue = None\n\t\tresult.append(value if isinstance(value, basestring) else key)\n\tif all:\n\t\treturn result\n\telse:\n\t\treturn result + self.__dict__.keys()\n\n\n# Set of all attributes defined by a type.\n__all__ = []\n\n// Copyright (C) 2013  Davis E. King (davis@dlib.net)\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn self.attrNames\n\n\ndef attributesOf(self, object): # pylint:disable=too-many-branches\n\tname = type(object)._controlflowType.name\n\tif name:\n\t\treturn str.format(', '.join(name.split(' ')[1:]))\n\treturn ()\n\n\ndef attributeList(self, object): # pylint:disable=too-many-branches\n\tobject = type(object)\n\tnames = [(name, getattr(object, name, '')) for name in object.__class__.__dict__ if name.startswith('"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t# Check\n\tself._set_interface_attribute()\n\tif all:\n\t\treturn self._attribute_names(interface=True)\n\treturn self._attribute_names(all=True)\n\n#-------------------------------------------------------------------------\n# Private Functions\n#-------------------------------------------------------------------------\n\n\n#-------------------------------------------------------------------------\n# Code\n#-------------------------------------------------------------------------\n\n\ndef _attribute_names(namelist=True):\n\t\"\"\"\n\t\tUtility function to return the attribute names defined by the interface.\n\t\"\"\"\n\tif namelist:\n\t\treturn _names(getattr(self, 'name'))\n\treturn"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tresult = []\n\treturn_list = []\n\n\tfor node in self.node_list():\n\t\tif node is None or isinstance(node, (type, object)):\n\t\t\tcontinue\n\n\t\tif all:\n\t\t\tresult_list = node.names()\n\n\t\t\tfor _name in result_list:\n\t\t\t\tif _name!= \"None\":\n\t\t\t\t\treturn_list.append(_name)\n\t\t\tbreak\n\n\t\tresult.insert(0, node.name() + \"\\n\")\n\n\treturn \"\".join(result)\n\n#"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn self.__names\n\n\nclass _FunctionInterface(inspect.ClassVisitor):\n\n\tdef __init__(self, ctx, module_name, function, sig, all=False):\n\t\tif all:\n\t\t\tself.found_by_module = False\n\t\t\tself.found_by_class = False\n\t\t\tif not module_name:\n\t\t\t\tmodule_name = ctx.get('__module__', None)\n\t\t\t\tif module_name:\n\t\t\t\t\tself.current_namespace = None\n\t\t\t\telse:\n\t\t\t\t\tself.current_namespace = []"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn self.class_attribute_names(self.get_attribute_names())\n\ndef __dir__():\n\treturn 'core.interface'\n\n/**************************************************************************/\n/*                                                                        */\n/*       Copyright (c) Microsoft Corporation. All rights"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn _names(self._attributes)\n\ndef attribute_list(self, all=False): # pylint:disable=redefined-builtin\n\t\"\"\"\n\t\tThis function allows getting,\n\t\tgetting, or even querying the list of defined attributes.\n\t\"\"\"\n\treturn [getattr(self, attr) for attr in self.names() if not is_attr(attr)]\n\ndef attribute_list_of_type(self, all=False, attr_type=None): # pylint: disable=redefined-builtin\n\t\"\"\"\n\t\tThis function allows getting,\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn self.getNames()\n\ndef interfaceNames(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn self.interfaceNames\n\ndef isAvailable(self, attr, args=1, kwargs = {}):\n\t\"\"\"\n\tCheck if the requested attribute is available for the given arguments\n\tof an optional call.\n\t\"\"\"\n\treturn self._is_attribute_available(True, attr, args, kwargs)\n\ndef isMandatory(self, attr, args=1, kwargs"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tpass\n\n# Local Variables:\n# tab-width:4\n# indent-tabs-mode:nil\n# End:\n# vim: set expandtab tabstop=4 shiftwidth=4:\n\n<?php\n/**\n * DO NOT EDIT THIS FILE!\n *\n * This file was automatically generated from external sources.\n *\n * Any manual change here will be lost the next time the SDK\n * is updated. You've been warned!\n */\n\nnamespace DTS\\eBaySDK\\Trading\\Types;\n\n/**\n *\n * @property \\DTS\\eB"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tif all: self.interfaces = [cInterface\n\t\tfor cInterface in self._interfaces\n\t\tif cInterface.name\n\t\tand not getattr(cInterface, 'name', None)]\n\telse: self.interfaces = []\n\treturn\n\t\ndef attributes(self):\n\t\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tif hasattr(self, 'attributes'):\n\t\treturn self.attributes\n\tfor v in self.interfaces:\n\t\tt = getattr(v, 'attributes', None)\n\t\tif t: return t"}]}
{"task_id": "62b8b3d6eb7e40a82d2d111c", "prompt": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tfrom. import pgen, pyxarg\n\tif(output is not None):\n\t\tassert(isinstance(output,pyxarg.Pyxarg))\n\tif(output is None):\n\t\toutput = pyxarg.Pyxarg()\n\t\n\t# Replace non-identifier arguments with Identifier\n\t# This may raise TypeError in Py3.2\n\tfor arg in sequence:\n\t\tif(isinstance(arg,(Identifier,pyxarg.Pyxarg,pyxarg.Literal,_Literal))):\n\t\t\tsequence.remove(arg)\n\t\telse:"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif not sequence:\n\t\treturn tuple()\n\targs = tuple()\n\tdefaults = []\n\tfor x in getmembers(sequence, _normalizespec):\n\t\tif x in defaults:\n\t\t\tcontinue\n\t\tdefaults += [(a,) for a in getallattributes(x)]\n\t\targs += x\n\tif defaults:\n\t\targs += tuple(defaults)\n\treturn args\n\t\nclass _normalize_interface(_normalizeargs):\n\t\"\"\"\n\tNormalize interface members in a class specification\n\t\"\"\"\n\tdef __init__(self, spec, *args):"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif output == None:\n\t\toutput = sys.stdout # This isn't needed for modules\n\telse:\n\t\toutput.write('normalizeinterface arguments ')\n\t\n\tfor i in sequence:\n\t\ti, = i\n\t\tif isinstance(i, Declarion):\n\t\t\tfor c in i.decls:\n\t\t\t\tc, = c\n\t\t\tif isinstance(c, tuple):\n\t\t\t\tfor t in c:\n\t\t\t\t\tif isinstance(t, tuple):\n\t\t\t\t\t\tt = tuple(t)\n\t\t\t\t\t\tif isinstance(t[-1], tuple"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t#\n\tif not sequence:\n\t\treturn \"\"\n\t\n\tall = [a.name for a in sequence]\n\tlist = []\n\t\n\t#\n\tif output is None:\n\t\toutput = sys.stdout\n\t#\n\tfor name in all:\n\t\tif not name.endswith(\"_spec\"):\n\t\t\tif name in output.write:\n\t\t\t\tif output.isatty():\n\t\t\t\t\tcontinue\n\t\telse:\n\t\t\tname = output.split(name, 1)[0]\n\t\t\n\t\tif name.startswith('_') or isinstance("}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tfor elt in sequence:\n\t\tif isinstance(elt, Declarators):\n\t\t\tfor subelt in elt.elements:\n\t\t\t\t_normalizeargs(subelt.arguments)\n\n\t\tif elt not in output:\n\t\t\toutput[elt] = tuple(elt.types)\n\n\t\tif isinstance(elt, Declarators):\n\t\t\tfor subelt in elt.elements:\n\t\t\t\t_normalizeargs(subelt.arguments)\n\n\treturn output\n\nclass FunctionPrototype:\n\t\"\"\"\n\tClass for wrapping function prototypes.\n\t\t\n\tThe function prototype"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\targlist = {}\n\t# Get declaration arguments\n\tfor sequence in sequence:\n\t\tif 'declar' not in sequence and hasattr(sequence, 'arglist'):\n\t\t\targlist.update(sequence.arglist)\n\t# Remove unused args from input\n\targlist = dict((a.arg,a) for a in arglist.values())\n\targlist = dict((a.arg,a) for a in arglist.values() if hasattr(a, 'arg'))\n\targlist = sorted(arglist.items(), key=_sortby('sortkey"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\toutput = []\n\tfor value in sequence:\n\t\tif isinstance(value, tuple):\n\t\t\tt_args = _normalizeargs(value, output)\n\t\telif isinstance(value, (list, tuple)) and isinstance(value[0], list):\n\t\t\tt_args = _normalizeargs(value[0], [x for x, in d in output])\n\t\telse:\n\t\t\tt_args = [value]\n\t\tif isinstance(value, tuple):\n\t\t\tt_args.append(value[-1])\n\t\tnormalize_declar("}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\toutput = []\n\t#if output is not None and isinstance(output,list): output = output.copy()\n\tfor seq in sequence:\n\t\tif isinstance(seq, DeclGroup) and seq not in _expandable:\n\t\t\tseq = sequence.copy()\n\t\t\t#if output is not None and isinstance(output,list): output += [seq]\n\t\t\toutput.append(seq)\n\t\telse:\n\t\t\tfor spec in seq:\n\t\t\t\tif isinstance(spec, tuple):\n\t\t\t\t\tassert False\n\t\t\t\telif isinstance(spec, (InterfaceType,"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t# Normalize arguments by removing duplicates\n\treturn list(reduce(lambda a, b: a | b, sequence, set()))\n\ndef normalize_arguments(body, output=None, filter=True):\n\t\"\"\"\n\t\tReturn a tuple of the form (args), possibly filtered.\n\t\n\t\tAll arguments are normalized.\n\t\n\t\tThe resulting argument tuple is stored in an argument slot. In the case of an\n\t\targument in the body passed as extra_argument, the body's argument slot contains all the\n\t\textra arguments to the function definition.\n\t\t\n\t\tArguments whose values"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tfor item in sequence:\n\t\n\t\tif isinstance(item, DeclarationExpression) or \\\n\t\t\t(isinstance(item, tuple) and item):\n\t\t\n\t\t\tif output is None:\n\t\t\t\tbreak\n\t\t\t\n\t\t\t# Replace interface specs with interfaces.\n\t\t\tfor param in item.parameters.values():\n\t\t\t\tparam.replace_interface(output.interfaces)\n\t\t\t\t\n\t\t\t\tif hasattr(param, 'default') and param.default == '':\n\t\t\t\t\tparam.default = output.interfaces[-1]\n\t\t\t\t\n\t\t\tif not hasattr(item, 'default'):\n\t\t\t"}]}
{"task_id": "62b8b3d5eb7e40a82d2d1110", "prompt": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\timport warnings\n\t\n\tif hasattr(sys, \"gettotalrefcount\"):\n\t\tfrom rpython.rlib.rarithmetic import estimate_float_round_to_zero\n\t\treturn sys.gettotalrefcount() > 10\n\telse:\n\t\twarnings.warn(\n\t\t\t\"The module RPython.rlib.c_optimizations is not suitable \"\n\t\t\t\"for a C optimizing system.\\n\"\n\t\t\t\"It is only available prior to C 5.6. In recent versions of RPython \"\n\t\t\t\"of Python, RPython will raise error."}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\treturn hasattr(sys, 'pypy_version_info') and sys.pypy_version_info.minor >= 7\n\ndef _c_version():\n\t\"\"\"\n\tReturn the C compiler, if available, otherwise an empty\n\tstring.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\tif sys.platform == \"win32\" and _c_optimizations_available():\n\t\t# C11, 6.1.0: This should return a tuple of 3 strings:\n\t\t# compiler, a string, and a string"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\tfrom. import c_optimizations\n\tif c_optimizations is None:\n\t\traise ImportError(__name__ + \".c_optimizations_available())\")\n\treturn c_optimizations\n\n_c_optimizations_available.signatures = [\n\t\t('supported', 'bool'),\n\t\t]\n\n_c_optimizations_available.exportDescriptions = \\\n[\t'supported',\n\t]\n\n_c_optimizations_available.classes = \\\n[\t\"supported C optimizations\",\n\t]\n\ndef _is_supported(self, module"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\timport optimizers\n\texcept ImportError:\n\t\treturn False\n\telse:\n\t\treturn optimizers.available()\n\n\ndef _c_version_string():\n\tver = _c_optimizations_available()\n\treturn '.'.join(str(x) for x in ver)\n\ndef _c_version():\n\tver = _c_optimizations_available()\n\treturn '.'.join(ver[0:3])\n\ndef _c_full_version_string():\n\tver = _c_optimizations_available()\n"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t# If Python module exists, use that, otherwise use the\n\t# dummy __import__() - should it raise an ImportError?\n\t\timport __import__('pkgconfig')\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n\ndef _check_c_option(args, option, required):\n\t\"\"\"\n\tConvert an argument list to a tuple suitable for passing\n\tto ``cmake``\n\t\n\t:param args: The arguments to convert.\n\t:param option: The option to use\n\t:param required: The string to return if the"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t    import pyxacc\n\texcept ImportError:\n\t    return False\n\telse:\n\t    return not pyxacc.is_available()\n\n\ndef _c_set_optimizations_available():\n\ttry:\n\t    import pyxacc\n\texcept ImportError:\n\t    return False\n\telse:\n\t    return False\n\n\ndef register_optimization(opt, pyxacc):\n\t\"\"\"\n\tRegister a new optimization.\n\tNote that this does not actually register optimizations (for instance, it will require"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\tname = 'numpy'\n\ttry:\n\t\treturn __import__(name)\n\tfinally:\n\t\t# It's possible that someone has already imported numpy, then\n\t\t# this function has returned False, in which case, do not\n\t\t# use optimization.\n\t\treturn False\n\n\ndef _optimization_enabled():\n\t\"\"\"\n\tReturn optimization on or off.\n\t\n\tNote that the optimization can be enabled on its own or for other reasons,\n\tor disabled using _set_optimization method. For example, if\n\tthe optimization is on, PyTorch"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\treturn (os.environ.get('CC', False) and\n\t\tnot _has_c_optimizations())\n\ndef c_opt_level():\n\t\"\"\"\n\tReturn the C optimization level: optimize, debug or release.\n\t\n\tThis gets used when the optimizations are requested but disabled.\n\t\n\tThe return value must be one of:\n\topt_level\t\tC_OPT_LEVEL_OPTIMIZE\n\topt_level\t\tC_OPT_LEVEL_RELEASE\n\topt_level\t\tC_OPT_LEVEL_DEBUG\n\topt_level\t\tC_"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\treturn not os.path.isfile(os.path.abspath(\"Optimizer.opt\"))\n\ndef _build_c_optimizations():\n\t\"\"\"\n\tBuild list of C optimizations, if available, else silently\n\traises ImportError.\n\t\"\"\"\n\tif not _c_optimizations_available():\n\t\t# We can't get optimizers; so use the default set\n\t\treturn ['opt']\n\treturn _c_optimizers()\n\nif os.name == 'nt':\n\timport _multiprocessing as mp\n\texec('import thread')\n\tmp"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\tpass\n# -- PLEASE READ docstrings ----------------------------------------------\n\nif _c_optimizations_available():\n\t\"\"\"Implement the C optimizations, returning the number.\"\"\"\n\tpass\n\n# -- C extensions -----------------------------------------------\n\n_c_extensions_available = True\ntry:\n\ttry:\n\t\tfrom ctypes.util import find_library\n\t\t_c_extensions_available = bool(find_library(\".dylib\"))\n\texcept Exception:\n\t\t_c_extensions_available = False\n\tif _c_extensions_available:\n\t\t_C_EXTENSIONS += ['.dll"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110f", "prompt": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tif not sys.warnoptions:\n\t\treturn False\n\t\n\tif PYPY and os.environ.get('PURE_PYTHON', 1):\n\t\treturn True\n\t\n#\tIf we're doing a non-debug build, then just use the C optimizations\n#\tto make sure the Python runtime is called (and only in debug builds)\nif debug:\n\t_should_attempt_for_debug_build = True\n\n################\n#   Compiler Options #\n################\n\n#   If not pure Python, then we force this to compile the tests\n#  "}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\ttry:\n\t\treturn bool(not os.environ.get(\"PURE_PYTHON\"))\n\texcept (AttributeError, KeyError): # not Python, we're not pure\n\t\treturn False\n\n\ndef _get_impl_mode():\n\t# type: () -> str\n\t\"\"\"\n\tReturn either the value of that environment variable or the\n\t``IS_USED_IN_C`` environment variable, which gives control if\n\t``--use-c`` is passed to this command. ``IS_USED_IN_C`` is False,\n\tno, while\n\t``IS"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tif isinstance(sys.version[0], str):\n\t\treturn \"0\"\n\treturn \"1\"\n\n\ndef _set_use_python_impl(use_python):\n\t\"\"\"\n\t\tSet the C API to use PYTHON as the implementation.\n\t\t\n\t\tIf *value* is an empty string, turn on use_python.\n\t\"\"\"\n\tif isinstance(use_python, str):\n\t\tif len(use_python) == 0:\n\t\t\tuse_python = _PYPY_OFF\n\t\telse:\n\t\t\t# PYTHON"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\treturn os.environ.get('PURE_PYTHON', \"0\") == \"1\"\n\n\ndef assert_no_optimizations():\n\t\"\"\"Assert that no optimizations are going on.\n\t\"\"\"\n\tif _should_attempt_c_optimizations():\n\t\tassert not C._optimizations_are_on\n\telse:\n\t\tassert C._optimizations_are_on\n\n\ndef _get_c_impl():\n\t\"\"\"\n\t\tReturn the appropriate C interface implemented by this interpreter.\n\t\"\"\"\n\tif not C._is_python:\n\t\t"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tpure_python = os.environ.get(\"PURE_PYTHON\")\n\tif pure_python:\n\t\ttry:\n\t\t\treturn int(pure_python)\n\t\texcept ValueError:\n\t\t\t# we had a pure Python, but not passing valid integers\n\t\t\tpass\n\tpure_py = os.environ.get(\"PURE_PY\")\n\tif pure_py:\n\t\ttry:\n\t\t\treturn int(pure_py)\n\t\texcept ValueError:\n\t\t\t# we had a pure Python, but not using pure Pypy (in which case\n\t\t"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tif _is_pytest_runnning():\n\t\treturn False\n\tos.environ.pop('PURE_PYTHON', None)\n\treturn getattr(_use_c_impl(),'should_run_c_optimizations', True)\n\ndef _optimizations_enabled():\n\t\"\"\"\n\t\tReturn whether optimizations can be enabled for the current platform.\n\t\t\n\t\tIf any platform is not supporting a C optimizer, this will return\n\t\t`False`. If `optimizations_enabled` is a list or a function, `optimizations_enabled`\n\t\twill be a list"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tif not hasattr(_use_c_impl, \"PurePython\"):\n\t\tpass\n\telif _use_c_impl.PurePython:\n\t\treturn True\n\t\n\t# If _use_c_impl is set to None, we don't expect to be on PyPy.\n\tif _use_c_impl.PurePython is None:\n\t\tif len(_get_c_impl_args()) > 0:\n\t\t\treturn True\n\t\n\treturn False\n\n\n# For some operations which return values\ndef _get_c_impl_args():\n\t#"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\treturn get_config_var('PY_PURE_PYTHON') == '1'\n\n\ndef _use_c_impl():\n\t\"\"\"\n\tReturn a string denoting whether we should try using PyPy's ``c``\n\timplementation.\n\t\n\tNote that ``c`` has only been tested a bit against every relevant\n\tPython version supported by PyPy.\n\t\"\"\"\n\t# See http://stackoverflow.com/questions/209756/determine-how-to-check-if-a-file-is-c-py3.\n\treturn sys.version_info"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tenv_var = \"PURE_PYTHON\"\n\t\n\treturn bool(environment.get(env_var, None) and \"ON\" in environment)\n\n#-----------------------------------------------------------------------\n#\n#                            PUBLIC API\n#\n#-----------------------------------------------------------------------\n\ndef _add_to_path(path, value):\n\t\"\"\" Add to the environment PATH the given string, to be returned.\n\t     The path given as a string will be returned as-is.\n\t\n\t  "}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\treturn envvar.get(\"PURE_PYTHON\") in (\"yes\", \"True\", \"1\")\n\n\ndef _use_c_impl():\n\t\"\"\" Get the list of possible implementations for C. \"\"\"\n\timpls = []\n\tif shutil.which(\"c\"):\n\t\timpls.append(\"ctypes\")\n\tif shutil.which(\"fortran77\"):\n\t\timpls.append(\"fortran77\")\n\tif shutil.which(\"ast\"):\n\t\timpls.append(\"ast\")\n\tif shutil.which(\"sympy\"):\n\t\timpls.append"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110e", "prompt": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\tpass\n\n\n_OPTIMIZATIONS_IGNORE_LIST = [\n\t\"c_numpy_warnings\",\n\t\"c_code_gen_no_import\",  # not yet supported\n\t\"c_numba_c_api\",\n\t\"c_warnings_unobserved\",\n\t\"c_exceptions_import\",\n\t\"c_cpp_object_inheritance\",\n\t\"c_cpp_name\",\n\t\"c_python_version\",\n\t\"c_python_implementation\",\n]\n\n_c_optimizations_ignored = _c_optimizations_"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn [f\"* {i}\" for i in range(10000)]\n\n\ndef _c_optimizations_required():\n\treturn [f\"* -O0 --no-warn-frame-pointer{pathlib.Path(__file__).absolute().parent.parent.parent / \"omp\", *(_c_optimizations_ignored()).copy() + [\"\"]]\n\n/*\n * Bibliothek - DockingFrames\n * Library built on Java/Swing, allows the user to \"drag and drop\"\n * panels containing any Swing-Component the developer likes to add."}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\tprint('This function is called the c_optimizations_ignored function')\n\n\ndef _validate_and_transform_params(n_sample_steps=1000, n_test_steps=1000,\n\tn_estimators=1000, learning_rate=3e-3, n_jobs=1,\n\tverbose=True):\n\t\"\"\"\n\tCreates a dictionary with 'n_batch' parameters where 'True' or 'False'\n\tmeans the 'correctly sampled' parameter.\n\n\t:param n_sample_steps: Number of steps to be sampled\n\t"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn True\n\n###############################################################################\n# Saving to files\n\n\ndef _write_saving_to_files(save_dir):\n\t\"\"\"\n\t\tSaves a list of paths to a folder for each step\n\t\tin the trajectory optimization.\n\n\t\tThe file will contain the following keys:\n\t\t     - `model_state_path`: an absolute path to a file in the `models` folder\n\t\t     - `model_weights_path`: an absolute path to a file in the  `models/` folder\n\t\t     - `"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\tpass\n\ndef _c_optimizations_required():\n\tmissing_optimizations = set()\n\t\n\t# Optimization level overrides\n\tfor _name in all_optimizations:\n\t\tvalue = get_optimization(_name)\n\t\tif isinstance(value, (\n\t\t\tstr,\n\t\t\tstr,\n\t\t\tlist,\n\t\t\tobject)) or isinstance(value, int):\n\t\t\tmissing_optimizations.add(\n\t\t\t\t_name)\n\t\n\tif missing_optimizations:\n\t\tlogging.error(_c_optimizations_missing_error(' '."}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn {\n\t\t'maximize_time_in_secs': 'The optimizer',\n\t\t'time_limit': 'Time limit',\n\t\t'batch_size': 'Run batch size',\n\t}\n\n# Optimization options\n_c_optimizations_required = {\n\t'maximize_time_in_secs': 'The required optimizer',\n\t'time_limit': 'Time limit',\n\t'batch_size': 'Run batch size',\n}\n\n# Additional options\n_c_additional_options = {\n\t'maximize_time_in_"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\td = Data()\n\treturn_values = []\n\n\tdef _cb_value_1(param):\n\t\tpass\n\n\t@optimizers_state(default='last')\n\tdef _cb_value_2(param):\n\t\treturn \"value 2\"\n\n\t@optimizers_state()\n\tdef _cb_value_3(param):\n\t\treturn (\"something else\",) * 3\n\n\tdef _cb_value(param):\n\t\treturn \"value\"\n\n\t@optimizers_state({ 'foo': True, 'bar':"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\tpass\n\n\ndef _c_c(obj):\n\treturn obj.p.c\n\n\ndef _c_f(obj):\n\treturn tuple(_c_c(obj[pos]) for pos in obj._arg.positions)\n\n\ndef _c_c_f_pos(obj):\n\treturn tuple(_c_c(obj[pos]) for pos in obj._arg.positions)\n\n\ndef _c_p(obj):\n\treturn obj._arg.p\n\n\ndef _c_f_p_pos(obj):\n\treturn tuple"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\tfrom..utils import docstring_strip\n\tfrom..utils import docstring_strip_blank\n\timport pdb\n\t\t\n\tif not pdb.get_var(lib_path = \"c_optimizations_ignored.pyx\",\n\t\t\t\t\t\tsym = \"lib_opt.so\"):\n\t\tif not lib_path.startswith(\"opt\"):\n\t\t\tprint(f\"No opt libraries detected in {lib_path}, skipping optimization\")\n\t\t\treturn\n\t\t\n\t\tlibrary_path_str = lib_path[14:].rstrip(os.sep)\n\t\tif not lib"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn False\n\ndef _c_optimizations_supported():\n\"\"\"\n\tThe opposite of `_c_optimizations_allowed`.\n\t\"\"\"\n\treturn True\n\ndef _c_optimizations_required():\n\"\"\"\n\tA valid C `Optimizer`; you can still perform optimizations if `can_run_code` returns `True`.\n\t\"\"\"\n\treturn _c_optimizations_supported() and _c_optimizations_allowed()\n\ndef _c_optimizations_allowed():\n\"\"\"\n\tA valid C `Optimizer`.\n\t\"\"\"\n\treturn _settings_to"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110d", "prompt": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn PurePython()\n\n@cpython_api([PyObject, ])\ndef _py_optimizations_required(opt, optimize=True):\n\t\"\"\"\n\t\tThis is the Python implementation of C optimizer optimizations.\n\t\t\n\t\tWe need C optimizations for the pure Python implementation of PyOptimizer\n\t\tin order to apply the optimizations we want.\n\t\t\n\t\tThis is not part of the C API and is only part of the public interface. It\n\t\tis not part of the public C API and only part of the C API.\n\t\t\n\t\tSo we will define a"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\tif \"PYHERION_CC\":\n\t\t# if PYHERION_CC is None, don't use pure python\n\t\treturn True\n\tif not (\n\t\t\"C_SUPPORT\" in sys.modules or\n\t\t# sys.flags support module level option so not using pure python\n\t\t(getattr(sys,\"flags\",{})[\"feature_disable_without_pure\"] or\n\t\t # not using pure python module, like py27?\n\t\t not (getattr(sys,\"exec_prefix\",{}) in [sys.exec_prefix,sys.prefix]\n\t\t\t\t and sys"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn ( ( ( CLinker.cc_pure_python() and \n\t\t    CLinker.cxx_library_search_path() ) or\n\t\t    ( CLinker.cxx_library_path() == CLinker.link_path() ) ) )\n\ndef _c_impl_required():\n\t\"\"\"\n\t\tReturn a true value if the C implementation is required.\n\t\"\"\"\n\treturn False\n\ndef _c_optimizations_available():\n\t\"\"\"\n\t\tReturn a true value if all optimizations are available.\n\t\t\n\t\tThe only other cases are optimization"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn (\n\t\t_use_c_impl('PURE_PYTHON') and\n\t\tint(_os.environ.get('CPYTHON3')) <= int(_os.environ.get('CPYTHON2'))\n\t)\n\n\ndef _check_c_improvements_available():\n\t\"\"\"\n\tCheck the C improvements of C++ and pycon\n\t\n\tReturns a bool showing if any of the C optimizations are available.\n\t\n\t\"\"\"\n\timproves = ('CFLAGS', 'LDFLAGS', 'LDSHARED', 'LDFLAGS_DLD', 'LD"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t# NOTE: Python 2.6+ only requires pure python 2.7\n\tpython = imp.get_module(module)\n\treturn bool(sys.satisfies(str(python._use_c_impl)))\n\n\ndef _use_c_impl():\n\t\"\"\"\n\tDetermine this platform's C API.\n\n\tThis may change once Python is compiled with the -X:CFLAGS=-XFLAGS CFLAGS\n\tenvironment variable.\n\t\"\"\"\n\ttry:\n\t\timport _cpyext\n\t\t_cpyext.set_extension_funcs()\n\t"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\tif os.environ.get('pure_python') == '1':\n\t\treturn True\n\tfor name in __settings__.get_all_settings():\n\t\tif not name.startswith('_'):\n\t\t\tcontinue\n\t\tvalue = os.environ.get(name)\n\t\tif value:\n\t\t\treturn True\n\treturn False\n\ndef _use_python(name):\n\t\"\"\"\n\tReturn True if the given Python module has ``_cffi_enabled`` set.\n\t\n\tThe actual code should be contained within ``setup.py``.\n\t\"\"\"\n\tif"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn not (config.options.pure_python and _use_c_impl() and\n\t\tconfig.options.pyconfig._c_optimize or\n\t\tconfig.options.pure_python and\n\t\tconfig.options.compiler.extra_postargs or\n\t\tconfig.options.pure_python_extra)\n\n\ndef _c_optimizations_disallowed():\n\t\"\"\"\n\tSame as _c_optimizations_required\n\t\"\"\"\n\treturn _c_optimizations_required()\n\n\ndef _optimize_impl(func, *args):\n\t"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\tif os.name == 'nt' and hasattr(sys.stdout, \"isatty\") and sys.stdin.isatty():\n\t\treturn True\n\tif sys.platform in ('win32', 'cygwin'):\n\t\tif hasattr(msvcrt, 'MSVCrt_VERSION'):\n\t\t\tif MSVCrt_VERSION:\n\t\t\t\treturn True\n\t\treturn False\n\t# otherwise it's Windows/CPython compatible\n\treturn True\n\ndef _cpp11_supported():\n\ttry:\n\t\timport _cpp_intf\n\t\treturn _cpp_"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn os.environ.get('C_OPTIMIZATIONS_REQUIRED',False)\n\ndef _use_c_impl():\n\t\"\"\"\n\tReturn a true value if the C implementation is used.\n\n\tThis is used by the ``pyopencl2.c`` implementation to select\n\tthe right optimization code depending on the version of C being\n\tcompiled. See the documentation in\n\t:mod:`opy.util.libext` for more information over C.\n\n\tThis uses ``omp`` as documented in\n\t:mod:`opy.util.libext`.\n\n\t"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\tif _use_c_impl():\n\t\treturn True\n\telif _PYTHON_IMP:\n\t\treturn True\n\telif 'PYTHON_CC_ON' in os.environ:\n\t\tif 'PURE_PYTHON' in os.environ:\n\t\t\treturn True\n\treturn False\n\t\ndef _use_c_impl():\n\tif sys.version_info[:2] < (3, 2):\n\t\treturn _USE_PYGC_2 == \"0\"\n\treturn _USE_PYGC_0 == \"0\"\n\t\ndef compile_file"}]}
{"task_id": "62b87b989a0c4fa8b80b35ee", "prompt": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t# First collect the histogram, then reset it\n\t# and collect them again.\n\t#\n    self.bins[self.key] = []\n\tself._collect()\n\tself.bins = []\n\treturn None\n\t\n\t\ndef _make_bins(context):\n\t\"\"\"\n\tAdd a single histogram to an existing context.\n\t\n\tParameters\n\t----------\n\tcontext : dict\n\t\tContext to make a histogram for.\n\t\n\tReturns\n\t-------\n\thistogram : dict\n\t   The newly created histogram.\n\t\"\"\"\n\t\n\thist = dict"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tfor e in reversed(self.__ego_list):\n\t\tself[e] = ego.dict()\n\n#-------------------------------------------------------------------------\ndef make_bins(size):\n    \"\"\"\n\t\tCreate a binned array for the specified size.\n\t\tReturn a numpy array with all zero entries.\n\t\t\n\t\tParameters match a binning dimension, *size* is a value in range\n\t\tfrom 1 to the largest dimension of the array.\n\t\"\"\"\n\tbins = ego.dict(initargs=[size], entry=0)\n\tfor e in self:"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself.__histo__ = {}\n\n\nclass Context(object):\n\t\"\"\"\n\tThis is the context for the histogram.\n\tIt is a single value.\n\t\"\"\"\n\t__version__ = 0\n\n\n# ------------------------------------------------------------------------\n\n\n######################################################################################\n# * *\n\n\ndef create_histogram():\n\t\"\"\" Returns a histogram for current context. \"\"\"\n\tfrom.. import util as ut_util\n\tfrom..base import Context\n\t\t\n\tctx = Context.get_active_ctx()\n\n\th = ctx.make_bins()\n\n\th[0"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t# TODO : Check for any more options.\n\tself.bins.clear()\n\tself.start = self.n = self.n_bins = self.n_points = self.n_features = self.n_components = self.n_bins = 0\n\t\n\t\n\tself.data.clear()\n\tself.bins = None\n\tself.n_bins = 0 # Initialize for all data as well as for all components.\n\t\n\tself.first_bin = None\n\tself.last_bin = None\n\tself.first_c = None\n\tself."}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tif self.context == \"default\":\t\n\t\thist = default_histogram.copy()\n\t\thist.bin_width.reverse()\n\t\thist.bin_counts = self.bin_count.reverse()\n\t\tself.initialize(hist)\n\telse:\t\n#\t\th.update_context(self.context)\n\t\thist = hist_context.copy()\n\t\thist.bin_width = list(range(len(hist.bin_counts)))\n\t\tself.initialize(hist)\t\n\tif self.initial_value == None:\n"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t# remove all the default bins\n\tself.histogram[:, -1:] = 0\n\tself.bin_length = 0\n\t\n\tself.bin_data.clear()\n\tself.bin_count.clear()\n\t\n\tself.bin_data[0] = 0\n\tself.bin_count[0] = bin_count() -1\n\tself.rebin()\n\n\tself.bin_index = self.bin_index + self.bin_count[0]\n\tif  self.bin_index < len(self.bin_index_2):\n"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t#\n\tself._bins = self._construct_bins()\n\n\t#\n\tself.initial_value = self.initial_value if self.__original_reset__ else {}\n\tself.histogram = []\n\tself.is_initialized = False\n\tself.is_frozen = False\n\n\tself._current_bins_changed()\n\tself._current_bins_cleared()\n\tself.current_bin()\n\tself.reset()\n\n\n\n\n\n\n###############################################################################################\n##                "}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from copy import deepcopy\n\tself.n_hist = numpy.zeros(self.n_bins, dtype=float32)\n\tfor index in xrange(self.n_bins-1):\n\t\tself.n_hist[index] = self._scale_value(self.n_hist[index+1], \n\t\t\t\t\t\t\t\t\t\t\tself.n_bin_to_value)\n\t\n\tfor index, hist_elem in enumerate(self.t_min_vals):\n\t\tself.n_hist[index] /= self._scale_"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tfor name in self().keys(): # Keep in sorted order.\n\t\tself()[name] = {}\n\t\n\t# Fill the other bins with initial values.\n\tfor name, attr in self().items():\n\t\tself()[name] = attr\n\t\n\t\n\tself.update(clear=False)\n\t\n\tself.reassign(initial_value=self.values)\n\tself.initial_value = self.values\n\tself.initial_value_list = {}\n\t\n\tself.update(clear=False)\n\t\n\tif not self.values:\n\t\tself"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself.map = {}\n\tself.bins = self.make_bins()\n\n\tdef clear_bins(bin_dict):\n\t\t\"Re-initialize bins.\"\n\t\t# Clearing bins are no-ops for this dictionary;\n\t\t# bins become empty as a no-op operation (note that\n\t\t# we clear everything in reverse order).\n\t\twhile bin_dict:\n\t\t\t# Note we do nothing here, just make it empty.\n\t\t\tdel bin_dict[0]\n\t\t\t# Update the dict.\n\t\t\tbin_dict."}]}
{"task_id": "62b87b859a0c4fa8b80b35d7", "prompt": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\treturn ToStrings(self, sep=\"\\r\\n\", header=True)[0]\n\n\ndef to_csv_csv(self):\n\t\"\"\"\n\tConvert graph's points to CSV and CSV is returned.\n\t\n\tIt converts :class:`Graph` objects as lists: values become strings\n\tand :class:`~list` becomes a tuple: all values become strings.\n\t\"\"\"\n\theader = \"index\"\n\toutput = []\n\tfor points in self._points:\n\t\tif points is not None:\n\t\t\tif header:\n\t\t\t\toutput.append(header)"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tif not isinstance(self, ToCSV):\n\t\traise TypeError(\"to_csv is not a class\")\n\tif not header:\n\t\theader = [\"x y z\"]\n\tresult = self.__graph.to_csv(sep=separator, header=header)\n\treturn result\n\ndef to_csv_with_header(self, header):\n\t\"\"\"\n\tConvert graph's points to CSV.\n\t\n\t*header*, if not ``None`` is the first string of the output\n\t(new line is added automatically).\n\t\n\t*separator* delimits values, the"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tif inspect.getmodule(self).name == \"lena\":\n\t\traise RuntimeError(\"to_csv(graph) can only be used inside graphlib.directed_graph\")\n\n\toutput = \"\"\n\tfor c in self.list:\n\t\toutput = output + sep.join([c.coordinate.get(name=name) for name in names]) + \" \"\n\t\n\tif header is None:\n\t\toutput = output + separators.join([str(c.coordinate).ljust(len(sep),'') for c in self.list]) + \" \"\n"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport pydot as pydot\n\n\tdef mkstr(x):\n\t\treturn str(x)\n\tdef mkstr(x):\n\t\tassert len(x) > 1, x\n\t\treturn x[1:]\n\tdef mkstr(x):\n\t\treturn x.join(\",\")\n\t\n\tfrom pydot import *\n\tfrom pydot.graph.greedy import *\n\t\n\t\"\"\"\n\tConvert all graph points to CSV.\n\t\"\"\"\n\t\n\tgraph = [\n\t\t0, 0, 0, 0, 0, 0, 0, 0"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tif (self.dim==2) or (self.dim>2) :\n\t\n\t\tx,y = [\"\"] * self.dim\n\t\t\n\t\tfor i in range(self.dim):\n\t\t\tif self.points[i].dim:\n\t\t\t\tx.append(\"\"+str(i) + \" \"+self.points[i].to_csv(separator))\n\t\t\telif self.points[i].to_csv is not None:\n\t\t\t\tx.append((str(i) + \" \" + self.points[i].to_csv(separator)))"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tfrom.output import ToCSV\n\t\n\treturn ToCSV(self._list)\n\ndef to_html(self, header=None, separator=\",\", html5=True):\n\t\"\"\"\n\tConvert graph's points to HTML.\n\t\n\t*header* delimits the returned HTML.\n\t\t\n\t*separator* delimits values.\n\t\n\t.. deprecated:: 0.5 in Lena 0.5 to_html is not used.\n\t      Iterables are converted to tables.\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tself.data[...] = (\n\t\tsep.join(line.strip() for line in\n\t\t\t\tsplit_records(self.point_data, separator))\n\t\t\tfor idx, line in enumerate(self.point_data))\n\tif header is not None:\n\t\tself.header = [header for _idx, header in enumerate(header) if header!= None]\n\t\n\treturn self\n\n\n\n\n\ndef get_point_data(point_data, header = ''):\n\t\"\"\"Return a CSV-stringified data table, formatted according to *header"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\treturn to_csv(self, separator, header)\n\n# TODO: should handle multi-dimension tensors\ndef to_tensor(self, dtype):\n\t\"\"\"\n\t\tConvert graph's points to a tensor object in the same format as\n\t\t`points`, or write a line in a CSV file.\n\t\"\"\"\n\tassert self.n_dims == 2, (\"Only square graphs are supported. \"\n\t                            \"Only {}-dimensional graphs are currently supported. \"\n"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tif not self.points or self.name is None:\n\t\traise RuntimeError(\"graph to csv should at first time call to convert\")\n\t\n\tif separator is None:\n\t\tseparators = \", \"\n\telse:\n\t\tseparators = separator+\" \"\n\t\n\twith ToCSV(self.to_points(), header=header) as d:\n\t\tfor i, a in enumerate(self.to_points()):  # TODO: change the format\n\t\t\td.write(separators.join([i+1, i[0], a[0], a[1"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tfrom typing import List\n\t\n\tpt = []\n\tvals = zip(range(len(self)), self)\n\tfor i, j in enumerate(j for j in vals if j):\n\t\tif i:\n\t\t\tpt.append(' ')\n\t\tpt.append(j.to_csv(separator=separator))\n\telif header is not None:\n\t\tpt.append(header)\n\telse:\n\t\tpt.append('')\n\treturn pt\n\n\nclass Graph(object):\n\t\"\"\"\n\tAn immutable graph.\n\t\t\n\t.. deprecated:: 0"}]}
{"task_id": "62b87b839a0c4fa8b80b35cb", "prompt": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\ttry:\n\t\tcoord = self[coord_name]\n\texcept KeyError:\n\t\treturn []\n\treturn np.where(self._coords_index == self.coord_names)[0][0]\n\n\ndef _check_coordinates(self, coordinates):\n\t\"\"\"\n\t\tPerform type checks.\n\t\"\"\"\n\tif isinstance(coordinates, str):\n\t\tcoordinates = coordinates.lower()\n\tfor name in coordinates:\n\t\ttry:\n\t\t\treturn self[name]\n\t\texcept KeyError:\n\t\t\t#raise ValueError(\"Unkown type of coordinate"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t# check coord_mask for error types\n\tif coord_name in self.coord_mask.valid_types_mask:\n\t\t# get axis labels\n\t\taxis_labels = [axis.name for axis in self.data]\n\t\t# find axis in mask\n\t\tmask = np.zeros(len(coord_name), dtype='uint32')\n\t\tfound = False\n\t\tfor axis in axis_labels:\n\t\t\ttry:\n\t\t\t\tim.set_attr(axis_labels[axis], mask)\n\t\t\t\tfound = True\n\t\t\t\tbreak"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tself.coordmap = dict([(x, i) for i,x in enumerate(self.coordmap)])\n\treturn self.coord_idx(coord_name, err_indices=list(self.coordmap.keys()),\n\t\t\t\t\t\t   allow_dup_points=True)\n\"\"\"\n\n\n##################################################\n# Coordinate conversion functions\n##################################################\n\ndef _get_coord_names(self):\n\t\"\"\"\n\tHelper function. Converts a `~astropy.wcs.Plane` object into a list of\n\t`~astropy.wcs.Coordinate` objects\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\ttry:\n\t\te = self._error[coord_name]\n\texcept KeyError:\n\t\treturn []\n\texcept Exception:\n\t\treturn [None]\n\t# Indices of each error\n\treturn [i for i in range(len(e)) if i not in e] # Skip if no entry in error\n\n\ndef _get_error_info(coord):\n\t\"\"\"\n\tGet the data for an error coordinate.\n\t\"\"\"\n\te, _ = _get_err_indices(coord)\n\treturn _get_error_info_no_coord"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tc = self._coord_to_index[coord_name]\n\tif c:\n\t\terr = c._get_err()\n\t\treturn err\n\ttry:\n\t\treturn tuple(range(self._max_n_errors)\n\t\t\tif c in self._error_indices else\n\t\t\tmin(err for j, err in enumerate(\n\t\t\tlist(map(lambda x: x[c], self.data)))) + 1\n\t\t\tfor c in self.data)\n\texcept IndexError:\n\t\traise KeyError(coord_name)\n\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tif coord_name.lower() in list(self.errors[coord_name]):\n\t\treturn self.errors[coord_name][coord_name]\n\treturn self.errors[coord_name]\n\n\nclass CoordinateList(CoordList):\n\t\"\"\"\n\t\tInterface to Coordinate class\n\t\"\"\"\n\tdef _fill(self, coordinates, ignore_errors=True):\n\t\tsuper(CoordinateList, self)._fill(coordinates, ignore_errors)\n\t\tself.errors.clear()\n\t\tself._add_coord(coord_name, coordinates)\n\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tif isinstance(coord_name, six.string_types):\n\t\tcoord_name = (coord_name,)\n\tcoord_name = _check_coord_name_and_dimension(coord_name)\n\treturn self._error_indices[coord_name]\n\n\t\ndef _init_errors_if_needed(self, coord_space, coord_name):\n\tif self._has_error_indices(coord_name):\n\t\treturn None # Skip this coordinate if there are multiple errors.\n\tif coord_name in self._error_indices:\n\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\treturn self.coord_errs[coord_name]\n\ndef _get_err_index_from_coord(self, coord_name, coords, coords_map, error_index_map):\n\"\"\"\n\tGet error index corresponding to a coordinate.\n\t\"\"\"\n\tcoord_err_index = self.coord_err_indices.get(coord_name)\n\tif coord_err_index is None: return None\n\telse:\n\t\treturn coords_map[coords[coord_err_index]]\n\npackage org.apereo.cas.authentication.surrogate;"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tresult = [\"0\", \"1\", \"2\", \"3\"]\n\tfor (coord_i, (err, pos_indices)) in enumerate(result):\n\t\tif \"error_index\" in self.err_indices:\n\t\t\tif err in self.err_indices[\"error_index\"]:\n\t\t\t\treturn (pos_indices.copy(), self.err_indices[\"error_index\"][err])\n\treturn (None, None)\n\n\ndef _get_pos_err_indices(self, coordinate_dict):\n\t\"\"\"Get error indices corresponding to coordinates.\"\"\"\n\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\treturn self.coordinates['ERROR: %s' % coord_name]\n\ndef _get_err_coords(self, coord_name):\n\t\"\"\"\n\t\tReturn error coordinates for a given coordinate name.\n\t\"\"\"\n\treturn self.coordinates['ERROR %s' % coord_name]\n\ndef _warn_no_coords(self, msg='No coordinates defined'):\n\t\"\"\"\n\t\tPrint warning message to warn user about no coordinates\n\t\t\"\"\"\n\tprint('WARNING: Coordinate info not available: %s' % msg)\n\ndef write_file(outfile,"}]}
{"task_id": "62b87b7e9a0c4fa8b80b35bc", "prompt": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\te = self[\"E\"]\n\t# Only one set of properties available\n\tif \"error\" in context.fields:\n\t\t# Get the error index\n\t\te_idx = context.error.index(context.fields[\"error\"])\n\t\tprint(\"Error:\", repr(e.fields[\"E\"]))\n\t\t# If a name was provided we create the new context variable under its name\n\t\t# e.fields[\"E\"], e.fields[\"error_E_{}\".format(e_idx)], e_idx, None\n\t\tif e.fields.get(\"E"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tself._context = {\"x\": context.x,\n\t\t\t \"y\": context.y,\n\t\t\t \"z\": context.z,\n\t\t\t \"t\": context.t,\n\t\t\t \"error_E_low\": context.error_E_low,\n\t\t\t \"field_A_A_low\": context.value.field_A_A_low or None,\n\t\t\t \"field_a_value_A_z\": context.value.field_a_value_A_z}\n\t\n\tself._n_fields = self._context.keys()\n\t\n\tself._update"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif context.data is not None:\n\t\treturn\n\tcontext.data = self._new()\n\n\tdef _update_error(entry_name, error_name, error):\n\t\t'''\n\t\tA convenience method for updating specific fields in an error's\n\t\tsubcontext.\n\n\t\tEntry name is the name of entry-level data with field \"E\",\n\t\tand error name is the name of the error data-level field\n\t\twhich correspond to index returned by :func:`data`.\n\t\t\n\t\t'''\n\t\tcontext.data.error["}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext = copy(context)\n\tfor f in self._fields.values():\n\t\tif _is_dict(context) and len(context[f]) > 1:\n\t\t\tcontext[f] = context[f].copy()\n\t\tcontext[f].update(context[\"E\"])\n\tif not set(_get_all_properties(context)).issubset({\n\t\t\"error\", \"error\", \"name\", \"x_low\", \"y_low\", \"z_low\"}},\n\t\t{\n\t\t\ttuple(context[f]) for f in _get_"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif context.error:\n\t\tfor idx, errname in enumerate(context.error):\n\t\t\tcontext.value[idx] = context.value[idx] - context.error[idx]\n\t\tdel context.error[:] # undo conversion to an equivalent structure\n\t\n\telse:\n\t\tcontext.value[context.key] = context.value[context.key] - context.value[0]\n\t\tcontext.value[0] = context.value[0] - context.value[1] # undo conversion to an equivalent structure\n\t\n\tdel context # allow deleting"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t# TODO could not be done in __enter__ (b/c subcontexts are not\n\t# yet set correctly because there is a circular dependency)\n\t#\n\tif False: #__debug_context__:\n\t\tprint(\"\\nUPDATE CONTEXT DEBUG (this should not be done)\")\n\t\tprint(context)\t\t\t\t\t\t# TODO!\n\t#\n\t# update internal properties of the graph\n\t# (the other two properties are never set because\n\t# they are not part of an object graph they belong to)\n\tif False: \n\t\tself._error_x = {} "}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext = context.copy()\n\tfor name, val in self.fields[\"E,t,error_E_low\"].items():\n\t\tif val[2] is None: val[2] = len(context)\n\t\tcontext[\":\" + name] = val\n\t#if a field has the values stored in it,\n\t#the list would be converted to an array\n\t\n\tif self.has_field(\"y\"):\n\t\toutdated_y = context[\"y\"].copy()\n\t\toutdated_y.sort()\n\t\tcontext[\"y\"] +="}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tdef _update_context_key(key, data):\n\t\tself._error_names = data[\"error\"] + self._error_names\n\t\tfor v in data[\"context\"]:\n\t\t\tif v[0:2] == \"context\": continue\n\t\t\tself._context.add(v[2], v)\n\t\tself._context.set_attr_dict(data[\"context\"])\n\t\tfor k in self._context:\n\t\t\tif k[-2:] == \"context\": continue\n\t\t\tself._context.add(k[4], k)\n\t"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tfor prop, value in context:\n\t\tif prop in self.fields:\n\t\t\tself._remove(prop)\t# removes this property from *context\n\t\t\tcontinue\n\t\t# Note: error property _could_ be changed by some other method.\n\t\t#       \"prop\" already has the error value.\n\t\tself.field[prop] = self[prop]\t# \"prop\" refers to value at *self[prop]\n\t\tif self.error is not None:\n\t\t\tself.error[prop] = self.error["}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tself._destruction_context = True\n\tcontext.value = {}\n\tcontext.error.clear()\n\t\n\t# Get names of errors.\n\tcontext.error.names = self.field_names\n\tfor field in self.fields:\n\t\tif field not in context.error: context.error[field] = []\n\t\tfor name in self.field_names[field]:\n\t\t\tcontext.error[field].append(name)\n\t\n\t# Find names/formats of fields.\n\tfor name in self.field_names:\n\t\tfor format in"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3580", "prompt": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tfrom. import histogram\n\tfrom math import pi\n\tfrom.histogram import _calc_bounds\n\tbins = int(bins)\n\tedges = _calc_bounds(edges)\n\tbounds = (-bins, 0., bins+edges, bins)\n\treturn histogram(bounds, bins)\n\ndef integral_fit(values, bins):\n\t\"\"\"\n\t\tFit the integral (e.g., scale) of the input histogram.\n\t\n\t\t:type: float\n\t\t:arg values: values of the input histogram\n\t\t:type: float\n\t\t:arg bins"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tfrom fem.numerics import integrate\n\t\n\timport numpy as np\n\t\n\t# find a number of different integral functions for each bin\n\tintegrand = np.ones(bins * bins)\n\tfor i, j in [0, 1, 2]:\n\t\tx = j * bins * np.cos(j * np.arange(bins))\n\t\tintegrand[i] = x * edges\n\treturn integrand\n\n\ndef bin_edges(bins):\n\t\"\"\"\n\t\tGenerate a binned numpy array representing `bins` of\n\t\tan edge."}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tx = np.linspace(-1, 1, bins.shape[0], endpoint=True)\n\treturn integral(x, edges)\n\n\ndef integral(x, edges):\n\t\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tdists = np.linalg.norm(x - edges, axis=1)\n\tdists /= np.linalg.norm(dists, axis"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t# We need to construct the mesh correctly now\n\t# So we do a proper normalization for integration\n\t# See :meth:`set_bins` method\n\t\n\t# First of all, the mesh is (edges, bins)\n\tbins = (edges/np.sum(edges), bins)\n\tmesh = np.zeros((edges, bins), dtype=long)\n\tfor bin in bins:\n\t\tmesh[bin] += 1\n\t# And the integral is\n\t# For each bin a, b, c, we can compute the integral as:\n\t#  a/(a"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t# Compute integral\n\tx = numpy.arange(0, edges[0], 1.0 / bins[0])\n\tdx = 0**bins[0]\n\n\ti = 0\n\tfor j in edges[1:]:\n\t\tdx += x[i] * (j - x[i - 1])\n\t\ti += 1\n\n\t# The integral is the scale for all bins in :math:`[bin_start, bin_stop]`\n\t# This is the range of integral :math:`[0.0, 1.0]` for single bins.\n\t"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tpass\n\ndef integral2d(bins, *edges):\n\tpass\n\ndef is_controled(mesh, verbose=False):\n\tpass\n\ndef is_periodic(mesh, verbose=False):\n\tpass\n\ndef is_periodic2d(mesh, verbose=True):\n\tpass\n\ndef is_periodic3d(mesh, verbose=False):\n\tpass\n\ndef meshgrid(mesh, zspacing=1., verbose=False):\n\tpass\n\ndef p_on_contour(kpts, **kwargs):\n\tpass\n\ndef project"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tfrom sp_utilities import get_image\n\tfrom sp_fundamentals import rot_shift3D\n\tfrom EMAN2 import Processor\n\tnx = 512\n\tny = 512\n\n\th, w = hist_histo(bins)\n\tx, y, z = x/(nx - 1), y/(ny - 1), z/(nz - 1)\n\n\tp0 = [0] * (nx*ny*nz)\n\tr = get_image(nx, ny, nz, myid)\n\tif nx%2 == 0:\n\t\tp0"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t# get bin edges \n\tedges = tuple( edge[2::] for edge in edges )\n\t\n\t# integrate\n\tintegral = Integral( bins[0] * edges[0], bins[1] * edges[1] )\n\n\treturn integral\n\n/*\n   +----------------------------------------------------------------------+\n   | HipHop for PHP                                                      "}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\traise NotImplementedError(\"integrally integrate with integral\")\n\n\n@docstring.dedent_interpd\ndef integral_scale(pgraph, edge_list, bin_list=None, scale=.5):\n\t\"\"\"\n\tCompute integral scaled (from bin centers) for a graph.\n\t\n\t:arg pgraph: the parent instance of the graph.\n\t:param edge_list: list of pgraph edges.\n\t:param bin_list: list of bin centers, or ``None``.\n\t:param scale: float, scale for integral\n\n\t*edges* are"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tfrom h5py import H5PY\n\tfrom numpy import array, shape, ones, zeros\n\n\th5d = H5PY.File('integration_bin_h5py', 'r')\n\th5d.create_group(\"edge_centers\")\n\th5d.create_dataset(\"edges\", data=edges)\n\th5d.create_dataset(\"bins\", data=bins, mode='element')\n\n\tfrom numpy import dot, asarray, transpose, zeros\n\n\tmesh = zeros((24, 20, 3))\n\tfor e in range"}]}
{"task_id": "62b87b199a0c4fa8b80b354e", "prompt": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tfrom stix.v20 import \\\n\t\t\tGetElementsByClass, \\\n\t\t\tSource, \\\n\t\t\tEvidence, \\\n\t\t\tEvidenceSource, \\\n\t\t\tTypeExtension, \\\n\t\t\tVersion, \\\n\t\t\tAction, \\\n\t\t\tVerificationStatus, \\\n\t\t\tAnnotation\n\n\tif is_sequence(seq):\n\t\tfor element in seq:\n\t\t\tif hasattr(element, \"value_as_string\"):\n\t\t\t\tresult = bool(element.value_as_string())\n\t\t\t\tif result:\n\t\t\t\t\treturn result\n\t\t"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\treturn (isinstance(seq, FillRequestSeq)\n\t\tand not seq.isEmpty())\n\ndef fill_request_list_to_set(list):\n\t\"\"\"\n\tConvert *list* to a FillRequestSeq.\n\t\n\tThis function does not work for Source sequences!\n\t\n\tNote - the FillRequestSeq could be a list, or a singleton list or set.\n\tThe only way to get a list is with the next() function, which gives you\n\tan immutable copy of that list.\n\t\"\"\"\n\trequests = set()\n\tfor i, (seq, _"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\treturn bool( isinstance(seq, FillRequest) or seq.__class__ in (Source, None) )\n\ndef find_fill_request_list(seq):\n\trequests = []\n\tif is_fill_request_seq(seq):\n\t\trequests = SequenceMatcher(seq)\n\t\n\treturn requests\n\ndef find(seq):\n\treturn find(seq, True)\n\ndef not_found(seq):\n\treturn find(seq, False)\n\ndef find_by_element(seq, element):\n\treturn find(seq, element)\n\ndef insert"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\treturn _is_fill_request_seq(seq)\n\ndef _from_xml_element(xml_element):\n\treturn fill_request_element_from_xml_element(xml_element)\n\ndef _to_xml_element(\n\t\tcontent_element,\n\t\txml_serializer,\n\t\tcanonical_element_order=None):\n\t\n\tif isinstance(content_element, str):\n\t\txml_element = xml_serializer.convert_string(\n\t\t\t\t\t\t\tcontent_element)\n\t\t\n\telif isinstance(content_element, (int, long"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tif len(seq) < 1:\n\t\treturn False\n\tele = seq[0]\n\tif ele is None:\n\t\treturn False\n\telse:\n\t\t#if seq.types.get(\"FILLREQUEST\",None)!= None :\n\t\tif isinstance(ele, FillRequest):\n\t\t\treturn True\n\t\treturn False\n\ndef is_fill_request(seq):\n\t\"\"\"\n\t\tTest whether *seq* can be converted to a FillRequest.\n\t\t\n\t\tTrue if it is a FillRequest or source element,\n\t\twithout regards"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tif not seq.source.fills:\n\t\treturn False\n\tif seq == SourceSeq:\n\t\treturn True\n\tif seq.contains is not None:\n\t\treturn seq.contains is not SourceSeq \n\treturn seq.source.fills is None\n\ndef is_fill_request_line(line):\n\t\"\"\"\n\tTest that *line* is a FillRequestLine.\n\t\t\n\tReturns\n\tTrue when it is a FillRequestLine\n\tand contains at least 1 such line\n\t\"\"\"\n\tif not line.source.fills:\n\t\t"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\treturn not bool(\n\t\t(\n\t\t\tnot elem.qname.text.strip()\n\t\t\tand \n\t\t\tnot e.is_source\n\t\t\tand is_a_fillrequest(e)\n\t\t\tand seq.source_seq == e.source_seq\n\t\t)\n\t)\n\ndef is_a_source(e):\n\treturn isinstance(e, xml.etree.Element) and is_a_fillrequest(e)\n\ndef strip_source_seq(seq):\n\treturn seq_to_ast(seq, strip_source"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tfor el in seq:\n\t\tif ((el.tag, el.attrib) == (FILL_REQUEST, FILL_SOURCE)):\n\t\t\treturn False\n\treturn True\n\ndef get_source_value_for_seq(seq, source_attrib):\n\t\"\"\"\n\t\tGet the attribute value corresponding to the attribute in the\n\t    sequence (either SOURCE or SINK), given the source value.\n\t\"\"\"\n\tfor el in seq:\n\t\tif ((el.tag, el.attrib) == source_attrib):\n\t\t\treturn el.text\n\treturn"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tfor seq in seqs:\n\t\tif is_fill_request_seq(seq):\n\t\t\treturn True\n\treturn False\n\n\nclass FillRequestSeq(common.AbstractSimpleType):\n\tmodule = \"http://www.etsi.org/deliver/etsi_ref/2016-08-06/pdf/FAQ_0020.pdf\"\n\txpath = (\n\t\torx\n\t)\n\txpath_name = \"http://www.etsi.org/deliver/etsi_ref/2016-08-06/pdf/FAQ_0020."}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\treturn bool(seq)\nall_seq.add((\"request\", \"ReqElement\", \"source\", \"Seq\"))\n\nis_set_request_seq = make_bool()\n\ndef is_set_request_elem(seq):\n\tfor elem, val in seq:\n\t\tif elem == \"request\":\n\t\t\treturn val == \"ReqElement\"\n\treturn False\nall_seq.add((\"request\", \"ReqElement\", \"source\", \"Seq\"))\n\ndef is_set_source_seq(seq):\n\tfor elem, val in seq:\n\t\tif"}]}
{"task_id": "62b87b099a0c4fa8b80b3538", "prompt": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn isinstance(obj, FillRequest)\n\ndef is_fill_response_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and'response'.\n\t\"\"\"\n\treturn isinstance(obj, FillResponse)\n\ndef is_fill_update_el(obj):\n\"\"\"\n\tObject is executable and contains executable methods 'fill' and 'update'.\n\tobj contains executable methods fill.\n\t\"\"\"\n\treturn isinstance(obj, FillUpdate)\n\ndef is_fingertree_el(obj):\n\"\"\"\n\tObject is executable and contains executable methods 'f"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn (hasattr(obj, \"executable\"))\n\t\t\ndef is_close_request(obj):\n\t\"\"\"\n\t\tobj: the executable object in question\n\treturns:\n\t\t\t:1 = obj can be closed,\n\t\t\t:2 = obj cannot be closed,\n\t\t\t:3 = an element with this attribute has been found,\n\t\t\t:4 = (a, v) tuple in an el of obj have been found\n\t\t\t\n\t\t:return:\n\t\t\n\t\t:param obj:\n\t\t\t\t\t\t\tobject to check \n\t\t:param bool force:\n\t\t\t\t\t\t\t"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn getattr(obj, 'handler', None) is not None\n\ndef is_fill_call_el(obj):\n\t\"\"\"\n\tObject contains executable methods 'fill' and 'call'.\n\t\"\"\"\n\treturn getattr(obj, 'handler', None) is not None\n\ndef is_fill_message_el(obj):\n\t\"\"\"\n\tObject contains executable methods 'fill' and'message'.\n\t\"\"\"\n\treturn getattr(obj, 'handler', None) is not None\n\n#\n#\n#\n\ndef is_request_el(obj):\n\t\"\"\""}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn isinstance(obj, (EFillRequest, ERequest))\n\ndef is_el_class(obj):\n\t\"\"\"\n\tObject contains executable methods 'cls' and'method'.\n\t\"\"\"\n\treturn isinstance(obj, (cls, method))\n\ndef is_el_class_str(obj):\n\t\"\"\"\n\tObject contains executable methods 'cls' and'method'.\n\t\"\"\"\n\treturn is_el_class(obj)\n\ndef is_el_field(obj):\n\t\"\"\"\n\tObject contains executable methods 'get' and 'field'.\n\t\"\"\""}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\tfrom is2.extensions.fill_helpers import get_fill_request_el\n\tobj = get_fill_request_el(obj)\n\treturn bool(obj)\n\n\ndef is_fill_request_el_list_comp(obj, other_list=[]):\n\t\"\"\"\n\tReturn True if it is not list comp of other object.\n\tOther of array is not contain the index where to set the object.\n\tOther is the object.\n\t\"\"\"\n\tfor i in other_list:\n\t\ti = int(i)\n\t\tif i is"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn _is_fill_request_el(obj)\n\ndef is_ellipsis_excepration_el(obj):\n\"\"\"\n\tObject contains executable methods 'ellipsis' and 'extraction'.\n\t\"\"\"\n\treturn _is_ellipsis_excepration_el(obj)\n\ndef is_fill_excepration_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'extraction'.\n\t\"\"\"\n\treturn _is_fill_excepration_el(obj)\n\ndef is_line_exce"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn obj in [fill, request, fill_request]\n\nimport React from \"react\";\n\nimport { Banner, Paragraph } from \"grommet\";\nimport { FormattedMessage } from \"react-intl\";\nimport classNames from \"clsx\";\nimport { Box } from \"@chakra-ui/layout\";\n\ninterface Props {\n  description: string;\n  image: string;\n  showInfo?: {\n    description: string;\n    image: string;\n  };\n  children: React.ReactNode;\n}\n\n"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') or hasattr(obj,'request')\n\n\ndef is_any_el(obj):\n\t\"\"\"\n\tChecks whether an executable element is an 'any' exeception.\"\"\"\n\treturn hasattr(obj, 'any') or is_element_exeception(obj)\n\n\ndef is_exe_fill_el(obj):\n\t\"\"\"\n\tChecks whether an executable element is an 'exeFill' object.\"\"\"\n\treturn hasattr(obj, 'exeFill') or is_element_exeception(obj)\n\n\ndef is_req_el"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn is_true(getattr(obj, '__is_fill_request_el__', False))\n\n\ndef is_type_el(obj):\n\t\"\"\"\n\t\tBoolean indicates that the execution should be performed against\n\t\teach type of 'object' in 'obj.type'.\n\t\"\"\"\n\treturn is_true(getattr(obj, 'type', False))\n\n\ndef get_obj_attr_from_el(el, attr_name, default=None):\n\t\"\"\"\n\t\tGets 'el' attribute from 'attr_name' by looking its value in\n"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn (obj is not None and isinstance(obj, (types.InstanceType, types.InstanceType0, types.ClassType)) and\n\t        obj.klass in [types.ReferenceType, types.InstanceMethod, types.InstanceMethod0, types.ConstantMethod0, types.ConstantMethod))\n\nclass _Method(tuple):\n\t\"\"\"\n\t\tType of executable method\n\t\t\"\"\"\n\tdef __new__(cls, arglist):\n\t\tnew = tuple.__new__(cls, arglist)\n\t\tnew[1] = self\n\t\treturn new\n\n"}]}
{"task_id": "62b87af99a0c4fa8b80b3524", "prompt": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn isinstance(obj, (Elvis, ElvisPython))\n\ndef is_setfile_el(obj):\n\t\"\"\"\n\t\tTest whether obj is an existing'set'.\n\t\t\"\"\"\n\treturn isinstance(obj, ElvisSet)\n\ndef get_elvis_set(objs):\n\t\"\"\"\n\t\tReturn sets if there is an elvis'set'.\n\t\"\"\"\n\treturn get_elvis_set_list(objs)\n\ndef get_evobj(obj, objs=None):\n\t\"\"\"\n\t\tReturn the first valid'set'."}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn isinstance(obj, type(run))\ndef is_runpy_el(obj):\n\t\"\"\"\n\t\tObject contains executable method 'run_py'.\n\t\"\"\"\n\treturn obj in runpy._run_id\ndef is_runit_el(obj):\n\t\"\"\"\n\tObject contains executable method 'runit'.\n\t\"\"\"\n\treturn callable(obj) and callable(getattr(obj, \"_execute\", lambda: None))\ndef is_runpyrun_el(obj):\n\t\"\"\"\n\tObject contains executable method 'run_pyrun'.\"\"\"\n\treturn"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn isinstance(obj, tuple(obj[:-1,], obj[1:] or None))\n\ndef is_function(obj):\n\t\"\"\"\n\t\tTrue if the argument is a callable.\n\t\"\"\"\n\treturn callable(obj)\n\ndef is_none(obj):\n\treturn obj is None\n\ndef is_str(obj):\n\treturn hasattr(obj,'__str__') and obj.__str__()\n\ndef is_list(obj):\n\treturn obj is not None and hasattr(obj,'__iter__')\n\ndef str2bool(s):\n\t"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn isinstance(obj, type(obj.__init__))\n\ndef is_thread_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn isinstance(obj.__self__, type(obj.__init__))\n\ndef is_coroutine_el(obj):\n\t\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn isinstance(obj, type(get_generator_delegate()))\n\ndef is_task_el(obj):\n\t\"\"\"\n\tObject contains executable method 'run' with the 'task'\n\tmethod returning a Task class."}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn isinstance(obj, types.MethodType) and obj.__name__.endswith(\"run\")\n\nclass W_Cmds(object):\n    \n    def __init__(self):\n        self._w_cmds = dict()\n        \n    def run(self, obj, exe):\n        \"\"\"\n\t\tRun one or more commands.\n\t\tThe second parameter can be None for run the commands from the original.\n\t\t\"\"\"\n        for name"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\tres = False\n\tfor i in obj.get_all_attrs(\"el_attrs\"):\n\t\ta = i.get_attr(\"run_method\")\n\t\tif a:\n\t\t\tres = (a.value == str(obj.text))\n\treturn res\n\nclass ObjectRun(object):\n\tdef __init__(self, el, method=\"run\"):\n\t\tself.el = el\n\t\tself.method = method\n\t\tself.run_function_text = None\n\n\t@property\n\tdef text(self):\n\t\t\"\"\"\n"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn isinstance(obj, (EL, _EL))\n\ndef is_stop_el(obj):\n\"\"\"\n\tObject contains not-yet-executed-function method'stop'.\n\t\"\"\"\n\treturn not hasattr(obj, 'func')\n\ndef is_step_el(obj):\n\"\"\"\n\tObject contains executable method'step'.\n\t\"\"\"\n\treturn isinstance(obj, (EL, _EL))\n\ndef _get_function_attr(obj, name, kind):\n\tif is_el(obj):\n\t\tattrs = _get_attribute(attrs={"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\ttry:\n\t\treturn getattr(obj, \"__run__\", False)\n\texcept AttributeError:\n\t\treturn False\n\ndef get_object_for_method(obj, method=None):\n\tif not obj._has_wrapped:\n\t\twrap = obj.__dict__.setdefault(\"__wrapped__\", {}).setdefault(\n\t\t\t\"__instance__\", None)\n\t\tif wrap: # was _getwrapped(obj)\n\t\t\tobj = wrap[method]\n\t\t\tif obj:\n\t\t\t\treturn get_object_for_method(obj, method)\n\n\t\t"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn obj is not None and obj.__class__ is  _Running:\n\ndef all_valid_classes():\n\tif hasattr(__pypy_module__, '__all_subclasses__'):\n\t\tfor cls in __pypy_module__.__all_subclasses__():\n\t\t\tif cls in ALL_VALID_CLASS_NAMES:\n\t\t\t\treturn True\n\treturn False\n\ndef new_class(name, parent_cls, cls):\n\t\"\"\"\n\tCreate an object based on a class with the given name.\n\t\"\"\"\n\tinst = cls(None, parent"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn isinstance(obj, types.MethodType) and \"run\" in obj.im_self.func_code.co_varnames\n\n\ndef is_code_object(obj):\n\t\"\"\"\n\t\tReturns whether object is a Python function\n\t\"\"\"\n\t# TODO: Check types for functions?\n\treturn not (obj.__origin__ is obj and obj if hasattr(obj, '__func__') and callable(obj.__func__))\n\n\ndef is_method(obj):\n\t\"\"\"\n\t\tReturns whether obj is a Python function\n\t\"\"\"\n\t# TODO: Check types"}]}
{"task_id": "62b87af69a0c4fa8b80b351a", "prompt": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn _check_fill_compute_el(obj)\n\ndef is_fill_add_comp(obj):\n\t\"\"\"\n\t\tObject contains executable methods 'fill' and 'add_comp'.\n\t\"\"\"\n\treturn _check_fill_add_comp(obj)\n\ndef is_fill_mul_comp(obj):\n\t\"\"\"\n\tObject contains executable methods 'fill' and'mul_comp'.\n\t\"\"\"\n\treturn _check_fill_mul_comp(obj)\n\ndef is_fill_div_comp(obj):\n\t\"\"\"\n\tObject"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn isinstance(obj, type(lambda x: fill(x).compute(x), (NDArray, type, None)))\n\ndef is_fill_add_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'add'.\n\t\"\"\"\n\tif not is_fill(obj):\n\t\treturn False\n\treturn (isinstance(obj, (type, NDArray, ndarray, None)) or (callable(obj) and is_fill_compute_el(obj)))\n\ndef is_fill_element(obj):\n\"\"\"\n\tObject is element of a matrix"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn isinstance(obj, Fill) or hasattr(obj, \"type_expr\")\n\n\ndef is_fill_type(obj, t):\n\t\"\"\"\n\t\tReturn True if an object is a fill in the specified Typer\n\t\"\"\"\n\tif t is None or t is Type.none: return False\n\treturn t in obj.__bases__\n\n\ndef _is_dtype(d):\n\tif d is None: return False\n\tif isinstance(d, Dtype):\n\t\treturn True\n\tfor dtype in d.__mro__[1:]:\n\t\tif"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn 'fill' in dir(obj)\n\n\ndef is_reshape_el(obj):\n\t\"\"\"\n\tObject contains executable methods'reshape' and 'compute',\n\twhich correspond to reshape op.\n\t\"\"\"\n\treturn len(obj) == 2 or obj[0] =='reshape' and 'compute' in obj[2]\n\n\ndef is_squeeze_el(obj):\n\t\"\"\"\n\tObject contains executable methods'squeeze' and 'compute',\n\twhich correspond to reduce op.\n\t\"\"\"\n\treturn'squeeze' in dir(obj)\n\n"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\ttry:\n\t\tfor fn in obj.__code__.co_varnames[:obj.__code__.co_argcount]:\n\t\t\tif fn in obj.__fillmethods.keys() \\\n\t\t\t\tand obj.__computemethods.keys():\n\t\t\t\treturn 1\n\t\treturn 0\n\texcept AttributeError:\n\t\treturn 0\n\treturn 0\n\n\nclass Python_code_object:\n\t_cached_code_object_class_name = []\n\t_cached_code_object_class_key = (\"Python_None\", 1)#this is always the first line, because"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn inspect.isfunction(obj) or inspect.ismethodcall(obj)\n\n\ndef is_fill_schedule_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill','schedule' for a scheduling_period.\n\t\"\"\"\n\treturn inspect.ismethoddescriptor(obj) or inspect.isfunc(obj) \\\n\t\t\tor inspect.ismethod(obj)\n\n\ndef is_binary_expression(thing):\n\t\"\"\"\n\tReturn true if the object is a binary expression.\n\n\t:Parameter thing: Any object in which to evaluate the expression."}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn _is_exe(obj, 'fill') or _is_executable(obj, 'compute') \n\ndef is_fill_update_el(obj):\n\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn _is_exe(obj, 'fill') or _is_executable(obj, 'compute') and obj.args[0].method.startswith('update')\n\ndef is_fill_set_el(obj):\n\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn _is"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn isinstance(obj, (base.FunctionObject, base.SequenceObject, base.MethodObject)) and obj._parameters and isinstance(obj, (base.ExecutableClass, _ParameterListObject)) # nosec\n\n\ndef is_indexing_compute_el(obj):\n\t# for indexing, it is not a compute-object\n\treturn isinstance(obj.data, base.IndexingData)\n\n\ndef _check_el(context, obj):\n\tret = {}\n\n\tif not isinstance(obj, base.CompiledMethodObject):\n\t\tret['name']"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn ((hasattr(obj, \"__fsspec__\") and callable(obj.__fsspec__)) or obj.__wrapped__ in (\"fill\", \"compute\", \"frompacked\") or\n\t\tisinstance(obj, np.ndarray) and is_numpy(obj))\n\ndef _is_vector(func):\n\t\"\"\"\n\t\tCheck if a function is a function implementing __vector__.\n\t\"\"\"\n\treturn hasattr(func, \"__vec__\")\n\ndef is_vector(obj):\n\t\"\"\"\n\t\tCheck if the supplied object implements __vector__.\n\t\"\"\"\n\t"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tif not isinstance(obj, (str, bytes, type(None))) and not isinstance(obj,\n\t\ttuple):\n\t\ttry:\n\t\t\treturn is_el_type_comp(obj) and not _is_computing_type(\n\t\t\t\tobj.type)\n\t\texcept IndexError:\n\t\t\treturn False\n\ttry:\n\t\treturn is_el_type_comp(obj) and not isinstance(obj, str) and not isinstance(\n\t\t\tobj, bytes)\n\texcept IndexError:\n\t\treturn False\n\tpass\n\n\n"}]}
{"task_id": "62b87af19a0c4fa8b80b34f7", "prompt": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\ttry:\n\t\treturn {}\n\texcept AttributeError:\n\t\tif hasattr(d1, 'keys') and not hasattr(d2, 'keys'):\n\t\t\td2 = d2.keys()\n\t\ttry:\n\t\t\tres = {}\n\t\t\td1, d2 = d1.items(), d2.items()\n\t\t\tfor k1, v1 in v1a(d1):\n\t\t\t\tfor k2, v2 in v2a(d2):\n\t\t\t\t\tif k1!= k2 or (k1 is None and v1!="}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tif not level in (0, 1, 2) and not d1.has_key(d2):\n\t\t# Not present in *d2*: need a higher key\n\t\t\n\t\treturn _getmerge(d1, d2, False, level)\n\telse:\n\t\treturn _mergedict(d1, d2, level)\n\t\t\ndef _copydict(_list_src, _list_dst):\n\tfor key in _list_src:\n\t\tif key not in _list_dst:\n\t\t\titem_src = []\n\t\t\titem"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tres = {}\n\ttry:\n\t\td1 = d1[:]\n\t\td2 = d2[:]\n\texcept (TypeError, AttributeError):\n\t\treturn res\n\tfor k in d1.keys():\n\t\ttry:\n\t\t\tif k in d2: continue\n\t\texcept KeyError:\n\t\t\traise ValueError, \"items from d1 {!r} and d2 {!r} not in {}\".format(d1, d2, d1)\n\t\ttry:\n\t\t\tdiff = d1[k] - d2[k]\n\t\texcept"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tif isinstance(d1, dict):\n\t\tout = d1.copy()\n\t\tif not out:\n\t\t\treturn dict()\n\t\treturn out\n\tif isinstance(d2, dict):\n\t\tout = d2.copy()\n\t\tif out:\n\t\t\treturn out\n\t\treturn d1\n\tif level!= 0:\n\t\tif isinstance(d1, list):\n\t\t\td2 = _listdifference(d1, d2)\n\t\telif isinstance(d2, list):\n\t\t\td1 = _listdifference("}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tfor k in (k for k, v in d1._items() if v!= d2._items()[k]):\n\t\tif k not in d1:\n\t\t\treturn d2._copy(level=level)\n\treturn d1\n\n\ndef intersection(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary from *d1* to *d2*, but with items\n\tcontained in *d1* but not in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tresult = d1.copy()\n\tfor key, value in zip(d2, d1):\n\t\tif key not in result and key in result:\n\t\t\tresult[key] = value\n\treturn result\n\n\ndef intersection(*d1, **kwds):\n\t# Intersection of *d1* with the optional parameters **kwds*.\n\t''' Create a dictionary that contains **all** the items in *d1** that\n\tare in *d1* but not in *d1**, but not in *d1**, that are contained in\n\t"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tif not d2:\n\t\td2 = {}\n\tif not d1:\n\t\treturn d2\n\tout = d2\n\tfor key in d1:\n\t\tout[key] = d1.pop(key)\n\t\tif not out[key]:\n\t\t\tout.pop(key, None)\n\t# recurse on keys - to save on lookups\n\twhile out:\n\t\tdif = d1.difference(out)\n\t\tif dif:\n\t\t\tout.update(dif)\n\t\tif out:\n\t\t\t# recurse"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\ttry:\n\t\td1 = d1.copy()\n\texcept AttributeError:\n\t\tpass\n\telse:\n\t\td2 = d2.copy()\n\t\td1 = d1.difference(d2)\n\t\t\n\treturn d1\n\n\n#===============================================================================\n# hash\n#===============================================================================\n\ndef hash(x):\n\t\"\"\"\n\tReturn the value of *x* (a :class:`dict`) as a 32-bit signed integer\n\t(only for Python 2).\n\t\n\t:func:`~python:hash` is a Python 2-to-"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tfor key in (d1.keys() + d2.keys()):\n\t\tif key in d1:\n\t\t\td1[key].difference(d2[key], level)\n\t\telif key in d2:\n\t\t\tkey2 = d2[key]\n\t\t\td1[key2] = d1[key]\n\t\t\td1[key] = key2\n\td1, d2 = d1[()], d2\n\n\treturn dict(d1)\n\ndef intersection(d1, d2, level=-1):\n\t\"\"\""}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\td1=[(k, v) for (k, v) in d1]\n\td2=[(k, v) for (k, v) in d2]\n\td3=[(k, v) for (k, v) in d1]\n\td4=[(k, v) for (k, v) in d2]\n\td5=[(k, v) for (k, v) in d1]\n\t\n\tdiff={(k, v, k2, v2):-k for k,v,k2,v2 in d"}]}
{"task_id": "62b87af09a0c4fa8b80b34f1", "prompt": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself._data._g.Fill(coord, int(weight*256))\n\t\n\t\n\t\n#######################################################################\n#######################################################################\n\t#-----------------------------------------------------------------\n\nclass _VCLineColorAccessor(object):\n\t\n\tdef getvcl(self):\n\t\t\"\"\"Get the vertical color of the line. \"\"\"\n\t\treturn self._data.vcl\n\tdef setvcl(self,color):\n\t\t\"\"\"Set the vertical color of *color*.\"\"\"\n\t\tself._data.vcl = color\n\tvcl = property(getvcl, setvcl)"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tfrom bisect import bisect_left\n\t\n\tself._d.fill(coord[0], coord[1], 0)\n\tpos = []\n\tweight = self.weight\n\tif coord[1]!= self.upper_bound:\n\t\tupper = self._d.values[self._d.index[self.upper_bound]]\n\t\tassert pos == upper\n\t\tpos = None\n\telse:\n\t\tpos = bisect_left(self.histogram, coord)\n\t\n\tself.buckets = None\n\tfor bucket in self.histogram"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\twith self.lock:\n\t\tself.stats.fill(coord, weight)\n\t\n\tself.dirty = False\n\t\n\tself.index_to_coord.clear()\n\ttotal = 0\n\t\n\tfor coord, value in sorted(self.histogram.items()):\n\t\ttotal += value\n\t\tself.index_to_coord[int(coord)] = coord\n\t\tself.index_to_stats[int(coord)] = value\n\t\t\n\treturn self.index_to_coord, total, self.stats\n\n### stats module methods\n\ndef get_"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself._data = self._data.fill(coord, weight)\n\ndef fill(self, x, y, weight=1):\n\t\"\"\"\n\tFill the histogram with the given *weight* for each dimension that \n\tbelongs to the first argument.\n\t\"\"\"\n\tself._data = self._data.fill(x, y, weight)\n\ndef interpolate(self, x, y, weight=1):\n\t\"\"\"\n\tInterpolate the histogram with the given *weight* at the coordinates *x* and *y*.\n\n\tCoordinates outside the histogram edges are ignored."}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.add(coord)   # fill histogram with weight 0\n\tself.add(coord, weight=0)\t# fill histogram w.r.t. *coord*\n\treturn\n\n#--------------------------------------------------------------------------\n#---------------------------------------------------\n# Histogram-based functions ---------------------------------------------\n#--------------------------------------------------------------------------\n\ndef peaks(self, nbins=16):\n\t\"\"\"\n\tFind the peaks of a histogram.\n\t\n\tNote that this function only works on the underlying\n\tdistribution. The underlying distribution is passed in so\n\tthat it is not converted to a Numpy distribution.\n\t\"\"\"\n\t"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif coord[:] == \"all\":\n\t\treturn\n\tif coord[:]!=(coord < np.min(self.bin_edge)):\n\t\tself.bin_edge = norm.maxnorm(coord)\n\t\tself.weight[:] = self.__get_weight(weight)\n\telse :\n\t\tbin_index = (coord < np.max(self.bin_edge))\n\t\tself.weight[:] = norm.maxnorm(np.sum(coord[bin_index]))\n\t\tcoord_ = np.sum(coord[bin_index])\n\t\tself"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tfrom sp_utilities import model_circle, get_refvol\n\t\n\tif(coord[0] < 0 and coord[1] < 0):\n\t\treturn \n\telse:\n\t\tself.n_filled(coord, mask = 0)\n\t\tif(self.mask): return\n\t\timg = sp_fundamentals.rot_shift2D(self.copy(), self.nx, self.ny, self.nz, self.dx, self.dy, self.dz, 1.0/float(self.dx*self.dy*self.dz"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\ta=[]\n\ta.append(0)\n\tfor h in self._buckets:\n\t\tx=h._max\n\t\tx-=a\n\t\tx-=coord\n\t\tw=weight/x\n\t\tif x>=coord:\n\t\t\th.insert(x, w)\n\t\t\th._max=x\n\treturn a\n\ndef fillall(self):\n\t\"\"\"\n\tFill all histograms and return as many histogram instances as there \n\tare.\n\t\"\"\"\n\tr=None\n\tfor h in self._buckets:\n\t\tr"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif self._bin == None or self._mask == None:\n\t\treturn\n\tbin = self._bin\n\tmask = self._mask\n\tfrom lib.histogram.lib import fill_histogram_bin_int\n\tfill_histogram_bin_int(bin, mask, bin - 1, mask, bin + 1, weight)\n\n# -------------------------------------------------- type definitions ----------------------------------------------------\nType = TypeVar('Type')\n\nclass Point(object):\n\n\tdef __init__(self, x,y):\n\t\tself.x = x\n\t\tself.y = y\n\t"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.data[:len(self.data)-2*self.hist_len] = self.data[3*self.hist_len:3*self.hist_len+self.hist_len] = self.hist[:]\n\tself.hist = []\n\t\n\ttry:\n\t\tfor i, x in enumerate(self.data):\n\t\t\tx.weight = coord.weight(self.data[i])\n\texcept ValueError: # no point in the histogram, try the entire array\n\t\tfill = i = 1\n\t\tfor i, x in enumerate(self"}]}
{"task_id": "62b86aa3b4d922cb0e688d36", "prompt": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 54.00 MiB (GPU 0; 22.16 GiB total capacity; 20.95 GiB already allocated; 27.88 MiB free; 21.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b86a9eb4d922cb0e688d25", "prompt": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tre_pattern = re.compile(resource_info_pattern)\n\treturn re_pattern\n\n\ndef _get_resource_type_regex():\n\"\"\"\n\tDetermine the resource type that will be used to validate the resource\n\tname\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions that are used to\n\t    validate the resource type\n\t\"\"\"\n\tresource_info_type_regex = re.compile(resource_type_pattern)\n\treturn resource_info_type_regex\n\n\ndef _get_resource_name_to"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tre.compile(r'^Krake +(?P<id>\\d{8}) '\n\t\t'(?P<label>[A-Za-z0-9-_.-]+)' \n\t\t'(?P<description>.+)$', re.IGNORECASE) \n\treturn re.compile(\"^Krake +(?P<id>\\d{8}) (?P<label>.*?)(?P<description>.*)$\", re.IGNORECASE)\n\n\ndef _get_resource_uuid(resource_name):\n\t\"\"\" Returns a UUID representation for this resource. \"\"\"\n\tif not os_resource"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tresource_names = [\"resources-v4.1.4_example\",\n\t\t\t\t\"resources-v1_example\",\n\t\t\t\t\"puppies-example\",\n\t\t\t\t\"titania-titania_example\",\n\t\t\t\t\"titania-example\",\n\t\t\t\t\"kraken_example\",\n\t\t\t\t\"titania-titans_example\",\n\t\t\t\t\"chassis-example\"]\n\treturn [re.compile(x, re.IGNORECASE) for x in resource_names]\n\n\ndef _get_resource_value_regex"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tr = re.compile(\"kraku(.+)\")\n\treturn r\n\n\n@_get_resource_name_regex\ndef resource_name(resource):\n\t\"\"\"\n\tGiven the name of a KRAKU, returns the regex that must be used for\n\tvalidating that the name is valid for the KRAKU.\n\t\n\tReturns:\n\t    (re.Pattern): the regular expression, to use for validating\n\t    the resource name.\n\t\"\"\"\n\tif \" \" in resource:\n\t\treturn re.compile(\"^kraku("}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t# Generate the regex\n\tfrom rdkit import Chem\n\tfrom rdkit import rd\n\tr = Chem.MolFromSmiles('[A-Z]([A-Z])')\n\tname_re = r.GetMolFromSmiles(r._crystal.get_atoms(), r._cif.get_atoms())\n\tre = Chem.RegexpBuilder(r.GetMolFromSmiles(r._crystal.get_atoms(), r._cif.get_atoms()), \"^[A-Z][A-Z]$\")\n\tfor k"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t#\n\t# regex that will be used when validate and build the name in a\n\t# case where a resource has the same name and version.\n\t#\n\tre_regex = r'(?P<re>.+)\\.'\n\t#\n\t# regex that will be used when validate and build the name in a\n\t# case where a resource has multiple names but the same version.\n\t#\n\trm_regex = r'(?P<rm_re>.+)\\.'\n\t#\n\trm_regex_single_name = r'maker_name(.*?)/'\n\t#"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\treturn _get_regex(\"_resource\", \"Name\")\n\ndef _get_resource_name_regexes():\n\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe names of the Krakus resources.\n\t\n\tReturns:\n\t    (list of str): the compiled regular expressions, to validate\n\t    the resource names.\n\t\"\"\"\n\treturn [\n\t\tre.compile(\n\t\t\tr\"\\g<name>(.*)</name>\",\n\t\t\tre.UNICODE\n\t\t),\n\t\tre.compile(\n\t\t\tr"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t# This regex is used to validate the name and its corresponding class,\n\t# which can be used to validate the resource.\n\tre_name_regex = re.compile(r\"^(\\d{4}[A-Z]{2}(\\-[A-Z]{3})$|[^a-zA-Z0-9_]*)$\")\n\n\tdef _convert_name(name):\n\t\t\"\"\"\n\t\tConvert the name into the defined valid resource name and group.\n\t\t\"\"\"\n\t\t# Group must be in the set for now.\n\t\tif name"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tregex_patterns = []\n\tregex_patterns.extend(re.compile(r\"^[a-zA-Z0-9_-]+$\", re.UNICODE))\n\n\treturn regex_patterns\n\nimport { getSerdePlugin } from \"@aws-sdk/middleware-serde\";\nimport { HttpRequest as __HttpRequest, HttpResponse as __HttpResponse } from \"@aws-sdk/protocol-http\";\nimport { Command as $Command } from \"@aws-sdk/smithy-client\";\nimport {\n  FinalizeHandlerArguments,\n  Handler,\n  HandlerExecutionContext,\n  HttpHandlerOptions as __HttpHandler"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\treturn re.compile(r'^(?P<group>[A-Za-z][A-Za-z0-9]*(?:\\.|(?:-[A-Za-z][A-Za-z0-9]*))*)-(?P<res>-[A-Za-z][A-Za-z0-9]*)-$', re.IGNORECASE)\n\n\ndef _get_resource_name_regex_from_str(name):\n\t\"\"\"\n\tRepresets the name in the regex.\n\t\n\tArgs:\n\t    name: the String to validate\n"}]}
{"task_id": "62b86a4fb4d922cb0e688cf8", "prompt": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t# regex\n\tRE_VALUE = \"([a-zA-Z:&-\\\\w:\\\\.\\\\[\\\\]\";\n\n\tif RE_VALUE in value:\n\t\t# valid regular expression\n\t\t# return True if matched\n\t\treturn True\n\n\t# else\n\t# do regular expression match\n\tif isinstance(value, bytes):\n\t\tmatch = re.findall(RE_VALUE, value)\n\n\t\t# check value for regex match\n\t\tif len(match) == len(value):\t# match: [a-zA-Z:]*\n"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif not re.fullmatch(r'^\\s+$', value):\n\t\traise ValidationError('', error_message=validate_message(\n\t\t\tu'The value should not include a space or a tab and should not contain any of \\\\' \\', \\': \\'. The following characters should be escaped.\\r\\n\\r\\n%s' % (\n\t\t\t', '.join(reg_ex),\n\t\t\t), error_message))\n\t\n\tval_re = re.compile(r'^[\\'\"^]+(\\'[^\\']*)?[\\'\"^]+=(?:\\"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif not value:\n\t\treturn False, \"Value cannot be empty\"\n\t\n\tif not regex.search(value):\n\t\treturn False, \"Value does not conform to the regular expression\"\n\t\n\treturn True, None, None, [value]\n\ndef validate_operator(operator):\n\t\"\"\"\n\tValidate the given operator against the corresponding regular expression.\n\t\n\tArgs:\n\t    operator: the operator we want to check for\n\t\n\tRaises:\n\t    ValidationError: if the operator is not conforming\n\t\"\"\"\n\tvalue, error = validate_"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tpattern = re.compile(value)\n\tfor match in pattern.finditer(value):\n\t\tif not pattern.match(match.group(0)):\n\t\t\traise ValidationError('Invalid value for {:s}: {:s}.'.format(\n\t\t\t\tvalue, match.group(0)))\n\treturn list(filter(None, pattern.group(0)))\n\n\n@decorators.is_callable(settings.SETTINGS)\ndef validate_form(form, errors):\n\t\"\"\"\n\tValidation of the given form, by default validates and processes\n\tall the form fields into a"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif not RE.pattern.match(value):\n\t\traise ValidationError('\"%s\" is not conform to the regular expression:' % value, 'html', 'Please input a value in the format <a href=\"http://www.imsglobal.org/en/im%E5/0623/HTML/HTML_reference.html\">HTML reference</a>.')\n\n\ndef validate_html_or_plain(value):\n\t\"\"\"\n\tValidate the given value against the plain/html standard.\n\t\n\tArgs:\n\t    value: the html or plain\n\t\n\t"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif not re.match(r'^[0-9]{4}$', value):\n\t\traise ValidationError('Not conforming'+ value)\n\telse:\n\t\treturn True\n\npackage judgeli.jophiel.persistence;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.ObjectWriter;\nimport com.fasterxml.jackson.dataformat.yaml.YAMLFactory;\nimport com.fasterxml.jackson.dataformat.yaml.YAMLGenerator;\nimport judgeli.jophiel.api.Rank"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\treturn re.compile(regex).match(value) is not None\n\n/*\n * File    : SimpleCache.java\n * Created : 24 AM\ufffd07..23 18\ufffd98\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif not re.match(VALID_CHARS, value):\n\t\traise ValidationError('\"%s\" is not a valid file size.' % value)\n\treturn value\n\n\ndef parse_value(value):\n\t\"\"\"\n\t\tParse a value that should not have invalid characters inside\n\t \t\n\t Args:\n\t    value: the value which string will be parsed\n\t\n\t Raises:\n\t\tValidationError: if the format string is invalid or contains invalid characters\n\t\t\tand the regular expression cannot be compiled.\n\t\"\"\"\n\tif re.search(VALID_CHARS, value):"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\timport re\n\t\n\tif not value:\n\t\traise ValidationError(_('The given value is not conform to the regular expression : ') + str(re.escape(_('^\\s*:\\s' + value))[:256]))\n\treturn value\n\n@register.filter(name='remove_quotes')\ndef remove_quotes(value):\n\t\"\"\"\n\t\tRemove double quotes around the contents of the given value.\n\t\t\n\t\tArgs:\n\t\t    value: the string to evaluate.\n\t\t\n\t\tReturns: the evaluated string\n\t\t\n\t\tRaises:\n\t\t    Validation"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif not value:\n\t\traise ValidationError('\"%s\" is not a valid value.')\n\n\tregex = re.compile(value, re.I)\n\tif not regex.match(value):\n\t\traise ValidationError('\"%s\" does not match the regular expression.'%value)\n\nclass Integer(ValidatorBase):\n\tdefault_options = {'max_digits': 2,'min_digits': 2, 'allow_underscores': False, 'allow_plus': True, 'allow_underscores_plus': True,\n\t\t'steps' : {'required"}]}
{"task_id": "62b86a4fb4d922cb0e688cf7", "prompt": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tregexp = r'^[A-Za-z0-9]+$'\n\tif re.search(regexp, key):\n\t\traise ValidationError('Key is not conform to regular expression: \"%s\".' % key)\n\n\ndef generate_random_string(length=32):\n\treturn ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(length))\n\n// Copyright 2015 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif not re.match(key, pattern):\n\t\traise ValidationError('\"%s\" is not matching regex: '\n\t\t\t\t\t\t\t\t 'Expected: \"%s\", given: \"%s\"' % (key, pattern,\n\t\t\t\t\t\t\t\t\t\tstr(re.search(key, pattern))))\n\n\n# The main class, provides common functions.\nclass ValidationError(Exception):\n\tdef __init__(self, message):\n\t\tself.message = message\n\n\tdef __str__(self):\n\t\treturn self.message\n\n\n# This is a simple class to represent a regular expression with a scope"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\ttry:\n\t\tregex.compile(key)\n\texcept (TypeError, ValueError):\n\t\traise ValidationError('Invalid key: %s' % key)\n\treturn regex.match(key)\n\n\ndef validate_string_parameter(param):\n\n\tassert isinstance(param, string_types), \\\n                \"Please provide a string, you can only pass one instance of string.\"\n\n\treturn param\n\ndef validate_list_of_string_parameters(keys):\n\n\tassert isinstance(keys,"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif not key:\n\t\traise ValidationError('Key must be provided to verify against the key pattern.')\n\ttry:\n\t\trregex = re.compile(key)\n\texcept:\n\t\traise ValidationError('Key must be alphanumeric and consist of only alphanumeric characters (a-z).')\n\t\n\treturn rregex\n\t\ndef validate_ip_address(ip_address):\n\t\"\"\"\n\tChecks to make sure that the given IP address is valid and belongs to the IP address pattern\n\t\"\"\"\n\ttry:\n\t\tipaddress.IPv4Address(ip"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\treturn re.search(r\"[a-zA-Z]*\", key)\n\n\ndef validate_key_list(key_list):\n\t\"\"\"\n\tReturns None if the given key_list is neither a string nor an array.\n\t\n\tRaises:\n\t    ValidationError if the given key_list is not of the type provided or\n\t    is a string and does not conform to the regular expression.\n\t\"\"\"\n\tif not isinstance(key_list, list) or isiterable(key_list):\n\t\traise ValidationError(\"A string is required.\")\n\n"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tkey_re = \"^\" + re.escape(key.lower()) + \"$\"\n\t\n\tif re.match(key_re, key):\n\t\t\"\"\"\n\t\tReturn the validated key.\n\t\t\"\"\"\n\t\treturn True\n\telse:\n\t\traise ValidationError\n\npackage types\n\nimport (\n\t\"time\"\n\n\tsdk \"github.com/cosmos/cosmos-sdk/types\"\n\n\t\"github.com/okex/exchain/x/backend/client\"\n)\n\n// QuoteInfo defines info regarding the quote"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\ttry:\n\t\tre.compile(key,re.IGNORECASE)\n\texcept re.error:\n\t\traise ValidationError('The supplied key is not a valid regular expression!')\n\tdel re\n\ndef get_value_by_key(key,value):\n\t\"\"\"\n\tGet the value set by the given key and set it to the corresponding value.\n\t\n\tArgs:\n\t    key: the key to get the value for\n\t    value: the value to set\n\t\n\tReturns:\n\t    The value set.\n\t\"\"\"\n\treturn"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\treturn re.match(regex, key)\n\ndef get_password_hash(password):\n\t\"\"\"\n\tReturn the hash calculated from the given password and return it.\n\t\n\tArgs:\n\t    password: the password to calculate from which the hash is supposed to go\n\t\n\tReturns:\n\t    a byte string of the password\n\t\"\"\"\n\thash = get_hash(password)\n\tif hash is None:\n\t\traise InvalidMessage(\n\t\t\t\"Invalid password format. The password has to contain at least two characters.\"\n\t\t)\n\treturn hash"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tvalid = True\n\treturn key.lower()\n\n\ndef validate_password(password, password_min_len):\n\t\"\"\"\n\tValidate that the given password does consist entirely of valid characters.\n\t\n\tArgs:\n\t    password: the password to validate\n\t    password_min_len: the length of the password to allow\n\t\n\tRaises:\n\t    ValidationError: if the given password is not conform to the regular expression.\n\t\"\"\"\n\tvalid = True\n\tfor char in password:\n\t\tif not bool(char.isalnum() and char"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t# Check the key is of the form %key%\n\tif not re.match(\"^\"+key+\"$\", key):\n\t\traise ValidationError(key, \"must start with a lowercase letter.\")\n\n#\tdef check_value(key, value):\n#\t\t\"\"\"\n#\t\tCheck that the given value is valid for the given key.\n#\t\t\"\"\"\n#\t\tvalue = string.capwords(str(value))\n#\t\tif len(value) > length:\n#\t\t\t# Key must be a valid number of symbols, cannot have longer key than max"}]}
{"task_id": "62b86a01b4d922cb0e688ccc", "prompt": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tif type(manifest_dict) is dict:\n\t\tnew_manifest_dict = deepcopy(manifest_dict)\n\t\tfor field_name in new_manifest_dict.keys():\n\t\t\tif not is_not_dict(new_manifest_dict[field_name]):\n\t\t\t\ttry:\n\t\t\t\t\tnew_dict = generate_default_observer_schema_dict(manifest_dict[field_name],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   create_default=True)\n\t\t\t\texcept Exception:\n\t\t\t\t\tnew_dict = {}\n\t\t\telse:\n\t\t\t\tnew_dict"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tbase_dict_copy = manifest_dict.copy()\n\tif not isinstance(base_dict_copy, dict):\n\t\tprint('Invalid format for custom resource observer_schema. The only allowed format is an object dictionary...')\n\t\traise TypeError('Invalid format for custom resource observer_schema.')\n\tobserver_schema = {k: d for k, d in base_dict_copy.items() if len(d) > 1}\n\tif first_level:\n\t\tfor k, v in manifest_dict.items():\n\t\t\tif not isinstance(v, dict)"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tdefaults = {\n\t\t\t\"enabled\": (\"true\", (0, 0, False)),\n\t\t\t\"metadata\": (\"{}\", {}),\n\t\t}\n\tif isinstance(manifest_dict, dict) and \"component\" in manifest_dict:\n\t\tif \"component\" not in manifest_dict or \"component\" not in manifest_dict[\"component\"]:\n\t\t\traise ValueError(\"Invalid component\")\n\t\tcomponent = manifest_dict.get(\"component\", {}).get(\"name\", None)\n\tif isinstance(manifest_dict, dict) and \"image\" in manifest_dict:\n\t"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tdefault_dict = {}\n\t\n\tif not first_level:\n\t\tfor entry in manifest_dict.values():\n\t\t\tfor i in entry.data.keys():\n\t\t\t\tif i in ['annotations', 'labels']:\n\t\t\t\t\tdefault_dict[i] = None\n\t\t\t\telif i.startswith('owner'):\n\t\t\t\t\tdefault_dict['owner'] = None\n\t\t\t\telse:\n\t\t\t\t\tdefault_dict[i] = None\n\t\n\telse:\n\t\tfor entry in manifest_dict.values():\n\t\t\tfor i in entry.data.keys():\n"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t# this is just a hack to ensure that only a single default will be used for this\n\t# resource. A user wishing to have two or more resources in a given ``application``\n\t# need to be able to customize in their own way.\n\tif not first_level and 'Default' in manifest_dict and \\\n\t       'Default' in manifest_dict['Default']:\n\t\tobserver_schema = generate_default_observer_schema(\n\t\t    manifest_dict['Application']['Defaults'], first_level)\n\t\tobserver_schema = update_dict"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tdef copy_dict(dict_in, dict_out, fields):\n\t\tif dict_in is None:\n\t\t\tdict_out = {}\n\t\telse:\n\t\t\tcopy_dict_from_dict(dict_in, dict_out, fields)\n\t\treturn dict_out\n\t\n\tdef copy_dict_from_dict(dict_in, dict_out, fields, fields_to_check=()):\n\t\t# Keep fields_to_check for future if needed\n\t\tfor field, val in fields_to_check:\n\t\t\tfld_"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\ttry:\n\t\tobserver_type = self.observer_type\n\t\tfield_names = self.field_names\n\texcept AttributeError:\n\t\tlog.error(\"Failed to locate observer_type in generate_default_observer_schema_dict\")\n\t\treturn { }\n\tif manifest_dict is None:\n\t\treturn { }\n\telse:\n\t\treturn {\n\t\t\tself.generator.create_record(field_name, None, {\"name\":\"{}\".format(field_name)})\n\t\t\tfor field_name in field_names\n\t\t}"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tnew = {}\n\tfor k in list(manifest_dict):\n\t\tif not k in new:\n\t\t\tif not first_level and k in [key for key in ['resourceSpec', 'observedObjects', 'desiredObjects', 'name']]:\n\t\t\t\tcontinue\n\t\tif not is_list_dict(k):\n\t\t\traise ValueError('The {} field in manifest_list of {} is not a valid {}'.format(k, manifest_dict))\n\t\tnew[k] = copy.deepcopy(manifest_dict[k])\n\n\treturn new\n\n// Copyright ("}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tobserver_schema = {}\n\tif isinstance(manifest_dict, dict):\n\t\tfor k,v in manifest_dict.items():\n\t\t\tif isinstance(v, dict):\n\t\t\t\ttemp = generate_default_observer_schema_dict(v, first_level=first_level)\n\t\t\t\tif isinstance(temp, dict):\n\t\t\t\t\ttemp.update(v)\n\t\t\t\telse:\n\t\t\t\t\ttemp.update(None)\n\t\t\t\tobserver_schema[k] = temp\n\telif isinstance(manifest_dict, list):\n\t\tfor entry_dict in"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tif not isinstance(manifest_dict, dict):\n\t\tmanifest_dict = generate_default_observer_schema_dict(manifest_dict)\n\t\n\treturn {\n\t    'name': manifest_dict['metadata']['name'],\n\t    'namespace': manifest_dict['metadata']['namespace'],\n\t    'first_level': first_level,\n\t    'creation_timestamp': None,\n\t    'creation_timestamp_sec': 0,\n\t   'spec': {\n\t        'object': manifest_dict.copy(),"}]}
{"task_id": "62b869ebb4d922cb0e688cc6", "prompt": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor response_item in last_applied_manifest:\n\t\tif len(response_item) <= 0:\n\t\t\tcontinue\n\t\tresponse_item = response_item[0]\n\t\tself.last_applied_manifest_dict_list = response_item[\"last_applied_manifest\"]\n\t\tfor observer_schema in response_item[\"observer_schema\"]:\n\t\t\tif len(observer_schema[\"schema\"]) <= 0:\n\t\t\t\tcontinue\n\t\t\tfor observer_schema_item in observer_schema[\"schema\"]:\n\t\t\t\tlast_applied_index_list ="}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tprint(\"[red(\"[red]--------------------Last-applied Manifest-------------------------------------------------[reset]\")\")\n\tfor k, v in last_applied_manifest.items():\n\t\tprint(\"%s: [%s] \" % (k, v))\n\tprint(\"--------------------Last-applied Manifest-------------------------\")\n\tlast_applied_manifest.setdefault(\n\t\t\"name\", \"last-applied-manifest\")\n\t\n\tk_list = last_applied_manifest\n\t\n\tif len(k_list)!= len(observer_schema):\n\t\tprint_error(\"[red][red]\" + \""}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in response['items']:\n\t\tif 'lastAppliedManifest' in field:\n\t\t\tif 'creationTimestamp' in field['lastAppliedManifest']:\n\t\t\t\tcontinue\n\t\t\tif field['lastAppliedManifest']['creationTimestamp'] == '':\n\t\t\t\tfield['lastAppliedManifest']['modificationTimestamp'] = None\n\t\t\t\tlogger.debug(\n\t\t\t\t\t'No fields in lastAppliedManifest, will be set to zero')\n\t\t\tfield['lastAppliedManifest']['modificationTimestamp'] = None\n\t\telse:\n\t\t\tif f'{field[\"lastAppliedManifest\"][\"selfLink\"]}' in"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\ttry:\n\t\tlast_applied_manifest = str_to_datetime( \\\n\t\t\tlast_applied_manifest).replace(tzinfo=dateutil.tz.gettz(tz))\n\texcept Exception as e:\n\t\tpass\n\telif len(last_applied_manifest) == 0:\n\t\treturn None\n\n\tlast_applied_manifest_list = []\n\n\tfor k, value in list(last_applied_manifest.items()):\n\t\tif isinstance(value, datetime):\n\t\t\tlast_applied_manifest_list.append(\n\t\t\t\t"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest = response.get('lastAppliedManifest', {})\n\tif last_applied_manifest is None:\n\t\treturn\n\t# If this was a recursive update, re-initialize last_applied_manifest_dict\n\tif type(last_applied_manifest) == dict and 'lastAppliedManifest' not in last_applied_manifest:\n\t\tlast_applied_manifest = last_applied_manifest.copy()\n\t\tlast_applied_manifest.update(response.get('lastAppliedManifest', {}))\n\tobserver_schema = response.get('observerSchema',"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in list_of(last_applied_manifest):\n\t\ttry:\n\t\t\tlast_applied_manifest[field].update({\n\t\t\t\tk: v\n\t\t\t\tfor k,v in update_last_applied_manifest_dict_from_resp.items()\n\t\t\t})\n\t\texcept ValueError as e:\n\t\t\tprint(e)\n\t\texcept:\n\t\t\tprint('Error while updating value for \\n%s', field)\n\t\telse:\n\t\t\tlast_applied_manifest.pop(field)\n\treturn last_applied_manifest"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\ndef _init_last_applied_manifest_list():\n\tlast_applied_manifest_list = []\n\tlast_applied_manifest = None\n\tfor k, v in kfp.fields.items():\n\t\tk = k.replace(V1_PREFIX, \"\")\n\t\tnew_k = k\n\t\tif isinstance(v, str):\n\t\t\tnew_k = k.replace(V2_PREFIX, \"\")\n\t\t\tlast_applied_manifest = last_applied_manifest_dict_from_resp(last_applied_manifest, str(v))"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfrom lxml.xpath import set_last_applied_manifest_dict_from_resp\n\n\tfor res in observer_schema:\n\t\tif isinstance(res, dict):\n\t\t\tif res[\"last_applied_manifest\"]:\n\t\t\t\tset_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\tres[\"last_applied_manifest\"],\n\t\t\t\t\t\tres[\"result\"][\"last_applied_manifest\"])\n\t\t\telse:\n\t\t\t\tset_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\tres[\"result\"][\"last_applied_manifest\"],\n"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest=[]\n\tfor m in last_applied_manifest_data.get_all_items():\n\t\tkey = \"last_applied_manifest.%s\" % m \n\t\tif key not in last_applied_manifest:\n\t\t\tlast_applied_manifest.append(key)\n\t\t\n\t\tvalue = m[m.start_with(\"last_applied_manifest_\"):m.start_with(\".last_applied_manifest_\")].replace(\" \", \"\")\n\t\tif not \",\" in value:\n\t\t\t# a single value doesn't include a comma,"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest=None\n\tlast_applied_manifest_field=None\n\tif hasattr(response, \"items\"):\n\t\tfor item in response.items:\n\t\t\tif item[0].endswith(\".last_applied_manifest\"):\n\t\t\t\tlast_applied_manifest=item[1][:-7]\n\t\t\t\tlast_applied_manifest_field=item[1][:field_len]\n\tif last_applied_manifest:\n\t\t# update last_applied_manifest in-place\n\t\tlast_applied_manifest_list=[]\n\t\tfor item"}]}
{"task_id": "62b869eab4d922cb0e688cc5", "prompt": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest = {}\n\tobserver_schema = {}\n\tresponse = {}\n\tfor field in K8sResourcesStatus.last_applied_manifest.__fields__:\n\t\tif field[:4]!= 'json.data.k8s-resource-observed.':\n\t\t\tif field == 'name':\n\t\t\t\tname = field[4:]\n\t\t\t\traise KeyError('Observed field to update is present before'\n\t\t\t\t             'the observed field is present in the response'\n\t\t\t\t        "}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tdef __init__(self):\n\t\tself.last_applied_manifest_was_not_added_to_response = False\n\t\tself.observer_schema_was_not_added_to_response = False\n\t\tself.response_was_not_added_to_response = False\n\t\tself.last_applied_manifest = None\n\t\tself.observer_schema = None\n\t\tself.response = None\n\t\t\n\t\tfor key in self.last_applied_manifest.keys():\n\t\t\tif key not in self.last_applied_manifest_"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfrom kubernetes.client import CustomObjects as ObjectMeta, RESTClient\n\n\tfrom core.config import Config\n\tfrom core.error_handling import error_resp\n\n\terror = error_resp(\"Unhandled fields in the response\")\n\tfields = error.get(\"unexpected_fields\") or dict()\n\tif \"error\" in fields and fields[\"error\"]!= \"observed_fields\":\n\t\traise KeyError(f\"Could not go backwards through updated fields in {error}\")\n\n\tcurrent_list = copy.deepcopy(latest_list)\n\n\tif \"config\" in fields:\n\t"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tpass\n\ndef update_last_applied_manifest_list_from_resp(\n\t# return:\n\tresp_obj,\n\t\n\t# return:\n\t[],\n\t\n\t# return:\n\t[],\n\t\n\t# return:\n\t[],\n\t\n\t# return:\n\t[],\n\t\n\t# return:\n\t[],\n\t\n\t# return:\n\t[],\n\t\n\t# return:\n\t[],\n\t\n\t# return:\n\t[],\n\t\n\t# return:\n\t[],\n\t\n\t# return:\n\t[],\n\t\n\t# return:\n\t[]"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor key, item in response.items():\n\t\titem_type = item.get('type')\n\t\tif key not in __all__['last_applied_manifest']:\n\t\t\t__all__['last_applied_manifest'][key] = {}\n\t\tif not(item_type =='structure' and key!='response'):\n\t\t\tcontinue\n\t\tresponse_item_type = item.get('type')\n\t\tif item_type in ['structure','map']:\n\t\t\tif key == 'items':\n\t\t\t\t__all__['last_applied_"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t)\n\tlast_applied = last_applied.replace(\"latest\", None)\n\tlast_applied_manifest.update(\n\t\tdict(source[\"spec\"])\n\t)\n\tif not last_applied_manifest[\"spec\"]:\n\t\treturn True\n\tif \"apiVersion\" not in last_applied_manifest:\n\t\tlast_applied_manifest[\"apiVersion\"] = \"v1\"\n\tif not last_applied.get(\"kind\") == \"ConfigMap\":\n\t\tlast_applied[\"kind\"] = \"ConfigMap\"\n\tlast_applied[\"data\"] = update_last_"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Updates the last applied manifest dict from a REST response\")\n\tparser.add_argument(\"last_applied_manifest\", metavar=\"\",\n\t    action=\"STORE\", type=str, dest=\"last_applied_manifest\", required=False, help=\"Partial response of one update operation.\")\n\tparser.add_argument(\"observer_schema\", metavar=\"\",\n\t    action=\"STORE\", type=str, dest=\"observer_schema\", required=True, help=\"Computed field of the updated object in last_applied_manifest\")\n\tparser.add_"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\twith tempfile.NamedTemporaryFile(\n\t\tdelete=False, prefix=\"kubestart-\", dir=\"./\", suffix=\".yaml\"\n\t) as fp:\n\t\tfile_yaml = yaml.safe_load(fp.name)\n\t\n\t\tif \"last_applied_manifest\" in file_yaml:\n\t\t\tfile_yaml[\"last_applied_manifest\"] = json.loads(\n\t\t\t\tf\"---\napiVersion: kubeapps.kubestart.io/v1\nkind: InitConfiguration\nmetadata:\n  name: {str(last_applied_manifest[\"name"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t# TODO: This is a little bit ugly. It only works for now. But, it gets in the way of handling\n\t#       recursive calls\n\t# TODO: This is pretty silly. We should split up the logic into a function instead of passing the\n\t#       actual response\n\tif \"error\" in response:\n\t\traise KeyError(response[\"error\"])\n\telif \"last_applied_manifest\" in response:\n\t\tlast_applied_manifest_dict = {}\n\t\tif \"last_applied_manifest\" in response"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest = last_applied_manifest.get(\"last_applied_manifest\", None)\n\t\n\tif last_applied_manifest is None:\n\t\tlast_applied_manifest = {}\n\t\n\t# Check all observed field from each of the last applied manifest\n\tfor key in last_applied_manifest.keys():\n\t\tif key not in observed:\n\t\t\t# Check if value exist in the returned dict\n\t\t\tprint(f\"The key '{key}' is not present in 'last_applied_manifest` for deployment and is empty\")\n\t\t\traise KeyError("}]}
{"task_id": "62b869eab4d922cb0e688cbf", "prompt": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tdef to_json(obs):\n\t\tfrom kodiak.core.attributes import (\n\t\t\tKObject,\n\t\t\tKSchema\n\t\t)\n\t\tfor k in obs.__dict__:\n\t\t\tif isinstance(obs, KObject):\n\t\t\t\tcontinue\n\t\t\tif isinstance(obs, KSchema):\n\t\t\t\treturn obs.to_json([k])\n\t\treturn []\n\t\t\n\tdef serialize_default_observer_schema(app, obs):\n\t\tif app.default_observer_schema is None:\n\t\t\to = {}\n\t\t\t"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tjson_schema = generate_custom_resource_schema(app)\n\treturn json_schema\n\ndef generate_custom_resource_schema(app):\n\t\"\"\"\n\tGenerate a default custom observer schema for the ``spec.custom_resource`` resource.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application):\n\t    \n\t        The specific application for which a custom observer schema\n\t        has been custom-provided\n\t\"\"\"\n\tjson_schema = {}\n\tfor name, resources in app."}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tkrister = app.rangerapp.application\n\tfor v in get_valid_vlist():\n\t\tif not str(v.name).startswith('kube-'):\n\t\t\tcontinue\n\t\tif v.namespace!= kube_namespace:\n\t\t\tcontinue\n\t\tif v.version not in kubernetes_versions:\n\t\t\tcontinue\n\t\tif v.version == kubernetes_versions[v.namespace][kubernetes_versions[v.namespace]['latest']]:\n\t\t\tcontinue\n\t\tfor k in kubernetes_version_changes['latest']:\n\t\t\tif not k == v"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t# generate the default spec\n\tdefault_observer_schema_resource_paths = []\n\tfor directory in app.spec.get_resources(\n\t\tResourceClass.DIRECTORY):\n\t\tdefault_observer_schema_resource_paths.append( \n\t\t\t(app.manifest[\n\t\t\t\tResourceClass.DIRECTORY].get_path(DirectoryResourcePaths.KRAKEN_OBSERVER_SCHEMA_RESOURCE_PATH) /\n\t\t\t\tdirectory.get_name())\n\t\t)\n\t\t\n\t# if the user has explicitly defined a non-empty schema path,\n\t# add an empty one"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tconfig = kr_config.KrakenConfig()\n\tschema = config[\"schemas\"][\"kube_default_deployment\"].get()\n\tfor resource in app.k8s.get_resources(include_resources=True):\n\t\tresource_kind = resource[\"kind\"]\n\t\tif resource_kind == \"PersistentVolumeClaim\":\n\t\t\tresource_name = resource['metadata']['name']\n\t\t\tif resource_name == \"kube-resources\":\n\t\t\t\tk8s_dict = {\"resource\": resource}\n\t\t\t\tschema[\"name\"] = \"observer.configmap.yaml\"\n\t\t\t\t"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tif app.config.global_observer.enabled:\n\t\treturn app.config.global_observer.default_model_resource_schema_path(\n\t\t    app.kubernetes_config.managed_resource_schema_path(\n\t\t    app.config.global_observer.enabled))\n\telse:\n\t\treturn None\n\ndef default_field_schema(app, schema_path):\n\t\"\"\"\n\tGenerate the default GraphQL field schema. Takes each field under a given\n\tpath, and then returns it as a dict or a GraphQL query.\n\t\n\tArgs:"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tapp_spec = app.spec.as_dict()\n\tapp_spec['resource_version'] = '6'\n\tapp_spec['annotations'] = app.spec.config.annotations\n\tapp_spec['kube_controller'] = apps.get_kube_controller(app)\n\tif app.custom_observer_schema is not None:\n\t\tapp_spec['custom_watch_schema'] = app.custom_observer_schema\n\telse:\n\t\tapp_spec['custom_watch_schema'] = generate_default_custom_observer_schema(app)"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tdefault_schema = {\n\t\t\"type\": \"object\",\n\t\t\"additionalProperties\": False,\n\t\t\"properties\": {\n\t\t\t\"kubecarrierconfiguration\": {\n\t\t\t\t\"type\": \"object\",\n\t\t\t\t\"properties\": {\n\t\t\t\t\t\"clientcrdtypes\": {\n\t\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\t\"minItems\": 1,\n\t\t\t\t\t\t\"items\": {\n\t\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\t\"enum\": [\n\t\t\t\t\t\t\t\t\"Verrazzano\",\n\t\t\t\t\t\t\t\t\"Watchee\"\n\t\t\t\t\t\t\t"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tfor _, obj in app.get_items(name='observer-schema'):\n\t\tif obj.get_item_attributes(name='include-type'):\n\t\t\tobj.set_item_attributes(\n\t\t\t\tname='include-type',\n\t\t\t\tdata[0] ='string',\n\t\t\t)\n\t\n\tdef generate_default_scheduler_schema(app, obj_list):\n\t\tif not obj_list:\n\t\t\treturn ''\n\t\t\n\t\treturn app.get_item(\n\t\t\tname='scheduler-schema',\n\t\t\tquery=dict("}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tapp_schemas = app.config[\"schema\"].get(\"kubernetes.schema\").split()\n\t\n\tfor _, app_schema in app_schemas:\n\t\tif app_schema == 'k8s':\n\t\t\tcontinue\n\t\t\n\t\tschema = app_schema + '.json'\n\t\t\n\t\tif schema not in _DEFAULT_EMITTER_SCHEMA_KEYS:\n\t\t\tyield schema, \"yaml\"\n\t\t\n\t\tif schema in _CUSTOM_DEFAULT_OBSERVER_SCHEMA_KEYS:\n\t\t\tschema_key = schema + \".yaml\"\n\t\t\tcustom_schema = _CUSTOM_"}]}
{"task_id": "62b43427903eeb48555d3ea5", "prompt": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\traise NotImplementedError\n\t\n\t\n\t@staticmethod\n\tdef validateQuery(*sql):\n\t\t\"\"\"\n\t\tGiven its input ``sql``, validates that this is the correct type.\n\t\t\n\t\tReturns one or several errors.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\t\n\t\n\tdef prepareSQL(self, sql):\n\t\t\"\"\"\n\t\tPrepares the query for the given SQL to be run, and returns the data\n\t\tit contains.\n\t\t\"\"\"\n\t\tif isinstance(sql, _bytes_type):\n\t\t\tsql = b_(sql)\n\t\tsql_"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tif isinstance(sql, bytes):\n\t\tsql = sql.decode('utf-8')\n\tsql = sql.decode(encoding,'replace').translate(_encodings)\n\t\n\tif isinstance(params, Mapping):\n\t\tout_params = dict()\n\t\tfor key, value in params.items():\n\t\t\tif isinstance(value, str):\n\t\t\t\tvalue = value.encode('utf-8')\n\t\t\tout_params[normalize_string_for_map(key)] = normalize_string_for_map(value)\n\telif isinstance(params, Sequence"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tpass\n\t\n\t\n\t\n\t\n\t# do not import this module directly, pass through to the sqlparse module\n\tfrom sqlparse import sqlparse, sql\nexcept ImportError as e:\n\t__import__('sqlparse')\n\tpass\n\n\n\nimport codecs\nimport fnmatch\nimport functools\nimport importlib\nimport logging\nimport re\nimport time\n\nlogger = logging.getLogger(__name__)\n\n\nclass SQLParseError(Exception):\n\t\"\"\"Base class for exceptions raised on parsing/querying\"\"\"\n\n\nclass QueryParserSQLParseError(SQLParseError):\n\t\"\"\"Raised whenever"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tdef __init__(self, sql, *params):\n\t\tself.sql = sql.strip()\n\t\tif '?' not in sql:\n\t\t\tsql_params, = sql.split('?', 1)\n\t\t\tself.params = dict(zip(get_params(sql_params.split('&'), '?'), get_params(sql_params.split('?'), '?')))\n\t\t\tself.default_params = dict(sql_params.split('?', 1)[:-1])\n\t\telse:\n\t\t\tself.params = dict(dict(zip(get_"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t@doc_utils.memo\n\tdef sql(self):\n\t\tif not self.SQLParams.in_style:\n\t\t\tself.convert_params()\n\t\treturn sql_text(self.sql)\n\n\t@doc_utils.memo\n\tdef params(self):\n\t\t\"\"\"\n\t\tThe converted out-style parameters.\n\n\t\t:rtype:  ``dict``\n\t\t\"\"\"\n\t\tparams = {\n\t\t\tk: v\n\t\t\tfor k, v in self.params.items()\n\t\t\tif not getattr(k,"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\treturn _convert(sql, params, format_kw,\n\t\t\t\t   _from_result(params), *format_kw_params, **kwargs)\n\n\ndef _from_result(params):\n\t\"\"\"\n\tConvert the parsed parameters into a dict of values.\n\t\"\"\"\n\t_validate_keys(params, tuple(), (\n\t\t('name', SQL_PARAMS),\n\t\t('type', SQL_PARAMS),\n\t))\n\tres = dict((v, k) for k, v in params.__dict__.items())\n\tkey       ="}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tpass\n\nclass SQLConnection(object):\n\t\"\"\"\n\tThe most typical SQL query in our application.\n\t\n\tIt has some SQL statements which are explained here in the :ref:`query\n\tintroduction`.\n\t\n\tThe most common usage is to create the :class:`~sqlalchemy.engine.interfaces.ResultProxy`\n\tobject, set up this class, and call the :meth:`~sqlalchemy.engine.Executor.execute` and\n\t:meth:`~sqlalchemy.engine.Executor.executemany` virtual methods to create an\n\tobject which can perform certain SQL statements.\n"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tmsg_format = '%(message)s'\n\t\n\tin_style_params = None\n\tif _format_parameter_style(SQLParams.in_style):\n\t\tin_style_params = [param_class(key) for param_class in SQLParams.in_style.keys()]\n\telif _format_parameter_style((SQLParams.in_style.lower(),)):\n\t\tin_style_params = [key for key, value in _SQLParams(SQLParams.in_style, parameter=parameter).items()]\n\t\n\tout_style = {'"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tparser = parse(\" \".join(sys.argv))\n\n\tformatter = {\"in_style\": _in_params_style, \"%d\": _in_params_style}\n\t\n\tif \"sql\" in parser.parse():\n\t\tsql = parser.pop(\"sql\")\n\t\tparams = set(parser.pop(\"params\"))\n\t\tcolumns = [(p, parser.pop(\"col_format\")) for (p, l) in parser.parse(\"columns\", l=True)]\n\t\tformatted = \"%s %s\" % (format(sql, columns, False), format"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tif SQLParser.PY2:\n\t\treturn sql_formatter.format_sql_to_py2(*args, **kwargs)\n\n\ttry:\n\t\tparser = SQLParser()\n\t\tparser.text = sql_formatter.text\n\t\tparser.delimiter = sql_formatter.delimiter\n\t\tparser.in_style = sql_formatter.in_style.upper()\n\t\tSQLParams = dict(args or ({},), **kwargs)\n\t\tSQLParser.parse(sql_formatter, *args, **kwargs)\n\t\treturn parser.get_converted"}]}
{"task_id": "62b43428903eeb48555d3eaa", "prompt": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tpass\n\n\n@format.add(\n\t\"\"\"\n\tApply SQL queries.\n\t\n\tNote: The following rules apply a specific SQL query or SQL string.\n\t\n\t*sql* (*OR* *AND* *NOT* *LIKE* *IN*)\n\t\n\t\t*sql* can be either a single SQL expression (as a string or\n\t\tas an expression) or an SQL string. (SQL strings are not\n\t\ttranslatable.)  In addition, :func:`SQL` can take an extra\n\t\t:class:`.QueryArgs` object, optionally containing parameters."}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tsql = sqlparser.parse(sql)\n\t# Set up the 'out_style' dict.\n\tall_out_style_values = [None]\n\t# Store in separate dict from out styles - this dict will only be used for\n\t# out attributes, not out style set values.\n\t# NOTE:  This is done in the first case of `many_params`.\n\tout_style_values = dict()\n\t# Check the 'out_style' dict for this query, and warn about it.\n\tsql_paramtypes, sql_paramnames = _get"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tpass\n\n\nclass SQLParams(object):\n\t\"\"\"\n\tSQL parameters (:class:`str` or :class:`int` etc.) are attributes of\n\tthe *params* (:class:`~models.SQLParams`) that are available at\n\trequest time. They are the primary source of the\n\t:class:`models.DBAPIQueryParameters` that are part of the\n\t*query* (not part of the ``query_args``).\n\t\"\"\"\n\tdef __init__(self, **kwargs):\n\t\tself.query_args = kwargs.pop('query_args', {})\n"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tmatch = re.match(r\"\\s\",sql)\n\tif match is None:\n\t\traise ValueError(\"The supplied query was empty.\")\n\tout_style = match.groupdict()\n\tif any(out_style.keys() < 1 for param in params.values()):\n\t\traise ValueError(\"Input query contains no parameters.\")\n\tif many_params is not None:\n\t\tout_style.update(many_params)\n\n\tall_params = []\n\tout_style_iter = itertools.chain(params.values(),out_style.values())"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tif isinstance(sql, (bytes, str)):\n\t\tsql = sql.decode(\"utf8\")\n\tparams = {}\n\titerator = zip(\n\t\tlist(_to_dict(sql)),\n\t\tlist(map(_asarray, params.items())),\n\t)\n\treturn (list(it) for it in itertools.zip_longest(*iterator, fillvalue=fillvalue))\n\n\ndef formatone(str_):\n\t\"\"\"Return a SQL query with a single parameter\n\n\tThis is the only function in use in the SQLAlchemy\n\t:"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tif not _connection_initialized:\n\t\traise ImproperlyConfigured(\"SQLite3Dialect is not initialized yet\")\n\t_ = prepare_parameters.formatmany\n\tif _:\n\t\tparam = {\"(\": {}, \"(\": {}, \")\": {\"!\": True, \"\": True}}[many_params]()\n\n\t\tparam.update(prepare_parameters.format(_))\n\telse:\n\t\tparam = prepare_parameters.formatmany\n\treturn tuple(formatargspec(*pargs) for pargs in params_format.values() if pargs.startswith(\"sql\") else"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t# This is used in the ``sql_to_params`` helper. We need to escape the\n\t# backslashes in the end of the string so we need to add a \"\\\\'\"\\'\n\t# prefix.\n\tSQLQuery = sql.escape(sql).replace(\"\\'\", \"\\\\'\").replace(r\"\\[\", r\"\\[\").replace(r\"\\]\", r\"\\]\")\n\t\n\tsql_args = [SQLQuery]\n\t\n\tif many_params:\n\t\tsql_args += many_params\n\t\n\tif map_params:\n\t\tfor name, values in many_"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tsql, many_params = _parse_sql(sql)\n\treturn many_params if many else []\n\n\ndef formatlist(\n\tlist_input,\n\tseparator=\", \"\n) -> Generator[tuple[Union[bytes, str], str], None, None]:\n\t\"\"\"\n\tIterate over input ``collections.abc.Sequence`` (but not ``Sequence[str]``).\n\t\"\"\"\n\n\tfor item in list_input:\n\t\tif isinstance(item, str):\n\t\t\tlist_input.append(_escape_value(item))\n\t\t\tlist_"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tsql_params = SQLParams\n\treturn _make_query(sql_params=sql_params)\n\n\ndef get_parameterid(\n\tf, paramtype, paramtype_or_class, columnname, column_name_or_param_name, \\\n\tparam_type_or_class\n):\n\t\"\"\"\n\tGets an expression for a parameter using :attr:`f.columns`\n\tand :attr:`f.columns.columns`, e.g. :const:`sqlalchemy.func.param`.\n\t\t\n\tThe column name is a `sqlalchemy.sql"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tif 'SQL':\n\t\tsql = sql.upper()\n\telif isinstance(sql, str):\n\t\tsql = list(sql)\n\t\tsql = [q.upper() for q in sql]\n\telif not is_iterable(sql):\n\t\tsql = [(None, None)]\n\t\tmsg = ('Please use a `sql` input argument to transform the '\n\t\t       'input parameters using :func:`~sqlalchemy.engine.resultset.Query.format` before using this '\n\t\t       'feature.')\n\t\t#"}]}
{"task_id": "62b45df05108cfac7f2109ce", "prompt": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\treturn validate_path(self.root, path)\n\n\n\ndef validate_path(self, path):\n\t\"\"\"\n\tCheck if `path` is a file, directory, etc.\n\t\"\"\"\n\tres = False\n\t\n\tis_file = False\n\tis_dir = False\n\tis_root = False\n\tfor root, subdirs, files in os.walk(path):\n\t\tfor fname in files:\n\t\t\tif is_dir:\n\t\t\t\tif fname == dir_name(root):\n\t\t\t\t\tis_dir = False\n\t\t\t\telif"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tself.log.debug(\"validate, path=%r\" % (path))\n\n\t# check the path's not empty\n\tif self.fileExists(path):\n\t\treturn True\n\n\t# error occurred\n\tself.log.debug(\"validate: %s not found\" % path)\n\treturn False\n\t\t\nclass OCFLObject(object):\n\t\"\"\"\n\tClass for interacting with the content of pyfs objects.\n\n\tProvides Python objects, including ocfl.OcflObject,\n\tand an iterator to iterate the contents with ocfl"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\treturn self.validate()\n\t\t\n\t\ndef read_object(file_or_obj):\n\t\"\"\"\n\tRead OCFL object.\n\t\"\"\"\n\tassert isinstance(file_or_obj, file)\n\twith open(file_or_obj, 'rb') as f:\n\t\tf.seek(0)\n\t\t# Skip BOM\n\t\tf.read(4)\n\t\tif file_or_obj.startswith(b'\\xef\\xbb\\xbf'):\n\t\t\tdata = f.read(file_or_obj.find(b'\\xff"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tif os.path.exists(os.path.join(path, 'osflc')):\n\t\tfsys.open(os.path.join(path, 'osflc'), 'r').close()\n\t\treturn True\n\treturn False\n\nclass OCFLDB(object):\n\t\"\"\"\n\tOCFL file dictionary management.\n\t\n\tThis class can be used to hold onto information about or\n\tlimit the file system, in a similar fashion to Plist.\n\t\n\tThis class reads the OS File List (OSEFS) file.  See the\n"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tif not self.IsContainer(path):\n\t\treturn False\n\tobj = self.FindObject(path)\n\tif isinstance(obj, File):\n\t\treturn False, \"Cannot import a file\"\n\tif not isinstance(obj, Object): \n\t\treturn False, \"Invalid OCFL object (Not a file, not a container)\"\n\tif hasattr(obj, \"Check\") and obj.Check():\n\t\treturn False, \"Object is read only\"\n\t\n\twarnings={}\n\t# Check if there is a \"parent\" attribute\n\tparent_path = self"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\ttry:\n\t\tself.__validate(path)\n\t\treturn True\n\texcept Exception:\n\t\treturn False\n\n############\n# Constants\n############\nclass FSPathError(Exception):\n\tpass\n\n###########\n# Class FSApi #\n###########\nif not FSPathError:\n\tclass FSApi(object):\n\t\t'''\n\t\tThis class provides FSApi functionality that enables to use FSI.\n\t\tIt was added as a place holder functionality to ease use process\n\t\tof FSI functionality. But this will be useful only for"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\ttry:\n\t\tf = open(path, 'r')\n\texcept Exception as e:\n\t\tprint(e)\n\n\t\treturn False\n\t\n\ttry:\n\t\tif self.type == 'OCFL':\n\t\t\tself.v['ocfl'] = _validate_ocs(f)\n\t\telif self.type == 'fsv':\n\t\t\tself.v['fsv'] = _validate_fsv(f)\n\t\tself._validate_valid()\t\t\n\t\tif True == self._valid:\n\t\t\tself.info['valid"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tif self.is_directory == True and os.path.isfile(path):\n\t\tfilename = os.path.split(path)[1]\n\t\t\n\t\tif '.pyfs/' in filename or filename == './':\n\t\t\treturn True\n\t\t\n\t\traise CfdValidationError(\"OCFL configuration error, '%s' at '%s' is not a valid OCFL file.\" %\n\t\t\t\t\t(self.type, self.path))\n\telse:\n\t\treturn True\n\ndef validate_type(self, ocf_type):\n\tif ocf_type"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tif not os.path.isdir(path):\n\t\tlog.warning(f\"Folder {os.path.dirname(path)} not found.\")\n\t\treturn False\n\telse:\n\t\tif os.path.exists(path) and os.access(path, os.R_OK):\n\t\t\treturn True\n\t\telif not os.path.exists(path):\n\t\t\tlog.warning(f\"Path {path} not found.\")\n\t\t\treturn False\n\t\telse:\n\t\t\treturn False\n\ndef _find_obfuscated_data"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tif not pyfs.exists(path):\n\t\treturn False\n\treturn validateOcflObject(self, path, False)\n\ndef validateOcflObject(object, path, warnifmissing=False):\n\tif \"path\" in object:\n\t\tpath = object[\"path\"]\n\tif \"name\" in object:\n\t\tif object[\"name\"]==\"\":\n\t\t\tprint \"Error: Can't validate without name.\"\n\t\telse:\n\t\t\treturn True\n\telif object[\"type\"]!= \"OcflObject\":\n\t\treturn True\n"}]}
{"task_id": "62b45df15108cfac7f2109dc", "prompt": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tinfo = self.status_map()\n\tif info:\n\t\treturn '%s%s' % (info, get_prefix(prefix, len(info)))\n\n\treturn '%s' % self.name\n\n\ndef _load_config_text(config_path, config_env=None, force_reload=False,\n                      env_prefix=None):\n\t\"\"\"\n\tLoads a configuration from an alternate path.\n\t\"\"\"\n\t# load configuration from a text file"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\treturn \"%s %s\" % (self.status_label, prefix.capitalize())\n\n\ndef has_error(self, message, line=-1, func=None):\n\t\"\"\"\n\tCheck if field is present.\n\t@param message: error message\n\t@param line: line number of error\n\t\"\"\"\n\tif not self.error and not message:\n\t\treturn True\n\tif message:\n\t\tif not func: func = self.field_error\n\t\tif func is not None: return func(line, message, [self.status])\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tif type(self) is None:\n\t\tlogger.debug('no validator created')\n\t\treturn None\n\t# logger(type(self))\n\tst = self.status()\n\tif len(st) == 0:\n\t\tif prefix:\n\t\t\tst = self.status(prefix)\n\t\telse:\n\t\t\tst = self.status()\n\tif prefix:\n\t\treturn '%s[%d]' % (self.__class__.name, st)\n\treturn '%s' % st\n\n\ndef validate(self, state):\n\t\"\"\"\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\treturn prefix + '[{:-60s} {}{}]'.format(\n\t\t\t\t\t\tstr(self.__class__.__name__), self.__class__.__doc__)\n\n\ndef validate(self, args=None):\n\t\"\"\"\n\tCheck the configuration.\"\"\"\n\tif self.isBuilt: # validate after instance creation\n\t\tstatus_str = '{:-60s} {}'.format(\n\t\t\t\t\t\tstr(self.__class__.__name__), self.__class__.__doc__)\n\t\tself.config.print_valid_msg(status_str)\n\n\t\tself.config.validate(args) #"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tstatus = self.status\n\tif status not in validators:\n\t\traise ValidationError(self, \"validation error: %s\" % status)\n\t\n\treturn prefix + \"%s: %s\" % (status_str(status),\n\t\t\t\t    validators[status](self))\n\t\n\ndef get_max_len(field_list):\n\t\"\"\"\n\tDetermine the maximum string length for the whole content of a field.\n\t\"\"\"\n\treturn max([len(val) for val in field_list])\n\n\ndef validate(field_list, min"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tresult = Status.name_str[(self.v_validator_status or self.v_committee_status) & V_STATUSES.VALID]\n\tif result == 'VALID' and self.v_is_valid:\n\t\treturn '%s: %s' % (prefix, status_to_str(self.v_validator_status))\n\telif not self.v_is_valid:\n\t\treturn '%s: %s' % (prefix, status_to_str(result))\n\treturn result\n\ndef message_str(self, prefix=False"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tcode = [\n\t\t\"\t\t\",\n\t\tprefix,\n\t\t\"\t\t_check\",\n\t\t\"\t\tresult = _get_status(\\n\",\n\t\t\"\t\t\",\n\t\t\"\t\tstatus = self.validate()\",\n\t\t\"\t\tif status == 1:\",\n\t\t\"\t\treturn _valid()\",\n\t\t\"\t\telse:\",\n\t\t\"\t\treturn _invalid()\",\n\t\t\"\t\tend\",\n\t\t\"\",\n\t\t\"\telse:\",\n\t\t\"\t\t_check\",\n\t\t"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tself.prefix = prefix + '- '\n\treturn '%s' % self.prefix + '%s' % self.validator_status\n\n/*\n * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n * \n * Licensed under the Apache License, Version 2.0 (the \"License\").\n * You may not use this file except in compliance with the License.\n * A copy of the License is located at\n * \n *  http://aws.amazon.com/apache2.0\n * \n * or in the \"license\" file accompanying this file. This file is distributed"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\treturn '%s %s' % (prefix, Status.value[self.status])\n\n\n\n######################################################################\n\n\ndef isname(msg):\n\t\"\"\"\n\tReturn True if given message has the appropriate attribute.\n\t\"\"\"\n\ttry:\n\t\tvalue = msg._unpack()\n\texcept AttributeError:\n\t\treturn False\n\treturn (not isinstance(msg, _MissingObject) and\n\t\t\t (isinstance(msg, str) \\\n\t\t\t\tor hasattr(msg, '__class__') \\\n\t\t\t\tor hasattr(msg, '__call__'))\n\t\t\t # XXX"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\treturn self.get_status_string(\n\t\t\tprefix='<span class=\"bold\">{0:d}</span>\\n <span class=\"italics\">{1:s}</span>'.format(\n\t\t\t\t\tself.status,\n\t\t\t\t\tself.prefix))\n\n\n# ---------------------------\n#   Common attributes\n# ---------------------------\n\nclass _CommonAttrs(object):\n\n\tdef __init__(self):\n\t\tself.attrs = {}\n\n\tdef get_required(self, attr):\n\t\t\"\"\"\n\t\tReturns value of attr in 'REQUIRED' case"}]}
{"task_id": "62b45df15108cfac7f2109dd", "prompt": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tres = \"%s: (\" % res\n\tif self.pass:\n\t\tres = res + \"PASSED: \"\n\tres = res + self.name + \" \"\n\tres = res + self.name + \" \"\n\tres = res + self.errors[0].message + \" \"\n\tres = res + prefix + \")\"\n\treturn res\n\n\ndef result_to_str(result):\n\tres = \"\"\n\tfor arg in result.args:\n\t\tif res.endswith(\",\"):\n\t\t\tres = res[:-1] + \" \" + arg"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn self.info.status(prefix).encode('utf-8')\n\n\ndef info(self, prefix=''):\n\t\"\"\"\n\tReturn info string representation.\n\t\"\"\"\n\treturn self.info.encode('utf-8','replace').rstrip()) % prefix or ''\n\ndef success(self, prefix=''):\n\t\"\"\"\n\tReturn success string representation.\n\t\"\"\"\n\treturn self.info.encode('br','replace').rstrip()) % prefix or ''\n\ndef warn(self, prefix='', name='', text=''):\n\t\"\"\"\n\tReturn warn string"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tstatus_dict = {\n\t\t'passed':        'yes',\n\t\t'skipped':       'no',\n\t\t'unexpected':    'no',\n\t\t'fatal':         'yes',\n\t\t'unsat'  :       'no',\n\t\t'unknown':       'no',\n\t\t'untested':      'yes',\n\t\t'invalid-tag' : 'yes'\n\t}"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\ttext = 'Validation'\n\tif self.failed():\n\t\ttext +='<b class=\"error\">{0}</b>'.format(self.failed_str())\n\n\t\tmsg = ''\n\telse:\n\t\tmsg = ''\n\n\treturn '{0}{1}{2}\\n{3}{3}{4}{3}'.format(prefix,\n\t\t\t\t\t\t text,\n\t\t\t\t\t\t status_str(self, prefix +''),\n\t\t\t\t\t\t self.msg,\n\t\t\t\t\t\t msg)\n\ndef _show_validator_failure_msg(validator_failures, message):\n\tif message"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tl = []\n\t\n\tassert(isinstance(prefix,string_types) or hasattr(prefix, 'value') or hasattr(prefix,'key')\n    \t       or hasattr(prefix, '__getitem__') or hasattr(prefix, '__iter__'))\n\tif isinstance(prefix, string_types):\n\t\tprefix = [prefix]\n\tfor entry in prefix:\n\t\tif isinstance(entry, string_types):\t\tentry = entry.value\n\t\tl.append('{:^17s}{:^17s}'.format(entry, status(entry"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tif not self.failed:\n\t\treturn '{name}: {status}'.format(name=prefix + self.__class__.__name__, status=self.status())\n\telse:\n\t\treturn '{name}: {status}'.format(name=prefix + self.__class__.__name__)\n\n/**\n  ******************************************************************************\n  * @file    stm32f7xx_it.c\n  * @brief   Interrupt Service Routines.\n  ******************************************************************************\n  *\n  * COPYRIGHT(c) 2018 STMicroelectronics\n  *\n  * Redistribution and use in source and binary forms, with or without"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tl = []\n\tfor v, l_ in self.__level_strings:\n\t\tif not v:\n\t\t\tcontinue\n\t\tif prefix!= '' and v[:3]!= prefix: # pragma: no cover\n\t\t\tcontinue\n\t\tl.append(f'{v}')\n\treturn f'{prefix}{l}'\n\n\n@lru_cache(maxsize=0)\ndef get_default_loglevel_values():  # pragma: no cover\n\t\"\"\"\n\tGet instance attributes and return a dict mapping log level names to\n\tlog level numbers as values in log."}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn_format = {'error': \"status\",\n\t\t\t\t\t'status': self.status_str()}\n\n\tif prefix:\n\t\treturn return_format['status'] + \": \" + prefix\n\n\treturn return_format['status']\n\ndef add_tags(context):\n\tfor result in context.results():\n\t\ttry:\n\t\t\tcontext.add_tags(result.data)\n\t\texcept ValueError:\n\t\t\tcontext.add_error(result.data, result.status_code, result.status_desc)\n\n\ndef get_results_"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tstatus_lines = []\n\tstatus_lines.extend(self.status_strs)\n\tif not self.info_lines:\n\t\tstatus_lines.append(self.test.msg % prefix)\n\tfor l in self.info_lines:\n\t\tstatus_lines.extend(l)\n\tif prefix:\n\t\tstatus_lines.append(prefix)\n\treturn '\\n'.join(status_lines)\n\n\ndef _is_string(obj):\n\treturn isinstance(obj, str)\n\ndef _is_unicode(obj):\n\t"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn \"%s%s\\n\" % (prefix, \" \" * len(prefix))\n\nusing System;\n\nnamespace BizHawk.Client.Common\n{\n\t[Flags]\n\tpublic enum Enums\n\t{\n\t\tNone = 0,\n\t\tUseFull = 1,\n\t}\n\n\t[Flags]\n\tpublic enum Enums2 : uint\n\t{\n\t\tNone = 0,\n\t\tUseFull = 1,\n\t}\n\n\tpublic class Enum\n\t{\n\t\t[Flags]\n\t\tpublic class ValueProvider\n"}]}
{"task_id": "62b45e135108cfac7f2109f4", "prompt": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\traise NotImplementedError() # pragma: no cover\n\n\ndef create_default_data_source_for_dataset(self, context, data_source_type):\n\t\"\"\"\n\t\tCreate a new data source for the default data source.\n\t\"\"\"\n\tpass # pragma: no cover\n\n\ndef create_data_source_from_file(self, identifier, path, data_source_type):\n\t\"\"\"\n\t\tCreate data source for the data source of the specified filename.\n \t\tReturns a path to the data source, and optionally, a data source type of the source."}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\n@register_type_to_id('{E35EDB7F-D01B-4929-9C3D-C33A14B5AC6D}')\nclass E35EDB7F(InterfaceBaseType):\n\t\"\"\"\n\tThe set of functions that this interface supports.  This includes both the list of functions as well as the dictionary of named arguments.\n\n\t\"\"\"\n\n\tdef __init__(self, ctx):\n\t\tsuper().__init__(ctx)\n\n\t\tself.funcs = {}\n\n\t\tself.name ="}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\ndef has_valid_signature(self, signature_identifier):  # pylint: disable=unused-argument\n\t\"\"\"\n\tReturn True if the given signature_identifier is a valid identifier of this\n\tclass, always True in this base implementation.\n\t\"\"\"\n\t# There's no default implementation, because I believe that there is no default impl or\n\t# impl implementation, but I couldn't find any reasonable example I could think of\n\t# (although it seems to be, I haven't figured out that it would be much easier to test\n\t# this way"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\traise NotImplementedError\n\n\n_CITATION_LINE = \"\"\"\\\nimport typing\n\nfrom mitmproxy import http, websocket, websocket_auth, websocket_errors, websocket_exceptions\n\nimport http.server as http_server\nimport websocket as ws\n\nfrom mitmproxy.tools.errors import NotFoundError\nimport mitmproxy.lib.extensions\nimport mitmproxy.lib.jsonschema\nimport jsonschema.validation\nimport mitmproxy.lib.filterset\nimport mitmproxy.lib.filterset.filters.base # pylint: disable=unused-import\nimport mitmpro"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\ndef _is_valid(identifier):\n\t\"\"\"\n\t\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn identifier is not None\n\n\nclass _Identity(object):\n\t\"\"\"Base class for identity.\"\"\"\n\t__slots__ = ()\n\n\tdef __new__(cls, identifier=None):\n\t\t\"\"\"\n\t\t\tConstructor for use with :py:class:`django.db.models.Model` and\n\t\t\t:py:class:`django.db.models.fields.Fields`.\n\t\t\t\"\"\"\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\traise NotImplementedError()\n\n#ifndef VIENNA_RNA_PACKAGE_H\n#define VIENNA_RNA_PACKAGE_H\n#ifdef VRNA_1_0_110\n#ifndef VRNA_WARN_DEPRECATED\n\n#  ifndef VRNA_WARN_DEPRECATED_RENAME\n#    define VRNA_WARN_DEPRECATED_RENAME\n#  endif\n\n#  /* we are deprecating VRNA_WARN_DEPRECATED_RENAME */\n#  if (VRNA_WARN_DEPRECATED == 1)\n#    undef VRNA_WARN_DEPRECATED\n#  "}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\traise NotImplementedError\n\n\nclass ID(unicode):  # pylint: disable=invalid-name\n\t'A unique identifier'\n\n\ndef is_valid(value):\n\tif isinstance(value, ID):\n\t\treturn value(value.lower())\n\telse:\n\t\treturn True\n\n\"\"\"\n    The MIT License (MIT)\n\n    Copyright (c) 2015 Andreas Marek and Contributors\n\n    Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files\n    (the \"Software"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\tpass\n\nimport os\nimport numpy as np\n\nfrom. import util\nfrom.util import import_util\nfrom. import data_util\nfrom.data_util import (\n    DataInterface,\n    DataMetaInfo,\n    DataInfo,\n    DataSplitter,\n    DataSplitterIterator,\n    BaseDataIterator,\n    BaseDataSplitterIterator,\n    UniqDataSplitterIter,\n    uniq_data_iterators\n)\n\n\nclass HDF5DataSplitter(DataSplitter):"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\ndef _is_valid(data):  # pylint: disable=invalid-name\n\tdata = re.sub(_r\"\\./\", \"/\", os.path.normpath(data))\n\tdata = re.sub(_r\".*[\\w\\.]\", \"(\\w|\\s)\", os.path.normpath(data))[0]\n\t# TODO: use more intelligent regex?\n\treturn bool(re.findall(_r\"\\/\\w+(\\.[^\\/\\w]+)?|\\*[\\w\\.]*$\", data))\n\n\nclass PathToImport(_Import"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\tret_val = True\n\treturn \"name\" in self.__metadata__\n\n/**\n * @author v.lugovsky\n * created on 16.12.2014\n */\n(function () {\n  'use strict';\n\n  angular\n   .module('firebotApp')\n   .component('firebotFilterMenu', {\n      templateUrl: 'partials/filter-menu.html',\n      controller: FirebotFilterMenuController,\n      bindings: {\n       "}]}
{"task_id": "62b45e145108cfac7f210a07", "prompt": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tself.inventory = self.inventory or inventory\n\tself.version = self.inventory.config.get(\"spec_version\")\n\tresult = self.version and ((self.version!= \"\") and (self.version == self.spec_version))\n\n\tif not result:\n\t\tself.spec_version = None\n\t\tself.inventory.config[\"spec_version\"] = None\n\n\t\t# TODO: Check if the inventory is actually a spec, and if so\n\t\t#       ignore the type\n\t\tresult = False\n\t\t\n\tif extract"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tif not inventory:\n\t\treturn False\n\tif extract_spec_version:\n\t\tspec_version, _ = inventory[\"description\"].rpartition(\"@\")\n\t\tif spec_version == None:\n\t\t\t# check for both a version and a version range\n\t\t\tspec_versions = tuple(self.spec_version)\n\t\t\tif len(spec_versions) == 0:\n\t\t\t\traise RuntimeError(\"Could not find a version range for %s.\" % self.name)\n\t\t\tif not spec_versions:\n\t\t\t\traise RuntimeError(\"Could not find a valid version"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tfor item in self.get_inventory(inventory):\n\t\tfor param in ITEM_PARAMETERS[item.type]:\n\t\t\tparam_type = get_param(item.type[param], item)\n\t\t\tif param_type not in PARAM_TYPES:\n\t\t\t\tself._log(LOG_ERROR, 'unknown item parameter %s (type %s)' % (item.path, item.type[param]), LOG_INVALID)\n\t\t\t\treturn False\n\treturn True\n\nclass InventoryManager(BasicManager):\n\t\"\"\"\n\tManager class defining the behavior of the inventory database that"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tself.spec_version = None\n\ttest_objects = None\n\tif not extract_spec_version:\n\t\tself.spec_version = inventory.spec_version\n\tif not self.setuptools and self.spec_version is not None:\n\t\ttry:\n\t\t\timport setuptools.extern.six as six\n\t\t\tspec_parser = setuptools.extern.six.moves.StringIO(self.spec_version)\n\t\t\tself.setuptools = True\n\t\t\tself.spec = specparser.parse_spec_string(spec"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tif extract_spec_version:\n\t\treturn self.extract_specversion\n\n\tinvalid_type_value = set()\n\tinvalid_type_name = set()\n\tfor obj in inventory:\n\t\tif type(obj) in self.valid_types:\n\t\t\tinvalid_type_value.add(obj.type)\n\t\t\tinvalid_type_name.add(obj.type_name)\n\n\tif invalid_type_value:\n\t\tif self.debug:\n\t\t\tprint 'Invalid type value. Please try one with a valid type value.'"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tfor item in inventory:\n\t\tif item.name.lower()!='minecraft_cooking_book':\n\t\t\ttry:\n\t\t\t\textracted_item = self.extract_inventory_item(item)\n\t\t\texcept ValueError:\n\t\t\t\ttry:\n\t\t\t\t\textracted_item = self.extract_inventory_item(item.parent_item.name)\n\t\t\t\texcept ValueError:\n\t\t\t\t\treturn False\n\t\t\texcept:\n\t\t\t\treturn False\n\t\telse:\n\t\t\textracted_item = item\n\t\t# Test the extractor to make sure"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tvalid = False\n\t\n\ttry:\n\t\tself.__validate_version(inventory, self.spec_version)\n\texcept UnprocessableEntity:\n\t\traise UnprocessableEntity(\"Inventory is not supported, not processing\")\n\telse:\n\t\tinventory.validate(self.spec_version)\n\t\t\n\t\tif not self.spec_version or not hasattr(self.inventory, \"spec\"):\n\t\t\tvalid = True\n\t\t\t\n\t\tspec = get_inventory_spec(inventory, self.spec_version)\n\t\t\t\n\t\tif extract_spec_version:\n\t\t"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tif self.spec_version:\n\t\tif extract_spec_version:\n\t\t\tself.spec_version = self.get_spec_version(\n\t\t\t\tself.spec_version)\n\t\tself.validate_items_are_equal(inventory,\n\t\t                              extract_spec_version=extract_spec_version)\n\t\n\telif len(self.items) > len(inventory):\n\t\traise Exception(\"Not enough items. Need to extract"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tif'version' not in inventory:\n\t\tself.fail(inventory, \"Missing version tag in inventory.\")\n\t\n\tversion = inventory['version']\n\tif version not in self.spec_version:\n\t\tself.fail(inventory, \"Invalid version tag in inventory. Expected %s, \"\n\t\t\t\"got %s. Error message: %s.\" %\n\t\t\t(self.spec_version[0], version, self.get_error_message(inventory)))\n\t\t\t\n\tif self.spec_version[1] < version:\n\t\tself.fail(inventory"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tif not isinstance(inventory, Inventory):\n\t\traise TypeError(\"inventory must be an instance of Inventory\")\n\tself.inventory = inventory\n\tself.extract_spec_version = extract_spec_version\n\t\n\t\ndef __init__(self, inventory, _):\n\twith _Context() as context:\n\t\tsuper(Inventory, self).__init__(inventory.version, context.dbstate, context)\n\t\tself.inventory = inventory\n\t\n\t\tself._inventory_type = inventory.type\n\t\t\nclass _Context:\n\t\"\"\"\n\t\tContext which encapsulates some"}]}
{"task_id": "62b45e145108cfac7f210a09", "prompt": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tdigest_index = 0\n\tfor manifest_file_name in manifest_files:\n\t\tif manifest_file_name!= self.manifest_name:\n\t\t\thashcat_hash = self.get_hash_from_file(str(manifest_file_name))\n\t\t\tif hashcat_hash == self.get_hash_from_file(digests_used):\n\t\t\t\traise ManifestMissingDigestError(\"Missing digest entry in manifests file %s\" %manifest_file_name)\n\t\telse:\n\t\t\tself.log().info(\"Manifest file \" +"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\tdigests = set()\n\t\ttry:\n\t\t\twith open(manifest_file) as f:\n\t\t\t\tfor line in f:\n\t\t\t\t\tdun = line.strip()\n\t\t\t\t\tif dun == '' or dun == '#':\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tdigests_for_line = filter(lambda _:\n\t\t\t\t\t\tdun in digests_used,\n\t\t\t\t\t\tset(digests))\n\t\t\t\t\tif len(digests_for_line) == 0:\n\t\t\t\t\t\tdigests.add("}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor filename in manifest_files:\n\t\ttry:\n\t\t\t_test_digest(filename, digests_used)\n\t\texcept digest.DigestError as e:\n\t\t\tself.log.debug(\"Manifest already present/used - skipping '%s': %s\",\n\t\t\t\tfilename, str(e))\n\t\t\t# Don't call hash in this case just to be sure that it will always fail in the case of a broken manifest\n\n\n\ndef _test_digest(filename, digests_used):\n\twith open(filename, \"r\") as file_obj:"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor p in manifest_files:\n\t\tif not self.get_file_md5(p).startswith('sha1:'):\n\t\t\tself.log_message('Invalid checksum type in'+ p + '. Expected \"sha1:\"')\n\t\t\tself.invalid_checksums += 1\n\t\t\tself.log_message('Invalid checksum present in'+ p + ', skipped')\n\t\t\tcontinue\n\t\t# if we have any file in list, but we can't verify (only first match)\n\t\tif any([x for x in manifest_files if not x.endswith"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\tmanifest_file_path = os.path.normpath(\n\t\t\tos.path.join(self.local_path, manifest_file)\n\t\t)\n\t\tdigests_used.add((manifest_file_path, f\"{manifest_file}/{manifest_file}.sha256\", None))\n\t\tdigests_used.add((manifest_file_path, f\"{manifest_file}.sha256\", \".gitignore\"))\n\n\tfor manifest_file in manifest_files:\n\t\tmanifest_file_path"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tif not any([digest in manifest_files.values() for digest in digests_used]):\n\t\traise NoSuchTag('Digests referenced but not listed: %r' % (digests_used,))\n\ndef get_digest_file(digest):\n\t\"\"\"\n\t\tReturn file containing Digest object for a digest.\n\t\"\"\"\n\tdigest_path = get_path_for_digest(digest, True)\n\tdigest_file = find_or_create_local_file(digest_path)\n\tdigest_file.load()\n\treturn digest_file\n\ndef"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor manifest_file, digest_set in DigestSet.manifests.items():\n\t\tcurrent_digests_used = set([file_name\n\t\t\t\t\t\t\tfor file_name in digest_set.files_to_check\n\t\t\t\t\t\t\tif manifest_file == file_name])\n\t\tif current_digests_used!= digests_used:\n\t\t\tprint(\"Digests used to check %s are invalid: %s\" % (\n\t\t\t\t\t\tdigest_set.manifest_filename,\n\t\t\t\t\t\tdigests_used))\n\t\t\traise ManifestNotFound(manifest_file=manifest"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor f in (os.path.join(self.data_dir, \"index.md\") for _ in range(self.metadata_checksum_version)):\n\t\tfhandle = open(f, 'rb')\n\t\tfor d in manifest_files:\n\t\t\t# check if file is a manifest\n\t\t\tif os.path.basename(f) == os.path.basename(d):\n\t\t\t\t# print \"Index file found\"\n\t\t\t\tself.debug(\"Index file found: %s\" % f)\n\t\t\t\tself.debug(\"Index content: %s\""}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t# Check digests present in the manifest that are needed\n\tif manifest_files == []:\n\t\tfor digest in self.config.get_digest_configs(self):\n\t\t\tif digest.config_digest_files!= []:\n\t\t\t\tdigest_file_list = digest.config_digest_files\n\t\t\t\t# Check all digests in the manifest that should be present and used\n\t\t\t\tself.config.digest_manager.present_manifest(digest_file_list)\n\t\t\t\tself.config.digest_manager.save_digest(digest)\n\t\t\t\t# Check"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tdigest_dir = self.create_tmp_dir()\n\tfor digest, used in digests_used.items():\n\t\tassert digest_dir.join(\"digests_used\").exists(), \"digest files already created\"\n\t\tfor path in os.listdir(digest_dir.join(\"manifests\")):\n\t\t\tnew_path = manifest_files[path]\n\t\t\tif not new_path:\n\t\t\t\tbreak\n\t\t\tif digest == new_path.join(\"digests_used\").join(\"digest\"):\n\t\t\t\traise ConfigError(\"Digest not present or"}]}
{"task_id": "62b45e165108cfac7f210a16", "prompt": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tif not isinstance(prior, InventoryValidator):\n\t\tself.error(\"%s: %s - expected an InventoryValidator instance but found %s\" \\\n\t\t           % (self.filename, self.linenum, repr(prior)))\n\telse:\n\t\tself.check_version(prior, '1')\n\t\n\t\t\n\t# Check that all required fields are specified\n\tfor name in self.required_fields:\n\t\tif not isinstance(prior[name], self.optional_fields[name]):\n\t\t\tself.error(\"%s: %"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tself_obj_prior = self.as_inventory_validator(prior.inventory_class, prior.data)\n\treturn prior_version_in_range(self_obj_prior, self)\n\t\n\t\n\t\nclass InventoryEntry:\n\t\n\tdef __init__(self):\n\t\tself.data = None\n\t\tself.priority = DEFAULT_PRIORITY\n\t\tself.data_type = DEFAULT_DATA_TYPE\n\t\tself.display_name = None\n\t\tself.display_name_hash = None\n\t\t\n\t\tself.inventory_class = None\n\t"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tif not(isinstance(prior, InventoryValidator)):\n\t\traise RuntimeError(\"%r is not an InventoryValidator, but %r\" % (prior, type(prior)))\n\tif self._version < prior.version:\n\t\traise RuntimeError(\"Inventory does not support this version: (%i) for \" +\n\t\t\t\t\t\t\t   self._version, self._version, prior._version)\n\n\tfor child_name in self.get_children(prior=\"\"):\n\t\tresult = self._get_children(item_name1=child_name, \n\t\t         "}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tif not self.valid_until:\n\t\tself.fail('[InventoryValidator.validate_as_prior_version] Expected valid \"until\" to have been parsed.')\n\tif not self.current:\n\t\tself.fail('[InventoryValidator.validate_as_prior_version] Expected valid current to be there.')\n\t\n\terror = None\n\tif not isinstance(prior, InventoryValidator):\n\t\tself.fail('[InventoryValidator.validate_as_prior_version] Expected InventoryValidator as prior.')\n\telif not prior.valid_until:\n\t\tself.fail('[Inventory"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tif not isinstance(prior, basestring) and not isinstance(prior, InventoryValidator):\n\t\treturn False\n\t\n\tvps = self.get_prior_versions()\n\t\n\tif prior not in vps:\n\t\tself._error(\n\t\t\t_('Inventor version is not in the inventory.') + '-' +\n\t\t\t\t_('Prior version: %s.' % str(prior))\n\t\t)\n\t\t\n\treturn vps.get(prior) is not None\n#\n#\tdef validate_version(self, version):\n#\t\t\"\"\"\n#\t"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tif not prior.has_method(self.validate_method):\n\t\tself.validate()\n\treturn prior.validate_method(self)\n\n/*\n * Copyright 2010 Google Inc.\n * \n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n * use this file except in compliance with the License. You may obtain a copy of\n * the License at\n * \n * http://www.apache.org/licenses/LICENSE-2.0\n * \n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tvalidator = InventoryValidator(prior)\n\tvalidator.validate()\n\tif not validator.check():\n\t\traise EInventory(self, 'prior.incorrect', 'Prior version is not a valid version of the current item.')\n\n\tinventory = validator.inventory\n\n\tif not inventory.can(config['plugin.inventory.validate_as_prior_version'], config['plugin.inventory.validate_as_prior_version.item_type']):\n\t\traise EInventory(self, 'prior_version', 'Prior version is not valid for current object of type [{}]'.format("}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tif \"version\" not in self.fields:\n\t\t# if self does not have a version, there is no prior\n\t\tif \"internal_reference\" not in prior and \"version\" not in prior.fields:\n\t\t\t# self is a required version\n\t\t\traise TypeError, \"'self' not found\"\n\t\tif \"internal_reference\" not in self:\n\t\t\t# self doesn't have a prior, but this shouldn't happen\n\t\t\traise ValueError, \"'self' has no 'prior' field, and there \" + \\\n\t\t\t      \"are no"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tvalidator = self._value_change_prior_to_object_validator(prior)\n\tif validator is None:\n\t\traise InvalidEquipmentVersionError(\n\t\t\tprior, _missing_check_name(prior, self.version, prior.name))\n\treturn validator\n\n\nclass InventoryValidator(object):\n\t\"\"\"\n\tA class for handling Inventory specific validation.\n\n\t@ivar _validator:\n\t\tThe parent inventory object to be used by this inventory validator, this\n\t\tattribute is set by the parent inventory as its :py:attr:`_validator`\n\t"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\treturn not self.check_prior_version(prior)\n\ndef validate_inventory(self, inventory, inventory_version = None):\n\t\"\"\"\n\t\tValid Inventory objects - return values are as in validate_as_inventory.\n\t\"\"\"\n\t# self is already verified for internal consistency.\n\treturn inventory\n\ndef validate_as_inventory(self, inventory, version = None):\n\t\"\"\"\n\t\tCheck Inventory objects.\n\t\"\"\"\n\treturn self.check_as_inventory(inventory, version)\n\ndef _create_node_from_uuid(name, uuid"}]}
{"task_id": "62b45e165108cfac7f210a17", "prompt": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tpath_to_files = get_path_to_files(inventory, version)\n\tlogical_path_to_files = get_logical_path_map(inventory, version)\n\t\n\tmap_path_to_files = {}\n\t\n\tfor version, state_file_name in path_to_files.items():\n\t\tstate_file_path = os.path.join(path_to_root, version, state_file_name)\n\t\tif state_file_path in logical_path_to_files:\n\t\t\tcontinue\n\t\tlogical_"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t# Get any versions that match that item name (if given)\n\tversion_match_found = False\n\ti = 0\n\tversions = [x[0] for x in version if isinstance(x, list)]\n\tversion_match = True\n\twhile version_match:\n\t\t# Get the next version in order of discovery \n\t\ti += 1\n\t\tversions = sorted(versions, key=lambda x: version[i])\n\t\tversion_match = any(version[i] for version in versions)\n\n\t# Load version data\n\tversion_data_"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tcontent_path_map = {path(path) for path in inventory.content_file_paths}\n\tcontent_files_dict = {}\n\tfor path in content_path_map:\n\t\tif version in inventory.content_file_versions:\n\t\t\tcontent_file_path = path.rsplit('/', 1)[-1]\n\t\t\tcontent_files_dict[content_file_path] = inventory.get_file_or_dir_and_copy(path)\n\t\n\treturn content_files_dict\n\n'''\ndef version_info(inventory, version"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tret = {}\n\tfor f in inventory:\n\t\tif not hasattr(f, 'physical_path'):\n\t\t\tcontinue\n\t\tif not (abs(f.physical_path) == abs(version)):\n\t\t\tcontinue\n\t\tif not f.physical_path in ret:\n\t\t\tcontinue\n\t\n\t\tret[abs(f.physical_path)] = set()\n\t\tpath_list = inventory[f.physical_path].get_logical_path()\n\t\t\n\t\tif len(path_list) > 1:\n\t\t\tpath_list.sort"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tlog.debug(inventory=inventory)\n\tlogical_path_in_state = {}\n\tfor content in order_by(inventory, 'logical_path'):\n\t\tlogical_path = content['logical_path']\n\t\tif version in content:\n\t\t\tfile_name_in_inventory = get_file_name_in_inventory(inventory, version, content)\n\t\t\tif file_name_in_inventory:\n\t\t\t\tlog.info('File \"%s\" found for version %s in inventory \"%s\"', file_name_in_inventory, version, inventory)"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tresult = set()\n\t\n\tfor dir in inventory.listdir():\n\t\tif version not in dir[\"version\"][\"inventory\"]:\n\t\t\tcontinue\n\t\t\n\t\tversion_path = os.path.split(dir[\"version\"][\"inventory\"][version][\"content_path\"])[0]\n\t\t\n\t\ttry:\n\t\t\tlogical_path = version_path.replace(version, \"\")\n\t\texcept UnicodeDecodeError:\n\t\t\tprint(\"Error: File paths are broken in version %d (name %s/%s)\" % (version, version_path, dir[\"name\"]))\n\t\t"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t# We do not cache if we could not find some files\n\tif inventory.should_use_filecache():\n\t\t# Return a cache\n\t\treturn inventory.get_version_cache(version=version)\n\t\n\t# Loop through looking in our cache\n\tcontent_files = {}\n\tfor key in inventory.VERSION_FILE_CACHE:\n\t\tfilename = os.path.join(inventory.VERSION_FILE_CACHE[key], version+\".json\")\n\t\tif os.path.exists(filename):\n\t\t\tcontent_files[key] = json.loads("}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tif isinstance(inventory, tuple):\n\t\tfiles_in_version = set()\n\t\tversion = inventory[0]\n\t\tfor file_spec in inventory[1:]:\n\t\t\tfiles_in_version.add(get_logical_path(file_spec, version))\n\telse:\n\t\tfiles_in_version = inventory.get_all_files()\n\t\t\n\tlogical_path_in_state = {}\n\tfor rel_path in files_in_version:\n\t\tsplit_path = rel_path.split(os.sep)\n\t"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tdef logical_path_map(content_files, version):\n\t\tif version in contents:\n\t\t\treturn contents[version]\n\t\telse:\n\t\t\tif not content_files:\n\t\t\t\treturn {}\n\t\t\tfiles = set()\n\t\t\tfor f in content_files: #XXX: optimize\n\t\t\t\trel_path = os.path.relpath(f._path, inventory.directory)\n\t\t\t\trel_name = os.path.basename(rel_path)\n\t\t\t\tfiles.add(rel_path)\n\t\t\t\tif version.startswith(\"__init__"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t#\n\t# Get the set of paths for each version in inventory of this package.\n\t#\n\tversion_contents = inventory[version]\n\t\n\t#\n\t# The set of logical paths for each version in inventory of this package.\n\t#\n\tlogical_paths = dict()\n\t\n\t#\n\t# Get the set of content files for the requested version\n\t#\n\tcontent_files = version_contents.get(version[1])\n\t\n\tif content_files is None:\n\t\t#\n\t\t# Return an empty set\n\t\t#\n\t\t"}]}
{"task_id": "62b45e175108cfac7f210a19", "prompt": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tself.logger.debug('Validating fixity block in inventory for path: %s', fixity.path)\n\tif fixity.is_valid:\n\t\tfor file_info in fixity.files:\n\t\t\tif not file_info['is_valid']:\n\t\t\t\tself.logger.warning('Error in fixity block: %s', file_info)\n\t\t\t\n\t\t# If there are files in the manifest don't reference them\n\t\tif not manifest_files:\n\t\t\tself.logger.warning('No manifest files referenced')\n\telse:\n\t\t"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tinventory = self.inventory\n\t\n\tif manifest_files:\n\t\tfixity_block_manifest = None\n\t\tfor manifest_entry in manifest_files:\n\t\t\tif self.ignore_file(manifest_entry['iname']):\n\t\t\t\tcontinue\n\t\t\t\n\t\t\tfixity_block_manifest = _filter_fixity_manifest(\n\t\t\t\tmanifest_entry['iname'],\n\t\t\t\tfixity_block_manifest\n\t\t\t)\n\t\t\t\n\t\t\tif self.ignore_file(fixity_block_manifest['manifest_file']):\n\t\t\t\tcontinue\n"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tsuper(Inventory, self).validate_fixity(fixity, manifest_files)\n\t\n\tassert fixity.data_type == \"module\"\n\t\n\tfor file_path in manifest_files:\n\t\t# Check that the file actually exist\n\t\tassert not os.path.isfile(file_path), \"Cannot import: %s in a non-empty directory: %s\" %(file_path, self.manifest_file)\n\t\t\n\t\t# Import to Python module:\n\t\timport module_tools.fixity_checker\n\t\t\n\t\tif module_tools"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tfor filename in self.inventory():\n\t\tif path.join(self.path, filename) == fixity.name:\n\t\t\tpath = path.join(manifest_files)\n\t\t\tpath.append(filename)\n\t\t\tinventory.run([self.path, filename], path)\n\t\t\tbreak\n\telse:\n\t\traise FileMissingError(path, filename)\n\t\t\n\treturn filename\n\n// <copyright file=\"BingMainDialog.cs\" company=\"App Metrics Contributors\">\n// Copyright (c) App Metrics Contributors. All rights reserved.\n// </copyright>"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tfor filename in list(fixity) + list(manifest_files):\n\t\tmanifestfile = open(filename)\n\t\tmanifesttext = manifestfile.read()\n\t\tmanifestfile.close()\n\t\tif '###' in manifesttext:\n\t\t\treturn False, filename\n\n\"\"\"\n\tParse fixity into a dictionary.\n\n\tFirstly, split the manifest file into lines and convert to a dictionary\n\t(one line per file).\n\n\tThe first line is the fixity (without the header).\n\t\"\"\"\n\twith open(getfixityfilename(filename"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tcheck.str_param(fixity, \"fixity\")\n\t# check if the file is in the manifest\n\tf = open(manifest_files,\"r\")\n\tf_contents = f.readlines()\n\tf.close()\n\tif len(f_contents) > 0 and f_contents[0].find(\"fixity_module\") >= 0:\n\t\treturn \"fixity_module\"\n\treturn None\n\ndef fixity_module(self, fixity):\n\timport os.path\n\t# Validate the fixity block.\n\tcheck.str"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tfor path in fixity.get_children():\n\t\tfilepath = self.__get_fixity_file_path(path)\n\t\tif not filepath:\n\t\t\tcontinue\n\t\t# NOTE there could be invalid files on another volume.\n\t\tif not self.__validate_manifest([filepath], manifest_files):\n\t\t\tdel fixity[path] # Remove the reference as it is no longer needed\n\t\t\tcontinue\n\t\t\t\n\t\t# Check if the fixity exists on remote file system,\n\t\t# if so skip it\n\t\ttry:\n\t\t\tif self._"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tf = open(os.path.join(manifest_files, f'inventory/{fixity[\"package\"]}.yml'))\n\tblock = yaml.load(f)\n\tf.close()\n\t\n\tfor file in block[\"fixity\"].keys():\n\t\tif file not in {\"manifest\", \"files\"}:\n\t\t\traise RuntimeError(\"Invalid fixity entry file reference in inventory\")\n\n\t\tif fixity[\"fixity\"][file] is None:\n\t\t\traise RuntimeError(\"Cannot generate fixity if fixity entry is missing in inventory.\")\n\ndef check_package(manifest_"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\timport os\n\timport tempfile\n\timport ast\n\n\t# This is kinda ugly hack.\n\t# We must make sure it compiles properly.\n\tif \"fixity\" not in __import__(\"pyomo.core.blockstorage\").__dict__:\n\t\traise ImportError(\"pyomo.core.blockstorage requires fixity module\")\n\ttry:\n\t\tfrom pyomo.core.blockstorage import BlockStorage\n\t\tfrom io import StringIO\n\texcept ImportError:\n\t\traise\n\telse:\n\t\twith open(\"pyomo/blockstorage/blockstorage."}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t# This is the validation block required when updating an existing inventory,\n\t# not checking the file type as the inventory may get created during the update.\n\tfile_list = fixity['file_list']\n\tfor item in file_list:\n\t\titem_uri = item['uri']\n\t  \n\t\tif item_uri!= manifest_files['file'][0]['uri']:\n\t\t\tself.validation_message('Error: duplicate file URI.')\n\t\t\tself.validation_message(item['label'], self.get_name(item) +'(expected'+ str("}]}
{"task_id": "62b463153879012d19481498", "prompt": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tl = []\n\tfor f in os.listdir(path):\n\t\td = os.path.join(path, f)\n\t\tif os.path.isdir(d):\n\t\t\tl.append(f)\n\t\telse:\n\t\t\tl.extend(glob.glob(name=f, suffix=name_suffix))\n\treturn l\n\ndef parse_path(root):\n\tfor c in root.split(\":\"):\n\t\tif c.startswith(\"/\") or c.startswith(\"..\") or c.startswith(x+'/'):\n\t\t"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\twith open(path, 'r') as f:\n\t\tret = f.read()\n\tret = ret.split()\n\tfor file in ret:\n\t\tif \"/\" in file:\n\t\t\tfilename = file[file.index(\"/\") + 1:file.index(\"-\")].strip(\" \\t\\x0b\")\n\t\telse:\n\t\t\tfilename = stripstr(file)\n\t\tif \"/\" not in filename:\n\t\t\tfilename = os.path.basename(filename)\n\t\tif file.endswith('.exe'):\n\t\t\tfilename = filename[:-3]"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\ts = pathutils.get_source_name(path)\n\tfiles = [s.replace(\".rst\",\".txt\") for s in os.listdir(path) if os.path.isfile(path+\"/\"+s)]\n\treturn files\n\ndef clean_file(filename):\n\t\"\"\"\n\t\tReturns filename in clean form\n\t\"\"\"\n\tpath = os.path.join(os.getcwd(),os.sep ,os.pardir,\"\")\n\treturn path + \"/\" + filename+\".txt\"\n\ndef clean_file_from_project(project_"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\twith open(path) as file:\n\t\tself.open(file, \"r\", read=True)\n\t#self.open(os.path.join(path, self.filename), \"r+\", os.path.join(path, \"meta\"))\n\t\tself.open(os.path.join(path, self.filename), \"r\")\n\twhile True:\n\t\tline = self.readLine()\n\t\tif line:\n\t\t\tyield self.split(line)\n\n\t#def createDict(self, data):\n\t\t#print line\n"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn os.listdir(path)\ndef get_filetype(path):\n\t\"\"\"\n\treturn the filetype of the file at `path`\n\t\"\"\"\n\treturn os.path.splitext(os.path.splitext(path)[-1])[0]\nclass FileList(object):\n  def __init__(self, path=None, files=None, files_list=files_list, get_filetype=get_filetype):\n    if path is None:\n      path = os.getcwd()\t\t\n    if files"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn [full_name(f) for f in sorted(os.listdir(path)) if type(f) in [str]]\n\n\ndef path_to_data(path):\n\t# Create a directory if necessary\n\tpath = os.path.dirname(path)\n\n\t# Determine the subdirectory\n\tdata_dir = os.path.join(path, \"img_images/\")\n\n\t# Determine the file name to open from the root (or the given path)\n\tdata_file = full_name(os.path.join('..', '..', '"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn os.listdir(path)\n\n/*\n * Copyright 2015 Red Hat, Inc. and/or its affiliates.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * \n *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tif os.path.isdir(path):\n\t\treturn [x.rstrip() for x in os.listdir(path)]\n\treturn (x.strip() for x in os.listdir(path))\n\ndef get_file_list():\n\tif os.path.isfile(filename):\n\t\treturn [x.rstrip() for x in os.listdir(filename)]\n\treturn []\n\ndef get_path():\n\tif system().lower() in ['windows', 'win']:\n\t\treturn os.path.normpath(os.path"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn sorted(glob(path + '/*'))\n\ndef get_path_in_env(key, path):\n\treturn get_path_in_env_by_key(os.environ, key, path)\n\ndef get_path_in_env_by_key(env, key, path):\n\tif key in env:\n\t\treturn get_path_in_env(env[key], path)\n\telse:\n\t\treturn path\n\ndef get_local_and_run_tasks(tasks):\n\ttasks.load_from_file()"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn [i.name for i in os.listdir(path) if os.path.isdir(os.path.join(path,i))]\n\n\n\"\"\"\nReturns the contents of a file.\n\"\"\"\ndef load_content(file):\n\tif \"http://download.mail.ru/messages/python.org/en/latest_software/mailserver/inbox/python-mailserver%200.7.5-win32-mingw-32.32-5.so\" in file:\n\t\treturn (file.split(\"http://download.mail."}]}
{"task_id": "62b463153879012d1948149a", "prompt": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\timport re\n\tfrom distutils import dir_util\n\tfrom xml.etree import ElementTree\n\tpkg_names = [] # names of packages in list\n\tfor xml_filename in xmls:\n\t\tif re.match(r'''(?i)^([-.0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+)\\.xml$''', xml_filename):\n\t\t\tpkg_names.append(xml_filename)\n\tif len(pkg_names)==0: return(dict())\n\n\tfiles_names = [] #"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\txml_to_groups_dict = _make_group_dict_from_xmls(source, xmls, files)\n\treturn _order_groups_by_name_and_name(files, xml_to_groups_dict)\n\n\ndef _group_file_by_xml_filename(source, xmls, file):\n\t\"\"\"\n\t\tGroup XML files\n\t\t\n\t\tGroup files by their XML filename and returns data in dict format.\n\t\t\n\t\tParameters\n\t\t----------\n\t\txml_filename : str\n\t\t    XML filename\n\t\tfile"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tdef _append_group(name, data):\n\t\tfiles = files + [data]\n\t\treturn {name: {xml_file: data}}\n\tfiles = sorted(files, key=len, reverse=True)\n\n\tdef _group_files_by_xml_name(data):\n\t\tname = data['name']\n\t\txml_files = _group_xml_files(data, files, xmls)\n\t\treturn _group_files_by_xml_filename(source, xml_files, files)\n\n\tdata = dict([("}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\txml_basenames_dict = None\n\tfiles_list = []\n\n\tfor xml_file_name in xmls:\n\t\tif xml_basenames_dict is None:\n\t\t\txml_basenames_dict = {xml_file_name: {xml_filename}}\n\t\telse:\n\t\t\txml_basenames_dict[xml_file_name] = {xml_filename}\n\t\tfiles_list.append(xml_basenames_dict[xml_file_name])\n\n\treturn dict(files_list)\n\n\n@lru_"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\txml_files = [ i for i in files if i.endswith('.xml')]\n\tnew_dict = { file: [] for file in xml_files }\n\tif xmls is None:\n\t\tfor xml_in_folder in source.get_child('file').get_children():\n\t\t\tif xml_in_folder.name == 'package':\n\t\t\t\tnew_dict[xml_in_folder.attrib['name']] = []\n\t\t\t\told_filename = xml_in_folder.get_attribute('file')\n\t\t\t\trelative_filename = relative_filename_"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tresult = {}\n\tfor xml_file in xmls:\n\t\txml_file_basename = os.path.basename(xml_file)\n\t\txml_basename = xml_file_basename.split(\".\", 1)[0]\n\t\tif xml_basename == xml_filename:\n\t\t\tif not os.path.isdir(xml_file) and not os.path.exists(xml_file):\n\t\t\t\tresult[os.path.basename(xml_file)] = {}\n\t\telse:\n\t\t\tresult[xml_basename] = {}\n\t\t"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\txmls_dict = {}\n\tfor xml_filename in xmls:\n\t\txml_filename = abspath_to_absolute(xml_filename)\n\t\txml_file_path = os.path.join(source, xml_filename)\n\t\txml_filename = os.path.basename(xml_filename)\n\t\tfiles_dict = {}\n\t\tfor file in files:\n\t\t\tfile_name = abspath_to_absolute(file)\n\t\t\tfile_path = os.path.join(xml_file_path, file_name)\n"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tdata = OrderedDict()\n\tgroup = {\"xml_files\": set(), \"files\": set()}\n\tadd_dicts(data, group, files)\n\tfor xml_filename, group_dict in group.items():\n\t\tfor file in files:\n\t\t\tif xml_filename == file[0]:\n\t\t\t\tdata[group_dict[\"xml_filename\"]].update(file)\n\t\t\t\tgroup.pop(group_dict[\"xml_filename\"])\n\t\t\tfor basename in [\".xml\", \".xml.gz\", \".xml.bz2\"]:\n\t\t\t\tif file["}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\txmls = [x[:-4] for x in xmls]\n\t\n\tfile_key = dict([(os.path.basename(x), x) \n\t\t\t  for x in xmls])\n\tfor file in files:\n\t\tfile_key[\"*.xml\"] = os.path.join(source, file)\n\treturn file_key\n\n\n\"\"\"\n\tFilenames matching all XML files in a ZIP file\n\t\n\tGroup xml files by the files in a ZIP. The filename pattern\n\t(which means this is a filename within the ZIP) will be matched\n\tif a"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\txmls = [x for x in xmls if x!= \"\"]\n\tgroup_data = {}\n\tfor xml in xmls:\n\t\tfor file in files:\n\t\t\tbasename = os.path.basename(file)\n\t\t\tif basename.startswith(xml):\n\t\t\t\tname = basename[:xml.split(os.sep)[0].index(xml), xml.split(os.sep)[1:].index(os.sep)].replace(\"/\", \"_\")\n\t\t\t\tif name:\n\t\t\t\t\tgroup_data[name] = file\n\t\n\treturn group_data\n"}]}
{"task_id": "62b463153879012d1948149b", "prompt": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\ttry:\n\t\tfd = open(file_path)\n\t\tfile_meta = fd.read()\n\t\tfile_metas = re.findall(r'^(.*), (.*),(.*)$',file_meta)\n\t\treturn (file_metas[0] == prefix)\n\texcept:\n\t\treturn False\n\n\n'''\n\tUtility: Find the number of pages a document has.\n\tReturn: A tuple of:\n\t- total_count: Total number of pages\n\t- page_count: Number of pages\n\t'''\ndef get_total_"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tif len(prefix)==0:\n\t\tprefix = \"*\"\n\tif len(file_path)==0:\n\t\tfile_path = \"*\"\n\t\t\n\troot = get_root(file_path)\n\t\t\n\tpkg_info = root.getChild(prefix + \".__init__\")\n\tif pkg_info is None:\n\t\treturn False\n\t\t\n\treturn True\n\n\ndef find_root(pkg_info_str):\n\t\"\"\"\n\tIdentify the root of the given `pkg_info_str`.\n\t\n\t\"\"\"\n\troot_node = pkg_info"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t# get file basename\n\tfile_ext = os.path.splitext(file_path)[1].lower()\n\t\n\t# check if file path is an application\n\tif not file_ext[0].islower() or \".\" not in file_ext:\n\t\t# return false with warning message\n\t\tprint(\"\\t\\t\\t\" + prefix + \" - file path must be an application to be considered!\")\n\t\tprint(\" \".join(sys.argv[1:]))\n\t\tprint()\n\t\n\t\t# set the flag to return the doc_package\n\t"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\ttry:\n\t\twith open(file_path) as f:\n\t\t\tdoc = yaml.load(f)\n\t\tif not doc or doc['name']!= '{}.doc'.format(prefix):\n\t\t\treturn False\n\t\tdoc = doc['spec']['doc']\n\t\tif doc['title']!= 'Anna Alegi':\n\t\t\treturn False\n\t\tif doc['file'] == 'doc/api/package/api-root':\n\t\t\treturn False\n\t\treturn True\n\texcept BaseException as e:\n\t\tprint(e)"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tif file_path:\n\t\t# Filepath is not a path, but prefix of the name\n\t\tfile_path_basename = os.path.basename(file_path)\n\t\tfile_name_prefix = os.path.split('/'.join(file_path.split('/')[0:2]))\n\t\t\n\t\tif file_name_prefix[0] == '' and file_name_prefix[1] == '':\n\t\t\treturn False\n\t\t\n\t\tif file_name_prefix[0] == '' and file_name_prefix[1] == '"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\timport pydoc\n\t\n\tif not file_path:\n\t\t\n\t\tfile_path = os.getcwd()\n\t\ttry:\n\t\t\tfile_path_list = file_path.split(os.path.sep)\n\t\t\tif len(file_path_list) > 1:\n\t\t\t\tfilename = file_path.split(\"/\")[-1]\n\t\t\t\tnameless_file = pydoc.getdoc(open(file_path))\n\t\t\t\tif filename in nameless_file[\"packages\"]:\n\t\t\t\t\treturn False\n\t\texcept:\n\t\t\tfile_path"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\twith open(file_path, 'r+') as f:\n\t\tdocument_type = f.read().strip() # document type line\n\t\treturn (re.search(\n\t\t\tr'^(?:\"{2,}' + prefix + r'|{1,4}(?:\"{1,3},\\.{3,1})+)',\n\t\t\tdocument_type,\n\t\t\tre.MULTILINE)!= None).islower()\n\n'''\nAPI\n'''\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tif prefix == '':\n\t\treturn False # empty prefix\n\t\n\tif prefix[0]=='\\'':\n\t\treturn False # start letter of the prefix (no leading digit)\n\t\n\tfile_path = os.path.basename(file_path)\n\t\n\tis_folder = False\n\ttry:\n\t\tis_folder = os.path.isdir(file_path)\n\texcept ValueError:\n\t\tpass # file path already processed\n\t\n\tif is_folder and file_path.count('\\\\')<2 and prefix!='src/' and not prefix.isdigit"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tif not os.path.exists(file_path):\n\t\treturn False\n\t\t\n\tfullpath = os.path.join(fullpath, file_path)\n\tfor root, dirnames, filenames in os.walk(prefix, topdown=False):\n\t\tfor file_name in filenames:\n\t\t\tfile_path = os.path.join(root, file_name)\n\n\t\t\tif file_path in fullpath:\n\t\t\t\treturn True\n\n\treturn False\n\n\ndef remove_inconsistencies_by_prefix_regex(regex, file"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\treturn os.path.basename(file_path).startswith(prefix)\n\npackage com.mossle.client.auth;\n\nimport java.util.Map;\n\nimport com.mossle.api.tenant.TenantHolder;\n\nimport com.mossle.client.store.StoreClient;\n\npublic class TenantClientHelper {\n    private TenantClientHelper() {\n    }\n\n    public static TenantHolder loadTenant(Map<String, Object> tenantMeta,\n           "}]}
{"task_id": "62b463153879012d1948149c", "prompt": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tfiles = [f for _f in files if _f.count(prefix)+len(prefix) > 0 for _f in files]\n\tfiles = files if len(files) > 0 else []\n\treturn sorted(files, key=lambda f : f.count(prefix))\n\ndef select_filename_by_id(id_or_filename, files):\n\t\"\"\"\n\tGet a file by id\n\t\n\tParameters\n\t----------\n\tid_or_filename : str\n\tfiles:\n\t    path to file\n\tReturns\n\t-------\n\tstr\n\t"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tassert(len(files) > 0)\n\tassert(len(prefix) > 0)\n\t\n\t# os fiches esse arquivo\n\tfiles = [f for f in files if f.startswith(prefix + '/')]\n\tassert(len(files) > 0)\n\t\n\t# lista os arquivos nomes em seguido\n\tall_files = []\n\tdirectories = [f for f in files if f.endswith(os.sep + 'd*.pyc')]\n\tfor dirname in directories:\n\t\tall_files = all_"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tfor i in range(len(files)):\n\t\tif files[i].strip().startswith(prefix):\n\t\t\tfor j in range(i + 1, len(files)):\n\t\t\t\tif files[i][-len(prefix)] == files[j]:\n\t\t\t\t\treturn [files[i], files[j]]\n\treturn []\n\ndef select_filenames_by_prefix_and_suffix(prefix, files, suffix):\n\t\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cuj"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tdef select_arquivos_by_filename(filename):\n\t\t\"\"\"\n\t\tFind elements which contain `filename` and extract the filename\n\t\t\"\"\"\n\t\tfilename = filename.lower()\n\t\tfile_path = extract_filename(prefix=prefix, filename=filename)\n\t\tif file_path is None:\n\t\t\treturn None\n\t\telse:\n\t\t\treturn [file_path]\n\t\t\n\treturn select_filenames_by_prefix(prefix, files).filter(\n\t\tfile_path__in=select_arquivos_by"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\treturn files\n\t\n\tdef select_filenames_by_tag(tag, files):\n\t\t\"\"\"\n\t\tGet files which belongs to a document.\n\t\t\n\t\tRetorna os arquivos com `prefix` e `tag`\n\t\t\n\t\tParameters\n\t\t----------\n\t\ttag : str\n\t\tfiles : str list\n\t\tReturns\n\t\t-------\n\t\tlist\n\t\t\n\t\t\"\"\"\n\t\tfiles = []\n\t\tfor f in select_filenames(tag, files):\n\t\t\tif f.lower().endswith(tag.lower()):\n\t\t\t"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tif prefix.endswith('.pdf'):\n\t\tfiles.extend(pdf_files(prefix).keys())\n\telif prefix.endswith('.zip'):\n\t\tfiles.extend(zip_files(prefix).keys())\n\telse:\n\t\tfiles.extend(txt_files(prefix).keys())\n\treturn files\n\n\ndef select_archive_files_by_prefix(prefix, files):\n\t\"\"\"\n\tGet all files with extensions that belong to a document package.\n\t\n\tRetorna os arquivos de todas as informa\u00e7\u00f5es sobre"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\treturn [ files + \".\" if f.endswith(prefix) else \"\" \\\n\t\t\t\t\tfor f in os.listdir(files) \\\n\t\t\t\t\tif f!= \"\" \\\n\t\t\t\t\t\tand not f.startswith(prefix + \".\") \\\n\t\t\t\t\t\tand \\\n\t\t\t\t\t\tf.startswith(prefix) \\\n\t\t\t\t\t\tand not f.endswith(prefix) \\\n\t\t\t\t\t\tand not f.endswith(prefix + \".\") ]\n\ndef run_command(command):\n\t\"\"\"\n\tRun a command on a given list of files\n\t\n\tParameters\n\t----------\n\tcommands : list of str\n"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tif not prefix:\n\t\tprefix = \"\"\n\t\n\treturn _file_utils.find_filenames_by_prefix(prefix=prefix, match=True, files=files)\n\ndef select_filenames_by_suffix(suffix, files):\n\t\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos dos arquivos da lista `files` cujos nomes iniciam com `suffix`\n\t\n\tParameters\n\t----------\n\tsuffix : str\n\t    Suffix\n\tfiles : str list\n\t "}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tfiles_dict = {}\n\t#print(\"files=%r\" % files)\n\tfor prefix in glob.glob(prefix):\n\t    if os.path.basename(prefix) in files:\n\t\tfiles_dict[prefix] = prefix\n\treturn files_dict\n\n\n#region File operations\ndef open(filename, mode='rb'): #file.open()\n\tif mode == 'rb': #open file in binary (rb) mode\n\t\tshutil.make_archive(filename, 'r:gz')\n\telse: #open file in text (rt"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tassert prefix not in __path\n\treturn []\n\n\ndef select_filenames_by_suffix(prefix, files):\n\t\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com suffix `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename suffix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which suffix files matches to prefix\n\t\"\"\"\n\tassert prefix"}]}
{"task_id": "62b463153879012d1948149d", "prompt": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\twith zipfile.ZipFile(folder) as zf: # open the folder and read the zip file to list folders\n\t\tdir_files = [] # list of files in the folder\n\t\tfor item in zf.namelist():\n\t\t\tdir_files.append(item)\n\treturn\tdict(_dir_files = dir_files)\n\n\ndef _find_files(folder, names, extension='.py', include_package=True):\n\t\"\"\"\n\tFind files in packages' folder\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tif folder in sys.path:\n\t\tfolder = os.path.split(folder)[0]\n\tdata = {}\n\ttree = ET.parse(folder)\n\troot = tree.getroot()\n\tfor entry in root:\n\t\tentry_attrib = entry.attrib\n\t\tif entry_attrib =='version.txt':\n\t\t\tpass\n\t\telif entry_attrib == 'description':\n\t\t\tdata['description'] = entry.text\n\t\telif entry_attrib == 'filename':\n\t\t\tdata['filename'] = entry.text\n\treturn data"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\ttree = ET.parse(folder)\n\tpackage = {}\n\tfor el in tree.getroot().findall(\"Package\"):\n\t\tpackage[el.get(\"Name\") or el.get(\"XRef\")] = folder + el.get(\"Name\") or el.get(\"XRef\")\n\treturn package\n\ndef _get_package(id):\n\tpackage = _explore_folder(\n\t\tos.path.join(BASE_DIR, BASE_FILE, DEFAULT_PACKAGE, id)\n\t   ,\n\t)\n\treturn _explore_folder("}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tpackage_dict_list = [(os.path.join(folder, x,'main.xml'), x, '')]\n\tpackage_dict_list_groups = list(self.__generate_xml_groups(folder))\n\tpackage_dict_list += package_dict_list_groups\n\tpackage_dict_file_list = list(self.__get_list_from_file_list(package_dict_list))\n\tpackage_dict = {}\n\tpackage_dict.update({x: os.path.splitext(x)[0] for x in package_dict_"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t# import explore\n\t\n\t# get dict data from explore.get_packages()\n\t# get data of 'exp_folder'\n\t# get data for 'exp'\n\t# group by data of 'exp_folder' into two dicts to make key, value\n\t\n\t\nclass XlsxPackage():\n\t# class XlsxPackage()\n\t\n\tdef _get_packages_from_file(file_path):\n\t\tdict_ = {}\n\t\n\t\t# read in xlsx files\n\t\tlines = fileutil.open(file_path)"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tbase_folder = _get_files(folder, 'pack')\n\tpackage_list = defaultdict(list)\n\n\tfor xml_file in base_folder:\n\t\tpackage_path = os.path.join(xml_file, 'package_data.xml')\n\t\tpackage_data = _read_package_data(xml_file)\n\t\tpackage_list[package_path].append(package_data)\n\n\treturn package_list\n\n\ndef list_items(path, exclude='all'):\n\t\"\"\"\n\t\tGet files as list to"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\titems = filter( lambda item: item[0] == '.',\n\t\\\n\t(\n\t\tfilter( lambda item: \"packages\" in item[1],\n\t\t\t\\\n\t\t\tget_package_files_from_str( folder + \"/\" + item[1][0:6] ), \\\n\t\t\tlist(([\"packages\"],[\"name\"]))) if len( item ) > 0\n\t))\n\treturn items\n\n\ndef filter_data(package_file, package_name, data):\n\treturn [item for item in data if item[1]!= package_name]"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tdata = {}\n\tsubdirs = _subdirs(folder)\n\tfor dir in subdirs:\n\t\tfiles = _explore_dir(dir)\n\t\tfor file_i in files:\n        \t\tbasename = os.path.basename(file_i[0])\n        \t\tfiles[basename] = _explore_file(file_i[1])\n\n        for file_d in files:\n            data[file_"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tprint('Get packages')\n\tpackages = {}\n\tfor file in os.listdir(folder):\n\t\ttry:\n\t\t\tfilename = os.path.basename(file)\n\t\t\tif not filename.endswith('_packages.xml') and not filename.endswith('py.class'):\n\t\t\t\t# Get the package file's data\n\t\t\t\tpackages[filename] = {}\n\t\t\t\twith open(file) as package_file:\n\t\t\t\t\tpackage_data = package_file.read()\n\t\n\t\t\t\t# Get the xml contents from the package file\n\t\t\t\tpackage_xml ="}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\ttry:\n\t\tfd = os.path.join(folder, 'packages.xml.gz')\n\t\tf = open(fd,'r')\n\t\tdata = f.read()\n\t\tf.close()\n\t\treturn data\n\texcept:\n\t\treturn None\n\ndef undist(path):\n\t\"\"\"\n\tRemove packages/ folder from the dist folder\n\n\tParameters\n\t----------\n\tpath : str\n\t\t       Path to dist folder\n\t\t\t\n\tReturns\n\t--------\n\tdict\n\t\"\"\"\n\tif sys.platform.startswith"}]}
{"task_id": "62b463153879012d1948149f", "prompt": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tprefix_len = len(prefix)\n\tfile_name = name[:prefix_len] + \"pacote.xml\"\n\t\n\treturn _load_file(file_folder + file_name)\n\n\n@_init_env.export(\"pacote\", \"assets\")\ndef _render_asset(pacote, asset):\n\t\"\"\"\n\tRender a pacote atual para a como asset do PDF.\n\t\n\tParameters\n\t----------\n\tpacote :\n\t    Pacote o ficheiro da tabela\n\tasset :\n\t   "}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tif file_path[-1:] == \"/\":\n\t\tfile_path = file_path[:-1]\n\t\n\twith open(file_path, 'r', encoding=\"utf-8\") as file:\n\t\tfiles = file.readlines()\n\t\tfiles = [os.path.relpath(f, file_folder) for f in files]\n\t\n\treturn _file_content.find_all(\"{} {}\".format(prefix, files[0]))\n\t\n\ndef _render_file(file_name, output_folder, **kwargs):\n\t\"\"\"\n\tIdentific"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tf = open(file_path)\n\twith open(f, 'r') as in_f:\n\t\tdata = dict([l.strip() for l in in_f.readlines()])\n\treturn data\n\t\t\t\ndef _add_packages(packagename, packages, dir_path, arquiveto_dir_path):\n\t\"\"\"\n\t\tIdentifica ao campo de pacote e os partecipos e atualiza\n\t\"\"\"\n\tpackagenames = set()\n\tfor package in packages:\n\t\tpackagenames.add"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tif not os.path.exists(file_path):\n\t\tos.makedirs(file_path,exist_ok=True)\n\telif os.path.isdir(file_path):\n\t\tpwd = raw_input('Digite o arquivo a partir do arquivo que permite? [Y/n] ')\n\t\tif pwd == 'Y':\n\t\t\treturn\n\t\tfiles = os.listdir(file_path)\n\t\treturn {'type':'module',  'list':files\n\t\t\t}\n\tmodules = set()"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tresult = {}\n\t\n\tif filename:\n\t\tprint('Evaluating %s to obtain %s' % (prefix, filename))\n\telse:\n\t\tprint('Evaluating %s as input' % (prefix))\n\t\n\tfile_folder = join(root_dir, file_folder)\n\t\n\tos.execv(\n\t\t\"open\",\n\t\t\"\"%(root_dir)s/%(file_folder)s/*\", \n\t\tfile_list=join(root_dir, file_folder))\n\t\n\tfor (relname, relpath,"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\txml = os.path.join(file_path, \"manifest.xml\")\n\tpackages = eval_manifest(xml)\n\tmanifest = dict(packages)\n\tfor prop in ['assets','renditions']:\n\t\tmanifest[prop] = []\n\trenditions = eval_renditions(xml)\n\tfor rend in renditions:\n\t\tmanifest[render(prefix, rend)] = []\n\tpackages[\"renditions\"] = renditions\n\tfor prop in ['package','modules']:\n\t\tmanifest[prop] = []\n\tpackage_assets = eval_package"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfilename = '_eval_file'\n\t\n\t# O arquivo XML sem arquivos com a exterior\n\tif file_path.endswith(sFileExt):\n\t\textension = file_path[len(sFileExt):].lower()\n\t\tfilename = filename + extension\n\telse:\n\t\tfilename = filename + sFileExt\n\t\n\t# Crie os arquivos como arquivos com sub-tem a execu\u00e7\u00e3o de o tipo\n\tfiles = {}\n\tfor subfile in glob.glob(sDirectory + '/' + prefix"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tpath_prefix = osp.dirname(os.path.abspath(file_path))\n\tfile_name = osp.splitext(osp.basename(file_path))[0]\n\t\n\tif path_prefix[-1]!= '/':\n\t\tpath_prefix = osp.join([osp.dirname(file_path), path_prefix])\n\t\t\n\tpkg_folder = osp.dirname(file_path)\n\t\n\tdata = dict()\n\troot = ET.Element('meta')\n\tdata[\"name\"] = file_name\n\t\n\troot."}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tlogger = logging.getLogger(__name__)\n\t\n\tif prefix!= None and file_path is None:\n\t\tfile_path = os.path.join(os.path.dirname(input()))\n\t\n\tfiles = find_files([file_path, f\"assets/{prefix}/{file_path}.xml\"] + \n\t\t\t\t\t\t\tos.path.commonprefix((os.curdir, \"assets\")) + \n\t\t\t\t\t\t\tglob.glob(\"**/*.xml\"))\n\t\n\tif files:\n\t\tprint(f\"\\nFound {len(files)} files\")\n\t\treturn _process_"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\txml_str = getXML(dir_path=file_path)\n\tdoc = parse(xml_str)\n\t\n\tfor element in doc.findall(\"asset\") + doc.findall(\"rendition\"):\n\t\tif not element.attrib or element.attrib.get(\"type\")!= \"pkg\":\n\t\t\tcontinue\n\t\t\n\t\tfile_name = element.attrib[\"file\"]\n\t\t\n\t\tif not file_name:\n\t\t\tprint (\"O arquivo selecionado nulo, para ainda n\u00e3o \u00e9 permitida.\")\n\t\t\treturn {}\n"}]}
{"task_id": "62b463153879012d194814a1", "prompt": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tself.renamer_renard.add_path(lang, self.target_language_path[lang])\n\n\tos.remove('en.pdf')\n\tos.remove('artigo02.pdf')\n\tos.remove('artigo02.en-us.pdf')\n\tself.target_language_path['en'] = self.target_language_path['en-us'] = self.target_language_path[lang] = file_path + '.en-us.pdf'\n\n\t# set lang to original lang, then recheck that no page exists"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tfile_contents = get_file_contents(file_path)\n\tfile_path = get_path_in_doc(file_path)\n\tself.append({\"original\": file_path, \"en\": \"artigo02-en.pdf\", \"main-title\": _(\"Artigos artigos on {{word}}\").format(word=file_path)})\n\n#def test_artigos_artigo_artigo_2(self):\n#\t\"\"\"Test artigos artigos on 2\n#\t{\n#\t\t\"original\": \"art"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tself.basefolder = 'BurpUtils/test/resources'\n\tself.lang = 'en'\n\tself.test_file_path = file_path\n\tself.bcr = BurpUtils.BurpUtils()\n\tself.bcr.setLanguage(self.lang)\n\t\n\t\n\t\nclass Test_get_file_name_list(unittest.TestCase):\n\tdef setUp(self):\n\t\tunittest.TestCase.setUp(self)\n\t\tself.file_name_list = ['test.txt']\n\n\tdef test_"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tself._base_format = 'PDF'\n\tself.lang = lang.lower()\n\tself.lang,self.original_file_path = lang,self.original_file_path\n\tself.file_path = file_path\n\tfile_name, file_ext = os.path.splitext(os.path.split(file_path)[-1])\n\tfile_path = file_path.replace('\\\\', '/')\n\tdata = {\n\t\t'en':self.original_file_path,\n\t\t'en.pdf':self.original_"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t# language=en\n\tlang=lang\n\t# file_path=rpi_ircs\n\tf=open(file_path,\"w\",encoding=\"utf-8\")\n\t# add\n\tself.add_document(f,lang)\n\t# add en\n\ten = {\n\t\t'Language' : 'en',\n\t\t'Original' : 'artigo02.pdf',\n\t\t'Original for article': 'artigo02-en.pdf',\n\t}\n\tf.write(json.dumps(en,'utf-8'))\n\t#"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tf = \"artigo02-en.pdf\"\n\tf.format(lang=lang)\n\tf.save(file_path)\n\ndef main(filename):\n\t\"\"\"\n\tReturns:\n\t\tfile_path              -    artigo2.pdf, the path to the PDF file in unicode.\n\t\tlang                   -    English\n\t\toriginal              "}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tif os.path.exists(\n\t\tos.path.join(self.get_mp_path(), lang, file_path)\n\t):\n\t\tf = open(\n\t\t\tos.path.join(self.get_mp_path(), lang, file_path),\n\t\t\t\"rb\"\n\t\t)\n\t\tf.read()\n\t\tf.close()\n\telse:\n\t\tos.makedirs(self.get_mp_path(), exist_ok=True)\n\t\tf = open(\n\t\t\tos.path."}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tself.logger.debug('Add rendition -> %s', file_path)\n\tfile_path = '%sph_path% /favicon.ico' % (file_path, os.name)\n\tself.pdf_writer.add_document('rendition', lang, file_path)\n\ndef add_trendition(self,  lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo_trendition.pdf\",\n\t    \"en\": \"artigo-trendition-en.pdf\",\n\t}"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t# remove duplicate renditions\n\tfor i in range(len(self._renditions) - 1, -1, -1):\n\t\tif self.get_rendition_name(self._renditions[i].language) == self._renditions[i + 1].language:\n\t\t\tdel self._renditions[i]\n\t\telse:\n\t\t\tdel self._renditions[  i]\n\n\t\"\"\"\n\t{\n\t    \"original\": \"artigo02-en.pdf\",\n\t    \"en\": \"Articoine02-en."}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tpass\n\n\ndef add_language(self, lang):\n\t\"\"\"\n\thttps://en.wikipedia.org/wiki/List_of_all_languages_in_a_text\n\t\"\"\"\n\tlanguage = \"en\"\n\ttry:\n\t\ti = 0\n\texcept NameError:\n\t\twhile i < len(lang):\n\t\t\ttry:\n\t\t\t\tlang[i], str = lang[i].split('-')\n\t\t\t\tlang[i] = str\n\t\t\t\ti += 1\n\t\t\texcept:\n\t\t\t\tpass\n\ttry:\n\t"}]}
{"task_id": "62b463163879012d194814a2", "prompt": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t\n\"\"\"\n\t{\"library_asset\": \"artigo02-gf03.tiff\", \"library_filename\": \"/path/artigo02-gf03.tiff\", }\n\t\"\"\"\n\n\t\"\"\"\n\t{\"library_asset\": \"/path/artigo02-gf03.jpg\", \"library_filename\": \"/path/artigo02-gf03.jpg\", }\n\t\"\"\"\n\n\t\"\"\"\n\t{\"library_asset\": \"/path/artigo02-gf03.png\", \"library_filename\": \"/path/artigo02-gf03.png\", }\n\t\"\"\""}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tt = get_image_file_by_format(basename)\n\ts = get_image_file_by_format(file_path, file_list=1, namerule=20)\n\ta = Image.open(t).resize((s.size[0], s.size[1]), Image.ANTIALIAS)\n\ta.save(t)\n\treturn t\n\ndef main():\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument(\"-m\", \"--mode\", choices=[\"simple\", \"gray_scale\"], default=\"\","}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tfor folder_name, file_name in assetDict.items():\n\t\tif basename in file_name:\n\t\t\tself.add_asset_to_folder(folder_name,basename,file_name,file_path)\n\n def save_file(self, file_path = \"/tmp/\") :\n\tself.save_image(file_path, width=1200, height=800)\n\t\n def save_image(self, path, width=1200, height=800):\n\twith open(path, \"wb\") as f:\n\t\tf.write("}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tf_name = os.path.split(file_path)[-1][1:]\n\tf_info = io.BytesIO()\n\timgio.imwrite(f_name, f_info)\n\twith open(self.image_list, 'wb') as f:\n\t\tf.write(f_info.getvalue())\n \n\twith open(basename, 'rb') as f:\n\t\tf_name = io.BytesIO(f.read())\n\t\twith open(f_name.name, 'rb') as f2:\n\t\t\t"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tpass\n\nclass Asset(object):\n\tdef __init__(self, assetinfo):\n\t\tself.__dict__ = assetinfo.__dict__\n\t\tfor index in assetinfo.keys():\n\t\t\tself[assetinfo[index]] = assetinfo[index]\n\n\tdef __str__(self):\n\t\treturn self.__repr__()\n\n\tdef __repr__(self):\n\t\tif self.__class__ == Asset:\n\t\t\treturn str(self.keys())\n\t\telse:\n\t\t\treturn str(\n\t\t\tsorted((k, self.get("}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tif file_path is None:\n\t\tfile_path = basename\n\tfilename = os.path.basename(file_path)\n\t\n\tif filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n\t\tself.images.append(file_path)\n\t\timg = pytesseract.imageUtils.loadImage(file_path)\n\n\telif filename.endswith(\".png\"):\n\t\timg = pytesseract.imageUtils.loadImage(file_path)\n\t\ttry:\n\t\t\timg = pytesseract.image"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tname, extension = os.path.splitext(basename)\n\tfile_path = os.path.join(self.dir, name + extension)\n\tfiles_to_add = False\n\n\tif os.path.exists(file_path):\n\t\tfiles_to_add = True\n\n\t\ttry:\n\t\t\timport pyximport; pyximport.install()\n\t\texcept ImportError:\n\t\t\tprint('pyximport not installed. Creating empty files.')\n\n\t\ttry:\n\t\t\tself._write_file_list(file_path,"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tcmd = 'add_asset \"artigo02-%(basename)s-%(file_path)s\"'\n\tout, err = run_command('%s -s %s %s' % (cmd, artigo_cmd, basename, file_path))\n\tif err is not None:\n\t\traise err\n\n\treturn out\n\n\ndef add_asset_from_assetfile(self, basename_as_key, file_name):\n\t\"\"\"\n\t\"\"\"\n\tcmd = 'add_asset \\\"%s\\\" --auto-adjust-level --add-assets \"%s"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tself.log('creating image for %s'%basename)\n\n\timagedef = self.imagedef.copy()\n\n\tfor f in os.listdir(file_path):\n\t\ttry:\n\t\t\tif os.path.isfile(os.path.join(file_path, f[0:len(self.prefix)+4])):\n\t\t\t\twith open(file_path, 'r') as jfp:\n\t\t\t\t\tfname = f[0:len(self.prefix)+4]\n\n\t\t\t\t\timagedef['artigo02-gf03.tiff"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tpath, file_name = os.path.split(file_path)\n\tfile_name = file_name.replace(\".tiff\",\"%.1f\")\n\tpath = os.path.join(self.dirname, basename, path)\n\twith open(path, \"wb\") as f:\n\t\tf.write(file_name)\n\n\nclass Artigos:\n\tdef __init__(self, base_url):\n\t\turllib.parse.urlretrieve(\"https://www.artigos.com/download\",self.base_url)\n\t\tself"}]}
{"task_id": "62b463163879012d194814a4", "prompt": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\timport zipfile, os\n\tf = zipfile.ZipFile(zip_path, 'r')\n\tlst = []\t\t\t#list of packages\n\tcnt = 0\n\tfor name in f.namelist():\n\t\tlst = f.read(name)\n\t\tif lst is None:\n\t\t\tcnt += 1\n\t\telse:\n\t\t\tlst = [x for x in lst.split('\\n') if x]\n\t\tfor z in lst:\n\t\t\tlst[z] = re.sub('-/',' ', z)\n\t\tf.close()\n"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\timport zipfile\n\tdata_path = zipfile.ZipFile(zip_path, 'r')\n\ttry:\n\t\tpackage_zip_data = data_path.read('package.xml')\n\t\tdata = package_zip_data['data']\n\t\tpackage_dict = {}\n\t\tpackage_dict['names'] = package_zip_data['name'].split('/')\n\t\tpackage_dict['dict'] = data\n\t\treturn package_dict\n\texcept zipfile.BadZipFile:\n\t\treturn None\n\npackage_name_to_content = _exp"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tarchive = zipfile.ZipFile(zip_path)\n\tcatalog_xmls = archive.namelist()[0]\n\t\n\tzipped_data = {}\n\n\tfor element in catalog_xmls:\n\t\tzipped_name = (\n\t\t\telement.filename\n\t\t\tor element.filename.replace('-', '_')\n\t\t\tor None\n\t\t)\n\t\tzipped_group = {'zip_group': zip_path}\n\t\t\n\t\tname = zipped_name.replace(\"xml\", \"\")\n\t\t\n\t\ttmp_zipped_data = {}\n"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\timport xml.etree.cElementTree\n\timport zipfile\n\t\n\tresult = {}\n\tfor node in zip_file(zip_path):\n\t\tresult.update(parse_zip_file(node))\n\treturn result\n\t\ndef parse_xml_file(file_info):\n\t\"\"\"\n\t\tParse.xml.gz file on a target.\n\t\t\n\t\tParameters\n\t\t----------\n\t\tfile_info : file_info instance\n\t\t\tfile_info object\n\t\t\n\t\tReturns\n\t\t-------\n\t\tNone\n\t\t \"\"\"\n\tfile_"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfrom zipfile import ZipFile\n\timport zipfile\n\timport sys\n\tsys.path.insert(0, \"..\")\n\n\ttry:\n\t\tzip = ZipFile(zip_path)\n\texcept:\n\t\tprint(\"ERROR, %s\" % sys.exc_info()[1])\n\t\treturn dict()\n\t\n\tdef _get_file_dict(source, package, include=[]):\n\t\titems = []\n\t\tfor f in source.namelist():\n\t\t\tp = package if f.endswith(\".\") else f\n\t\t\titems.append(p"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tpackage_data = {}\n\tfor f, finfo in _zip_files(zip_path):\n\t\tpkg_data[finfo.name] = {}\n\t\textract_zip(f, packages_path=finfo.name)\n\t\tfor name, c in packages_path:\n\t\t\tif name =='src':\n\t\t\t\tpackage_data[name] = {'name': name}\n\t\t\t\tfor ext in ['.zip', '.js', '.ts', '.md', '.yaml']:\n\t\t\t\t\timport_path = os.path.join(c, finfo"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\twith zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n\t\tpkg_dict = {}\n\t\tfor zip_file in zip_ref.infolist()[1:]:\n\t\t\tpkg_dict[os.path.basename(zip_file.filename)] = zip_file\n\t\treturn pkg_dict \n\npackage com.alibaba.fastjson.deserializer.issue3696.bean;\n\nimport com.alibaba.fastjson.parser.ParserConfig;\n\nimport java.lang.reflect.Field;\nimport java.lang.reflect."}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\timport subprocess\n\timport os\n\treturn extract_packages_from_zip_file(zip_path=zip_path)\n\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport json\n\nfrom alipay.aop.api.constant.ParamConstants import *\n\n\nclass AlipaySocialBaseContentlibVersionCreateModel(object):\n\n    def __init__(self):\n        self._content_version = None\n\n    @property\n    def content_version(self):\n  "}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tdata = {}\n\txml_path = zipfile.ZipFile(zip_path)\n\tfor file in xml_path.namelist():\n\t\tif file.lower().endswith(\".xml\"):\n\t\t\tentry = xml_path.get_data(file, \"utf-8\")\n\t\t\tdata[entry[\"title\"]] = entry\n\tdata = _parse_xmldata(data)\n\treturn data\n\n_unzipFiles = [_explore_zipfile(path) for path in unzip_list]\n\n_explore_unzipfiles = [_load_xml"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t# get the XML zip file\n\tzip_file = tarfile.open(zip_path)\n\treturn_dict = {}\n\tzip_file_list = [str(p) for p in zip_file.getmembers() if \" \" not in str(p) and  str(p).startswith(\"<?xml\") and str(p).endswith(\"?>\\n\")]\n\tfor xml_zip_file in zip_file_list:\n\t\tif not zipfile.is_zipfile(xml_zip_file):\n\t\t\tcontinue\n\t\tfor xml_zip_file_"}]}
{"task_id": "62b463163879012d194814a6", "prompt": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tfrom collections import namedtuple\n\treturn list(zip_path.glob('./*' + namedtuple('*', 'jpg')))\n\t\ndef create_folder(path):\n\t\"\"\"\n\tCreates the folder structure in path and returns a tuple with the final absolute path\n\t\n\tExample:\n\t\n\t```\n\t>>> os.mkdir('gdrive')\n\t>>> os.mkdir('gdrive/dir')\n\t>>> os.mkdir('gdrive/dir/gdrive')\n\t\n\t...\n\t```\n\t\"\"\"\n\tfolder = path[0] # First drive"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tf = zipfile.ZipFile(zip_path)\n\tfiles = []\n\tfor file in f.filelist():\n\t\tfname = file.filename\n\t\tsplit = fname.split(r'\\.')\n\t\tif fileName in split:\n\t\t\ttmp = split[len(fileName)-1]\n\t\t\tfiles.append(file.name[len(fname):])\n\t\n\t# The zip file is in this format:\n\t# file        => file_name\n\t# --------     :------\n\t# file_name  "}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\traise NotImplementedError()\n\n\ndef is_subdirectory(path:str,\n                    base_name:str = 'root') -> bool:\n\t\"\"\"\n\tCheck that `path` is a subdirectory of `base_name`\n\n\tExample:\n\n\t```\n\timport os, os.path, shutil\n\t# check for directories\n\trootdir = os.path.expanduser(\"~/anaconda4/\") if os.path.isdir('/anaconda4/') else"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tdef count_files(zip_path):\n\t\tres = 0\n\t\tflist = ['.']\n\t\tfor root, _, files in os.walk(zip_path):\n\t\t\tfor f in files:\n\t\t\t\tflist.append(os.path.join(root, f))\n\t\t\tres += len(flist)\n\t\treturn res\n\t\t\t\t\n\treturn count_files(zip_path)\n\n#------------------------------------------------------------------------------\n# Function to test a user defined file is loaded\ndef file_is_loaded(filename):\n\ttry:\n\t\twith"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t# https://stackoverflow.com/a/45278620\n\tfile_list = files_list(zip_path)\n\t# Get the files\n\tfrom IPython.core.image import get_image_data\n\timage_list, image_data = get_image_data(file_list[0])\n\t# Fill with images\n\tfor image in image_list:\n\t\timage_path = os.path.join(zip_path, image)\n\t\t# Check for.jpg\n\t\tif os.path.exists(image_path) and"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tf = open(zip_path, 'r')\n\tfiles = []\n\tfor l in f:\n\t\tf.readline() # Skip first line\n\t\tlist = []\n\t\tline = ''\n\t\tfor char in l:\n\t\t\tif char in '0123456789':\n\t\t\t\tlist.append(char)\n\t\t\telse:\n\t\t\t\tif line.endswith('\\n'):\n\t\t\t\t\tline += line[0: -4]\n\t\t\t\telif line.endswith('\\r'):\n\t\t\t\t\tline += line[0: -5]\n\t\t\t\tline += char\n"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tif not isinstance(zip_path, str):\n\t\tprint(\"Zip file path must be a str\")\n\t\treturn None\n\tzip_f = open(zip_path, \"r\")\n\tzlist = zip_f.read('*')\n\tzip_f.close()\n\treturn zlist\n\t\n#def image_list_from_zipfile(zip_path):\n#\treturn list(imread(zip_path, mode='RGB'))\n\n/*\n *  /MathJax/extensions/TeX/autoClosingBrackets.js\n *\n * "}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tglobal files\n\treturn files_list_from_zipfile(zip_path)\n\t\ndef files_list_from_zip_stream(zip_stream, max_files=None, include_dir=False):\n\t\"\"\"\n\t\n\tThe file `zip_stream` must be a file handle open for reading\n\t\n\tReturns a list of filenames in `zip_stream`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t\t['2710-4128-8137-4131-0a5f-0c5f-33-e20006."}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tfiles = {\n\t\t'1/2318-8e17-d5fb0-00ab': get_file_in_folder(get_path_from_url(zip_path)),\n\t\t'2/2318-0889-tinf-33-0421': get_file_in_folder(get_path_from_url(zip_path)),\n\t\t'3/2318-0889-tinf-33-e200069.pdf': get_file_in_folder(get_path_from_url(zip_path)),\n"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\ttry:\n\t\tfrom urllib.error import URLError\n\texcept:\n\t\ttry:\n\t\t\timport urllib2 as urllib\n\t\texcept:\n\t\t\tprint(\"Missing http:// or https:// support\")\n\t\t\traise\n\t\t\treturn\n\tdoc = urlopen(zip_path)\n\tfiles = []\n\t_dict = dict()\n\tfor sub in doc.infolist():\n\t\tif sub.filename == None: continue\n\t\ttry:\n\t\t\tfilename = sub.filename\n\t\texcept:\n\t\t\tfilename = sub.filename."}]}
{"task_id": "62b4631b3879012d194814dd", "prompt": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t# fixing prefix\n\tfixprefix = \"w:\"\n\tif \"w-\" in content:\n\t\t# fixing namespace prefix\n\t\tif len(set(content.rpartition(\"-\"))) == 1:\n\t\t\t# remove prefix from namespace-declarations if any (w- prefix is only in case of a ns-declaration)\n\t\t\tfixprefix = \"\"\n\twith closing(String.DoubleQuote) + fixprefix + content or None:\n\t\treturn content\n\treturn content.replace(\"w:st='\", \"w-st='\")\n\n\ndef fix_namespace_"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t#\n\t#\tFirst get the prefix and replace it between w and t or w-\n\t#\n\n\t#\n\t#\tParse the st: prefix\n\t#\n\tns, prefix = \"w-\", \"t-\"\n\n\ttry: str(content)\n\texcept: pass\n\n\tfor i in range(0, len(content), 2):\n\t\tif content[i] == prefix and content[i+1]==\"-\":\n\t\t\tcontent = content[:i] + content[i+2:]\n\n\ttry:\n\t\t#\tAdd namespace"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\treturn content.replace('\"', '').replace('<', 'w:')\n\n\ndef fix_namespace_prefix_b(content):\n\t\"\"\"\n\t\tConvert text that defaults to 'w:st=\"w:\"'\n\t\"\"\"\n\tcontent = content.replace('\"', '').replace('<w:', '&w:')\n\treturn content.replace('w:', 'w-')\n\n\ndef fix_xml_namespaces(content):\n\t\"\"\"\n\t\tConvert text that defaults to '''<namespace:/>''''' to '''<namespace/>'''\n\treturn content.replace('<namespace:', '</namespace"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tif not content:\n\t\treturn content\n\n\tprefix, content = content.rsplit(\" \", 1)\n\tif prefix.find(\"w\")!= -1:\n\t\tprefix, content = prefix.split(r\"(w:st)/\", 1)\n\n\treturn (prefix + r\"w-st\" + content) if content.find(\"w-st\")!= -1 else content\n\n\ndef fix_namespace_prefix_h(content):\n\t\"\"\"\n\t\tConvert text that defaults to 'h:st=\"' to 'h-st=\"'\n\t\"\"\"\n\t"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tcontent = re.sub('w:', 'w:st=\"W:\\/\\/' + W_PREFIX_DEFAULT_PREFIXING ', content)\n\tcontent = re.sub('w-st:\\S+', 'w:{W_PREFIX}{W_PREFIX_DEFAULT_PREFIXING}', content)\n\treturn content\n\n\ndef fix_xml_name_w(content):\n\t\"\"\"\n\t\tConvert text from 'xml:space=\"preserve\" to 'w:tx=\"' \n\t\t\"\"\"\n\tcontent = re.sub(r'xml:(space=\"preserve\")', r'w:tx"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tif content[:2] == '\"':\n\t\treturn content\n\treturn '\"%s\"' % content\n\n\ndef get_word_bounds(doc):\n\tbounds = []\n\tfor seg in doc.a: # Find all TextElement start/end points\n\t\tfor node in seg.iterfind(\".//p\"):\n\t\t\tpos = node.span(\"w:st\")\n\t\t\tstart = pos.int_pt(0) if pos.is_integer() else node.a[pos.start_point() - 1].a.p.x\n\t\t\tlength ="}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tcontent = content.replace('w:st=\"\"', 'w-st=\"\")')\n\tcontent = content.replace('w:entWrap=\"true\"', 'w-entWrap=\"true\")\n\tcontent = content.replace('w:tblLook=\"false\"', 'w-tblLook=\"false\")\n\tcontent = content.replace('w:tc=\"1\"', 'w-tc=\"1\")\n\tcontent = content.replace('w:tc=\"4\"', 'w-tc=\"4\")\n\tcontent = content.replace('w:tc=\"10\"', 'w-tc=\"10\")\n"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\ttext = re.sub('([a-z0-9]+)=\"', \"' \", text)\n\ttext = re.sub('(' + \"'w-st='''\", \"'w-st='\\\"'\", text)\n\t\"\"\"\n\tConvert \" \" in \"w - st \" to'' and \"w - st=''\" to'w=\"'\n\t\"\"\"\n\twhile text!= content:\n\t\tif text.find('=') == -1: return ''\n\t\telif text[0] == '\"' and text[1] == '\"' and text[-1] == \"'\" :\n"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tcontent = textwrap.dedent(content).strip()\n\treturn re.sub('\\\\w+@', 'w-' + content + '-', content)\n\npackage main\n\nimport (\n\t\"net/http\"\n\n\tstripe \"github.com/stripe/stripe-go\"\n)\n\nfunc main() {\n\tsource := stripe.NewAPI(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tfmt.Fprintln(w, \"Hello, Stripe API!\")\n\t}))\n\tdefer source.SetEndpoint(\"\")\n\n\tc"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tprefix = []\n\tfor word in content.split(' '):\n\t\ttry:\n\t\t\tsplit = word.split(':')\n\t\t\tif len(split) == 1:\n\t\t\t\tprefix.append(word)\n\t\t\telse:\n\t\t\t\tprefix += split\n\tprefix = re.split('w?:st=\\'(.+?)\\'', prefix)\n\treturn prefix\n\n\ndef fix_namespace_name(item):\n\t\"\"\"\n\t\tConvert attribute key namespace to Namespace Name\n\t\t    -1 -> nsNone\n\t\t-2 -> ns 'ns"}]}
{"task_id": "62b463283879012d1948153d", "prompt": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tnode_date = node.xml_date(pubdate_xpaths)\n\t#date must be in the range [0, 2^32]\n\tif node_date < datetime.min.date():\n\t\treturn node\n\n\tfor match in range(0, len(pubdate_xpaths) - 1):\n\t\tif node_date == pubdate_xpaths[match + 1]:\n\t\t\treturn node\n\n\treturn None\n\ndef match_time(node, seconds_xpaths):\n\ttime_xpath = \"\"\n\tfor xpath in seconds"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tmatch = node.get('match')\n\t\tif match is None:\n\t\t\tcontinue\n\t\tif pubdate(match.text) == str(pubdate_xpath).replace('//', '/'):\n\t\t\treturn pubdate_xpath\n\treturn None\n\ndef match_pubdate_without_leading_slash(node, pubdate_xpaths):\n\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor pubdate_xpath in pubdate"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tresult = None\n\tfor val in pubdate_xpaths:\n\t\ttry:\n\t\t\tif node.text() == val:\n\t\t\t\tresult = val\n\t\texcept (ValueError, AttributeError): pass\n\tpass\n\treturn result\n\n\ndef pubdate_to_xml(pubdate_xpaths):\n\t\"\"\"\n\tConvert pubdates to their corresponding XML representation.\n\t\"\"\"\n\tmatch_xpath_node = False\n\t# build XML\n\tout = xml.sax.make_parser()\n\txpaths = [str(x) for x in pubdate"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tmatch = None\n\tlast_xpath = \"\"\n\tfor index in 0, pubdate_xpaths.count():\n\t\tif pubdate_xpaths[index][0].strip() in node.pubdate:\n\t\t\tmatch = index\n\t\t\tbreak\n\tif match == None:\n\t\tfor index in (pubdate_xpaths.count() - 1), 0, len(pubdate_xpaths)-1:\n\t\t\tlast_xpath = pubdate_xpaths[index][len(last_xpath):]\n\t\t\tif last_xpath =="}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor mod_xpath in pubdate_xpaths:\n\t\tmatch_xpath = False\n\t\tfor xpath_element in mod_xpath:\n\t\t\tmatch_xpath = True\n\t\t\telem_text = xpath_element.text\n\t\t\tif XPath2Node(node).string_pattern(elem_text) == \"true\":\n\t\t\t\tmatch_xpath = True\n\t\t\t\tbreak\n\t\tif not match_xpath:\n\t\t\treturn None\n\treturn match_xpath\n\ndef match_date(node):\n\t\"\"\"\n\t\tReturns"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor path in pubdate_xpaths:\n\t\ttry:\n\t\t\tdate_match = datetime.datetime.strptime(\n\t\t\t\tnode.get('pubDate'), '%Y-%m-%dT%H:%M:%SZ'\n\t\t\t).astimezone(pytz.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n\t\texcept (KeyError, ValueError):\n\t\t\tdate_match = None\n\t\tif date_match is not None:\n\t\t\treturn time_to_pubdate(date_match)\n\nimport React from '"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tif node is None:\n\t\treturn None\n\t# First find the pubdate that we really have.\n\tpubdate = list(set(pubdate_xpaths[0:6]))\n\tl = len(pubdate) # Number of pubdates\n\tfor i in range(1, l):\n\t\tif pubdate[0] in node.dates:\n\t\t\tfor j in range(6, l):\n\t\t\t\tif node.dates[j] == pubdate[j]:\n\t\t\t\t\treturn node.nodes[i]\n\t\t\tl = j - 2\n"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tmatch_node, match_pubdate = node\n\n\tif match_node in pubdate_xpaths:\n\t\treturn True\n\n\treturn False\n\ndef match_xmlname(node, xmlname_xpaths, pubdate_xpaths):\n\t\"\"\"\n\tReturns the first match in the xmlname_xpaths list\n\t\"\"\"\n\tmatch_node, match_name = node\n\n\tif match_node is None or not node.lower().endswith('name'):\n\t\treturn None\n\n\txmlname_substrings = []\n\tfor subnode in node"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tif node is None: return None\n\tfor node in pubdate_xpaths:\n\t\tif hasattr(node, 'pubdate') and not node.pubdate == '':\n\t\t\tif hasattr(node, 'title') and not node.title == '':\n\t\t\t\tif node.title.lower() == pubdate_xpaths[0].name.lower():\n\t\t\t\t\treturn node.text.lower()\n\t\t\telif hasattr(node, 'xpath') and \\\n\t\t\t     str(node.xpath).lower() == pubdate_xpaths[0].name."}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tmatch = None\n\tfor xpath in pubdate_xpaths:\n\t\tfor rnode in xpath.iterchildrens():\n\t\t\tif not rnode.hascontent():\n\t\t\t\tcontinue\n\t\t\tif rnode.hasattr('pubDate') and rnode.hasattr('lastpubdate'):\n\t\t\t\tmatch = [rnode]\n\t\t\t\tbreak\n\treturn match\n\n\ndef match_node_nodes(node, node_xpath):\n\tmatch = None\n\tfor xpath in node_xpath:\n\t\tfor xpath in xpath.iterwhere"}]}
{"task_id": "62b463303879012d19481579", "prompt": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumber = 1\n\t\n\t# Check, if issue is a numbered item\n\tif isinstance(issue,etree.Element):\n\t\tnumber_elements = issue.xpath('string(.)')\n\t\tsuppression = int(number_elements[1])\n\t\t\n\t\twhile len(number_elements) > 2:\n\t\t\tsuppression *= 2\n\t\t\tnumber_elements = next(issue.xpath('string(%s)' %(number_elements[0])))\n\t\t\tsuppression += 1\n\t\t\t\n\t\tnumber = number_elements[0].replace(',', '.0')\n\t\t\n\t"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tif not issue.number.strip() and not issue.suppl.strip():\n\t\treturn None\n\t# First line of format the values.\n\tnumber_and_suppl_dict = {}\n\tnumber_and_suppl_dict['number'] = issue.number.strip()\n\tnumber_and_suppl_dict['suppl'] = issue.suppl.strip()\n\t# Then, parse the contents from issue.\n\n\t# Check for the \"##\" pattern\n        pattern = re.compile(r'(?:## )?(["}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tfor key,value in issue.items():\n\t\tif 'number' in value and'supplemental' in value:\n\t\t\treturn value['number'], value['supplemental']\n\t\treturn None, None\n\n\ndef _extract_short_form(issue):\n\t\"\"\"\n\tExtract a short form from an issue.\n\t\"\"\"\n\tshort_form = {}\n\tfor key,value in issue.items():\n\t\tif value['short_description']:\n\t\t\tname, value = value['short_description'].split(' ', 1)\n\t\t\tshort_form["}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t#if isinstance(issue, etree._Comment):\n\t#\treturn (issue.text.strip(), issue.level)\n\t#\treturn issue.element.string\n\t#elif isinstance(issue, etree.Comment):\n\t#\treturn issue.data\n\n#------------------------------------------------------------------------------\n# Extract the possible values of number and suppl from the list of issues\n#------------------------------------------------------------------------------\ndef _extract_number_and_supplment_from_issues(issues):\n\t\"\"\"\n\tExtract the possible values of number and suppl from the list of issues.\n\t\"\"\"\n\tout"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\treturn {\n\t\t\"number\": issue.find(\"span\", {\"class\": \"issue-issue--number\"}).text,\n\t\t\"supplement\": issue.find(\"span\", {\"class\": \"issue-issue--supplement\"}).text\n\t}\n\ndef _get_issues_from_data(\n\tdata,\n\ttitle,\n\tissue_status_field,\n\tissue_type_field,\n\tissue_fields_table,\n\tis_main_issue_item=False,\n\tis_main_issue_issue_item=False\n):\n"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t# Extract the type and format of the number/suppl from the issue element.\n\treturn type(issue.child('Number') or issue.child('Supplement'))\n\n\ndef issue_element_to_number(issue):\n\t# type: (xml.dom.minidom.Element) -> Union[Type[SuppliedValue], List[Type[SuppliedValue]]]\n\tif isinstance(issue, minidom_element_class) and \"number\" in issue.name:\n\t\t# Extract the type of suppl from the element and the contents of the element and"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tfrom report.fields import Numeric\n\ttry:\n\t\tif len(issue) == 0 or len(issue[0]) == 0:\n\t\t\treturn '', Numeric(0, '0')\n\t\telif len(issue) == 1:\n\t\t\treturn issue[0][0], Numeric(0, '0')\n\t\telse:\n\t\t\tfrom report.supplemental import Format\n\t\t\tnumber, suppl = issue[0].split(';', 1)\n\t\t\tformat, _ = Format.get_by_name(suppl.lower())\n\t\t\tif not format"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumber = issue.attrib.get('number')\n\tissue.text = issue.text.split()\n\tsuppl = issue.tag.rsplit('>', 1)[1]\n\n\tif number:\n\t\t# if the number is available in the xml, we are on the issue and the\n\t\t# supplment is the number\n\t\tif number in {'1': '0', '8': '0', '2': '1'}.get(suppl, False):\n\t\t\t# it's a bug, add it back in for the issue\n\t\t\treturn {"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tif \"#\" in issue.text():\n\t\t# supplements\n\t\tsuppl_re = re.compile(r\"([a-zA-Z0-9])([0-9]+)\\b\")\n\t\tre_suppl = re.compile(r\"\\d+\")\n\t\tfor match in re_suppl.finditer(issue.text()):\n\t\t\t# number\n\t\t\tnumber = match.group(1)\n\t\t\t# supplement\n\t\t\tsup = match.group(2)\n\t\t\tif not re.match(suppl_re,"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumber_values = issue.xpath(\"@value\")\n\t# extract possible value of number and supplement.\n\tif number_values:\n\t\tfor text_value in number_values:\n\t\t\tmatch_number, _ = clean_value(text_value)\n\t\t\tif match_number in Suppl.number_and_supplement:\n\t\t\t\tsupplment = Suppl.number_and_supplement[match_number]\n\t\t\t\treturn supplment.suppl\n\t\t\telif match_number == '-' and not Suppl.number_and_supplement:"}]}
{"task_id": "62b46740d2f69a53b466171a", "prompt": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\ttry:\n\t\tinfo = inspect.getframeinfo(inspect.currentframe())\n\t\tname = info[0]\n\t\tmod = info[1]\n\texcept AttributeError:\n\t\tname = \"<unknown>\"\n\t\tmod = \"<not defined>\"\n\n\treturn indent(11) + '<module name=\\'%s\\' type=%s>' % (name, mod)\n\ndef _print_doc():\n\tfrom. import sys\n\tif sys.flags and not sys.flags.no_write:\n\t\tflags = sys.flags\n\t\tmod_"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tindent = indent*self.INDENT_SIZE\n\tindent = indent + \" \" * self.INDENT_SIZE\n\t\n\treturn indent + format_str(self._str, self.STYLE, self.PRETTY)\n\n\nclass Trace(Object3D):\n\t\"\"\"\n\tA simple 3D object for tracing\n\t\"\"\"\n\tSTYLE = 0\n\tALIGN = False\n\tCENTER = False\n\tSCALEX = 1\n\tSCALEX = 1\n\tCENTERX = 0\n\tSCALEXX = 1\n\n\nclass Color(ColorBase):\n\t\"\"\"\n\tBase class for"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tout = \"<class 'bpy.types.bpy_struct.Struct'>\"\n\tout += pretty_type(self.data, \"\", False)\n\tout += \"</class>\\n\"\n\treturn out\n\ndef find_duplicate_objects(self, objects=[]):\n\t\"\"\"\n\tIterate over all duplicated objects, if any.\n\t\"\"\"\n\tfor o in objects:\n\t\tif o.duplicate_check(ignore=[]):\n\t\t\tprint(\"WARNING! Possible duplicate (in %r) '%s' referenced in %r.\" % (o.name,o.name"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tif not self.is_valid:\n\t\traise ValueError(\"Attempt to print a node whose type has not yet been set; aborting!\")\n\tr = []\n\tif debug:\n\t  print(\"pretty()\")\n\tfor t in self.node_tree.keys():\n\t\tif t.is_constant():\n\t\t\tcontinue\n\t\tr.append('%s:\\n%s' % (str(self.node_tree.keys()), t.get_pretty()))     \n\treturn '[' +''.join(r) + ']'\n\t\t\ndef get_node"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tindent_str ='' * 4 * indent\n\tif debug:\n\t\tprint('', indent_str)\n\t\tfor field in self.fields.values():\n\t\t\tprint(field, field.name)\n\t\tprint('', indent_str)\n\t\treturn\n\tfor field in self.fields.values():\n\t\tif field.debug:\n\t\t\tprint(field)\n\t\telse:\n\t\t\tprint('', field.name)\n\t\t\tfor line in field.dump(indent_str=indent_str):\n\t\t\t\tprint(' '*indent_str*"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\ttry:\n\t\tresult = [self.__class__.__name__]\n\texcept:\n\t\t# It's possible (to the string method) to get a non-pretty method\n\t\tresult = repr(self)\n\n\tif self.has_key('__module__'):\n\t\tmod_name = self.get('__module__')\n\t\tif mod_name is not None:\n\t\t\tmodule_name = mod_name.__name__\n\t\telse:\n\t\t\tmodule_name = str(self.get('__module__'))\n\n\t\tresult"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\ttry:\n\t\tself.dump_data()\n\texcept:\n\t\treturn None\n\tif debug:\n\t\tprint('Pretty: %s' % self._repr_fields())\n\t\treturn {'data': self._data}\n\treturn None\n\n\t#-------------------------------------------------------------------------\n\n\t# Private function kept for backward compatibility\n\nclass NodeList(list):\n\t\"\"\"\n\tBase class for node lists.  This is the base node list class that defines\n\thow to handle node lists.  It supports adding, popping and removing nodes,\n\tand removing them.\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tindent = \" \" * indent\n\tres = []\n\tif debug:\n\t\tdebug_start(indent, 'print')\n\tfor var in self.vars:\n\t\tres.append(self.vars[var][0]._repr(indent + \"  \"))\n\treturn \"print(\" + ''.join(res) + \")\"\n\n\ndef extract_variables(expr):\n\t# this is to maintain backwards compatibility to the original\n\t# python-doc/functions.c.print:function_pretty() function.\n\treturn [var.extract('py_extract_from"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\td=dict(self.__dict__)\n\ttry:\n\t\tkeys, values=d.items(), [d[k] for k in sorted(d.keys())]\n\t\td = dict(zip(keys, values))\n\texcept TypeError:\n\t\t# d is of unexpected type\n\t\tpass\n\tif not d: d={}}\n\tindent=indent-1 if debug else ''\n\tif isinstance(self.__dict__,dict):\n\t\tindent=''* indent\n\treturn '{%s%s}' % (indent, sorted(d.items()))\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tif debug:\n\t\tlines = 1\n\telse:\n\t\tlines = 5\n\treturn self.__class__.__name__ + '({%d: %s}, '%(self._width, self._val)\n\t'| ', lines\n\n#####################################################################\n\nclass _Empty(object):\n\tpass\n\n#####################################################################\n\nclass Node(dict):\n\tdef __init__(self, val=None, width=None):\n\t\tsuper(Node, self).__init__()\n\t\tself._val = self._create_val()\n\t\tself.width ="}]}
{"task_id": "62b46746d2f69a53b4661722", "prompt": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\targs = get_args(args)\n\tfor expr in args:\n\t\tif expr is not None and expr.args!= ():\n\t\t\ta = self._apply(absorb(expr.args), args + [expr])\n\t\t\t# return a\n\t\telif expr is None or expr is not None:\n\t\t\t# an expression or its negation raises a ValueError\n\t\t\traise NotImplementedError(\n\t\t\t\t\"negative absorption not supported, use 'absorb'\"\n\t\t\t)\n\treturn args\n\n\n__all__ = (\n\t\"Absorption"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\treturn [self.map.func(*args), -self.apply()]\n\n\n############################\n##   Function - Apply the function\n############################\n\ndef func(*args):\n\t\"\"\"\n\tGiven an object `args`, return a new object.\n\t\n\tIn this scenario `args` will be a list or tuple of arguments, which may\n\tinclude operators such as & (A & B), (A &), (A | B) which will not add\n\toperators.  The functions in the list/tuple are treated as ordinary objects\n\tand should not be used in conjunction with arguments"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\texpr = args[0]\n\tcond = args[1]\n\tif not isinstance(expr, Sequence):\n\t\treturn expr, False, False\n\telse:\n\t\treturn expr, cond, True\n\ndef ac(x):\n\treturn x\n\ndef ad(x):\n\tx.append(1)\n\treturn x\n\ndef adj(x, d):\n\tx.append(d * 2)\n\n\nclass F_test(unittest.TestCase):\n\n\tdef __init__(self, fun, a=None, b=None):\n\t"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\targs = list(args)\n\targs.append('~(A & B)')\n\targs.append('~(~A & B)')\n\treturn '|'.join(\"{}\".format(e) for e in args)\n\ndef add_to_file(file_name, line_string):\n\tfile_name = file_name.replace(' ', '_')\n\tfile_name = file_name.replace('\\t', '_t')\n\tfile_name = file_name.replace('\\n', '_n')\n\tfile_name = file_name.replace('\\r', '_"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\telif isinstance(args[0], list) and len(args) == 2 and isinstance(args[1], list):\n\t\targs = args[1]\n\t\targs_set = set([expr.as_unary_op() for expr in args])\n\t\t\n\t\tnew_ops = []\n\t\tfor op in args:\n\t\t\tnot_ok = True\n\t\t\tfor expr in op.args:\n\t\t\t\tfor op in op.args:\n\t\t\t\t\tif op.is_binary_op() and isinstance(expr, list) and expr!= []:\n\t\t\t\t\t\t"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\treturn args\n\n\n@def lift(self, args):\n\t\"\"\"\n\t\tGiven a list of expressions, return some expressions that, when\n\t\tthe value of an expression is `lifted` to `self`, replace the original\n\t\tvalues with `new` and return the updated argument `val`.\n\t\t \n\t\tThis function always preserves the order of expressions but the\n\t\tinterpretation of args::\n\t\t\n\t\t    [A & (B | (C | D)) = A | (B & (C | D)) = A]\n\t\t\n\t\tThis means"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\targs = list(args)\n\tasargs = lambda x: x.asarg(args)\n\tdef f(self, args):\n\t\t\"\"\"\n\t\tReturn the list of expression applying ``self`` on ``args``.\n\t\t\n\t\tArgs must be of one of the following forms:\n\t\t\n\t\t\t* expr:  Apply ``self`` to ``expr``.\n\t\t\t* list:  Apply ``self`` to each element.\n\t\t\"\"\"\n\t\tif len(args) == 1:\n\t\t\treturn args\n\t\telif len(args) == 2:\n\t\t"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\targs = list(args)\n\t\n\tif len(args) == 0:\n\t\treturn []\n\telif len(args) == 1:\n\t\treturn [args[0]]\n\t\n\top = args[0]\n\topargs = args[1:]\n\texpr = args[:1]\n\t\n\tif _is_binary(op):\n\t\top = op.conjugate()\n\t_expr = op.apply(opargs)\n\t\n\t# The absorption law is like \"A & B\" -> (A | B) = A & B\n\t"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tif len(args) <= 2:\n\t\treturn args\n\targs = [ x.strip() for x in args ]\n\tif len(args) > 1:\n\t\treturn args, True\n\telse:\n\t\texpr = args[0].strip()\n\t\tif isinstance(expr, ast.Expr):\n\t\t\texpr = expr.op\n\t\treturn [\n\t\t\texpr for expr in expr.args\n\t\t\tif (expr.op == \"__ne__\"\n\t\t\t\tor expr.op == \"__and__\"\n\t\t\t\tand not ast.expr."}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tif not args:\n\t\treturn []\n\tif isinstance(args[0], Mul):\n\t\tm, n = args\n\t\tif m and not isinstance(m, Mul):\n\t\t\targs = [m, n]\n\t\treturn args\n\treturn args\n\n/* -*- mode: C++; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n// vim: ft=cpp:expandtab:ts=8:sw=4:softtabstop=4:\n#ident \"$Id$\"\n/*\nCOPYING CONDITIONS NOTICE:\n\n  This program"}]}
{"task_id": "62b86707b4d922cb0e688c2a", "prompt": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\thooks = {'before': [], \n\t\t'after': [], \n\t\t'make_request': [],\n\t\t'trigger': []}\n\n\tresult = list(hooks['before']) + [hook]\n\t\n\thooks['before'].append(hook)\n\treturn (result, hooks)\n\ndef default_on(hook):\n\t\"\"\"\t\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t# if an \"on\" is already present on this list, keep this from removing it\n\ttry:\n\t\tif listener_on:\n\t\t\tfor l in listener_on:\n\t\t\t\ttry:\n\t\t\t\t\tl.remove()\n\t\t\t\texcept Exception:\n\t\t\t\t\t# can happen if some event handler is called before any of the above\n\t\t\t\t\tpass\n\texcept ValueError:\n\t\tpass\n\t\n\t# if this is the first listener handler from this list, do we now have\n\t# another list?\n\tif listener_on is None:\n\t\treturn lambda *"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\treturn lambda: self._registry.on(hook, self)\n\ndef once(self, hook):\n\t\"\"\"\n\tDecorator function to add a new handler to the registry\n\t\n\tHook type should be an event handle or an event instance\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\treturn lambda: self._registry.once(hook)\n\ndef off(self, hook):\n\t\"\"\"\n"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef decorate(hook):\n\t\thook.on('after')\n\t\t\t   \n\t\tif hook.handler is None:\n\t\t\thook.register(self.callback_function)\n\t\t\t\n\t\treturn hook.handler\n\t\t\nclass CallbackRegistry(object):\n\t\"\"\"\n\tRegisters callbacks to a particular hook.\n\t\n\tEach callback is called for an instance of the given hook type. The method\n\tof the callback is responsible for removing the event listener for the\n\tpassed instance, or simply passing the event data unmodified for the\n\tpassed instance.\n\t\"\"\"\n\tdef __"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef add_listener(attr=None, hook=None):\n\t\tif attr is None and hook is None:\n\t\t\traise ValueError(\"Attribute or hook name must be supplied\")\n\n\t\tif hook is not None and not isinstance(hook, HookType):\n\t\t\traise ValueError(\"Hook must be a Hook instance or None\")\n\t\t\n\t\treturn AttributeDecorator('get_listener', add_listener,\n\t\t\tattr=attr, hook=hook)\n\t\t\n\tdef add_logger(attr=None, hook=None):\n\t\tif attr is None and hook is None:\n"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tassert hook is not None\n\tif hook not in sys.hooks:\n\t\traise ValueError('Invalid hook identifier supplied.')\n\treturn function(hook, (), self.handler)\n\n# -*- encoding: utf-8 -*-\n\nimport unittest\nimport io\nimport logging\n\nimport os\nimport sys\n\nimport pandas as pd\nimport numpy as np\n\nimport qgis.dumb as dumb\nimport qgis.core.exceptions as exceptions\nimport featuretools.utils\n\ntry:\n    from osgeo import ogr\nexcept ImportError:\n      "}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\treturn _lambda_hook_factory(\"_on\", hook)\n\ndef off(self, hook):\n\t\"\"\"\n\t    Decorator function to remove the specified handler from the registry.\n\t\n\t    Args:\n\t\t    hook (HookType): Name of handler to remove.\n\t\n\t    Returns:\n\t\t    callable: Decorator for unregistering listeners for the specified\n\t\t    hook.\n\t    \"\"\"\n\treturn _lambda_hook_factory(\"_off\", hook)\n\ndef register(self, hook):\n\t\"\"\"\n\t    Decorator function"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\treturn decorated(on, hook)\n\ndef on_new_hook(self, hook):\n\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\treturn decorated(on_new_hook, hook)\n\ndef clear(self):\n\t\"\"\"\n\tDecorator function to remove all handlers from the registry.\n\t\n\tReturns:\n"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\treturn _OnHookFunction(self, hook)\n\n@registry.listen(ALL, 'PRE_RUN')\ndef onall(self, hook):\n\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\treturn _OnHookFunction(self, hook)\n\n@registry.listen(ALL, 'POST_RUN')\ndef"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tif not hasattr(handler_mapping, hook):\n\t\thandler_mapping[hook] = set()\n\treturn decorator(on_generic_func(hook, on_handler = self.on))\n##\n\n####\n# Internal\n\ndef on_handler(hook):\n\t\"\"\"\n\tFunction: internal wrapper hook to be removed from registry.\n\t\"\"\"\n\thandler_mapping[hook].add(hook)\n\tif callback_mapping[hook]:\n\t\tcallback_mapping[hook].add(True)\n\ndef remove_handler(hook):\n\t\"\"\"\n\tFunction"}]}
{"task_id": "62b86729b4d922cb0e688c2f", "prompt": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tconf = copy.deepcopy(DEFAULT_CONF)\n\tconf.update(user=str(user).upper())\n\tconf[\"username\"] = str(user).upper()\n\tconf[\"admin_access_token\"] = str(create_access_token(etcd_host).encode()).upper()\n\tconf[\"private_access_token\"] = str(create_access_token(etcd_host, \"private\").encode()).upper()\n\tconf[\"admin_private_token\"] = str(create_access_token(etcd_host, \"admin\").encode()).upper()\n\tconf[\""}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tbase_config = {\n\t\t'version': \"1\",\n\t\t'user': user,\n\t\t'database_storage': {\n\t\t\t'src_directory': '/data',\n\t\t\t'sub_directory': '/data/sub_directory',\n\t\t\t'default_collection': '',\n\t\t\t'db_url': 'http://127.0.0.1:26379',\n\t\t\t'host': None\n\t\t},\n\t\t'logging_level': \"NONE\"\n\t}\n\tuser_config = base_config.copy()\n\tuser"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tcfg = {\n\t\t\"version\": 1,\n\t\t\"auth_type\": 2,\n\t\t\"default_ttl\": \"2m\",\n\t\t\"ttl\": \"2m\",\n\t\t\"write_journal_size\": \"5000\",\n\t\t\"auth_method\": {\"name\": \"BASIC\", \"user\": user},\n\t\t\"auth_type_method\": {\"name\": \"BASIC\", \"user\": user},\n\t\t\"auth_type_group_method\": {\"name\": \"BASIC\", \"user\": user},\n\t\t\"max_ttl\": \""}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t# Setup default configuration\n\tdefault_config = {\n\t\t\"email\": \"\",\n\t}\n\n\t# Setup a config for the user that is trying to authenticate with etcd\n\tauth_config = {\"credentials\": {\"path\": etcd_host, \"cert\": etcd_port}}\n\tauth_config[\"username\"] = user\n\treturn auth_config\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tassert config.valid()\n\t\n\tconfig.CONF.set_bool('etcd.enabled', True)\n\tconfig.CONF.set_long('etcd.store.memory.mb', '5000')\n\tconfig.CONF.set_bool('auth.enabled', True)\n\tconfig.CONF.set_long('auth.provider.database.ttl', '30 days')\n\tassert config.CONF.valid()\n\tconfig.CONF.set_str('auth.provider.database.url', 'http://url')\n\n\tconfig.CONF.set_bool('key.enabled',"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tconfig = {}\n\t\n\t# The base config uses the host setting for the certificate name, the default one for the domain\n\tconfig[\"server\"] = {\"cert\": {\"host\": \"%s:%s\" % (etcd_host, etcd_port)}}\n\n\treturn config\n\n/**\n * Copyright (c) 2000-present Liferay, Inc. All rights reserved.\n *\n * This library is free software; you can redistribute it and/or modify it under\n * the terms of the GNU Lesser General Public License as published by the Free\n * Software Foundation; either version 2.1 of the License"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\treturn {\n\t\t\"auth\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"login_timeout\": 10,\n\t\t\t\"http_timeout\": 30,\n\t\t\t\"ssl_timeout\": 5,\n\t\t\t\"default_url\": \"http://\" + etcd_host + \":\" + etcd_port + \"/w\",\n\t\t\t}\n\t\t}\n\t\n\t\ndef run_from_yaml(config_dict, yaml_file, args=None):\n\t\"\"\"\n\tConverts yaml based parameters in a configuration,\n\t\twith an optional default value for user."}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t# set up logging\n\tlog = logging.getLogger(__name__)\n\t\n\t# define the default parameter values\n\tkwargs = {\n\t\t\"cluster\": \"my_clusterName\", \n\t\t\"namespace\" : \"my_namespace\", \n\t\t\"username\" : \"my_username\", \n\t\t\"password\" : \"my_password\", \n\t\t\"pool size\" : 1, \n\t\t\"cache size\" : 1, \n\t\t\"query size\" : 10, \n\t\t\"request rate\" : 1, \n\t\t\"response rate\" : 5, \n\t"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tstatic_schema = ConfigSchema()\n\tstatic_schema.add_option(Option(\"host\", default=\"localhost\", help=\"the host for the database.\"))\n\tstatic_schema.add_option(Option(\"port\", default=2379, help=\"the port for the database.\"))\n\t\n\tconfig = {'options': static_schema.get_options()}\n\n\tif not user:\n\t\tconfig['options']['user'] = \"\"\n\n\tif not etcd_host:\n\t\tconfig['options']['etcd_host'] = \"\"\n\n\tif not etcd_port:\n"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tconfig = dict()\n\tnew_conf = dict()\n\tnew_conf[users_root_path] = os.path.join(os.getcwd(), f\"conf/user_{user}/conf.json\")\n\tjson.dump(new_conf, open(new_conf[users_root_path], \"w\"), indent=2)\n\tconf_file = open(new_conf[users_root_path], \"r\")\n\tconf = json.load(conf_file)\n\tfor c in conf[None]:\n\t\tnew_val = \"new_"}]}
{"task_id": "62b8a4a4755ee91dce50a3d2", "prompt": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif not isinstance(dt, tzinfo):\n\t\t# this is a valid datetime in a timezone; what else is it?\n\t\t# This must be a datetime object.\n\t\t# XXX we're converting \"datetime.datetime\" objects at\n\t\t# the moment since I can't believe that they are chronologically\n\t\t# ordered, so I'm going to do it now.\n\t\tdt = date_from_float(dt.year, dt.month, dt.day)\n\t\t\n\t# calculate a timezone-aware datetime instance from an unambiguous\n\t#"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif not isinstance(dt, datetime):\n\t\traise Exception(\"Invalid dt type %r: %r\"%(dt,type(dt)))\n\n\ttz = self._default_timezone.timezone if self._default_timezone is not None else dt.tzinfo #\n\tutc_now = dt.astimezone(tz)\n\t# We only care about \"folded\" state on a new timezone, so try to just\n\t# guess \"true\" here.\n\tif utc_now.utcoffset(False) > dt.utcoffset(True):\n\t\tself._"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\ttm = datetime(*t for t in self.tzinfo.normalize_times(dt))\n\treturn self._fromtimedelta(tm.replace(tzinfo=self.tzinfo))\n\n\nclass timezone:\n\t\"\"\"\n\t:class:`timezone` implements a time zone that is aware of local time.\n\t\n\t   >>> from datetime import timezone\n\t   >>> timezone('America/Los_Angeles')\n\t   timezone('America/Los_Angeles')\n\t   \"\"\"\n\t_offset = timedelta(0)\n\t\n\t_name_parts = [\""}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# We want a date in that timezone\n\tdt.tzinfo = None\n\t# Convert our \"best guess\" into a \"new timezone\"\n\tnew = dt.replace(_tzinfo_global_utc)\n\t\n\t# If we're in a fold state, the given datetimes should fall on a\n\t# \"fold\" state instead of one that's ambigous. We need to set the\n\t# ambiguous zone so timezone operations don't fold into a fold state.\n\tif new.fold:\n\t\ttry:\n\t\t\t# The only way I can realistically see"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# TODO\n\t# this should probably be a timezones.gettz function, as it's an inline\n\t# function that looks much like that, but we don't currently have a working\n\t# locale for that\n\t\n\t# convert date/time pairs into datetime objects\n\tdt = self._dt_localize(dt)\n\tdt = dt.tzinfo\n\t\n\t# find the timezone in which the datetime is ambiguous, and\n\t# fold all instances where possible\n\tts = dt.replace(tzinfo=self._dt_TZ)\n\t\n\t# fold the datetime, since it"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\traise NotImplementedError()\n\ndef set_default_timezone(utc=True):\n\t\"\"\"\n\tLet this function be called by the system.\n\t\n\t:param bool utc:\n\t    Whether we want this function to set the default time zone to use,\n\t    or assume the default from a local time zone.\n\t\"\"\"\n\t\n\ttimezone = None\n\ttry:\n\t\tnow = udate()\n\t\t\n\t\tif utc:\n\t\t\t# timezones get in UTC\n\t\t\twith timezone(TIMEZONE_UTC):\n\t\t\t\ttime.tzset()\n\t\t\t"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# get the current timezone\n\ttimezone_offset_hours = get_current_timezone_offset_hours()\n\t\n\t# now get the difference between this timezone and the \"time zone\n\t# we are considering\", so that if we have the same time and\n\t# timezone but differed on a different time then we can always\n\t# guess \"the first occurrence of this datetime\"\n\tnew_timezone_offset_hours = dt.timetuple().tm_sec % 60\n\t\n\t# now look for ambiguities (the largest)\n\tambiguous_timestamps = get_amb"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif not self.tzinfo:\n\t\tself.tzinfo = TIME_ZONE\n\n\tif hasattr(dt, 'tzinfo'):\n\t\tdt += self.tzinfo\n\n\treturn dt.replace(tzinfo=None)\n\nimport { h } from 'omi';\nimport createSvgIcon from './utils/createSvgIcon';\nexport default createSvgIcon(h(\"path\", {\n  d: \"M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zM1."}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\toffset = 0.0;\n\tdate = dt.replace(tzinfo=self.tzinfo).tzinfo.utcoffset(dt) + self.min\n\n\t# Try to calculate the new timezone using the timezone offset\n\t# and date for the ambiguous time.\n\t# \n\t# XXX This needs a lot of work.\n\t#\n\t# This loop could probably be replaced by one that takes the timezone offset as\n\t# an offset, but I think it's more consistent to do it here as there could be\n\t# ambiguity.  It's not a big deal"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tt = time.gmtime(self.microseconds)\n\tresult = dt.replace(tzinfo=timezone.utc).astimezone(t)\n\tif result.tzinfo is not None: # tzinfo is None => ambiguous timezone\n\t\treturn date.min\n\treturn date.max\n\nclass Duration(object):\n\t'''\n\tCreate a Duration from any date\n\n\tArgs:\n\t\tdt:\n\t\t\tdate or DateTime object; If None, creates a :class:`duration.Duration`\n\n\tRaises:\n\t\tTypeError:\n\t\t\tunless `"}]}
{"task_id": "62b8982f755ee91dce50a241", "prompt": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn relativedelta(relativedelta(days=1.5, hours=2).normalized())\n\t\n\t# ToDo: Use this method when comparing dates\n\tpass\n\t\n\t# >>> delta = date.Date(2018, 5, 4)\n\t\n\t# >>> delta.normalized()\n\t# relativedelta(days=4, hours=5, microseconds=342400000)\n\t\n\t# >>> delta.normalized()\n\t# relativedelta(days=+4, hours=+55, microseconds=342400000)\n\t\n\t#...\n\t\n\t# >>> _ = date.Date(2018,"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn relativedelta(relativedelta.unit.factory(int(x)), int(x) % 12)\n\n\n\n# Utility functions\n\ndef isoformat(self):\n\t\"\"\"\n\t\"Format a datetime for display in a time zone\"\n\t\n\t>>> isoformat(datetime(1997, 4, 3, 10, 0, 0))\n\t'Mon Feb 17 00:00:00 1997'\n\t\n\tNote: If you pass in a timezone object as the value or\n\ttimezone.tzinfo object, and that time zone is known to\n\tare in effect, then:\n\t\n"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn self.copy_with_attrs(attrs=self.attrs)\n\ndef relativedelta(self, attrs=None):\n\t'''\n\t    Return an \"delta\" instance of this object.\n\t\n\t:param attrs:\n\t\tIf not ``None``, then the new delta will have the\n\t\tattributes of the other object.  See the\n\t\tdocumentation of the other :class:`dateutil.relativedelta.relativedelta`\n\t\tmodule for detailed description of the attributes.  For example::\n\n\t\t\tdelta_obj = relativedelta(years=+100, months"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn relativedelta(relativedelta_dict(self))\n\nclass relativedelta(object):\n\t\"\"\"\n\tRepresent a relative delta against a datetime object, representing a\n\tdifferent time:\n\t\n\t>>> dateutil.relativedelta(days=1.5, hours=2)\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:param days Number of days.\n\t:type days: integer\n\t\n\t:param hours Number of hours.\n\t:type hours: integer\n\t\n\t:param minutes Number of minutes.\n\t:type minutes: integer\n\t\"\"\""}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn relativedelta(*normalize_datetime(y=self.year, m=self.month, d=self.day),\n\t                     timedelta(h=self.hour, m=self.minute, s=self.second),\n\t                     timedelta(hour=-1))\n\n\ntimedelta.microseconds = timedelta.timedelta = _timedelta\n\n\ndef _timedelta_from_datetime(dt):"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\traise NotImplementedError('%s.normalized()' % self.__class__)\n\t\n\t\n\t\ndef __init__(self, delta):\n\t\"\"\"\n\tConstructs an absolute and cascading deltas from a delta object.\n\t\"\"\"\n\t\n\tself.years    = delta.years\n\tself.years_tt = delta.years - delta.days * 365.25\n\tself.days     = delta.days     # 365 days\n\tself.seconds  = delta.seconds  # floating point seconds\n\tself.minutes  = delta.minutes  # floating point"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\tres = relativedelta(self.days, self.hours)\n\treturn self._make_delta(self.days, self.hours, res)\n\n\n\n\ndef isdst(self, dst):\n\t\"\"\"\n\tReturns `True` if the time delta, `self`, is relative to DST, `dst`,\n\tand not negative.\n\t\n\t>>> delta = relativedelta(days=1.5, hours=2)\n\t>>> isdst(delta, '-10:-20')\n\tTrue\n\t>>> delta = relativedelta(days=-5.0, hours=2"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\tself._relativedelta_dict['days'] = self._relativedelta_dict['hours']\n\treturn self._relativedelta\n\ndef relativedelta(self, delta=None, validate=True,\n\t\trelativedelta_factory=None,\n\t\thour=None, minute=None,\n\t\tsecond=None, **kwargs):\n\t\"\"\"\n\tConstruct a relative date instance representing a value *delta*,\n\trepresenting the timedelta since midnight in *hour*, *minute* and\n\t*second*.  The absolute value of *delta* depends on the date\n\tinterpretation"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn relativedelta(*self._relative_attributes)\n\n/*******************************************************************************\n * Copyright (c) 2019, 2021 IBM Corp. and others\n *\n * This program and the accompanying materials are made available under\n * the terms of the Eclipse Public License 2.0 which accompanies this\n * distribution and is available at http://eclipse.org/legal/epl-2.0\n * or the Apache License, Version 2.0 which accompanies this distribution\n * and is available at https://www.apache.org/licenses/LICENSE-2.0.\n *\n * This Source Code may also be made available under"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\tfrom dateutil.relativedelta import relativedelta\n\treturn relativedelta(self._epoch, self._start)\n\ndef relativedelta(self, other=None, tolerance=True):\n\tif tolerance and isinstance(other, dateutil.relativedelta.RELATIVE_OTHERS):\n\t\tfrom dateutil import relativedelta, relativedelta_days\n\t\tother = relativedelta_days(*relativedelta(self, other))\n\treturn relativedelta(other, self, tolerance)\n\ndef relativedelta_days(*args):\n\tif args:\n\t\traise TypeError('relativedelta_days()"}]}
{"task_id": "62b89640755ee91dce50a114", "prompt": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t# Note: To be able to use same variable in both cases and case as a closure,\n\t# this method MUST return bytes!\n\t# In Python2 Unicode are stored like bytes, but we want to return a list \n\t# of unicode strings rather then bytes\n\t# We will convert the return from bytestrings to unicode strings from bytestrings:\n\t#      utf8bytes(bytestring) -> utf16_str(Unicode string)\n\t# This also converts the bytestring to a list of unicode strings like bytes -> unicode strings\n\t# TODO: There is no way to"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tdef __init__(self):\n\t\tsuper(tzname_in_python2,self).__init__()\n\t\tself.__name__ = namefunc.__name__\n\t\n\tdef _str_wrapper(self,namefunc):\n\t\targ=namefunc()\n\t\ttry:\n\t\t\toutput=str(arg.__self__()._bytes_,'utf-8')\n\t\texcept AttributeError:\n\t\t\t#namefunc() needs bytes\n\t\t\toutput=unicode(arg, 'utf-8')\n\t\treturn output\n\t\n\tdef _format(self,obj,"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tdef _str_or_bytes(name, *args): \n\t\treturn name(*args)\n\treturn util.dict_merge(_str_or_bytes, unicode)\n\n\nclass tznames(tznamer):\n\t'''\n\tNew 'tznames' API\n\t'''\n\tdef new_tz(self):\n\t\t'''\n\t\tTweak a timezone name in tznamer to a tzname\n\t\t'''\n\t\tdef _str_or_bytes(name, *args): \n\t\t\ttry:\n\t\t\t\treturn _new_tuple(name,"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tif len(namefunc)>=2:\n\t\treturn namefunc(name)\n\treturn name\n\nclass namefunc_in_python3(namefunc):\n\t\"\"\"\n\t\tFor python2, unicode-ness is in python3\n\t\t\"\"\"\n\tdef __call__(self, name):\n\t\treturn name\n\nclass namefunc_in_python3_3(namefunc):\n\t\"\"\"\n\t\tFor python2, unicode-ness is in python 3.3\n\t\t\"\"\"\n\tdef __call__(self, name):\n\t\t#return unicode(name"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tif sys.versions[0] <= 2:\n\t\treturn namefunc\n\telse:\n\t\treturn namefunc.encode('utf8')\n\n# Local Variables:\n# tab-width:4\n# indent-tabs-mode:nil\n# End:\n# vim: set expandtab tabstop=4 shiftwidth=4:\n\n/** \n \nCopyright 2013 Intel Corporation, All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t# Python 2 needs some unicode support\n\tdef convert_tzname(name):\n\t\t# Note that this raises an error, not an exception, if given invalid unicode.\n\t\tname = name.decode('UTF-8')\n\t\t# Also, don't try to encode None as a length. Python itself will call\n\t\t# str.maketrans() so don't add it to the output.\n\t\tif name:\n\t\t\ttry:\n\t\t\t\tname = unicodedata.normalize('NFD', name).encode(unicodedata.name) # Normalize before encoding\n\t\t"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\treturn tzname_in_python3(namefunc)\n\ndef timezonedata_in_python2():\n\t\"\"\"\n\t\tChanges the data type in Python 3. In Python < 2.4 the list() method \n\t\twill return 'time', 'gtime', 'et', 'zonewin', 'utm', 'zoneinfo' and \n\t\t'localtime', respectively, as returned by localtime. Instead of a \n\t\tunicode string the code below will work with any unicode string as well.\n\t\t\n\t\tIn Python >= 2.2 `timezonewin`"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tname = namefunc.__name__\n\t\n\t# For Python < 2.5 we use the C extension\n\tif PY2.get_python_version() < 3:\n\t\tnamefunc = namefunc.encode('ascii')\n\t\t\n\treturn name\n\n### date stuff\n\n## this should be datetime.datetime, but that needs fixing. we're stuck with that,\n# and it's too late.\nif not PY3:\n\ttry:\n\n\t\timport datetime\n\texcept ImportError:\n\t\tpass\n\t## dateutil knows all Python 2 code so we can"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\treturn namefunc\n\ndef tzname_in_python3(namefunc):\n\t\"\"\"\n\t\tChange unicode output into bytestrings in Python 3\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\treturn namefunc\n\ndef _get_win_size(platform, winver):\n\t\"\"\"\n\t\tReturn (x, y) screen size using WinAPI functions\n\t\n\t\tPlatform should be a 2-tuple like (name, version)\n\t\t\n\t\twinver version as returned by GetSystem"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tu = unicode(namefunc())\n\tif isinstance(u,str):\n\t\tu = unicode(u,u)\n\t\tif u not in \"utf-8\" and u not in \"\\n\":\n\t\t\traise ValueError(\"invalid time zone string\")\n\t\tif hasattr(time, \"tzinfo\"):\n\t\t\treturn u\n\t\treturn bytes(u)\n\telse:\n\t\treturn u\n\n\ndef timezone(name):\n\t\"\"\"\n\tConvert time zone string to numeric tzinfo object\n\t\n\tname= time zone to read from\n\t\"\"\"\n\t"}]}
{"task_id": "62b87d24d292efb640a55670", "prompt": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tif 'DCONPY_VERSION_INFO' in os.environ:\n\t\tverstr = os.environ['DCONPY_VERSION_INFO']\n\t\tversion = float(verstr[:2])\n\t\tfputils.put_maybe_multi(verstr, ['version', version, 'compiler_version', PY3, 'build_type'])\n\t\tos.environ['DCONPY_VERSION_INFO'] = version\n\t\treturn version\n\telse:\n\t\tcmd = ['/usr/bin/version', ]\n\t\tr = subprocess.Popen"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\ttry:\n\t\twith open(os.path.join(HERE, 'VERSION')) as f:\n\t\t\t# Read version information\n\t\t\tversion_file = f.read().splitlines()\n\t\tif VERSION_FILE not in version_file:\n\t\t\traise RuntimeError('Version information not found in {}'.format(os.path.join(HERE, 'VERSION')))\n\t\t# Replace escaped newlines with newlines\n\t\tversion_string = re.sub(r'(?<=\\\\\\\\|\\\\n|\\r\\n|\\r|\\f)|(?<=\\n\\\\\\\\|\\\\r\\n|\\f"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\timport string\n\t_version = string.split(\n\t\t_version_re.search(open(_abspath_os)).group(1),\n\t\t'')\n\t# if there is no _version, something is wrong.\n\tif len(_version) < 3:\n\t\traise ValueError(\"Unable to find version information\")\n\tversion = _version[1]\n\tif not _version_re.search(version):\n\t\traise ValueError(\"Unrecognized version: %s\" % version)\n\t# otherwise, return the proper version tuple\n\treturn _version\n\n\ndef"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t__v1_info = {}\n\tget_distribution_version.__doc__ = __doc__\n\n\ttry:\n\t\tfor _, v in get_distribution((\"https://pypi.python.org/pypi/package\",\n\t\t\t\t(\"package\",)), quiet=True):\n\t\t\t__v1_info[(get_distribution_version.__name__, v)] = v\n\texcept DistributionNotFound:\n\t\treturn __v1_info\n\n\treturn __v1_info\n\n\ndef parse_version(__version):\n\t\"\"\"\n\t\tParse a given version string"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\ttry:\n\t\t__import__('pkg_resources').get_distribution('pyparsing').__version__\n\t\treturn pkg_resources.get_distribution('pyparsing').__version__\n\texcept:\n\t\treturn _PY_VERSION_\ndel get_versions\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tif versions_file is not None:\n\t\tfile_exists = PathManager.exist(versions_file)\n\t\tif file_exists:\n\t\t\treturn versions.load_versions_from_file(versions_file)\n\t\telse:\n\t\t\traise Exception(\"Unable to get version information from %s\" % versions_file)\n\treturn default_versions\n\n\ndef get_release_filename(release_name):\n\tif release_name[-4:] == \".txt\":\n\t\treturn release_name[:-4]\n\telse:\n\t\treturn release_"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t# This needs to be defined in both web and Python.\n\treturn {\n\t\tconfig(\"CARGO_CFG_TARGET_VERSION\") : config(\"CARGO_CFG_DEFAULT_VERSION\"),\n\t}\n\ndef get_version():\n\treturn get_versions()[config(\"CARGO_CFG_TARGET_VERSION\")]\n\ndef is_python():\n\timport distutils.core\n\treturn distutils.core.is_python_package(distutils.core.pkg_resources.Requirement.parse(\"pybind11\"))\n\ndef is_julius():\n\t"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t_version_list = _read_build_ini()\n\tif _version_list:\n\t\t_version = _get_next_from_parsed_version(_version_list)\n\t\t_date = _version * 1000 + 2000\n\t\t_version_list = ''\n\t\tfor v in _version_list.split():\n\t\t\tif v.isdigit():\n\t\t\t\t_version = int(v)\n\t\t\t\tif v > _min_python_version:\n\t\t\t\t\t_version = _version_list + ['{}.{}.{}{}{}'.format(_min_python"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tversions = [str(x) for x in get_system_version().split('-')] if version is not None else []\n\t#print 'get_versions: ', versions\n\treturn versions\n\nversion = get_versions()\n\n\nif __name__ == '__main__':\n\timport sys\n\texec(compile(open(sys.argv[1]).read(), sys.argv[1], 'exec'))\n\nusing System;\nusing System.Collections.Generic;\nusing System.IO;\nusing System.Linq;\nusing System.Threading.Tasks;\nusing Microsoft.AspNetCore;"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t__revision__ = 'a85f07f2eb1a88bb8'\n\t__title__ = 'Aseman Client 3.8'\n\t__author__ = 'Aseman'\n\t__email__ = 'aseman@example.com'\n \n\tupdate (\t\t\n\t\tdepends_on=['version_control','libarchive_python'],\n\t\t# if we don't know the version, there won't be module data, it's a \n\t\t# fresh install of this module\n\t\tdepends=['version_control','libarchive'],\n\t\t# no need"}]}
{"task_id": "62b87d24d292efb640a5566f", "prompt": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tfor piece in pieces:\n\t\tif ',' in piece:\n\t\t\tpieces[ piece ] = int( piece )\n\treturn '{%s}'%('{ '.join(pieces) %}' )\n\ndef version_to_revision( sVersion ):\n\t\"\"\" Convert a version number into a revision number.\n\t\"\"\"\n\ttry:\n\t\treturn tuple(map(lambda x: int(x) if (x >= sVersion and x <= sVersion + 1) else 0, sVersion.split('.')[:2]))\n\texcept ValueError:\n\t\treturn 0\n\n\nclass"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tfor piece in pieces:\n\t\tif piece == 'oldstyle':\n\t\t\tstyle = False\n\t\t\tcontinue\n\t\tif piece == 'newstyle':\n\t\t\tstyle = True\n\t\t\tcontinue\n\t\tif not style:\n\t\t\tsetattr(pieces, piece, getattr(Style, piece))\n\n# coding: utf-8\n# Copyright (c) 2016, 2021, Oracle and/or its affiliates.  All rights reserved.\n# This software is dual-licensed to you under the Universal Permissive License (UPL) 1.0 as shown at https://oss.oracle"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tfor piece in pieces:\n\t\tif isinstance(piece, str):\n\t\t\tpieces[piece] = render_string(pieces[piece])\n\t\telif isinstance(piece, dict):\n\t\t\tpieces[piece] = render_dict(pieces[piece])\n\treturn ''.join(pieces).encode(style or 'utf-8')\n\ndef render_string(text, style=False):\n\t\"\"\"\n\tRender the given text/style piece string into the requested style.\n\t\"\"\"\n\tif isinstance(text, bytes):\n\t\treturn text.decode(style or"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"error\"]:\n\t\traise ValueError(\"invalid git-archive tag\")\n\n\treturn _compile_single(pieces, style)\n\n\ndef git_archive(pieces):\n\t\"\"\"Render the git archive into its pre-compiled version\"\"\"\n\treturn render(pieces, \"git-archive\")\n\n/*\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 ("}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tversion = pieces[\"version\"]\n\tif not style or style == \"default\":\n\t\tif not version:\n\t\t\traise ValueError(\"Invalid SemVer version string: %s\" % version)\n\t\tfrom pygments.styles import get_style\n\t\treturn get_style(version)\n\n\nclass ScmstyleSubmenu(MenuEntry):\n\t\"\"\"\n\tAdds support for Submenu style entries to submenus.\n\n\tParameters:\n\t-----------\n\tsubmenu : Menu\n\t\tMenu to add to.\n\taction : MenuAction\n\t\tMenu action to add"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tfor piece in pieces:\n\t\tif isinstance(piece, str):\n\t\t\tpieces[piece] = piece + style\n\t\telif isinstance(piece, Style):\n\t\t\trender(piece.pieces, piece.style)\n\n\nclass Version:\n\t\"\"\"\n\t\tA class that defines a particular release of Flask.\n\t\tCan be replaced with a tuple of two elements:\n\t\t\t(tag, versions_tuple)\n\n\t\ttag and versions_tuple give the tag of the release and the tuple\n\t\t\t(str, str) to use.\n\t\t\tIf"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tfor piece in pieces:\n\t\tstyle = getattr(style, 'copy', piece)(pieces)\n\treturn style\nversion_info = __version__\n\n# Output version info, then exit.\n__version__ = \\\n\t\t'8.10-dev'\n__author__ = 'Nir Khuryan, Todd Beard'\n__credits__ = \\\n\t\"\"\"\\\n\".join(['Raziel Twainwheatvijst', 'Todd Beard']), \\\nRhythmbox and Antoon Beechings, Inc., for rhy"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tfor piece in pieces:\n\t\tresult = _render(piece, style)\n\t\tif result!= '':\n\t\t\treturn result + os.linesep + piece + os.linesep\n\treturn ''\n\n\ndef _render(piece, style):\n\tpass\n\t#if piece and piece[-1] == os.path.sep and piece[:-1] in options.ignored_platforms:\n\t#\treturn ''\n\t#print('render %s %s' % (piece, style))\n\n\t#if options.use_git:\n\t#\ttry"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tfor piece in pieces:\n\t\tif isinstance(piece, str):\n\t\t\tpieces[piece] = render_text(piece, style)\n\t\telif isinstance(piece, Template):\n\t\t\tpieces[\"__%s__\" % piece.name] = render_template(pieces, piece, style)\n\t\telse:\n\t\t\tpieces[\"__%s__\" % piece] = piece\n\treturn \"\".join(pieces[\"__\"])\n\n\"\"\"\nGiven a series of versions, Render them into the requested style for inclusion\nin the API docs.\n\"\"\"\ndef render(parts"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tfor piece in pieces:\n\t\tif style == \"pep440\":\n\t\t\tif getattr(settings, 'PEP440_REFACTOR', 1) == 1:\n\t\t\t\tfor k,v in settings.pep440_repr.items():\n\t\t\t\t\tpieces[k].replace(settings.pep440_repr[k], v)\n\t\t\t\t\tpieces['PEP440_USERDEF'] = v\n\t\telif style == \"pep440_unuserdef\":\n\t\t\tpieces['PEP440_USERDEF'] = \"\"\n\treturn render_text(pieces,"}]}
{"task_id": "62b87d24d292efb640a5566d", "prompt": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tp = pieces[0]\n\tif p == '.':\n\t\treturn p\n\telif p == '..':\n\t\tfor piece in pieces[1:]:\n\t\t\tif piece == '..':\n\t\t\t\traise ValueError(pieces)\n\t\t\telif not piece:\n\t\t\t\traise ValueError(pieces)\n\t\telse:\n\t\t\traise ValueError(pieces)\n\t\treturn ''\n\tr = random.getrandbits(32)\n\treturn ''.join([\n\t\trandom.choice(string.ascii_letters + string.digits + '_')\n\t\tfor"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\treturn ''.join(pieces)\n\ndef python_version():\n\t# type: () -> Tuple[str, str]\n\t\"\"\"\n\tReturns a tuple of the python version.\n\t\"\"\"\n\tif sys.version_info.major == 2:\n\t\treturn (sys.version_info.major, sys.version_info.minor)\n\telse:\n\t\treturn (sys.version_info.major, sys.version_info.minor, sys.version_info.releaselevel)\n\ndef set_sys_argv(argv):\n\t# type: ("}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tfor piece in pieces:\n\t\tif piece[0] == \"+\" and piece not in pieces:\n\t\t\treturn piece\n\ndef renderPart(pieces, index, invert, callback):\n\tif index % 2: #even number\n\t\treturn \"<font color='green'>~</font><sub><b>^</b></sub>\"\n\telse: #odd  number\n\t\treturn \"<font color='red'>{<b>!</b>}</font>\".format(callback(invert(pieces-index/2)))\n\nmodule.exports = {\n  extends: ["}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tfor piece in pieces:\n\t\tif piece == '.':\n\t\t\treturn ''\n\treturn '.' + pieces[-1] if pieces else '.'\n\ndef plus_or_dot_as_dash(pieces):\n\t'''\n\t\tReturn a + if we don't already have one, else return a.\n\t\tIf there is a dash in pieces...\n\t'''\n\tif len(pieces) > 1:\n\t\treturn plus_or_dot(\".+(\" + pieces[0] + \")\")\n\n#  Don't return dots...\ndef last_dot(pieces):"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tfor piece in pieces:\n\t\tif '.'.join(pieces[piece]) not in pieces:\n\t\t\treturn piece\n\treturn pieces[-1]\n\ndef plus_or_dot_to(piece, string):\n\treturn plus_or_dot(list(filter(None, [piece, *pieces[piece], string])))\n\ndef dot_or_quote_to(piece, string):\n\treturn quote_plus_or_dot(piece) + string\n\ndef quote_plus_or_dot(piece):\n\tif piece == '..':\n\t\treturn '"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tresult = ''\n\tfor piece in pieces:\n\t\tif piece!= '':\n\t\t\tresult += piece + '.'\n\t\telse:\n\t\t\treturn pieces.pop()\n\n\traise ValueError('No %s on %s found' % (result, pieces))\n\n\nclass _Number(str, metaclass=abc.ABCMeta):\n\n\t\"\"\"\n\tDefines :class:`float` number formatting.\n\t\"\"\"\n\n\tdecimal_sep = '.'\n\tspecial_format = 'e'\n\n\t@abc.abstractproperty\n\tdef format(self):\n"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif \"+\" in pieces and pieces[0]!= \" \":\n\t\treturn pieces[0] + \".\"\n\treturn \".\"\n\ndef zero_fill(number, target_size):\n\t\"\"\"\n\tUseful for implementing zero_padding for variable_size fields whose actual\n\tvalue is the length of the variable.\n\t\"\"\"\n\twhile str(number).count(\".\") == (number.count(\".\") - 1):               # No point in adding trailing zeros\n\t\tif isinstance(number, str):\n\t\t\tnumber = str"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t# don't have a prime #'s to worry about\n\tif '+' not in pieces:\n\t\treturn '. '\n\treturn '+'+ pieces.replace('+', '.')\n\n\ndef indent(n):\n\t\"\"\"\n\t\t'With a space' indicates a non-empty sequence of spaces to\n\t        be followed by some amount of indentation.  The default\n\t        indent is four spaces.\n\t\"\"\"\n\tif n == 0:\n\t\treturn'' * 4\n\treturn '\\n' +'' * ("}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tresult = pieces[0]\n\tfor piece in pieces[1:]:\n\t\tif piece!= '.':\n\t\t\treturn pieces[0] + piece\n\treturn result\n\ndef flatten_dicts(dicts):\n\t\"\"\"\n\t\tGiven a list of dictionaries, turn each dictionary into a single\n\t\tflattened dictionary. Return it, or None if it can't be flattened.\n\t\"\"\"\n\tfor data in dicts:\n\t\tif not isinstance(data, dict):\n\t\t\treturn\n\t\tnew_data = flatten_dicts(data.items"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tfor piece in pieces:\n\t\tif piece not in ('.', '..'):\n\t\t\tcontinue\n\t\tif piece!= '.':\n\t\t\treturn piece\n\n\treturn ''\n\n\ndef render_together(pieces):\n\t''' \n\t    Given a list of options and a piece-to-go, return a \n\t    single-line string representing all the pieces combined \n\t    together (i.e.' '-options'-'  '-piece-to-go'-''). \n\t''' \n\toutput = '' \n\tfor piece in pieces: \t\t\n\t\t"}]}
{"task_id": "62b87d23d292efb640a5566b", "prompt": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t_verbose = verbose\n\t_hide_stderr = hide_stderr\n\t_env = os.environ.copy()\n\t_env['DJANGO_WORKSPACE'] = cwd or os.getcwd()\n\t_env['DJANGO_VERBOSE'] = _verbose\n\t_env['DJANGO_HIDE_STDERR'] = _hide_stderr\n\n\t_os = None\n\ttry:\n\t\t_os = _subprocess.Popen(commands, _env, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines="}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport optparse\n\tparser = optparse.OptionParser(\"option options\")\n\n\t# Parse arguments\n\targs, _ = parser.parse_args(argv=args)\n\topt_dict = vars(args)\n\n\tif opt_dict[\"version\"]:\n\t\tmy_prog = \"git \"\n\t\tif Version.version!= Version.GIT_HEAD:\n\t\t\tmy_prog += \"git-%s \" % Version.version\n\t\tmy_prog += \" --version\"\n\telif len(args) > 0:\n\t\tmy_prog = \""}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport subprocess\n\tstdout  = subprocess.PIPE\n\tstderr  = subprocess.PIPE\n\tretvals = 0\n\n\tfor com in commands:\n\t\tsys.stdout = stdout\n\t\tsys.stderr = stderr\n\t\t# TODO: use cmd.stdout or com.stdout\n\t\trc = com(cwd, args, verbose, hide_stderr, env)\n\n\t\tif rc!= 0:\n\t\t\tretvals += 1\n\treturn retvals\n\npackage com.yugabyte.app.yugastore.service.impl;\n\nimport java.util"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tlog.info(log_wrap('command', args))\n\tif cwd:\n\t\tcwd = expand_path(cwd)\n\t\tif not os.path.exists(cwd):\n\t\t\traise FileNotFoundError(cwd)\n\n\t# Process the commands.\n\tfor cmd_name, cmd_args in command_list.items():\n\t\t# Do the command.\n\t\tresult = None\n\t\ttry:\n\t\t\tresult = _runCommand(commands, cwd, cmd_name, cmd_args, verbose, cmd_args, hide_stderr, env=env"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t__tracebackhide__ = True\n\t_process_function_commands = None\n\n\tdef __traceback():\n\t\tif __tracebackhide__:\n\t\t\treturn\n\t\t__tracebackhide__ = False\n\n\t\tif cwd is None:\n\t\t\tcwd = os.getcwd()\n\t\tif verbose is None:\n\t\t\tverbose = len(sys.argv) > 1\n\t\tif cwd:\n\t\t\tcwd = os.path.abspath(os.path.expanduser(cwd))\n\t\tif not(chdir_exists(cwd):\n\t\t\t"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport subprocess\n\tfrom collections import defaultdict\n\tif cwd is None:\n\t\tcwd = os.getcwd()\n\n\tif isinstance(args, str):\n\t\targs = [args]\n\telif not args:\n\t\targs = None\n    \n\tcmd_map = {}\n\targs_map = {}\n\n\tfor name in args:\n\t\ttry:\n\t\t\tif name in cmd_map:\n\t\t\t\targs_map[name].append(cmd_map[name])\n\t\texcept:\n\t\t\tcmd_map[name] ="}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif env is None:\n\t\tenv = {}\n\tenviron = dict(os.environ)\n\tcommand = commands.get(args)\n\tif command and len(args):\n\t\tenviron.update(command)\n\t\tif cwd is None:\n\t\t\tcwd = command.cwd if command.cwd else environ['CWD']\n\tif 'DJANGO_SETTINGS_MODULE' in environ: # We're using \"settings\" in Django\n\t\tmodule_name = environ['DJANGO_SETTINGS_MODULE'].split('.')[0]\n\telif not args:"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t# always check-in /dev/null before checking in the root dir\n\tenv = env or {}\n\tenv[\"PYTHONUSERBASE\"] = os.path.join(os.getcwd(), \"lib\")\n\tenv[\"PYTHONPATH\"] = os.path.join(base, \"bin\")\n\n\tdef _exec_command(cmd, cwd, verbose):\n\t\texec_func(cmd, cwd, env)\n\n\t\t# always hide stderr\n\t\tif hide_stderr:\n\t\t\tsys.stderr.write('')\n\n\t\tif verbose:\n\t\t\tprint('"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif env is None:\n\t\tenv = {}\n\tif cwd is not None:\n\t\tenv['PYTHONPATH'] = cwd\n\n\tcmd = \"%s %s\" % (commands, quote(args))\n\n\tresult = None\n\tfor _ in range(5):\n\t\ttry:\n\t\t\tresult = __subprocess__.call(cmd, universal_newlines=True,\n\t\t\t                            stdout=subprocess.PIPE,\n\t\t\t      "}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\ttry:\n\t\timport getopt\n\texcept ImportError:\n\t\t__version__ = None\n\n\t\tdef parse_args():\n\t\t\tusage_str = \"usage: %prog [options] arg1 arg2... argn\"\n\t\t\tusage_str = usage_str % dict(\n\t\t\t\tversion=__version__,\n\t\t\t\tcommands=commands,\n\t\t\t\targs=args\n\t\t\t)\n\t\t\treturn parse_common_command_line(usage_str)\n\t\treturn __version__, parse_args()\n\n\n\f\n@pytest_mark.parametrize"}]}
{"task_id": "62b87d23d292efb640a55668", "prompt": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tc = VersioneerConfig()\n\tc.git_dir = 'parquet-sample'\n\tc.version = '0.0.1'\n\tc.version_dev = '0.0.1'\n\tc.commit = 'bf1ba7bc68e8ab0d4d3d8b0de6a0cb0d1d0bc2a9d3df8ef0b8da9f0cfbb38b34d'\n\tc.branch = 'test-branch'\n\tc.verbose = False\n\tc."}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tversionfile_abs = os.path.join(PROJECT_DIR, \"VersioneerConfig.py\")\n\tversionfile = open(versionfile_abs, \"w\")\n\tversionfile.write(\"\"\"\n\n{meta}\nversion = VERSION\nresolution = \"python\"\nbase = sys.prefix\n\nCLASP = CUSTOM\nCMDLINE = sys.argv[1:]\n\nENABLE_COLOR = True\nENABLE_COVERAGE = True\nENABLE_COVERAGE_COLOR = True\n\nif BUILD_TYPE == \"Debug\":\n\tBUILD_PYTHON = os.path.join(BUILD_ROOT"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tshort_version = VersioneerConfig().get_config_short()\n\tfull_version = VersioneerConfig().get_config_full()\n\n\t# version number should be numeric\n\tif not versioneer_config:\n\t\traise VersioneerConfigError('--version is required to determine your Git '\n\t\t\t'directory structure. Please run \"pip install -U.\" or \"setup.py sdist\" '\n\t\t\t'before upgrading.')\n\n\tif full_version and not isinstance(full_version, six.string_types):\n\t\traise VersioneerConfigError"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tclass VersioneerConfig:\n\t\tVENV_DIST_URL = \"https://registry.npmjs.org/venv/-/venv-dist-1.0.tgz\"\n\t\tVFILE_VERSION = \"3.2.3\"\n\t\tUA = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.96 Safari/537.36 Vivaldi/1.5.5.3268.2\"\n\t\tAPP_"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tclass VersioneerConfig:\n\t\t__slots__ = (\"use_ebuild\",)\n\t\tdef __init__(self):\n\t\t\tself.use_ebuild = False\n\t\t\t\n\t\tdef set_use_ebuild(self, use_ebuild):\n\t\t\tself.use_ebuild = use_ebuild\n\t\t\t\n\t\tuse_ebuild = property(get_config, set_use_ebuild)\n\t\t\t\n\t\tdef use_user_site_packages(self):\n\t\t\tshutil.rmtree('.user_site')\n\t\t\tshutil."}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tversion_file = None\n\tif hasattr(sys, 'frozen'):\n\t\t# sys.executable is handled specially by setup.py, see #1890\n\t\t_file = getattr(sys.executable, '__file__', None)\n\t\tif _file is not None:\n\t\t\t_version_file = _file + '.version'\n\t\telse:\n\t\t\t_version_file = versioneer_snapshot.version.replace('.', '_')\n\n\t\twith open(_version_file, 'r') as inf:\n\t\t\tfor line in inf:\n\t\t\t\t"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tVCS = VersioneerConfig()\n\tVCS.VCS = Scm(versionfile_source, versionfile_build)\n\tVCS.timestamp = timestamp\n\tVCS.versionfile_source = versionfile_source\n\tVCS.versionfile_build = versionfile_build\n\t# XXX should these be global?\n\tVCS.cmdclass = VcsCmdclass\n\tVCS.uploadstrategy = uploadstrategy\n\tVCS.userspecific = userspecific\n\tVCS.timeout = timeout\n\tVCS.proxy = proxy\n\tVCS."}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tversion_file = os.path.join(PROJECT_INFO,'version.py')\n\tVCS = {}\n\tgit_version_num, git_hash = get_git_hash(version_file)\n\tif VCS[git_version_num] == 'No version provided' or not VCS[git_version_num]:\n\t\traise Exception('[ERROR] git_version_num not found in project version.py: (%d) %s' % (git_version_num, version_file))\n\n\twith open(version_file) as f:"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tversion_file = os.path.join(PROJECT_ROOT, \"packaging\", \"versioneer.py\")\n\t_version_py = open(version_file, \"r\", encoding='utf-8').read()\n\tversion_spec = _version_py.strip()\n\n\tdef parse_version_spec(spec):\n\t\t\"\"\"\n\t\tConvert a version specifier string into a VersionSpec object.\n\n\t\tWe support parsing the following strings here:\n\t\thttp://releases.k8s.io/release-0.8/docs/user/config."}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tif sys.version_info > (3,):\n\t\treturn VersioneerConfig()\n\telse:\n\t\treturn None\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to you under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n * http://www."}]}
{"task_id": "62b87d23d292efb640a55667", "prompt": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t@wraps(method)\n\tdef _decorator(self, func):\n\t\tself.vcs.funcmap[vcs] = method\n\t\treturn method\n\treturn _decorator\n\ndef register_vcs_methods(module, vcs_dict={}, vcs=None):\n\t\"\"\" Add methods used by all registered VCS's to the module\n\t    (so subclasses of it can register methods as well)\n\t\"\"\"\n\tmodule.register_vcs(\n\t\tregister_vcs_handler,\n\t\t'handle_%s' % _vcs_alias(vcs_dict"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t@functools.wraps(method)\n\tdef decorator(cls, *args, **kwargs):\n\t\ttry:\n\t\t\tvcs = cls.__bases__[0]\n\t\t\treturn getattr(vcs, method)(*args, **kwargs)\n\t\texcept AttributeError:\n\t\t\treturn None\n\treturn decorator\n\n#\n# Set up some common customizations for the test\n#\n\ndef install_common_customizations() -> None:\n\t\"\"\"Set up the setuptools specific customizations.\"\"\"\n\n\nclass TestPackage(TestPackageBase):\n\t\"\"\"\n\tHelper class"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorator(hook, self=self):\n\t\t\"\"\"\n\t\tFunction decorator that sets a handler bound to the first\n\t\tparameter in the sequence of callables if all the parameters\n\t\tare bound to the handler (the first one in the sequence is\n\t\ttaken by default).\n\t\t\"\"\"\n\t\tif self._vcs:\n\t\t\tself._vcs.hooks.remove(hook)\n\t\t\tself._vcs.hooks.append(hook)\n\t\treturn hook\n\treturn decorator\n\n# This file is part of Hypothesis, which may be found at"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorator(f):\n\t\t@wraps(f)\n\t\tdef wrapper(*a, **kw):\n\t\t\ttry:\n\t\t\t\tresult = method(vcs, *a, **kw)\n\t\t\texcept _versioncheck.exc_info as error:\n\t\t\t\traise error\n\t\t\tif result: return result\n\t\t\traise\n\t\treturn wrapper\n\treturn decorator\n\ndef init(\n\t\tpackages=None,\n\t\toptions=None,\n\t\tcache_version=0,\n\t\tcache_subdir='/',\n\t\tpackage_data="}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef wrapper(self):\n\t\t\"\"\"\n\t\tdecorator to set an attribute as handler of the method of a VCS\n\t\t\"\"\"\n\t\tif not callable(lambda _method=self.method, _args=1, _kwargs=None, vcs=self.vcs, _method_name=method, __wrapped__=self):\n\t\t\traise AttributeError('Method %s does not have a handler' % method)\n\t\treturn self.wrapper\n\treturn decorator\n\n\nclass BaseCommandModule(VcsMixin, BaseCommand):\n\t\"\"\"Base command class that provides all command"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t@wraps(vcs)\n\t@util.memoized_function\n\tdef handler(self, *args, **kwargs):\n\t\tif self.on_vcs_created is not None:\n\t\t\tself.on_vcs_created(self)\n\t\t\tself._vcs_created = True\n\n\t\treturn method(self, *args, **kwargs)\n\n\tdef __init__(self, vcs=None, **kwargs):\n\t\t\"\"\"\n\t\tInitiate a handler. When the handler is constructed, it'll listen to\n\t\tvcs.vcs.versions."}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorating_handler(self, ignored_api_responses, handler, method,\n\t\t\t\t\t\t  *_args, **_kwargs):\n\t\tif isinstance(handler, collections.abc.Callable):\n\t\t\tself.request_method_map[method+'_handler'] = handler\n\t\t\treturn handler\n\t\telse:\n\t\t\tdef update_handler_class(handler):\n\t\t\t\t# In general should not update class name when using @handle_requests.\n\t\t\t\t# Instead we just check the current class name, it is updated if needed.\n\t\t\t\tclass_name = handler.__"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t# Get function\n\t@_register_handler\n\t@wraptify\n\tdef handler(*args, **kwargs):        # function\n\t\t\"\"\"\n\t\tFunction: handler\n\t\t\t@return: the decorator\n\t\t\"\"\"\n\n\t\treturn method(*args, **kwargs)   # wrapper\n\n\treturn handler\n\ndef _register_handler(name, klass):\n\t\"\"\"\n\t\tDecorator for classes that extend a function handler.\n\n\t\tFor VCS implementations, the decorator will register a VCS object\n\t\tand method to"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\tif vcs == 'hg':\n\t\t\treturn handler_generic\n\t\treturn handler_hgx\n\tmethod = decorate_method(method)\n\treturn wrapper\n\nclass VCSHandler(VCSHandlerBase):\n\tdef __init__(self, vcs_dir):\n\t\tVCSHandlerBase.__init__(self, vcs_dir)\n\t\tif vcs_dir.startswith('hg.'):\n\t\t\t# This happens if git is not installed.\n\t\t\tself.handler = 'hg'\n\t"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorator(f):\n\t\t@functools.wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif getattr(vcs, method, None):\n\t\t\t\t# skip if already exists\n\t\t\t\treturn f(*args, **kwargs)\n\t\t\telse:\n\t\t\t\t# register version\n\t\t\t\tvcs._version.append(method)\n\t\t\t\twrapper = _wrap_method(f, _class=MethodWrapper, method=method)\n\t\t\t\tsetattr(vcs, method, wrapper)\n\t\t\treturn wrapper(*args, **kwargs)\n\treturn"}]}
{"task_id": "62b45e945108cfac7f210a4a", "prompt": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects   = 0\n\tgood_objects  = 0\n\t# Validate all objects in the storage pool\n\tfor coid in self.storage_list:\n\t\tif coid in self.storage_pool:\n\t\t\tself.storage_pool[coid].verify(validate_objects=validate_objects)\n\t\n\t\telse:\n\t\t\tself.storage_pool[coid] = StorageObject(self.storage_pool_filepath, self.storage_pool_name, coid=coid, validate_objects=validate_objects)\n\t\t\n\t\t"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tif self.storage_root is None:\n\t\traise ValueError(\"Storage root is None!\")\n\tif validate_objects:\n\t\tfor obj_name in self.storage_root.object_names:\n\t\t\tif not self.IsValidObject(obj_name):\n\t\t\t\tself.storage_root.object_add(obj_name)\n\t\tself.validate_hierarchy(None)\n\tif check_digests:\n\t\tself.ValidateAllDistributions()\n\tnum_objects = len(self.storage_root.objects)\n\tgood_objects = 0"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tif validate_objects:\n\t\tlogger = logging.getLogger(self.__class__.__name__)\n\t\tlogger.setLevel(logging.DEBUG)\n\t\tself.validate_objects()\n\n\t\tif self.get_root_storage() is not None:\n\t\t\tlogger.debug(\"Running validate storage for storage root hierarchy...\")\n\t\t\tnum_objects, num_digests = self.get_storage_root_hierarchy()\n\n\t\t\tif not num_objects:\n\t\t\t\t# No objects found, so no need to check them again\n\t\t\t\tif show_warnings"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tself.log(\"validate_hierarchy called.\")\n\tif validate_objects is False:\n\t\treturn 0\n\tif len(self.objects) == 0:\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n\telse:\n\t\tnum_objects = len(self.objects)\n\t\tgood_objects = len([obj for obj in self.objects])\n\t\n\tif show_warnings:\t\n\t\tself.log_message(\n\t\t\t\"\\n\\t\".join([\n\t\t\t\tf\"num_objects: {num_objects.percent_"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tif not self._valid_digests:\n\t\treturn 0\n\t\n\ttry:\n\t\troot = validate_objects or self._storage_roots.get(self._digest)\n\texcept ValueError:\n\t\treturn 0\n\t\n\t# print('root =',root)\n\t\n\tgood_objects = 0\n\tnum_objects = 0\n\tvalid_objects = []\n\t\n\t# For each object in the blob store storage, we validate it.\n\tfor file_or_dir in iter_files(root):\n\t\ttry:\n\t\t\tif file_or_"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tif self.storage is None:\n\t\treturn 0\n\t\n\thierarchy = []\n\tgood_objects = 0\n\tnum_objects = 0\n\tfor object in self.objects:\n\t\tif object.data is None:\n\t\t\tcontinue\n\t\tgood_objects += 1\n\t\tif (object.data.object_type == '0') and (not validate_objects or not check_digests):\n\t\t\tcontinue # TODO: Check for no-checksums\n\t\ttry:\n\t\t\tsha_str = self.object_sha(object)\n\t\t"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tif validate_objects:\n\t\tfor obj in self:\n\t\t\tobj.validate()\n\tif self.depth == 0:\n\t\treturn 0\n\t\n\tfound_digests = 0\n\ttotal_files = 0\n\tcheck_files = []\n\tfor obj in self.objlist:\n\t\tif obj.is_valid():\n\t\t\tfound_digests += 1\n\t\t\ttotal_files += obj.total\n\t\t\tobjectdigest = ''\n\t\t\tobjects = obj.get_tree_hash()\n\t\t\tfor o in objects:\n\t\t\t\tif"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects, depth, good_objects = self.hierarchy_count, self.number_of_objects, self.good_objects\n\tif validate_objects==True and not(show_warnings):\n\t\tprint \"--- Validation of storage root --------------------------------------\"\n\t\tfor o in self.objects:\n\t\t\to.validate_hierarchy(validate_objects=False)\n\n\tif validate_objects==True:\n\t\tif not(show_warnings):\n\t\t\tprint \"--- Checking storage root hierarchy ---------------------------------------\"\n\t\tself.objects = []\n\t\tself."}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tt_list = []\n\tbad_objects = []\n\tnum_objects = 0\n\tgood_objects = 0\n\t\n\t# Gather valid objects and verify\n\tfor object in self.__class__._objects:\n\t\tobject._validate()\n\t\tobject.validate_object()\n\t\tif object.valid_digest:\n\t\t\tobj_good = object.get_good_digest()\n\t\t\tobj_bad = object.get_bad_digest()\n\t\t\t\t\n\t\t\tif (obj_good!= obj_bad):\n\t\t\t\tbad_objects.append(1)"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from math import *\n\tpass#IMPORTIMPORTIMPORT from random import Random\n\tpass#IMPORTIMPORTIMPORT from time import sleep\n\tpass#IMPORTIMPORTIMPORT import os\n\tpass#IMPORTIMPORTIMPORT from urllib.request import Request\n\tpass#IMPORTIMPORTIMPORT from zipfile import ZipFile\n\tpass#IMPORTIMPORTIMPORT from xml.dom import minidom\n\t#from. import base\n\t#from. import maker\n\t\t\n\n\tlog = None\n\t#store a"}]}
{"task_id": "62b45e515108cfac7f210a3c", "prompt": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tpass\n\t\n\nclass OcflStorageRoot(Root):\n\t\"\"\"\n\t\tA root class used for managing OCFL storage.\n\t\t\n\t\t>>> from uer.ocfl import OcflStorageRoot\n\t\t>>> OcflStorageRoot.root\n\t\t<ocf lstore class=\"OCFLStorage\">\n\t\t\t>>> def dump(obj, **params):\n\t\t\t\t...\n\t\t\t>>> print\n\t\t\t\t>>> def load(obj):\n\t\t\t\t\t...\n\t\t\t>>> obj = Root.load(obj)\n\t\t\t>>> obj = obj.inspect()"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tpass\n\n\nclass StorageRoot(object):\n\t\"\"\"\n\tBase class for storage objects.\n\t\"\"\"\n\n\tdef __init__(self):\n\t\tself.store = None\n\t\tself.logger = None\n\t\tself.loglevel = None\n\t\tself.filehandle = None\n\t\tself.filehandlestring = None\n\t\tself.cachefilepath = None\n\t\tself.cachefiletime = None\n\t\tself.cachefilesize = None\n\n\tdef __str__(self):\n\t\treturn \"\"\n\n\tdef start(self):\n"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tpass\n\ndef create(self, name, dpath=None):\n\t\"\"\"\n\tCreate a new OCFL storage root using the given path and\n\tthe directory to create the files in.\n\t\"\"\"\n\tif dpath is not None:\n\t\tself.setpath(dpath)\n\tself._root = self.create(name)\n\treturn self._root\n\n\"\"\"\n\tGet a reference to the storage root at the specified path.\n\t\"\"\"\n\tif not dpath:\n\t\tdpath = self.getpath()\n\t#print \"O"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tpass\n\n\nclass OCFLStorage_base:\n\t\"\"\"\n\tStorage element for base objects.\n\tNote:\n\t\tAlso used in the file loading system.\n\t\"\"\"\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tConstructor for OCFLStorage_base.\n\t\t:param parent: parent folder element\n\t\t:param name: name for this file directory\n\t\t:param size: size of the file directory\n\t\t:param size_raw: raw size of the file directory\n\t\t:param directory: parent of the file directory\n\t"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\treturn OCFLStorageRoot(ROOT_NAME, ROOT_DIR)\n\n\nclass OCFLStorageElement(StorageElement):\n\tdef __init__(self, *args, **kwargs):\n\t\tStorageElement.__init__(self, *args, **kwargs)\n\t\tself.path = kwargs.get('path', None)\n\t\t\n\t\tself.type = kwargs.get('type', self.type)\n\n\t\tself.mode = kwargs.get('mode', None)\n\n\t\tself.size = kwargs.get('size', self.size)\n\t\t"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.Root = None\n\ndef getRoot(root):\n\t#\n\t#\t\tGet the default Root node.\n\t#\n\n\tif root is None:\n\t\t return None\n\t\n\t#\n\t#\t\tWe have some Root nodes.\n\t#\n\t#\n\n\tif not root.hasChildNodes():\n\t\t return None\n\tif root.name().nodeValue!= 'root':\n\t\t return None\n\tif not (root.firstChild(False).hasChildNodes() and root.firstChild(False).name().nodeValue == 'object'):\n\t\t return None\n"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.root = self.path()\n\tself.root.mkdir()\n\tself.path.setmode(0777)\n\tself.path.chmod(0777)\n\tself.init_file = open('default.json', 'wt')\n\tself.init_file.write(json.dumps(dict(), separators=(',', ':'), indent=2))\n\tself.init_file.close()\n\n\n#------------------------------------------------------------------------------\ndef write_default():\n\t\"\"\"\n\t\tWrite the OCFL API default config and return\n\t\"\"\"\n\t# create default"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.storage = root.Storage(self.location)\n\n\tself.storage.create_empty_file('key')\n\n@classmethod\ndef _format_and_to_string(d):\n\t\"\"\"\n\t@summary Convert a dictionary to a string usable in a JSON file.\n\t@param d The dictionary to transform.\n\t@returns string\n\t\"\"\"\n\treturn ','.join(str(pair) for pair in d.items())\n\ndef create(dict, location=None):\n\t\"\"\"\n\t\tCreate an object from a JSON dictionary.\n\n\t\t"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.root_dir = root_dir = config_path(True)\n\t\n\tif (root_dir is not None):\n\t\tos.makedirs(root_dir, exist_ok=True)\n\t\n\tfor x in (\"config_file_name\", \"config_file_content\"):\n\t\tself.add_file(io_path(path=\".\" if x == \"config_file_name\" else x), path=config_path(True) + x, is_root=True)\n\t\tself.files[\"./\" + x] = path ="}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tif not self.__initialized:\n\t\tself.__storage = cpy.Folder(self.__storage_dir)\n\t\t# Make sure the storage directory exists, otherwise it will fail to\n\t\t# get a parent directory.\n\t\ttry:\n\t\t\tos.mkdir(self.__storage_dir)\n\t\texcept OSError:\n\t\t\tpass\n\t\ttry:\n\t\t\tos.mkdir(os.path.join(self.__storage_dir, 'info'))\n\t\texcept OSError:\n\t\t\tpass\n\t\troot = self.__storage.create('root"}]}
{"task_id": "62b45e2eb89c9fd354170232", "prompt": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t# Do a pattern scan in case we've just seen a single digit: \">=0.\"\n\tre_ver = re.compile(\n\t\tr\"\"\"^(([\\d]+\\.[\\d]+|[\\d]+.[\\d]+)+)\\s*$\"\"\", re.I)\n\tfor match in re_ver.finditer(version):\n\t\tyield version[:match.start()]\n\t\tif match.group(0):\n\t\t\t# Skip version numbers: \"0.2\", or just \"0\".\n\t\t\tcontinue\n\t\tyield version[match.end():]"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t_version, _ = _version_parser(version.lower())\n\tfor number, segment in enumerate(_VERSION_SEGMENTS.split(version)):\n\t\tif not segment:\n\t\t\tbreak\n\t\tversion = \".\".join([_version, number])\n\treturn _version.strip(\".\") if number > 1 else version\nversion_parser = version_parser or version_parser = lambda s: s.lower()\n_base = \"pyrsistent\" if \"pyrsistent\" in __import__(\"rope.base\").__dict__ else \"pyrsistent\"\n\n\n#"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tif not version:\n\t\treturn 0\n\n\tparts = version.split('.')\n\tnum, rest = [int(parts[0]), version]\n\tfor rep in reversed(parts[1:]):\n\t\tif rep: num *= rep\n\treturn (num << 16) + (rest if rest else 0)\n\n\ndef is_release_candidate_version(version):\n\t\"\"\"\n\t\tReturn True if the specified version is a valid candidate to be a\n\t\trelease candidate.\n\t\"\"\"\n\treturn ((version >= 70100 and version!= 70100) or"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tv = version and version.strip() or version\n\tv = v.replace('.', '')\n\tv = v.replace('-', '').strip()\n\tv_parts = v.split('.') # ['abc...def', 'abc...defx']\n\tn = len(v_parts)\n\tcount = 0\n\twhile (True):\n\t\tcount += 1\n\t\tprev_value = 0\n\t\tif count == n:\n\t\t\tbreak\n\t\tfor k, v in dict.__dict__.items():\n\t\t\tif not v is None:\n\t\t\t\tif"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tnext_prefix = next_non_zero_prefix = None\n\tversion_len = len(version)\n\t\n\tfor i in range(version_len):\n\t\ttry:\n\t\t\tval4 = next(re_number)\n\t\t\tval3 = next(re_digit)\n\t\t\t\n\t\t\tnext_non_zero_prefix = (\n\t\t\t\tval3 // 10 if version[i+2] == '0' else\n\t\t\t\t('0' if val3 < 10 and val3 > 1 else val3) + val4\n\t\t\t)\n\t\texcept Index"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t_version = str(version)\n\tif _version[1] == \"0\":\n\t\treturn _version\n\telse:\n\t\treturn \"\" + _version\n\n\ndef increment_ref_count(ref):\n\tassert isinstance(ref, string_types)\n\tif ref.startswith(\"\\\"\"):\n\t\treturn \"'\" + ref\n\tif \".\" in ref:\n\t\treturn \"{}#{}\".format(ref[1:-1], ref[-1])\n\telse:\n\t\treturn \"#{}\".format(ref)\n\n\ndef parse_ref(ref):\n"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tversion = version.split('_')\n\tnumber = len(version)\n\ti = version.index('@')\n\tif i == -1:\n\t\treturn ''\n\tversion = version[0:i]\n\ti = version.index('a')\n\tif i == -1:\n\t\ti = version.index('b')\n\t# number, version, and branch\n\tversion = version[0:i]\n\tif number!= 0:\n\t\treturn version + string.from_bytes('0', 1)[number-1:]+'@'+ string."}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tif version.startswith('v'):\n\t\treturn version[1:]\n\telse:\n\t\treturn int(version)\n\ndef get_versions(start=None, end=None, **kwargs):\n\t\"\"\"\n\tHelper to get version numbers for a given string.\n\t\n\tVersion strings can contain any combination of ``v``- and ``<v>``.\n\t\n\tOptional version strings can contain one or more numbers, e.g. ``v2.8``.\n\t\"\"\"\n\tif end is None:\n\t\tend = get_final_version()\n\twhile True"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tcurrent_version = int(version)\n\twhile (current_version + 1)!= 0:\n\t\tcurrent_version = version + (1)\n\tresult = current_version\n\treturn result\n\ndef is_version_greater_than(version1, version2):\n\t\"\"\"\n\t\tTest to see if two version numbers are less than or equal for\n\t\tboth versions.  \n\t\t\n\t\tIf both values are a single value, True.\n\t\tIf both values are an array of same size, False.\n\t\"\"\"\n\tcheck = (\n\t\tnext"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\ttry:\n\t\tresult = next(version.split('+', 1))\n\texcept ValueError:\n\t\traise ValueError('Invalid version number %s:'% version)\n\tif '.' in result:\n\t\t# Versions with zero or more dots - only an integer\n\t\treturn int(result) or 0\n\treturn result\n\n# -- end version functions\n\n# -- version constants\nversion = 1\n# The current version, including increment and postfix\nCURRENT_VERSION = '0'\n# The last released version,\nLAST_BUILT_IN = CURRENT_VERSION\n"}]}
{"task_id": "62b45e23e0d4551b0392c90a", "prompt": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tfor version_dir in version_dirs:\n\t\tassert isinstance(version_dir, (str, unicode))\n\t\tassert len(version_dir) > 0, version_dir\n\t\tversion_dirs += (version_dir,)\n\n\tversion_root = version_dirs[0]\n\tversion_nums = []\n\tnum_versions_valid = 0\n\n\t# Validate content of root inventory\n\tfor root_version in version_dirs:\n\t\troot_root\t= self._get_root_inventory_dir( version_root )\n\t\troot_"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tv_dirs = ['__version_dirs__']\n\twith warnings.catch_warnings():\n\t\twarnings.simplefilter('ignore')\n\t\tversion_dir = Path(self.root_version_dir)\n\t\tfor (version_subdir, version_name) in zip(version_dirs, version_names):\n\t\t\tversion_file = version_dir.joinpath( version_subdir + '/' + version_name + '.whl')\n\t\t\tif not version_file.is_file():\n\t\t\t\traise ValueError('version {}/{} not in root inventory file {}"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tfor _dir in version_dirs:\n\t\tfor (dirpath, filepath) in self.get_version_seq(version_dirs):\n\t\t\tif not os.path.exists(dirpath): continue\n\t\t\t\n\t\t\t# read the file content\n\t\t\twith open(filepath, 'rb') as data:\n\t\t\t\tdata = data.read()\n\t\t\t\n\t\t\t# get the root inventory up to the data version\n\t\t\troot_dir = self.v_root_inventory()\n\t\t\t\n\t\t\t# validate this file version\n\t\t\tself.v_sanity()\n\t\t\t\n\t\t"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tversions = []\n\tfor version_name in version_dirs:\n\t\troot_inventory = {}\n\t\tif version_name is None:\n\t\t\traise Exception('Expected a name for the root inventory.')\n\t\tif version_name[0] == 'c':\n\t\t\troot_inventory['root'] = self._path_for_c_version(version_name)\n\t\telse:\n\t\t\troot_inventory['root'] = version_name + \"/\" + version_name\n\t\t\t\n\t\tversion_dir = self._path_for_c_version(version_"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t# Check to see if the inventory version history should be up to that point.\n\t# TODO this logic may move into a lot of places to make the version history\n\t# the same as that in the root inventory.\n\tif version_history_size(ver, version_dirs) > int(self.version_range):\n\t\t# If all of the versions in the version dir have the same version history\n\t\t# then verify that they are the same number (and only same version)\n\t\tif int(self.version_range) <= int(version_history_size(ver"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tfor version in self.version_dirs():\n\t\tif os.path.getsize(os.path.join(\n\t\t\tversion_dirs, version.version()._path)) <= 0:\n\t\t\t\tlog.warn(\"[warn] %s (%s)\" % (\n\t\t\t\t'missing', repr(version.version()))\n\t\t) and version:\n\t\t\tversion.load_content()\n\t\n\t\t# If we've got a local inventory, we need to load the content digests\n\t\t# from the inventory and update their data structures.\n\t\tif os.path"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tfor version_dir in version_dirs:\n\t\ttry:\n\t\t\tpath = self.get_resource(version_dir, os.listdir, True)\n\t\texcept:\n\t\t\t# TODO what would be an error?\n\t\t\tlog.error(\"Unknown version %s\" % version_dir)\n\t\t\treturn 0, 0\n\t\tif not os.path.exists(os.path.join(self.inventory_root, version_dir)):\n\t\t\tlog.error(\"Missing %s\"%version_dir)\n\t\t\treturn 0, 0\n\t\t\t\n\t"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tif len(version_dirs) == 0:\n\t\traise ValueError(\"the version sequence length must be at least 1\")\n\n\t# Get a list of all of the content version directories that match the path\n\t# prefix in the root inventory.\n\tmatching_version_dirs = version_dirs[:]\n\tmatching_version_dirs.extend(self._get_root_inventory_version_dirs())\n\n\t# Iterate over all of the versions, make sure that each is valid.\n\tversion_count = len(matching_version_dirs)\n\twhile (version_count > 0"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tfor version_dir in version_dirs:\n\t\tfor version_base in version_dir.split(\"/\")[:-1]:\n\t\t\t# We're building the path in two pieces:\n\t\t\tversion_path_parts = version_dir.split(\"/\")[:-1] + version_base.split(\".\")\n\t\t\t\n\t\t\tfor version_version in version_path_parts[:-1]:\n\t\t\t\ttry:\n\t\t\t\t\tself.validate_version_inventories(self.make_inventory_path(version_version))\n\t\t\t\texcept Exception as e:\n\t\t\t\t\tlog_message(\"Error"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tfor dir in version_dirs:\n\t\tversion_file = os.path.join(self.root_version_dir, dir, \"inventories.txt\")\n\t\ttry:\n\t\t\twith open(version_file, \"r\", encoding=\"utf-8\") as vlist:\n\t\t\t\tlist_of_hashes = vlist.readlines()\n\t\t\tif len(list_of_hashes) == 1:\n\t\t\t\tself.validate_version_inventory(list_of_hashes[0], dir, 0) # just a simple test\n\t\t\telse:"}]}
{"task_id": "62b45e21e0d4551b0392c8ed", "prompt": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\troot = self[path]\n\t\n\tif root.path[0:len(self.root) - len(\"0=*\")] == \"0=*\":\n\t\treturn \"root\"\n\t\n\tif self.objects.has_key(root.path):\n\t\treturn \"object\"\n\t\n\tif osp.isdir(root.path):\n\t\treturn \"file\"\n\npackage org.jikesrvm.compilers.opt.ir;\n\nimport org.jikesrvm.compilers.opt.OptimizingCompilerException;\nimport org.j"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\timport OCFL\n\tif not os.path.isabs(path):\n\t\treturn path+\" is not an OCFL path \"\n\telse:\n\t\timport OCL\n\t\tif OCL.isCLD: #(has OCL_DIR):\n\t\t\tdir_path = OCL.get_dirpath(path, False)\n\t\t\tif dir_path.get_kind() == OCLTK.DirKind.DIRECTORY:\n\t\t\t\treturn \"root\"\n\t\t\telif dir_path.get_kind() == OCLTK.DirKind."}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\ttry:\n\t\t# Check on current directory\n\t\tif ocfl.__main__[0:1]==0:\n\t\t\treturn 'root'\n\t\tstoragePath = _get_path(path)\n\t\tresult = ocfl.__main__[0:-1]\n\t\tif storagePath.startswith('file://'):\n\t\t\tstoragePath = storagePath[7:-1]\n\texcept TypeError:\n\t\tif '0=*' in path: # check the root directory, if present, this is the type of object\n\t\t\tresult = 'object'\n\t"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t#print 'look in', path, 'for', type(path)\n\tif path[:9] == '/':\n\t\treturn 'file'\n\tif path[:9] == '/OCFLStorageRoot/':\n\t\treturn 'root'\n\tif path[:9] == '/OCFile/' and not os.path.isdir(path):\n\t\traise\n\tif path[-1] == ':':\n\t\treturn 'file'\n\tif path[:5] == '/DVD/':\n\t\treturn 'directory'\n\tif path[:6] == '/"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tresult = \"\"\n\tif not path.endswith(\".json\"):\n\t\treturn result\n\t\t\n\troot = path.rstrip(\".json\")\n\troot_dir = OcflDir(root)\n\tif not root_dir:\n\t\treturn result + \"error: root is not a directory or has no OCFL objects\"\n\t\t\n\tif not root_dir.is_valid():\n\t\treturn result + \"error: error reading the top level directory\" \n\t\t\n\t\t\n\tresult += \"object: \"\n\tfor filename in OcflStorage.list_files(root_"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tpath = '0=' + path\n\ttry:\n\t\tfile = root.find(path)\n\texcept root.error as e:\n\t\treturn 'file'\t\n\t\t\n\tif file is not None:\n\t\ttry:\n\t\t\tfile.attrs['nm_type']\n\t\t# catch exceptions\n\t\texcept (Exception, TypeError):\n\t\t\treturn 'root'\t\n\t\treturn 'root'\t\t\n\t\t\n\treturn 'object'\t\n\nclass NamasteStorage(storage.Storage):\n\n\tdef create(self, path=None, **kw):\n\t"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t#print(\"find_path_type on %d\" % (os.path.normcase(path)))\n\tfrom_path = os.path.normcase(path)\n\tfrom_path = pathlib.Path(from_path).as_posix()\n\tfrom_path = pathlib.Path(import_module(from_path)).as_posix()\n\tif importlib.machinery.is_mac_bundle():\n\t    from_path = from_path[0].upper() + from_path[1:]\n\telse:\n\t    from_"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tfor root in ('root', '0=*'):\n\t\tif path.startswith(root):\n\t\t\treturn path[len(root)+1:]\n\ndef get_storage_root(path):\n\t\"\"\"\n\t\treturn the actual type of object within the given OCFL path\n\t\n\t\tIf the file type is 'unknown' assume it's either a.ocfl or something similar.\n\t\"\"\"\n\troot = find_path_type(path)\n\tif root == 'unknown':\n\t\treturn 'unknown'\n\tif root == 'file':\n\t"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tresult = \"\"\n\t#\n\t# Find up in ocfl the path, looking through OCFL\n\t# to find the file(s) we are looking for.\n\t#\n\tpath = os2native(path)\n\ttry:\n\t\tif os.path.isfile(path):\n\t\t\tresult = \"file\"\n\t\t\tif os.path.isdir(path):\n\t\t\t\tp = path\n\t\t\telse:\n\t\t\t\tp = os.path.dirname(path)\n\t\telse:\n\t\t\tp = path\n\texcept:"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tbase = path[-1]\n\t\n\tif len(path) > 1:\n\t\tif ':' in path[-1]:\n\t\t\treturn 'file'\n\t\ti = path[-2]\n\t\tif isinstance(i, str):\n\t\t\ti = i\n\t\t\n\t\tif i!= '0=*':\n\t\t\treturn 'root'\n\n\n\treturn 'object'\n\n\ndef get_path(obj):\n\t\"\"\"\n\tReturn a string describing the object at path/offset. A path\n\tcontains a name, and a offset. This is for the"}]}
{"task_id": "62b45b396decaeff903e1001", "prompt": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tfor bugid in bug_ids:\n\t\tfor param in params:\n\t\t\tif bugid == bugid:\n\t\t\t\tself.setparam(bugid, param, \"name\", param)\n\t\t\t\tself.setparam(bugid, param, \"description\",\n\t\t\t\t\t\tparam.replace(\"$\", \" \")\n\t\t\t\t\t\treplace('\"', \"\"))\n\t\t\t\tbreak\n\telse:\n\t\tself.logdebug.append(\"BUG! Couldn't find bugs to amend\")\n\nclass bugzilla_plugin(pluginbase.plugin_t):\n\n\t\"\"\"A plugin"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tlogging.info(f\"{self.__class__} Starting AmendBzekParams\")\n\tself.params = params\n\tself._bug_ids = bug_ids\n\tfor bug_id in bug_ids:\n\t\tself._bug = Bugzilla(bug_id)\t\t\n\t\tself._update_bug()\n\t\n\tlogging.info(f\"{self.__class__} Amend finished\")\n\tlogging.info(f\"{self.__class__} Update the bug list to {self._bug.get_ids}\")\n\n# -*- coding: utf-8 -*-\n\n"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tif not hasattr(self, \"app\") or not self.app.config.option.bugzilla:\n\t\treturn\n\tself.set_params(self.config.option.bugzilla, params, bug_ids,\n\t\t\tfields=self.app.options.amend_fields)\n\tself.amend_bug_ids()\n\tsettings = self.app.config.option.settings\n\ttry:\n\t\twith open(app.config.get(\"bugzilla_data\", \"default\")) \\\n\t\t\tif app.config.get(\"bugzilla_data"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tprint \"Amending the params...\"\n\tparams['host'] = self.HOST\n\tparams['project'] = self.PROJECT\n\tparams['username'] = self.USERNAME\n\tparams['bug_id'] = str(bug_ids[0])\n\tif \"password\" in params:\n\t\ttry:\n\t\t\tparams['password'] = (\n\t\t\t\tself.DEFAULT_PASSWORD\n\t\t\t\t+ params['password']\n\t\t\t\t+ params['bug_id']\n\t\t\t)\n\t\texcept ValueError as ex:\n\t\t\traise KeyError(str(ex) +':"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams1 = {\"from\": \"<%s>\" if self.config.options.get(\"email\")==None and self.config.options.get(\"first-name\")!=None and\n\t\t\t\t\t\t\t(\"user\" in params or \"user\" in bug_ids) else \"\"}for key, value in iteritems(params)\n\t\t\t\tif value:\n\t\t\t\t\tvalstring = key+\"=\" + str(val) + \" and \"+key\n\t\t\t\t\tparams1[key] = valstring\n\t\t\t\tif \"bugID\" in params:\n\t\t\t\t\tparams1[\"bugid\"]=str(params[\""}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tif \"bug_id\" in params:\n\t\tparams[\"bug_id\"] = params[\"bug_id\"].lower().replace(\" \", \"_\")\n\n\tif \"project\" in params:\n\t\tparams[\"project\"] = params[\"project\"].lower().replace(\" \", \"_\")\n\n\tif \"bug\" in params:\n\t\tparams[\"bug\"] = params[\"bug\"].lower().replace(\" \", \"_\")\n\n\treturn super(Bugzilla, self).amend_params(params, bug_ids)\n\ndef amend_headers(self, request, bug_id,"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tbzdb.set_param(self.db_xref, 'bugzilla/all', params)\n\tself.update_params(bug_ids)\n\n\t# Update bug_ids so that bug IDs are\n\t# included in bug_ids later\n\tif bug_ids:\n\t\tbug_ids.add('amend')\n\t\n\t# Update self.bug_ids so that no bug IDs are\n\t# included in the Bugzilla Params again\n\t#\n\t# Note:'self.get_bug_ids' is a list of Bugzilla IDs\n\tself."}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tresult = []\n\tparams = ['id'] + params\n\tfor bugid in bug_ids:\n\t\tbugid = int(bugid)\n\t\tself.check_access(bugid)\n\t\tbugid = self.filter_bad(bugid, 0)[0].replace('-','')\n\t\tparams.extend(['bug_id',bugid])\n\t\tresult.append(self.query_params(params))\n\treturn result\n\ndef getbug(self, bug_ids):\n\t\"\"\"\n\tGet a list of the bugids\n\t\"\"\""}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tif params is None:\n\t\tself.bugzilla_query.exec_query(\"AmendParams\")\n\t\treturn self.bugzilla_query.my_db.cursor().execute(\n\t\t\t\"SELECT id,key_name,value FROM params WHERE \"\n\t\t\t\"key_name IN (\" +\n\t\t\t\",\".join(bug_ids) + \")\", []\n\t\t)\n\n\trow = self.bugzilla_query.my_db.cursor()\n\tfor bug in row:\n\t\tparams.add((int(bug['id']), bug[1],"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\treturn params\n\n\n# def amend_bzopen(self, params, bug_ids, force=False):\n# \t\"\"\"\n# \tReturns a bzopen object\n# \t\"\"\"\n# \tfilename = None\n# \ttry:\n# \t\tf = os.popen('bzadd -T%1 -x \"%2\" -d \"%3' \"\"\" % (sys.argv[0],\n# \t\t\t\t\t\t                               self.id"}]}
{"task_id": "62b4567ed7d32e5b55cc83d9", "prompt": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 1.43 GiB (GPU 0; 22.16 GiB total capacity; 19.06 GiB already allocated; 571.88 MiB free; 20.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b4567ad7d32e5b55cc83af", "prompt": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\timport argparse\n\t\tparser = argparse.ArgumentParser(\n\t\t\t           description=\"Script to download an archive\")\n\t\t\tparser.add_argument('-i','--input','input_filename','File to be downloaded')\n\t\t\tparser.add_argument('-o','--output','output_filename','File to be written')\n\t\t\targs = parser.parse_args()\n\n\t\t\t# if there is more than one input file, it is a required argument to open a file for input\n\t\t\tif args.input and args.input.endswith(\"."}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tap = ArgumentParser(description=__doc__.strip())\n\tap._add_args(arguments)\n\treturn ap\n\n\ndef write_arguments(arguments):\n\t\"\"\"\n\tGiven an argparse instance, write out the contents of our argparse arguments to a file with the\n\tusage information as a header.\n\t\"\"\"\n\tif not arguments.parser:\n\t\tprint(__doc__)\n\t\treturn\n\n\tdef write(file):\n\t\tprint(file)\n\t\tprint(file.write('\\n'.join(arguments.parser.format_help())))\n\n\t"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tresult = ArgumentParser(*arguments).parse_args()\n\tresult.check()\n\treturn result\n\ndef print_defaults(args):\n\t\"\"\"\n\tGiven args, print out all default values\n\t\"\"\"\n\tfor key in args:\n\t\tif not args[key].startswith('-'):\n\t\t\ttry:\n\t\t\t\tprint(\"%-50s = %s\" % (key, args[key]))\n\t\t\texcept AssertionError:\n\t\t\t\tprint(\"%-50s = %r\" % (key, args[key]))\n\nif __name__ == '__main__':\n\n"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(description='Validate a Pillar Manager repository')\n\tparser.add_argument('-o', '--options', metavar='repository', dest='options', type='path', required=True,  help=\"repository file\")\n\tparser.add_argument('-m', '--mount', metavar='mount', dest='options', type='path', required=False,  help=\"Filesystem mount point\")\n\tparser.add_argument('path', nargs='?', metavar='path', type=str, required=True,  help=\"path to pillar\")\n\tparser."}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(usage=__usage__)\n\n\tparser.add_argument(\"--verbose\", action=\"store_true\", default=False, help=\"Be verbose\")\n\tparser.add_argument(\"-v\", action=\"store_true\", help=\"Display verbose messages\", default=False)\n\tparser.add_argument(\"--debug\", action=\"store_true\", default=False, help=\"Set to true to enable debug logging\")\n\n\tlogging.basicConfig(format=\"%(levelname)s:%(pathname)s: %(message)s\")\n\n\tparser.add_argument(\"--log"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(command=__name__)\n\n\t# read in command line arguments into a list\n\tparser.add_argument(\"-X\", \"--X\", dest=\"x\", required=True, choices=(0,1,2,3,4,5,6,7), help='Specify the x in the specified coordinate')\n\tparser.add_argument(\"-n\", \"--n\", dest=\"n\", required=True, help='Specify the n in the specified coordinate')\n\tparser.add_argument(\"--print\", dest=\"p\", required=True, help=\"Provide a text description of the"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(__doc__)\n\tparser.add_argument(\"options\", \n\t\thelp=\"\"\"Optionally specify command-line arguments which will be passed to the script\n\t\t\"\"\")\n\targs = parser.parse_args()\n\treturn args\n\n\ndef main():\n\t\"\"\"Main function that runs the script.\n\t\"\"\"\n\targs = parse_arguments(__doc__)\n\t\n\tif len(args) == 0:\n\t\tprint(\"\"\"Command-line options are required.\n\t\tPlease see \"sv install scripts\" command for more information and syntax of the script"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(\n\t   formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t   description=__doc__)\n\n\tdef _(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tThis is invoked when the script is invoked by the script itself.\n\t\tDo not access the arguments directly, but pass them to argparse.\n\t\t\"\"\"\n\t\tif not args:\n\t\t\targlist = sys.argv[1:]\n\t\t\targs = []\n\t\t\tfor arg in arglist:\n\t\t\t\targs.append(arg)\n\t\tkwargs"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(usage=script_func.__doc__)\n\tparser.add_argument(\"--debug\", \"-D\", help=\"debug mode (not used unless you set -v)\")\n\tparser.add_argument(\n\t\t\"--inputfile\", help=\"File to read from for reading input files\", default=\"stdin\"\n\t)\n\tparser.add_argument(\n\t\t\"--inputfileformat\",\n\t\thelp=\"File format required for reading input files (e.g. --inputfile --version)\",\n\t\tdefault=\"{filename}.{extension}\"\n\t)\n\tparser"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tif len(arguments) == 0:\n\t\tparser = _argument_parser()\n\telse:\n\t\tparser = _argument_parser(str(arguments))\n\treturn parser\n\ndef main():\n\tusage = 'usage: python%s -m <microservice> [options] [script args]'\n\tparser = _argument_parser()\n\tparser.add_argument(\n\t\t'-a', '-s', '--action', metavar='<action>', required=True,\n\t\tdefault=None, help='run the specified action (default: %s)' % ("}]}
{"task_id": "62b45679d7d32e5b55cc83a9", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = []\n\twhile True:\n\t\ttry:\n\t\t\tflag = parser.parse_args().pop()\n\t\t\tif not flag:\n\t\t\t\tbreak\n\t\t\tflags.append(flag)\n\t\texcept:\n\t\t\tbreak\n\treturn flags\n\ndef formatError(msg, args, exitCode):\n\t\"\"\"\n\tFormat the given error using the formatter's format function. This\n\trecurs on the value of exitCode being 0 unless an exception is\n\traised by a parser or formatter.\n\t\n\tformatError() doesn't do the right thing if"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn parser.add_argument.parse_flags(True).split()\n\n\ndef get_argument_type(parser, parser_context=None, **metavar):\n\t\"\"\"\n\tGiven a argparse.ArgumentParser instance, return its argument type.\n\t\"\"\"\n\tflags = parser_flags(parser)\n\n\t# If no flags were specified, take the first argument type in the order they were declared.\n\tif flags is None:\n\t\tfirst_type = parser.parse_known_args[0].type\n\t\t# Assume that we get the same type for all"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn ''.join(['{} [{}]'.format(k, v) for (k,v) in parser.parse_known_args()])\n\n\ndef print_version(print_func):\n    \"\"\"\n    Given a function (self.parser.parse)\n    and a string, print a pretty-printed version\n    of that string out\n    \"\"\"\n    # TODO: Rework so that we don't have to rely on argparse's help()\n    def print_one_line(*args, **opts):"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tif parser is None:\n\t\traise RuntimeError('no parser given')\n\treturn [str(opt) if opt.default is not None else None for opt in parser.nargs]\n\n\ndef parser_group(parser):\n\t\"\"\"\n\t\tGiven a parser, return the default argument group (if any) associated to it.\n\t\"\"\"\n\tif parser is None:\n\t\traise RuntimeError('no parser given')\n\treturn parser.groups[-1] if parser.groups else ''\n\n\nclass GroupedArgumentParser(argparse.ArgumentParser):\n\t\"\"\"\n\t\t"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tif hasattr(parser, \"parse_args\"):\n\t\tparsed_args = parser.parse_args()\n\t\tflags = ''\n\t\tif len(parsed_args) > 0:\n\t\t\tif (parsed_args[-1]) is argparse.Namespace:\n\t\t\t\tflags += ', --'\n\t\t\t\tflags += '{}'.format('' if parsed_args[-2] else '--')\n\t\t\t\tflags += parsed_args[-1](**parsed_args[-3])\n\t\treturn flags[1:]\n\telif parser.default_groups:\n\t\treturn''.join"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn''.join(['-%s' % arg.dest for arg in parser.args]) +'--' +''.join(parser.defaults) + (' -h' + str(parser.header_help))\n\n\ndef get_input_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring, if --help is enabled.\n\t\"\"\"\n\tif hasattr(parser, 'help'):\n\t\treturn''.join(parser.args)\n\n\treturn''.join(parser.defaults)\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tstring = [str(x) for x in parser.values]\n\tif parser.parser_id!= \"\":\n\t\t# Add arguments with names from this parser back into the string\n\t\tflags = ', '.join([\"%s=%s\" % (k, v) for k, v in string if v is not None])\n\t\t# Replace all spaces with underscore\n\t\treturn \"--%s\" % flags\n\t\t#if parser.description!= \"(No description available)\" and parser.dest is not None:\n\t\t#\tflags += \", %s\" %(parser."}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = re.compile('^--([^-]*)$', re.I)\n\treturn flags.format(parser)\n\n\ndef print_help(parser=None):\n\t\"\"\"\n\tPrint usage/help\"\"\"\n\tif parser is None:\n\t\timport argparse\n\t\tparser = argparse.ArgumentParser(formatter_class=RawTextHelpFormatter)\n\t\tparser.add_argument('-h', '--help', help='Print more help info', action='help',\n\t\t                        "}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = []\n\tfor key, val in list(parser.flags.items()):\n\t\tflags.append('%s=\"%s\"' % (key, strval(val)))\n\treturn''.join(flags)\n\ndef default_flags(parser):\n\t\"\"\" Returns a space-separated string of default flags to be set on the given parser instance. \"\"\"\n\tflags = []\n\tfor key, val in list(parser.flags.items()):\n\t\tflags.append('%s=\"%s\"' % (key, strval(val)))\n\treturn''.join(flags)\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tif hasattr(parser, 'add_argument'):\n\t\treturn getattr(parser, 'add_argument')(parser.nargs, parser._groups, parser.add_mutually_exclusive_group)\n\treturn parser.choices\n\n\ndef option_parser(parser, option_string, description=None):\n\t\"\"\"\n\tGiven a parser, return an OptionParser function, and a parser instance\n\tto use with that callable.\n\t\"\"\"\n\tparser = parser.option_parser(option_string, description)\n\tparser._optionals.append(option_string)\n\treturn"}]}
{"task_id": "62b45665d7d32e5b55cc8365", "prompt": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tresult = dict()\n\n\t# For each of the remaining command-line arguments, add the parsed argument to the dict and\n\t# return it as a Namespace instance.\n\tunparsed_arguments += map(partial(add_to_namespace, arg_part, unparsed_arguments), unparsed_arguments)\n\n\t# Return any remaining arguments since they're subcommand arguments.\n\treturn result\n\n\ndef add_to_namespace(result, arg_part, unparsed_arguments):\n\t\"\"\"\n\t\tAdd the parsed argument to the namespace with the subparser name as a key"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog='eval.py', description='Evaluation script.')\n\tparser.add_argument(\n\t\t'file',\n\t\ttype=argparse.FileType('r'),\n\t\thelp='Example usage: python eval.py <input_file>'\n\t)\n\tparser.add_argument(\n\t\t'input_file',\n\t\ttype=argparse.FileType('r'),\n\t\tdefault=argparse.SUPPRESS,\n\t\tmetavar='input_file'\n\t)\n\tparser.add_argument(\n\t\t'-v"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser              = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\t_subparsers_for_use = parser.add_subparsers(title=\"subcommands for use\", dest=\"subparsername\")\n\n\t\n\t# General subparser arguments\n\tparser.add_argument(\"version\", choices=[(\"1.0\", _(\"1.0\")), (\"1.1\", _(\"1.1\")), ])\n\n\t# Subparsers for the \"build-script\" subparser\n\tsubparser_use_scriptnames"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tfinal_arguments = {}\n\tfor sub_argument in unparsed_arguments:\n\t\tif sub_argument.dest is None:\n\t\t\ttry:\n\t\t\t\tsub_argument.dest = eval(sub_argument.parser[0])\n\t\t\texcept:\n\t\t\t\tsub_argument.dest = py_module_name\n\t\telse:\n\t\t\tsub_argument.dest = eval(sub_argument.dest)\n\t\tfinal_arguments.__dict__ = sub_argument.parsed_kwargs\n\t# make sure that the \"global\" subparser is applied at the top"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tsubparsers = parser.parse_known_args(*unparsed_arguments).add_subparsers(description=\"Arguments help with script execution.\")\n\tcommand_parser = subparsers.add_parser(\"script\", help=\"Script to take when executing a script\",\n\t\t\t\t\t\t\t\t\t\t  formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n\t_subcommand_arguments = subcommands._subcommands.pop(0)\n\n\tif subparsers.error:\n\t\traise SystemExit(1)\n\n\tunparsed_arguments = list(vars(subparsers).keys())\n\n\tfor un"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tglobal args\n\tparser = argparse.ArgumentParser(prog='run-from-args-and-attributes')\n\n\tparser.add_argument('-e', '--engine', type=to_str,\n\t\t\t\t\t\t\thelp='name of the engine to use (default is'+ default_engine + ')',\n\t\t\t\t\t\t\tdefault=default_engine)\n\n\targs, unparsed_arguments = parser.parse_known_args(args)\n\n\treturn args, unparsed_arguments\n\ndef main():\n\t'''\n\tMake sure that no more than one command-line argument or environment-like"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\targs = argparse.Namespace()\n\n\tfor name, value in unparsed_arguments.items():\n\t\tif name == '': name = \"global\"\n\n\t\tkwargs = {\n\t\t\t'names': [],\n\t\t\t'tags': [ 'global' ],\n\t\t\t'convert_dates': False,\n\t\t\t'date_format': None,\n\t\t\t'date_parser': None,\n\t\t\t'error_on_unknown': False,\n\t\t\t'error_on_undefined': False\n\t\t}\n\n\t\tfor tag, prefix in value.items"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t# create and register subparser classes\n\tsubparsers = {}\n\tsubparsers_order = []\n\tfor unparsed_argument in unparsed_arguments:\n\t\tif not unparsed_argument.startswith(\"-\"):\n\t\t\tsubparsers_order.append(unparsed_argument)\n\t\tis_subparser = issubclass(unparsed_argument, argparse._SubParsersAction)\n\t\tif not is_subparser:\n\t\t\tsubparsers_order.append(unparsed_argument)\n\t\tsubparsers[unparsed_argument.replace(\"-\", \"_\")] = argparse"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=__name__, description=__doc__)\n\n\tparser.add_argument('-d', '--debug', dest='debug', default=False,\n\t\thelp=\"Enable debug output\", action='store_true')\n\n\tparser.add_argument('-g', '--generate_help', dest='generate_help', default=False,\n\t\taction='store_true',\n        help='Generate usage and exit')\n\n\tparser.add_argument('--version', dest='version', action='version',\n\t\thelp=\""}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tknown_args = {\n\t\t\"_parser:main\": argparse.ArgumentParser(),\n\t\t\"_parser:search\": argparse.ArgumentParser(),\n\t\t\"_parser:parse.config\": argparse.ArgumentParser(),\n\t}\n\tknown_args = {\n\t\tk: argparse.Namespace(v) for k, v in known_args.items()\n\t\tif v is not None\n\t}\n\tknown_args[\"_parser:main\"].add_argument(\n\t\t\"-p\", \"--parser\", help=\"Parser class name\", default=\"main\"\n\t)\n\t"}]}
{"task_id": "62b45665d7d32e5b55cc8364", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser_namespace_to_actions = {}\n\n\t# Parse subparser names\n\tfor action_name, action in subparsers.items():\n\t\tsub_parser_object, _ = action()\n\t\targuments_parser = sub_parser_object.parse_args(unparsed_arguments)\n\n\t\t# Add this subparser to an associative dict, in which all subparsers share a single\n\t\t# argument parser, but the argument parser is also a parser\n\t\tparser_namespace_to_actions[action_name] = sub_parser_object\n\n\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\targparser = argparse._ArgumentParser(subparsers=subparsers, add_help_option=False)\n\tfor parsed_argstring in unparsed_arguments:\n\t\targparser.add_argument(*parsed_argstring.split('='))\n\t\tif hasattr(argparser, parsed_argstring):\n\t\t\targparser.add_argument(argparser.parse_args()[parsed_argstring])\n\t\telse:\n\t\t\tlogger.warn(\"Missing argument %s\" % ('=' + parsed_argstring))\n\t\tfor action_namespace, action_instance in"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser_namespace = argparse._SubParsersAction\n\t\t\t# namespace for all sub_parsers and their names\n\t\n\t_add_namespace = parser_namespace._add_subparser_namespace\n\t\n\t# we do not want this to be called twice, hence this circular\n\t# list\n\targs_with_extra = [_add_namespace(namespace, argument) for namespace, argument in\n\t_namespace_kwargs_map.items()]\n\n\t# put subparser actions first\n\t# in order to support subparsers that define their own namespace\n\t# we use \"Namespace()\" as the"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t# check input arguments\n\tif len(unparsed_arguments) < len(PARSER_ALIASES):\n\t\traise InvalidArgumentError('Expected {} arguments, got {}.'.format(len(PARSER_ALIASES) - 1, len(unparsed_arguments)))\n\tfor unparsed_argument in unparseables:\n\t\tmatched_argument = False\n\t\tfor subparser_name in PARSER_ALIASES:\n\t\t\t# match known subparsers\n\t\t\tif subparser_name in subparsers['actions']:\n\t\t\t\tmatched_argument = True\n\t\t\t\tbreak;\n\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\targument_name2parser = {\n\t\tk:name_parser(arg)\n\t\t#k.add_argument\n\t\tfor name, parser in augw.utils.parameterized_names.items()\n\t\tif hasattr(name, \"__iter__\")\n\t\tand isinstance(name, type)\n\t\tfor name in parser.__all__\n\tif \"CommandParser\" in augw.utils.parameterized_names.__all__:\n\t\tfor key in (\"__name__\", \"__parent__\", \"parser\"):\n\t\t\targument_name2parser.update({"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser_args = []\n\t\n\tfor e in unparsed_arguments:\n\t\ttry:\n\t\t\tparser_args.append(subparsers[e].parser)\n\t\texcept KeyError:\n\t\t\tparser_args.append(subparsers[e])\n\t\n\treturn parser_args, unparsed_arguments\n\n\ndef configure_parser_arguments(parser_args, subparser_arguments, parsed_arguments, argv):\n\tstart_parser_opts = os.environ.get(OSK_ARGS_PREFIX)\n\t\n\t# This is common to all subparsers, but"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparsers = [k for k in subparsers if isinstance(k, argparse.ArgumentParser)]\n\tnamespace: dict[str, argparse.ArgumentParser] = {}\n\tnamespace_subparsers: dict[str, argparse.ArgumentParser] = {}\n\tfor subparser_part in subparsers:\n\t\targuments = subparser_part.parse_known_args(unparsed_arguments)\n\t\tfor action_part in subparser_part.action_list:\n\t\t\tnew_subparser = action_part.setup_new_parser(unparsed_arguments)\n\t\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser_names = {unparsed_arguments[name].__name__ for name in subparsers.__dict__ if\n\t\tname in unparsed_arguments}\n\t# Find any arguments we don't care about, and remove them from any remaining subparsers' dictionary\n\t# This removes \"Repository: \" as the \"repository: \" prefix makes the common arguments valid.\n\tshorthand_action_args = [arg_names[i] for i in [k.strip() for k in subparsers.keys() if k.startswith(\"Repository:\")]]\n\tall_args = parser_names"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tres = {}\n\tfor group_opt in subparsers:\n\t\tif group_opt.group_short:\n\t\t\tfor command, argparser in unparsed_arguments.items():\n\t\t\t\tif group_opt.short:\n\t\t\t\t\tif command in res.keys():\n\t\t\t\t\t\tdel res[command]\n\t\t\t\t\tres[command] = argparser.parse_args(group_opt.groups[group_opt.group_short],\n\t\t\t\t\t\tgroup_opt.args[group_opt.group_short])\n\t\t\t\telse:\n\t\t\t\t\tres[command] ="}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t# Get the set of all valid arguments\n\tparser_args = _get_parser_args()\n\tmissing_arguments = set(unparsed_arguments).difference(parser_args.keys())\n\tremaining_arguments = unparsed_arguments[notmissing_arguments]\n\n\t# If the result wasn't provided, go ahead and do the action\n\tif not remaining_arguments:\n\t\treturn {}\n\n\t# The parsed argument dict\n\tparsed_arguments = dict(remaining_arguments)\n\n\t# Loop through each argument and if there is a name claiming them, call the"}]}
{"task_id": "62b45665d7d32e5b55cc8363", "prompt": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser_name ='make_parsers'\n\tparser = get_parser_by_name(parser_name)\n\t# Add parser functions which are called frequently\n\tparser.add_argument('-O', '--output-suffix', action='store', dest='output_suffix',\n\t\ttype=check_str(valid_unicode, default=VALID_SUFFIX), help='The output file extension (.tar,.zip, or.7z)')\n\tparser.add_argument('--force', action='store_true', dest='force',\n\t\tdefault=False, help='force overwriting of"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\ndef make_subparsers(config=None, aliases=None):\n\t\"\"\"\n\t\tGenerate a subparser object and return it as a (parser, alias) tuple.\n\t\tAlso, aliases can be used to override subparser defaults.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(add_help=True)\n\t\n\tparser.set_defaults(\n\t\t# add_help=False, # FIXME: should be done before this line in code to allow usage in scripts\n\t\tsubcommands=dict(parser=parser),\n\t);\n\t\n\tif not aliases:"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\n\tparser.add_argument('-i', '--installation', action='store_true', help='Install Python library.')\n\tparser.add_argument('path', nargs='?')\n\treturn parser.parse_args()\n\n\nif __name__ == '__main__':\n\targs = make_parsers()\n\tfor d in range(len(args.path)):\n\t\tmodule = importlib.import_module('{}.{}'.format('python' if args.installation else'restd', args.path[d]))\n\t\tmod"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tp = argparse.ArgumentParser()\n\tp_arg_parser = p.add_argument_group('Parser arguments')\n\tp_arg_parser.add_argument('-v', '--verbose', action='store_true', default=False)\n\tp_arg_parser.add_argument('-h', '--help', action='store_true', default=False)\n\tp_arg_parser.add_argument('--version', action='version', version='%(prog)s'+ __version__)\n\n\tsubparsers_group = p_arg_parser.add_subparsers()"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser_module = argparse.ArgumentParser(description=\"A sample argparse top-level program\")\n\tparser_module.add_argument('--sample', metavar='s', type=str,\n\t                           help='This is the sample string that you want to parse')\n\tparser_module.add_argument('-m', '--match', metavar='m', type=str, nargs='?',\n\t                    "}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t# TODO add a test for this function\n\tparser = argparse.ArgumentParser(description='An example-to-be-formatted parser and parser for the Pacemaker framework')\n\tparser_subparsers = parser.add_subparsers()\n\n\tparser_subparsers.required = False\n\tparser_name = 'example1'\n\tparser_name_parser = parser_name\n\tparser_name_arg_name = 'name'\n\tparser_name_arg_parser = parser_name\n\tparser_name_arg_default = parser_name\n\n\tparser_optional"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(formatter_class=lambda prog: argparse.HelpFormatter().pformat(prog))\n\tparser.add_argument(\n\t\t'args',\n\t\tnargs='*',\n\t\thelp='''\n\tThe following arguments will be given to the subcommand as subcommand-specific\n\targuments.  The argument named \"args\" in main() will be the name the argument,\n\twith optional default values for the remaining arguments.\n\t\t\n\t\t* -m, --match  Specify a regular expression, which can be negated (e.g.,\n\t\t\t"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog='deepvac', description='Deep Virtual Acquisition Classification')\n\tparser.add_argument('cfg', help='a config file', type=str, default=None)\n\t# general arguments\n\tparser.add_argument('-k', '--keep-results', action='store_true', default=False,\n\t\thelp='keep results if set to True')\n\tparser.add_argument('-v', '--visualize', action='store_true', default=False, help='visualize')\n\tparser.add_argument('-f', '--"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser_text = \"\"\n\tparser_parents = []\n\tsubparsers_text = \"Parser for the following:\\n\"\n\tsubparsers_parents = []\n\tfor p in parsers:\n\t\tsubparsers_text += \", \".join([\"%s:\\n    %s:\\n\" for s in p.splitlines()]) + \"\\n\"\n\t\t# Use a default indentation for each line\n\t\tsubparsers = Parser(**p.default_values)\n\t\tsubparsers.indent_level = max(subparsers.indent_level, len(subparsers_text"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tprog=\"mrg-ml10\",\n\t\tdescription=__doc__,\n\t\tadd_help=True,\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\tformatter_list=[\n\t\t\targparse.RawDescriptionHelpFormatter,\n\t\t\targparse.RawDescriptionHelpFormatter,\n\t\t])\n\n\tadd_default_metavar_labels()\n\tadd_default_handler_labels()\n\tadd_default_handlers()\n\n\tadd_cli()\n\n\tfor section in _SECTION"}]}
{"task_id": "62b438ba66fea644fe22cca2", "prompt": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 1.43 GiB (GPU 0; 22.16 GiB total capacity; 19.06 GiB already allocated; 571.88 MiB free; 20.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b438b666fea644fe22cc78", "prompt": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(add_help_option=True, strip_whitespace=True, description=__description__)\n\tparser.add_argument('--foo', action='store',dest='bar',default='boo')\n\tparser.add_argument('--no-foo', action='store_const',dest='fooo', const=2)\n\tparser.add_argument('-n', metavar='n', default=0, type=int, dest='no')\n\tparser.add_argument('--no-foo-n', action='store_const',dest='foobar', const=-5"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(usage=USAGE, description=DESCRIPTION)\n\n\tparser.add_argument('-a', '--add-argument', action=\"store\", help=\"Argument key to add(string type expected)\")\n\tparser.add_argument('-f', '--force', action=\"store_true\", help=\"Forces reindex even if a different set of arguments exists. Defaults to True if the set of arguments was not specified or parsed by the user. If a non-default force_arguments_file is specified, this will override any other provided args with the new args.\")\n\tparser.add_argument('-"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(description=\"The script has been invoked with\\n\"\n\t\t\"the following arguments: {0}\".format( [str(i) for i in arguments]) ).add_arguments('-v', '--verbose')\n\n\tparser.add_argument('file', help=\"The file containing the command-line arguments.\")\n\tparser.add_argument('-q', '--quiet', action='store_true',\n\t\tdest='quiet', help=\"Do not print the version information.\")\t\n\tparser.add_argument('-s', '--start-time', default=[],\n\t\thelp=\""}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tap = argparse.ArgumentParser(prog='fritz2')\n\tap.add_argument('target',nargs='*',metavar='targetname',help='[start] Target name to download [end]')\n\tap.add_argument('-f', '--flag', default=0, help='--flag of file to write output to')\n\tap.add_argument('-o', '--output', nargs=2, metavar='outputfilename', help='[default] Name of output file [default: %s]' % (DEFAULT_OUTPUT_FILENAME,))\n\t# Output format and file name\n"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\targ_parser = ArgumentParser(prog=__file__)\n\targ_parser.add_argument(\"-h\",\"--help\", action=\"store_true\", help=\"show this help and exit.\")\n\targ_parser.add_argument(\"--no-color\", action=\"store_true\", help=\"skip ANSI colors.\")\n\targ_parser.add_argument(\"-i\", \"--interactive\", choices=[\"yes\", \"no\"], default=\"no\", help=\"run interactive mode.\")\n\targ_parser.add_argument(\"-v\",\"--version\", action=\"version\", version=\"v{}\".format(version.VERSION"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(\n\t\tusage=u'%prog [option] command arg...'\n\t          % globals(),\n\t    epilog=u\"Options: \\n\\n\".join([u'  option: '+u'  '+u'  Arg: '+u'  'u'u' ', u'command arg...'])+u\"\"\"\n\t          % globals())\n\n\t# the argument parser can parse all the arguments passed through\n\t# by using -A, -E, -M and"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(usage=\"%prog [options] [argument...]\")\n\n\tparser.add_argument(\n\t\t\"output_file\",\n\t\thelp=\"output file (default is stdout)\",\n\t)\n\tparser.add_argument(\n\t\t\"--input\",\n\t\thelp=\"input CSV file\",\n\t\ttype=FileType(formats=[\"csv\"]),\n\t)\n\treturn parser\n\n\n######################\ndef main():\n\targs = parse_arguments(sys.argv[1:])\n\toptions, remainder = args.parse_options(remainder=sys."}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(prog=__name__)\n\tif len(arguments) > 0:\n\t\targuments, unparsed = parser.parse_known_args(arguments)\n\t\t#print(\"Arguments: \", arguments)\n\t\t#print(\"Unparsed arguments:\", unparsed)\n\treturn parser\n\ndef _parse_args():\n\tparsed_args = None\n\ttry:\n\t\tparsed_args = parse_arguments(sys.argv[1:])\n\texcept Exception, err:\n\t\tprint err\n\treturn parsed_args\n\ndef version_check(*cmd"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(description=(\n\t\t\t'Converts TSV files to the specified format, '\n\t\t\t'with specified arguments. This can also transform TSV records to/from a CSV dataset '\n\t\t\t'data set (e.g., a dataset whose TSV record identifiers are all integers).'))\n\n\n\n\tparser.add_argument('source',\n\t\t\t\t\t\t\thelp='An input TSV file (comma-separated values) or a file-like object.',\n\t\t\t\t\t\t\trequired=True,\n\t\t\t\t\t\t\taction=FileOrFile,\n\t\t\t\t\t\t\tdefault=argument_default("}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(\n\t\tprog = 'Pinta', \n\t\t\tversion = '1.1')\n\tparser.add_argument(\t'--help', dest = 'help', type = str, help = \"Shows this help message\")\n\tparser.add_argument(\t'--version', dest ='version', default = False, type=bool,help= \"Displays the Pinta version information\")\n\treturn parser\n\n\ndef run(argv):\n\t\"\"\"\n\t\tRun the given ArgumentParser.\n\t\t\"\"\"\n\t\n\timport argparse\n"}]}
{"task_id": "62b438b666fea644fe22cc72", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tresult = []\n\t# get_argspec is a function that gets the parser function and turns it into a list\n\tget_argspec = parser.get_argspec or inspect.getargspec\n\tfor arg, flag in zip(get_argspec(parser),\n\t\tparser.choices or []):\n\t\tif arg is Ellipsis: flag = \"...\"\n\t\tresult.append(flag)\n\treturn \"\".join(result)\n\n\nclass CommandLineArguments:\n\t\"\"\"\n\tProvides all possible and easily accessible command line arguments.\n\t\"\"\"\n\tdef __"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t# Turn off flags passed to the underlying ArgumentParser constructor:\n\treturn 0\ndef getLogger(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return a logger instance\n\t\"\"\"\n\tlogger = SimpleLogger()\n\tparser.add_argument('-v', '--verbose', action='store_true', default=False,\n\t\t\t\t\t\t\thelp='Show verbose logs')\n\tparser.add_argument('-q', '--quiet', action='store_true', default=False,\n\t\t\t\t\t\t\thelp='Make this program quiet; not recommended with long-running processes.')\n\treturn logger\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tparser.add_argument(\n\t\t'command',\n\t\ttype=str,\n\t\thelp='command to execute (multiple commands joined with \"|\" would be executed as one argument)'\n\t)\n\topt = parser.add_argument_group('options', title='options')\n\n\tdef _set_opt(opt, option_name, state):\n\t\topt.add_argument('--{0}.{1}'.format(option_name, state), action=option_value, default=state,\n\t\t\t\t\t\t\t  help='#{0}.{1}'.format(option_name"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tret = []\n\tfor key, value in parser.items():\n\t\tif key in arg_flags_values:\n\t\t\tret.append(value.split(\",\")[-1])\n\t\telse:\n\t\t\tret.append(key.upper())\n\treturn \" \".join(ret)\ndef argparse_run(args, parser, version_parser):\n\t\"\"\"\n\tRun a child parser of a parent parser and write the results into a file.\n\tThis uses the same Python version as the parent because the format of\n\tparsed config files can use other versions in the future"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tresult =''.join(parser.add_argument_group(title='flags').args)\n\treturn result\n\n\ndef add_parser_help_argument(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument help in a space-separated\n\tstring.\n\t\"\"\"\n\treturn'' + parser.add_argument_group('help').args[0].help\n\n\ndef add_parser_metavar_argument(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument metavar arg in a space-separated\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t# Define the argparse flags\n\tflags = []\n\tflags.extend(\n\t\t[\n\t\t\t'-A', '--all',\n\t\t\t'-p', '--port <int>',\n\t\t\t'-l', '--log <file> (--log-level DEBUG)',\n\t\t\t'-x', '--exit-on-error',\n\t\t\t'-Y', '--no-show-banner',\n\t\t\t'-C', '--check-clients',\n\t\t\t'-S', '--shutdown',\n\t\t\t'-n', '--no-reconnect',\n\t\t\t'-R', '--no-verify-rsh"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tparser.add_argument(\"--random-state\",  action=\"store_true\", help=argparse.SUPPRESS)\n\tparser.add_argument(\"--seed\",  type=int,  action=\"store\", metavar=\"S\", help=argparse.SUPPRESS)\n\n\tparser.add_argument(\"random-seed\", type=int, metavar=\"S\", help=\"Random seed\",\n\t\taction=\"store\", default=0)\n\tparser.add_argument(\"seed\",           type=int, help=\"Random seed\")\n\n\ndef main"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = ['-'*80]\n\tfor arg in parser.args:\n\t\tif getattr(arg, 'choices', None) is not None and isinstance(arg, parser.Choice) \\\n\t\t\t\tand arg.value not in flags:\n\t\t\tflags.append(arg.value)\n\treturn \"\".join(flags) + \"\"\n\n\nclass HelpFormatter(argparse.HelpFormatter):\n\t\"\"\"\n\tWrap another HelpFormatter for use in printing help texts.\n\t\"\"\"\n\tdef _split_help_text(self, args, formatter):\n\t\ttext, pos"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tparser.add_argument('-v', '--verbose', action='store_true', default=False,\n\t\t\t\thelp='Enable verbose mode')\n\tparser.add_argument('-d', '--debug', action='store_true', default=False,\n\t\t\t\thelp='Enable debug mode')\n\tparser.add_argument('-p', '--prefix', required=True, type=int, default=128, metavar='S',\n\t\t\t\thelp='Sets the prefix to be printed in each line of debug logs')\n\tparser.add_argument('-k', '--kernel-id', type=int"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = []\n\tfor name, flag in iter(parser.flags):\n\t\tflag_list = f'{name}: {flag}'\n\t\tif name in parser.group_required and flag_list:\n\t\t\tflags.append(flag_list)\n#\t\tflags.append(flag)\n\treturn ''.join(flags)\n\ndef handler(handler_path, config, args, arg_parser=argparse.Namespace()):\n\tstart_time = time.time()\n\tif isfile(arg_parser.parse_args.__file__):\n\t\t"}]}
{"task_id": "62b438b566fea644fe22cc70", "prompt": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\treturn __usage__\n\ndef bash_complete(line):\n\t\"\"\"\n\t\tA bash completion shell. Add the shell-type prefix to the line here.\n\t\"\"\"\n\treturn completion(bash_completion(), line)\n\ndef completion(command_str, line):\n\t\"\"\"\n\tReturns the command string with completion for the given python line\n\tor the line itself if completion for the command is not available.\n\t\"\"\"\n\tshell = sys.version[:2]\n\tif command_str in ('exec', 'bash'):\n\t\tcmd ='source %s"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tprint(\"\\n\\n\\n\")\n\t\n# borg_completer.bash_completion()\n\nusing System.Text.Json.Serialization;\n\nnamespace Essensoft.Paylink.Alipay.Domain\n{\n    /// <summary>\n    /// AlipayOpenAppAppcontentItemDeleteModel Data Structure.\n    /// </summary>\n    public class AlipayOpenAppAppcontentItemDeleteModel : AlipayObject\n    {\n        /// <summary>\n        /// \u8bf7\u6c42\u6765\u6e90"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tgroup_completion = parser.add_argument_group(\"completion\")\n\tgroup_completion.add_argument(\"-e\", \"--exec-completion\",\n\t\t\t\t\t\t\t\t\ttype=bool,\n\t\t\t\t\t\t\t\t\tdest=\"eexec\",\n\t\t\t\t\t\t\t\t\tdefault=False)\n\tgroup_completion.add_argument(\"-s\", \"--short-completion\",\n\t\t\t\t\t\t\t\t\ttype=bool,\n\t\t\t\t\t\t\t\t\tdest=\"sexec\",\n\t\t\t\t\t\t\t\t\tdefault=False)\n\tparser.set_defaults(eexec=False, sexec=False)\n\targs = parser.parse_args"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tparser = argparse.ArgumentParser( description='Borgmatic command-line completion script.' )\n\tparser.add_argument( '-c', '--command' )\n\tparser.add_argument( '-q' )\n\tparser.add_argument( '--format', type=str,\n\t\t\t\t\t\t  default='vim', choices=['vim', 'json', 'json', 'py', 'cpickle'])\n\tparser.add_argument( '-f', '--format-options', type='int', default=0 )\n\tparser.add_argument( '-s', '--subfolder', type='action"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tos.environ[\"BORGM_COMPLETION_SCRIPT\"] = \"/bin/bash\"\n\tos.execv(\n\t\tPYTHON_EXE, [\"borgm-completion\",\n\t\t\t\t\t\t \"-v\",\n\t\t\t\t\t\t \"borgm-completion\",\n\t\t\t\t\t\t \"--no-input\",\n\t\t\t\t\t\t \"-o\",\n\t\t\t\t\t\t \"-d\",\n\t\t\t\t\t\t \"-D\"])\n\treturn \"/dev/urandom\"\n\npackage com.github.freeacs.shell;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.charset.Charset;\n\npublic class FileUtils"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tparser = ArgumentParser(description=\"Create a new borgmatic script\")\n\tparser.add_argument('target', nargs='?', required=True, help=\"The path to your borgmatic for-file\")\n\tparser.add_argument(\"--completion-style\", action=\"store_true\",\n\t\tdefault=False, help=\"Also generate code completion scripts\",\n\t\tdest=\"completion_style\", nargs=\"?\")\n\tparser.add_argument('-l', '--local-dir', metavar='local_dir',\n\t\tdefault=os.curdir, help='The directory for"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\treturn sh.Command().get_output(bash_completion_path, \"bash completion\", \n\t\tf\"import getopt; {script}\" )\n\ndef bash_completion_path():\n\treturn path.join(__file__, \"..\", \"..\", \"..\", \"..\", \"..\", \".bash_completion\")\n\nborg_prefix = borg_dir_prefix = 'borg_src/'\ntry:\n\tfor file in os.listdir(borg_dir_prefix):\n\t\tfile = path.join(borg_dir_prefix, file)\n\t\tif file"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t_ = \"bash completion\"\n\treturn __commands__.find( [a.name for a in __commands__ if a.name!= \"$COMPL$COMP\" ] )\n\ndef bash_completion_ext(self):\n\t\"\"\"\n\tReturn a bash completion script for the borgmatic command, replacing the result.\n\t\"\"\"\n\tcmd = self.__commands__.find( [a.name for a in __commands__ if a.name!= \"$COMPL$COMP\" ] )\n\tself._setscript( \"__borgcomp_{0}__bash_completion\".format("}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\treturn __mods__['cmd.borg']['default'].main_usage(\n\t\t__mods__['cmd.borg']['usage'])\n\n__mods__['cmd.c_wrapper']['aliases'] = (\n\t\"shell\", \"shell_function\", \"shell_function_function\", \n\t\"shell_command\", \"shell_command_function\",\n)\n__mods__['cmd.c_wrapper']['config'] = [\n\t['type', 'help', {}, ['Usage: help <command>'],\n\t\t['parser', 'add_help_option', {'action':'store"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tshell = os.system('bash -c \"completion.bash\" script.bash')\n\treturn shell\n\n\ndef bash_completions(shell_string):\n\treturn'set -e \"' + bash_completion() + '\" ;'+ shell_string\n\n# -----------------------------------------------------------------------------\n\ndef format_name(name):\n\t'''\n\tFormat a new name to borg or borgme.\n\t'''\n\tif name == \"borg\":\n\t\treturn \" borg\"\n\tif name == \"borgme\":\n\t\treturn \" borgme\"\n\n"}]}
{"task_id": "62b438a266fea644fe22cc2e", "prompt": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=os.path.basename(sys.argv[0]), description=\"\"\"\n\t\tThe parser invoked by the script invokes a series of subparsers which are passed in the\n\t\tprogram arguments. The main argument is specified by the global \"main\" commandline argument.\n\n\t\tThis script is not intended for end-users to parse but should accept arguments like so:\n\t\t   python -m \"parser\" parser_1.py\n\t\t   python -m \"parser\" parser_2.py\n\t\t   python -m \"parser\" parser_3."}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\thelp_str = 'Creates parser namespace with any number of subparser, globals or both.'\\\n\t\t\t\t\t\t'All arguments are required for this script.\\n' \\\n\t\t\t\t\t\n\tparser_help = __doc__\n\tparser_help = parser_help.replace('\\n', '\\n\\n')\n\tparser_help = parser_help.replace('\\r', '\\r\\n')\n\tparser_help = parser_help.replace('&', '&amp;')\n\tparser_help = parser_help.replace('>', '&gt;')\n\tparser_help = parser_"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog='pbr.py', add_help=False)\n\tparser.add_argument(*global_arguments)\n\targs = parser.parse_args(['--local', '--list', 'https://www.google.com/m4b/feeds', ])\n\n\tif args.local:\n\t\tlist_uri = 'https://www.google.com/m4b/feeds'\n\t\tlist_uri = 'https://' + json.dumps(list_uri) + '/feeds'\n\t\tparser.add_"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tprog=__name__,\n\t\tadd_help=False,\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter, # RawDescriptionFormatter is not provided as argparse.ArgumentParser\n\t\tdescription=argparse.SUPPRESS_HELP_DETAILS,\n\t\tdisable_date_time=True,\n\t\t**__kws\n\t)\n\n\tfor i, argument in enumerate(unparsed_arguments):\n\t\tif argument.dest in (None, 'command', 'args'):\n\t\t\tcontinue\n\n\t\tparser ="}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\treturn parser.parse_arguments(*unparsed_arguments)\n\ndef read_template_file(filename):\n\t\"\"\"\n\t\tread a template file, returns a dictionary\n\t\t\"\"\"\n\tr_dict = {}\n\tfor line in open_file(filename):\n\t\tr_line = line.strip()\n\t\tif not r_line:\n\t\t\tcontinue\n\t\tr_dict[r_line] = parse_arguments(r_line)\n\treturn r_dict\n\ndef process_file(filename, output_fn, skip_file_if_"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tsubparsers = parser.parse_known_args(unparsed_arguments)\n\targuments = {}\n\n\tfor subparser in subparsers:\n\t\targuments[subparser.name] = argparse.ArgumentParser(addprefix=False, usage=(\n\t\t\t\"usage: %s%s\" % (sys.argv[0], subparser.description)\n\t\t))\n\t\targuments[subparser.name].add_argument(dest=subparser.action,\n\t\t\taction=subparser.help, nargs=subparser.nargs, metavar=subparser.metavar"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=sys.argv[0])\n\texisting_args = parser.parse_known_args()\n\n\t# print help for global parser subcommand\n\tif existing_args.subcommand == \"help\":\n\t\t# This help prints the global parser usage.\n\t\tparser.print_usage()\n\n\t\t# These exit commands are intended to display the help messages on a console.\n\t\t# When you're in your child command's main() function, exit code 3 is returned.\n\t\t# The user should then take the opportunity to"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers(title=\"parser\",dest=\"subparser\")\n\t\n\tsubparsers.required = True            # this parser is only intended to run when invoked via script\n\tsubparsers.dest = \"subparser\"\n\t\n\tfor unparsed_argument in unparsed_arguments:\n\t\tsubparser = subparsers.add_parser()                              "}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=__name__, description='\"\"A tool for providing a tool for interacting with an Amazon RDS database using Python.\"\"\")\n\tparser.add_argument('target', nargs='?', help='Your target Amazon RDS instance (e.g., `my_instance`) to provision a database to. (If no target is given, a new database will be provisioned in the current instance (if it is available).')\n\n\targs, unknown = parser.parse_known_args(unparsed_arguments)\n\n\tif args.target:\n\t\tif not"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Print the help message and exit.')\n\tsubparsers = parser.add_subparsers(help='subcommands', dest='cmd')\n\tglobal_help = subparsers.add_parser('global')\n\tglobal_help.set_defaults(cmd=None)\n\tglobal_help.set_defaults(cmd=[])\n\tglobal_help.add_argument('-l', '--long', '--long-opt', help='Long options to long options.', action=\"store_true\")\n\tglobal_help.add_argument('-A', '--admin',"}]}
{"task_id": "62b438a266fea644fe22cc2d", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tlogger.debug(\"Starting subparser arguments parsing...\")\n\tsubparsers = kwargs_from_subparser_arguments(unparsed_arguments, subparsers)\n\targs_by_subparser = { k: v._parser for k, v in subparsers.items() }\n\tparsed_args   = dict(__main__.__defaults__=False, )\n\tparsed_args.update(kwargs_from_subparser_arguments(unparsed_arguments, subparsers))\n\tfor arg in unparsed_arguments:\n\t\ttry:\n\t\t\tif args_by_subparser:\n\t\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tresult = {}\n\t\n\tif unparsed_arguments:\n\t\t\n\t\tfrom brian2.api import arguments, mainParser, unparsed_args\n\t\t\n\t\timport textwrap\n\t\t\n\t\ttext_max_width = 80\n\t\t\n\t\tfor name, action in subparsers.items():\n\t\t\t\n\t\t\t# Handle a long subparser.\n\t\t\tif (hasattr(action, 'parser') and hasattr(action.parser,'subParser')\n\t\t\t\tand hasattr(action.parser.subParser, 'option_strings')):\n\t\t\t\t\n\t\t\t\t# Parse the arguments.\n"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tresult = {}\n\tfor name, args_parser in iteritems(subparsers):\n\t\tsubparser = (subparsers[name]()\n\t\t\t    if name in subparsers._names()\n\t\t\t    else argparse.ArgumentParser)\n\n\t\t# get all parsed args\n\t\tparser_args = args_parser.parse_args(unparsed_arguments)\n\n\t\t# get all remaining parser's args in kwargs\n\t\tparser_args = parser_args.values() if isinstance(parser_args, dict) else parser_args.items()\n\n\t\t#"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\t\n\tparser_args = []\n\tfor subparser_name in list(subparsers.keys()):\n\t\targs = OrderedDict()\n\t\tfor parg in list(unparsed_arguments.keys()):\n\t\t\ttry: \n\t\t\t\tpos = unparsed_arguments[parg].index(subparser_name)\n\t\t\texcept KeyError:\n\t\t\t\tpos = len(unparsed_arguments)\n\t\t\t\tpass\n\t\t\tpos = unparsed_arguments.index(parg, pos)\n\t\t\ttry:\n\t\t\t\tparser_args.append"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparsers = dict([(k, v) for k, v in subparsers.items()])\n\targs = []\n\targ_names = set()\n\targ_names.add(\"*\")\n\targs.append((\"register\", subparsers))\n\tfor key, subparser in unparsed_arguments:\n\t\tfor value in subparser.values_to_command(key):\n\t\t\tif not arg_names.issubset(value):\n\t\t\t\targs[-1].append(value)\n\t\t\t\targs.append((\"--\" + value, []))\n\treturn args, args"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser_arguments = {}                                 # The arguments that we need to parse\n\tvalid_subparsers = set()\n\tfor sub_class, sub_parser in subparsers:\n\t\tsub_arg_names = []\n\t\tfor a in sub_parser.add_argument_groups():\n\t\t\targ_name = a.arg_name\n\t\t\tvalid_subparsers.add(arg_name[0])\n\t\t\tsub_arg"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tnew_subparser_namespace = Namespace()\n\targuments_shorthand = subparsers\n\tremaining_arguments = collections.defaultdict(list)\n\t\t\n\tsubparser = None\n\n\tfor name in unparsed_arguments:\n\t\tsubparsers = argument_parsers[name].parse_args(unparsed_arguments, subparsers)\n\t\t\n\t\tsubparser = subparsers['subparser'] if subparsers else None\n\t\tif not subparser:\n\t\t\tif subparser_type_to_parse_handler[name]:\n\t\t\t\tfor shot in self._args_map"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tresult = {}\n\tcurrent_argument = None\n\twhile len(unparsed_arguments) > 0 and not current_argument:\n\n\t\tcurrent_argument = unparsed_arguments.pop(0)\n\t\tsubparsers[current_argument.split()[0]] = current_argument\n\t\ttry:\n\t\t\tsubparsed_arguments = subparsers[current_argument].parse_known_args(current_argument)[0]\n\t\texcept Exception as ex:\n\t\t\t# if it's a custom subparser, try parsing the command line for that subparser\n\t\t\tfor"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tif not hasattr(subparsers, \"actions\"):\n\t\treturn unparsed_arguments\n\tnew_unparsed_arguments = []\n\tfor name, argument in unparsed_arguments:\n\t\tparsed_namespace = {}\n\t\tunparsed_arguments[name] = argument\n\t\tnamespace_list = argument.namespace\n\t\t# handle --namespace (first argument)\n\t\tif isinstance(namespace_list, basestring):\n\t\t\tparse_namespace(namespace_list, parsed_namespace)\n\t\t# handle subparser/subparser arguments (second argument)\n\t\telse"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t# Add any missing arguments to the args list of the parser\n\tfor k in unparsed_arguments:\n\t\ttry:\n\t\t\ta = unparsed_arguments[k] # try this before the k==k, in case k accidentally bees a subparser name\n\t\t\targs = parserutils.add_missing_arg(subparsers, k) # add the subparser name in the list of unparsed_arguments\n\t\texcept KeyError:\n\t\t\tcontinue\n\t\tif k not in a.__dict__:\n\t\t\tcontinue\n\t\t# This is a subparser argument,"}]}
{"task_id": "62b438a266fea644fe22cc2c", "prompt": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = ArgumentParser()\n\tparser.add_argument(\"--version\", help='print version number', action=\"version\", version=\"0.1\")\n\n\treturn parser.parse_known_args()\n\n\nif __name__ == \"__main__\":\n\tmain()\n\t\n\n/*\n * CDDL HEADER START\n *\n * The contents of this file are subject to the terms of the\n * Common Development and Distribution License (the \"License\").\n * You may not use this file except in compliance with the License.\n *\n * You can obtain a copy of the license at usr/src/OP"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t# Build parser\n\tparser = argparse.ArgumentParser(\n\t\tprog='%s.py',\n\t\tusage='%(prog)s [options]...',\n\t\tdescription='Converts Open Cortana binary files to Python files '\n\t\t\t'and optionally outputs to given folder, using pandoc for '\n\t\t\t'processing.',\n\t\tauthor='David Luke',\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter\n\t)\n\tparser.add_argument(\n\t\t'input_filename',\n\t\thelp=('Input filename pattern for converted files"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom prompt_toolkit.application  # Must happen early!\n\tassert isinstance(\n\t\tparser, _BaseParser\n\t)\n\tparser = parser.bind\n\t# We use argparse to call a class with an argument-less constructor\n\t# that is independent of the arguments of __init__.\n\t# pylint: disable=no-member\n\tparser1 = parser.__init__(\n\t\t\"python -m treq\",\n\t\tdescription=None,\n\t\tusage=None,\n\t\tepilog=\"\"\"{0}\n    Usage: python -m treq"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tgroup_parser = parser.add_argument_group('command line argument group')\n\tgroup_parser.add_argument('--arg-1', nargs='?', default=3, help='A simple integer value')\n\tgroup_parser.add_argument('--arg-2', nargs='?', default=-3, help='Another simple integer value')\n\tgroup_parser.add_argument('--number-arg-1', type=int, action='store', dest='arg_1')\n\tgroup_parser.add_argument('--number-arg-2',"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser() # we're going to just pass it through to the base parser\n\tparser.add_argument('-v','--version','Display the __version__ message and exit')\n\tparser.add_argument('--help',dest='help',action='help?', help='display help message', default=True)\n\tparser.add_argument('--list-modules',dest='listmodules',action='store_true',help='Include a list of modules in the output',default=False)\n\treturn parser.parse_args()\n\n\nif __name__ == '__main__':"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"A simple command line interface for Python\" + _(\"coconut v\" + VERSION))\n\n\tparser.add_argument('-c', '--coconutconfig', help=\"\"\"\n\t\tThe config file for the configuration-specific parameters.\n\t\t\"\"\")\n\n\tparser.add_argument('-x', '--execute', help=\"\"\"\n\t\tIf set, will execute the command using the command given in the --coconutconfig\n\t\t(in fact, this command would be something like '-c foo.py').\n\t\t\"\"\")\n"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tdef parsergen(dest, **kwargs):\n\t\ttry:\n\t\t\toptlist = parser.parse_known_args(dest, **kwargs)\n\t\texcept UnknownOption:\n\t\t\treturn argparse.ArgumentParser.parse_known_args(dest, **kwargs)\n\t\telse:\n\t\t\toptdict = optlist[0]\n\t\t\tif len(optlist) > 1:\n\t\t\t\tprint(\"*** Options:\", optlist)\n\t\t\tparser_id = '__' + optlist[0]\n\t\t\tif opt"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t# Generate the basic command line that will be passed to __init__()\n\tparser_text = r\"\"\"\n\tUSAGE: pygments.py [OPTIONS]\n\n\tThis is a tool for parsing XML input.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=sys.argv[0])\n\tparser.add_argument('infile', metavar='FILE',\n\t\t\t\t\t\t\thelp='file containing XML data')\n\tparser.add_argument('-v', '--verbose', dest='verbose', action='store_true',\n\t\t\t\t\t\t\thelp='output pygments info')\n\tparser.add"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser_bases = tuple([\n\t\tbuild_parser_for_command_line(cmd_class) for cmd_class in COMMAND_CLASSES\n\t\t])\n\tparser_for_command_line = __get_parser_for_command_line(parser_bases)\n\tsubparsers = parser_for_command_line.subparsers\n\treturn sorted(subparsers, key=lambda x:x.__name__)\n\n\ndef build_parser_for_shell_command(\n\t\t\t\t    cmd_type: Literal['command','shell'],\n\t\t\t    parser_"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t\tdescription=\"\"\"\n\t\t    Examples of command line options provided via argparse:\n\t\t        %(prog)s\n\t\t        %(parser)s\n\n\t\t    or via argparse and subparsers:\n\t\t        %(parser)s --help\n\t\t        python -m examples.pipparser --help\n\t\t\"\"\")\n\tparser.add_argument(\"--examples\", nargs=\"*\", help"}]}
{"task_id": "62ece4982e6aefcf4aabbd5f", "prompt": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tfrom requests import Request\n\tfrom models import Device\n\tfrom utils.paging import paginate\n\tfrom utils.utils import load_cookies\n\t\n\ttotal = len(response)\n\tif total > max_results:\n\t\tpage_str = \"{}/{}\".format(1, max_results)\n\telse:\n\t\tpage_str = \"{}/{}\".format(1, total)\n\t\n\tcookies = load_cookies(page_str, get_raw_cookie=False)\n\tresponse = paginate(response, cookie_str=cookies)\n\t\n\treturn response"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tmax_pages_num = min(max_results, response[-1] + 1)\n\tfor i in range(0, max_pages_num):\n\t\turl = response[i][0]\n        page = response[i][1]\n        page_object = pywikibot.Page()\n\t    for obj in page:\n\t        page_object = page[obj]\n            yield page_object\n      "}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\timport requests\n\tfrom  webservices import wsr\n\t# 1. query wsr for paging result\n\tif response!= None:\n\t\tpage = max_results\n\t\twhile True:\n\t\t\tr = requests.get('{}/page'.format(response['next_url']), params = {\n\t\t\t\t'api_key': _api_key,\n\t\t\t\t'offset': page * page_size,\n\t\t\t}, timeout=3)\n\t\t\tif r.status_code == 401 :\n\t\t\t\tbreak\n\t\t\tresult, msg  = r.json()\n"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tif response and response[0][0]['name'] not in PAGE_TYPE:\n\t\tfor r in response:\n\t\t\tif r[0]['name'] not in PAGE_TYPE:\n\t\t\t\tRESPONSE['Pagination'][PAGE_START + str(len(PAGE_TYPE)):PAGE_END] = (PAGE_START,\n\t\t\t\t\tr[0]['object'].get('id', ''))[1] +'(Page %s)' % (PAGE_SUFFIX,)\n\t\tPAGE = RESPONSE['Pagination'][PAGE_START][PAGE_START + len(PAGE_TYPE)]\n\t"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tpass\n\t\ndef get_object(file_name, page, max_results):\n\t\"\"\"\n\t\n\tArgs:\n\t    file_name (str): Current file name\n\t    page (int): Current page number index\n\t    max_results (int): Maximum number of items per pages\n\t\n\tReturns:\n\t    (dict) of WAPI response objects.\n\t        Response objects contains fields related with the type of request\n\t        and response.\n\t\"\"\"\n\tpass\n\n/**********************************************************************\n *"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tfor page in range(1,int(max_results)+1):\n\t\tyield_object = api_response_paginated_list(response, page)\n\t\t\n\t\tyield from iter(lambda x: x.__class__(**yield_object), \"\")\n\nclass Pagine(object):\n\t\n\tdef page(self, request, pagename, page_size, page_range, page, reverse=False):\n\t\tpage = list(PaginatedPage(request, Page, page_range))\n\t\tpage.pagename = pagename + \" \" + _(\"(\") + str"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tfor page in response:\n\t\tyield response[page]\n\nimport React from'react';\nimport { shallow } from 'enzyme';\nimport { mount } from 'enzyme';\n\nimport { HeaderInfo } from '../Header';\nimport HeaderRow from '../HeaderRow';\n\nconst mockProps = {\n\ttitle: 'Test',\n\tonToggle: () => {},\n\tonClear: () => {}\n};\n\ndescribe('Header', () => {\n\ttest('should match snapshot', () => {\n\t\tconst view = shallow(<HeaderInfo {...mockProps"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tpage_generator_obj = []\n\tfor i in range(0, len(response), max_results):\n\t\tres = response[i:]\n\t\tpage_generator_obj.extend(res)\n\t\t\n\t# Get all page from generator\n\tpage_objs = list(six.iteritems(page_generator_obj))\n\t#print response\n\treturn page_objs\n\ndef test_paging(response, max_results):\n\t\n\tn_pages = 100\n\t\n\tif response is not None:\n\t\tpaginator = next\n\t\n\tpage_obj"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\twhile response:\n\t\tyield (sorted(list(response), key=lambda item: item['order'], reverse=True)[0], response)\n\t\tresponse = response_paging(max_results - 1)\n\ndef response_pagination(response):\n    \"\"\"\n       A generator object that collects objects that are paginated.\n       \n    Args:\n        response (list): Object that should be paginated based on its class\n    Returns: \n       "}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tif isinstance(max_results, six.integer_types):\n\t\tmax_results = min(max_results, 1000)\n\tp = [response[r] for r in response]  # Recursion!\n\treturn {p[0]: p[1:] if len(p) > 1 else p[0:1]}\n\n@wagtail.hooks.apply_to_collection\ndef paginate(collection_name, **kwargs):\n\tdef default_get_data():\n\t\treturn {\n\t\t\t\"query\": {\n\t\t\t\t\"filter\": {\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd60", "prompt": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tsuffixes = ['B', 'KB', 'MB', 'GB', 'TB', 'PB', 'EB', 'ZB', 'YB']\n\tval = float(size.strip().replace(' ', ''))[0:3]\n\tfor suffix in suffixes:\n\t\tval *= 10**suffix\n\treturn int((val + 0.5) * 1024)\n\ndef get_file_size(path) -> Dict:\n    \"\"\"\n\tReturn file size in bytes.\n\t\n\tArgs:\n\t    path: Path to file\n\t\n\tOutput:\n\t\t"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\treturn int(size.split('K')[0]) * 1024\n\n\n#---- main() function\ndef main() -> int:\n\t# Initializing a process instance\n\t# It will run on a background thread, but can be launched to do a request.\n\t# It will also be terminated once main event handler exits.\n\t# To make other functions safe, initialize from background and to terminate\n\t# all running instances.\n\tprocess_inst = None\n\tprocess_args = sys.argv[:]\n\twhile True:\n\t\ttry:\n\t\t\t# Main event is fired every"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tif hasattr(size, \"__bases__\") and len(size) == 2 and isinstance(size[0], int):\n\t\tsize = \"%.2f\" % size\n\telse:\n\t\tif size is None:\n\t\t\traise ValueError(\"Size cannot be None\")\n\n\t\tif float(size[0]) == 0:\n\t\t\traise ValueError(\"Size cannot have floating point number of seconds\")\n\n\t\tsize = str(size)[2:]\n\tsize = size.lower()\n\n\tif size.endswith(\"b\"):\n\t\tsize = size[:-1]\n"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tfor unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n\t\tif size.lower() not in unit.lower():\n\t\t\traise RuntimeError(\"Conversion between size and bytes is not possible!\")\n\t\t\n\tif int(size) > 1000001:\n\t\traise ValueError(\"Size must be in one decimal place.\")\n\treturn int(size)\n\n/**\n * @license\n * Copyright 2011-2014 Andrey Bodov. All rights reserved.\n */\n\nimport {\n  isPresent,\n  isFunction,\n "}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tval = float(size)\n\tif math.ceil(val) == val:\n\t\treturn math.ceil(val)\n\telse:\n\t\treturn math.log(val) / math.log(1000)\n\n\ndef read_json(file_or_dict: Union[str, Dict[str, Union[str, dict]], None]) -> Dict:\n\tif isinstance(file_or_dict, dict):\n\t\treturn {}\n\telif isinstance(file_or_dict):\n\t\tif isinstance(file_or_dict, str):\n\t\t\t"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tassert isinstance(size, str),'size must be a string.'\n\t\n\tsize = size.lower()\n\n\tif size in ['kb', 'kib', 'kib', 'kip', 'psi',\n\t'pb', 'ebi', 'eb', 'pbi', 'ebp']:\n\t\treturn int(size)\n\telif size == 'eb':\n\t\treturn int(math.ceil(float(self.file_size) / 1024))\n\telse:  # For unknown file sizes, just try to use the default.\n\t\treturn int("}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t# Convert size to bytes.\n\tif size.isdigit():\n\t\tsize = int(size)\n\treturn len(str(int(size)))                    # Ensure at least one decimal point.\n\n\ndef to_human_readable(bytes: number) -> str:\n\t\"\"\"\n\tConvert bytes to human readable string.\n\t\n\tArgs:\n\t\tbytes: A number of bytes to convert (eg: 2K)\n\t\n\tReturns:\n\t\tA human readable string representing the input bytes."}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\treturn int(round(float(size)/1000))\n\n\n################################################################################\n################################################################################\n#                                                                                    #\n#                     "}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\treturn round(float(size) / 1e6, 2)\n\t\n\t\ndef create_directory(directory: str, overwrite=False) -> None:\n\t\"\"\"\n\tCreate a directory.\n\t\n\tThe command exists only when executed using Windows or DOS command.\n\t\n\tArgs:\n\t    directory: The input value\n\t\n\t    overwrite: If True, then if the directory already exists it will be overwritten\n\t               without checking if it's writable.\n\t\n\tReturns:\n\t    True if the directory is"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tif not size:\n\t\treturn 0\n\t\n\tvalue_str = str(float(int(size)))\n\tmatch = re.match(r\"(\\d+)\\s+(K|M|G|T)$\", value_str)\n\tif match:\n\t\treturn int(match.group(2)) * 1000\n\t\n\treturn int(float(size_match))\n\n\ndef size_match_to_bytes(size_match: int) -> str:\n\t# 1 MiB/s\n\tif size_match < 1024:\n\t\treturn \"{0:."}]}
{"task_id": "62ece4982e6aefcf4aabbd61", "prompt": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tresult = {}\n\tfor d in dicts:\n\t\tfor (k, v) in d.items():\n\t\t\tif k in result:\n\t\t\t\tresult[k] += (v, )\n\t\t\t\telse:\n\t\t\t\t\tresult[k] = (v, )\n\t\t\telse:\n\t\t\t\tresult[k] = (v, )\n\treturn result\n\n\ndef _dictlen(d):\n\treturn len(d)\n\ndef _dictlen_empty(d):\n\tif _dictlen(d):\n\t\treturn True\n\treturn"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tresult = copy.deepcopy(dicts)\n\t\t\n\tfor d in result:\n\t\tdictvalue = _to_dictvalue(d)\n\t\tfor dictkey in dictvalue:\n\t\t\tfor sub_key in dictvalue[dictkey]:\n\t\t\t\tresult[dictkey][sub_key] = d[dictkey]\n\t\t\t\t\n\treturn result\n\t\n\ndef _get_from_iterablestructure(data_array):\n\tresult = []\n\t\n\tif isinstance(data_array, list):\n\t\tfor idx, ele in enumerate(data_array):\n\t\t\t"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\treturn dict(zip_longest(*iter(dicts)))\n\n\ndef _set_difference(set_1, set_2):\n\t\"\"\"\n\t\tSet set_1 to the intersection of this and set_2.\n\t\t\n\t\t>>> _set_difference([{'a': 1, 'b': 2}, {'a': 3, 'b': 0}])\n\t\t{'a': 3, 'b': 0}\n\t\"\"\"\n\treturn set(set_1) & set(set_2)\n\ndef _set_intersection(set_1, set_2):"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tfor d in dicts:\n\t\tif isinstance(d, Dictionaries):\n\t\t\tdicts = _merge_dictionaries(d)\n\t\tfor k, v in dicts.items():\n\t\t\tif k in d:\n\t\t\t\td[k] += v\n\t\t\telse:\n\t\t\t\td[k] = v\n\treturn d\n\ndef _get_items(items, d):\n\t\"\"\"\n\tFind dictionary items by the given dictionary d. The function can\n\talso be used to retrieve key values.\n\t\n\t>>> _get_items({('"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tres = []\n\tfor dict in dicts:\n\t\tif isinstance(dict, set):\n\t\t\tres += [f'{str(k)}: {w.get('value', None)}' for k,w in sorted(dict.items())]\n\t\telse:\n\t\t\tres += dict.items()\n\treturn res\n\ndef normalize_filename(filename):\n\t\"Normalize name characters for comparisons\"\n\treturn re.sub('[^A-Za-z0-9_-]', '_', filename)\n\ndef validate_filename(filename):\n\t\"Strip"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t_dicts = list(dicts)\n\tdef _join(x, y):\n\t\tif isinstance(x, _dictsum):\n\t\t\tx.append(y)\n\t\telse:\n\t\t\treturn x.append(y)\n\treturn _join([{'a': x[i], 'b': y[i]} for i in range(len(x))], {})\n\n\ndef _recursive_dict_value_recursive_recursive(_dict, key):\n\tif not _dict:\n\t\treturn {}\n\tvalue = _recursive_dict_value_"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\td = {(k, int(v)) for k, v in iteritems(dicts)}\n\treturn dict([(k, sum(d.get(k, 0) for k in d)) for d in dicts])\n\ndef merge(*dicts):\n\t\"\"\"\n\t\tReturns a new dictionary composed of all the values of dicts\n\t\"\"\"\n\treturn dictmerge(*list(_dictsum(dicts)))\n\n#!/usr/bin/python\n# -*- coding: utf-8 -*-\n# ####################################################################\n#   Copyright (C) 2010 Brian C. Big"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tfor k, v in dicts:\t\n\t\tif isinstance(v, dict):\n\t\t\tyield k, _dictsum(v.values())\n\t\telse:\n\t\t\tyield k, dict(v)\n\n@pyrat.docstringfrom(dict)\ndef _dictsum_ordered(dicts, key, order):\n\t\"\"\"\n\t\tCombine values of the dictionaries supplied by iterable dicts at `key`,\n\t\tin the specified order.\n\t\n\t\t>>> _dictsum_ordered([{'a': 1, 'b': 2}, {'a': 5"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tresults = {}\n\t_counter = {}\n\tfor dictlist in dicts:\n\t\tfor key, value in dictlist.items():\t\n\t\t\toldcounter = _counter.get(key, None)\n\t\t\t_counter[key] = sum([getvalue(dictlist, key) for key, value in dictlist.items() if value is not None]) \n\t\t\tif value is None:\n\t\t\t\tif oldcounter is not None:\n\t\t\t\t\t_counter[key] -= oldcounter\n\t\t\tif oldcounter:\n\t\t\t\tresults[key] = oldcounter\n"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tcombined = {}\n\tfor d in dicts:\n\t\tfor key in d.keys():\n\t\t\tcombined[repr(key)] = d[key]\n\treturn combined\n\n\ndef _split(keyword, sep=\" = \"):\n\t\" Split the keyword into the substrings separated by sep. Any extra spaces will be removed.\"\n\tsp = \"''\"\n\treturn keyword.split(sep)\n\n\ndef _dictsplit(string, sep=\", \"):\n\t\" Split string into pairs of key=value separated by sep. Empty key will be removed from"}]}
{"task_id": "62ece4982e6aefcf4aabbd62", "prompt": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tfor name_value in url_args:\n\t\tif isinstance(url_args[name_value], basestring):\n\t\t\turl_args[name_value] = url + '?' + url_args[name_value]\n\treturn url_args\n\n\ndef _ensure_file_url(url):\n\tif not url:\n\t\treturn url\n\telse:\n\t\tif is_file_url(url):\n\t\t\treturn url\n\t\telse:\n\t\t\treturn url + '#' + url.replace(' ', '%20')\n\n\ndef _parse"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t# Check to see if all URLs have values and remove URLs with empty value list.\n\t# Check any of URL args - that matches anything else - as valid inputs.\n\t# Replace args in URL if args contains values.\n\n\t_allowed_custom_urls = [\n\t\t# We will use this for custom URL argument detection later\n\t\tURL_ARG_VALUE + URL_ARG_PREFIXES +\n\t\t\tURL_ARG_ARGS + URL_ARG_DEFAULT_GROUP,\n\t\t# Custom URL arguments are allowed and are always quoted\n\t\tURL_ARG_VALUE + URL"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tif isinstance(url_args, dict):\n\t\turl_args = url_args.copy()\n\tif not isinstance(url_args, list):\n\t\turl_args = [url_args]\n\tfor v in url_args:\n\t\tv = str(v)\n\t\tif v.find('&') >= 0:\n\t\t\tif v.lower().find('http') >= 0:\n\t\t\t\turl_args[-1] = urllib.parse.quote(v.split('=')[1])\n\t\t\telif v.lower().find('https')"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tif url:\n\t\tfor string in url.split('?'):\n\t\t\tvalue = string[url_args.index(string)]\n\t\t\t# Replace any '?xyz' with 'xyz'\n\t\t\t# Replace anything with '' to keep URL shortening\n\t\t\turl = url[:url_args.index('?')] + value + url[url_args.index('?')+1:]\n\treturn url\n\n\ndef is_local_file(file_path):\n\treturn os.path.isfile(file_path)\n\n\ndef is_remote_"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\turl_item = url_args.get(\"item\")\n\tif url_item and callable(url_item) and callable(url_item()):\n\t\turl_args[\"item\"] = url_item()\n\n\treturn url_args\n\n\ndef _url_args_match_regex(url_args: Dict, regex: str):\n\tfrom regex import escape\n\n\tparts = regex.split(\"|\")\n\tfor part in parts[:-1]:\n\t\tpart = escape(part)\n\t\ttry:\n\t\t\ttry:\n\t\t\t\tpart = part.encode"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tmatch = re.search(r'([^\\s,]+)', url)\n\turl = match.group(1) if match else url\n\n\treturn url, url_args\n\n\ndef _build_relative_doc_url(doc_url, relative_path, doc_path):\n\t\"\"\"\n\t\tReturn a URL path based on a relative path, eg. /api/v1/posts/1\n\n\t\tThe relative_path is determined by _strip_doc_url, where the first\n\t\tpart of the URL has already been removed from the doc_url\n"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\treplace = {\n\t\t'*': url_args['*'],\n\t\t'/': url_args['/'],\n\t\t':': url_args[':'],\n\t\t'@': url_args['@'],\n\t}\n\treturn replace\n\n\ndef isnumeric(key):\n\t\"\"\"\n\tCheck if a key is a number\n\t\"\"\"\n\ttry:\n\t\tfloat(key)\n\t\treturn True\n\texcept ValueError:\n\t\treturn False\n\n\ndef get_all_data(location, api_key=None, custom_type=None"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tif args is None:\n\t\targs = {}\n\tfor name in url_args:\n\t\targs.setdefault(name, url_args[name])\n\treturn args\n\n#pragma once\n#define NOMINMAX 150\n#include \"Config.h\"\t\t\t// for CONFIG\n\n//------------------------------------\n// The following is taken from the old \"common.h\"\n//------------------------------------\n\n#define MAX_FILE_NAME_LEN_SIZE 64\n#define MAX_PATH_LEN 512\n#define MAX_PATH_WITH_SLASH (MAX_PATH_LEN +"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tif '?' in url_args:\n\t\t# url_args has key/value, therefore we can use a regular expression\n\t\t# for url\n\t\treturn re.compile(url_args.replace('?', r'[^/?]*'), re.I).search(url)\n\t# otherwise return the URL\n\treturn url\n\n\ndef get_image_size(file_path):\n\t\"\"\"\n\t    Get the current filesize in bytes\n\t\"\"\"\n\treturn os.path.getsize(file_path)\n\ndef get_thumbnail_size(thumb_"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tif type(url_args) is dict:\n\t\tfor key, value in url_args.items():\n\t\t\turl_args[key] = urllib.quote_plus(value)\n\telif callable(url_args):\n\t\turl_args = urllib.quote_plus(url_args())\n\telif isinstance(url_args, str):\n\t\turl_args = url_args.encode()\n\turl_args = urllib.parse.urlencode(url_args)\n\treturn url_args\n\n\ndef _url_split(url):\n\tif"}]}
{"task_id": "62ece4982e6aefcf4aabbd63", "prompt": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\tif val is None:\n\t\treturn True\n\treturn False\n\n\ndef is_none_number(val: number) -> bool:\n\t\"\"\"\n\tCheck if a number represents a None value.\n\t\"\"\"\n\treturn not isnan(val)\n\n\ndef is_none_datetime(val: datetime) -> bool:\n\t\"\"\"\n\tCheck if a date/time represents a None value.\n\t\"\"\"\n\tif val is None:\n\t\treturn True\n\treturn False\n\n# -----------------------------------------------------------------------------\n#             "}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn not bool(val)\n\ndef is_false():\n\treturn False\n\ndef is_true():\n\treturn True\n\ndef is_number_expression(val: any):\n\tif isinstance(val, ast.Number):\n\t\treturn True\n\treturn False\n\ndef is_lambda(val: Any):\n\tif isinstance(val, (ast.Lambda, AstNode)):\n\t\treturn True\n\treturn False\n\ndef is_lambda_with_args(val: Any):\n\tif isinstance(val, (ast.Lambda, AstNode)):\n\t"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\tif isinstance(val, str):\n\t\treturn False\n\treturn val is None or val is np.nan\n\n\ndef format_dtype(dtype: Union[str, np.dtype]) -> str:\n\t\"\"\"\n\tGiven a dtype, return it format and dtype string representation.\n\tThe return string is converted into the format of the data type.\n\tIf dtype is already known to be a numeric type but not\n\ta data type, this function does nothing.\n\t@param dtype: the dtype\n\t\"\"\"\n\tif dtype is None or dtype is np.nan:\n\t"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\tif isinstance(val, str):\n\t\tif val is not None:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\ndef is_null_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a null value.\n\t\"\"\"\n\tif not isinstance(val, str):\n\t\treturn False\n\tif val is None:\n\t\treturn True\n\treturn False\n\ndef is_all_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string is a null string or a string in a"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\tif isinstance(val, str):\n\t\treturn False\n\treturn True\n\n\ndef is_none_dict(data: any) -> bool:\n\tfor keyval in data:\n\t\tif not is_none_string(keyval):\n\t\t\treturn False\n\treturn True\n\n\ndef is_valid_string(s: str) -> bool:\n\t\"\"\"\n\tDetect whether a string should be a valid string.\n\t\"\"\"\n\tif s!= u'':\n\t\tif len(s) > 1 and not is_none_string(s[0"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\ttry:\n\t\treturn bool(val)\n\texcept (TypeError, ValueError):\n\t\treturn False\n\n\ndef is_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a string value.\n\t\"\"\"\n\treturn isinstance(val, six.string_types) and val\n\n\ndef is_list(val: any) -> bool:\n\t\"\"\"\n\tCheck if a list of item types.\"\"\"\n\treturn isinstance(val, list)\n\n\ndef is_number(val: object) -> bool:\n\t\"\"\"\n\tCheck"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\tif isinstance(val, type(None)):\n\t\treturn True\n\tif isinstance(val, str):\n\t\treturn True\n\treturn False\n\n\n@pytest.mark.parametrize(\"val\", [None, \"\", \"42\", \"5\"]), is_none_string(val)\ndef is_none_int(val: int) -> bool:\n\t# type: (int) -> bool\n\tif not isinstance(val, (int, float)):\n\t\treturn False\n\n\treturn (\n\t\tisinstance(val, (int, float))\n\t\tand not val"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\ttry:\n\t\tif val is None:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept Exception:\n\t\treturn False\n\n\ndef is_bool_string(val: AnyStr) -> bool:\n\t\"\"\"\n\t\tCheck if a string represents a boolean value, either in the form\n\t\t\tTrue, False, false, or True, True\n\t\"\"\"\n\tif isinstance(val, bool):\n\t\treturn True\n\telif isinstance(val, str):\n\t\treturn val in ['True', 'False', 'False', 'True"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t__type__: (readonly Class) -> boolean\n\t\"\"\"\n\tGets the class of the given type, allowing for a None value.\n\t\n\t:arg: value - The None value that's checked, if this is a string type.\n\t\"\"\"\n\tif not isinstance(val, str) and not isinstance(val, bytes):\n\t\treturn True\n\tif not isinstance(val, str):\n\t\treturn False\n\treturn val == '' or not '\\n' in val\n\n#ifndef VISP_C_FILE_TYPE_MANAGER_H\n#define VISP_C_"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\tif isinstance(val, str):\n\t\ttry:\n\t\t\treturn val == \"None\"\n\t\texcept:\n\t\t\treturn False\n    # try to convert to a string\n    try:\n        return str(val)\n    except:\n        return False\n\ndef is_empty_string(val: Any) -> bool:\n\treturn not (is_none_string(val) and val is None or val is \"\")\n\ndef is_null_string(val: str) ->"}]}
{"task_id": "62ece4982e6aefcf4aabbd65", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = []\n\tfor flag in dir(parser):\n\t\tif flag.startswith('_'):\n\t\t\t\tcontinue\n\t\tflag = parser.parser_flags.copy().update(dict(parser_flags(parser.parser_)))\n\t\tflags.append(flag)\n\treturn''.join(flags)\n\n\ndef dict_as_list(d):\n\t\"\"\"Utility for formatting lists with commas (',').\"\"\"\n\treturn [tuple(v.split(',')) if isinstance(v, str) or isinstance(v, bytes) else v for v in d.values"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t\targs = parser.parser_args()\n\t\tif not args._raw_parser:\n\t\t\treturn \"\"\n\t\telse:\n\t\t\t# Only show the parsing flags for the first argument on the list.\n\t\t\t# This means if more than one argument is parsed, the extra\n\t\t\t# whitespace won't be visible.\n\t\t\treturn args.args[1]\n\nparser_flags.unravel_args = lambda s: s.split()\n\ndef parse_args(parser=None):\n\t\"\"\"\n\t\tParse the command line given in the parameter list"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tif isinstance(parser._parser, argparse.ArgumentParser):\n\t\t_arg_str = str_to_pystring(parser._parser.prog)\n\t\tflags = \"-W %s -o %s\" % (_arg_str, parser._parser.prog.__name__)\n\telif isinstance(parser._parser, configparser.ReadConfigParser):\n\t\tif not parser.is_config_file_exists() and not any([arg == \"-\" for arg in parser.__dict__]):\n\t\t\t# if we're in config_filename mode and we expect a config file, we need"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflag_list = []\n\tfor flag in parser.options:\n\t\tflag_list.append(flag)\n\treturn''.join(flag_list)\n\n\nclass ArgumentParser(object):\n\t\"\"\"\n\tThis class wraps the ArgParser module for use by the `args` attribute. It also\n\thandles automatically printing the --help message for options given via CLI\n\tparameters.\n\n\tThis class has helper functions to return, from the main ArgParser module and\n\tadd those options using add_option()\n\n\tArgs\n\t-------\n\tparser : argparse.Argument"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tparser_obj = getattr(parser, \"__obj\", None)\n\n\tif not parser_obj:\n\t\treturn \"\"\n\treturn \" [{}]\".format(parser_obj.flag_list)\n\ndef get_module_obj(module_name):\n\tfor module_obj in sys.modules.values():\n\t\tif type(module_obj).__module__ == module_name:\n\t\t\treturn module_obj\n\n\ndef _main():\n\targs = parse_arguments_args()\n\tconfig_dict = parse_arguments_config(args.config_file"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tres = []\n\tnargs = len(parser.args)\n\tlong = \"long\" if len(parser.nargs) > 4 else \"short\"\n\tshort = parser.short_description if parser.args is None else parser.short_description.capitalize()\n\t\n\tif parser.action in [\"store_true\", \"store_false\",'store_and_clear']:\n\t\tres.append('store_true=%s' % parser.action)\n\t\tres.append('')\n\telif parser.action == \"count\":\n\t\tres.append('count=%s"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\targs = []\n\tfor arg in parser.add_argument_group():\n\t\targs.append(parser.parse_args(arg)[0].split()[-1])\n\treturn''.join(args)\n\n\ndef main(argv=None):\n\timport argparse\n\tparser = argparse.ArgumentParser(prog=argv[0],\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser_flags(parser)\n\tprint(str(parser))\n\n\nif __name__ == '__main__':\n\tmain()\n\n/*\n * Copyright"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflag1 = []\n\tflag2 = []\n\tflag3 = []\n\tflag4 = []\n\n\tfor arg in vars(parser).keys():\n\t\tif arg in [\n\t\t\t\"flags\",\n\t\t\t\"flag1\",\n\t\t\t\"flag2\",\n\t\t\t\"flag3\",\n\t\t\t\"flag4\",\n\t\t]:\n\t\t\tflag1 += parser.convert_arg_str(arg).strip().split()\n\t\t\tflag2 += parser.convert_arg_str(arg).strip().split()\n\t\t\tflag3 += parser.convert"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tparser.add_argument(\n\t\t\"-h\", \"--help\",\n\t\thelp=\"show this help message and exit\",\n\t\taction=\"store_true\")\n\tparser.add_argument(\n\t\t\"-v\", \"--verbose\",\n\t\thelp=\"\"\"Show verbose output\"\"\")\n\treturn parser.parse_args()\n\n\nif __name__ == \"__main__\":\n\tparser = argparse.ArgumentParser()\n\n\tparser.add_argument(\n\t\t\"--data-path\".split(),\n\t\ttype=str,\n\t\tdefault=\"\")\n\tparser"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = []\n\tfor flag in parser.dest_list:\n\t\tif flag in PADDING_FLAGS:\n\t\t\tflags.append(PADDING_FLAGS[flag])\n\t\telif flag._longopts:\n\t\t\tflags.append(flag._longopts[0])\n\t# remove the flag that was given with argparse\n\treturn \"\".join(flags)\n\ndef handle_special_exception_args(parser, args):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, handle special exception\n\targuments:\n\t\tname: The name of the argument (in front"}]}
{"task_id": "62ece4982e6aefcf4aabbd66", "prompt": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\ttry:\n\t\toutput = os.path.realpath(path_name)\n\texcept OSError:\n\t\treturn False\n\tif os.path.isabs(output) and os.path.exists(os.path.join(processed, output)):\n\t\treturn True\n\tif verbose:\n\t\tprint(\"Pathname %s does not exist, processing...\" % path_name)\n\toutput_path = os.path.join(processed, output)\n\tif os.path.exists(output_path):\n\t\treturn True\n\telse:\n\t\t"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tif path_name.isdigit():\n\t\ttry:\n\t\t\tpath_name = os.path.normpath(os.path.join('/', *path_name))\n\t\texcept ValueError:\n\t\t\tif path_name.endswith('\\\\'):\n\t\t\t\tp = path = os.path.abspath(path_name)\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tp = os.path.dirname(path_name)\n\t\t\t\texcept TypeError:\n\t\t\t\t\tpaths = path.split('/')\n\t\t\t\t\tif not paths[-1]:\n\t\t\t\t\t\tpath = os.path"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tif path_name in processed:\n\t\treturn False\n\tprocessed.add(path_name)\n\n\t#\n\t# Check if the path is a directory\n\t#\n\ttry:\n\t\tdir_obj = os.path.dirname(path_name)\n\t\tif os.path.basename(dir_obj) in processed:\n\t\t\treturn False\n\texcept AssertionError:\n\t\tif verbose:\n\t\t\tprint(\"Can't find directory '{0}' \"\n\t\t\t\t\"in path to process\".format(path_name))\n\treturns_true = True"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tif processed.has_key(path_name):\n\t\tif verbose: print \"Skipping: %s already processed\" % path_name\n\t\treturn True\n\t\t\n\tprocessed[path_name] = True\n\t\n\tif path_name in processed:\n\t\tif verbose: print \"Skipping: Directory already processed\" % path_name\n\t\treturn True\n\t\t\n\ttry:\n\t\tif os.path.isfile(path_name):\n\t\t\tprint \"Skipping: %s is a file\" % path_name\n\t\t\treturn True\n\texcept:\n\t\t"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tfull_path_names = processed.copy()\n\tif not path_name.startswith('.'):\n\t\tpath_name = './' + path_name\n\t\t\n\tfor entry_name in full_path_names:\n\t\tif entry_name not in processed:\n\t\t\tprint('\\n\\n>>> %s: Not adding [%s] (already processed)' % (os.path.basename(path_name), entry_name))\n\t\t\tcontinue\n\t\t\n\t\tif verbose:\n\t\t\tprint(\"\\n>>> %s: Already in %s\" % (os.path"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tresult = True\n\tsub_path = path_name[len(processed.path):]\n\tpath = sub_path + '/'\n\n\tif path in processed:\n\t\tpath = path.replace(processed.path, '') # replace\n\t\t# add filename\n\t\tif path_name is not None:\n\t\t\tpath = path + ':/'\n\t\t\tif verbose:\n\t\t\t\tconsole.debug('Skipping [%s]: existing path %s' % (path_name, os.path.join(path, sub_path)))\n\t\telse:\n\t\t\tconsole"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tabs_path = realpath(path_name)\n\tif abs_path in processed:\n\t\treturn False\n\telse:\n\t\tprocessed[abs_path] = set(\n\t\t\t\t\t\t[k if abs_path else '.%s' % abs_path\n\t\t\t\t\t\tfor k in os.listdir(abs_path)\n\t\t\t\t\t\t\t\t\tif os.path.isabs(abs_path)\n\t\t\t\t\t\t  ] + [os.path.relpath(abs_path, root_path)]\n\t\t\t\t\t\t\t+ [path_name]\n\t\t\t\t\t\t\tif path_name == 'root"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tif path_name not in processed:\n\t\t# Already processed\n\t\tif verbose:\n\t\t\tprint(\"'%s' already processed.\" % path_name)\n\t\tprocessed.add(path_name)\n\t\t\n\tresult = Path(path_name).exists()\n\tif result is True:\n\t\tif verbose:\n\t\t\tprint(\"'%s' has already been processed.\" % path_name)\n\t\tresult = True\n\t\t\n\tif result is False:\n\t\tif verbose:\n\t\t\tprint(\"'%s' is an unprocessed file or directory.\" %"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = normalize_path_name(path_name)\n\n\tif normalize_path_name(processed, path_name, verbose):\n\t\treturn True\n\n\tif path_name in processed:\n\t\treturn True\n\t\n\tprocessed.add(path_name)\n\n\tif processed_children(processed, path_name):\n\t\treturn True\n\n\treturn False\n\n\ndef set_processed(processed, path_name, verbose):\n\t\"\"\"\n\t\tSet a file or directory already processed.\n\n\t\tThis call is meant to allow for"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\treturn False\n\ndef read_source(url_to_file):\n\t\"\"\"\n\tRead the contents of a file into a string.\n\t\n\tReturns the contents of the file, if it contains raw data.\n\n\tArgs:\n\t    url_to_file: URL to a file\n\t\"\"\"\t\n\tfile = open(url_to_file, \"r\")\n\tdata = None\n\n\ttry:\n\t\tdata = file.read()\n\t\tfile.close()\n\texcept IOError:\n\t\tpass\n\n\treturn data\n\nclass File"}]}
{"task_id": "62ece4982e6aefcf4aabbd67", "prompt": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tnum_points = len(vertices)\n\tpositions = [[0, 1, 2], [1, 2, 3], [2, 3, 0]]\n\tpoints = []\n\tfor point in vertices:\n\t\tpoints.append([point.x, point.y])\n\n\tfor point_num in range(3, num_points - 1):\n\t\t# check the first position\n\t\tif (\n\t\t\tpoint_num * 3 + point_num - 1 in positions\n\t\t) and (len(positions[point_num * 3 + point_num - 1"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tassert isinstance(vertices, (list, tuple))\n\tif len(vertices) < 3:\n\t\traise ValueError(\"vertices must be a simple list or tuple of length >=3\")\n\tpoly_vertices = list(zip(*vertices[:-3]))\n\treturn zip(poly_vertices[1:3], poly_vertices[0:3])\n\ndef normal_triangulation(triangle, edge, return_nontree_triangleangles=True):\n\tassert isinstance(triangle, (list, tuple)) \n\tassert len(triangle) >= 3, 'Triangle must have more than 3 points'"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\ta,b,c = vertices // 2 \n#Print a,b,c #For debugging\t\n\tprint(\"First    \", a,b,c)\n\tprint(\"Second   \", a+1, b,c) - vertices == len(vertices)-1\t\ndef vertex2list(vertices):\n\t\"\"\"\n\t\tUse this to extract the coordinates.\n\t\tReturns a list of 1:1 tuples\n\t\"\"\"\n\td= {}\n\tprint(vertices) #For debugging\n\ta,b,c = vertices // 2, vertices - 1, vertices //"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tassert vertices.ndim >= 3, \"Too few vertices for polygon. (Expected <3)'\"\n\t\n\tvertices = np.asarray(vertices, order=\"C\")\n\tedges = np.asarray([(p1[0],p1[1]) for p1 in vertices])\n\tp2 = 2.0 * ([(p1[0],p1[1]) for p1 in edges]) + np.array([2.0 * (p1[0],p1[1]) for p1 in edges])\n\ttriangle_indices = np.argwhere(np."}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t# Create the 3 points for the polygon that we want to traverse:\n\t# These start and end vertices will be the vertices.\n\tpoints4 = [(0,1), (1,3), (3,1), (2,1)] # ABCD\n\tvertices = [point[2]+point[0]-point[1]+point[3] for point in points4] # ABCA\n\n\t# Create and return the vertex points for the polygon.\n\tpoints = sorted(vertices, key=cmp_func(vertices[i][2]), reverse=True)\n\treturn points\n"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tvertices_ = []\n\tfor vertex in vertices:\n\t\tvertices_.append(tuple(vertex3tuple(vertex)))\t\n\ttuples = [tuple(vertices_), tuple(vertices_)]\n\treturn tuples\n\n@_vertex4tuple\ndef vertex4tuple(vertices):\n\t\"\"\"\n\treturns 4 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCDE\n\t#DAB, DDA, CDAB, CDA            will return\n\t#"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tnum=len(vertices)\n\t# make sure it's not just a list of 3 points\n\tvertices_ = [vertices]\n\tfor i in itertools.product(range(num)):\n\t\t# get the vertex (3-tuples on each side)\n\t\tvertices_, side_list = itertools.tee(vertices),\n\t\tvertex0, face0, vertex1, face1, vertex2, face2, vertex3, face3 = get_3_list(i,side_list)\n\t\t\n\t\t# set the vertex\n\t\tvertices_.append((vertex"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tpvertices = [0] * len(vertices)\n\tfor i in range(len(vertices)):\n\t\tif int(vertices[i]) == 0:\n\t\t\tpvertices.append(len(vertices[i]))\n\t\telse:\n\t\t\t#if vertices[i] is None:\n\t\t\t#\tbreak\n\t\t#else:\n\t\t\tpvertices[i] = vertices[i]\n\treturn pvertices\nvertex3tuple.func_code = \"\"\"\ndef vertex3tuple(vertices):\n\t#A\n\ttuples = [(0, 0, x"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tA_v, B_v, C_v, D_v = vertices\n\n\tif B_v[0] == A_v[0]: b = C_v\n\telse:                 b = D_v\n\tA_v                       = b + (C_v * B_v[0] * B_v[0] + C_v * B_v + B_v * C_v"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tnpoints=len(vertices)%3\n\tnpoints2=npoints//3//2\n\tpoints= vertices[::npoints2]\n\tnpoints1=npoints/3//2\n\tpoints1= [[1,0,0]]*npoints1\n\tret= []\n\tfor i in range(1,npoints1+1):\n\t\tfor j in range(1,npoints1+1):\n\t\t\tret.append(((points1[i][0], points1[i][1], points1[j][0], points1[j]["}]}
{"task_id": "62ece4982e6aefcf4aabbd68", "prompt": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tif not alphabet:\n\t\treturn \"\"\n\tdigits = len(alphabet)\n\tif padding:\n\t\tdigits -= padding\n\treturn str((alphabet[-1] + 1))[-digits:]\n\ndef pad_string(s: str, padding: int = None) -> str:\n\t\"\"\"\n\tPad the string to be the given number of characters\n\t\"\"\"\n\treturn s if padding is None else (\n\t\t(\" \" * padding) + s[\n\t\t\tlen(s) - padding - 1 :  # trim spaces\n\t\t\tmax(1, len(s"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tif isinstance(number, bytes):\n\t\treturn bytes_to_string(number, alphabet, padding)\n\treturn number_to_string(number, alphabet, padding)\n\ndef pad_string(string: str, length: int, alphabet: List[str]) -> str:\n\t\"\"\"\n\tPad a string to a certain length with the given alphabet.\n\t\n\tThe output has the least significant digit first.\n\t\"\"\"\n\tif length < 0:\n\t\tlength = max(alphabet) + 1 + length\n\twhile true:\n\t\tpadded = string + str("}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\treturn str(number + 1 - (padding if padding is not None else 0)) + \"-\"[: alphabet[length - 1]]\n\n\ndef set_max_list(lst: List, max: Optional[int]):\n\t'''\n\tGiven a L in the given L list, set the max index to the index\n\t'''\n\tif max is not None:\n\t\tlst.append(max)\n\treturn lst\n\n\ndef flatten_string_list(in_list: List[str]) -> List[Any]:\n\t'''\n\tGiven a string, return the list flatt"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t# Convert to upper case\n\tif padding is not None:\n\t\tif type(padding) == dict:\n\t\t\treturn alphabet.get(\"\") + str(number) + \"\".join(sorted(c.upper() for c in alphabet.values()))\n\t\telse:\n\t\t\treturn alphabet.get(\"\") + str(number)\n\t\n\t# Convert to lower case\n\telif number < 10:\n\t\treturn alphabet.get(str(number)) + \"\".join(sorted(list(c.upper() for c in alphabet.values())))\n\n\t# Convert to hexadecimal\n\telif"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tif padding is None:\n\t\tpadding = number.bit_length() // 8\n\tdigits = []\n\tfor digits_position in range(padding):\n\t\tdigits.append(\"0\")\n\tfor a in alphabet:\n\t\tdigits.append(digits_position + a if number % a else digits_position)\n\treturn \"\".join(reversed(reversed(digits)))\n\n@numerics_docstring\ndef string_to_int(string: str, separator: str = \",\") -> int:\n\t\"\"\"String-representing function that takes a string and returns the integer ("}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\ttry:\n\t\tdecimals = len(alphabet)\n\t\tlength = min(max(number, 0), len(alphabet) - 1)\n\t\tnumber = int(number)\n\t\tassert len(number) == length\n\t\tresult = ''\n\t\tfor i in range(length):\n\t\t\tif i < length // 2:\n\t\t\t\tresult += ''.join(alphabet[number - i + 1: number + i])\n\t\t\telse:\n\t\t\t\tc = alphabet[number - i - 1]\n\t\t\t\tif 0 <= number and number <= length -"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tresult: List[str] = [padding, None] * (10 - len(alphabet))\n\t# Start with nothing so that we can distinguish integers from other numbers\n\tfor i in range(len(alphabet)):\n\t\tresult[i * 2] = alphabet[i]\n\t\tresult[i * 2 + 1] = padding\n\treturn''.join(result)\n\n\n@def int_to_file(number: int, filename: str, keep_ending: bool = False, padding: Optional[int] = None) -> dict:\n\t\"\"\"\n\tConvert a"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tif not isinstance(number, int): return \"\"\n\t\n\tif padding is not None and alphabet[-2] == \"M\":\n\t\tfor x in reversed(padding * string.ascii_uppercase_max_ints): \n\t\t\talphabet.append(\"M\") # only keep the first of each character\n\t\n\toutput = \"\"\n\twhile(output!= \"\"): \n\t\talphabet.append(int(number % 10)) # convert all the digits to 1's before writing them\n\t\tnumber //= 10 # every multiple of the number will appear in front of the integer\n\t\toutput += alphabet"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tmax_len = len(alphabet) - 1\n\tif not padding:\n\t\tpadding = 0\n\treturn \"\".join([str(i) for i in range(0, max_len, alphabet[-1] + padding)])\n\n\ndef float_to_string(number: float, suffix: str = \"\") -> str:\n\tif not suffix.isnumeric():\n\t\tsuffix = \".0\"\n\treturn str(number) + suffix\n\ndef make_timestamp(seconds, optional_nanos = 0):\n\treturn date.fromtimestamp(seconds, timezone.utc).strftime"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\treturn \"\".join(sorted(set(alphabet))) + \" \" + number.ljust(padding)\n\ndef float_to_string(number: float, precision: int, padding: Optional[int] = None) -> str:\n\treturn str(float_to_long_decimal_exponent(number, -precision)).rjust(padding, '0').rstrip(\".\\n\")\n\n#ifndef _ASM_X86_ATOMIC_H\n#define _ASM_X86_ATOMIC_H\n\n#ifndef _LINUX_ATOMIC_H\n#error only <linux/atomic."}]}
{"task_id": "62ece4982e6aefcf4aabbd69", "prompt": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tfor each_flow_id in flow_params:\n\t\tif hasattr(flow_params[each_flow_id],'register_info'):\n\t\t\tif register_number==register_number_of_flow_id:\n\t\t\t\tflow_params[each_flow_id].register_info[reg]=(register_value)\n\t\t\t\treturn\n\t\t\n\t\tif register_number==register_number_to_replace:\n\t\t\tflow_params[each_flow_id][reg]=register_value\n\t\t\treturn\n\t\t\n\tflow_params[register_number[reg"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tflow_params_dict = {}\n\tfor k in range(len(flow_params)):\n\t\tflow_params_dict[k] = flow_params[k]\n\treturn flow_params_dict[register_number]\n\ndef _create_flows_for_all_queues(flow_params_dict):\n\tif _use_one_queue():\n\t\tif _use_two_queues():\n\t\t\tflow_params_dict[0] = _create_flows_for_all_queues(flow_params_dict[0])\n\t\tflow_"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t# create dict with dictionary to store all register values\n\treg_dict = {}\n\tfor param_num, param in enumerate(flow_params):\n\t\treg_value = None\n\t\tif not param.get('type') == 'ip':\n\t\t\t# get registered type\n\t\t\treg_type = type(param).split('_')[1]\n\t\t\tif not reg_dict.has_key(reg_type):\n\t\t\t\t# not register for this type\n\t\t\t\t# store value for this register again\n\t\t\t\treg_dict[reg_type] = {}\n"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tfor old_reg, new_reg in zip(flow_params['registers'], register_number):\n\t\tif old_reg!= new_reg:\n\t\t\tflow_params['registers'][old_reg] = register_value\n\t\t\treturn True\n\treturn False\n\n\ndef _is_flow_in_range(flow_id, flow_params):\n\t\"\"\"\n\tFor any flow type that has the flow_id 'flow_id' parameter in flow params\n\tlist then the range is limited to flows that pass the check\n\t\n\t:param flow_id: The"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tif \"1\" in register_value:\n\t\treturn flow_params[register_number][\"1\"] is register_value\n\telse:\n\t\treturn False\n\ndef _is_register(flow_params, register_name):\n\t\"\"\"\n\tCheck if specified number represents register\n\t\n\t:param flow_params: Dictionary containing defined flows \n\t:param register_name: String with the register name\n\t:return: Boolean indicating result of the check\n\t\"\"\"\n\tif register_name not in flow_params[register_name]:\n\t\treturn False\n\telse:"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tif not flow_params:\n\t\tlogger.debug(\"No flows defined\")\n\t\treturn None\n\t\n\tfor parameter in flow_params:\n\t\tif register_number == parameter[\"number\"]:\n\t\t\tparameter[\"value\"] = register_value\n\ndef _parse_input(cmd):\n\t\"\"\"\n\tHelper function for 'flow' input handling\n\t\t\n\t:param cmd: the command line (string with the command)\n\t:returns: flow dictionary with all parameters as key value\n\t\"\"\"\n\tr = re.compile(\n\t\tr\"\"\"\\b(\\w+)"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tfor parameter in flow_params:\n\t\tif parameter['register_type'] == FLOW_REGISTER_TYPE_REG:\n\t\t\tif parameter['register_number'] == register_number:\n\t\t\t\tif parameter['register_value'] == register_value:\n\t\t\t\t\tparameter['register_value'] = register_value\n\t\t\t\t\tparameter['register_number'] = register_number\n\n\ndef _build_parameters(register_params):\n\t\"\"\"\n\tBuild the parameter dictionary including register number\n\t\"\"\"\n\tadd_register_parameters(register_params)\n\tadd_instruction_"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\treturn_type = 0\n\tflow_type = flow_params['type']\n\tflow_mode = flow_params['mode']\n\tflow_direction = flow_params['direction']\n\tif type(register_value) == int:\n\t\tval = register_value\n\t\tregister_value = str(register_value)\n\telif type(register_value) == float or type(register_value) == str:\n\t\tval = register_value\n\telif type(register_value) == list:\n\t\tval = reg_replace(register_number,"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tfor flow in flow_params:\n\t\tflow[REGISTER_KEY] = register_number\n\t\tif register_number in flow:\n\t\t\tflow[REGISTER_VALUE] = register_value \n\t\n\treturn flow_params\n\t\t\t\n\n\"\"\"\n\tFlows that get mapped to some other registers will receive this set of values\n\tThe map_input will be the set from REGISTER_VALUE to REGISTER_NUMBER\n\t\n\tThe flow will be mapped to the following registers:\n\t\t\"h1#h1_4#h1_5#h1_6#h1_7"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tfor flow_param in flow_params:\n\t\tflow_param[register_number] = register_value\n\ndef execute(config, steps, task):\n\t\"\"\"\n\tExecutes the supplied steps\n\t\n\t:returns: Dictionary containing the output\n\t\"\"\"\n\twith get_output(steps) as output:\n\t\treturn output\n\ndef get_output(steps):\n\t\"\"\"\n\tGets the outputs from specified steps\n\t\n\t:param steps: List of tuples describing every flow for the flow graph\n\t:returns: Dictionary which contains one key -\n\t\t\t'time"}]}
{"task_id": "62ece4982e6aefcf4aabbd6a", "prompt": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tif not value:\n\t\treturn value\n\treturn value.replace('[.]', arg)\n\n\ndef fix_quotes(value):\n\t\"\"\"\n\t\tFix quotes in value\n\t\"\"\"\n\t# Strip all quotes, except for whitespace\n\tvalue = value.strip('\"')\n\treturn value\n\n\ndef replace_quotes(value):\n\t\"\"\"\n\t\tReplace all values of '\\' to their literal value\n\t\"\"\"\n\treturn value\n\n\ndef escapechars(value, replace_chars=None):\n\t\"\"\"\n\t\tEscape any characters in value\n\t\"\"\"\n"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tif not value.startswith(\".\"):\n\t\treturn value\n\tif not arg.startswith(\".\"):\n\t\treturn arg + \".\"\n\telse:\n\t\treturn arg\n\n// Copyright (c) 2018, 2021, Oracle and/or its affiliates. All rights reserved.\n// Licensed under the Mozilla Public License v2.0\n\npackage oci\n\nimport (\n\t\"context\"\n\n\t\"github.com/hashicorp/terraform-plugin-sdk/helper/schema\"\n\toci_database \"github.com/oracle/oci-go-sdk/v49/database\"\n"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tout = value.replace('.', arg)\n\n\treturn out\n\n\ndef replace_underscores(value, arg):\n\t\"\"\"\n\tReplaces all values of '_' to arg from the given string\n\t\"\"\"\n\tout = value.replace('_', arg)\n\n\treturn out\n\n\ndef insert_dots(value, arg):\n\t\"\"\"\n\tInserts '.'' into the given string inserting arg as a separator\n\t\"\"\"\n\tout = value.insert(0, '.')\n\n\treturn out\n\n\ndef insert_underscores(value, arg):\n\t\"\"\"\n"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tvalue = str(value)\n\tdotlist = ['.', '.',\n\t\t\t\t',', ',']\n\tif '.' in value:\n\t\tname, value = value.split('.')\n\t\tfor i in xrange(len(name)):\n\t\t\tif name < i:\n\t\t\t\tvalue = value[0:i-1]\n\t\t\t\tdotlist[i] = '.';\n\tdotlist.append('.')\n\tvalue = value.replace('.',dotlist[0:len(name)])\n\targ = arg.replace('.',dotlist[0:len(name)])\n"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tif value and not value[0].isdigit():\n\t\tvalue_start = value.find('.')\n\t\tif value_start!= 0:\n\t\t\tvalue = value[:value_start] + arg\n\t\telse:\n\t\t\tvalue = arg\n\treturn value\n\n\ndef get_type(obj, objtype=None):\n\tif obj is None:\n\t\tif hasattr(obj, '__name__'):\n\t\t\tif obj.__name__[:1] == '?':\n\t\t\t\treturn 'builtin'\n\t\t\telif obj.__name__[-1] == '?"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tvalue = value.replace('.','/')\n\t\n\t\n\tif not arg:\n\t\treturn value\n\tif arg and is_ascii(arg):\n\t\tvalue = value.decode('utf-8')\n\treturn value\n\n\ndef filter_string(args, filters, remove_quotes):\n\t\"\"\"\n\t\tfilter_string function for command line argument\n\t\tReturn filter string and replace all filter delimiters\n\t\tto remove quote or escape characters in argument\n\t\t\"\"\"\n\t#if args is not a string, then error\n\t#error(\"\"\"You must have a"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tvalue_re = re.compile(r'^[A-Za-z_]')\n\treturn value_re.sub(lambda x: x.group(), arg)\n\n\ndef main():\n\tprint(os.path.basename(__file__.split('/')[-1]))\n\t\"\"\"\n\tThis script will build all directories, and if they do\n\tneed to be edited, they will be in the right location\n\t\"\"\"\n\ttry:\n\t\timport json\n\t\tmain_module = Module(join(abspath, \"bin\", \"command-line\", \"git-commands\"))\n"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tvalues = arg.split('.')\n\tfor value, replaceit in values:\n\t\tvalue = value.strip()\n\t\tvalue = value.replace(replaceit, value)\n\treturn value\n\n\nclass FileUploader:\n\n\t# Create a file uploader to upload file to remote cloud\n\tdef __init__(self, source: Union[str, File], localpath: str=None):\n\t\t\"\"\"\n\t\t \n\t\t :source: source to upload or file path for file to upload\n\t\t :localpath: path to uploaded file on the target cloud, it must"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tassert callable(value)\n\treturn value.replace('.', arg)\n\ndef readdirp_all(args):\n\t\"\"\"\n\tRecursively calls readdirp\n\tArgs:\n\t    args[0]: string containing one or more replacements\n\t    args[1..]    : optional strings to pass to subreplayer\n\t    return value   : the output subreplayer\n\t    return value   : if return value!= 0\n\tReturns:\n\t    0 on no errors\n\t\"\"\"\n\tassert callable(args[0])"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\treturn (value[:1] + '.' + value[1:len(value)-1] + value[len(value)-1:]\n\t\t+ arg)\n\ndef split_in_pairs(in_string, separator, maxiters=MAX_IN_PAIR_ITERS):\n    \"\"\"\n    Split string in two strings using a list of separators (string or tuple, e. g. ('1', '5')).\n    If input is already a string, it is simply split in the middle.\n    \"\"\"\n    values = list(filter"}]}
{"task_id": "62ece4982e6aefcf4aabbd6b", "prompt": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tfor c in cls.mro():\n\t\tmro_super = getattr(cls, '__' + str(c.__name__), None)\n\t\tif mro_super is not None:\n\t\t\tmro_super.mro().extend(mro_super.mro())\n\ndef all_superclasses(cls):\n\tr'''\n\t\tReturn all subclasses of a class\n\t\t\n\t\t- returns all descendents in the class.\n\n\t\t- returns mro() and mro(), not the class name\n\t'''\n\treturn [c for c"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tclass Descendant(object):\n\t\tdef __init__(self, owner, descendant_cls):\n\t\t\tself._owner = owner\n\t\t\tself._descendant_cls = descendant_cls\n\n\t\tdef __getattr__(self, attr):\n\t\t\ttry:\n\t\t\t\treturn getattr(self._owner, attr)\n\t\t\texcept AttributeError:\n\t\t\t\tself._descendant_cls.get_descendant(self._owner)\n\t\t\t\treturn self._descendant_cls.get_descendant(self._owner)[attr]\n\n\tdescrtrys_"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tfor c in cls.__bases__:\n\t\tsubclasses(c)\n\nexport default function loadScript(id) {\n  if (get(`_${id}.elevate`)) {\n    return;\n  }\n  if (typeof self._loadScript === 'function') {\n    return self._loadScript(id);\n  }\n  // This can occur if the requested script does not exist.\n  // Note that we don't care about errors since the script wouldn't be\n  // in the cache.\n  //\n  // We could fix"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tfor superclass in superclasses(cls):\n\t\ttry:\n\t\t\tklass = globals()[superclass]\n\t\texcept Exception:\n\t\t\tcontinue\n\t\tcls._superclasses.append(klass)\n\t\tcls._superclasses[-1] += (klass._superclasses or [] )\n\treturn cls\n\n_superclasses = None\n\nclass _ContextManager(object):\n\t\"\"\"\n\tContext manager that remembers stack of callers and\n\tdispatches them to subcontexts.\n\tThis is only used once and then stored in __weakref__ and\n\t__context__"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tprint(\"Generating subclasses list for \", cls.__name__)\n\ttry:\n\t\tsuperclasses = cls.__bases__[1:] + cls.__subclasses__()\n\texcept AttributeError:\n\t\tsuperclasses = []\n\t\t\t# for c in cls.__subclasses__()\n\ttry:\n\t\tsuperclasses.sort(key=lambda s: s.name)\n\texcept TypeError:\n\t\t# sort them by reverse order of bases, to make a list of the superclasses of a type\n\t\tfor s in cls.__subclasses__()[::-1]:\n\t\t\tsuperclasses"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tglobal _subclasses\n\tif _subclasses is None:\n\t\tfrom. import __all_subclasses__\n\t\t_subclasses = [a for a in __all_subclasses__() if hasattr(a, 'get_subclasses')]\n\treturn _subclasses\n\n\ndef find_class():\n\tclassname = demangle(sys._getframe().f_globals.get('__name__', sys.prefix+'__main__'))\n\tif classname is not None:\n\t\tcls = None\n\t\ttry:\n\t\t\tcls = globals()[classname]#, _"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tresult = []\n\tif os.path.isclass(cls):\n\t\ttry:\n\t\t\tfor cls in dir(cls):\n\t\t\t\tresult.append(getattr(cls, 'name', None))\n\t\t\treturn [cls for cls in result[::-1]]\n\t\texcept AttributeError:\n\t\t\treturn []\n\treturn result\n\n\ndef parse(file, parser=HTMLParser()):\n\tparser.feed(file.read())\n\tline = parser.close()\n\tparser.close()\n\treturn line\n\n\ndef replace_entities(text):\n"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = []\n\tfor cls in reversed(sys.modules[sys.modules['__main__'] + '.__subclasses__'].keys()):\n\t\tsubclasses += subclasses(cls)\n\treturn subclasses\n\nif main:\n\tif getattr(sys.modules['__main__'], '__subclasses__', None):\n\t\tfor sc in reversed(sys.modules['__main__'].__subclasses__):\n\t\t\tsubclasses = subclasses | subclasses(sc)\n\tprint(subclasses)\n\n##############################################################################\n#\n# Copyright (c) 2003 Zope"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\ttry:\n\t\tattr_list = inspect.getmro(cls)\n\texcept AttributeError:\n\t\treturn []\n\n\tif isinstance(attr_list[0], str):  # pragma: no cover\n\t\treturn []\n\tif hasattr(cls,'mro'):  # pragma: no cover\n\t\tfor cls in attr_list:\n\t\t\tfor subclass in subclasses(*cls.mro):\n\t\t\t\tyield subclass\n\telif hasattr(cls, '__bases__'):  # pragma: no cover\n\t\tfor subclass in subclasses(cls.__bases__):\n"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tif cls.__supers__ is None:\n\t\treturn []\n\telse:\n\t\treturn cls.__supers__\n\ndef all_classes():\n\tsubclasses = subclasses(BaseMeta)\n\treturn subclasses\n\ndef all_classes_recursive():\n\tsubclasses = subclasses(BaseMetaRecursive)\n\treturn subclasses\n\n\n# ---------------------------------------------------------------------- #\n# Main\n# ---------------------------------------------------------------------- #\n#########################\nclass _MetaObject:\n\t\"\"\"\n\tMeta Class describing a Python abstract class or instance\n\t\"\"\"\n\tdef __init__(self, meta_base):\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd6d", "prompt": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\ta, e, i, l, x, y = map(int, string.split(\" \"))\n\n\tif l == 1:\n\t\treturn a\n\telse:\n\t\treturn x + (a << i)\n'''\n\nfrom __future__ import absolute_import\n\n__version__ = \"0.12.1\"\n\n<?php\n\n/**\n * Copyright 2019 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\ttry:\n\t\treturn int(string)\n\texcept ValueError:\n\t\treturn alphabet.index(string)\n\n#include \"ace/Get_Opt.h\"\n#include \"ace/Sched_Params.h\"\n#include \"ace/Message_Block.h\"\n#include \"ace/Task.h\"\n#include \"ace/Thread_Manager.h\"\n#include \"ace/OS_NS_Thread.h\"\n\nconst ACE_TCHAR *ior_output_file = ACE_TEXT(\"\");\nconst ACE_TCHAR *ior_input_file = ACE"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tn = int(sum(string[i:] * float(alphabet[i]) for i in range(len(string))))\n\twhile alphabet:\n\t\tif alphabet[n]:\n\t\t\treturn n\n\t\tn -= 1\n\treturn n\n\ndef string_to_lower_case(string: str) -> str:\n\t\"\"\"\n\tConvert a string to lower case, using the given alphabet (default to lower case).\n\t\"\"\"\n\treturn string.lower() if isinstance(string, str) else string[0].lower()\n\ndef string_to_upper_case"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\ttry:\n\t\tint_conv = \"\"\n\t\tfor index in alphabet:\n\t\t\tfor ch in string[:index]:\n\t\t\t\tint_conv += ch if ch else \"\"\n\t\treturn int_conv + string[index:]\n\texcept (TypeError, ValueError) as err:\n\n\t\traise ValueError\n\n\n#\n# Some utilities for converting a list of strings to a list of integers.\n#\n\ndef string_to_list(string: str, alphabet: List[str]) -> List[int]:\n\t\"\"\"\n\tConvert a string to a list of integers"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\treturn int(string_[::-1], 2)\n\n\ndef main():\n\t# Read the arguments\n\tfilename = \"sample.csv_v\"\n\talphabet = \"abcdefghijklmnopqrstuvwxyz\"\n\tdata_out_fn = read_csv(filename, alphabet)\n\tdata_in_fn = (\n\t\tdata_out_fn.split(\"$\")[0].split(\".\")[:-1]\n\t).shift()[::-1]\n\t# Set up the algorithm\n\trandom.seed(42)\n\trandom.shuffle(data_in_fn)\n\tdata_in ="}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t# Find the last character (from right to left) whose least significant digit is most significant.\n\tmost_significant_digit = int(string[string.find(alphabet[0], 0)])\n\t\n\tfor c in alphabet[1:]:\n\t\tmost_significant_digit += int(string[string.find(c, 0)])\n\t\n\treturn most_significant_digit\n\ndef num_to_string(num: int) -> str:\n\treturn str(int(num))\n\ndef create_new_line(chars: Iterable[str]) -> str:"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\ttry:\n\t\toct, hex = [], []\n\t\tfor ch in string:\n\t\t\tif not ch.isupper():\n\t\t\t\traise ValueError\n\t\t\toct.append(ord(ch))\n\t\tif not all((oct[i] for i in range(len(oct)/2))):\n\t\t\traise ValueError\n\t\thext = int(hexadecimal, 8)\n\t\tfor i in range(len(oct), 0, -1):\n\t\t\tif oct[i] in string[0:i+1]:\n\t\t\t\thext = int("}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tres = 0\n\twhile len(string) > 0:\n\t\ttry:\n\t\t\tres = int(string[:2]) % 36 if alphabet else int(string[:2])\n\t\t\tstring = string[2:]\n\t\texcept ValueError:\n\t\t\tthrow(\"Invalid integer provided - non-compliant number\")\n\treturn res\n\n@deprecated(\"\")\ndef string_to_bool(string: str, alphabet: List[str]) -> bool:\n\tthrow(\"boolean provided - non-compliant number\")\n\n@deprecated(\"\")\ndef int_to_bool(int:"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\ttry:\n\t\treturn int(string.split()[0]) + \\\n\t\t\t   int(alphabet.index(string[len(alphabet)])\n\t\tif string.endswith('.') or string[-3:] == '.':\n\t\t\treturn int(string[-2:]) + int(alphabet.index(string[-1])\n\t   )\n\texcept ValueError:\n\t\traise ValueError(\"string is not a number\")\n\n\n# TODO: Add missing functions\n# TODO: Add unicode string support\n# TODO: Add a 'has_suffix' function and the same function\n# TODO: Add"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t# Get the number of characters, using a stack.\n\tdigit_count = len(string)\n\tmost_significant_digit = string[0]\n\twhile most_significant_digit == string[0]:\n\t\tdigit_count -= 1 \n\t\tmost_significant_digit = string[digit_count]\n\t\n\treturn int(most_significant_digit) + int(alphabet[0]) * int(digit_count)\n\ndef int_to_string(number, alphabet = list('ABCDEFGHJKMNPQRST')) -> str:\n\t\"\"\"\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd6f", "prompt": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\ttry :\n\t        url = URLDecoder().unescape(url)\n\texcept: # url not in a valid base64 string\n\t        url = url + \".tar.gz\"\n\ttry :\n\t        response = requests.get(url, stream=True, proxies=self.proxies)\n\texcept requests.exceptions.HTTPError :\n\t        return False\n\telse:\n\t        if response.status_code  == requests.codes.ok:\n\t\t\t "}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\tprint(\"Downloading %s...\" % url)\n\tpath = download_url_to_path_archive_from_name(url, os.getenv(\"GOPATH\",os.path.dirname(os.getcwd())) + \"/\")\n\tprint(\"Extracting %s...Done!\" % path)\n\n\tos.makedirs( path )\n\tarchive = Requests( url ).json()[\"manifest\"][\"archives\"].pop() if url.endswith(\".tar.gz\") else Requests(url)\n\t\n\t# Read Archive \n\twith open( str( path ), \"rb\") as f:"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\ttry:\n\t\tdownload_url = requests.get(url)\n\t\t\n\t\tf = open(destination_path + '.gz', 'wb')\n\t\twith zipfile.ZipFile(download_url, 'r') as zf:\n\t\t\tfor url in zf.namelist():\n\t\t\t\tf.write(z.read(url))\n\t\tf.close()\n\texcept Exception:\n\t\treturn None\n\t\n\tf.close()\n\treturn destination_path\n\n// Licensed to Cloudera, Inc. under one\n// or more contributor license"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\timport tarfile\n\timport re\n\n\turl = url.replace('/',os.sep)\n\turl = url.replace('<',os.sep)\n\turl = url.replace('>','')\n\n\tif isinstance(url, six.string_types) and url[-1]!= os.sep:\n\t\turl += os.sep\n\t#Url = url.replace('/',os.sep)\n\t#url += os.sep\n\t#url = url.replace('<','')\n\t#url += os.sep\n\t#url = url.replace('>','')"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\treturn get_path_containing_file(url, destination_path)\n\ndef download_pkg_archive(pkg_hash, destination_path, filename):\n\t\"\"\"\n\tGiven a file name of the downloaded package hash for the package,\n\tdownload that file and convert it into a directory name.\n\n\tArch Linux installs archives into your package directory called 'arch' and archives\n\twhich are named as architecture names do not include '.arch'. A typical architecture\n\tname for an arch Linux installation would be 'core'. The Arch Linux install utility\n\twill download archive from this name"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\tresponse = requests.get(url, headers={\"Accept\": \"text/xml,application/xml\"})\n\t# Get the archive status code.\n\t# Check for status code 0 (OK)\n\tarchive_status_code = response.status_code\n\tif archive_status_code == 0:\n\t\t# Check if the destination_path exists\n\t\tfull_destination_path = os.path.join(\"/\", \"dist\", \"archlinux.org.tar.gz\")\n\t\tif not os.path.exists(full_destination_path):\n\t\t\t# Download archive"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\timport tarfile\n\timport tarfile as tgz\n\tfrom pathlib import Path\n\t\n\twith requests.utils.timezone_fixup() as tz:\n\t\twith tarfile.open(mode='r:gz', fileobj=tgz.open(fileobj=tarfile.TarFile(tarfile.open(url, \"r:\" + tz.now().replace(\"Z\", \" \"), encoding = \"utf-8\").read()))) as archive:\n\t\t\tfor pth in archive.getmembers():\n\t\t\t\tif \"desc\" in pth[\"name\"]:\n\t\t\t\t\tdestination_"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t# Get archive details from URL\n\tr = requests.get('http://'+url+'/archives/desc/'+destination_path,headers=HEADERS)\n\t# Check result\n\tif r.status_code!= 200:\n\t\traise Exception('Error: Archive does not exist. Error:'+ str(r.status_code))\n\t# Create output directory\n\tif not r.headers[\"Content-Type\"].startswith('text'):\n\t\tr.headers['Content-Type'] = 'text/plain'\n\tif not r.status_code == 200:\n\t"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\timport os\n\tos.makedirs(\"archive\", exist_ok=True)\n\t\n\tdef download_archive(url, destination_path):\n\t\tresult = requests.get(url)\n\t\twith open(destination_path, 'w') as stream:\n\t\t\tstream.write(result.content)\n\t\n\t# Open url\n\ttry:\n\t\tdownload_archive(url, 'archlinuxdownload/packages.tar.gz')\n\texcept Exception as e:\n\t\tlog.error(\"Failed to retrieve 'desc' file (%s) of %s\""}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\tif not destination_path.exists(): destination_path.mkdir()\n\t\n\tfilename_regex = r'^[\\.\\/\\:\\-\\:]'\n\twhile True:\n\t\tresp = requests.get(url=urllib.urljoin(url,filename_regex), stream=True).text\n\t\tif resp == 'false':\n\t\t\tlog.error('Failed to download'+ url +'to'+ destination_path.name + '\\n')\n\t\t\tbreak\n\t\turl_splitted = re.findall(r'(\\.tgz|\\.tar\\.gz)')"}]}
{"task_id": "62ece4982e6aefcf4aabbd70", "prompt": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\timport subprocess\n\timport sys\n\ttry:\n\t\tsubprocess.Popen(\"uname -sr\")[0].strip()\n\t\tos_mac = True\n\t\t# print(\"Currently is mac OS\")\n\texcept:\n\t\t# if macOS isn't available, it may be macOS, then fall-back to Linux\n\t\tos_mac = False\n\treturn os_mac\n\ndef checkIfItIsGMAccount(accountId):\n\t\"\"\"\n\tChecks if the selected OS is GMAccount OS\n\t\n\t:param int accountId: Selected account"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\treturn \"macosx\" in sys.platform or \"darwin\" in sys.platform\n\npackage leetcode\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc Test_maxSlidingWindow(t *testing.T) {\n\ttests := []struct {\n\t\tnums    [][]int\n\t\tsliding [][]int\n\t}{\n\t\t{\n\t\t\t[][]int{\n\t\t\t\t{4, 2}, {2, 4}, {3, 3}, {5, 2}, {2, 3}, {4, 3},\n\t\t\t"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\treturn True\n\ndef get_platform_type():\n\t\"\"\"\n\tChecks if the os is Windows\n\t\n\t:return: Windows\n\t:rtype: bool\n\t\"\"\"\n\treturn Windows\n\ndef get_windows_info():\n\t\"\"\"\n\tChecks how to use win32 or macOS\n\t\n\t:return: win32, macOS\n\t:rtype: tuple(int, int)\n\t\"\"\"\n\tos_type := platform.system()\n\tif get_platform_type() or os_type == \"Windows\":\n\t\treturn (os.name, os_"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tpath = pyfuturist.getcwd() + os.sep + '/Applications/System.framework/Library/CoreServices/System.framework/Frameworks/CoreServices.framework/Versions/Current/MacOS'\n\treturn os.path.exists(path)\n\ndef IsMacOS():\n\t\"\"\"\n\tChecks if the current operating system is apple\n\t\n\t:return: True is apple\n\t:rtype: bool\n\t\"\"\"\n\tpath = pyfuturist.getcwd() + os.sep + '/Applications/System.framework/Library/PrivateFrameworks/ImageIO/"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tos_type = platform.system()\n\tif 'Darwin' in os_type:\n\t\treturn True\n\tif 'Darwin' in platform.release() and platform.release()[0] > 4:\n\t\treturn True\n\telif 'Amazon' in platform.release() and platform.release()[2] == \"1\":\n\t\treturn True\n\telif 'Linux' in platform.release() and platform.release()[2] == \"6\":\n\t\treturn True\n\telse:\n\t\treturn False\n\t\n\n\nif __name__ == '__main__':\n"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\timport sys\n\tif sys.platform.lower().startswith('darwin'):\n\t\treturn True\n\t\n\treturn False\n\nif  os.name == 'nt' or sys.platform == 'linux2':\n\tos_is_mac = True\nelif os.name == 'posix' or sys.platform == 'darwin':\n\tif not check_mac() and os_is_mac == True:\n\t\tprint('No check macOS for python OS.')\n\tos_is_mac = False\n\n\"\"\"\nCheck if python python is on linux2 macOS\n\n:return"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tif \"mac\" in os.environ:\n\t\tfound = True\n\t\tif sys.platform.startswith(\"linux\") and \"Darwin\" in os.environ:\n\t\t\tfound = False\n\t\telif \"Windows\" in os.environ:\n\t\t\tfound = False\n\t\telif \"Linux\" in os.environ:\n\t\t\tfound = False\n\t\telif \"Darwin\" in os.environ:\n\t\t\tfound = False\n\t\telse:\n\t\t\tfound = False\n\t\treturn found\n\telse:\n\t\treturn False\n\t\ndef Get"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tdef check_macOS():\n\t\tos_is = _linux_is()\n\t\t\n#\t\tif os_is == True:\n#\t\t\tprint(\"OS is windows.\")\n#\t\t\treturn True\n#\t\telse:\n#\t\t\tprint(\"OS is not windows.\")\n#\t\t\treturn False\n\t\t\n#\tdef _linux_is():\n#\t\tis_mac = False\n#\t\ttry:\n#\t\t\tos = platform.system()\n#\t\texcept:\n#\t\t\tis_mac = \"No OS Is\"\n#\t\t"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\treturn os.getuid() == 0\n\n\ndef is_windows():\n\t\"\"\"\n\tChecks if the os is windows\n\t\n\t:return: True if it is windows, False else\n\t:rtype: bool\n\t\"\"\"\n\treturn False if 'OS_SOFTWARE' in os.environ or sys.platform == 'win32' or sys.platform.startswith('win'):\n\t\treturn True\n\treturn False\n\n#ifndef __ASM_GENERIC_SPINLOCK_H\n#define __ASM_GENERIC_SPINLOCK_H\n\n#ifndef __KERNEL__\n\n#include <asm/"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t# http://stackoverflow.com/questions/19239612/how-is-os-mac-os-slower-than-windows-1\n\t\n\timport sys\n\tosVersion = sys.platform\n\t\n\ttry:\n\t\t# Check the Python version. Use the \"cStringIO\" library if available.\n\t\tfrom io import StringIO\n\t\t\n\t\t# Check if os is Windows\n\t\tif system.get_os_version() == \"Windows\":\n\t\t\t# http://stackoverflow.com/questions/2140769/how-can-i-find-"}]}
{"task_id": "62ece4982e6aefcf4aabbd71", "prompt": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\treturn {}\n\ndef build_map_type(keyvals, dtype=None):\n\tr\"\"\"\n\tBuild a map or dict that holds a keyval dictionary\n\t\"\"\"\n\treturn {\n\t\tkey: build_obj({'key': item})\n\t\tfor item in keyvals\n\t}\n\ndef find_re(re, string):\n\t\"\"\"\n\tSearch a regex for a string in an array of strings and return the first\n\tmatch found\n\n\tArguments:\n\t\tre: regular expression to search\n\t\tstring: strings array (e.g. ['"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tregex_keys = {}\n\t\n\tfor key in item.keys():\n\t\tregex_keys['/'.join(key.split(\"/\")[0].split('.'))] = [key]\n\t\n\trules = {}\n\t\n\tfor key in item[\"*.h\"]:\n\t\trules['*.h'] = {}\n\t\trules['*.h'] = list(regex_keys.keys())\n\t\trules['*.h'].append('.h')\n\t\trules['*.h'].append(re.escape(key))\n\t\n\tfor key in item[\"*.cpp\"]:\n"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\treturn re.sub(\n\t\t       r\"(\\w+)\\Z\",\n\t\t       lambda match: key_to_regex_regex(\n\t\t           {\"key\": match[1], \"_last_check\": match[0]}),\n\t\t       regex_keys\n\t\t       )\n\n\ndef key_to_regex_regex(item):\n\t\"\"\"\n\tThis is intended to convert keys found in *.cpp or *.h files into regexes.\n\t\n\tArgs:\n\t  "}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\trules = {\n\t\t'subst_match': {\n\t\t    'dir': 'dir/a.cpp',\n\t\t    'header': 'header/a.h'\n\t\t}\n\t}\n\t\n\treturn re.sub(r'\\s', '', item.get('subst_match', ''))\n\n\ndef main():\n\t\"\"\"Parse regex_dict file and convert the resulting rule into a dict\n\tfor easier parsing in re\n\t\n\t\n\tArgs:\n\t\t* rule_dict: a dictionary containing all of the regex rules\n\n\tReturns:\n"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tpass\n\ndef get_wildcard_regex(key):\n\t\"\"\"\n\tReturns the wildcard regexes to match in the regex files.\n\t\n\tArgs:\n\t    key: the wildcard to match\n\t\"\"\"\n\tglob_filenames, wildcard_regexes = g_wildcard_regex_to_regex(key)\n\treturn wildcard_regexes\n\n\n\t\ndef find_key_in_file(file_name, find_keys):\n\t\"\"\"\n\tGiven a dict of filenames containing wildcard keys and a dict of regex keys\n\tto convert them to equivalent regex strings that"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\treturn {key: re.compile(v, re.I)}\n\ndef regex_dict(item: Dict[str,List[Tuple['regex', str]]]) -> Dict[str,List[Tuple[regex_key, str]]=]:\n\t\"\"\"\n\tGiven a dict where the keys are all filenames with wildcards, convert each\n\tkey (e.g. *.cpp) to a regex (e.g. '.+!') and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t      "}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tfor filename, regex in items_dict.items():\n\t\tfor regexes, key in regex.items():\n\t\t\tif not re.search(regexes, filename):\n\t\t\t\tvalues = {filename : key}\n\t\t\t\tyield filename, values\n\t# end for\n\n# These rules are a bit complicated due to the fact that the keys may take an\n# optional value and in the case of a wildcard, it is a straight key\n\n# A bunch of characters that are legal for regex keys\nlegal_characters = {\n    '\\\\': '(?:\\x1f/(["}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tfor k in args:\n\t\tif re.match(r'.*\\*.[^.]*(.[^.]*).*', k):\n\t\t\tnew_k = [f\"{k}.*\" for f in regex_keys]\n\t\t\targs[k] = re.sub(new_k[0], f\"{new_k[1]}.{new_k[2]}\", args[k])\n\t\t\n\treturn args\n\n// Copyright 2013 The go-github AUTHORS. All rights reserved.\n//\n// Use of this source code is governed by a BSD-style\n// license that"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tfor file, keylist in regex_dict.items():\n\t\tfor key in keylist:\n\t\t\tnew_regex = re.escape(key)\n\t\t\tif os.path.basename(file).lower() not in keylist:\n\t\t\t\tnew_key = new_regex + os.path.basename(file).split('.')[0]\n\t\t\telse:\n\t\t\t\tnew_key = new_regex\n\t\t\tif new_key in item:\n\t\t\t\tprint('*** Key is already in {}'.format(new_regex))\n\t\t\t\tcontinue\n\t\t\titem["}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tfor line in open(item_file).read().split('\\n'):\n\t\ta, b, c, f = line.strip().rsplit('\\t', 1)\n\n\t\tif not f.endswith('.h'):\n\t\t\tcontinue\n\n\t\tt = {}\n\t\tt[f] = a + b + c\n\n\t\tfor k in line.split(';'):\n\t\t\tr = re.split('\\s+', k.strip())\n\t\t\tif not r[0].endswith('.h'): continue\n\n\t\t\tt[r[0]] = r"}]}
{"task_id": "62ece4982e6aefcf4aabbd72", "prompt": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tdef echoinp(self, string):\n\t\tmatches = re.findall(r\"\\s{0,}#?\\n\".format(string), string.lower())\n\t\tfor match in matches:\n\t\t\tself.erase(self.strip(match))\n\n\tdef esort(self, string):\n\t\t\"\"\"\n\t\tRemoves all occurrences of'string'. You cannot change the order of the items.\n\t\t\"\"\"\n\t\tself.insert(0, '\\0')\n\t\t#self.insert(0, string)\n\t\tself."}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tdef _( name, quotation = QUOTATION_OFF ):\n\t\t\"\"\"\n\t\tName replacement function.\n\t\tIt is called when the user edits one of the quotes.\n\t\t\"\"\"\n\t\tre.sub( quotation + r'\\g<1>', lambda g: '' if g.startswith( name ) else f'<1>{name}\"', f\"<1>{re.sub('#|^_!|#\\\\{\\'\",f\"<1>{re.sub( quotation + r'\\g<2>', f\"<1>{name}'\", f\"'<'\", quote( name"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tif \" \" in src_str:\n\t\tresult = src_str.replace(\" \", \"\")\n\treturn result\n\ndef replace_quote(name):\n\t\"\"\"Replace quote in the string with \\n or \\r\"\"\"\n\tif \" \\n\" in name:\n\t\tname = name.replace(\" \\n\", \"\").replace(\" \\r\", \"\")\n\treturn name\n\n# Main method for parsing the string into tokens.\ndef get_tokens():\n\t\"\"\"Tokenizes the string as it is parsed.\n\t\tReturns a list of :class:`.Token`; each item is"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t#     p = f\"{t[1]}$*$0{$*}*`+\\\\x\"\n\t#         ^^^^^|^/\n\t#         |     |\n\t#         |     x+\n\t# p\n\t#         |     /  .\n\t#         v     /\n\t#       "}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tdef quote_removed(self, name):\n\t\tif name.strip() == 'True\\n':\n\t\t\treturn self.do_quote(self.curframe_locals)\n\t\t\treturn self.do_quote(self.current_function)\n\t\telse:\n\t\t\tself.curframe_locals['__builtins__'][name] = None\n\n\t# this regex matches Python regexp with 'escape.builtin' escapes\n\t# special regexp used in do_literal replacement.\n\t# This resembles all others like do_literal replacement.\n\t#\n\t_"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tdef __init__(self, name):\n\t\tself.name=name\n\t\"\"\"\n\tName is the name of the quote, used in __init__\n\t\"\"\"\n\tdef get_name(self):\n\t\treturn self.name\n\n\"\"\"\n\tAdd text with quotes to a symbol\n\"\"\"\ndef add_quotes_(string, delimiter=','):\n\tpieces = string.split(delimiter)\n\tif len(pieces[0]) == 0 :\n\t\treturn\n\tfirst = pieces[0]\n\tlast = pieces[-1]\n\tquote_index  = delimiter"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tif text.startswith(\"``\") and text.endswith(\"``\"):\n\t\ttext = text[2:]\n\treturn text\n\n# Remove quote from the given name.\ndef remove_quote (text):\n\t\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\treturn text[1:-1] if text.endswith(\"``\") else text[:-1]\n\n# Removes first and last non-quote from the given name.\ndef remove_non_quote(s):\n\t# Returns reversed string. (We don't support unicode, though.)\n\treturn \"\".join("}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tname = re.sub(r\"\\s\", \"\", name)\n\treturn name\n\n\ndef replace(string):\n    \"\"\"Return (string | substitute) after checking for name substitution.\n    \"\"\"\n    newstring = string\n    i = 0\n    result = \"\"\n    if string:\n        newstring = string.replace(\"%20\", \" \")\n        result = \"\".join([ result.strip() if result else \"\" for i in range(0, len(string.split"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tif isinstance(name, basestring): name=name.strip(\"\\\"\")\n\tres=re.search('\\''+name.strip(\"'\").strip(\"'\")+\"'\\'\",\"\")\n\tif isinstance(res,basestring):\n\t\t\tres=re.split('\"\\''+name.partition('\\\\').strip(\"\\\"\").strip('\"').strip('\\\"\"'),'')\nif 'quote' in str(type(res[0])):\n\tdel res[0].find('quote')\n\tres=str(res)\n\tif res:\n\t\tres.insert(0,'')\n\"\"\""}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tsub = re.sub(r'[()\\(\\)]', '', name)\n\t# remove the quotes (and no more) from beginning/end and then clean the name\n\treturn name[:name.find(r'[')] == sub\n\ndef create_name(name):\n\t\"Create a name using the given character\"\n\tif not name.strip():                                             # Remove quote if present"}]}
{"task_id": "62ece4982e6aefcf4aabbd73", "prompt": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\ttry :\n\t\timport _rex\n\t\tdef split(s, sep, maxsplit):\n\t\t\t\"\"\" Do sub-expression split from s.  sep defaults to '\\s'.\n\t\t\tAn empty sep defaults to a comma (',').\n\t\t\tmaxsplit = length of s to be searched. defaults to sys.maxsize.\n\t\t\t\"\"\"\n\t\t\treturn [c.strip() for c in s.split(sep, maxsplit)]\n\texcept ImportError, exc:\n\t\tif exc.name=='test':\n\t\t\tinfo(\"cannot import _rex from 'test'\")\n"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tflags = (flags | REG_NOSUB << 5)\n\n\tdef _split(self, command, executable, env, flags=0):\n\n\t\t# strip leading path, drive, dir separator\n\t\tcommand = command.rstrip()\n\n\t\t# default envp\n\t\tenv = env or {}\n\t\t\n\t\t# try to shlex\n\t\tif not os.path.isabs(executable): executable = os.path.normpath(executable)\n\t\tif not os.path.isabs(command): command = executable\n\t\ttry:\n\t\t"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\ndef fast_split(text, split, encoding=DEFAULT_ENCODING, errors='strict'):\n\ttry:\n\t\treturn split(text, encoding=encoding, errors=errors)\n\texcept Exception as e:\n\t\tmsg = six.text_type(e)\n\t\tif 'Illegal sequence of escaped characters' in msg:\n\t\t\t# An illegal sequence of escaped characters happens in shlex.split(), which results\n\t\t\t# in UnescapeError being raised on Windows but causes infinite recursion or other\n\t\t\t# issues. Here we raise it.\n\t\t\traise ValueError("}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tdef _split_command\n\t\tplatform, *argv = $CHILD_PM.first\n\t\tif platform == 1\n\t\t\tplatform = 'posix'\n\t\tend\n\t\t\n\t\tplatform\n\tend\n\t\n\t# In case the last char of the command line is a backslash (\\\\), that kind\n\t__backslash = /(?:^|[^\\\\]|\\\\)\\\\\\\\/.match(ARGV[ARGVN - 1])[0]\n\t\n\t# The main function\n\tdef squeezecmd(cmd)\n\t\tcmd, *child_argv = @CHILD_PM."}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t# The fast regexp (not unicode characters in Python 3) is slower\n\t# than the fast str/file split on Windows, and still faster on\n\t# other platforms (i.e. the standard Python).  However, we're\n\t# not using it as part of other parsing/creation functions\n\t# and, as such, the fast REGEX cannot be used on POSIX.\n\t#\n\t# So, this function will use the system's default code page for\n\t# the files.  This function might be used in other ways, so we\n\t# try to keep that code"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tfrom subprocess import Popen, PIPE, Popen, STDOUT, STDERR, \\\n\t\t\tcheck_call, close_pipe_fds, PIPE\n\timport os as _os\n\t\n\t@classmethod\n\tdef split_line(cls, line) -> list[string]:\n\t\t\"\"\"Split words on spaces and return tokens, not including single quotes.\n\n\t\tIt is designed to handle quoting. Unescape all backslashes before splitting.\n\t\tQuotes are doubled and quoted. If a word ends in a single quote, then\n\t\tend will be unescaped.\n\t "}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tclass _SplitResultClass(object):\n\t\t\"\"\"\n\t\tType of individual result values from splitString on-the-fly.\n\t\tOn-the-fly conversion is possible and performed once.\n\t\t\"\"\"\n\t\tdef __init__(self, result, commandline, error, _type, is_posix):\n\t\t\tself.result = result\n\t\t\tself.commandline = commandline\n\t\t\tself.error = error\n\t\t\tself._type = _type\n\t\t\tself.is_posix = is_posix\n\t\t\t\n\t\t@property\n\t\tdef"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t#from sys import exit as system_exit\n\t\t\n\tif sys.platform.startswith(\"win\"):\n\t#\timport winreg\n\t\twin = winreg.OpenKey(\n\t\t\twinreg.HKEY_LOCAL_MACHINE,\n\t\t\t\"ThisKey\"\n\t\t\t)\n\t\tmy_list1 = []\n\t\tmy_list2 = []\n\t\tfor name in win.enumKeyPaths():\n\t\t\tmy_list1.append(name)\n\t\tfor name in win.enumValues(\"REG\"):\n\t\t\tmy_list2.append(name"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t# Python 2\n\tfrom sys import platform as py_sys_platform\t\t\n\tif platform == py_sys_platform:\n\t\t# Python 2\n\t\tpy_split = lambda text:  text.splitlines()\n\telif platform == \"cli\":\n\t\t# Python 3\n\t\tpy_split = lambda text: list(map(str.split, text.strip().split(\"\\n\")))\n\telse:\n\t\traise ValueError(\"unknown platform \"+str(platform))\n\n\t# Windows version\n\tsplitter = ['']\n\tif platform == \"windows\":\n\t\t"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tif platform == 1:                        \n\t\traise SyntaxError('shlex.split() only works on Windows')\n\tif platform in (1, 0):                     # windows, cmd\n\t\tif len > 0 :\n\t\t\t#if len < 4 and os.path.isfile(os.path.join(sys.argv[2],'shlex.cfg')) and sys.platform.lower().startswith"}]}
{"task_id": "62ece4982e6aefcf4aabbd74", "prompt": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\treturn download_from_github(archive_path, archive_file)\n\ndef get_url_from_archive(archive_path):\n\treturn download_from_github(archive_path)\n\n\ndef download_from_github(archive_path, archive_file):\n\t# If archive_path is empty string, return the zip file.\n\tif archive_path.count(\"/\") == 0:\n\t\treturn get_archive_file_urls(archive_file, archive_path)\n\t\n\t# If archive_path is a folder, go down the url tree and download"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\toutputDir1 = os.getcwd()\n\tcmd_unzip = 'tar xfzf --add-files {archive_path}'.format(**locals())\n\tos.system(cmd_unzip)\n\t\n\trepoLocation = os.path.join(outputDir1, \"repo\", archive_path)\n\trepoUrl = subprocess.check_output(('tar xfzf --add-files {archive_path} --name repo'.format(**locals())))\n\t\t\n\treturn repoLocation, repoUrl\n\ndef unzip(inputLocation, archiveLocation, repoUrl):\n\tarchiveLocationFolder ="}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tresult_filepath = None\n\tresult_url = None\n\ttry:\n\t\tzip_reader = zipfile.ZipFile( archive_path, mode='r')\n\texcept zipfile.BadZipfile as e:\n\t\tlogger.get_logger().error(\"Error uncompressing archive: %s\", e)\n\t\treturn False\n\tfor file in zip_reader.filelist:\n\t\tif file.filename.lower() in filenames:\n\t\t\tresult_filepath = file.filename\n\t\t\tresult_url = url\n\tzip_reader.close()\n\tif"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\timport zipfile, zipfile._cmp\n\twith zipfile.ZipFile(archive_path) as zf:\n\t\tzf._extract_archive(os.path.basename(archive_path).split('.')[0], lambda z: zf)\n\t\tzf[-2].close()\n\t\tzf[0] = ''\n\tarchive_url = {}\n\tfor file in os.listdir(archive_path):\n\t\t# This is kind of hacky but is needed because uncompress(3) doesn't seem to work.\n\t\tif file.endswith('.pkl'):\n"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tpath = os.path.normpath(str(archive_path))\n\tarchive_url, archive_file_name, archive_file_size = path_extractor.extract(path)\n\tassert archive_file_name!= 'No archive exists.'\n\t\n\treturn {'https://archive.schamam.net/archive_url/' + archive_url + '/%s' % archive_file_name, \\\n\t\tarchive_file_size}\n\n\ndef upload_content(data: bytes, url: str, content_dict: dict, filename: str='Untitled"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tcmd = 'gzip -9 -d > %s' % archive_path\n\n\ttry:\n\t\tsubprocess.call(cmd, shell=True)\n\texcept subprocess.CalledProcessError:\n\t\tprint('Uncompression of', archive_path, 'failed.')\n\t\treturn None\n\n\telse:\n\t\t\"\"\"If all went well we can return archive_path.\"\"\"\n\t\treturn archive_path\n\n\ndef undecompress(archive_path):\n\t\"\"\"\n\t\tGiven an existing archive_path, uncompress it.\n\t\tReturn a file repo url which"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\ttry:\n\t\tinput_archive = inputFile\n\texcept:\n\t\ttry:\n\t\t\tinput_archive = open(inputFile,'rb')\n\t\texcept:\n\t\t\traise FileNotFoundError(\"Unable to open input file: '\"+inputFile+\"'\")\n\tif os.path.exists(inputFile):  os.remove(inputFile)\n\tpath = os.path.dirname(inputFile)\n\tlocaldir = os.path.join(path, \"Local.zip\")\n\tif os.path.exists(localdir):\n\t\tos.remove("}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tdef uncompress(existing_path):\n\t\twith open(existing_path, 'rb') as existing_archive:\n\t\t\tfile_repo_url, _ = urlretrieve(existing_archive.read(), _downloads_url)\n\t\t\twith urlquickopen(file_repo_url, 'r') as repo:\n\t\t\t\tfor line in repo.readlines():\n\t\t\t\t\tfile_path = str(urllib.unquote(line.strip()))\n\t\t\t\t\tfile_path = file_path.split('zip/')[-1].split('.')[0].strip('.')."}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tcmd = \"uncompress %s\" % archive\n\twith subprocess.Popen(cmd.split()) as proc:\n\t\toutput = \"\"\n\t\tfor line in proc.stdout:\n\t\t\toutput += line + \"\\n\"\n\t\t\n\treturn output\n\ndef unpack_file(dest_path, unpack_folder):\n\t\"\"\"Unpack a file to a desired path.\"\"\"\n\tdest_path = Path(dest_path)\n\tif unpack_folder == \"\":\n\t\tunpack_folder = \".\"\n\tsrc_path = Path(unpack_folder, dest_path.name"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tcmd = ('uncompress.exe', '-i', f'{archive_path} | xargs gent.exe uncompress.exe')\n\tsubprocess.call(cmd, shell=True)\n\tif os.path.exists(ARCHIVE_PATH): # check if we could uncompress\n\t\tprint(f'Uncompressed archive {ARCHIVE_PATH}',)\n\t\twith open(ARCHIVE_PATH, 'rb') as f:\n\t\t\tfile_data = f.read()\n\t\treturn os.path.join(ARCHIVE_PATH, os.path.splitext(file_data"}]}
{"task_id": "62ece4982e6aefcf4aabbd75", "prompt": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\ntry:\n\tout = subprocess.check_output([\"git\", \"diff\", \"--name-only\", \"--quiet\", \"HEAD\"],stdout = subprocess.STDOUT)\n\tignored_files = [ a.decode(\"utf-8\") for a in out.decode(\"utf-8\").split(\"\\n\") ]\n\tignored_files = sorted(os.listdir(\"ignored_files\"))\n\nexcept:\n\tignored_files = []\n\t\nignored_files_lines = list( ignored_files )\nwhile 1:\n\ttry:\n\t\tif \"HEAD -> %s\""}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tcmd = 'echo \"ignored_filenames\" && git rev-list --all -r 1'\n\tstdout = subprocess.check_output( cmd, stderr=subprocess.STDOUT )\n\tignored_filenames = stdout.strip(\"  \\r\\n\").split(\"\\n\")\n\tsorted_ignored_filenames = []\n\tfor i in ignored_filenames:\n\t\tsorted_ignored_filenames.append(i.split(\" \")[1].split(\".\")[0]) \n\treturn sorted_ignored_filenames\n\"\"\"\n\tTo get the full list of files to be removed, replace the entire directory"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tignoreList = '*.html,*.htm,*.shtml,*.asp,*.aspx,*.py,*.jsm'\n\tcmdList = subprocess.list2cmdline( ['git'] )\n\tcmdListStr = ','.join(cmdList)\n\treturn re.sub(ignoreList + \"'\", '', cmdListStr).split(\",\")\n\n\ndef get_sub_projects_from_projectname(projectname):\n\t\"\"\" Find all subprojects of projectname\n\n\t\tArgs:\n\t\t\tprojectname: The name of the project to retrieve subprojects from\n\n\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\ndef gitListFilesInDirectory(dirPath):\n\t\"Find files contained in a directory. A directory needs to be a git repository if you want to ignore any file by filename.\"\n\targs = []\n\tgitArgs = \"log -f {}\".format(dirPath)\n\tgitOutput = subprocess.Popen(gitArgs, stdout=subprocess.PIPE,stderr=subprocess.PIPE, shell=True)\n\tworkingDirectory = [line.rstrip() for line in gitOutput.stdout.readlines()] + [x for x in gitOutput.stderr.readlines()]\n\toutput ="}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t# Ignore git specific files like.gitignore\n\tos.startfile(r'git.info')\n\ttry:\n\t\treturn subprocess.check_output([\"git\", \"info\", \"--all\", \"--ignored\"], stderr=subprocess.STDOUT, universal_newlines=True)\n\texcept:\n\t\treturn None\n\ndef git_ignore_filename_list(lines):\n\tlines = lines.splitlines()\n\n\tfor i, line in enumerate(lines):\n\t\tignore_list = line.strip().split(',')\n\t\tignore_list = filter("}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tcmd_str2 = \"\"\n\tfor cmd in subprocess.list2cmdline(\"git rev-list -f -C -2\"):\n\t\tcmd_str =  \"{} : {}\\n\".format(cmd.strip(),cmd.strip())\n\t\tcmd_str2+=cmd_str\n\t\tcmd_str = cmd.strip()\n\ta = \" \".join(cmd_str2)\n\ta=a.split(\",\")\n\ta.sort()\n\tfor n in a:\n\t\tif(n.index(\"-\")<0&&n.strip()+\"\"):\n\t\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tcmd = 'git ls-tree -l'\n\t# print(\"cmd = \", cmd)\n\tfor line in cmd.split(','):\n\t\tif line == '': break\n\t\tfilePaths = line.split(',')\n\t\tfilenames = [i.strip() for i in filePaths]\n\t\tfilenames = sorted(filenames)\n\t\tprint(filenames)\n#\t\tfor i in filenames:\n#\t\t\tprint(i)\n\t\t\n\"\"\"\n\tUse python to remove any ignored files.\n\t\"\"\"\n\t#\n\tcmd = 'python pepper"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tpath = ['git']\n\toutput = ''\n\tfor dir_path, dirnames, filenames in os.walk(os.getcwd()):\n\t\tfor filename in filenames:\n\t\t\tpath.append(os.path.relpath(dir_path/''.join([filename]), dir_path))\n\tfor i in range(len(path) - 1):\n\t\tif os.path.basename(path[i + 1]) not in ['.', '..']:\n\t\t\toutput += path[i + 1] + ','\n\t\telse:\n\t\t\tif output!= '':\n\t\t\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tpossibleIgnoredFiles = [f for f in os.listdir(\".\" + os.path.expanduser('~') or os.getcwd() + \"/.git\") if os.path.isdir(f)]\n\tignoredFiles = []\n\ttemp_ignored_file = \"  \\\\\"\n\tfor f in possibleIgnoredFiles:\n\t\tf_temp = tempfile.NamedTemporaryFile(suffix='.txt', dir=f)\n\t\tres = subprocess.call([sys.executable, '-c', \"git reconvert --progress 2>&1 \", f_temp.name])\n\t\t\n"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tcmd = [\"git\", \"diff\", \"-U\", \"-i\", \"--\", \"--\"]\n\tcmd += [\"--\"]\n\tcmd += [\"-l\", \".\"]\n\tcmd = cmd + filename_list\n\tout, err = cmd.read_text()\n\tif err!= None:\n\t\tlog.exception(\"git diff\")\n\treturn out\n\n\n\ndef get_ignore_list(ignore_cmd):\n\tlog = sys.stdout\n\n\tcmd = ignore_cmd.split()\n\tout = []\n\tfor i in range(len(cmd)):\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd76", "prompt": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tsuffix = os.path.splitext(filename)[1]\n\tif suffix in type_suffix:\n\t\treturn type_suffix[suffix]\n\telse:\n\t\treturn False\n\n\n\"\"\"\n\tThe following code is based on the code in\n\thttp://pyworld.bintray.com/public/index.html. Re-use this file? Save it as\n\tstatic file and keep the old file on the command line.\n\"\"\"\n\n<?php\n\n/**\n * @group Core\n */\nclass ConfigCoreTest extends TestCase {\n\tpublic const CONFIG_"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\treturn os.path.splitext(filename)[0] in (\".exe\",\".dll\")\n\n\ndef get_image_info(filename):\n    \"\"\"\n\tFunction used to read the image information extracted by MindMeld.py\n\n\tArgs:\n\t    filename: The binary file that was processed to extract images\n\tReturns:\n\t    Image header information\n\t\"\"\"\n    try:\n        exe_path = os.path.dirname(filename)\n\t\t_cmd = \"get_var(%s)\" %("}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tif \"type\" not in globals(): globals()['type'] = {}\n\tif filename in globals(): globals()['type'][filename] and \\\n\tisinstance(globals()['type'][filename], bool): return globals()['type'][filename]\n\treturn False\n\n#check if file extension can be run through the type checking module\ndef is_extension_supported(filename, extension, filetype):\n\t\"\"\"\n\tReturn whether a file extension is supported in this module\n\t\n\tArgs:\n\t    filename: Filename to match\n\t\"\"\"\n\tfext = os.path.split"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\treturn type(filename).startswith('.' + os.path.splitext(filename)[0])\n\ndef is_windows():\n    return os.name == 'nt'\n\ndef is_osx():\n    return os.name == 'darwin' or os.name == 'linux'\n\nmodule Fog\n  module Compute\n    class AWS\n      class Real\n        # Create a DNS zone with specified DNS records\n        #\n        # ==== Parameters"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\treturn os.path.splitext(filename)[1] in FILETYPE_MAP\n\n\ndef IsTypeSupported(extension, file_type=None):\n\t\"\"\" Is the requested file type supported by this module? \"\"\"\n\textension = extension.upper()[:-1]\n\tif extension in FILETYPE_MAP:\n\t\treturn True\n\tif file_type:\n\t\tif extension == \"json\" and file_type in JSON_TYPES:\n\t\t\treturn True\n\treturn False\n\npackage com.android.server.rms.iaware.memory.filter;\n\nimport"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\treturn (\n\t\t(\n\t\t\tget_provider_type(filename) in [\n\t\t\t\t\"google\", \"google_drive\",\n\t\t\t]\n\t\t)\n\t\tor (\n\t\t\tos.path.isfile(\n\t\t\t\tos.path.join(\n\t\t\t\t\tos.path.dirname(os.path.abspath(__file__)),\n\t\t\t\t\t\"providers\",\n\t\t\t\t\tfilename\n\t\t\t\t)\n\t\t\t)\n\t\t)\n\t)\n# This function is run on startup.  It returns either an\n# id or an error message. "}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tif not type(filename).__module__.startswith('PyOpenGL'):\n\t\treturn False\n\treturn True\n\ndef _isSupported(filename, isGL):\n\t#print 'isSupported', filename, isGL\n\ttry:\t\n\t\tgl = ctypes.CDLL(filename)\n\t\tglGenTextures = gl.glGetGenTextures(1)\n\t\tglEnable = gl.glGetBooleanv(gl.GL_ENABLE)\n\t\tglDisable = gl.glGetBooleanv(gl.GL_DISABLE)\n\t\tglIsEnabled = gl.glGetBooleanv(gl.GL"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tif not os.path.splitext(filename)[-1].lower() in ['.xml', '.txt']:\n\t\treturn False\n\treturn True\n\n/*\n   For more information, please see: http://software.sci.utah.edu\n\n   The MIT License\n\n   Copyright (c) 2020 Scientific Computing and Imaging Institute,\n   University of Utah.\n\n   Permission is hereby granted, free of charge, to any person obtaining a\n   copy of this software and associated documentation files (the \"Software\"),\n   to deal in"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tdef matchType(name):\n\t\treturn os.path.isfile(name) and name[-4:] in supportedTypesMap\n\n\t\t\n\t# Match the file name\n\tmatch = re.search(r'filename =\\s*\"(.*?)\"\\s*', filename)\n\n\tif match:\n\t\treturn match[1:-1]\n\n\t# Any value that is not an \"filename:\" prefix\n\tmatch = re.search(r'filename =.+?', filename, re.I)\n\n\tif match:\n\t\treturn match.group(1)\n"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\treturn __import__(abs(filename)[-len(os.extsep+'exe')])\n\n//#############################################################################\n//#                                                                           #\n//#  Copyright (C) <2014>  <IMS MAXIMS>             "}]}
{"task_id": "62ece4982e6aefcf4aabbd77", "prompt": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\ttry:\n\t\treturn datetime.__unit_ratio__().convert(freq, unit)\n\texcept BadParam:\n\t\treturn None\n\ndef string_to_freq(text):\n\t\"\"\"\n\t\tConvert a string to a frequency\n\t\t\n\t\tParameters:\n\t\t\n\t\ttext - string or frequency in given unit\n\t\t\n\t\tReturns:\n\t\t\n\t\tfrequency\n\t\t\n\t\tRaises ValueError if text is not a frequency or unit string.\n\t\t\n\t\t\"\"\"\n\ttry:\n\t\tif text[0] == \"s\":\n\t\t\treturn S\n\t\telif"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\ttry:\n\t\tfrequency =  frequency.lower().strip()\n\texcept:\n\t\traise ValueError(\"frequency must be a string or None.\")\n\tfreq = [x.strip() for x in frequency.split(\",\")]\n\ttry:\n\t\tunit =  unit.lower().strip()\n\texcept:\n\t\traise ValueError(\"unit must be a string.\")\n\tunit = [x.strip() for x in unit.split(\"-\")][-1:]\n\treturn timedelta(int(freq[0]), int(freq[1]), int(freq[2])) if len"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t# Validate frequency\n\tif frequency is None or frequency not in (\"always\", \"hourly\", \"daily\"):\n\t\t# error\n\t\treturn None\n\t\t\n\t# Extract unit specifier\n\tparsed = re.split('[\\d]', frequency)[0]\n\t\n\t# Parse\n\tfunit = \"\"\n\tif frequency not in (\"hourly\", \"daily\") and frequency!= \"always\":\n\t\tif parsed not in list(Frequency.VALID):\n\t\t\tprint(\"Invalid frequency \", parsed)\n\t\t\treturn None\n\t\t\n\t\tfunit = parsed\n\t\t\n\tunitdict"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tif frequency is None or frequency == \"\":\n\t\tfrequency = \"always\"\n\tif unit is None:\n\t\tunit = \"hours\"\n\tresult = _parse(frequency, unit)\n\tif type(result) == int:\n\t\treturn result\n\tif result <= 0:\n\t\traise ValueError(\"Frequency must be a positive integer.\")\n\t\n\ndef now(timeout=0.001):\n\t\"\"\"Return a number of seconds since the epoch\n\t\n\tThe function return the number of seconds since the Unix epoch of 00:00:00 UTC on January 1st 2013."}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\ttry:\n\t\tlunit = parseUnit(frequency)\n\texcept ValueError:\n\t\traise ValueError\n\tif lunit is not None and lunit not in (\"weeks\", \"hour\", \"minute\"):\n\t\t\traise ValueError\n\t\tfreqStr = convertUnit(frequency, lunit)\n\telif not frequency:\n\t\traise ValueError\n\treturn None if not lunit or lunit in \"weeks\" or lunit in \"hour\" or lunit in \"minute\":\n\t\tfreqStr = frequency[0:24]\n\ttry:\n\t\tdelta = datetime"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t# This function uses strptime which has been inspired on the Python C api\n\tfrom  math import pi / 180\n\t# this is a Python datetime.timedelta\n\ttry:\n\t\tfrom math import pi as pi_py\n\texcept ImportError:\n\t\traise ValueError(\"please install pi with: https://pypi.python.org/pypi/pi/\")\n\t\n\ttime_match = ( re.compile(r\"\\b%s\\b\" % r\"^\\d\\d\\d\\d\\d\"\n\t\tr\"%s\\b\" % pi"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tfrom math import floor, lerp, pow, ceil, modf\n\ttry:\n\t\treturn (\n\t\t\t\t\t\tfloat(freq)/pow(2, 6),\n\t\t\t\t\t\tint(floor(modf(\n\t\t\t\t\t\t\t\t\t(pow(2, 6)+freq)/3)))\n\t\t\t\t\t)\n\texcept (ValueError, OverflowError):\n\t\treturn None\n\ndef _validate_frequency(freq):\n\t# Valid frequency values:\n\t#\n\t#  0           : 1,\n\t#  1, 3, and 6 : 5"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tif freq is None or \"always\" not in units_dict[freq]:\n\t\tif freq is None:\n\t\t\t\traise ValueError(\"the input frequency is None (%s)\");\n\t\t\telse:\n\t\t\t\t\traise ValueError(\"the input frequency '%s' cannot be parsed\" % freq)\n\t\t\t\traise ValueError((\"the input frequency '%s' is for \"\n\t\t\t\t\"always frequency, you should use \"\n\t\t\t\t\"the input_format '%s', \"\n\t\t\t\t\"or define your own unit format\")) % freq\n\n\tif freq in units_dict:\n\t\treturn _"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tif freq is None or 'always' in freq:\n\t\treturn datetime.timedelta(0)\n\t\t\n\tstart_hour, start_minute, start_second = int(freq[0:3])\n\t\n\tstart_day_part = start_hour//10 + ((start_hour%10) // 2)\n\tstart_year_part = start_year_digits_to_digit(start_day_part)\n\t\n\tif start_second:\n\t\tstart_hour, start_minute = start_hour%10, start_minute%10\n\t\tstart"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tif frequency is None or frequency == \"always\":\n\t\t# raise ValueError if the given frequency cannot be parsed.\n\t\treturn None\n\tfrom dateutil import parser, timezone\n\t\n\tresult = parser.parse(frequency, timezone.as_local(timezone.utc))\n\t# print(dateutil.parser.format(result['value'],result['weekday']), result['unit'])\n\treturn result['value']\n\n\ndef as_timedelta(frequency):\n\treturn as_timedeltamod(str(frequency),'second')\n\n\ndef as_timedeltamod(frequency"}]}
{"task_id": "62ece4992e6aefcf4aabbd78", "prompt": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tif socket.gethostbyname(host) == socket.gethostname() \\\n\t  and b'localhost' in host:\n\t\treturn True\n\telse:\n\t\treturn False\n\n<?php\n\nnamespace AvoRed\\Framework\\Database\\Controllers;\n\nuse AvoRed\\Framework\\Database\\Contracts\\DataCleaningInterface;\nuse Illuminate\\Support\\Collection;\nuse Illuminate\\Support\\Facades\\Facade;\n\nclass DataCleaner\n{\n    /**\n     * Handle a request for data cleaning.\n     * @return"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import select\n\ttry:\n\t\timport socket\n\texcept ImportError:\n\t\treturn False\n\telse:\n\t\ts = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n\t\ts.connect(('127.0.0.1',80)) \n\t\tdata = s.recv(2048)\n\t\ts.close()\n\t\treturn data.find(\"127.0.0.1\")!= -1\n\n# End of sfp_isLocalhost\n\n# Function: sfp_resolveURL"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tif not isinstance(host, basestring):\n\t\traise TypeError(f\"{host!r} is probably not a hostname.\")\n\n\tglobal _get_hostname\n\ttry:\n\t\tres = _get_hostname()\n\t\tif res == host:\n\t\t\tpass\n\t\telse:\n\t\t\traise socket.error(f\"The host's IP address ({host!r}) does not match.\")\n\texcept socket.error as e:\n\t\tprint(e)\n\t\tif 'EAI_AGAIN' in str(e):\n\t\t\treturn True  #"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\thost = socket.gethostbyname(host)\n\treturn host in (\"localhost\", \"127.0.0.1\", \"::1\")\n\ndef get_system_version():\n\t#https://stackoverflow.com/questions/34191870/how-to-get-the-system-version-of-an-existing-windows-program-using-python\n\t__version__ = \"\"\n\tret = win32service.QueryServiceConfig(\n\t\twin32service.SERVICE_CONFIG_DESCRIPTION.ConfigType,\n\t\twin32service.SERVICE_CONFIG_DESCRIPTION.Service"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\ttry:\n\t\tip; mask = socket.inet_aton(host.split('.')[-1].split(']:'))\n\texcept:\n\t\tip = ip_address(host)\n\t\tmask = ip.netmask\n\t\tif ip.packed()!= mask.packed():\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\n// Copyright (c) Microsoft Corporation.\n// Licensed under the MIT license.\n\nimport { traceInfo, TraceInfo } from \"../models\";\n\nexport default traceInfo;\n\n/*\n * Licensed to the Apache Software"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t# Check if a hostname is available or not\n\thostname, port = socket.getfqdn(hostname or 'localhost') \n\treturn hostname == 'localhost' or port == int(port)\n\ndef is_localhost(hostname):\n\t\"\"\"\n\tChecks if a hostname corresponds to localhost\n\n\tArgument: [hostname]\n\tReturns: boolean true if the hostname is localhost\n\t\"\"\"\n\t\n\t# Check if a hostname is available or not\n\thostname, port = socket.getfqdn(hostname or 'localhost') \n\treturn hostname == 'localhost' or socket.gethostname"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tdef check_localhost(host):\n\t\tif isinstance(host, str) and host == 'localhost':\n\t\t\treturn True\n\t\t# for IPv6, we need to check if ipv6 is present\n\t\t# on the local IP address\n\t\tip_address = socket._inet_aton('::')\n\t\tif ip_address[2][0] == 0 and socket.inet_aton(host)[2]:\n\t\t\tip_address = socket.inet_aton(host)\n\t\t\tif ip_address[2][0] == 0:\n"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\timport platform\n\timport socket\n\tif platform.system() == \"Windows\":\n\t\ttry:\n\t\t\tsocket.inet_aton(socket.gethostbyname(host))\n\t\texcept socket.error:\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\telif platform.system() == \"Linux\":\n\t\tpass\n\telse:\n\t\tprint(\"ERROR: Failed to resolve the host\")\n\t\tprint(\"The hostname could not be resolved\")\n\t\tprint(\"Type \\\"go help hostname\\\" for a list of supported commands\")\n\t\t"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tif ':' in host:\n\t\thostparts = host.split(':')\n\t\tif socket.getfqdn(hostparts[0])!= socket.getfqdn(hostparts[1]):\n\t\t\treturn False;\n\treturn True\n\ndef check_if_local_ipv4(ip_address):\n\t\"\"\"\n\tChecks if the host is in the IPv4 address range\n\t:param ip_address: The ip address in the net\n\t:return: True if the host is in the IPv4 address range\n\t\"\"\"\t\n\tif socket.getfqdn(ip"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\timport socket, sys, struct, random\n\thostname = socket.gethostname()\n\tif hostname in ['0.0.0.0']: return True\n\tlocal_address = socket.gethostbyname(hostname)\n\tif local_address == socket.getnameinfo(hostname)[0]: return True\n\treturn False\n\n\n\n\n\ndef is_ipv4(domain):\n\t# Returns a boolean if the given domain is ipv4 or ipv6\n\tif isinstance(domain, six.string_types):\n\t\ttry:\n\t\t\thost = socket.getaddrinfo("}]}
{"task_id": "62ece4992e6aefcf4aabbd79", "prompt": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tpattern_regexes = [re.compile(\"^(.*)$\"), re.compile(\"^([^.]*)$\")]\n\t\n\tdef find_paths(self, parts):\n\t\t# type: ( Sequence[ str ], Sequence[ str ], float ) -> List[str]\n\t\tres = [] # type: List[str]\n\n\t\tglob_pattern = self.options.find_options.pattern\n\n\t\tfound_paths = set() # type: Set[ str ]\n\t\tpattern_regexes = []\n\t\tfor path_fragment in parts:\n\t\t\tmatched ="}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tfound = set() # path set\n\tfor pat in glob_patterns:\n\t\tfound.add((pat,)) # pat -> paths to remove\n\n\t# now transform the paths with remove_if\n\tfor pat,paths in found:\n\t\tre_pat = re.compile(pat)\n\t\tout = []\n\t\twith open(overriding_paths, 'w') as f:\n\t\t\tfor p in paths:\n\t\t\t\tf.write(p + '\\n')\n\t\t\treturn out\n\t\traise FileNotFoundError(\"'--find' transform failed:"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t# TODO: It would be nice if the pattern was able to parse the \"patterns\" argument\n\t# to Python, and we would prefer this pattern over paths (or even be a list).\n\t# We should fix the patterns so it can run properly.\n\t# http://stackoverflow.com/questions/10758872/django-rumprun-pattern-not-recursing\n\n\tdef __init__(self, find_paths):\n\t\t\"\"\"\n\t\tInitializes an instance of find_paths with patterns to match\n\n\t\t >>> from find_paths import * # doctest"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t# TODO\n\tdef find_paths(self, path, pattern):\n\t\tself.path = re.compile(pattern, re.DOTALL).findall(path)\n\t\tif self.path:\n\t\t\tfor i in range(len(self.path)-1, -1, -1):\n\t\t\t\t# Strip trailing slashes from the last path element\n\t\t\t\tself.path[i] = self.path[i].lstrip('/')\n\t\n\t\n\t@staticmethod\n\tdef find_files(paths):\n\t\t\"\"\"\n\t\tIterate over all sub-paths"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t# Create a new, empty list, for the sequence.\n\tin_list = list()\n\t\n\t# Loop through each of the sequences.\n\t# Pass thru from the front to back\n\twhile len(in_list) > 0:\n\t\t# Get the element from the last location in the list\n\t\tstart_idx = in_list[len(in_list) - 1]\n\t\t# Is it a path fragment? If so, add it to the output file.\n\t\tif re.match(r'^(\\w+:?/\\*.*?\\*\\/"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\treturn_val = False;\n\tpatterns = [s.strip() for s in args if s and s[0]!= '/']\n\t#for w in os.walk(path):\n\t#\tfor i in w if i.startswith(PatternsDir):\n\t#\t\tfor n in w.iterdir():\n\t#\t\t\tif not n.endswith('.py'):\n\t#\t\t\t\t# We want to find paths for 'patterns[]' that are not in\n\t#\t\t\t\t# 'PathGlobs'\n\t#\t\t\t\tval = re.findall(PatternsDir"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t#TODO: Add check for illegal name\n\t\n\tif len(seq) == 0:\n\t\t# Return nil\n\t\treturn []\n\t\t\n\tfound = []\n\tpathMap = {}\n\tfor s in seq:\n\t\ts=re.escape(s)\n\t\ttry:\n\t\t\tr=re.search(r\"\\w*\\.\\w*\",s)\n\t\t\t# remove parent dir as we're already trying to split that path out\n\t\t\tbase,p=r.groups()\n\t\t\tp = base+\".\"+p\n\t\t\tif pathMap.has"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t# TODO check this against glob patterns and try to return list or tuple\n\tif isinstance(pattern, list):\n\t\tpattern = pattern[0]\n\t\t\n\tpats = [re.split(pattern, os.path.sep)]\n\t\n\tnew_patterns = [\n\t\ttuple(split) + (re.split(pattern, os.path.sep) if len(pattern) > 0 else ([], ''))\n\t\tfor split in transforms.find_paths(os.path.sep.join(file_list) + wildcard + line + path)\n\t]\n\t\n"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\ttarget_files = set(path.split('/')[:-1] for (path, pat) in find_paths)\n\t#\n\t# 1. Remove any path prefix that might exist after last segment\n\t#    (e.g. sh: foo.txt, or the following sh:):\n\t#\n\t# [sh] is a special case. It has no suffix. It makes sure that\n\t# all path components with the same suffix also get removed.\n\t#\n\tfor f in list(target_files):\n\t\tf_path = path.split(f"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t#\t\tprint \"pattern=\", [t for t in patlist if re.search('\\.', patlist)]\n\t\tpatlist=[]\n\tn=len(patternlist.pattern_list)\n\tfor i in range(n):\n\t\ttry:\n\t\t\tpatlist.append(patternlist.pattern_list[i])\n\t\texcept IndexError:\n\t\t\tpass\n\t\t\n\t#\t\tprint patlist\n\t\t\n\t#\t\tprint i, pat\n\n\t\ttry:\n\t\t\tpath=os.getcwd()\n\t\t\tos.chdir(\"/\")\n\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd7a", "prompt": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t#print \"git is on windows %s\" % platform.system()\n\t#print \"git is using native Ghostscript\" # Ghostscript is NOT part of python\n\tif os.name!= \"nt\":\n\t\treturn True\t\n\t\nexcept ImportError:\n\treturn False\n\t\n'''\n\treturns True if running under Windows or macosX\n'''\t\t\n\n### \n# returns if a.git bash is installed using git installed\ndef IsGitBashInstalled(platform=\"windows\"):\n\tif os.name==\"nt\":\n\t\treturn IsGitBashInstalled"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tresult = subprocess.run(['git','rev-parse', '--git-dir'],shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n\treturn result.stdout.find(\"gitsh\") >= 0\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\npackage acousticfield3d.gui.math;\n\nimport java.awt.Font;\nimport java.awt.event.ActionEvent;\n\nimport javax.swing.JButton;\n\nimport acousticfield3d.gui.Main;\nimport acousticfield3d.math.M"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tos.system('git-config --global core.windows.allowInstallingNewLocalProjects=true')\n\tassert \"git-config -r core.windows.allowInstallingNewLocalProjects\" in \\\n\t\t\t\tresult.output, \"git-config did not allow it to run\"\n\"\"\"\n\t\ndef has_git_cmd(args):\n\treturn os.path.splitext(os.path.basename(_find_git_file_using_command_line(args.split(\" \", 1)[0])))[0] == \"cmd\"\n\ndef main():\n\t\"\"\""}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tif os.name == 'nt':\n\t\treturn True\n\t\n\treturn False\t\n\t\n\n/*\n * Copyright (c) 1999, 2012, Oracle and/or its affiliates. All rights reserved.\n * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n *\n * This code is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License version 2 only, as\n * published by the Free Software Foundation.\n *\n * This code is distributed in the hope that it will be useful, but WITHOUT\n * ANY WARRANTY; without even the"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tfrom subprocess import check_output,run\n\tcommand=\"git bash -s 'c:\\windows\\git.exe\"\n\tos.system(command)\n\tif(output=run(command, shell=True, stdout=PIPE, stderr=PIPE))[0]==None:\n\t\treturn False\n\treturn(run(command, shell=True, stdout=PIPE, stderr=stdout))[0]==b\"git bash\"\n\npackage org.geekbang.time.commonmistakes.commonmistakes.webcommon;\n\nimport org.springframework.boot.SpringApplication"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tgitlog = open(os.path.join(os.path.split(os.path.realpath(__file__))[0],\"git.log\"),\"r+\")\n\tret = gitlog.read()\n\tgitlog.close()\n\tprint(\"gitlog in platform\")\n\treturn((\"GIT_SHA=b29df08e1eca58d9a3b58ea7a79d5d56bd\" in ret, True, \"git was in Windows by Hue-Chuan\" ))\n\ndef git_is_gitgit_linux():\n\tgitlog"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tos.system('git diff --name-only --diff-filter=ALL --staged -x \"%s\" %s'.format(git_version, os.getcwd()))\n\n\n\"\"\"\n\tprint 'git version is %s' % git_version\n\tprint 'git version is %s, git version is %s' % (git_version, git_versions_string)\n\tprint 'git version is %s, git version is  %s, git version is %s' % (git_version, git_versions_string, git_versions_string)\n\n\n\"\"\""}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\treturn os.getenv(\"GIT_SSH_COMMAND\")!= \"\"\n\n\n\ndef get_ssh_user():\n\t\"\"\"\n\treturns the username of GIT_SSH_USER\n\t\n\t:return: the username of GIT_SSH_USER\n\t\n\t\"\"\"\n\treturn os.getenv(\"GIT_SSH_USER\")\n\n\ndef get_ssh_password():\n\t\"\"\"\n\treturns the password of GIT_SSH_PASS\n\t\n\t:return: the password of GIT_SSH_PASS\n\t\n\t\"\"\"\n\treturn os.getenv(\"GIT_SSH"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t#get git config:\n\tgit_config='-c git config'\n\tgit_config={}\n\tgit_cmd=os.system(git_config)\n\t#get git info:\n\t#git_info=os.system()\n\t#get git rev\n\t#git_rev=os.system()\n\t#get git path\n\tgit_path=os.path[os.path.index(os.path.pardir,\"%1\")+1:-2]\n\t#get git version\n\tgit_version=os.system(\"git describe %1\")"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tif self.has_gitbash:\n\t\tshutil.command.Subprocess(self._gitbash_command).wait()\n\t\t\n\t\n\t\t'''\n\t\t@self.has_git_file\n\t\t@self.has_git_directory\n\t\t@shutil.command.Subprocess(self._git_command).wait()\n\t\tif self.has_git_file:\n\t\t\tself.has_git_file = False\n\t\telif self.has_git_directory:\n\t\t\tself.has_git_directory = False\n\t'''\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd7b", "prompt": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tif os.path.exists(target_config_filename):\n\t\tos.rename(target_config_filename, target_config_filename + '_old')\n\tif os.path.exists(target_config_rendered_yaml):\n\t\tos.rename(target_config_rendered_yaml, source_config_filename)\n\ndef write_target_config_to_old_files(target_config_filename, source_configs_files):\n\t\"\"\"\n\tGiven an target config filename, write it out to a list of config files. It is either\n\ta sequence"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\ttry:\n\t\tif os.path.isfile(\"%s/target.json\"%(target_config_file)):\n\t\t\t\n\t\t\tprint(\"Creating target.json file as a target...\")\n\t\t\tif os.path.isdir(\"%s/target.json\"%(target_config_file)):\n\t\t\t\tos.rmdir(\"%s/target.json\"%(target_config_file))\n\t\t\tfout = open(\"%s/target.json\"%(target_config_file), 'w')\n\t\t\tfout.write(str(render(target_cfg)))\n\t\t\tfout."}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\toutput_filename_template = '%s-final.conf'\n\toutput_filename = os.path.join(output_path, output_filename_template % target_config_filename)\n\n\tif not os.path.exists(output_filename):\n\t\tos.makedirs(output_filename)\n\n\twith open(output_filename, 'w') as conf_file:\n\t\t# Print our modified config:\n\t\tif target_config_yaml!= \"\":\n\t\t\tconf_file.write(target_config_yaml)\n\t\tif rendered"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tif not os.path.exists(filename):\n\t\tos.makedirs(filename)\n\n\toutFile = open(filename, \"w\")\n\n\tyamlStr = yaml.dump(config)\n\n\toutFile.write(yamlStr)\n\toutFile.close()\n\n\tos.chmod(\n\t\tfilename,\n\t\tstat.S_IREAD | stat.S_IWRITE | stat.S_IEXEC)\n\n/*!\n * VisualEditor DataModel MWParserNode class.\n *\n * @copyright 2011-2020 VisualEditor Team and others"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tif not os.path.exists(target_filename) :\n\t\tos.makedirs(target_filename)\n\t\tif not os.path.exists(target_subdirec) : \n\t\t\tos.makedirs(target_subdirec)\n\t\tif not os.path.exists(target_fullfilenode) : \n\t\t\tos.makedirs(target_fullfilenode)\n\t\tif not os.path.exists(target_file) : \n\t\t\tos.makedirs(target_file)\n\t# Save the config YAML"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\ttarget = cfg.path(\"cfg:target\")\n\tif target.exists():\n\t\tcfg.log(\"Aborting write configuration\", level=1, silent=True)\n\t\traise NameError(\"Invalid config path, target file exists\")\n\tif not cfg.path.isfile(target):\n\t\tcfg.log(\"Aborting write configuration\", level=1, silent=True)\n\t\traise NameError(\"Invalid config path, target file is not a file\")\n\n\tif not os.path.exists(target):\n\t\tos.makedirs(target)\n"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t_file_name = os.path.splitext(os.path.basename(target_config_yaml))[0]\n\t_file_path = os.path.join(destination_directory, os.path.basename(_file_name))\n\tif not os.path.isfile(_file_path):\n\t\tos.makedirs(_file_path)\n\t\twith open(_file_path, 'w') as file:\n\t\t\t# Write the rendered YAML to the file\n\t\t\tfile.write(rendered_config)\n\t\t# If the file already exists"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tprint(\"Writing config file.\")\n\tos.system(\"python \"+os.path.basename(os.path.dirname(__file__)))\n\ttarget_dir = os.getcwd()+\"/config/\"\n\tnew_file_name = os.path.join(target_dir, file_name)\n\tif os.path.exists(new_file_name):\n\t\tprint(\"Filename already exists.\")\n\t\treturn\n\tif os.path.isdir(target_dir):\n\t\tfor config_name in sorted(configs_dict.keys()):\n\t\t\tprint(\"Trying"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tif not os.path.isfile(config_file):\n\t\tif not os.path.isdir(target_directory):\n\t\t\tos.makedirs(target_directory)\n\t\tos.chdir(target_directory)\n\t\ttry:\n\t\t\twith open(config_file, \"w\") as config_file_write:\n\t\t\t\tconfig.update(yml.dump(render_config(config, yaml_safe=False)))\n\t\texcept:\n\t\t\terror_handler(\"config file\", config, yaml_safe=False)\n\t\tos"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\troot = os.path.split(os.path.abspath(__file__))[0]\n\tif os.path.exists(root + '/config.yml') and not overwrite:\n\t\treturn\n\tnew_config = open(os.path.join('config.yml'), 'r').read()\n\tos.makedirs(root, exist_ok=True)\n\twith open(os.path.join(root, 'config.yml'), 'w') as out_file:\n\t\tyml = open(new_config, 'r')\n\t\tyml.write"}]}
{"task_id": "62ece4992e6aefcf4aabbd7c", "prompt": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\tif ':' in line:\n\t\t\t# This works for Python 2 and Python 3.\n\t\t\tscript = \"%s\\n\"%script\n\t\tt = shlex.split(script)\n\t\tif ':' in t[1]:\n\t\t\tt[0] = '%s\\n'%t[0]\n\t\telse:\n\t\t\tt[0] = ''\n\t\treturn '\\n'.join(t)\n\n\ndef run_ssh(p,args):\n\n\t# Convert ssh command and script to one line command\n\tif p == 1:\n\t"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tout = b\"/bin/bash -c 'echo %s\\n' > /tmp/somefile\\n\" % script\n\t\"\"\"\n\t(b) use a shell for execute\n\t(b) create two commands to execute\n\t(b) get stdout\n\t\"\"\"\n\n\tret = subprocess.Popen(\" \".join(out), stdout=subprocess.PIPE,\n\t\t\t\t\t\t\t\t #stdin=subprocess.DEVNULL, stderr=subprocess.PIPE)\n\t# ret = subprocess.Popen(\"cat /tmp/somefile\", stdout=subprocess.PIPE,shell=False)\n\tif ret"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tscript = textwrap.wrap(script)\n\tcmd = \"\"\"python -c 'import shlex;' \\\n\t\tcommand='' '$@'\"\"\"\n\tcommand = script.replace(\"$@\", cmd)\n\tcommand = command.strip()\n\treturn command\n\nclass SSHServer(object):\n\t\"\"\"\n\tMain class for the ssh server. It takes as input a file containing the script and the\n\tscript will be executed and outputs the generated file by script name: ssh_script_name.py\n\t\"\"\"\n\tdef __init__(self, log: bool = True) ->"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\toutput = ''\n\tfor token in tokenize(script):\n\t\tif token[0] == ';':\n\t\t\tnewline = token[1].strip('\\n')\n\t\t\tif newline.endswith('! '):\n\t\t\t\tnewline = newline[:-len('! ')]\n\t\t\telif not newline.endswith(' '):\n\t\t\t\tnewline = newline +''\n\t\t\toutput += ''\n\t\t\toutput += token +''\n\t\t\toutput +='' + newline\n\treturn\n\ndef main():\n\t\"\"\"\n\tThis script converts a script to one line command and runs it"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t# Run a raw command\n\tout = script()\n\t# Append a line separator\n\tout = out.rreplace(b\"\\n\", b\"\\r\\n\")\n\tresult = out.replace(b\" \", b\"\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r\")\n\treturn result\n\n\ndef run_ssh_commands(script):\n\t\"\"\"\n\t:type script: String\n\t:param script:\n\t"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t# convert s to lower\n\tscript = textwrap.dedent(script).lower()\n\tcmd = [script, script]\n\tcmd = ''.join(cmd)\n\tlines = cmds_to_lines(cmd)\n\n\t# now add each line to commands\n\tfor line in lines:\n\n\t\t# add each line to the result.\n\t\treturn line.strip() + '\\n'\n\n    return \"\\n\".join(lines)\n\n    \ndef _get_cmd_to_run(cmd:str):\n\n\t"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t# if no command is given return with help\n\tif not cmd:\n\t\tprint(\"\"\"\nUsage\n---\n\nNote: This script can only be run on the same machine\n\nUse :\n    ssh -i <master> [-p <public-key>] [-c <host-key>] \\\n-f <full_fame> \\\n-t <path> \\\n-x <file> \\\n-a <alias> \\\n\nExample\n$ ssh -i /home/me/user1 -u user2 -k /home/user1/key."}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tif len(script) == 0:\n\t\treturn \"\"\n\treturn get_script_command(script)\n\nclass GetScriptParameters(object):\n\tdef __init__(self):\n\t\tself.host = None\n\t\tself.port = None\n\t\tself.username = None\n\t\tself.password = None\n\t\tself.script = None\n\t\tself.publickey = None\n\t\tself.publickeytype = None\n\t\tself.keysfile = None\n\t\tself.verbose = options.verbose\n\t\tself.no-host-"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t#print(textwrap.indent('{}',2))\n\t#print(script.replace(\"\\n\",\" \").replace(\"\\t\",\"\\t\\t\"))\n\t#\n\t#print(textwrap.indent(\"cmd1() cmd2()...\",2,indent=True))\n\t#print(textwrap.indent(\"\\nx\",indent=True))\n\t\t\n\tif(len(script)==0):\n\t\tscript = \" \".join(getline(\" \".join(command)).split())\n\t\t#textwrap.indent(command)\n\t#\n\tcommand = cmd + \" \" +"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tif len(script) <= 0: return \"No scripts found!\"\n\treturn script[1]\n\n#------------------\n\ndef _split_list(o, delim):\n\t\"\"\" Return all items delim in s, but replace delim with a space \"\"\"\n\tout = []\n\ti = -1\n\twhile i >= 0:\n\t\tif i!= len(o)-1 and o[i+delim] == delim:\n\t\t\tout.append(o[i][0])\n\t\t\ti += 1\n\treturn out\n\ndef cleanstring(s):\n\t\"\"\""}]}
{"task_id": "62ece4992e6aefcf4aabbd7d", "prompt": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tif inspect.isfunction(func):\n\t\tprint(\"\"\"running a function, got %s\\n\\n\"\"\", func.__name__)\n\t\tstart = time.time()\n\t\ttry:\n\t\t\tfunc(*args, **kwargs)\n\t\t\tprint(\"subprocess took %f seconds\\n\\n\"\"\" % ( time.time() - start ))\n\t\texcept:\n\t\t\tprint(\"error executing the function:\\n\\n\")\n\t\tsubprocess.run(*args, **kwargs)\n\n\telse:\n\t\tprint(\"don't know how to run a function from"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tsubprocess.run(subprocess.check_output(func(*args, **extra_env)))\n\n@contextmanager\ndef save_check(check_fn, args, env_dict):\n\t\"\"\"\n\t\tIndicate that check_fn returns true. Will save the return value\n\t\tin the check_fn and store the original value (before passing as args to check_exe)\n\n\tParameters\n\t----------\n\tcheck_fn : function\n\t    An ``check_fn`` that should return ``True`` if check is complete, otherwise\n\t\tan ``False`` value which"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\timport subprocess\n\tif os.name == 'nt':\n\t\t# this does what Windows does internally\n\t\targs = ['\"%s\"' % arg for arg in args]\n\t\tprocess = subprocess.Popen(args,\n\t\t                           stdin=subprocess.PIPE,\n\t\t                           stdout=subprocess.PIPE,\n\t\t           "}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\treturn subprocess.run(args, **kwargs)\n\ndef start_pserver(configfile, config, verbose, portname=None, port=DEFAULT_SERVANT_PORT, bind=\":\", use_ssl=False):\n\t# Set up the port for RPC on which subprocess will listen.\n\n\t# Set up a Python module for the subprocess module.\n\t# We need to dynamically load modules because of cross-compilation\n\t# issues.\n\tif importing:\n\t\tfrom subprocess import *\n\telse:\n\t\timport subprocess\n\n\t# We need to set these"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\texec(args, (func, *args, **extra_env))\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser()\n\t\n\t# Arguments that aren't passed to the sub-process\n\tparser.add_argument(\n\t'args',\n\thelp='command line arguments',\n\tnargs='*',\n\tdefault=sys.argv[1:])\n\n\t# A dictionary of environment variables that may be set\n\t# for each subprocess\n\tparser.add_argument('env')\n\n\t# If you want"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\timport os\n\timport subprocess\n\timport logging\n\timport errno\n\t\n\tlogger = logging.getLogger(os.environ['LOG_NAME'])\n\tlogger.setLevel(logging.INFO)\n\tlogger.addHandler(logging.StreamHandler())\n\n\ttry:\n\t\treturn subprocess.run(str(func), args = args, shell=True, env=extra_env)\n\texcept (OSError, subprocess.SubprocessError) as e:\n\t\t# This method might be removed, because subprocess raises its exception\n\t\t# rather than OSError when it fails\n"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\timport subprocess\n\tsubprocess._run = True\n\tsubprocess.run(subprocess.check_output, (func(*args, **extra_env)))\n\n\t\n#------------------------------------------------------------------------------\n# Module Import\n#------------------------------------------------------------------------------\n\n\ndef _import_modules(argv):\n\t\"\"\"\n\tHelper for importing all Python modules that Python has imported.\n\t\n\tParameters\n\t----------\n\targv : str[int]\n\t    Any command line arguments to be passed in.\n\t\"\"\"\n\tmodules = []\n\tfor i in xrange(len(argv)):\n\t\tif str("}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tsubprocess.run(func, *args, **(extra_env.copy()))\n\ndef run_with_kwargs(func, *args, **kwargs):\n\t\"\"\"\n\tWrap an arbitrary function as a subprocess.Call object.\n\t\n\tRuns the first positional parameters, and any additional parameters.\n\t\n\tNotes\n\t-----\n\tAll keyword options will be passed to ``subprocess.call``. Parameters\n\twill be passed to the callable object when run.\n\t\"\"\"\n\treturn subprocess.run(func, *args, **kwargs)\n\ndef run_single(func, *args):"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\timport os, sys\n\timport importlib \t# for getting module to import, importing on demand.  Not needed if we are here\n\timport traceback\n\t\n\targs = sys.argv[:]\n\t\n\ttry:\n\t\timportlib.import_module(sys.argv[1]) # force to import later\n\texcept ImportError as e:\n\t\t# If we can't import an importable module, just pass in a function\n\t\targs.append(func) # append the function to be run\n\t\targs.append(args[0]) # first arg is filename of"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tdef subprocess(cmd, proc_exit=True):\n\t\t\"\"\"\n\t\tRun the sub-process with the given command and return\n\t\tthe result.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tcode = subproc.call( cmd, *args,\n\t\t\t\t\t\t\t *process_env, **extra_env)\n\t\texcept:\n\t\t\tcode, out, err = proc_exit if proc_exit else subprocess.call(cmd, *args, **extra_env,)\n\t\t\traise subprocess.SubprocessError(code, out, err)\n\t\treturn code"}]}
{"task_id": "62ece4992e6aefcf4aabbd7e", "prompt": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tsuccess = True\n\tenvr = os.environ.copy()\n\tif envr and 'GITHUB_TOKEN' in envr:\n\t\tfrom get_github_token import get_github_token\n\n\t\tgithub_token = get_github_token()\n\t\tif github_token is None:\n\t\t\tsuccess = False\n\t\t\tprint \"Could not find GITHUB_TOKEN variable, using default value.\"\n\t\telse:\n\t\t\tsuccess = getattr(github_token, envr['GITHUB_TOKEN']) == envr['GITHUB_"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tif not os.environ.has_key(vname):\n\t\traise error, \"Environment variable {name} does not exist. Available variables are: {env_names}\" \\\n\t\t\t\t.format(name=vname, env_names=os.environ.keys())\n\treturn os.environ[vname]\n\n\ndef get_user_data_from_file(user_data_dir, username), password=None, prompt=None, default='None', verbose=False, **kwargs\n    try:\n\n        default_values"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tif match_input_to_env_variable(\"ENV_VAR_TEST_1_value\",\"\") is not None:\n\t\ttry:\n\t\t\treturn int(match_input_to_env_variable(\"ENV_VAR_TEST_1_value\") )\n\t\texcept:\n\t\t\traise Exception(\"Invalid value supplied. Expected integer value\")\n\traise Exception(\"No default value supplied\")\n\ndef get_env_variable_from_file(file_name):\n\t\"\"\"\n\tGet a value from a key named file_name. If the variable is not defined in OS"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tdef GETENV( name, default_value=None ):\n\t\tfor _, k, v in vars(self).items():\n\t\t\tif k == name:\n\t\t\t\treturn v\n\t\t\telif v is None and default_value is not None:\n\t\t\t\treturn default_value\n\t\traise KeyError( \"Environment variable not defined: %s\" % (name,) )\n\t\n\"\"\"\n\tGet all the key-value pairs of all environment variables from configuration file.\n\"\"\"\n\tdef GETENV_ALL_VALUES( self ):\n\t\t# NOTE: Use the 'env'"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tif len(os.environ) > 1:\n\t\tprint('Please select environment from environment-variables.py')\n\t\traise(1)\n\n\tdefault_env = os.environ.get('environment') or 'default'\n\tdefault_value = None\n\tvalue = None\n\tfor var in os.environ:\n\t\tif (default_env.lower()!=var.lower()):\n\t\t\tbreak\n\t\telse:\n\t\t\tdefault_env = os.environ.get('environment') or 'default'\n\t\t\tdefault_value=var\n\n\tif len"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tif not os.path.exists(environment):\n\t\traise Error('Invalid Environment Path %s' % environment)\n\tif default_value:\n\t\treturn env_variable(environment, var_name) or default_value\n\ttry:\n\t\treturn env_variable(environment, var_name)\n\texcept Exception as exception:\n\t\traise Error('error: %s' % exception)\n\n\ndef env_variable(env_content, var_name):\n\t# convert contents to env list\n/*\tfor var_item in env_list:\n\t\t for"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\timport os\n\tif name is None:\n\t\tname = os.getenv('MATCH_NAME')\n\tif not name:\n\t\traise OSError('Match environment variable \"MATCH_NAME\" not set.  Check environment variable section of the documentation for matching patterns.')\n\tvalue = match_env_default(name, default)\n\treturn(int(value)) if value is None else value\n\n\ndef __read_bool(arg, default=False):\n#\tif __verbose__ == 1:\n\tif os.getenv('PATTERN_VERBOSE')!= '1':\n\t"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tfrom collections import namedtuple\n\tfrom envtools import Env\n\n\tenv_key = os.environ.get(\"MY_VAR\", \"\")\n\tenvkey = \"MYENVVAR\"\n\n\tdef get_value(func):\n\t\t\"\"\"\n\t\tA helper that can be called only with a matching function.\n\t\tThe `func` parameter is the expected function to get the value from, if a non-string function\n\t\tis passed as argument.\n\t\tThen it is executed in the context of the current context.\n\n\t\t@return the value of the variable"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t# get the value\n\tvalue, exists = get_var_env(name=name, default=\"\")\n\tif not exists:\n\t\traise KeyError(\"variable %s not found\" % name)\n\n\tif os.environ.get(name, \"\")!= value:\n\t\traise BadParameterError(\"variable %s contains environment values that differ only by case\" % name)\n\n\ndef add_parameter_to_env(name, parameter, value, default=None, comment=''):\n\t# put the value into the environment as a list of values.\n\t# add the"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tresult = {}\n\tif _ENV_VARS is None:\n\t\tif len(self.variables) > 0:\n\t\t\t_ENV_VARS = [ var.split(\".\")[-1] for var in self.variables ]\n\t\tfor var in _ENV_VARS:\n\t\t\tret = os.environ.get(var)\n\t\t\tif ret is not None:\n\t\t\t\tresult[var] = ret\n\tif self.variables:\n\t\tfor var in self.variables:\n\t\t\tenv_var = \".\".join(var.split(\".\")[:-1])"}]}
{"task_id": "62ece4992e6aefcf4aabbd7f", "prompt": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t# get image ID\n\timage_id = urllib.parse.quote_plus( image_href.split(  '/', 1)[ 1 ] )\n\t# extract image location (only if relative URI)\n\tfull_uri = urllib.parse.urlparse( image_href )\n\t# get image object\n\timage = Image.from_url(\n\t\t\t\tfull_uri,\n\t\t\t\tverify=True\n\t\t\t\t\t\texcept:\n\t\t\t\t\t\traise ValueError('Image not found.')\n\t\t)\n\t# check if all parts of the href are in the right place\n\tif len("}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t# this is needed for urllib3 to figure out the correct port\n\tparts_list= urllib.parse.urlparse(image_href).scheme.split(\":\")\n\tif not parts_list[0]: raise ValueError(\"Invalid image url\")\n\timage_id= parts_list[0]\n\t# url decode href\n\timage_href= urllib.parse.urlunparse(parts_list[(len(parts_list)+1) :])\n\timgdom= parseString(urllib.parse.urlparse(image_href).query).getroot()\n\timgdom.attrib"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tinfo = urllib2.urlopen(image_href).info()\n\thttp_headers = [info.getheader('Host')]\n\thttps_headers = [info.getheader('SSL'), info.getheader('X-Forwarded-SSL')]\n\thost_parts = hostparts_reverse[info.getheader('Host').lower()]\n\thttp_parts = http_headers +https_headers\n\n\t# find the http part and remove it from the host parts to get the proper part\n\n\tfor i, http in enumerate(host_parts):\n\t\tif http not in http"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\timage, http_result, use_ssl, netloc, _ = parse_href( image_href = image_href )\n\treturn image_id, http_result, use_ssl, netloc, http_result\n\ndef parse_href( image_href ):\n\t\"\"\"Convert an image href into an (Image, HttpResult, UseTlsVersion, Netloc) tuple\"\"\"\n\tif '\\r' in image_href:\n\t\timage_url ='' + image_href\n\t\thref = image_url\n\telse:\n\t\timage_url =''"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\ttry:\n\t\timginfo = ImageInfo(http_content(image_href, http_headers))\n\texcept Exception as ex:\n\t\traise ValueError(\"Cannot download '%s' of type '%s': %s\" % (image_href, imageType, str(ex)))\n\ttry:\n\t\timage_id = str(imginfo.id)\n\texcept:\n\t\t# It is an image, no netloc.\n\t\treturn (None, None, False)\n\tif image_id in ('favicon', 'link', 'video'):\n\t"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tif not isinstance(image_href, (str, bytes, unicode)):\n\t\traise ValueError('The type of image_href is {}.'.format(type(image_href)))\n\n\t# get image information\n    image_info = get_image_info(image_href)\n    if image_info is None:\n        #raise ValueError('The specified image has no information in {}'.format(image_href))\n        image_id = None\n    else:\n       "}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tsplitted_image_part, _ = urllib.urlparse.urlsplit(image_href)\n\treturn splitted_image_part, splited_image_part.netloc, splited_image_part.path, splited_image_part.query, splited_image_part.fragment \n\ndef get_image_from_href(image_href):\n\t\"\"\" Given an href, try to construct an image.\n\t\n\t:param image_href: href of an image\n\t:returns: A tuple of (image_id, netloc, use_ssl)"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tif not image_href or re.findall(r'https', image_href):\n\t\traise ValueError(\"Bad image format specified, no 'https' url found in href\")\n\tresult = urlparse(image_href)\n\timage_url = result.scheme + \"://\" + result.netloc + \":\" + result.path\n\tif len(netloc) > 0:\n\t\timage_url = image_url + netloc\n\tif result.query(None) is not None:\n\t\timage_url += \"?\" + result.query(None)\n\tif"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tfrom http_parser import UrlParser\n\turl = UrlParser.parse(image_href)\n\t# split the url into\n\tparts = url.path.split(\"/\")\n\t# get the http parts\n\thttp_parts = [part.split(\":\", 1) for part in parts if part]\n\t\n\t# get the ftp info\n\tftp_info = get_ftpinfo(parts[1])\n\t# return the id\n\treturn (parts[0], parts[1], ftp_info)\n\n\ndef get_ftpinfo(path):\n\t\"\"\"\n\tUse a"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tresult = parse_image_href(image_src)\n\tif result is None:\n\t\traise ValueError(\"no image href given\")\n\n\timage_id, netloc, use_ssl = result\n\timage_src = image_id\n\tuse_ssl = use_ssl and \"https\" or \"http\"\n\n\tuse_ssl = \"http://\" if use_ssl in (\"http\",\"https\") else \"\"\n\n\turl = f\"file://{image_src}\";\n\treturn url, image_id, use_ssl\n\n\ndef parse_image_href"}]}
{"task_id": "62ece4992e6aefcf4aabbd80", "prompt": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tfor element in input_list:\n\t\tif len(element) < 1 or element[len(element)-1] == os.path.sep:\n\t\t\tdel input_list[len(input_list)-i-1]\n\t\telse:\n\t\t\tbreak\n\treturn input_list\n\ndef remove_invisible_file_name(input_filename):\n    \"\"\"\n\tIf the filename is not already in the input_filename\n\tthen remove it.\n\t\n\tArgs:\n\t    input_filename: string with path to filename to remove.\n"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t# First, collect the pathname seperators\n\tfile_list = [c for c in input_list] if isinstance(input_list, list) else []\n\t\n\t# Iterate and remove them, until we've found all seperators\n\twhile len(file_list) > 0:\t\n\t\t# We have to go to the last seperator. If this character is not first to loop\n\t\t# will stop.\n\t\tnext_seperator = file_list[-1]\n\t\tif not(file_list[0] == next_seperator[:-"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tif len(input_list)<=0:\n\t\traise TypeError(\"Empty input_list\")\n\ttry:\n\t\tfor entry in input_list:\n\t\t\t# Check if it is a pathname seperator\n\t\t\tif entry[0]==\" \":\n\t\t\t\t# Get last character\n\t\t\t\t# if last char is a seperator, remove it\n\t\t\t\tif entry[-1]==\" \":\n\t\t\t\t\tentry = entry.rsplit(\" \", 1)[-1]\n\t\t\t\telse:\n\t\t\t\t\t# Set string length to zero\n\t\t\t\t\tentry = \"\"\n\t\t\t\t"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t# Test if the string list is a non empty list\n\tif len(input_list) == 0:\n\t\treturn (input_list,)\n\t\t\n\toutput_list = []\n\t\n\tprint \"Running through the list...\"\n\t\n\tfor string in input_list:\n\t\t\n\t\t# Remove all but the last character from the string\n\t\tif string!= os.sep:\n\t\t\toutput_list.append(string[len(string) - 1])\n\t\t\n\t\t# Remove all but the last character from the string\n\t\tif string!= os.sep:\n\t\t"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tfor i in range(len(input_list)):\n\t\tif not re.search(r'\\bstring\\b: \\b', input_list[i]):\n\t\t\tinput_list[i] = input_list[len(input_list)-1 - i]\n\n\toutput_list = []\n\tfor input_string in input_list:\n\t\toutput_list.append(input_string[:-1])\n\t\n\treturn output_list\n\ndef _replace_filename_by_ext(filename):\n\t# \treplace \".\" and \"/\" by appropriate path"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tresult = []\n\tlast = input_list[-1].strip().split(os.sep)[0]\n\tfor item in input_list:\n\t\tif len(item) > len(last):\n\t\t\tif item[-1] in os.sep:\n\t\t\t\tresult.append(item[:-1].strip())\n\t\telse:\n\t\t\tresult.append(item.strip())\n\treturn result\n\ndef check_file(filename):\n\t\"\"\"\n\tCheck if the specified file exists\n\t\n\tArgs:\n\t    filename: string\n\t\n\tReturns:"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\toutput_list = []\n\tfor i in range(len(input_list)):\n\t\tif len(input_list[i]) > 1:\n\t\t\tinput_list[i] = input_list[i][0:1] + os.path.sep_seperator + input_list[i][1:]\n\t\tout = input_list[i].split(os.path.sep)\n\t\toutput_list.append(out)\n\treturn output_list\n\t\n\t\n\t\nif __name__ == \"__main__\":\n  \n   import sys"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tif not isinstance(input_list, (list, tuple)):\n\t\traise TypeError\n\toutput_list=[]\n\ti=0;j=0\n\twhile(j<len(input_list)):\n\t\tif strnlen(input_list[j],5)==len(input_list[j])+3 and input_list[j].startswith(\"\\\\\"):\n\t\t\t#Remove the separator of a drive\n\t\t\tfor k in range(j+1,len(input_list)):\n\t\t\t\tinput_list[k] = input_list[k][2:].replace"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\treturn iter(string.strip(input_list, string.s()))\n\t\"\"\"\n\n\ndef str_count(str1, str2):\n\tif str1 == str2:\n\t\treturn 0\n\treturn str1.count(str2)\n\n\ndef get_user_list(user_dir = os.path.expanduser('~')):\n\tif not os.path.exists(user_dir):\n\t\tos.makedirs(user_dir)\n\tfiles = os.listdir(user_dir)\n\tresult = \"\"\n\t"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tassert len(input_list) > 1, \"The input_list must not be empty!\"\n\t\n\tpath_seperator = \"\"\n\t\n\tfor i in range(len(input_list)):\n\t\tnew_path_len = len(path_seperator) + input_list[i].find(os.pathsep)\n\t\tpath_seperator += input_list[i][:new_path_len]\n\t\n\treturn input_list[0:len(path_seperator)]\n\"\"\"\n\nclass Solution(object):\n   "}]}
{"task_id": "62ece4992e6aefcf4aabbd82", "prompt": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef removeComment(self, text):\n\t\tstart, end = 0, len(text) - 1\n\t\tfor c in text:\n\t\t\tif c == \"\\n\":\n\t\t\t\tif end == start: continue\n\t\t\t\tend += 1\n\t\t\telif c == \"\\r\":\n\t\t\t\tif end == start: continue\n\t\t\t\tend += 1\n\t\t\tend += 1\n\t\treturn re.compile(text[start:end])\n\n/**\n  ******************************************************************************\n  * @file    USB_Host/HID_Standalone/Inc/stm32f7xx"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\trePattern = string.replace('\\n','')\n\trePattern = rePattern.replace('\\n','').replace(\"\\n\", \" \")\n\trePattern = rePattern.replace(\"[\\n]*\\n\", \"\\n\").replace(\"[\\n]\", \" \")\n\trePattern = rePattern.replace(\" \",\"\")\n\n\tregex = rePattern.replace(\"[\"+p+\"]\", \"\")\n\n\tif len(regex) > 6:\n\t\tregex = regex[0:4] + \".*?(\"+regex[2:6]+\"*)+\"\n\n\treturn re.compile(regex"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\treturn re.escape( text)\n\ndef RemoveEndChars( text):\n\t#\n\t# given the string to remove end chars\n\t#\n\ttext = text.replace( chr( 27), \"\")\n\ttext = text.replace( chr( 94), \"\")\n\ttext = text.replace( chr( 125), \"\")\n\ttext = text.replace( chr( 35), \" \")\n\ttext = text.replace( chr( 41), \"\")\n\ttext = text.replace( chr( 58), \"\")\n\ttext = text.replace( chr( 44), \"\")\n\ttext ="}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef RegexPatternToRegex(self,str):\n\t\tif str not in RegexPatternToRegexDict.keys():\n\t\t\tRegexPatternToRegexObj = re.compile(pattern=str,flags=re.IGNORECASE)\n\t\t\tRegexPatternToRegexDict[str] = RegexPatternToRegexObj\n\t\treturn RegexPatternToRegexDict[str]\n\n\n\n\ndef IsFilePresent(path):\n\tif path not in str(os.listdir(\".\")):\n\t\treturn \"false\"\n\telse:\n\t\treturn \"True\"\n\nnamespace Lucene"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tpattern = r'(?i)'.join(re.findall(r'[a-zA-Z]*$', str(row)) for row in string_info)\n\treturn re.compile(pattern)\n\ndef get_string_info(string_list):\n\t\"\"\"\n\t\tgiven a list of string that a list of string_info can return\n\t\tReturn all possible string_info for the given list of string\n\t\"\"\"\n\tstring_info = [[] for _ in string_list]\n\tfor item in string_list:\n\t\tfor row in string_"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tregex = r'(\\d[\\.]){3,4}(\\d[\\d]{1,}(\\d[\\d]{1,}){0,2})'\n\treturn re.compile(regex,re.I) # returns the regex re\n\n\"\"\"\n\t:copyright:  Copyright 2018-2021 Codelon Woods\n\t:license:    MIT\n\"\"\"\n\nrequire \"net/https\"\n\nmodule Rubyvis\n  class Client < HttpLib::Client\n    attr_reader :path\n\n    def base_url\n    "}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t# get regex patterns\n\tresult_re = []\n\tfor group in re.finditer(\"\"\"(?xi)([^{}]+?)(?=(.*))\"\"\",\"\"):\n\t\tresult_re.append( group.group(1) )\n\n\t# get regex for regex patterns\n\tregex_pattern = {}\n\tfor regex in result_re:\n\t\tregex_pattern[\"\\\\{}*{}\".format(regex[0], regex[5])] = \\\n\t\t\tre.compile( regex[1], re.I | re.S )\n\treturn regex_pattern\n\n"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef _compile(self):\n\t\treturn self.replace('a','(?:')\n\"\"\"\n\tThis method provides the basic pattern search method, using a basic\n\t\tpattern of the string'self'\n\t\"\"\"\n\tdef search(self, string):\n\t\treturn re.findall(self,string)\n\"\"\"\n\tThis method returns a regular expression match result\n\t\"\"\"\n\tdef getMatchCount(self):\n\t\treturn re.search(self, self._compile()).group(1)\n\n\"\"\"\n\tThis class is used to parse the regexp pattern in the url"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tpattern = re.compile('([a-z0-9._-]+)' if pattern == '0' else '([a-z0-9]*)')\n\"\"\"\n\tThis method returns the pattern\n\t\"\"\"\n\treturn pattern\n\n\ndef get_regex_pattern_for(pattern):\n\t\"\"\" Returns the regex pattern for a given string.\n\t\tIf the pattern contains '0'\n\t\tthe first  integer only\n\t\tif the pattern contains '1'\n\t\t... etc.\n\n\t\"\"\"\n\n\tpattern = pattern.replace('0', '9')\n"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef convert(self,pattern):\n\t\tfor i in range(0,len(pattern)):\n\t\t\tpatt = pattern[i]\n\t\t\tif re.findall(patt, self[i].source):\n\t\t\t\tself[i] = re.sub(patt, self[i].pattern, self[i].source)\n\t\treturn self\n\n\"\"\"\n\tCreate search class\n\"\"\"\nclass search():\n\tdef __init__(self,file_path, pattern, mode):\n\t\tself.file_path =file_path\n\t\tself."}]}
{"task_id": "62ece4992e6aefcf4aabbd83", "prompt": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t@staticmethod\n\tdef cmdloop(cmd, stdin=None, stdout=None, stderr=None, wait=True):\n\t\tshell = subprocess.Popen([cmd], stdout=subprocess.PIPE, \n\t\t\t\t\t\t\t\t\t stderr=subprocess.PIPE, stdin=stdin, \n\t\t\t\t\t\t\t\t\t cwd=os.getcwd())\n\t\toutput, error = shell.communicate()\n\n\t\tif error:\n\t\t\traise error\n\t\treturn output or \"\"\n\n\t@staticmethod\n\tdef cmdloop(cmd, args):\n\n\t\tif args:\n\t\t\tcmd += \" %s\" %"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tprint \" \".join(shellCmd[1:])\n\ndef shellParse(args):\n\t'''\n\tShell call to make it work like a command line app\n\t'''\n\tsubparsers = parser.SubParsers()\n\tsubparser = subparsers.add_parser('cmd','Command to call with shell')\n\tsubparser.add_argument('command', help='the command')\n\tsubparser.add_argument('args', nargs='*', help='the command arguments')\n\tsubparser.set_defaults(func=shellCmd)\n\treturn subparser\n\n\"\"\"\n\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tdef log(s):\n\t\tpass\n\n\t# print \"start\"\n\tif len(sys.argv) > 1:\n\t\t# print \"process arguments... (\" + sys.argv[1:] + \")\"\n\t\twith open(sys.argv[1], \"w\") as f:\n\t\t\tfor line in process_args:\n\t\t\t\tf.write(line + \"\\n\") # Append new line to end of file so we are able to see the command output\n\n    if len(sys.argv) > 2:\n       "}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif len(command) > 1:\n\t\tcmd = os.fsep.join(command)\n\t\targs ='\".join(sys.argv[0] + [sys.executable] + cmd)\n\t\tos.system(args)\n\n/*\n *\tA generic 3-way parallel multi-object allocator.\n *\n *\tHeavily borrowed from the following project:\n *\n *\t\"Fernando Herrera\" <fherrera@eurielec.etsit.upm.es>.\n *\n *\tFor the original copyrights look"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tdef call_process(command, target, cwd, env=None, timeout=None, env_list = None,\n\t\t\t\t\t\t\t# This is where the systemd dbus daemon will find it.\n\t\t\t\t\t\t\t# In some cases, you need to use another method.\n\t\tcmd_systemd=False, **env_vars):\n\n\t\t#\n\t\t# Command is a simple list or string\n\t\t#\n\n\t\t# Call the user supplied shell.\n\t\t# This is optional but helps when running \"sudo x\" on Windows.\n\t\t# It also"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t\tself.run_command_list_and_capture_output(command_list, capture_output)\n\n\t# This is just a dummy\n\"\"\"\n\t\"\"\"\n\t\tself.run_command_list_and_capture_output(commands, capture_output)\n\n\t\t\"\"\"\n\t\tself.log_string( \"Command list:\" )\n\t\tself.log_string( \"==================\" )\n\n\t\tfor command in commands:\n\t\t\ttext = command[2]\n\t\t\tif self.is_debug_level_selected():\n\t\t\t\ttext"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\ttry:\n        subprocess.Popen( cmd, stderr=subprocess.DEVNULL, shell=False )        \n\texcept OSError as e:\n\t\timport errno\n\t\t\n\t\tif e.errno == errno.EPERM:\n\t\t\tprint 'You do not have permissions to execute %s' %(cmd)\n\t\t\tprint 'See the \"Create a new file\" page.'\n\t\t\treturn \n\t\t\t\n\t\tif e.errno == errno.ENOENT:\n\t\t\tprint (\"Can't find %s as 'execvp': %s"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tdef __init__(self, cmd = '', *args, **kwds):\n\t\tself.cmd = cmd\n\t\tself.args = args\n\t\tself.kwds = kwds\n\t\n\tdef __repr__(self):\n\t\treturn''.join(['{0.cmd.upper():02X} {0.args!r}'.format(c) for c in sorted(self.kwds)])\n\t\n\tdef get_output(self, timeout=60):\n\t\toutput = subprocess.check_output( [self.cmd] + self.args, timeout=timeout"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\ttry:\n\t\tsubprocess.call( command.split( '.' ), shell=ShellExecute.SHELL_EXECUTABLE )\n\texcept OSError:\n\t\traise NotImplementedError( 'ShellExecute.ShellExecute() did not exist. Try a different user.' )\n\n\ndef GetUserName(User = ''):\n\tUserLoggedIn = False\n\tfor User in GetUserList():\n\t\tif User == User || User == GetUserName(User): UserLoggedIn = True\n\tif UserLoggedIn == False: return 'Unknown User'\n\treturn User\n\ndef GetUser():\n\tUser"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t#\n\tcmds = {\n\t\t'install-dbase' \t: [\"Create a new database.\"],\n\t\t'move-dbase'\t\t: [\"Move a dbase file to a different location.\"],\n\t\t'rename-dbase'\t: [\"Rename a dbase file.\"],\n\t}\n\n\tdef __call__(self, argv, exit_on_error):\n\t\ttry:\n\t\t\tif len(argv)<4:raise self.UsageError,'Must have at least 3 args'\n\t\t\tcommandname = str(argv[2])"}]}
{"task_id": "62ece4992e6aefcf4aabbd84", "prompt": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t#test ip address.\n\t#This function takes IP address and returns \n\t#result boolean value\n\t#if an IP address  is a multiple IPv4 address.\n\t#If there is  no IPv4 address or  if  an IPv4 address is\n\t#passed  as a string that is not a valid IPv4 address.\n\t#Then a boolean value of 0 is returned.\n\t\n\t#This function takes IP address and returns if it is a\n\t#single IPv4 address. \n\t#If there is  one IPv4 address or if the IP address passed\n\t#"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tipaddress.__new__(ipaddress)\n\tprint mac(address, 0) \t\t\t\n\t\n\"\"\"\n\tTest if address or not\n\t \n\t    \n\t\"\"\"\n\taddress.__new__(address)\n\tprint mac(address, 0) \t\t\n\n\"\"\"\n\tTest if IP address or not \n\t \n\t    \n\t\"\"\"\n\tipaddr.__new__(ipaddr)\n\tprint ipaddress(ipaddress(ipaddress('127.0.0.1'))) \t\n\t    \n\t\"\"\"\n\tTest if IPv4 address or not\n\t \n\t    \n\t\"\"\""}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tif not ipaddress.ip_address(b'192.168.1.1') == '192.168.1.10':\n\t\tprint(\"Test Failed: Not a valid IPv4 address\")\n\"\"\"\n\nif __name__ == \"__main__\":\n\tprint(\"Test Passed: IPv4 Address Checking\")\n\npackage com.jeffmonymedia.player;\n\n/*\n * Copyright (c) 2002, 2020, Oracle and/or its affiliates.\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without modification, are\n *"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\ta = address.IPv4Address( \"2.32.1.4\" );\n\tb = address.IPv4Address( \"/24\" );\n\tc = address.IPv4Address( \"255.255.255\" );\n\td = address.IPv4Address( \"2001::1\" );\n\n\tprint 'IPv4address'\n\tprint \"a.String: %s\" % a.String()\n\tprint \"a.ToByteArray() returns %s\" % a.ToByteArray()\n\tprint \"a.ToByteArray(a.String()) returns %s\" %"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tif isinstance(ip,str) and ip.startswith(\"172.168.\"):\n#\t\tpprint(\"Trying to test if a valid IP is given\")\n#\t\tpprint(ip)\n\t\tif ip==\"172.168.0.255\":\n\t\t\treturn True\n\treturn False\n\n\"\"\"\n\tTest if a string is a valid IP notation\n\n\tExample\n\tif ip==\"172.168.X.Y\":return(False)\n\t\"\"\"\n\tif ip==\"172.168.0.255\":return(True)\n\treturn False\n\n\n"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tipaddr, host, mask\t= get_host_addrs('::/0') \n\tif(not ipaddr):\n\t\tprint('Error in get_host_addresses(32)')\t\t\n\tif(not ipaddress.is_valid('127.0.0.1')):\n\t\treturn(False)\n\treturn(ipaddress.is_ipv4address(ipaddr))\n\t\n\n\n\"\"\"\nTest if IPv6 address or not\n    \n\t\"\"\"\n\tipaddr, host, mask\t= get_host_addrs('fc00::/10') \n\tif("}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tv4address = re.compile(r'^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$')\n\n\tdef __init__(self, v4=False):\n\t\tglobal __ip\n\t\tif not v4:\n\t\t\t__ip = socket.getaddrinfo(0, 0, 0, socket.AF_INET,"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tdef TestIPv4(self):\n\t\tipaddress.ip_address('1.1.1.1')\n\t\t\"\"\"\n\t\tTest if IP address as input IP\n\t\t\"\"\"\n\t\tipaddress.ip_address('1.1.1.1')\n\t\t\n\tdef TestIPv6(self):\n\t\tipaddress.ip_address('2001:2000:2001:2001:2001:2001:2001:2001')\t\n\t\t\"\"\"\n\t\tTest if IP address as input IP and not IPv6\n\t\t\"\"\"\t\n\t\tipaddress.ip_interface"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tipaddr = get_ipaddr_by_name\n\tipaddr = get_ipaddr_by_host\n#\t\t\tipaddr = get_ipaddr_by_domain\n\tipaddr = get_ipaddr_by_name_prefix\n\t\n\tipaddr = get_primary_ip_address\n\tipaddr = get_secondary_ip_address\n#\tipaddr = get_primary_ip_address\n\t\n\tipaddr = get_domain_ipv4\n\tipaddr = get_broadcast_network_ip_address\n\tipaddr = get_domain_ipv4_suffix\n\n\tipaddr = get_ipv4_in"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tres = run_command(\"ipaddress\", {\"ipaddress\": \"192.168.0.30/32\"})\n\tres = run_command(\"ipaddress\", {\"ipaddress\": \"192.168.0.10/25\"})\n\tcheck_status(\"/1 /1 \" + ipaddress_output)\n\t\n#============================================================================================================\n# Delete IPv6\n#============================================================================================================\n\ndef ipaddress_output():\n\treturn \"\"\"\\xa0\n\t<ip type='ipv6'>\n\t\t<address>10.10.10.10</address>\n\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd85", "prompt": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\ndef _check_for_invalid_rdf(graph, rdflib_input, start_type):\n\tfrom rdflib import RDFS\n\tfrom rdflib.plugins.utils import \\\n\t\tcheck_RDF_uri_valid_for_sparql_query\n\ttry:\n\t\tstart_type = int(start_type, 10)\n\texcept:\n\t\tprint(\"Invalid RDF start triple: %s\" % start_type)\n\t\texit()\n\tif \"subClassOf\" in rdflib_input or rdflib_input"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t# TODO need better handling of class\n\t# If the current class is a broader, the result is a set\n\tif RDFlib.RDFS.subClassOf == \"broader\" or RDFlib.SKOS.broader in RDFlib.findSkos(graph, RDFlib.SKOS.broader):\n\t\treturn set(\n\t\t\t\t\t\t\trdflib.findSkos(graph, RDFlib.SKOS.broader).keys()\n\t\t)\n\t# If the current class is not a broader, the result is a set\n\tif"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\tgraph = find_roots(graph, parent_class).as_list()\n\twhile len(graph):\n\t\tprint()\n\t\tprint(\"graph contains \", graph)\n\t\tfor s in graph:\n\t\t\tprint()\n\t\t\tprint(\"graph contains \", s)\n\t\t\tfor n in find_roots(s, subClassOf).as_list():\n\t\t\t\tprint()\n\t\t\t\tprint(\"n contains \", n)\n\t\t\t\tfor i in find_roots(n, \"broader\").as_list():\n\t\t\t\t\tprint()\n\t\t\t\t\tprint(\"i contains"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\tdef find_roots(graph, rdflib.RDFS.subClassOf):\n\t\t\"\"\"\n\t\tFind all the roots of the given hierarchy.\n\t\t\"\"\"\n\t\tif isinstance(graph, rdflib.GraphSubject):\n\t\t\tgraph = list()\n\t\telse:\n\t\t\tgraph = [graph]\n\t\n\t\tresults = rdflib.find_all(graph, rdflib.RDFS.subClassOf)\n\t\tresults.sort(key=lambda t: t[3])\n\t\troots = {}\n\t\tstack = []"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\ttree = construct_rdfs_tree(graph)\n\t#print(tree)\n\t\n\treturn find_roots(tree, RDFList)\n\t\n\ndef find_roots(tree, parents=[], exclude=[None, None]):\n\t#print(tree)\n\troots = []\n\tfor sub in tree.subClassOf:\n\t\tif True in exclude:\n\t\t\tif True in set(sub.get(\"exclude\", [])) or not(sub.get(\"exclude\", [])):\n\t\t\t\tcontinue\n\t\tsub_type = sub.get(\"type\", \"\")\n\t"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\tclass_names = set()\n\tfor v in graph.rdf.values():\n\t\tif v.Class!= \"object\" and v.Class not in class_names:\n\t\t\tclass_names.add(v.Class)\n\t\tif v.Class == \"broader\" and v.value.rdfType == rdflib.RDF_TYPE.Resource:\n\t\t\tbreak\n\treturn {v.value.Resource for v in graph.triples_iter()}\n\ndef find_roots(graph, RDF):\n\t\"\"\"\n\tReturns a complete set of all sub"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\tfor i, child in enumerate(graph.objects(RDF.firstclass, rdflib.Resource)):\n\t\tfor j, prop in enumerate(rdflib.PropertyChain(child),1):\n\t\t\tif prop is not None:\n\t\t\t\tfor k, parent in enumerate(find_roots(child, rdflib.IRIRef())):\n\t\t\t\t\tif parent.name() and parent.name()!= rdflib.Node:\n\t\t\t\t\t\tparent.name().parent().add(j,k)\n\n\treturn {}\n\ndef rdflib_findRoots(graph):"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\tdef find_roots(graph, klass):\n\t\tif not isinstance(graph, rdflib.Graph):\n\t\t\traise RuntimeError(\"find_roots() called on non-graph\")\n\t\tif klass is None:\n\t\t\treturn set()\n\t\troot = graph.create_node(\"subclassOf\")\n\t\tt0 = graph.create_node(\"classOf\")\n\t\tt1 = graph.create_node(\"broader\")\n\t\tt2 = graph.create_node(\"broader\")\n\t\tt3 = graph.create_node('broader"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\tn = len(graph)\n\troots = {}\n\tfor parent in graph:\n\t\tif not parent:\n\t\t\tcontinue\n\t\tfor child in graph[parent]:\n\t\t\tchildren = set(graph[child])\n\t\t\t# Find the roots of the children\n\t\t\tfor child in children:\n\t\t\t\t# Check each of the children...\n\t\t\t\tif parent == child:\n\t\t\t\t\tcontinue\n\t\t\t\tnew_children = find_parents(parent, child)\n\t\t\t\tif new_children:\n\t\t\t\t\tfor c in new_children:\n\t\t\t\t\t\t#..."}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\treturn find_roots(graph, rdflib.RDFS.subClassOf)\n\n\ngraph = parse(\"https://raw.githubusercontent.com/worm/rdflib/master/graphs/rdflib_simple_example.ttl\")\n\n\"\"\"\n\tFind all root objects (rather than just the given value) directly from a list of triples\n\"\"\"\n\tfind_root(graph, [rdflib.RDF.type, rdflib.RDF.property, rdflib.RDF.subClassOf])\n\n\nif __name__ == '__main__':\n\tfind_"}]}
{"task_id": "62ece4992e6aefcf4aabbd86", "prompt": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\"\"\"\n\tfrom collections import OrderedDict\n\timport textfields as tf_\n\n\t@tf__(\n\t\tfilename=\"t3c.pkl\",\n\t\tserialize=False,\n\t\tserialize_with=\"json\",\n\t\tname=\"json\",\n\t\tjson=tf.JSONEncoder,\n\t\tjson_with=lambda x:json.dumps(x, cls=tc.TFEncoder),\n\t\tindent_width=None,\n\t\tindent_string=\"\",\n\t\tcompact_output=False,\n\t\tline_width=None,\n\t)\n"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t#pylint: disable=no-self-use\n\tf = io.BytesIO()\n\te = yaml.NodeDumper()\n\n\te.representer = yaml.SafeRepresenter(unicode, allow_unicode=True)\n\tf.write(yaml.dump(e, stream=f, allow_unicode=True, default_flow_style=False))\n\ts = f.getvalue()\n\t#pylint: disable=no-self-use\n\tf.close()\n\t#pylint: enable=no-self-use\n\n\treturn s\n"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t_ = dump\n\"\"\"\n\tA wrapper around python 2.x json that converts unicode string\n\t\"\"\"\n\tdef dump(data):\n\t\tif isinstance(data['content'], str):\n\t\t\treturn yaml.dump(data['content'])\n\t\telse:\n\t\t\treturn data['content']\n\t\"\"\"\n\tWrite the data to ``sys.stdout``\n\t\"\"\"\n\tsys.stdout.write(dump(data))\n\t\"\"\"\n\tReturn the data written to ``sys.stdout``\n\t\"\"\"\n\treturn data\n\n\ndef get_args():\n\t\"\"\""}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tdef dump_args(cls, kwargs):\n\t\t\"\"\"Make args dump (py2-unicode/py3-string)\"\"\"\n\t\tkw = \" {0} \".format(cls.__name__)\n\t\targs = []\n\t\tfor key, value in kwargs.items():\n\t\t\tkw += \" {0} {1}.  {2}\".format(key, type(value).__name__, value)\n\t\t\tif not key.startswith('_'):\n\t\t\t\t kw += \" \"\n\t\targs.insert(0, kw)\n\t\treturn args\n\n\t## Dump\n"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tdef dumpTo(self,stream=None):\n\t\t\"\"\"\n\t\tDump to a stream\n\t\t\"\"\"\n\t\tif self.file_obj:\n\t\t\tfile_contents=self.file_obj.read()\n\t\telse:\n\t\t\tfile_contents=None\n\t\tif stream is None:\n\t\t\tstream = sys.stdout\n\t\tif isinstance(stream,io.TextIOWrapper):\n\t\t\tstream.write(file_contents)\n\t\telse:\n\t\t\tstream.write(file_contents.encode('utf-8'))\n\n"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\ttry:\n\t\tdata = yaml.load(yaml_data)\n\t\tfor k, v in data.items():\n\t\t\tif type(v) == str:\n\t\t\t\tyaml_str += \"\\n{\"+str(k)+':'+ str(v) +'}\\n'\n\t\t\telse:\n\t\t\t\tyaml_str += v+'\\n'\n\texcept KeyError:\n\t\traise ValueError('No type found in data')\n\n\treturn yaml_str\ndef pprint_dict( d,  indent='  '):\n\t\"\"\"\n\td:"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tdef dump(self, stream=None, default=''):\n\t\t\"\"\"Dumps (dumps) value for human inspection\"\"\"\n\t\tif stream is None:\n\t\t\tstream = sys.stdout\n\t\tr = _yamldump.dump(self)\n\t\tif default is not '_':\n\t\t\tr = r.replace(default, 'default')\n\t\tif not r:\n\t\t\tstream.write(r)\n\t\treturn r\n\nclass _yamldump(object):\n\t\"\"\"Class for dumping values for human inspection\"\"\"\n\n\tclass _Dump"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t# Convert to a string.  If it is a py2-unicode\n\t# then this should probably use unicode in\n\t# py2-strings\n\tif bytes_version < Version('2.0'):\n\t\tfrom.six import python_2_unicode_compatible\n\n\t\tdef is_py2_unicode_compatible(a):\n\t\t\treturn python_2_unicode_compatible(\n\t\t\t# Python 2.x needs to be loaded if unicode_literals is available.\n# \t\t\t'builtins', PythonUnicodeLiteral)\n\t\t\tfor byte in byte_data:\n"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tdef dump(data, indent=\"\\t\"):\n\t\t_indent=indent+\" \"\n\t\tstring = py2_dump(data, _indent, encoding=data.encoding,\n\t\t\tensure_ascii=False)\n\t\tif _indent in linesep:\n\t\t\tpass\n\t\telif PY3:\n\t\t\treturn string\n\t\telse:\n\t\t\treturn string.encode(encoding)\n\ndef py3_dump(data, indent=0, encoding=None):\n\tif isinstance(data, basestring):\n\t\tdata = data.encode(encoding,"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t#pylint: disable-msg=E0401\n\tif (type(self) is String) or (type(self) is type('')):\n\t\t(s_data,) = self.split()\n\t\treturn _dumpdata(s_data) #pylint: disable-msg=E0401\n\telif type(self) is type(''):\n\t\t(s_data,) = self.split('+')\n\t\treturn \"%s\\n\" % _dumpdata(s_data) #pylint: disable-msg=E0401\n\telse:\n\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd87", "prompt": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tprint(\"Logger is: \", Logger.name)\n\tos.system('start_logger > Logger.logfile')\n\t\n\"\"\"\n\tParameters:\n\t\tparamList\t\t\tlist of  Parameters, each entry is a dictionary\n\t\tparamList.string \tstring to be passed to log call of logger\n\t\tparamList.int \tint to be passed to log call of logger\n\t\tparamList.float \tfloat to be passed to log call of logger\n\t\tparamList.float \tint to be passed to log call of logger\n\t\tparamList.string"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef __init__(self,log_file,log_flag=0,log_level=logging.INFO,log_file_mode=\"a\"):\n\t\t\"\"\"\n\t\tArguments:\n\t\t\tlog_file {str} -- [description]\n\t\t\tlog_flag {boolean} -- [description]\n\t\t\tlog_level {int} -- [description]\n\t\t\"\"\"\n\t\tif log_file == None:\n\t\t\tself.log_flag = 0\n\t\telse:\n\t\t\tif log_flag ==0:\n\t\t\t\tself.log_file_"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t_cur_logfile = None\n\t_cur_file = None\n\t_level = logging.NOTSET\n\n\tdef __init__(self, filename=None):\n\t\tself._cur_file = os.path.abspath('../')\n\t\tself._cur_logfile = os.path.abspath('.')\n\t\tset_logging_level(\n\t\t\t\tfilename if 'filename' in self.__dict__ else None,\n\t\t\t\tlevelname=self._level,\n\t\t\t\tdebugfile=self.debug)\n\t\n\n\tdef debug(self, msg, *args, **"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tfrom logging import handlers\n\n\t# Setup logging\n\timport logging\n\t# Create root folder to store log files. Root folder is\n\t# where logs are stored\n\tcurrent_path = os.path.dirname(os.path.abspath(__file__))\n\tlog_path = os.path.join(current_path,'log')\t\n\ttry:\n\t\tos.makedirs(log_path)\n\texcept:\n\t\tpass\n\tlogger = logging.getLogger()\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef __init__(self,file = None, log = 0):\n\t\tself.file = file\n\t\tself.log = log\n\t\t\n\t\ttry:\n\t\t\tself.root = os.path.dirname(file)\n\t\texcept:\n\t\t\tpass\n\t\telse:\n\t\t\ttry:\n\t\t\t\tself.filename  \t\t= file\n\t\t\t\tself.filesize \t\t= os.path.getsize(file)\n\t\t\t\ttry:\n\t\t\t\t\tself.fileext  \t\t= os.path.splitext(file)[1]\n"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tlogger = logging.getLogger(os.name);\n\n\"\"\"\n\tThe main entry point of the application.\n\tIt can do basic logging, output logging and debugging.\n\tIt can also display a user-defined information and the state\n\tof the application (the GUI).\n\n\tIt also supports the following features:\n\n\n    :  - debug\n    :  - info\n    :  - warn\n    :  - error\n    :  - fatal\n\n\n\t.. versionadded:: 0.5.0\n\"\"\"\ndef"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tlogging.basicConfig(filename='PythonLog.log', level=logging.INFO,\n    format='[%(asctime)s] %(levelname)s: %(message)s', datefmt='%H:%M:%S')\n\tlogging.getLogger('PyQuery').setLevel(logging.DEBUG)\n\tlogging.getLogger('QtWidgets.QApplication.instance').setLevel(logging.INFO)\n\tlogging.getLogger('pyqt_dock').setLevel(logging.INFO)\n\tlogging.getLogger('dbconnector').setLevel(logging.INFO)\n\n\tlogging.info"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef __init__ (self):\n\t\t\n\t\tif sys.version_info > (3, 0):\n\t\t\tself.logfile = self.__class__.__name__ + '.log'\n\t\telse:\n\t\t\tself.logfile = self.__class__.__name__ + '.log' + str(0 + int(os.getpid()))\n\t\t\n\t\tlogging.basicConfig(level=logging.INFO, format='%(asctime)s %(name)s %(levelname)s: %(message)s')\n\t\t\n\t\n\tdef __str__ (self):\n\t\treturn self.logfile"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef write(self,args):\n\t\tfrom.logger import Logger\n\n\t\tlogger = Logger(args['level'],\n\t\t\ttitle=args['title']+\": \"+args['message'],\n\t\t\ttextWriter=args['displayWriter'])\n\t\tprint(\"      \",args['filename'],\":    \", args['line'])\n\t\tprint(args['args'])\n\t\tprint()\n\t\t\nclass ConfigFile(object):\n\n\tdef __init__(self,filename=\"config.yaml\"):\n\t\tself.file = open(filename,'r')"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef get_app_logs(self):\n\t\tlogging_root_dir = os.environ.get(\"LOGGING_ROOT_DIR\", \"\")\n\t\tif not logging_root_dir:\n\t\t\tlogging_root_dir = self.get_default_logging_dir()\n\t\n\t\t# get_log_folder creates the directory if it does not exist\n\t\tlog_folder = os.path.join(logging_root_dir, '%s-%s' % (self.id,os.getpid()))\n\n\t\t# this is where your application store its logs\n\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd88", "prompt": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\treturn np.zeros(shape, dtype=dtype)\n\ndef empty(shape,dtype=np.dtype('S1')):\n\t\"\"\"Creates an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n\treturn np.zeros(shape, dtype=dtype)\n\ndef fill_diagonal(matrix):\n\t\"\"\"Fills the diagonal of the matrix with zeros.\n\t\n\tParameters\n\t----------"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tnum = 1\n\tfor dim in shape:\n\t\tnum *= dim\n\t\t\n\tassert num >= array_len, \"Infinite size to set up the array\"\n\t\t\n\treturn np.zeros(num, dtype=dtype).reshape(shape), array_len\n\ndef get_random_array(shape, np_dtype, dtype):\n\t\"\"\"\n\t    getRandomArray \n\t    Randomly generates a tensor with shape, shape is used for testing.\n\t    The dtype of the returned tensor is dtype.\n\t\"\"\"\n\tfrom random import uniform, randint\n"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tshape = tuple([item for item in shape])\n\tdtype = np.dtype(dtype)\n\t\n\treturn np.empty(shape, dtype=dtype)\n\ndef reshape(array, shape, offset=0):\n\t\"\"\"\n\tRearranges the given array (in place) in-place.\n\t\n\tSame shape with offset of 0.\n\t\n\tParameters\n\t----------\n\tobject : the array to reshape\n\tshape : tuple\n\t    size of the reshaping\n\toffset : int\n\t    offset in number of elements\n\t\"\"\"\n\t#"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\treturn np.array(shape, dtype)\n\t\n\n\n\"\"\"\ndef create_matrix(rows: int, cols: int, dtype: numpy.dtype):\n\t\"\"\"\n\tCreated matrix of shape rows x cols and datatype dtype.\n\t\n\tParameters\n\t----------\n\trows : int\n\t\tnumber of rows\n\tcols : int\n\t\tnumber of cols\n\tdtype : numpy.dtype\n\t    data-type of the matrix\n\t\n\tReturns\n    ---------------\n\tmatrix : numpy.ndarray\n\t\"\"\"\n\t\n\tif rows!= cols:\n\t\traise"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\treturn np.array(shape, dtype)\n\ndef create_empty_array(shape, dtype=\"float64\"):\n\t\"\"\" Creates an array of at least `shape` dimension with type `dtype` to contain `numpy.empty` values\n\t\n\t\tIf `shape` is a tuple, the shape must be a tuple with length >= 1 and all values in the tuple will\n\t\tbe used for creating the array.\n\t\t\n\t\tThe `numpy.empty` values, if available, will be filled with these values.\n\t\"\"\"\n\tif len(shape)!= 1 or len(shape"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tif len(shape) == 0 and dtype is None:\n\t\traise ValueError('If shape or dtype are None, there must be atleast one dimension.')\n\ttry:\n\t\tret = np.array((), dtype)\n\texcept TypeError:\n\t\traise TypeError('If dtype is not given, the array must be a numpy array of the appropriate shape.')\n\tif len(shape)!= 0:\n\t\tret.shape = shape\n\tif dtype is not None:\n\t\tret.dtype = dtype\n\treturn ret\n\n_DEFAULT_REPR = re.compile('"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tpass\n\t\"\"\"\ndef create_new(shape, dtype):\n\tpass\n\ndef create_empty(shape, dtype):\n\t\"\"\"Create a new empty array with given shape & dtype\"\"\"\n\tpass\n\ndef create_full(shape: tuple, dtype: np.dtype):\n\t\"\"\"Create a full dataset of the given shape and dtype.\n\n\tParameters\n\t----------\n\tshape : tuple\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tpass\n\ndef get_device():\n\tpass\n\t\"\"\""}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tif isinstance(shape, tuple):\n\t\treturn np.full(shape, 0).astype(dtype)\n\telse:\n\t\treturn np.full(shape, dtype)\n\n// The MIT License\n//\n// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n//\n// Copyright (c) 2020 Uber Technologies, Inc.\n//\n// Permission is hereby granted, free of charge, to any person obtaining a copy\n// of this software and associated documentation files (the \"Software\"), to deal\n// in the Software without restriction, including without limitation the rights\n//"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tif len(shape) < 2: \n\t\traise ValueError('the arg shape list must contain at least two elements.')\n\tif len(shape) < len(np.shape(in_): 0):\n\t\traise ValueError('the number of dimensions must be greater than 0, got {} dimensions.'.format(len(np.shape(in_))[0]))\n\tif len(shape) > len(np.shape(in_)):\n\t\traise ValueError('the number of dimensions must be smaller than or equal to the number of dimensions present in the input array specified by the shape list. Found"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tprint \"The shape is:\", shape\n\tnew_array = np.ndarray(shape, dtype=dtype)\n\tnew_array[:] = value\n\treturn new_array\n\ndef fill_box_with_numpy_array(value, shape, dtype='float64'):\n\tshape_array = fill_box_with_numpy_array_int(shape, dtype)\n\tvalue_array = fill_box_with_numpy_array_int(value, dtype)\n\tvalues = np.hstack((shape_array, value_array))\n\tfilled=np.full("}]}
{"task_id": "62ece4992e6aefcf4aabbd89", "prompt": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tsigma = 0.1\n\tM = np.array([    0.00        / (2 * np.pi),\n\t                0.16        / (2 * np.pi),     0.32,\n\t                0.50        / (2 * np.pi),     0.54,\n\t         "}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tx = np.array([0.14, 0.13, 0.19])\n\ty = np.array([0.26, 0.25, 0.28])\n\tz = np.array([0.2, 0.3, 0.4])\n\n\tres = gaussian(x, y=y, sigma1=0.1, stds=0.1)\n\tnp.testing.assert_allclose(res(z), -1.4326, atol=1e-4)\n\n/*\n * JBoss, Home of Professional"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\timg_ref = img.copy()\n\tsigma = 0.05\n\timg_ref[sigma//2,::,:]=img_ref[sigma-::,::,-:]\n\timg_ref[sigma//2:] = img_ref[sigma-(sigma-0.2)//2:-::,::,-:]\n\timg_ref[:sigma//2,:][:,::,:] = img_ref[:sigma-::,::,-:]\n\timg_ref[sigma//2][:][:,::] = img_ref[sigma-::,:::-][:,::]["}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tdef mean(arr):\n\t\treturn sum(arr) / len(arr)\n\tdef cov(arr):\n\t\treturn (sum([ abs(arr[i])**2 for i in range(len(arr)-1)])**0.5)\n\tdef rms(arr):\n\t\treturn a_cos_theta(arr) / len(arr)\n\n\tdef center_around(x,y):\n\t\treturn mean(  np.array([ x,y ],dtype=np.float32))\n\tdef get_gaussian_loc(gaussian):"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tsize = len(data)\n\tsigma = np.sqrt(size**2) / np.power(np.sqrt(size ** 2), 1.5)\n\tdata[data < 1] = 1\n\tdata[[1]] += data.size * sigma\n\tdata[data < 1] -= np.sqrt(size)\n\tresult = np.dot(data[1:], data[0:1] - data[1:, :]) - data[0]\n\tresult[result < 1e-10] = 0\n\treturn result\n\n\nif __name__ == '__"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t# Generate random data\n\txdata = np.random.randn(1000) * 1.4 + 0.2\n\t\n\t# The Gaussian object is the fitted Gaussian\n\tg = Gaussian(sigma=0.1*xdata, dim=100)\n\t\n\t# This is the original data\n\tydata = g.sample(100)\n\t\n\t# Fit the sample into the data\n\tg = g.fit(ydata)\n\t\n\t# Solve the model using the transformed data\n\treturn g.transform(ydata)\n\n# Gaussian's original data"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\timport scipy.ndimage as N\n\tfrom utils.validation import valid_data, validate_args\n\n\tif(data['sigma'] <= 0):\n\t\tprint(\"sigma must be a positive number\")\n\t\treturn False\n\n\tsize = int(data['size'])\n\tif(size!=N.dtype.width):\n\t\tprint(\"dataset'sigma' and data'size' must have the same type\")\n\t\treturn False\n\n\tx = data['x']\n\ty = data['y']\n\n\tsigma = data['sigma']\n\n\t"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tX = [0.1, 0.1, 0.2, 0.2]\n\tY = [0.1, 0.1, 0.2, 0.2]\n\tX_norm = norm.rvs(X, size=X.size)\n\tY_norm = norm.rvs(Y, size=X.size)\n\tsigma = np.sqrt(np.sum(X_norm*Y_norm))\n\tX_centered = np.around(X/sigma,1)\n\tX_centered = X_centered[:,1"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tsigma_pos = 0.1\n\t# Generate points & weights\n\tX, y = np.meshgrid(np.linspace(-10, 10), np.linspace(-10, 10))\n\txx, yy = np.meshgrid(np.linspace(-1, 10, 20), np.linspace(-1, 10, 20))\n\tweights = np.exp(-(xx*xx + yy*yy) / 12)\n\t\n\t# Construct the Gaussian\n\tG = gaussian_centered(X, weights, sigma_pos=sigma_pos)\n\t\n\t"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tsigma = 0.1\n\t# Make sure it is not None. Some values can not be None, which it is anyway\n\t# to make sure.\n\tassert(sigma == None)\n\t# Get the centers, and also make sure they are the same.\n\tkmeans_obj = kmeans(centers, sigma=sigma, sample_num=10000)\n\tassert(kmeans_obj[\"centers\"][0] == 0.2)\n\tassert(kmeans_obj[\"centers\"][1] == 0.1)\n\t\n\t# Make sure it is not"}]}
{"task_id": "62ece4992e6aefcf4aabbd8a", "prompt": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tfrom collections import namedtuple\n\timport os\n\tfrom.utils import Utils\n\tfrom.utils import makepath\n\tLog = Utils.Log()\n\tLog.Init()\n\t\n\tlogger = logging.getLogger()\n\tlogger.setLevel(logging.DEBUG)\n\t\n\t\n\tfor srcname, path, filename in self.Configurations:\n\t\tif not os.path.isfile(filename):\n\t\t\tLog.Fatal('Configuration file \"%s\" not found' %(filename))\n\t\telse:\n\t\t\tsourcefile = None\n\t\t\tfor fname in Utils.Get"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t# Get the configuration from disk (if applicable there is a file in the directory containing\n\t# the configuration)\n\tlog.debug('Getting configuration from directory')\n\tif not osp.exists(osp.join(conf_dir, cnf_filename)):\n\t\traise OSError('Configuration file for %s not found' % cnf_filename)\n\t\tlogging.error(\"Configuration file for %s not found\", cnf_filename)\n\t(log.debug('Configuration file for %s found', cnf_filename))\n\tconfiguration_dict = {}\n\tfor"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tif len(args)!= 1:\n\t\treturn {\"Error\": \"Must provide one filename\"}\n\tparser.add_argument(argparse.ArgumentParser().add_argument, \"filename\", help=\"Config file to load\")\n\tlogging.addLevelName(logging.INFO, logging.DEBUG)\n\n\tparser.add_argument(argparse.ArgumentParser().add_argument, \"config\", nargs='+', help=\"Configuration file\")\n\treturn parser.parse_args(args)[0]\n\n\n#\n# Parse an input file\n#\n# Returns the configurations parsed in"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tlogging.basicConfig(level=logging.INFO)\n\tlogging.getLogger('bayeslite.experiment.loadconfig').setLevel(logging.WARNING)\n\tfilenames = config_files.config_filenames\n\tfilenames.sort()\n\n\tdef validate(filename):\n\t\t# Check if config file is valid. It should be in the form:\n\t\t# config = xxx.conf\n\t\twith open(filename, 'r') as f:\n\t\t\tconfig = dict(f.read().splitlines())\n\t\tresult = {}\n\t\tif len(config) <"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tif log is None:\n\t\tlog = logging.getLogger(\"pyparsing\")\n\t\tlog.setLevel(logging.DEBUG)\n\tlog.name = \"main\"\n\tlog.debug('Parsing config files')\n\n\t_log = get_log()\n\t_parse_conf = dict()\n\t_logger = logging.getLogger(\"pyparsing\")\n\t_logger.setLevel(_log.debug0)\n\t_logger.debug('Config files parsed. Processing them...')\n\tfor filename in filelist:\n\t\ttry:\n\t\t\t_logger.debug(\"\\"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t# Parse the filenames and validate each file\n\tfiles = {}\n\tfrom xml.parsers import safestringfindall\n\tfor configFile in configFiles:\n\t\tparsed = {'configFilename': configFile}\n\t\tparsed.update(parseConfigFile(configFile))\n\t\tlog.warning('Using config file \"%s\"', configFile),\n\t\tif parsed['configFilename'] not in files:\n\t\t\tlog.warning('Loading %s', configFile)\n\t\t\tfiles[parsed['configFilename']] = parseConfigFile(configFile)\n\t\tif fileExists(files"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tlogging.basicConfig(format='%(asctime)s %(levelname)s %(message)s', level=logging.INFO) #pylint: disable=expression-not-assigned\n\tlogging.info('Reading configuration files')\n\ttry:\n\t\tconfig_filename_list = config.read_configuration_files()\n\texcept EnvironmentError, msg:\n\t\traise Exception('Could not load configuration files. %s' %msg)\n\n\tconfig_filenames, error_string__ = [], None\n\tfor cf in config_filename_list:\n\t\tconfig_filenames."}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\twith open(f'{configuration_directory}/config_file_config.txt', 'r') as config_file:\n\t\tconfig = dict(logging.ConfigDict())\n\t\tcontent_lines = config_file.readlines()\n\n\t\tfor line in content_lines[1:]:\n\t\t\tline = line.strip()\n\t\t\tif not line:\n\t\t\t\tcontinue\n\t\t\titem_start, item_end = '', ''\n\t\t\tif line.rfind(':')!= -1:\n\t\t\t\titem_end = f'{line.rfind(':') +"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tlogging.basicConfig()\n\tlogging.addLevelName(logging.INFO, \"INFO\")\n\tparser = configparser.ConfigParser()\n\tparser.read(open(fileNames))\n\tparser.add_section(\"Config\")\n\tfor l in parser.sections():\n\t\tlogging.log(logging.INFO, str(l) + \", \" )\n\tlogging.removeLevelName(logging.INFO \"ERROR\")\n\tlogging.basicConfig()\n\tlogging.startLogging(\"config_sequence\", config_root)\n\tlogging.addLevelName(logging.INFO, \""}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tdef load(self, filename, use_cache=True) -> \"Configurator\":\n\t\t\"\"\"\n\t\tGiven a sequence of configuration filenames and a cache object (which will be the cache of\n\t\tthe parsed configuration, or the sequence of logging.LogRecords returned by load()) yield:\n\t\t\tdict of configuration filename to corresponding parsed configuration,\n\t\t\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\t\"\"\"\n\t\ttry:\n\t\t\twith Config(filename=filename, cache=use_cache) as config:\n\t\t\t\treturn config.config\n\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd8b", "prompt": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isinstance(obj, bytes):\n\t\treturn obj\n\tif isinstance(obj,str):\n\t\treturn obj.encode('utf-8')\n\treturn obj\n\ndef is_null(value):\n\t\"\"\"\n\t\tChecks if ``value`` is a valid null value\n\t\"\"\" \n\tif value!= None and value.strip()!= '':\n\t\treturn True\n\treturn False\n\t\ndef check_value(value):\n\t\"\"\"\n\tChecks if ``value`` is a valid value.\n\t\"\"\"\n\tval = value\n\tif not isinstance(val"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif type(obj) in [bytes, bytearray]:\n\t\treturn obj\n\tif isinstance(obj, str):\n\t\t# Convert Unicode bytestrings to bytes\n\t\tif isinstance(obj, (str, unicode)):\n\t\t\tobj = obj.encode('utf-8')\n\t\telif isinstance(obj, bytes):\n\t\t\tobj = obj.decode('utf-8')\n\t\telif isinstance(obj, (list, tuple, set)):\n\t\t\treturn []\n\t\telse:\n\t\t\traise TypeError(obj)\n\telif type(obj) == dict:"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isinstance(obj, str):\n\t\treturn obj.encode(\"utf-8\")\n\telse:\n\t\traise TypeError(\"obj must be str or bytes object\",obj) \n\t\n\ndef test(arg, arg2, test_name):\n\tif arg is True:\n\t\treturn arg2\n\treturn test_name\n\n\ndef get_string(data, startindex, count):\n\t\"\"\"\n\t@return: None or a string of bytes from the specified slice of the specified range. \n\t@type data: list of data\n\t@type startindex: int"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isinstance(obj, str):\n\t\tres = obj.encode(encoding)\n\telse:\n\t\tres = obj.tostring(encoding)\n\treturn res\n\ndef _normalize(obj, encoding = 'utf-8'):\n\tif isinstance(obj,str):\n\t\treturn obj.encode(encoding)\n\treturn obj\n\ndef _test():\n\timport os, sys\n\twith open(sys.argv[1]) as f:\n\t\ts = f.read()\n\t_b = u''.join(s)\n\ta = _normalize"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\treturn obj.encode(encoding, errors)\n\n\ndef object_to_string(obj):\n\t\"\"\"\n\tConverts an object to a string (UTF8 encoded)\n\t\"\"\"\n\treturn (obj).encode(encoding, errors)\n\n\ndef string_to_object(str):\n\t\"\"\"\n\tConverts a string to a Python class.\n\t\"\"\"\n\treturn str.decode(encoding, errors)\n\n\ndef load(filepath):\n\t\"\"\"\n\tLoads the file located at ``filepath``.\n\t\"\"\"\n\tf = open(filepath, \"rb"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isinstance(obj,str):\n\t\treturn obj\n\telse:\n\t\treturn binascii.unhexlify(obj).decode('utf-8')\n\n\t\n\t\n# This function returns the bytes object corresponding to the\n# object in the given dictionary and the given key\ndef dict_get(obj,key):\n\t\"\"\"Return the bytes object corresponding to the given key in the given dictionary\"\"\"\n\treturn get_dict_dict_bytes(obj,key)\n\t\t\n\t\n\t\n# This function returns the bytes object corresponding to the\n# dictionary and the given key in the"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t# from __future__ import unicode_literals as wfl\n\t# with open('unicode_list.txt','r') as f:\n\t# \tres = f.read().split(':') \n\t# if len(res) > 1  else None\n\treturn res[0] if isinstance(obj, str) else obj.decode('utf8')\n################################################################################\nclass get_object(object):\n\t###############################################################################\n\tdef __init__(self, a_list):\n\t\tif sys.version_info >= (3, 0) :\n\t\t\ta_list"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isNumeric(obj): \n\t\tif isinstance(obj, str): \n\t\t\treturn s(obj)\n\t\telse: \n\t\t\treturn obj.encode('utf8')\n\telif isinstance(obj, (numpy.integer, numpy.longinteger, numpy.longlong)): \n\t\t# numpy.integer or numpy.longlong is numpy.string_\n\t\t# and string is not.\n\t\tobj = obj.item()\n\t\tif isinstance(obj, str): \n\t\t\treturn s(obj)\n\t\telse: \n\t\t\treturn obj.encode('utf8"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isinstance(obj, str):\n\t\ttry:\n\t\t\tobj = obj.encode('utf-8')\n\t\texcept UnicodeEncodeError:\n\t\t\tobj = obj.encode('latin1').decode('utf-8')\n\treturn obj\n\ndef get_file(in_path, out_path, ext):\n\t'''Creates the file in in_path.  \n\tReturns :\n\t  1 -> True (true)\t: if file with this suffix is already there\n\t\t\t\t\t\t\t\t(this isn't a problem as the function\n\t\t\t\t\t\t\t\twill try to create the file in"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isinstance(obj, str):\n\t\treturn BytesIO(obj)\n\treturn obj\n'''\n\ndef run(function, *args, **kwargs):\n\t\"\"\"\n\tSimple function used to run the program.\n\t\"\"\"\n\tresult = getattr(sys.modules[__name__], function)(*args, **kwargs)\n\treturn result\n\ndef load_bytes(filepath):\n\t\"\"\"\n\tWrapper around `sys.get_bytes`.\n\tWill either return the bytes object corresponding to filepath,\n\tor raises the `IOError` if there was an error reading the file."}]}
{"task_id": "62e60723d76274f8a4026b76", "prompt": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tstart=ticks/86400000000000\n\tend=ticks%86400000000000\n\treturn Time((start,end,0,0,0,0,0,0,tz))\n\ndef to_ticks(cls, val):\n\t\"\"\"\n\tConvert a time/interval object to nanoseconds (nanoseconds since midnight)\n\t\n\t:param val: time\n\t:type val: time\n\t:return: nanoseconds since midnight\n\t:rtype: int\n\t\n\t:raises ValueError: if interval cannot be represented as ticks\n\t\"\"\"\n\tstart=val.start.to_interval()\n"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif ticks < 0:\n\t\traise ValueError('ticks must be >= 0, got %r' % ticks)\n\t\n\treturn datetime.time(ticks * 1e9, tz)\n\n\n#\n# A list of dates, which are the same in the time format used by datetime module.\n#\nclass DateList(list):\n\t\"\"\"\n\tA list of (x, y) pairs in days-hour-minute-second-sec format.\n\t\n\t| DateList\n\t|---------------------\n\t| \n\t|  - datetime module:\n\t|  - *seconds"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif ticks == 0:\n\t\traise ValueError(\"%r cannot be zero.\" % secs)\n\t\n\tticks = abs(ticks) if ticks > 0 else 0\n\tsecs  = ticks / 1000000000\n\tticks = (ticks / 1000000) % 100\n\tmicros = ticks / 1000\n\t\n\tif secs < 0:\n\t\tsecs += 1\n\t\tticks -= 1\n\t\n\t# Get timezone if not provided\n\tif tz is None:\n\t\t# Force UTC\n\t\tif tzmod is not None:\n\t\t\ttz = tzmod.tzlocal()\n\t\ttz"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\ttz = tz or pytz.UTC\n\tif ticks % timedelta(9)!= 0:\n\t\traise ValueError('ticks value is out of bounds')\n\t\n\tmin = time()\n\treturn time(max(ticks, min), 1)  # ticks\n\n#include <stdio.h>\n#include <unistd.h>\n\nint main() {\n        int x;\n        int n;\n        int p = 1;\n\n        while(scanf(\"%"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif (ticks >= 0) and (ticks < 1):\n\t\tfrom math import floor // needed to use ceil\n\t\treturn cls.from_secs(secs=ticks)    \n\tif (ticks == -1):\n\t\treturn cls.from_sec(-pts/1e9)  # -1.day in Python 2.6\n\tif (ticks < 0):\n\t\traise ValueError, \"ticks must be at least 0\"\n\thours, ticks = divmod(ticks, 24*3600)\n\tmins, ticks = divmod(ticks, 60*60"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif not ticks:\n\t\traise ValueError(\"ticks cannot be 0\")\n\tif ticks < 0:\n\t\tticks = timedelta(days=1)\n\treturn cls(datetime.fromtimestamp(ticks*timedelta(microsecond=1000000)).timestamp())\n\ndef from_date(date):\n\t\"\"\"\n\t\tCreate time from date string\n\t\"\"\"\n\tif not date:\n\t\traise ValueError(\"date cannot be empty\")\n\treturn FromDate(date)\n\ndef from_calendar(calendar):\n\t\"\"\"\n\t\tCreate time from calendar\n\t\"\"\"\n\t"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t#\n\t#  if ticks < 0 or ticks > 86400000000000 :\n\t#  raise ValueError\n\t#\n\t#\tCreate time object from UTC ticks (ticks * 1000)\n\t#\n\t#  We have 3 cases: hours of 0 and minutes of 0,\n\t#\tthen add hours of 0, then add minutes.\n\t#\n\t\n\tif ticks < 0:\n\t\traise ValueError(\"hours of 0 or minutes of 0 (0,000)\")\n\tif ticks > 86400000000000:\n\t\traise ValueError(\"hours of 0 or minutes of 0 (86400000000000"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\treturn cls(time.time(), ticks)\n\ndef utc(cls, seconds, microseconds=0):\n\t\"\"\"\n\tCreate a time from microseconds.\n\n\t:param seconds: microseconds\n\t:type seconds: float\n\t:param microseconds: nanosecond part of the new time\n\t:type microseconds: int\n\t\n\t:rtype: Time\n\t\"\"\"\n\treturn cls(time.time() + microseconds * (1e6 * SECOND_IN_SECOND))\n\n\n#------------------\n# TimeDelta\n#------------------\n\nclass TimeDelta(Time):\n\t\"\"\"\n\t"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tsecs = ticks // Ticks\n\tticks %= Ticks\n\treturn cls(secs, ticks, tz)\n\n/****************************************************************************\n**\n** Copyright (C) 2016 The Qt Company Ltd.\n** Contact: https://www.qt.io/licensing/\n**\n** This file is part of Qt Creator.\n**\n** Commercial License Usage\n** Licensees holding valid commercial Qt licenses may use this file in\n** accordance with the commercial license agreement provided with the\n** Software or, alternatively, in accordance with the terms contained in\n** a written agreement between you and The Qt Company. For licensing"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\ttry:\n\t\tticks += Time.NANO_TIME\n\texcept:\n\t\traise ValueError()\n\t\t_ = 1.0 # used to prevent the compiler from recognizing a function call that does not evaluate to True \n\t\t_ = float(ticks)\n\t\tif 0.7 <= _ <= 6.9:\n\t\t\treturn Time(secs=_+0.001, nsec=_)  # convert to seconds since midnight in a timespan\n\t\traise ValueError()\n\t\t\n\treturn cls.from_epoch(ticks=ticks, tz=tz)\n\n@overload\n"}]}
{"task_id": "62e60873d76274f8a4026bd8", "prompt": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tversions = {}\n\tfor version, handler_class in bolt_protocol_handlers.items():\n\t\tif protocol_version is not None and version not in protocol_version:\n\t\t\ttry:\n\t\t\t\tversion = int(version)\n\t\t\texcept ValueError:\n\t\t\t\t# Invalid version specification\n\t\t\t\traise TypeError(\"Invalid protocol version specified\")\n\t\telse:\n\t\t\t# We haven't received a given protocol version.\n\t\t\tif protocol_version is not None:\n\t\t\t\traise TypeError(\"Invalid protocol version specified\")\n\t\t\tversion_available = None\n\t\t\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t    protocol_version = (0,) * len(BOLT_PROTOCOL_VERSIONS)\n\t\n\tprotocol_version = tuple(protocol_version)\n\t\n\treturn cls.__get_handlers(protocol_version)\n\n\nclass ServerState(object):\n\t\"\"\"\n\t  A server state holds a connection to a server and a list of\n\t  server commands that were processed. To run a server, start\n\t  the server with a list of commands, then add their result to\n\t  the server state so that they get processed.\n\t\n\t  Note"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tassert isinstance(protocol_version, tuple), \\\n\t\t(\"protocol_version must be a tuple; %r\" % protocol_version)\n\tprotocols_to_handlers = {}\n\tprotocol_major_version = version_from_tuple(protocol_version)\n\tfor version in protocol_version:\n\t\tif isinstance(version, int):\n\t\t\tif protocol_major_version == 1:\n\t\t\t\tprotocol_major_version = version\n\t\t\telse:\n\t\t\t\tprotocols_to_handlers[version] = \\\n\t\t\t\t    protocol_handlers[version % protocol"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tprot_vers = {}\n\t\n\tif protocol_version is None:\n\t\tfor protocol in BoltProtocol.__instances__[-1]:\n\t\t\tif protocol.version not in protocol_vers:\n\t\t\t\tprot_vers[protocol.version] = protocol\n\telse:\n\t\tprot_vers[protocol_version] = protocol_handlers[protocol_version]\n\t\n\tfor protocol in prot_vers.values():\n\t\tfor meth in protocol.__class__.__mro__:\n\t\t\tmro = cast(MethodSpec, meth)\n\t\t\tif meth.name == 'handle"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tversion = protocol_version\n\tif protocol_version is None:\n\t\treturn cls.protocols\n\tfor version_tuple, cls in cls.protocols.items():\n\t\tversion_tuple = tuple(version_tuple)\n\t\tif version_tuple in cls._supported_versions:\n\t\t\treturn cls.protocols[version_tuple]\n\traise TypeError(f'{cls.__name__}: protocol version not valid') \n\n# -*- coding: utf-8 -*-\n# Generated by Django 1.11.14 on 2018-11-02 03:09\nfrom __future__ import"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_version = tuple([i.version for i in BoltProtocols])\n\t\n\tversion_handler = BoltVersionDict(protocol_version)\n\tversion_handler.update(protocol_handlers(BoltProtocols))\n\t\n\treturn version_handler\n\n\nclass BoltVersionDict(MutableMapping):\n\t__slots__ = (\n\t\t'start',\n\t\t'end',\n\t\t'clear',\n\t\t)\n\n\tdef __init__(self, protocol_version=None):\n\t\tsuper().__init"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t# Do not make this function return a dictionary. It has a potential\n\t# to return unexpected results for invalid parameters, since it is\n\t# likely a bug in Bolt.\n\tif not callable(protocol_version):\n\t\tprotocol_version = tuple(protocol_version)\n\n\tprotocol_handlers = {}\n\n\tfor name, version in _PROTOCOL_MAP.items():\n\t\tprotocol_handlers[version] = protocol_handlers[version].get(name,\n\t\t\t\t\t\t\t\t\t\t    cls(name, version))\n\n\treturn protocol_handlers\n\n\ndef get_"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_version = (\n\t\t\t\t(3, 5), (4, 7), (5, 6), (6, 14), (8, 15), \n\t\t\t\t(12, 16), (13, 17), (14, 19)\n\t\t)\n\telse:\n\t\tif version.parse(protocol_version[0]) > version.parse(\n\t\t\t\tprotocol_version[1]):\n\t\t\t# Bolts <= 3.0\n\t\t\treturn {\n\t\t\t\t0 : ProtocolHandlerWithVersion(\n\t\t\t\t\tprotocol_"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_versions = tuple(x for x in _KNOWN_PROTOCOL_VERSIONS.values())\n\t\tprotocol_version = max(\n\t\t\ti for i, x in enumerate(protocol_versions)\n\t\t\tif protocol_versions[(i - 1) % len(protocol_versions)] == x\n\t\t)\n\t\tprotocol_version_str = \"%s@%s\" % (_SUPPORTED_PROTOCOL_VERSIONS[protocol_version], protocol_version)\n\telse:\n\t\tprotocol_versions = protocol_version\n\t\tif"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\treturn {}\n\n\n_handlers = {}\ndef __init__(self, protocol_version=None):\n\t\"\"\"\n\t\t:proto protocol_version: tuple identifying a specific protocol,\n\t\t\tor \"None\", which matches all versions.\n\t\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_version = (3, 5)\n\tassert (protocol_version is not None), 'protocol version \"None\" is not valid'\n\tif protocol_version not in _handlers.keys():\n\t"}]}
{"task_id": "62e60e3bd76274f8a4026d1a", "prompt": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tif values == []:\n\t\treturn Bombed()\n\tbookmarks = []\n\tprev_len = len(values[0])\n\tfor val in values:\n\t\tif len(val) > prev_len:\n\t\t\tbookmarks.append(BOM(val))\n\t\tprev_len = len(val)\n\treturn Bombed(b=bookmarks)\n\n\nclass Bombed(MutableMapping):\n\t\"\"\"\n\tCreates a bookmarks object from a dictionary of raw bookmark values.\n\t\n\tBookmarks are used as keys in a dictionary"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\treturn cls(values[1:-1])\n\ndef items_from_raw_values(cls, values):\n\t\"\"\" \n\t\tCreate list of bookmarks from a raw bookmark string \n\t\tvalues.\n\t\"\"\"\n\traw_vals = values.split(',')\n\tbk = cls(raw_vals[1])\n\tfor i in range(len(vals)-1):\n\t\tbk |= BookmarksItems(vals[i])\n\treturn bk\n\n# -*- coding: utf-8 -*-\n# This implementation accepts an image file as input (only the\n# first"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t_raw = values.find(\"\")\n\tif _raw > -1:\n\t\tvalues = values[_raw:]\n\tbookmarks = cls(0, None, 0, values)\n\treturn bookmarks\n\n\nclass Bookmarks(bytes):\n\t\"\"\"\n\tThis class provides an implementation of the ``bstr`` and ``Binary``\n\tclass interfaces. The ``Bookmarks`` class handles bookmarks. For\n\texample, it handles parsing and output to an instance of\n\t:class:`BiffViewer.Bookmarks`.\n\t\n\tThis is a subclass of :class:`bytes`,"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tbookmarks = cls()\n\tfor item in values:\n\t\t# The raw values might be a multi-line string, which would result in a single line, which is\n\t\t# a problem in some notebooks\n\t\tif not item:\n\t\t\tcontinue\n\t\t# Split the string on each newline, delimited by either tabs or newlines. All tab positions\n\t\t# are in the same line.\n\t\tline = item.rstrip('\\n').split('\\t')\n\t\tindent = len(line[-1])\n\t\tblock = []\n\t\tnew_"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\ttry:\n\t\treturn cls(from_iter(values))\n\texcept:\n\t\traise Exception('error decoding input')\n\ndef from_raw_dict(data):\n\tif 'raw' in data:\n\t\treturn from_raw_values(dict, data['raw'])\n\telse:\n\t\treturn from_iter(data['values'])\n\n# Helper functions for bookmark.\ndef from_iter(iter):\n\tresult = tuple()\n\twhile True:\n\t\ttry:\n\t\t\titem = iter.next()\n\t\texcept StopIteration"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tif not values:\n\t\treturn None\n\tfor item in values:\n\t\tif not item:\n\t\t\tcontinue\n\t\ttry:\n\t\t\treturn cls(item)\n\t\texcept AttributeError:\n\t\t\tpass\n\treturn None\n\n# -*- coding: binary -*-\n\nrequire'msf/core'\n\n##\n# This class describes a database connection to a database.\n\n# MSF uses Ruby's core and Ruby extensions to the database connection\n# architecture, but it can also work with other types of database drivers.\n\nmodule Msf::DBManager::Database"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t_type = type(values)\n\tif _type is bytes:\n\t\treturn Bookmarks(from_bytes(values))\n\tif _type is unicode:\n\t\treturn Bookmarks(from_unicode(values))\n\treturn cls(from_list(values))\n\n\n# Bookmarks represent the bookmarks object in MacOS. Here's how it's created:\n#\tBookmarks\n#\t\t- uses the bytes object for MacOS\n#\t\t- provides some helper functions and methods for conversion\n#\t\t- creates a binary object from a list of bytes\n#"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tassert isinstance(values, Iterable)\n\tassert all(isinstance(value, bytes) for value in values)\n\n\treturn cls(bytes.fromhex(''))\n\n\n#---- Bookmark ----\n\n\nclass Bookmark:\n\t\"\"\"\n\tA representation of an open file system object.\n\t\"\"\"\n\t\n\t__slots__ = ('_offset', '_size', '_name', 'get_size')\n\t\n\tdef __init__(self, offset, size, name, get_size):\n\t\tself._offset = offset\n\t\tself._size = size\n\t\tself._"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tif values == \"\":\n\t\treturn None\n\traw = valuemapping.make_from_raw_values(values)\n\treturn from_raw_values(cls, raw)\n\ndef from_raw_values(cls, raw):\n\tobj = cls(raw)\n\treturn obj\n\nif not py2:\n\tdef from_raw(cls, raw):\n\t\treturn from_raw_values(cls, raw.encode(\"ascii\"))\nelse:\n\tdef from_raw(cls, raw):\n\t\t# Python 2 & 3 will treat non-byte"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\treturn bookmarksFromSerializedValues(values)\n\ndef from_serialized_string_values(values):\n\t\"\"\"\n\t\tCreate a Bookmarks object from a list of serialized string values.\n\t\n\t\tNote that this constructor does not read the values but instead uses\n\t\tthat list of raw values to create a bookmarks object.\n\t\t\n\t\t:param values: Serialized string values (serialized bookmarks)\n\t\t:type values: Iterable[str]\n\t\"\"\"\n\traise NotImplemented(\"Bookmarks.from_serialized_string_values()\")\n\ndef from_serialized_string("}]}
{"task_id": "62b87b199a0c4fa8b80b354c", "prompt": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif hasattr(seq, 'type'):\n\t\treturn seq, getattr(seq, 'type')\n\telif hasattr(seq, 'value_type'):\n\t\treturn seq, seq.value_type\n\telif hasattr(seq, 'items'):\n\t\t# seq_items is a special case that will only be called\n\t\t# when len(seq) == 1\n\t\tif seq.__len__() == 0:\n\t\t\traise TypeError(\"Cannot determine sequence length of empty sequence.\")\n\n\t\t# for seq in ('Hello', range(3)):\n\t\t# \tseq"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif seq.endswith('>'): raise ValueError('`seq` ends with '>'')\n\t# (seq, type)\n\tif isinstance(seq, type):\n\t\tif seq == 'None':\n\t\t\treturn (None, None)\n\t\treturn (seq, type)\n\t# (seq, type [, start]  : end)\n\tif isinstance(seq, (list, tuple)):\n\t\treturn _get_seq_with_type(seq[0:0]) + (seq[-1], seq[0][1:])\n\telif isinstance(seq, str):\n"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif seq is None or isinstance(seq, types.StringTypes):\n\t\treturn seq, 'S'\n\tif not isinstance(seq, Iterable):\n\t\ttry:\n\t\t\treturn (seq, type(seq))\n\t\texcept:\n\t\t\treturn tuple(seq), 'S'\n\tt_name = None\n\tif seq[0] in {'[': t_name, '%': t_name}:\n\t\ttry:\n\t\t\tt_name = None\n\t\t\tfor i in seq.index:\n\t\t\t\tif i!= 0:\n\t\t\t\t\tt"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif isinstance(seq, (SequenceType, SEQUENCE_TYPES)):\n\t\treturn dict(seq), StringType\n\telif isinstance(seq, SEQUENCE_TYPES):\n\t\titem = dict(seq)\n\t\tif isinstance(seq, (tuple, list)):\n\t\t\titem[\"seq\"] = type(seq)\n\t\t\titem[\"type\"] = _get_seq_with_type(seq[-1], bufsize=bufsize)\n\t\t\treturn item, item[\"type\"].__name__\n\t\treturn item\n\n\ndef _coerce(type_, obj, funcmap, *, strict=False"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif isinstance(seq, type):\n\t\tif issubclass(seq, (BaseSeq)): # if it is a subclass\n\t\t\treturn seq, 'Seq'\n\t\telse:\n\t\t\treturn seq._get_seq_with_type(bufsize=bufsize), 'Seq'\n\t else:\n\t\treturn seq, 'Seq'\n\ndef _format_valseq(value):\n\tif isinstance(value, Sequence):\n\t\tvalue = _format_valseq(value)\n\treturn '(%s,..., %r)'%(value, )\n\ndef format_"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif _PY3:\n\t\tres = seq\n\telse:\n\t\tres = (_asbytes(seq),)\n\tif _PY3:\n\t\treturn res, \"str\"\n\treturn res, \"str\"\n\n\ndef _format_class_value(v):  # cpython3\n\treturn _byteswap(v, -1)\n\n\nclass _BaseSeqType(object):\n\t\"\"\"\n\tRepresents a data structure that can create sequences of\n\tdifferent lengths and types.\n\n\tThe sequence can be initialized from a list, string,"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif not isinstance(seq, (list, tuple)):\n\t\t# A single argument, a sequence\n\t\treturn (seq, 0)\n\tif isinstance(seq, bytes):\n\t\t# A sequence of (bytes, bytearray)\n\t\tfirst, rest = seq, seq\n\t\treturn (first, 0)\n\tif isinstance(seq, unicode_type) \\\n\tand len(seq)!= 1:\n\t\t# A sequence of one character, a sequence\n\t\treturn (seq, 0)\n\t# Try to convert to a'sequence' subclass\n\t"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif is_sequence(seq):\n\t\treturn (list(seq), 'i')\n\n\tif hasattr(seq, '__iter__'):\n\t\tfor subseq in _get_seq_with_type(seq, bufsize):\n\t\t\tyield subseq\n\t\tyield from _get_seq_with_type(seq, bufsize)\n\n\telif hasattr(seq, '_sequence_attrs') and seq._sequence_attrs is not None:\n\t\tfor subseq in _get_seq_with_type(seq._sequence_attrs, bufsize):\n\t\t\tyield subseq"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif isinstance(seq, str):\n\t\tseq = seq.split()\n\tif isinstance(seq, (list,tuple)):\n\t\tif len(seq)==1:\n\t\t\tif seq[0].isupper(): seq=[\"-\"*4, seq[0].upper()]\n\t\t\tfor p in seq:\n\t\t\t\tif p.isupper():\n\t\t\t\t\tif p not in seq[:2] or (not len(seq) and p in seq[:2]):\n\t\t\t\t\t\treturn p, \"SEQUENCE\"\n\t\t\treturn seq[-2], \"SEQUENCE\"\n\t\telse:"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif isinstance(seq, basestring):\t\t\t\n\t\tif hasattr(seq, \"items\"):\n\t\t\t# sequence with items\n\t\t\treturn seq, type(seq)\n\t\telif hasattr(seq, '__getitem__'):\n\t\t\t# sequence with items, first item is item\n\t\t\tseq = seq[0]\n\t\t\ttypestr = \"{%s}\" % type(seq).__name__\n\telif not hasattr(seq, \"__iter__\") and hasattr(seq, \"__len__\"):\n\t\t# sequence with length items\n\t\tif type(seq).__name__=='dict':"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3581", "prompt": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tfrom. import histogram\n\tif other is None:\n\t\treturn histogram.scale(self)\n\tif type(other) is str:\n\t\tother = parse_scale(other)\n\n\tself_scale = histogram.scale(self)\n\tother_scale = histogram.scale(other)\n\n\tself_scales = np.multiply if self_scale == 0 else self_scale\n\tother_scales = np.multiply if other_scale == 0 else other_scale\n\n\tself_total, self_count = np_histogram(self, *self_scales)"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is not None:\n\t\tif self.scale == other.scale: # Both scales are the same...\n\t\t\treturn self\n\t\telif self.scale > 0 and other.scale == 0:\n\t\t\tscale = self.scale\n\t\telif self.scale == 0 and other.scale!= 0:\n\t\t\tscale = other.scale\n\t\telse: # Either both scales are not the same...\n\t\t\tscale = min(self.scale, other.scale)\n\t\t\tif self.scale!= 0 and other.scale!= 0:\n\t\t\t\tfscale"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tnew_scale = self.get_scale()\n\t\n\tif not (self and other):\n\t\traise ValueError('Either *self or *other* must be provided')\n\t\t\n\t# we set our scale to something of what's already in the histogram\n\t# (even if they are equal!)  so that we can recompute it later...\n\tif self.scale is None:\n\t\tnew_scale = other.get_scale()\n\t\t\n\telif other.scale is None:\n\t\tnew_scale = self.get_scale()\n\t\t\n\telse:\n\t\told"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t_warn_if_not_recompute(recompute)\n\t\n\tself.scale = other.scale\n\treturn self\n\n\ndef histogram(self, ndigits=8, min=0, max=10, kind=Kind.int64):\n\t\"\"\"\n\tInitialize *self* with range *N* and scale *N*\n\t\n\t:param ndigits: number of digits to use for histogram.\n\t\n\t:param min: lower bound for histogram\n\t\n\t:param max: upper bound on histogram\n\t\n\t:param kind: kind of histogram\n\t\n\t:raises:"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tlog = self.log_p\n\t\n\tif other is None:\n\t\tother = _NoneType()\n\n\tself._check_scale_change(other)\n\tself._check_not_none()\n\t\n\tif self._is_new:\n\t\tself._resize_cache()\n\n\t# compute or set new scale of self\n\tif self.is_empty:\n\t\tself._scale = 0.0\n\telse:\n\t\tself._scale = self._compute_scale()\n\t\n\tself.scale_is_computed = True\n\n\t"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif self.scale not in (None, 0):\n\t\treturn\n\t\t\n\tscales = []\n\tif other is None:\n\t\tother = self.clone()\n\n\tdef normalize():\n\t\tif self.scale > 0:\n\t\t\tscale_factor = float(self.scale) / float(other.scale)\n\t\t\treturn other - self * scale_factor\n\t\telif other.scale == 0:\n\t\t\treturn (other, self)\n\t\n\tscale_factor = normalize()\n\tfor i, v in enumerate(other):\n\t\tw ="}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tself.scale = self.calc_scale(other)\n\treturn self.scale\n\n\ndef copy_histogram(self, other):\n\t\"\"\"\n\tCopy histogram.\"\"\"\n\tcopy = self.__class__()\n\tcopy.histogram = self.histogram.copy()\n\tcopy.bins = [bin.copy() for bin in other.bins]\n\tcopy.counts = [self.counts[:] + other.counts]\n\treturn copy\n\n\ndef make_histogram(values=None, step=None, interpolation=None,\n\t\t    bins=None, **kwargs):\n"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif not self.scale:\n\t\tif other == None:\n\t\t\tself._scale_compute(recompute)\n\t\t\tself.scale = self._scale\n\t\telse:\n\t\t\tself._scale_compute(recompute)\n\t\t\tself._scale = other\n\telif (other and not isinstance(other, float)):\n\t\tother = float(other)\n\t\t# Scale is zero if one was not computed\n\t\tself._scale_compute(recompute)\n\t\tself._scale = other\n\t\n\treturn self._scale\n\ndef add(self"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif type(other) is str:\n\t\tassert'scale' in histogram, \"must provide explicit scale\"\n\t\tassert'recompute' not in histogram, \"must not perform rescaling\"\n\t\tother = self._recompute(other)\n\tself.__rescale(other)\n\treturn self\n\n#-----------------------------------------------------------------------------\ndef __recompute(self, other=None):\n\t\"\"\"\n\tCompute and set scale and/or rescale other to match this one.\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf it *other"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif self.is_null():\n\t\traise RuntimeError(\"Cannot rescale histogram with no data\")\n\tif other is None:\n\t\treturn self._scale\n\ttry:\n\t\treturn self._scale = int(other)\n\texcept Exception:\n\t\treturn self._scale\n\ndef __eq__(self, other):\n\treturn self.is_null() == other.is_null()\n\n\ndef __hash__(self):\n\ttry:\n\t\thash(self._scale)\n\texcept Exception:\n\t\traise TypeError(repr(self))\n\t"}]}
{"task_id": "62b87b519a0c4fa8b80b3583", "prompt": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\tother = GraphComponent()\n\tif self.scale == other.scale:\n\t\tresult = GraphComponent()\n\telse:\n\t\tresult = self.copy()\n\tresult.scale = other.scale\n\treturn result\n\ndef stretch(self, other=None):\n\t\"\"\"\n\tGet or set the stretch of the graph.\n\t\n\tIf *other* is ``None``, return the stretch.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tOtherwise the graph scale will be rescaled"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\t# Get scale\n\t\tscale = self.z * self.y / self.x\n\t\tself.scale = scale\n\t\tif abs(scale) > math.pi / 2.0:\n\t\t\traise LenaValueError(\"Cannot scale graphs into that size.\")\n\t\tif scale > 1e-6:\n\t\t\traise MaxLenError(\"Cannot scale a graph within a graph length\")\n\t\treturn scale\n\t# Set scale\n\tif self.y is None or self.x is None:\n\t\tself.x = other"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tres = self.rescal_(other=other)\n\tif not res:\n\t\tif self.coords_:\n\t\t\tx = self.coords_[-1]\n\t\t\tres = x[0] ** -1\n\t\t\tfor ix in x:\n\t\t\t\tres = self.rescal_(other=ix, res=res)\n\t\telse:\n\t\t\traise ValueError(\"Graph has unknown/zero Scale.\")\n\treturn res\n\ndef center_lonlat(grids):\n\t\"\"\"\n\t\tReturn the center lon and lat coordinates for grids.\n\t\"\"\""}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif not (self or other): raise ValueError()\n\tif other is None: return self.get_rescaled(normalize=True)\n\telif isinstance(other, Number): return self.get_rescaled(other)\n\telse: return self.get_rescaled(normalize=False)\n\n\ndef get_rescaled(normalize=True):\n\t\"\"\"\n\t\tGet/set the scale.\n\t\n\t\tRescaling is done by rescaling only the numeric and positional\n\t\tcoordinates of the nodes.\n\t\tTo get meaningful results, graph"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif self._scales == None: _scale(self)\n\telse:\n\t\tif other: self._scale(*other)\n\t\telse: self._scale()\n\treturn self._scales\n\nclass _scale(object):\n\t\"\"\"\n\t\tRescales coordinates to *u* and converts *to* ``(u.x, u.y, u.z)``\n\t\tcoordinate units as ``u * u.z / u.y``.\n\t\t\n\t\tParameters\n\t\t----------\n\t\tscale : float\n\t\t\tthe scale\n\t\tto : numeric\n"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\tif self['coord'] is None:\n\t\t\traise ValueError(\"the graph has no graph to scale\")\n\t\tother = self\n\tscale = self['coord'].scale()\n\tif other['scale_val'] is None:\n\t\tother['scale_val'] = self['coord'].getScaleValue()\n\tif other['coord'] is None:\n\t\tother = self['coord']\n\tother.scaleBy(scale)\n\treturn {key: scale*other[key] for key in other}\n\n@_dispatch(\n"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif self._is_mutable:\n\t\tself._graph.scale(other)\n\telse:\n\t\tif other is None:\n\t\t\tself._scaled = 2.0\n\t\telif other in [-1]:\n\t\t\traise TypeError('Cannot set an absolute scale for graph')\n\t\telse:\n\t\t\tself.scale((other, other, 0))\n\t\n\treturn self\n\n\ndef translate(self, *coords):\n\t\"\"\"\n\tTranslate the graph by *coords*.\n\t\n\tIf *coords* is ``None``, return the graph.\n\t\n\tIf"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\tself._scale = self.get(\"scale\")\n\t\tself.set_coordinate(list(self.coordinates.items()))\n\telif isinstance(other, dict):\n\t\tother = [self.scale for name, scale in other.items()]\n\t\tself._scale = scale * len(self.coordinates.values())\n\t\tfor value in self.coordinates.values():\n\t\t\tfor (k, v) in other:\n\t\t\t\tif hasattr(v, \"items\"):\n\t\t\t\t\tv[k] = value / len(v)"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\ttry:\n\t\tself.__scale\n\texcept AttributeError:\n\t\tself.__scale = get_scale(self)\n\telse:\n\t\tfor f in ('x', 'y', 'z'):\n\t\t\tif getattr(self,'x') is None:\n\t\t\t\traise self.LenaValueError(\"Graph %s has x coordinate \"\n\t\t\t\t\t\"and no `x.field`\\n                           \"\n\t\t\t\t\t\"or the `x` keyword argument\\n "}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\tother = self._scale\n\treturn other * (self._x_scale or self._y_scale or self._z_scale)\n\n\nfrom.bbox import bbox\n\ndef copy(self):\n\t\"\"\"\n\tReturn a deep copy of this graph.\n\tThis is typically useful when a graph\n\tis being reconstituted and repainted.\n\t\"\"\"\n\treturn Graph()\n\ndef set_coord_range(self, minx, miny, maxx, maxy):\n\t\"\"\"\n\tSet the minimum and maximum"}]}
{"task_id": "62b87b869a0c4fa8b80b35e1", "prompt": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tif not isinstance(hist, histogram):\n\t\traise TypeError(\"hist must be a histogram\")\n\tif not hist.data.dtype.char:\n\t\traise TypeError(f\"hist must be of dtype {hist.data.dtype}\")\n\tif len(hist.data.shape)!= 1:\n\t\tprint(f\"hist should have one element: {hist.data.shape}\")\n\t\traise RuntimeError\n\tif not hist.bounds:\n\t\traise RuntimeError(\"hist must contain at least one histogram bin\")\n\tif not hist.bounds.is_valid:\n\t"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t# check that field names exist\n\tif get_coordinate not in [\"left\", \"right\", \"middle\"]:\n\t\traise ValueError('Coordinate must be \"left\" or \"right\".')\n\t\n\t# check that number of bins doesn't exceed dimension\n\tif len(hist) > histogram.MAX_HITS:\n\t\traise ValueError(\"Histogram is too many bins. \"\n\t\t\t\"Maximum size is %(max_hits)d.\"\n\t\t\t% dict(max_hits=len(hist) // 2 + 1))\n\t\n\tif hist.width!= 1:\n"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\thist = np.asanyarray(hist)\n\tresult = hist_from_histogram(hist, xname=\"x\", field_names=field_names,\n\t\t\tset_unit=\"bin\", scale=scale, make_value=make_value,\n\t\t\tget_coordinate=get_coordinate)\n\treturn result\n\ndef hist_to_hist_plot(hist, title=\"\", axes_title=\"\", field_order=False,\n\tylim=None, histtype=\"step\"):\n\t\"\"\"Create a histogram plot with axes\n\n\t*hist* must be normalized\n\t"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t# check that hist is valid histogram, make_value is callable or name\n\thas_valid = hist.check_valid()\n\tif has_valid:\n\t\thist = hist\n\t\tmake_value = make_value\n\t\tif hasattr(make_value, '__call__'):  # use as is\n\t\t\tmake_value = make_value.__self__\n\t\n\t\t# make_value is a callable function that accepts a single value of bin_,\n\t\t# converts it in such a way that the bin content is bin_{x} + error_{y_mean,"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tif hist.bins is None:\n\t\traise histogram.InvalidArgument('histogram has 0 bins')\n\tif isinstance(hist, histogram.Bin):\n\t\thist = hist._data\n\t\n\tif make_value is fn_none:\n\t\tmake_value = make_hist\n\t\n\tf = make_value(hist)\n\txdata_names = [name for name, _ in f.iteritems()]\n\t\n\tif not isinstance(f, histogram.Histogram):\n\t\traise histogram.InvalidArgument(\n\t\t\tf'{type(f)} has wrong signature')\n\ty"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tcoord = None\n\tif hasattr(hist, \"coord\"):\n\t\tcoord = hist.coord\n\t\n\tfield_names = get_field_names(hist)\n\t\n\tif field_names:\n\t\tif isinstance(field_names, (int, float)):\n\t\t\tfield_names = [field_names]\n\t\n\tdata = [_format_hist(hist[x], make_value, coord) for x in order(field_names)]\n\t\n\tif not make_value and hist.count:\n\t\tif hist[int(len(data)*_SCALE_"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t# Make value if necessary\n\tif make_value is not None:\n\t\thist.value() = make_value(bin_)\n\telse:\n\t\thist.value() = hist.bin_width().scale(bin_scale_default)\n\t\n\t# Make coordinate\n\tif make_value is not None:\n\t\th_, _ = hist.h_mean()\n\t\tg_, _ = hist.g_mean()\n\t\tif not isinstance(bin_scale_default, np.ndarray):\n\t\t\tfield_names = hist.field_names()\n"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tif isinstance(hist, histogram) and make_value is None:\n\t\tmake_value = binify\n\t\n\tif make_value is None:\n\t\tmake_value = make_histogram_bincontent\n\t\n\tfrom sporco.graph import graph_factory\n\treturn graph_factory.graph_from_hist(\n\t\thist, make_value, field_names, scale)\n\ndef make_histogram_bincontent(bin_content, make_value=None, get_coordinate='left',\n\tfield_names=None, scale=None, **args)"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tgraph_class = graph.Graph\n\tif make_value is None:\n\t\tmake_value = lambda bin: (bin.mean, bin.mean_error)\n\thist = ensure_histogram(hist)\n\tgraph = graph_class(hist=hist, bin_coords=bin_coords, make_values=make_value)\n\tfield_names = field_names or field_names_from_context(context=context)\n\tscale = np.ones_like(bin_coords).copy() if scale is None else scale\n\tgraph = graph.set_bin"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tfrom.numeric_histogram import NumericHistogram\n\tfrom.histogram import binning_of_points\n\tfrom..statistics import weighted_sum_of_points\n\t\n\treturn Histogram(\n\t\thistogram=hist,\n\t\tbin_method=NumericHistogram,\n\t\tmake_value=make_value,\n\t\tget_coordinate=get_coordinate,\n\t\tbinning_of_points=binning_of_points,\n\t\tscale=scale,\n\t\tweight_sum_of_points=weighted_sum_of_points,\n\t\tmake_errorbar"}]}
{"task_id": "62b8b4baeb7e40a82d2d1136", "prompt": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tt = tentative\n\tt += (type(iface) is not zope.interface.implementer)\n\tif not vtype:\n\t\tvtype = None\n\telif vtype and vtype is not t:\n\t\traise t\n\n\tret = True\n\twhile True:\n\t\tif not iface._verify(iface):\n\t\t\tbreak\n\n\t\tfor m in iface._implementations:\n\t\t\tcand = getattr(iface, m.name, None)\n\t\t\ttry:\n\t\t\t\tif cand:\n\t\t\t\t\tif m.provided"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif candidate.providedBy is None:\n\t\tcandidate._error(\n\t\t\tcandidate.name, \"doesn't provide\", \"interface \" + str(iface))\n\t\treturn True\n\n\tif (candidate.providedBy.isInstance and\n\t\tnot isinstance(iface, type(candidate.providedBy)) and\n\t\tisinstance(candidate.providedBy, type)):\n\t    if candidate.providedBy not in iface.providedBy:\n\t\tcandidate._error(\n\t\t\tcandidate.name, \"doesn't provide \" +\n\t\t\ttype(candidate.providedBy"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tfrom collections import deque\n\t\n\tcandidate.__name__                    = candidate.__name__\n\tcandidate.__doc__                     = candidate.__doc__\n\t\n\tcandidate.__bases__                   = candidate.__bases__\n\t\n\tcandidate._methods_ = deque()\n\t\n\tfor method in candidate._methods_:\n\t\tif method not in candidate._methods_:\n"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tpass_obj = []\n\tfail_obj = []\n\t_t_obj = []\n\t_f_obj = []\n\n\tif _candidate_providedBy(iface, candidate):\n\t\t# If we are a method, it might be better to not actually check\n\t\t# the signature\n\t\tpass_obj.append(True)\n\t\tfail_obj.append(candidate)\n\t\t_t_obj.append(iface)\n\t\t_f_obj.append(candidate.providedBy)\n\t\t# But... for this check we might be just"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t# Ensure the candidate is the correct type\n\tif isinstance(candidate, zope.interface.provider.IProvides) or isinstance(\n\t\tcandidate, zope.interface.implementer.IImplements\n\t):\n\t\t# Check the interfaces that provide the candidate\n\t\tprovidedByInterfaces = [x.providedBy for x in iface['providedBy']]\n\t\trequireAllInterfaces = [x.requireForAll for x in iface['requireForAll']]\n\t\tprovidedInterfaces = [x.providedBy.implementer for x in iface['providedBy']['provide']]\n\t"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tpass\n\n\n@classmethod\ndef _implements(iface, types=None):\n\t\"\"\"\n\tGet the list of all interfaces that can provide or implict a *type*, in\n\ta recursive fashion (which is usually more costly than checking\n\t``__interfaces__``.\n\t\n\tIf *types* is None, then only ``zope.interface.implemented``.\n\t\"\"\"\n\tif not types:\n\t\ttypes = []\n\tif not isinstance(iface, zope_interface.ImplementedInterface):\n\t\tfor type_ in iface.__mro__[mro_"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tcandidate._verify(iface)\n\t\n\tif not candidate._providedBy(tentative):\n\t\treturn False # no need to call superclass method\n\t\n\tcandidate._verify(iface, vtype)\n\t\n\tif vtype is None:\n\t\tvtype = candidate.declaring_type or iface.__class__\n\t\n\tdef check_candidate():\n\t\t\"\"\"\n\t\tChecks that the candidate can successfully provide the proper\n\t\ttype and provides the specified interface\n\t\t\"\"\"\n\t\tresult_check = check_valid(candidate, vtype, tentative, vtype"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif not vtype:\n\t\tvtype = zope.interface.getRegistered(iface)\n\tif not vtype:\n\t\treturn False\n\tif vtype is None:\n\t\tpass\n\telse:\n\t\tif vtype not in _getVTypes(iface):\n\t\t\tif vtype.isSubclassOf(zope.interface.mixin.IProvider):\n\t\t\t\tzope.interface.mixin.IProvider(iface).providedBy(candidate)\n\t\t\telse:\n\t\t\t\tif not tentative:\n\t\t\t\t\tzope.interface."}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t_ifaces = getattr(sys.modules, 'zope.interface.interface')\n\tif hasattr(zope.interface, 'implementer'):\n\t\tfor imp in _ifaces():\n\t\t\tif types.isSubclass(imp.providedBy, tentative) and imp.providedBy.providedBy.name not in iface.providedBy and \\\n\t\t\t\t(ttype is None or iface.providedBy == tentative or isinstance(iface.providedBy, types.InstanceType)):\n\t\t\t\tttype = type(iface)\n\t\t\t\tif ttype in tentative"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\terrors = []\n\tif vtype:\n\t\tdef f(iface, tentative=tentative): return _verify_(\n\t\t\tiface, iface.providedBy(tentative)==vtype)\n\t\terrors += list(map(f, candidate.interfaces))\n\tif errors:\n\t\traise _error_t(errors[0], iface)\n\ndef _verify_(iface, is_provider, exact=False):\n\t\"\"\"\n\t\tVerify the given interface's provided method *iface* is also a\n\t\tprovider. In particular check that the method declaration *"}]}
{"task_id": "62b8b4baeb7e40a82d2d1137", "prompt": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tdef isimplemented(meth):\n\t\tif hasattr(iface, meth) and meth in iface.providedBy:\n\t\t\treturn meth\n\t\treturn False\n\ttry:\n\t\tm = verifyMethod(impl, candidate, \"Provides\", tentative=tentative)\n\t\tif isimplemented(candidate):\n\t\t\traise Invalid(candidate, \"is implemented\")\n\t\treturn m\n\texcept Invalid:\n\t\tpass\n\t\t\n\ttry:\n\t\tm = verifyMethod(impl, candidate, \"Instanciates\", tentative=tentative)\n\t"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tassert not iface.providedBy, 'interface must not have a package'\n\t\n\tcandidate.__implementer__ = iface\n\tcandidate.__provider__ = None\n\t\n\tfrom zope.interface import interfaceException\n\ttry:\n\t\tres = candidate()\n\texcept interfaceException as e:\n\t\treturn str(e)\n\treturn res\n\t\ndef _test_package():\n\t\"\"\"\n\tRaise a useful exception if the *zope.lib._testpackage* value is set, *but*\n\tis empty.\n\t\"\"\"\n\tif zope.lib"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\treturn _verifyObject(iface, candidate, tentative=tupative)\n\n\ndef verifyMethod(iface, candidate):\n\t\"\"\"\n\tCheck if interface is valid for a named method call on the passed\n\tinterface. This method does additional validation in order to make sure that\n\tthe returned arguments are of the correct types. If this fails, it is likely\n\tthat the wrong type is in the passed interface.\n\t:return bool: Returns a true value if everything that could be\n\t    checked passed.\n\t:raises zope.interface.Invalid: If the method call"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tprevious_valid = False\n\tfor method in candidate.providedBy: \n\t\tfor attribute in method.providedBy:\n\t\t\tprevious_valid = all(t.providedBy.has_key(attribute) for t in candidate.providedBy\n\t\t\t\t\t   if attribute in t.providedBy)\n\t\t\tif previous_valid!= t.providedBy.has_key(attribute) \\\n\t\t\t\tor not t.providedBy[attribute].providedBy in candidate.providedBy:\n\t\t\t\t\treturn False\n\t\t\tprevious_valid = True\n\tif previous_valid:\n\t\t"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif not candidate: return False\n\t\n\tcandidate_attrs = set(zutil.attrdict(candidate).items())\n\tif [k for k,v in iface.providedBy.items() if v]!= candidate_attrs:\n\t\tif not tentative:\n\t\t\tyield f\"Cannot provide {candidate.__class__.__name__}\"\n\ttry:\n\t\tiface.provide(candidate)\n\texcept zope.interface.Invalid as e:\n\t\tif config.options.verbose:\n\t\t\tmsg = \"\\n\".join(map(zopfli._util"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tfor mname in [m[0] for m in candidate]:\n\t\tif isinstance(m, interfaces.Interface) and m.providedBy!= iface:\n\t\t\t# Not the interface being implemented, nor the interface\n\t\t\t# which it is provisified. Skip to the next candidate.\n\t\t\tcontinue\n\t\ttry:\n\t\t\tm.verifyObject(iface)\n\t\texcept exceptions.VerifyError as err:\n\t\t\tif tentative:\n\t\t\t\treturn terr = str(err)\n\t\t\telse:\n\t\t\t\traise\n\t\texcept exceptions."}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif not hasattr(iface,'providedBy'):\n\t\treturn True\n\tif not iface.providedBy(candidate):\n\t\tif candidate.providedBy(iface):\n\t\t\traise Invalid(candidate, iface, \"Not a subclass of %s\" % iface)\n\t\telse:\n\t\t\terror('%s.%s does not provide %s'%(iface.classname(), candidate.classname(), iface))\n\tif not hasattr(candidate,'declaredBy'):\n\t\treturn True\n\tif not getattr(candidate,'declaredBy',None):\n\t\treturn False\n\tif not hasattr(candidate"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tcheckCandidateInterfaceClass(iface, candidate, tentative)\n\tif candidate in knownInterfaces:\n\t\treturn True\n\tknownInterfaces.add(candidate)\n\tiface.providedBy = candidate\n\tif \"providedBy!= None and candidate.providedBy == None \":\n\t\tpass\n\telif \"providedBy!= None and candidate.providedBy!= None\":\n\t\ttry:\n\t\t\tiface.providedBy.providedBy(candidate.providedBy)\n\t\texcept zope.interface.UnknownError as u: # pylint: disable=broad-except\n\t\t\tpass\n"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif not candidate:\n\t\treturn True # Can't handle invalid objects here\n\t\n\tfrom zope.interface.interfaces import AttributeInfo\n\n\tdef __init__(self):\n\t\tself._candidate = (candidate[0], ) if len(candidate) == 1 else candidate\n\t\tself._declaredMethods = []\n\t\tself._providedMethods = []\n\t\tself._providedAttributes = {}\n\t\tself._implementedMethods = []\n\t\tself._availableMethods = []\n\t\tself._error = None\n\t\treturn\n\t\n\tdef __repr__(self):\n"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\ttry:\n\t\trequire(iface)\n\texcept ImportError:\n\t\traise Invalid(\"Trying to provide an interface to a %r, but it does not seem to be a valid interface\" % iface)\n\n\tif hasattr(candidate, 'providedBy'):\n\t\treturn candidate._providedBy(candidate)\n\tif tentative:\n\t\tif candidate.implementedBy:\n\t\t\treturn candidate.implementedBy(candidate)\n\t\telse:\n\t\t\treturn candidate.implements(t.loadClass(candidate))\n\tif hasattr(candidate, 'provides'):\n\t\tif"}]}
{"task_id": "62b8b4c1eb7e40a82d2d1139", "prompt": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif not iface.supports_type_var(candidate) and not iface.is_type_var(candidate, tentative=tentative):\n\t\treturn False, \"Type-var \" # + candidate.get_name(tentative=tentative) + \" \" + iface.get_type().get_name(tentative=tentative) + \" not available in \" + iface.get_name(tentative=tentative)\n\treturn True, \"Type-var \" # + candidate.get_name(tentative=tentative) + \" \" + iface.get_type"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif isinstance(candidate, interfaces.PySide2):\n\t\treturn verifyPy2Class(iface, candidate, tentative)\n\telif isinstance(candidate, interfaces.PySide2.PyInterface):\n\t\treturn verifyPy2Object(iface, candidate,\n\t\t\ttentative=tentative)\n\tassert False, \"unknown class: %r\" % (candidate.__class__.__name__,)\n\ndef verifyObject(iface, object, tentative=True):\n\t\"\"\"\n\tVerify that the *object* provides *iface*.\n\t\"\"\"\n\tif isinstance(object,"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif tentative:\n\t\tt0 = debugPrint(candidate)\n\t# Try to resolve the actual named type in the *iface*.\n\tdef resolve(iface):\n\t\ttry:\n\t\t\tif isinstance(iface, basestring):\n\t\t\t\tiface = types.namedtype(iface)\n\t\t\tif not hasattr(iface, 'im_func'):\n\t\t\t\traise TypeError, \"interface %s is not implemented\\n\"% iface\n\t\texcept TypeError:\n\t\t\tif iface in interfaces:\n\t\t\t\traise\n\t\t\treturn None\n\t\treturn getattr(iface,"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tfrom.ClassDef import ClassDef\n\tc = ClassDef(iface)\n\tassert c.is_a_class\n\tassert (c.name == candidate.name)\n\tassert (c == candidate)\n\tif tentative:\n\t\tassert (c.is_tentative == candidate.is_tentative)\n\telse:\n\t\tassert (c.is_tentative == c.is_tentative)\n\ndef assertRaisesWithMessage(f, msg, *args):\n\t\"\"\"\n\tAssert that a function raises *args* but takes \"message\""}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\timport unittest\n\ttry:\n\t\tunittest.TestCase(candidate)\n\texcept TypeError:\n\t\tif not isinstance(candidate, unittest.TestCase):\n\t\t\traise unittest.SkipTest(\"TestCase does not provide *repr* method.  You just need to add assertEqual(...) to test your own custom type's equality methods and that's all.  You can do this by adding assertEqual(something, something).\")\n\n\n\n\t# Try the __slots__ slot, but if it exists, that means that there are\n\t# attributes available, not just the attributes themselves.  For example\n\t#"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tfor key in tentative:\n\t\tif verifyClass(iface, key):\n\t\t\traise ValueError(f\"Missing key in {iface!r}: {key}\")\n\treturn candidate\n\n\ndef verifySubclass(a, b):\n\t\"\"\"\n\tVerify that the *a* is a subclass of the *b*.\n\t\"\"\"\n\tif not hasOnlyTheseNewStyleSubclasses(a, b):\n\t\traise ValueError(f\"Bad subtypes for {a!r})\")\n\treturn a\n\n\ndef verifyType(a, b):\n\t\"\"\"\n\t"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tassert interfaceProvides(iface, candidate)\n\ndef createClass(baseName, superClass, classCode, initCode) :\n\tname = baseName+'_'+classCode\n\tcls=classCode+'_'+name\n\tcls2 = cls._replace('\\\\',' '_')\n\taddTypeToClass(cls,cls2)\n\tiface=newClassType(baseName, superClass, 'L{L_}{}'.format(cls2),initCode)\n\tif type(cls)==type(u''):\t# We have a simple string.\n\t- The code will generate a"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t__tracecheck__ = True\n\tif tentative:\n\t\treturn\n\tif isinstance(candidate, tuple):\n\t\tiface = type(candidate[0])\n\t\tcandidate = candidate[0]\n\n\tif iface in iface.interfaces():\n\t\t__tracecheck__ = False\n\t\tif candidate in iface.interfaces():\n\t\t\treturn iface.interfaces()[candidate].name\n\n\tif iface not in __all__:\n\t\tif __debug__:\n\t\t\tprint(\"Interface not on list: %s at %s\" % (candidate, iface))\n\ndef"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tcandidates = [candidate]\n\tif candidate and target.matchClass(\n\t\t\tcandidate, target, check=check):\n\t\tif check or target.isSame(iface):\n\t\t\treturn True\n\t\tif candidate.isSame(iface):\n\t\t\treturn candidate, iface, -1\n\t\tcandidates.append(candidate)\n\tif candidate and not target.matchClass(\n\t\t\tcandidate, target, check=check):\n\t\tif check:\n\t\t\tcandidates.append(candidate)\n\t\tcandidates.append(iface)\n\t\tif candidate.isSame(iface):"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tassert issubclass(iface, Class), type(iface)\n\tif tentative and issubclass(iface, TentativeClassMixin):\n\t\t# Tentative classes are handled by inheritance.\n\t\t# Tentative and Tentative classes must have the same\n\t\t# inheritance relationships.\n\t\traise Exception(\n\t\t\t\"The given interface is not a subclass of both its \"\n\t\t\t\"base interface and its 'tentative'.\"\n\t\t\t\"Use TentativeClassMixin for the base interface.\"\n\t\t)\n\tif issubclass(iface,"}]}
{"task_id": "62b8b559eb7e40a82d2d11f6", "prompt": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmc = {}\n\n\t# Get base classes for metaclasses\n\tif explicit_mc is not None:\n\t\t# Explicit \"meta\" class has priority\n\t\texplicit_mc = mc.setdefault(explicit_mc, {})\n\t\texplicit_mc.__metas = list(bases + explicit_mc.__metas)\n\t\treturn explicit_mc\n\n\t# Not explicit \"meta\" class has priority\n\tmetaclasses = bases._base_classes._metaclasses\n\tfor metaclass in metaclasses:\n\t\tmc[metaclass] = object()\n\t\t"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmetaclasses, bases = metas(bases, explicit_mc)\n\tif metaclasses and not bases:\n\t\traise ValueError(\"Cannot determine metaclass from %r\"\n\t\t\t% tuple(bases))\n\tif not bases and metaclasses:\n\t\tmetaclass = type(metaclasses[0])\n\telif bases or metaclasses:\n\t\tmetaclass = type(bases[0])\n\treturn metaclass\n\n\ndef memoizes(func, *bases):\n\t\"\"\"\n\tMemoize a function, so that the result of func(x)"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif not bases or len(bases)==1:\n\t\treturn None\n\texplicit_mc = explicit_mc or _explicit_metaclass_cache\n\tmc = explicit_mc[get_new_bases(bases, '__metaclass__', lambda b: b.ob_type)]\n\tmc = mc() if mc is not None else metaclass\n\tif explicit_mc:\n\t\treturn mc\n\tmc.update(bases)\n\treturn mc\n\n@staticmethod\ndef get_new_bases(bases, new_base, new_base_func):\n\t# type should"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif isinstance(bases, str):\n\t\tcls = type.__metaclass__\n\telif bases < 0 or bases > float('inf' or 'infinity', str='mixed'):\n\t\tcls = type\n\telif bases == 0:\n\t\tcls = type\n\telif hasattr(type, '__metaclass__'):\n\t\tif type == type.__metaclass__:\n\t\t\tcls = type\n\t\telse:\n\t\t\tcls = None\n\t\tbases += 1\n\telse:\n\t\tcls = None\n\treturn cls\n\n\n\n\n#----\n# --- main"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmetaclass = None\n\tfor base in bases:\n\t\tmc = explicit_mc and explicit_mc.get(base, base)\n\t\tif mc and not metaclass and issubclass(mc, types.BuiltinMethodType):\n\t\t\tmetaclass = base\n\t\t\tbreak\n\n\treturn metaclass\n\n@_faked(True)\ndef is_valid_subscript(s): return isinstance(s, (\n    lambda: None, types.Index, types.Tuple, types.Type,\n    types.BuiltinMethodType, types.BuiltinMethodType,\n"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t# no __metaclass__ is None, so we need to take account of\n\t# in the explicit class\n\tif explicit_mc is None and __metaclass__ is None:\n\t\t_metaclass = type.__self__.__class__\n\telse:\n\t\tif explicit_mc is None:\n\t\t\t_metaclass = type.__name__.lower()\n\t\telse:\n\t\t\t_metaclass = explicit_mc.__name__\n\n\tif bases:\n\t\t# check if 1.x metaclass works\n\t\tif __metaclass__ in [type.__name"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\texplicit_mc = explicit_mc if explicit_mc is not None else metaclasses\n\tbases = [_ for _ in range(len(bases))] + [a for a in bases]\n\tmetaclass = next(m for m in metaclasses if m not in bases)[1]\n\treturn metaclass(bases=bases, explicit_mc=explicit_mc)\n\n\ndef _check_isinstance(cls, thing):\n\t\"\"\"\n\t\tCheck to see whether some object is of a given type\n\t\"\"\"\n\tif not isinstance(thing, cls) or not issubclass("}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmc = None\n\tfor base in bases:\n\t\tif isinstance(base, type):\n\t\t\tmc = base\n\t\t\tbreak  # type\n\telse:\n\t\tmc = explicit_mc\n\tif mc is None:\n\t\tmc = get_metaclass(typename)  # type: ignore\n\treturn mc\n\n\ndef get_dict_items(iterable):\n\treturn iter(dict.items(iterable))\n\n\ndef get_set_items(iterable):\n\treturn iter(set.union(*dict.fromkeys(iterable).values()))\n\n"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmc = type(('__metaclass__', bases, explicit_mc))\n\tif mc.__name__ == '__dict__':\n\t\tmc = type(\"__dict__\",bases,explicit_mc)\n\telif mc.__name__ in (m for mc in type.__mro__ if m.__name__!= 'MRO'):\n\t\tmc = type(m, bases, explicit_mc)\n\tif not mc.__name__ == '__metaclass__':\n\t\treturn None, mc\n\tif mc in bases.__bases__:\n\t\tmetaclass = mc\n\telse:\n\t\tmet"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmeta = None\n\tfor base in bases:\n\t\tif base.__class__ is metaclass or (explicit_mc is not None and base.__metaclass__ is explicit_mc.__metaclass__):\n\t\t\tmeta = base\n\t\t\tbreak\n\telse:\n\t\tif meta is None:\n\t\t\tmeta = type\n\t\telse:\n\t\t\tif isinstance(meta, type):\n\t\t\t\tmeta = meta\n\treturn meta\n\n\ndef _get_class_name(cls):\n\t\"\"\"\n\t\tRetrieve name of given metaclass object\n\t\"\"\"\n\t"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47d", "prompt": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif key in self:\n\t\treturn self[key]\n\tif default is __marker:\n\t\treturn default\n\traise KeyError\n\ndef popitem(self):\n\tfor item in self:\n\t\treturn item.rsplit(':', 1)\n\treturn ()\n# End of django-nose package\n\nimport re\nimport sys\nimport inspect\nfrom enum import Enum\n\n__all__ = ['ErrorMode', 'get_verbosity_level', 'get_max_lines_in_doc']\n\n\ndef get_verbosity_level(verbosity_level):\n "}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif key not in self:\n\t\traise KeyError(key)\n\tvalue = self[key]\n\tdel self[key]\n\treturn value\n\ndef keys(self):\n\t\"\"\"\n\tD.keys() -> list of D's (key,value) items\n\tin the proper order.\n\t\"\"\"\n\treturn self[:] = set.union(*[k for k,_v in self.getiter(False)])\n\t\ndef values(self):\n\t\"\"\"\n\tD.values() -> list of D's (key,value) items\n\tin the proper order"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tvalue = self.cache[key]\n\tdel self.cache[key]\n\treturn value if value is not default else self.absent_item()\n\ndef replace(self, old=None, new=None, safe=True):\n\t\"\"\"\n\tReplace currently set value (or slice of items) with new one.\n\tIf new is None it uses self.absent_item().\n\tIf old is not None, it must not be greater than the current length of the stack.\n\tNote that new doesn't override value (but copy doesn't override new).\n\tA copy is"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif self.has_key(key):\n\t\tdel self[key]\n\t\treturn d if default is __marker else default\n\traise KeyError('pop from empty Stack')\n\n@stack.utils.decorators.specify\ndef pop_all(self, key, default=__marker):\n\t\"\"\"\n\tD.pop_all(k[,d]) -> S: stack with items removed\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tfor i in range(0, len(self), -1):\n\t\t"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif key not in self:\n\t\td = default\n\t\t#print 'pop', self, key, d\n\t\terr_msg = 'pop: invalid key \"{}\" in {}'.format(key, self) \n\t\traise KeyError(err_msg)\n\treturn self.data.pop(key, d)\n\n#def popitem(self):\n#    \"\"\"\n#    Return the next pair of key, value.\n#    Elements are returned in order, and duplicate keys are not ignored.\n#    Note that this kind of operation"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif key in self:\n\t\treturn self[key]\n\td = self._dict.pop(key, default)\n\tif not d:\n\t\traise KeyError(key)\n\treturn d\n\n#pragma checksum \"..\\..\\..\\..\\Views\\Editor\\Settings\\SettingsManagerEx.xaml\" \"{ff1816ec-aa5e-4d10-87f7-6f4963833460}\" \"9D06B3EB8CB1C12E1DE4DFDE3BEBCC5D4DA4F6CC3F78"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif key in self:\n\t\tdel self[key]\n\tif default is __marker: return default\n\ttry:\n\t\treturn self[default]\n\texcept KeyError:\n\t\tpass\n\ndef popitem(self, key):\n\t\"\"\"\n\tD.popitem() -> item, remove specified key and return the corresponding value.\n\tIf key is not found, KeyError is raised for that key.  Otherwise, it's returned.\n\t\"\"\"\n\ttry: self[key]\n\texcept KeyError: raise KeyError(key)\n\treturn self[key]\n\n"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tres = default\n\tfor k in self:\n\t\tif d is res:\n\t\t\treturn self[k]\n\t\telif k == key:\n\t\t\tdel self[k]\n\t\telse:\n\t\t\tres = res[k]\n\traise KeyError(key) or res\n\n@dataclass(frozen=True)\nclass A:\n\tx: Any\n\ty: Any\n\tz: Any\n\n@dataclass(frozen=True)\nclass B:\n\tx: A[\"x\"],\n\ty: A[\"y\"],\n\tz:"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tc = self.data\n\tv = c.pop(key, default)\n\tif v is None:\n\t\traise KeyError('Key not found: %r' % key)\n\treturn v\n\nD.clear = pop\nD.update = update\nD['<private>'] = pprint.pformat\nD['</private>'] = lambda x: x\n\n#==============================================================================\nclass Stack(object):\n    \"\"\"\n    Base class for a stack of data.\n    \"\"\"\n\n    __slots__ = 'data',\n"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif k is None:\n\t\tdef get(d, i):\n\t\t\tif not i.used: raise KeyError(k)\n\t\t\treturn d.pop(i.key)\n\t\tk = key\n\telse:\n\t\tk = str(key)\n\tv, _, i = self.treedict.get(k, (None, None, None))\n\t\n\tif v is None:\n\t\ttry: v = default\n\t\texcept ValueError: pass\n\ttry: del self.treedict[k]\n\texcept KeyError: pass\n\treturn v"}]}
{"task_id": "62b8d23748ba5a41d1c3f497", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\treturn (x for x in self._map if x > 0 and x % _num_bins == _num_bins)\n\n\ndef _count(self):\n\t\"\"\"\n\tA Pythonic way of counting the values in an iterable.\n\t\"\"\"\n\treturn sum([isinstance(x, tuple) for x in self])\n\n\ndef values(self):\n\t\"\"\"\n\tReturn a map view of the values.\n\t\"\"\"\n\treturn imap(pop, self)\n\n\ndef take(self, k, [default]):\n\tu\"\"\"\n\tTake a value from an"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t# This method is a generator, so each time a non-trivial `popitem` item is\n\t# encountered, we'll remove the other.\n\t# Use self.__chain to restore the original state.\n\tfor i, element in enumerate(self.__chain__):\n\t\tif element is None:\n\t\t\tdel self.__chain__[i]\n\t\telse:\n\t\t\ttry:\n\t\t\t\telement.pop(self.__key__)\n\t\t\t\tbreak\n\t\t\t# It's possible for item_was_modified() to fail, in which case we're not doing anything,"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\treturn list(itertools.islice(self, -1,-1,False))\n###\n### `iteritems` and friends\n###\n### Iterclass has `.next() -> value` members which get populated with the pairs\n### returned by `items()`.\n#\n### Python 2.x code:\n###\n###     %s = {}\n###     for key, value in iteritems(mymap):\n###         %s[key] = value\n###\n### Python 3.x code:\n###\n###  "}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\treturn (key, self.__getitem__(key))\n\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/pkg/errors\"\n\t\"golang.org/x/time/rate\"\n\n\t\"github.com/kiali/kiali/config\"\n\t\"github.com/kiali/kiali/kubernetes\"\n\t\"github.com/kiali/kiali/kubernetes/kubectl/opts\"\n\t\"github.com/kiali/kiali/models\"\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\trv = (rvlist, rvdict)\n\ti = len(rv)\n\tfor e in rv.entries:\n\t\trv.e[e] = rv[-1]\n\trv.popitem(last=i)     # remove the last (most often used) entry (key, value)\n\treturn rv[-1]\n\n\n\n######################\n###                         ##\n### Key/value/entries      ##\n###  "}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tpass\n\ttry:\n\t\tfor item in self.rawitems():\n\t\t\tvalue = item[0]\n\t\t\tkey = item[1]\n\t\t\tdel self[key]\n\t\t\treturn (key, value)\n\tfinally:\n\t\tself.untrack = False\n\t\tself.pop_track_refs()\n\t\tself.reset()\n\nclass BaseObject(object):\n\n\t__slots__ = ['_state', '__weakref__']\n\tname = ''\n\n\treadonly = True\n\t_key = \"\"  # cached raw key if"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\ttry:\n\t\tkey, item = self.popitem_if_present()\n\texcept ValueError:\n\t\treturn\n\treturn (key, item)\n\n#################################################\n\n\n_thread_local = threading.local()\n\n_current_threads = []\n\n\ndef getCurrentThreads():\n\t\"\"\"\n\t\tDetermine the list of currently\n\t\trunning threads.\n\t\"\"\"\n\treturn list(_current_threads)\n\n\ndef _add_current_thread(id, handle):\n\t# Get current threads list.\n\tif type(id)!="}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\treturn self.__getitem__(-2)\n\n#pragma once\n\n#include <Engine/Rendering/IRenderPass.h>\n#include <Engine/Debug/DebugLog.h>\n\n#include \"Scene/Mesh.h\"\n\nclass CubeMesh : public IRenderPassNode\n{\npublic:\n\tCubeMesh(const SRenderingPassInfo& passInfo);\n\t\n\tvoid Draw() override;\n\nprivate:\n\tstd::unique_ptr<IResource> Resource;\n\tstd::unique_ptr<CScene> Scene;\n\n\tstd::unique_ptr<"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tif self.current!= {}:\n\t\tnode = self.current\n\t\tself.current = {}\n\t\treturn node[self.key]\n\treturn ({}, None)\n\nclass LinkedList:\n\t\"\"\"\n\tA Node in the \"front\" of the LinkedList.\n\n\tClass: LinkedList\n\n\tA list of items: ``[int,int,int]``.\n\t\tNote that this is not a list.\n\t\tNote that this class makes it easier to add and remove items\n\t\tbetween other lists.\n\n\tExample:\n\t\tfor elem in"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tself._pairs.popleft()\n\treturn self._pairs.pop()\n\n#pragma once\n\n/*+\n________________________________________________________________________\n\n (C) dGB Beheer B.V.; (LICENSE) http://opendtect.org/OpendTect_license.txt\n Author:        A.H. Bril\n Date:          Jan 2004\n________________________________________________________________________\n\n-*/\n\n#include \"uiattrselmod.h\"\n#include \"attribdesc.h\"\n#include \"ptrman.h\"\n\n"}]}
{"task_id": "62b8d23a48ba5a41d1c3f499", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\treturn self.values.popitem()\n\n\n# Map of names and classes\nitems = {}\n# All the registered classes\nregistered_classes = {}\n# An enumeration of all registered types (classes and enums)\nregistered_types = {}\n# A dictionary mapping class names to registered class names\nregistration_name_mapping = {}\n# Cache of registered classes.  This is necessary since instances of this class\n# cannot be changed.\nregistered_classes_cache = {}\n# Map of methods to classes\nmethods_to_classes = {}\n# Map of methods to classes, methods and classes with"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\treturn (key, value)\n\n\ndef popmap(self):\n\t\"\"\"\n\tRemove and return the `(key, value)`-pair least recently used.\n\tmap should be sorted on key.\n\t\"\"\"\n\tnewdict = {}  # map may contain duplicates.\n\tfor key in sorted(self.dict.items()):  # copy dictionary.\n\t\tif key[0] == key[-1]:  # remove duplicate: (key[0], key[-1])...\n\t\t\tdelself = True\n\t\telse:  # check whether there is a reference to"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tif self._top is None:\n\t\treturn\n\tkey,value = self._top._popitem()\n\tself._top = self._top._popitem()\n\treturn (key,value)\n\n\ndef sort(self, comp=None, key=None, reverse=False):\n\t\"\"\"\n\n\t\tSorts a list of data based on the specified sorting algorithm.\n\n\t\t:param comp: the comparison function, by default None\n\n\t\t:param key: the primary key in the result data object, if not None\n\n\t\t:param reverse:"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tfor key, value in self.__dict__.items():\n\t\tif key in self:\n\t\t\tdel self.__dict__[key]\n\t\t# return value\n\t\treturn value\n\t\n\n\nimport re\nfrom urllib.parse import urlsplit\n\n# Default list of emails sent (without whitespace)\nEMAILS = [] # list of Email objects\n\n# Default list of email templates\nEMAILTEMPLATES = {} # list of EmailTemplate objects\n\n# Default URL to post forms\nURL = '/default-form/'\n\n# Minimum time\nTIME_MIN = 60 * 30 # 10 min"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tif self._len > 0 :\n\t\tlast = self._data[self._top]\n\t\tself._len--;\n\t\tself.push(None, last)\n\treturn self._popval(remove,last)\n\ndef _dict_add_and_pop(self, last, key, value):\n\t\"\"\"\n\tAdd all objects in `dict`.\n\n\tParameters\n\t--------\n\tlast : `list`\n\tkey : `str`\n\tvalue : `None` or `Any` or `dict`\n\t\"\"\"\n\tfor x in itertools"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t# Return either the first item if at lest one or two items if\n\t# the last item.\n\tif len(self.data) < 2:\n\t\treturn 0,    # empty\n\telif self.count() <= 1:\n\t\tpop = self.first, self.second\n\t\treturn pop, None\n\tpop1, pop2 = self.popitem()\n\treturn pop1, pop2\n\n\t# This is a python3 style version of the above.\n\n\n#---------------------------------------------------------------------------\n\nclass Queue:\n\t\"\"\"\n\tThis class"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\treturn(t.popitem)()\n\n\n# Map Iterator.\nclass iter(object):\n\"\"\"\n\tclass iterator():\n\t\tdef next():\n\t\t\tobj = (key,value)\n\t\t\tif x>y : obj[0]=x\n\t\t\telse if x<y : obj[1]=y\n\t\t\telse if y>z : obj[2]=z\n\t\t\telse : obj[3]=z\n\t\t\tobj += tuple(map(t.next, obj[2:])\n\"\"\"\n\n# iterator object.\nclass iteritems(object):\n\"\"\""}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\treturn self.sorted[min(key, self.len)]\n\ndef pop(self, key, default=None):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\ttry:\n\t\tres = self.popitem()\n\t\treturn res[key] if default is None else res[key] == default \\\n\t\t\tor res.get(key, default)\n\texcept pydantic.UnboundLocalError:\n\t\treturn default\n\ndef setdefault(self, key, default=None):\n\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tif not self._free:\n\t\treturn (self.start, None)\n\telse:\n\t\tresult = self._free.pop()\n\t\tself._free = self._free[0:0]\n\t\treturn (result[0], result[1])\n\n\n# The next few functions have been deprecated.\ndef sort_key(self, key):\n\t# no longer True, since this doesn't support ordering\n\traise DeprecationWarning(\n\t\t'sort_key() is deprecated in favor of numpy.sort(arr).'\n\t )\n\ndef"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\twhile self[0]:\n\t\tkey, value = self[0]\n\t\tself[0] = self[1]\n\t\tself[1] = key\n\t\treturn key, value\n\treturn \tkey, None\n\n<?php\n\n/**\n * This code was generated by\n * \\ / _    _  _|   _  _\n * | (_)\\/(_)(_|\\/| |(/_  v1.0.0\n * /       /\n */\n\nnamespace Twilio\\Rest\\Api\\V2010\\Account"}]}
{"task_id": "62b8d23c48ba5a41d1c3f49b", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\treturn list(self)[-1]\n\nclass DataStore_Py3(object):\n\t\"\"\"\n\t\tClass for interacting with and controlling database\n\t\tcontents in Python 2 and 3.  Currently uses\n\t\tthe Python 2 and 3 database classes based on\n\t\tcPickle.\n\t\"\"\"\n\tdef __init__(self,file=None,mode=\"r\"):\n\t\tpass\n\n\tdef __getattr__(self,name):\n\t\tif not name.islower():\n\t\t\traise AttributeError\n\t\tif name == \"file\":\n\t\t\treturn self"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tif sys.version_info[0] == 2 and isinstance(self._cache, dict):\n\t\tself._cache.popitem()\n\tif sys.version_info[0] == 2 and isinstance(self._cache, list):\n\t\tself._cache.popitem()\n\n\ndef iterkeys(self):\n\tfor k in self.keys():\n\t\tyield k\n\n\ndef itervalues(self):\n\tfor k in self.keys():\n\t\tyield self._cache[k], k, self[k]\n\n\ndef iteritems(self):"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tself.cursor.moveToPrev()\n\titem = self.cursor.item()\n\tdel self.cursor\n\treturn self.handle_callback(item, PopItemCallback(self.path))\n\n\t# The path callback is called with the entire PathData\n\n\ndef add_path_data(self, path_data):\n\t\"\"\"\n\t\tAdd path data to self.\n\t\"\"\"\n\tself.handle_callback(PathDataHandle(self.path, path_data), PathDataCallback(self.path))\n\tself.handle_callback(self.root(), Root"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tk, v = self.popitem_old()\n\tif isinstance(v, list):\n\t\treturn tuple(k, v)\n\treturn k, v\n\ndef popitem_old(self):\n\t\"\"\"\n\tSame as popitem for the old syntax, but avoids the extra lookup in most subclasses.\n\t\"\"\"\n\tk, v = self.popitem()\n\tif k is None and v is not None:\n\t\treturn v[0], v[1]\n\treturn k, v\n\ndef pop(k, *args, **kwargs):\n\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\treturn (self._head._head, self._tail._tail)\n\ndef popfirst(self):\n\t\"\"\"\n\tRemove and return first entry.\n\t\"\"\"\n\treturn (self._head._head, None)\n\ndef unique(self):\n\t\"\"\"\n\tGenerate a sorted sequence of (key, value) pairs in O(N) time.\n\t\"\"\"\n\tsorted_items = self[: -1]\n\twhile True:\n\t\tfor head, tail in sorted_items[: -1]:\n\t\t\tyield (head, tail)\n\ndef update(pair,"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\ttry:\n\t\treturn list(self)[-1]\n\texcept IndexError:\n\t\treturn ''\n\ndef keys(self):\n\t\"\"\"\n\tReturn all the keys of the hash, sorted by name but most recent first.\n\t\"\"\"\n\t# TODO: should get the keys from the dictionary?\n\tkeys = self.items()\n\tkeys.sort() # the order of items is undefined and depends on hash's key length\n\treverse = dict(zip(keys, range(0, len(keys))))\n\t#print(keys, reverse)\n\treturn list("}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tresult = None\n\twhile True:\n\t\ttry:\n\t\t\tkey, value = self.iter_next()\n\t\t\tresult = (key, value)\n\t\t\tbreak\n\t\texcept StopIteration:\n\t\t\tbreak\n\treturn result\n\ndef get(self, *keys):\n\t\"\"\"\n\tGet a specified key.\n\t\"\"\"\n\tres = None\n\tfor key in keys:\n\t\ttry:\n\t\t\tres = self.__getitem__(key)\n\t\texcept AttributeError:\n\t\t\tres = self.__getitem__(key.lower())\n\treturn"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\treturn self.keys().popitem()\n\ndef sortedkeys(self, *args, **kwargs):\n\t\"\"\"\n\t\tReturns an sorted list of all keys in the dicts that comprise *args*, **kwargs*.\n\n\t\tArgs:\n\t\t\t- Key comparer\n\t\"\"\"\n\treturn sorted(key for key in self.keys() if self.keys()[key][-3] == 'key' and self.keys()[key] % args[1] == kwargs[key])\n\ndef reduce(iter):\n\t\"\"\"\n\t\tA reduce function that reduces iteratively"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tkeyval = self._keyval\n\tfor k,v in self._dict[keyval]:\n\t\tdel self._dict[keyval][k]\n\t\t#print (\"Popped key: %s value: %s\"%(str(k), str(v)))\n\treturn keyval, v\n\n@defget\ndef pop():\n\tif isfunction(self._function):\n\t\t#print (\"Popping function\")\n\t\ttry:\n\t\t\tval = self._function.__code__.co_stackmanager[-1][0]\n\t\texcept:\n\t\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\treturn tuple(x for x,y in self._values.items() if y == self._index)\n\n\n__all__ = ['Stack', 'StackMap']\n\nexport { default as PopperGroup } from'src/app/Popper';\nexport { default as Popper } from'src/app/Popper/Popper';\n\n# -*- coding: utf8 -*-\n# This file is part of Shuup.\n#\n# Copyright (c) 2012-2021, Shuup Commerce Inc. All rights reserved.\n#\n# This source code is licensed under"}]}
{"task_id": "62b8d23e48ba5a41d1c3f49e", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tif self._random_state is not None:\n\t\tkey = self._random_state.choice(self._keys)\n\t\tself[key] = self._random_state.random()\n\t\tdel self._random_state\n\treturn key, self._values[key]\n\n#pragma once\n\n#include \"Game/AI/Action/actionSwirlAction.h\"\n#include \"KingSystem/ActorSystem/actAiAction.h\"\n\nnamespace uking::action {\n\nclass SwirlSwirlAction : public SwirlAction {\n  "}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\ttry:\n\t\tkey, last_value = self.randomly_popitem_pairs_tail()\n\texcept IndexError:\n\t\tkey, last_value = None, None\n\treturn key, last_value\n\n\ndef reseed_python2(seed):\n\t\"\"\"\n\tHelper function for reseeding the python version.\n\t\"\"\"\n\ttry:\n\t\tos.environ['PYTHONIOENCODING'] = 'utf-8'\n\t\tfrom math import ldexp, log2\n\t\ts = str('').join([str(ldexp(i +"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\trv = None\n\tif self.seed is None:\n\t\t_seed = 0\n\t\tself.seed = _seed\n\t\trv = \"(key=val=%d)\" % self.seed\n\t\tself.seed += 1\n\treturn rv\n\npackage ledis\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"math/rand\"\n\t\"net\"\n\t\"net/http\"\n\t\"os\"\n\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tresult = [(random.randint(1, len(self)),self[key]) \n\t\t\tfor (key, value) in self.items()]\n\tif isinstance(result[0], tuple):\n\t\tif result[0][0]!= result[0][1]: \n\t\t\tprint result\n\t\treturn (result[0][0], self.pop(key) for key in result[0])\n\telse:\n\t\treturn result\n\ndef print_dict(data):\n    \"\"\"Print dictionary, including keys and values.\n    \"\"\"\n   "}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\treturn self.random.popitem()\n\ndef pop(self):\n\t\"\"\" Return a random element, remove it and return it. \"\"\"\n\treturn self.random.popitem()\n\ndef rpop(self):\n\t\"\"\" Return a random element, remove it and return it. \"\"\"\n\treturn self.random.rpopitem()\n\ndef rot(self, n):\n\t\"\"\" Return a new `dict` with `n` random keys dropped from the top. \"\"\"\n\tdict = {}\n\tfor i in range(0, n): dict[i] = self.random"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\treturn (self.randomKey(), self.randomKey())\n\ndef random(key):\n\t\"\"\" Return a random value using a key created from `randomKey` and `key`. \"\"\"\n\treturn key ** (random.random() / math.sqrt(2))\n\ndef random2(min, max):\n\t\"\"\" Return a random value, scaled between `min` and `max`. \"\"\"\n\treturn random(min) * (max-min) + min\n\ndef rand(min):\n\t\"\"\" Return a random value from the given distribution, scaled by `min`. \"\"\"\n\treturn"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tkey = random.random()\n\tif 0 > key:\n\t\treturn (-2,-1)\n\ttry:\n\t\treturn (self.r.m.m(key),self.r.m.m(self.index(key)))\n\texcept ValueError:\n\t\tkey = None\n\t\treturn (-2,-1)\n\nclass RulerThread(Thread):\n\n\t\"\"\"\n\tRuler(n=-1)\n\t\"\"\"\n\n\tdef __init__(self, parent=None):\n\t\tThread.__init__(self)\n\t\tself.parent"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\treturn self.f.popitem()\n\t\n@def pprint(self):\n\t\"\"\"\n\tPrint the sorted `(key, value)` triples in the dictionary.\n\t\"\"\"\n\treturn \"{0} -> {1}\".format(self.keys, self.values)\n\t\n\nclass SortedDict(dict):\n\t\"\"\"\n\tA Python dict which compares elements using the keys.\n\tThe comparison algorithm is `key < comparison < value` for key-value pairs,\n\tnot for tuples.\n\t\n\tFor example suppose `items` is an OrderedDict, a dictionary containing\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\treturn self._popitem_impl('popitem')\n\n\tdef __reversed__(self):\n\t\tself._keys('keys')\n\t\twhile self._popitem_impl('__reversed__'):\n\t\t\tcontinue\n\n\tdef update(self, d=None, **kwargs):\n\t\tself._popitem_impl('update')\n\t\tself._update_impl(d, **kwargs)\n\n\tdef pop(self, *args, **kwargs):\n\t\tself._keys('pop', 'popitem')\n\t\treturn self._popitem_impl('pop"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\treturn (self._array, self._next_key())\n\n\nclass MapLike(MutableMapping):\n\t\"\"\"\n\tAn implementation of `MutableMapping`, using a dictionary-like structure.\n\t   ``{ key: val }`` is equivalent to::\n\n\t       m = Map(key_function=self._key_wrapper, val_function=self._val_wrapper)\n\t       print(m)\n\t       print(repr(m), '==', repr(self), '->', repr(m.keys()))\n"}]}
{"task_id": "62b43425903eeb48555d3ea1", "prompt": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tself._log('create_in_regex')\n\tpattern = self.parameter('pattern').pattern()\n\tpattern = re.compile(pattern, re.M)\n\tself._log('result')\n\treturn pattern\n\t\n\t\n\t\ndef get_in_regex_parameters(self) -> Dict[str, Pattern]:\n\t\"\"\"\n\tReturns the in-style parameter regular expression parameters.\n\t\n\tReturns the in-style parameter regular expression parameters (:class:`dict`).\n\t\"\"\"\n\tself._log('get_in_regex_parameters')\n\tparameters = {}\n\tparameters"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tregex = re.compile(r'\\[[0-9]+\\]\\s+(.*)\\s*to\\s+(.*)\\s*(#.*)?$')\n\treturn regex.match(self.in())\n \ndef _create_in_param_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tparam_regex = re.compile(r'\\s(.*)$')\n\treturn param_regex.match(self.in_param())"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t#\n\t# Create the regular expression for the in-style parameter.\n\t#\n\t#\n\t# The following matches are allowed\n\t#\n\t#   \\b   Match non white space.\n\t#   \\f   Match any other character except for the following newline.\n\t#   \\r   Match any special character.\n\t#   \\s   Match any character except newline.\n\t#   \\S   Match any character except newline and the special space character.\n\t#\n\t# The following characters are allowed\n\t#"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tself.regex = re.compile(r\"\"\"\n\t\tin\n\t\t\t(\n\t\t\t)?\n\t\t\t\\A\n\t\t\t(\\s*\n\t\t\t\t(?<in>\\s*(?:\\b?in\\b\\s*)|(?<in>\n\t\t\t\t\t(?<in>[a-z]+)+\n\t\t\t\t\t\\A\n\t\t\t\t)\\s*=\\s*\n\t\t\t)?\n\t\t\t\\W\n\t\t)+\n\t\\Z\n\"\"\", re.UNICODE)\n\treturn self.regex\n\n###############################################################################\n#\n# Create the in-style parameter regular expression that"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tin_regex = re.compile(r\"\"\"\n    (?P<pre>\n        (?P<header>    # First header\n            *\n            \\)\n            >\n            |\n            )\n        (?:[ \\t]+\n         "}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(self.param.value)\n\n#-----------------------------------------------------------------------------\n# Public API.\n#-----------------------------------------------------------------------------\n\n\ndef _set_pattern(self, param: str, pattern: str, default=None, description=\"\",\n                 param_type=\"in\", allow_empty=False):\n    \"\"\"\n\tSet the value of the given parameter on all parameters.\n\t\n\tThis is the same as `:meth:`pprint`, but with the exceptions\n\t`:meth:`_set_group`, which"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tre = re.compile()\n\treturn Pattern(_IN_FORMAT, re._pattern, flag=re.IGNORECASE)\n\nclass Column(_Column):\n\t\"\"\"A column.\"\"\"\n\tconstructor = property(lambda self: Column,\n\t\tlambda s: (Column.__init__, s, None))\n\t_format = None\n\t#: default format\n\t_db_type = None\n\t\n\tdef get_db_type(self):\n\t\t\"\"\"Return type of column/column dictionary.\n\t\t\n\t\tUse :attr:`Column.db_type` to set database type."}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn self._re_pattern\n\n\ndef _create_regex(self) -> RERegularExpression:\n\t\"\"\"\n\tCreate the regular expression instance.\n\t\n\tReturns the regular expression instance (:class:`re.RERegularExpression`).\n\t\"\"\"\n\treturn self._re_regex\n\n\ndef _parse_regex_type(self, value, reg_flag,\n\t\t\t\t\t   error_message, is_error=False):\n\t\"\"\"\n\tTries to parse the regular expression string/parameter value into a tuple.\n\t\n\tArgs:\n\t\tvalue\t\t\tThe regular expression"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tfrom re import Pattern\n\tr\"(?:(?:${1})+|\\b(?:${1}|$))\\b\"\n\treturn Pattern(r\"(?P<in>${1}+|${1}+)(?P<exact>(?:${1})|$)\".replace('|', '\\\\|' ), re.M)\n\t\nclass _Match:\n\t\"\"\"\n\tMatch based pattern.\n\t\n\tMatches based on start position and length of text.\n\t\n\t\"\"\"\n\tdef __init__(self, match:'re.Match'):\n\t\tself.match = match\n\t\tself."}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t# TODO: We should check whether this is the case using the class.\n\tif self.has_default_values is not None or self.default_values is not None:\n\t\t# We don't have a default value, but we can have either\n\t\t# a default-value parameter or a default value and\n\t\t# that default value is in parameters.\n\t\tself.expr = self.expr.replace(\"+\", \"\\\\+\", True)\n\t\tself.expr = self.expr.replace(\"$\", \"\\\\$\", True)\n\n\t# We have a default-value parameter,"}]}
{"task_id": "62b43426903eeb48555d3ea2", "prompt": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tpass\n\n\tclass _Converter(type):\n\t\tdef convert(cls, x):\n\t\t\ttry:\n\t\t\t\treturn self\n\t\t\texcept cls(*self._options[1:]) as e1:\n\t\t\t\ttry:\n\t\t\t\t\treturn cls(x)\n\t\t\t\texcept cls(*self._options[2:]) as e2:\n\t\t\t\t\traise ConvertError(e1, e2)\n\t\t\tfinally:\n\t\t\t\treturn None\n\t\t@classmethod\n\t\tdef from_string(cls, s, **kwargs):\n\t\t\tself = cls(**kwargs) "}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tconverters = {}\n\t\n\tfor name, cls in _name_mapping.items():\n\t\tif not issubclass(cls, _converting._Converter):\n\t\t\tcontinue\n\t\t\n\t\tconverter = cls()\n\t\t\n\t\tfor cname in cls.name_converter_methods:\n\t\t\tconverter.set(cname, getattr(cls, cname))\n\t\t\t\n\t\tconverters[name] = converter\n\t\n\treturn _converting._Converter(**converters)\n\n\ndef _create_reader(data):\n\t\"\"\"\n\n\tParameters\n\t----------\n\tdata\t\n\t\tThe"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tif self.value is None:\n\t\traise AttributeError(\n\t\t\t\"Cannot create converter for '{}' {}. \".format(\n\t\t\t\tself._parameter_name, repr(self._default_value_formatter)\n\t\t\t) + \"Try implementing \" + self._parameter_name + \"() \"\n\t\t)\n\t# Do default validation, and then add converter if valid.\n\treturn _converting._Converter(\n\t\tparameter_value=self.value,\n\t\tlabel_value=self._name,\n\t\tis_required=False,\n\t\tis"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tif self._style_converter is None:\n\t\tself._style_converter = _converting._Converter(self, _style=self._param_style)\n\treturn self._style_converter\n\n# =====================================================================\n#  INTERNAL FUNCTIONS\n# =====================================================================\n\n# ======================\n#  PARSER\n# ======================\n\nclass _Parser:\n\t\"\"\"\n\tInternal parser used by all functions in the module.\n\t\"\"\"\n\n\tdef __init__(self, parser_name, param_style):\n\t\t\"\"\"\n\t\t:param str parser_name:\n\t\t\t"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tparams = self.convert()\n\treturn _converting.AbstractRegistry(self.logger.logger,\n\t\tself.input, self.output,\n\t\t\tself._converting._AbstractParameterStyle)\n\n\n# class for the :func:`ParameterStyle` converter\nclass _converting._Converter(object):\n\t\"\"\"\n\tConverter :class:`._parameterStyle._ParameterStyleConverter` for\n\tparameter type conversion.\n\t\"\"\"\n\t# type information for the converter\n\ttypeinfo = Union[str, Callable[[str], str],\n\t\tCallable[[str, Callable],"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t# Make sure to create converters in case user supplied a converter that does not exist.\n\tconverter = _converting._Converter(\n\t\t_create_converters(self.converters, 'parameter', name) )\n\tconverter.parameters = self.parameters or {}\n\tconverter.parameters.__doc__ = \"\"\"\n\t\tDictionary of the converter parameters.\n\t\"\"\"\n\tif self.additional_parameters is not None:\n\t\tfor key, value in list(self.additional_parameters.items()):\n\t\t\tconverter.additional_parameters[ key ] = _converting._Converter(\n"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tself.converter = _converting._create_converter()\n\tself._converter_init()\n\treturn self.converter\n\n\ndef _create_converter(self) -> Converter:\n\t\"\"\"\n\tCreates a converter object.\n\t\"\"\"\n\tself.converter = Converter()\n\tself.converter.params_defaults = ()\n\tif not self.converter.do_init() and self.converter.has_params():\n\t\tself.converter.init()\n\treturn self.converter\n\n\ndef _load_parameters_from_file(self, filename: str):\n"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _paramstyle._make_converter(self.__value, self.__location, self.__style)\n\ndef _create_converter_dict(self) -> _converting._Converter:\n\"\"\"\n\tCreate the ``_converting._Converter`` parameter style converter\n\tfor the converter dictionary.\n\t\n\tReturns the ``_converting._Converter`` instance.\n\t\n\t:returns: The ``_converting._Converter`` instance.\n\t:rtype: :class:`._converting._Converter`\n\"\"\"\n\treturn _paramstyle._make_converter_dict(self.__value, self.__"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tconverter = _converting._Converter()\n\tconverter._add_properties(self.base_parameter)\n\treturn converter\n\ndef _create_converter(self, method_name: str) -> _converting._Converter:\n\t\"\"\"\n\t\tCreate a converter which converts :class:`.Parameter` objects into the\n\t\tgiven ``method_name`` style.\n\t\t\n\t\t:param method_name: The name of the converter.\n\t\t\n\t\t:type method_name: ``str``\n\t\t\"\"\"\n\tconverter = _converting._Converter(method_name)\n"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tdoc = \"\"\"\n\t\tCreate the parameter style converter.\n\t\"\"\"\n\timport cdecimal, csv, datetime, decimal\n\t_set_type_converter(self, doc, self.TypeConverter(), True, True)\n\t\n\tdef _convert_from_str(self, value):\n\t\t\"\"\"\n\t\tParse the string to float format.\n\t\t\n\t\tParameters\n\t\t----------\n\t\tvalue : `str`\n\t\t\tThe value of the parameter.\n\t\t\n\t\tReturns\n\t\t-------\n\t\t```\n\t\tAny :class:`cdecimal.Decimal"}]}
{"task_id": "62b8966c755ee91dce50a154", "prompt": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 644.00 MiB (GPU 0; 22.16 GiB total capacity; 19.75 GiB already allocated; 571.88 MiB free; 20.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b896de755ee91dce50a183", "prompt": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 730.00 MiB (GPU 0; 22.16 GiB total capacity; 19.80 GiB already allocated; 495.88 MiB free; 20.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b8a4a4755ee91dce50a3d3", "prompt": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# Try the timezone of the datetime. If that fails, fall back to the\n\t# (not always possible) UTC timezone.\n\ttry: # may want this to return a string or not.\n\t\treturn self.timezone.fromutc(dt)\n\texcept AttributeError:\n\t\tself.timezone = self._local_timezone(dt.astimezone())\n\t\tself._change_tz() # make changed; will update the timezone in the constructor\n\t\treturn self.fromutc(dt)\n\t\t# fall back to UTC\n\n#\n#  TODO:\n#"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tdt = dt.replace(tzinfo=None)\n\ttz = dt.tzinfo\n\tu = datetime(year=dt.year,\n\t\t\tmonth=dt.month,\n\t\t\tday=dt.day,\n\t\t\thour=dt.hour,\n\t\t\tminute=dt.minute,\n\t\t\tsecond=dt.second,\n\t\t\tmicrosecond=dt.microsecond,\n\t\t\ttzinfo=tz)\n\tu.fold = False\n\treturn u\n\n\ndef utcnow(dt):\n\tif hasattr(dt, \"tzinfo\"):\n\t"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif not isinstance(dt, datetime):\n\t\treturn dt\n\n\t# determine whether the datetime is ambiguous (if any)\n\t#\n\t# It would be nice if this weren't in the past, but at least it gives\n\t# us some opportunity to change our timezone of the return value to\n\t# indicate ambiguous.\n\t#\n\t# We have to check whether it's been marked ambiguous since we are\n\t# uncertain about this.\n\t#\n\t# While we could consider it ambiguous at first (we could have unambiguous\n\t# a value but"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# A new datetime is never ambiguous if it's in an ambiguous\n\t# timezone.\n\tif dt.tzinfo:\n\t\treturn self.new(dt.tzinfo).tz.fromutc(dt)\n\t# If there's no ambiguity, we fall back to the old way.\n\treturn dt.astimezone(self)\n\n\nclass tz_range(tzinfo_classfor):\n\t\"\"\"\n\tA :class:`tzinfo` subclass that sets *all* calendars to the\n\tsame *UTC* UTC.  A range is determined by one calendar"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt.tzinfo is not None:\n\t\traise ValueError('Ambiguous time-related objects are not currently supported.')\n\t\t\n\tif not dt.tzinfo or dt.tzinfo is None:\n\t\treturn super(dt_time, self).fromutc(dt)\n\t\t\n\ttzoffset = dt.tzinfo.utcoffset(dt) - self.tzinfo.utcoffset()\n\tif tzoffset == 0 and 0.0 >= dt.tzinfo.dst(dt):\n\t\treturn fromutcoffset(dt, self.tzinfo.dst(dt))"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\ttry:\n\t\t# XXX: This may not work in some situations...\n\t\tresult = pytz.datetime.fromutc(dt // datetime.timedelta(seconds=2),\n\t\t\t\t\t\t\t\t\t   timezone(dt) / lambda dt=self.timezone_name):\n\t\tresult.astimezone(self.timezone)\n\texcept pytz.tzinfo.UnknownTimeZoneError:\n\t\t# we couldn't find an unambiguous timezone in this timezone,\n\t\t# so assume that we have ambiguous datetime\n\t\tpass\n\treturn result\n\t\n\t\n\t\ndef now("}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# Get the timezone we'll work with.\n\ttz = _get_tz(dt)\n\t\n\t# Determine, if the local timezone is ambiguous.\n\t#\n\t# If we're on the local system at some point after DST, the system time\n\t# is ambiguous. The difference between DST and local timezone is recorded\n\t# to be negative. We therefore check whether DST is ambiguous by comparing\n\t# the difference value of local time and the current system time.\n\tsystem_at_utc = (tm = (dt.replace(hour=0, minute=0, second=0"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t#\n\t# Make sure we are dealing with a naive datetime instance, otherwise\n\t# an OSError will be raised when this function is called.\n\t# \n\tif isinstance(dt, datetime.datetime):\n\t\tdt = dt.replace(tzinfo=self.timezone)\n\telse:\n\t\traise \n\t\t\n\t#\n\t# Finally, return the timezone-aware timezone-aware datetime object.\n\t#\n\treturn time.tzset() if self.tz is None else time.tzset(self.tz)\n\t\t\t\n\n\t#~ returns\n\t#~"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tdt = localtz(dt)\n\t\n\t# Calculate the timezone offset\n\toffset = self.to_tz(dt.tzinfo).tzoffset()\n\t\n\t# If there's ambiguity, use timezone-aware datetime instead of timezone\n\t# time aware datetime.\n\ttz = dt.tzinfo\n\ttzo = tzo = tz\n\tif (tzo == offset or tzo == \"UTC\"       ) and len(tz) > 0:\n\t\tdt = dt.replace(tzinfo=tz)\n\t\n\t# Ensure dates are not"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\ttry:\n\t\tfrom collections import OrderedDict\n\t\t_ambig = OrderedDict(sorted(tz if tz else [self.tz], -freq))\n\texcept ImportError:\n\t\t_ambig = OrderedDict()\n\t# _ambig = OrderedDict(sorted(tz if tz else [self.tz], -freq))\n\t# The first occurrence of an unambiguous datetime will always result in the default zone\n\tambig = _ambig.get(_ambig.default or list(self.tz for _ in self.tz), self)\n\tif ambig:\n\t\tdt"}]}
{"task_id": "62b8a7b2755ee91dce50a4a7", "prompt": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif not isinstance(dt, datetime):\n\t\tif dt is not NaiveDate:\n\t\t\told = dt_as_epoch(dt)\n\t\t\ttry:\n\t\t\t\tdt = dt.replace(tzinfo=tzinfo)\n\t\t\texcept datetime.tzinfo.UnknownTimeZoneError:\n\t\t\t\tdt = dt.astimezone(tzinfo)\n\t\t\told = dt_as_epoch(old)\n\t\t\tif dt.tzinfo!= old.tzinfo:\n\t\t\t\tnew = dt_as_epoch(dt)\n\t\t\t\ttry:\n\t\t\t\t\tdt"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tfrom.base import DateTime, localzone_from_datetime\n\n\tis_local = isinstance(dt, localzone_from_datetime)\n\tif is_local:\n\t\tdt = dt._tz\n\tdt = (dt - tzinfo._utcoffset).replace(tzinfo=tzinfo)\n\tif is_local:\n\t\tdt = localzone_from_datetime(_make_aware(dt))\n\telse:\n\t\tdt = localzone_from_datetime(dt)\n\treturn dt.replace(tzinfo=tzinfo)\n\n\n@overload"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t# check and make sure we actually have a datetime\n\tif not isinstance(dt, datetime):\n\t\tdt = parse(dt)\n\tif not isinstance(tzinfo, tzinfo):\n\t\ttzinfo = deepcopy(tzinfo)\n\tif not isinstance(tzinfo, tzinfo):\n\t\traise ValueError(\n\t\t\t\"'tzinfo' parameter must be a tzinfo or None, not %s\"\n\t\t\t% type(tzinfo))\n\treturn default_tzinfo(dt, tzinfo)\n\ndef _get_first_zone(tzinfo_list):\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif not isinstance(dt, datetime):\n\t\tdt = parse(dt, tzinfo)\n\treturn dt\n\t\ndef parse_date(date_string, tzinfo=None, parsefunc=default_parse_datetime):\n\t\"\"\"\n\tGiven a date/time string in one of three formats, returns the correct\n\t:py:class:`datetime.date` object (or ``None`` if the date string has an\n\tinvalid format).\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil."}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt._tzinfo_utcoffset:\n\t\tdt = dt.astimezone(dt._tzinfo_utcoffset.utcoffset())\n\tif tzinfo == tzoffset.tzoffset(None):\n\t\treturn dt\n\telif tzinfo is not None and tzinfo.utcoffset() == tzinfo._UTCOffset():\n\t\td = tzoffset_to_datetime(dt._tzinfo_utcoffset, datetime=dt,\n\t\t\t\t\t\tnormalize=False)\n\t\treturn dt + d\n\telse:\n\t\treturn dt.replace("}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t# if the datetime is naive, convert it to an aware datetime\n\ttry:\n\t\tdt_aware = datetime.datetime.fromisoformat(dt.isoformat())\n\texcept (TypeError, ValueError):\n\t\tdt_aware = datetime.datetime.utcfromtimestamp(dt.utctimetuple())\n\tdt = dt_aware if dt_aware else dt\n\t# update tzinfo to be aware\n\tif tzinfo is None:\n\t\ttzinfo = dltz3.tzinfos.tzlocal()\n\telif isinstance(tzinfo, tzoffset):\n\t"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif not isinstance(dt, tzinfo):\n\t\tdt = tzinfo\n\ttz = tzinfo.tzzone(dt.hour)\n\tif tz in _tzinfos:\n\t\tdt = dt.astimezone(tz)\n\treturn dt\n\n\nclass _datetime_tz(datetime):\n\t\"\"\"\n\tA naive datetime subclass using the local timezone\n\t\"\"\"\n\ttimezone = tzinfo\n\n\tdef __new__(cls, tz):\n\t\t\"\"\"\n\t\t:return:\n\t\t\ta :py:class:`datetime.datetime` instance\n\t\t"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif not dt.tzinfo is None:\n\t\treturn dt\n\telse:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\n#include <iostream>\n#include <climits>\n#include <cmath>\n\n#include \"opencv2/opencv.hpp\"\n\n#include \"DendroPy.hpp\"\n#include \"DendroPyImpl.hpp\"\n\nusing namespace opencv;\nusing namespace std;\n\nnamespace dp\n{\n\tusing namespace std;\n\n\tDendroPy::DendroPy()\n\t{"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tfrom dtfi.datetime import datetime\n\treturn datetime.datetime.normalize(datetime(dt.timetuple(), tzinfo))\n\nusing OgamaControls;\nusing OgamaControls.ImportExport.TableControl;\nusing OgamaControls.ImportExport.SpreadSheet;\nusing OgamaControls.ImportExport.Xciibo;\nusing MahApps.Metro.Controls;\nusing System.Collections.Generic;\nusing System.ComponentModel;\nusing System;\nusing System.Diagnostics;\nusing System.IO;\nusing System.Threading;\nusing System.Windows."}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is False:\n\t\tdt = dt.replace(tzinfo=None)\n\telse:\n\t\tdt = dt.astimezone(dt.tzinfo.zone)\n\treturn dt\n\n\nclass _dateutil_parse_isoparse(object):\n\t\"\"\"\n\tWrapper around ``parse``.  Accepts arbitrary string as input, but does\n\tnot set any parameters that may have default values.  Parameters that are\n\tpassed to ``parse`` are automatically inserted onto the end of the\n\tstring, so it is generally safe for ``parse``"}]}
{"task_id": "6305f9991d275c6667163c50", "prompt": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself.cut_chars = before\n\tself.cut_chars_left = after * 2\n\n\nclass BytesPerLine(BytePerLine, metaclass=abc.ABCMeta):\n\t\"\"\"Represents a BytesPerLine for the main BytesPerLine object.\n\n\tThis class is used for when an instance is constructed.\n\tIt provides a class property to get the number of lines in the main\n\tBytesPerLine object.\n\t\"\"\"\n\tdef __init__(self):\n\t\tself.num_lines = 0\n\n\nclass ReachSplitter(abc.ABC):"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tif self._buffer is None:\n\t\treturn\n\n\tif self._file_position is not None:\n\t\tself._file_position += len(before) + len(after)\n\tself._buffer.reset()\n\tio_buf = self._buffer\n\n\tio_buf.seek(0)\n\tif self._file_position < 0:\n\t\tbefore = before.ljust(self._file_position)\n\tif self._file_position < 0 or self._file_position >= len(before):\n\t\tself._set_slice_char"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tif len(before) == 0:\n\t\tself.before = []\n\telse:\n\t\tbefore = bytes(before)\n\t\tif len(before) == len(after):\n\t\t\tself.before = before\n\t\telse:\n\t\t\tself.before = before + [b'\\x00' for x in after]\n\n#ifndef BOOST_MPL_SEQUENCE_C_HPP_INCLUDED\n#define BOOST_MPL_SEQUENCE_C_HPP_INCLUDED\n\n// Copyright Aleksey Gurtovoy 2000-2004\n//\n// Distributed under the Boost Software License, Version 1.0"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself.splits_before[SPLITS_START] = before\n\tself.splits_after[SPLITS_START] = after\n\n\ndef set_merge(self, file_name: bytes, options: bytes = None) -> None:\n\t\"\"\"\n\t\tSet options on output files so files split based on this are merged with this file. This is used\n\t\tto prevent duplicate filenames.\n\t\"\"\"\n\tfile_name = file_name.replace('\\\\', '/')\n\tself.merge[file_name] = merge(options)\n\n\ndef get_merge(self"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tif self.__data and self.__data_written:\n\t\traise AttributeError(\"Can't set the data before/after in slice\")\n\tself.__data = b\"\"\n\tif before is None:\n\t\tself.__data_written = False\n\t\tself.__data, self.__data_len = self.__data_start()\n\telif before!= b\"\" and self.__data_written:\n\t\traise AttributeError(\"Can't set after in slice in non-empty slice\")\n\tself.__data = bytearray(before)  # Bytes to slice\n\t\n\t\n\tif after is None"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself._before = before\n\tself._after = after\n\t\n\t# print(self._before)\n\t# print(self._after)\t#\n\t\n\tpass\n########################################################################################\ndef get_cutter(cutter: str = 'cut'){\n\tself._cutter_code = cutter\n\t\n\treturn self\n}\n########################################################################################\ndef save_slice(slice: slice, suffix: str ='slice'){\n\t# ----------- Save slice -----------\n\t\n# --------- Clip to the last cut point of that slice ---------\n\tself.cut(suffix"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = self.line_cutter(before)\n\tself.after = self.line_cutter(after)\n\t\n\tself.line_number = None\n\tself.line_start = None\n\tself.line_end = None\n\nclass cutter(object):\n\t\"\"\"\n\t    Class that encapsulates a stylized window of size CutterSize for\n\t    the purpose of providing a cutter with the help of the rest of the\n\t    codebase.\n\t\"\"\"\n\tclass Cutter(object):\n\t\t\"\"\"\n"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself.set_meta(b'X-Meta-Cut-Chars', before)\n\tself.meta_split()\n\tself.meta_write()\n\tself.meta_close()\n\tself.write(after)\n\ndef set_keep_parts(self, after: bool) -> None:\n\"\"\"\n\tSet the byte to keep the parts of the split file.\n\t\n\tArgs:\n\t    after: Set the number of parts after the split file.\n\t\"\"\"\n\tself.set_meta(b'X-Meta-Keep-Parts', after)"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before_chars = bytes(before)\n\tself.after_chars = bytes(after)\n\n@add_method({\n\t\"before\": \"0\",\n\t\"after\": \"2\"\n})\n@add_method({\n\t\"before\": \"2\",\n\t\"after\": \"4\"\n})\n@add_method({\n\t\"before\": \"2\",\n\t\"after\": \"4\"\n})\n@add_method({\n\t\"before\": \"2\",\n\t\"after\": \"4\"\n})\n@add_method({\n\t\""}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after\n\n\ndef set_cut_chars2(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after\n\n\n# -- Utility Functions -- #\n\n\ndef add_point(self) -> None:\n\t\"\"\"Add a point to the split file.\"\"\""}]}
{"task_id": "6306292052e177c0ba469f09", "prompt": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t# For private messages, the header field (public) contains a boolean value for the message header field. The \"status\" field (public) will always have the value 0.0 (OK).\n\tif request.headers.get(\"status\") is not None:\n\t\ttry:\n\t\t\treturn False\n\t\texcept:\n\t\t\treturn True\n\t# For public messages, the header field (private) contains a boolean value for the message header field. The \"response\" field (public) will always have the value 0.0 (OK).\n\telif request.headers.get(\"response\")"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tif not request.user.bot:\n\t\treturn \"\", None\n\tif getattr(request.message, 'is_private', False):\n\t\tif not request.message.is_private():\n\t\t\treturn \"\", None\n\t\tif getattr(request.message.message, 'is_diaspora', False):\n\t\t\treturn request.user.username, None\n\tif request.message.media.is_video:\n\t\treturn request.user.username, None\n\trequest.message.send_media()\n\tprivate = request.message.message.get_"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t# First try public message\n\tif (request.is_request == False):\n\t\treturn False\n\t\n\ttry:\n\t\t_ = getattr(request.method, request.uri)\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n\ndef _send_request(request: RequestType):\n\ttry:\n\t\t_ = identify_request(request)\n\texcept UnicodeDecodeError:\n\t\tr = ResponseType()\n\telse:\n\t\tr = getattr(request.method, request.uri).request(\"POST\", request.header)\n"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\twith_traceback('', 'trying to identify'):\n\t\treturn request.isPublic()\n\n\n\"\"\"\nprivate message has data\n\"\"\"\ndef private_message_with_json_body(request: RequestType):\n\twith_traceback('', 'trying to parse json body in private message'):\n\t\tjson_body, headers = request.body().read()\n\tprint(\"json_body:\\n{}\\nheaders:\\n{}\\n\".format(json_body.decode() if isinstance(json_body, unicode) else json_body, headers))\n\tprint"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\ttry:\n\t\trequest.is_request_insecure()\n\texcept:\n\t\t# this is a public / legacy message\n\t\treturn True\n\telse:\n\t\ttry:\n\t\t\trequest.body\n\t\texcept AttributeError as e:\n\t\t\t# TODO : handle this gracefully\n\t\t\treturn False\n\t\telse:\n\t\t\tfor line in request.body.splitlines():\n\t\t\t\t# first line looks like this. This is a public Message\n\t\t\t\tis_public = \"Public_\" in line\n\t\t\t\trequest.is_private == is_"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tif request.target!=1338: # We can ignore the request if it isn't coming from the bot\n\t\tchatting, _, client_id = get_client_key(channel_id=request.target)\n\t\tif (bool(chatting) and not request.source.lower() in ['bots', 'diaspora']):\n\n\t\t\t# This function is supposed to validate request parameters\n\n\t\t\tif request.channel!=chatting:\n\t\t\t\traise ValueError(f'A request is not a request from a channel that's not the message channel {request."}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tif request.destination == \"api\":\n\t\tif (request.body_part == \"/send\" and request.method in [\"post\", \"put\"] and request.version == \"2\"):\n\t\t\treturn \"2.0\"\n\t\tif is_payload(request):\n\t\t\treturn identify_payload(request)\n\telif request.destination == \"api_message\":\n\t\tif request.id and is_payload(request):\n\t\t\treturn \"[x]\"\n\telse:\n\t\treturn \"\"\n\ndef identify_payload(request: Type<RequestType>) -> str:"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tif hasattr(request, 'payload'):\n\t\treturn request.request_type == \"DIASPORA\" and request.payload.get(\"request_id\") == settings.DIASPORA_IDENTIFIER\n\telse:\n\t\tif request.request_type == \"Diaspora\":\n\t\t\treturn request.payload.get(\"request_id\") == settings.DIASPORA_IDENTIFIER\n\t\telif request.request_type == \"DIASPORA_LEGACY_PAYLOAD\":\n\t\t\treturn False\n\t\telse:\n\t\t\traise AssertionError(\"Unknown request type: %"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tis_diaspora_request = check_is_diaspora_request(request)\n\n\tif is_diaspora_request:\n\t\treturn request\n\t\t\ndef request_type_identify(request: Message):\n\t\"\"\"\n\tFind the request from the incoming message and identify it.\n\t\"\"\"\n\n\tif isinstance(request, RequestType):\n\t\treturn request or RequestType.unknown()\n\n\treturn identify_request(request)\n\t\t\n@request_type_identify\ndef public_message_request_from_user(request:"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tif request.startswith(\"Diaspora public\"):\n\t\treturn {\n\t\t\t'times': request.split(' ')[1:] if request.endswith('') else lambda d: request.split(' ')[1:],\n\t\t\t'version': int(request.split(' ', 1)[1])\n\t\t}[int(request[9:12])]\n\telif request.startswith(\"Diaspora private\"):\n\t\treturn True, int(request.split(' ', 5)[7])\n\telse:\n\t\traise ValueError(\"Unknown request {}\".format(request))\n\ndef handle"}]}
{"task_id": "6306292152e177c0ba469f0d", "prompt": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\trequest.event.identify_matrix()\n\treturn request.event.is_matrix()\n\ndef validate_request(request: RequestType) -> bool:\n\"\"\"\n\tIf it is a Matrix request, verify the matrix and return True if an error happened\n\t\"\"\"\n\tuser_event = request.request.event if not request.request.event else type(request.request)(request.request)\n\treturn user_event.is_matrix()\n\ndef normalize_matrix(request: RequestType) -> bool:\n\"\"\"\n\tConvert matrix form to a Matrix form\n\t\"\"\"\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\treturn request == RequestType.MATRIX_MATRIX_DETERMINISTIC_DETECTION\n\n\ndef make_id_generator_with_seed_value(*_: Any) -> Callable[..., Awaitable[T]]:\n\t\"\"\"\n\tWraps to make ID generator with random value.\n\n\tTODO: add check if request was rejected at random\n\n\t:param (callable)_: callable function.\n\t'''\n\treturn chain(\n\t\tdelayed(\n\t\t\tseed_for_requests(1),\n\t\t\tdelayed(get_next_available_id, id_generator_counter()),\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tparsed = request.parsed()\n\tis_matrix = parsed.get_method() == 'POST' and parsed.get_request().body\n\treturn is_matrix\n\n\ndef identify_matrix(parsed: ET.Element) -> bool:\n\t# TODO: check if Matrix request, as there's a lot of the common\n\t# code paths in the Matrix REST handlers.\n\treturn False\n\n\ndef identify_response(response: ET.Element) -> bool:\n\tparsed = response.get_text()\n\treturn parsed == '<html><body>Hello <b>Matrix"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t_type = request.method\n\tif _type == 'GET':\n\t\ttry:\n\t\t\tif request.uri.find('matrix/api/v0/') >= 0:\n\t\t\t\treturn True\n\t\texcept ValueError:\n\t\t\treturn False\n\tif _type == 'POST' and get_body_as_json_string(request.body):\n\t\ttry:\n\t\t\t_type = request.headers['Content-type']\n\t\t\tif _type == 'application/json':\n\t\t\t\treturn True\n\t\texcept KeyError:\n\t\t\treturn False"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tif request.type in [\"GET\", \"PUT\"] and not request.url.query:\n\t\tpass\n\telif request.type in [\"POST\", \"PATCH\"]:\n\t\tpass\n\telif request.type.startswith(\"$\"):\n\t\t# handle all other HTTP requests\n\t\treturn False\n\telse:\n\t\treturn False\n\ndef format_content(content: Optional[str]) -> Optional[str]:\n\t\"\"\"\n\tUsed by the :code:`jsonschema` module to format the output from a single function\n\t\"\"\"\n\tret = []\n\tfor line in content"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tresponse = requests.post(\n\t\tAPI_URL + \"validate/v1.1/authenticate\",\n\t\tjson={\n\t\t\t\"username\": config[\"username\"],\n\t\t\t\"password\": request[\"password\"],\n\t\t\t\"otp\": request[\"otp\"],\n\t\t}\n\t)\n\tif response.status_code!= requests.codes.ok:\n\t\tprint(\"Authentication failed!\")\n\t\treturn False\n\treturn True\n\n/*\nCopyright (c) 2003-2018, CKSource - Frederico Knabben. All rights reserved.\nFor licensing, see"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\treturn request.path.rsplit('/', 1)[-1] =='matrix'\n\ndef get_access(request: RequestType) -> int:\n\t\"\"\"\n\tCheck which access (if any) should be allowed to this request\n\t\"\"\"\n\treturn 'view' in request.scheme\n\ndef get_cache_control(request: RequestType) -> str:\n\t\"\"\"\n\tSet the cache control headers on the request headers that this handler will apply to (not headers from the server)\n\t\"\"\"\n\treturn request.headers.get('Cache-Control', None)\n\ndef"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tif not request.has_matrix() or request.version not in (1, 0):\n\t    return False\n\tversion = request.version\n\t\n\tif request.method in ('GET', 'POST'):\n\t\tif version < 1:\n\t\t    from zope.interface.verify import verifyClass\n\t\t    verifyClass(request.META, IMatrix)\n\t\t    return True\n\t\t\n\tfor key, value in request._META().items():\n\t    if version >= 1 and key == 'MATRIX' and not value.startswith('['):\n\t\t"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\treturn request == RequestType.Matrix\n\n\ndef identify_response(response: ResponseType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix response\n\t\"\"\"\n\treturn response == ResponseType.Matrix\n\n/*\n * Copyright 2017-2021 ProfunKtor\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2."}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tif f\"{request.type_key.lower()}_{request.slug}\" in request.user.keys():\n\t\treturn request.user[f\"{request.type_key.lower()}_{request.slug}\"] == request.user[f\"{request.type_key.lower()}_{request.slug}.id\"]\n\n\treturn False\n\n\ndef identify_user_with_permission(user: User, permission: Permission) -> bool:\n\treturn user.has_perm(\"matrix.read_public\", str(permission.name)) or user.has_perm(\"matrix.write"}]}
{"task_id": "6306292252e177c0ba469f11", "prompt": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn datetime.datetime.utcfromtimestamp(dt).strftime('%Y-%m-%d %H:%M:%S')\n\n################################################################################\n################################################################################\n#                        Cute Framework\n#\n# Copyright (c) 2015 PfadonConvenience SL\n# Copyright (c) 2018-2021, Antonin Delpeuch\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tif not isinstance(dt, datetime):\n\t\traise TypeError(\"expected datetime, got %r\" % dt)\n\n\tfrom_text = getattr(dt, 'from_text', None)\n\tto_text = getattr(dt, 'to_text', None)\n\n\tif from_text is not None:\n\t\tif from_text is not None:\n\t\t\treturn f'%s as DATETIME(dt.year, dt.month, dt.day, {from_text}, {to_text})'\n\t\traise TypeError('expected non-None from_text"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tfrom_dt, to_dt = dt\n\tif from_dt._is_datetime or to_dt._is_datetime:\n\t\targs = (from_dt, to_dt)\n\t\treturn dt.strftime(format_string_dt, *args)\n\n\ndef format_dt_iso(dt):\n\t\"\"\"\n\t\tConvert a datetime to a formatted date.\n\t\tOnly one iso8601-style specifier exists so far.\n\t\"\"\"\n\tfrom_dt, to_dt = dt\n\tif from_dt._is_datetime or to_dt._"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn datetime.datetime\n\n\ndef format_time_dt(dt):\n\t\"\"\"\n\t\tFormat a datetime in the way that t* nodes expect.\"\"\"\n\treturn datetime.time(None, 0, 0)\n\n\ndef format_offset(dt, offset, precision):\n\t\"\"\"\n\t\tFormat a datetime in the way that 'offset' is a positive number\n\t\tof days from now.\"\"\"\n\tif 0 >= precision:\n\t\treturn 'unknown'\n\treturn '00'+str(offset / 3600) +''+str(offset % 3600) +' '+ str"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n\ndef format_dt_time(dt_time):\n\tif not dt_time:\n\t\treturn \"?????\"\n\tvalue = dt_time.strftime(\"%y-%m-%d %H:%M:%S %z\")\n\treturn \"[%s] %s%s\" % (\n\t\tvalue, value, value,\n\t\t\"seconds\" if value.startswith(\"0:\") else \"minutes\", \"seconds\")\n\n\ndef format_time(s):\n\treturn \"%s"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn dt.strftime('%Y-%m-%dT%H:%M:%S')\n\n\ndef format_date(dt):\n\"\"\"\n\tFormat a date in the way that D* nodes expect.\n\t\"\"\"\n\treturn dt.strftime('%Y-%m-%d')\n\n\ndef format_time(dt):\n\t\"\"\"\n\tFormat a time in the way that D* nodes expect.\n\t\"\"\"\n\treturn dt.strftime('%H:%M:%:S')\n\n\ndef build_node_list(node_set, context):\n\t'''\n\tBuild a node's"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tif not hasattr(dt, \"tzinfo\"):\n\t\treturn dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n\treturn dt.astimezone(pytz.FixedOffset(dt.tzinfo.utcoffset)).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n\n\ndef format_dt_utc(dt):\n\t\"\"\"\n\t\tFormat utc according to the format_dt function.\n\t\"\"\"\n\tif not hasattr(dt, \"tzinfo\"):\n\t\treturn dt.strftime(\"%Y%m%d"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn datetime_formatter(dt, use_relative=True)\n\n\nclass DateTimeItem(QStandardItem):\n\t\"\"\"\n\tDisplay one DateTime item to the user\n\n\tA DateTimeItem has the following attributes\n\n\t* title: the item title\n\t* text: the time formatted text string\n\t* itemType: the name of the corresponding Qt Designer item type class\n\t* parent: the parent item\n\t* typeId: the item type id, typically this will be'string' as some D* widgets\n\t\"\"\"\n\n\tdef __init__(self, item_type,"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn to_string(dt).replace('*','')\n\n##################################################################################\n#\n# Functions to handle a file object.\n#\n##################################################################################\ndef get_file_contents(path, encoding='utf-8'):\n\twith open(path, 'rb') as f:\n\t\tif PY3:\n\t\t\treturn f.read()\n\t\telse:\n\t\t\treturn f.read()\n\t\ndef get_files(path):\n\twith open(path, 'r') as f:\n\t\treturn [line for line in f]\n\ndef"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn datetime.datetime.strptime(str(dt[0]),\"%Y-%m-%d\")\n\ndef to_bool(bool_val):\n\tif bool_val:\n\t\treturn True\n\treturn False\n\ndef is_string(string_val):\n\tif string_val is not None and not isinstance(string_val, (six.string_types, bytes)):\n\t\treturn True\n\treturn False\n\ndef is_int(num_val):\n\tif num_val > 0 and num_val in (-0x7fff-1, 0x"}]}
{"task_id": "6306292352e177c0ba469f1d", "prompt": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tif not hasattr(replacer,\"__call__\"):\n\t\treplacer = lambda text: text\n\t\n\treturn set(re.finditer(text, replacer=replacer))\n\n\ndef make_filename(path: str) -> str:\n\t\"\"\"\n\tMake a filename safe.\n\t\n\tOnly supports ASCII characters.\n\t\"\"\"\n\tfor char in path:\n\t\tif char.isalnum():\n\t\t\treturn char\n\t\tif char.islower(): # Make a filename safe.\n\t\t\treturn '_'.join(map(lambda x: ord(x"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tfor tag in re.findall(r'(?s)^(?:<[^>]*>)*(?:</[^>]*>[^>]*>)*', text):\n\t\t# The word is considered inside a single opening block, so just ignore it here.\n\t\tif not tag.endswith('.py'):  # py:ignore(backslash in string)\n\t\t# Find a replacement string, so that we can do replacements with it.\n\t\tregex = r'\\b%s\\b[^\\:]+\\b%s' % (re.escape(tag[:-1]), tag[-1:])\n"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttag = find_tag(text)\n\tif tag is None:\n\t\treturn text, \"\"\n\tif replacer is not None:\n\t\treturn replacer(tag), tag + text + replacer(tag)\n\treturn list(re.finditer(tag, text, re.IGNORECASE))\n\ndef find_tag(text: str) -> str:\n\t\"\"\"\n\tFind tag in text.\n\t\n\tReturn the tag (if it is one)\n\t\"\"\"\n\tif text.startswith(\"<\") or text.startswith(\">\"):\n\t\treturn text["}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t# Find tags inside text and save them in a set so we can use them while saving\n\t# it later\n\ttags = set()\n\ttag_string = \"\"\n\tfor index, char in enumerate(text):\n\t\tif char in WHITESPACE:\n\t\t\ttag_string += char\n\t\telif tag_string == \"\":\n\t\t\tif char in BLACKLIST:\n\t\t\t\ttags.add(\"!\" + char)\n\t\t\telse:\n\t\t\t\ttags.add(tag_string)\n\t\t\t\ttag_string = \"\"\n\t\telif index == index:"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\treturn utils._get_tags(text, False, replacer, True, True)\n\ndef find_tags_in(text, tags: List[Tag], replacer: Callable[[Tag], str]) -> List[Set]:\n\t\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tReturns a list of sets covering each tag.\n\t\n\tOptional arguments:\n\n\t- **tags** a list of Tag objects to search within.\n\t- **replacer** a function to replace the tag word found. Default is the \"replacer"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t# TODO: Implement this with str.replace(\"'\", \"\") here.\n\t# This is an attempt to make the parsing functions possible.\n\tif replacer is None:\n\t\treplacer = str.replace\n\t\t\n\ttags = {}\n\tword = ''\n\tresult = ''\n\t\t\n\t# Split text up into lines from all the text.\n\ttext = text.split('\\n')\n\t# Break text into lines and process them.\n\tfor line in text:\n\t\tif line.strip():\n\t\t\tif word!= '' and word!=''*len(word):\n"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\treturn (f.iter_tags(text, replacer) for (i, f) in enumerate(text.split(',')))\n\ndef find_ref_tag(tags: Set, replacer: callable = None, allowlist: str = None, replace: str = None) -> Set:\n\t\"\"\"\n\tFind reference tags in a Tagged String, and replace tag if it matches a list of tags\n\tor a tag with the same name in a tag with the same name.\n\t\n\tThis functions is case-insensitive.\n\t\n\tIf the \"replace\" argument is supplied, this will"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t#\n\t# first, go through all text and collect all tags\n\t#\n\tdef find_tags_recursive(self, text: str) -> Iterable[dict]:\n\t\tfor self.cur_tag in self.tagdefs:\n\t\t\t# print(self.cur_tag, text)\n\t\t\tif self.has_tag(text, self.cur_tag):\n\t\t\t\t# print(self.cur_tag, text)\n\t\t\t\t# do something to the tag\n\t\t\t\tself.cur_tag = self.cur_tag.replace_with(self."}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttext = text.strip()\n\tif not text:\n\t\treturn set(), \"\"\n\ttag_parser = TagPattern.Pattern(TagPattern.TagLine(block_tags=('bold', 'italic'), **re_tags) | re_syntax)\n\ttext = re_tag_parser.findall(text)\n\tif replacer:\n\t\treplacer(text)\n\ttag_parser.parse(text.strip())\n\tret = {}\n\tfor match in re_tag_parser.findall(text):\n\t\tret[match['name']] = match"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttext = remove_tags(text) # remove all HTML tags from text (a common example)\n\t\t\n\tif replacer == None:\n\t\treplacer = lambda word: word  # use an empty replacer if given a function\n\t\t\n\ttags = set()\n\t\n\ttext = _replace_tag(text, replacer)\n\ttext = _replace_tag(text, \"\u2022\u2022\u2022\u2022\u2022\") # replace all single tags with spaces (only in code)\n\t\n\ttext = text.replace('*','') # replace all dashes with a single unders"}]}
{"task_id": "6306292352e177c0ba469f1e", "prompt": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tlinks = [(1, 'http://www.pavlovia.com'), (2, 'http://www.deut.de/recherche/en/indexing-articles/'), (3, 'http://www.wwwixin.de/pavlosis/'), (4, 'http://www.hiphotos.co/pavlosis/photo-crise.pdf?fid='), (5, 'http://www.wwwixin.co/'), (6, 'http://www.wwwixin.co/pavlosis/'),"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tfor t in text.split('\\n'):\n\t\ttext = re.sub('^\\s*#[^\\s*]+\\n*', '', t)\n\t\tif text == '\\n':\n\t\t\ttext =''\n\t\tif t[0]!= '#':\n\t\t\tt = re.sub('^\\s*(?<!(?:[a-z])[0-9a-z]+\\s*)+', '', t)\n\t\tif t[0] == '#':\n\t\t\tt = (re.sub(\n\t\t\t\tr'(?<!\\[)(\\.\\."}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tif text:\n\t\ttext = replace_links(text)\n\t\tif text.startswith('#') and not text.startswith('^'):\n\t\t\ttext = '^' + text\n\t\tif text.endswith(\"\\n\"):\n\t\t\ttext = text[:-1]\n\t\tfor url in doc_uris:\n\t\t\tif text.startswith(url):\n\t\t\t\tstart = re.search(r'<a[^>]+src=\\s*([\\'\"]?)[\\'\"]?\\s*>(?:href=|src=)(\"|\\')([^>]+)(>|$)', text, re"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tfor link in text:\n\t\tif not link:\n\t\t\tcontinue\n\n\t\tif not re.match(r'\\s+', link):\n\t\t\tcontinue\n\t\tif link and link!=''.join(_link_chars):\n\t\t\t_link_chars.add(link)\n\n\t\tif link.isalnum():\n\t\t\t# It's a word character, e.g. `link` becomes `a.link`\n\t\t\t_link_replacement_map.setdefault(link.lower(), link)\n\t\t\t_link_url_regex.add(("}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tlinks = []\n\turls = []\n\tfor i in range(len(text)):\n\t\t# if there are no backticks (which are present in urls)\n\t\tif len(text[:i+1]) > 0 and text[0:i+1] == '```':\n\t\t\turls.append(text[i+1:])\n\t\t\ti += 1\n\t\telif len(text[:i]) > 0 and text[0:i] == '```:\n\t\t\tlinks.append(text[i:])\n\t\t\ti += 1\n\t\telse:"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t# TODO: If it looks similar to https://github.com/pywemo/pywemo/issues/1820,\n\t# don't use linkify_urls in the following code.\n\t# The issue is an issue with this behavior, I am only interested in the\n\t# difference in behaviour.\n\tlinks = []\n\tfor match in re.finditer('^<(a|blockquote)>(.+?)</body>\",\n\t                         text):\n\t\tattrs = []\n"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = remove_ampersands(text)\n\t# remove empty links and image links\n\ttext = remove_links(text)\n\n\t# clean text\n\t# Remove extra spaces\n\ttext = clean_text(text, remove_special_spaces=True)\n\n\t# Convert some special symbols\n\tif convert_alts:\n\t\ttext = convert_alts(text)\n\n\treturn text, convert_links\n\n\n# -----------------------------------\n# \t:                     "}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tif not text:\n\t\treturn \"\"\n\t# remove trailing [xxx] from text\n\ttext = re.sub(\"[x*[0-9]+[ ]*x\", \"\", text)\n\t# remove leading [xxx] from text\n\ttext = re.sub(\"[x*[0-9a-z-]+[ ]*x\", \"\", text)\n\t# replace [xxx] with links\n\ttext = text.replace('[x*[0-9]+[ ]*x]', '[xxx]')\n\t# replace [xxx] with links\n\ttext = text.replace('[x*["}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tif 'href' in text:\n\t\ttext = text.replace(' href=\"%s\"' % text.split(' href=')[-1], text, 1)\n\ttry:\n\t\t# Remove unicode characters from the text\n\t\ttext = str(text, 'utf8').replace('\\\\xA0', '-')\n\texcept TypeError:\n\t\treturn False\n\turl = text.split('\\n  href=\"\\')[1].strip(' \\\\s\\r\\n') or 'http://' + text.split('\\n  href=')[1].strip(' \\\\s\\r\\n')"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext.replace(\"<\", \"&lt;\")\n\ttext.replace(\">\", \"&gt;\")\n\ttext.replace(\"\\n\", \"&#xA;\")\n\ttext.replace(\"\\r\", \"&#xD;\")\n\ndef process_links(linkings):\n\tfor i in linkings:\n\t\tlink     = linkings[i]\n\t\thref     = link[\"href\"]\n\t\ttitle    = link[\"title\"]\n\t\ttitle    = title.replace(\"&\", \"&amp;\")\n\t\ttitle    ="}]}
{"task_id": "6306292652e177c0ba469f34", "prompt": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\timport mimetypes\n\timport urllib\n\tmimetype = mimetypes.guess_type(url)[0]\n\tfor extension in ['.html', '.css', '.js', '.gif', '.srt']:\n\t\tfilename = urllib.url2pathname(urllib.url2url(urllib.urlopen(url).read()))\n\t\tif filename.find(extension) >= 0:\n\t\t\treturn mimetype\n\t\nclass HTTPUriExtractor(SimpleHandler):\n\tdef _should_remove_extension(self, filename: str) -> bool:\n\t\treturn \".\" not in"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\trequest, error = build_request(url)\n\ttry:\n\t\trequest_info(request)\n\tfinally:\n\t\tif request.status_code == 429:\n\t\t\traise HttpRetryError(request.url, request.args, connection=False)\n\treturn request.get_header(\"Content-Type\")[0]\n\n#include <algorithm>\n#include <tuple>\n\n#include \"../util/check.h\"\n#include \"../util/debug.h\"\n#include \"../util/path.h\"\n#include \"../util/string.h\""}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\turl = urllib.parse.urlparse(url) if is_url_supported else url\n\turl = urlparse.urlsplit(url) if is_url_supported else url\n\t\n\twith contextlib.closing(urllib.request.urlopen(url).read()) as rsp:\n        code = rsp.code\n        resp_msg = rsp.msg\n        resp_headers = rsp.headers\n\n        if code!= 200:\n        "}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tfurl = url + \"/\"\n\tresponse = requests.head(furl)\n\tcontent_type = response.headers[\"content-type\"]\n\tif content_type:\n\t\treturn content_type\n\treturn None\n\n\ndef urlinfo_get(url: str) -> Tuple[str, Callable]:\n\n\tcontent_type: Optional[str] = None\n\ttry:\n\t\tcontent_type = fetch_content_type(url)\n\texcept:\n\t\treturn \"\"\n\ttry:\n\t\treturn requests.get(url).text\n\texcept"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tcontent_type = None\n\ttry:\n\t\tr = urllib.request.urlopen(url)\n\texcept urllib.error.HTTPError as e:\n\t\traise e\n\telif not r.headers.has_key(b\"content-type\"):\n\t\traise Exception(\"Could not find content-type header.\")\n\texcept urllib.error.URLError as e:\n\t\traise e\n\texcept HTTPException as e:\n\t\tprint(\"Error fetching content-type, trying HTTP.\", file=sys.stderr)\n\t\tsys.exit(1)\n"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t# If \"fetch_content_type\" is invoked without parameters,\n\t# or if \"get_content_type\" was invoked with some request parameters,\n\t# it returns None and no exception is raised.\n\ttry:\n\t\tif url.startswith(\"http://\") or url.startswith(\"https://\"):\n\t\t\turl = get_http_url(url)\n\t\treq = requests.head(url, timeout=20, verify=False)\n\t\tcontent_type = req.headers.get(\"Content-Type\")\n\t\tif content_type:\n\t\t\t"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tfrom urllib3 import getproxies, proxy_bypass_config\n\n\tremote_file = proxy_bypass_config(\n\t\turl, PROXY_BYPASS_CONFIG[\"ftp_proxies\"][\"anonymous\"]\n\t)\n\ttry:\n\t\tdata = getproxies(\n\t\t\turl=remote_file, proxies=PROXY_BYPASS_CONFIG[\"ftp_proxies\"]\n\t\t)\n\texcept socket.timeout:\n\t\t# This may happen when ssh tunnel is used as default, such as\n\t\t# in an ssh setup.\n\t\tif settings.get(\""}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\ts = spider.loadurl(url)\n\thead_parsed = s.head(spider.headers['content-type'])\n\treturn head_parsed.headers['content-type']\n\n\ndef validate_content_type(parsed_content_type: str) -> bool:\n    return (parsed_content_type == 'text/plain' or\n            parsed_content_type == 'application/json'\n            or _accept_xml_or_json_parser)"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tcurl = curl.Curl()\n\tcurl.setopt(curl.URL, url)\n\n\tcontent_length = None\n\ttry:\n\t\tresponse = curl.exec_content()\n\n\t\tif response.info.code == 200 or response.info.code == 206:\n\t\t\tcontent_length = response.info.headers.get(\"Content-Length\",\n\t\t\t\t\tNone)\n\t\telse:\n\t\t\tif response.info.code == 404:\n\t\t\t\traise DownloadError(\"Could not find remote {0}\".format(url))\n\t\telse:\n"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t# If not http, do nothing\n\tif 'http://' not in url:\n\t\treturn url\n\n\t# Get the HEAD of the file\n\t_head_ = None\n\tfor _ in ('HTTP_RANGE', 'HTTP_IF_RANGE', 'HTTP_IF_MATCH', 'HTTP_IF_UNMODIFIED_MATCH'):\n\t\t_head_ = http_head(url)\n\t\tif _head_:\n\t\t\tbreak\n\t\telse:\n\t\t\turl = url.split('/', 1)[0]\n\t# If there is no HEAD, bail\n\t"}]}
{"task_id": "6306292a52e177c0ba469f41", "prompt": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tif tag == \"S\":\n\t\treturn True\n\tif tag == \"T\":\n\t\treturn False\n\n\ttags = [x for x in tags if x[1] == tag]\n\tif len(tags) == 1:\n\t\treturn True\n\tnew_tags = sorted(\n\t\tset(tags) | set([\"s\", \"S\", \"t\"])\n\t\t| set(tags) | set([\"t\"]))\n\t#print new_tags\n\treturn new_tags == tags[1:]\n\ndef test_no_tag(word: str)"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tif tag.lower().startswith('train') or \\\n\t   tag.lower().startswith('test'):\n\t\treturn True\n\telse:\n\t\traise ValueError(\"Unknown tag type %s\" % tag)\n\n\nclass BertGenerationConfig:\n\t\"\"\"A class that returns a training config for Bert\"\"\"\n\tclass TokenizeConfig:\n\t\t\"\"\"Class to hold train config and token_type\"\"\"\n\t\tclass TokenizerType:\n\t\t\treturn type(words)\n\n\tinput_config_type = (\n\t\t(\"model\", tf.keras.layers."}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t# Arrange\n\tparser = BertConfigParser()\n\t# Act\n\tresult = parser.parse(tag)\n\t# Assert\n\tprint(result)\n\tassert result['type'] == 'tag'\n\ndef test_user(user: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a user.\n\t\"\"\"\n\t# Arrange\n\tparser = BertConfigParser()\n\t# Act\n\tresult = parser.parse(user)\n\t# Assert\n\tprint(result)\n\tassert result['type'] == 'user'\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tif not tag:\n\t\treturn False\n\tif tag in taggables:\n\t\treturn True\n\n\tprint(tag)\n\tif tag.lower() == \"punct\":\n\t\treturn True\n\tif tag.lower() == \"del\" and tag == \"f\":\n\t\treturn True\n\tif tag.lower() in taggables:\n\t\treturn True\n\treturn False\n\n\n@pytest.mark.slow\ndef test_in_place(word: str) -> bool:\n\t\"\"\"\n\tTest a word whether it could be accepted as"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t@click.option(\n\t\t\"-t\",\n\t\t\"--tag\", help=\"Test a tag (test one)\")\n\t@click.option(\n\t\t\"-y\", \"--yes\", is_flag=True, help=\"Use the word to do a yes test.\")\n\n\t@command()\n\tasync def _(event):\n\t\t\"\"\"\n\t\tTest a word\n\t\t\"\"\"\n\t\ttag: Optional[str] = None\n\t\tyes: bool = False\n\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\ttags = await input(f\"Please"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tif not re.search(r\"/[^a-zA-Z]+/\", tag):\n\t\treturn False\n\treturn True\n\n\ndef test_get_config() -> None:\n\tif not _config():\n\t\traise Exception(\"Could not load config file\")\n\n\ndef test_get_path() -> None:\n\tpath = re.sub('[^\\w\\.\\/]+', '', _config.get('path'))\n\tassert os.path.isdir(path), path\n\n\ndef test_get_config_path() -> None:\n\tpath = _"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tresult = list_tags(tag, 'word')\n\treturn len([x if x >=0x80 else int(x, 16) for x in result])==1\n\n\ndef test_subword(sub: str) -> bool:\n\"\"\"\n\tTest whether sub could be accepted as a word.\n\t\"\"\"\n\tsub_result = list_tags(sub, 'word')\n\treturn len(sub_result) > 2\n\n###########################################\n\n\n\n  ################################################################\n\n\n\n\n def test_list_words(file: str) -> bool"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tif tag in tag_dict:\n\t\treturn True\n\treturn False\n\n\ndef load_dict(input_file, key_to_word, output_list: List[str]) -> str:\n\t\"\"\"\n\tload word list using the corpus dict and word list file\n\n\tReturn:\n\tList of strings representing word list\n\t\"\"\"\n\tif not os.path.exists(\"%s\" % output_list):\n\t\terror_message(\"Did not find dictionary file %s\" % output_list)\n\telse:\n\t\twith open(\"%s\" %"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tif re.match(r\"^\\w\\s*$\", tag) is None:\n\t\treturn False\n\tif re.match(r\"\\w$\", tag) is None:\n\t\treturn False\n\treturn True\n\nrequire 'test_helper'\n\nmodule Workarea\n  class BillingIntegrationTest < IntegrationTest\n    class InvalidProviderTokenTest < IntegrationTestCase\n      setup :default_billing_profile\n\n      def test_invalid_provider_token\n        assert true\n      end"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\twords = _splitOnWords(tag)\n\tfor w in words:\n\t\tif not w in _WORD_ACCEPTED_TAGS:\n\t\t\treturn False\n\treturn True\n\n\ndef test_tagcase(tag: str) -> bool:\n\"\"\"\n\tTest a sentence whether it could be accepted as a tagcase.\n\n\t:return:\n\t\t- False: no, it could be accepted,\n\t\t\tso it's the same as no;\n\t:param:\n\t\t- a word\n\t\t\t(e.g., \"Its\" for no"}]}
{"task_id": "6306298b52e177c0ba469fdc", "prompt": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tdict = {}\n\tif node is not None and node.has_nodes() and \\\n\t\t(node.tag == 'xml' or node.tag == 'xml:lang'):\n\t\tfor child in node.nodes():\n\t\t\t\n\t\t\tif isinstance(child, xml.sax.Text):\n\t\t\t\tdata = child.data.decode(encoding='utf-8',errors='replace')\n\t\n\t\t\t\ttry:\n\t\t\t\t\tdict[child.tag] = data.decode(encoding='utf-8',errors='replace')\n\t\t\t\texcept UnicodeDecodeError:\n\t\t\t\t\tdict"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tdata = {}\n\tfor child in node:\n\t\tdata[child.tag] = xml_children_as_dict(child)\n\t\t# recurse into this node's children\n\t\tif child.tag in data:\n\t\t\tdata[child.tag] = dict__recurse(data[child.tag])\n\treturn data\n\n/*\n * Copyright (c) 2016, Xilinx Inc. and Contributors. All rights reserved.\n *\n * SPDX-License-Identifier: BSD-3-Clause\n */\n\n#include <metal/errno.h>\n#include <"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tnode = _child_iter(node)\n    nodes = [_node_for_xml(child) for child in node]\n    return dict((_tag(node),) for node in nodes)\n\n<?php\n/**\n * File containing the ezcGraphChartCommonOptions class.\n *\n * @package Graph\n * @version 1.4\n * @copyright Copyright (C) 2005-2009 eZ Systems AS. All rights reserved.\n * @license http://ez.no/licenses/new_bsd New BSD License\n */\n/**\n * The common options of chart"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tout = {}\n\t\n\tfor cname in node.documentElement.children:\n\t\tname = cname.elementName()\n\t\tout[name] = xml_children_as_dict(cname)\n\n\treturn out\n\ndef xml_tag_iter(node):\n\"\"\"\n\tUse this function to iterate over a sequence of sibling xml nodes,\n\textracting all of the subnodes.\n\t\n\tThis is used by xml_children_as_dict().\n\t\n\t@return all of the key/value pairs used to create the string in\n\t'xml' format."}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\treturn dict(\n\t\tid = id_,\n\t\ttag_name = tag_name_,\n\t\tchildren = node.childNodes.items()\n\t)\n\ndef xml_find_propertytag_by_property(property,\n                                     tags_to_fetch=()\n                                "}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tchildren = []\n\tfor child in node.iter(tag=lambda node: node[\"tag\"]):\n\t\tchildren.append(xml_children_as_dict(child))\n\t\n\treturn children\n\ndef xml_attr(node, key):\n\treturn node[key]\n\ndef xml_text(node):\n\treturn node[\"text\"]\n\ndef xml_find_text(node):\n\tfor child in (n for n in xml_children(node)):\n\t\tif xml_text(n[\"node\"]) == xml_text(child):\n\t\t\treturn"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\treturn dict([ (k,v[0]) for k,v in node.items() if isinstance(v,xml.Node) and v.tag == \"xml\" ])\n\t\ndef xml_attr(node, attr):\n\t#\tCheck for nodes that are allowed in certain attributes.\n\t\n\treturn node.get(attr,None)\n\t\ndef validate_xml(node):\n\t#\tValidate that node is a valid XML\n\t\n\ttry:\n\t\tnode.toxml()\n\texcept:\n\t\t_ = \"{} not of type '\".format(repr("}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tresult = {}\n\tchildren = node.getchildren()\n\tfor child in children:\n\t\tresult[child.tag] = xml_children_as_dict(child)\n\treturn result\ndef xml_child_tag_from_string(start_tag_name):\n\t\"\"\"\n\tTurn the specified tag name as a child of a node.\n\t\n\tReturns the innermost tag with the specified name, or the provided\n\tnode itself if it doesn't include it.\n\t\n\tRaises ChildNotFoundError if no tag with the specified name is found.\n\t\n\tThis is only shallow"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tif isinstance(node, xml.dom.NodeBase):\n\t\treturn getattr(node, 'children', list())\n\tif hasattr(node,'childNodes'): return xml_children_as_dict(node.childNodes)\n\treturn {}\n\ndef xml_child_as_dict(node, tagName, attributes=None, parent=None):\n\ttry:\n\t\tif not isinstance(node, xml.dom.NodeBase):\n\t\t\tnode = xml_transform_non_dom(node, xml)\n\texcept AttributeError:\n\t\tpass\n\tresult = xml_element_"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tdata = {}\n\tif node is not None:\n\t\tfor tag in node:\n\t\t\tif tag.lower().startswith('http://') or tag.lower().startswith('https://') or tag.lower().startswith('urn:'):\n\t\t\t\tdata[tag.lower()] = True\n\t\n\treturn data\n\t\n\t\nclass MyClass:\n\tpass\n\nif __name__ == '__main__':\n\tt = MyClass()\n\tprint(xml_child_and_attributes_as_dict(t))\n\tprint(xml_child_attributes_as_dict("}]}
{"task_id": "6306299052e177c0ba469fe8", "prompt": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle!= payload['SenderObject']['guid']:\n\t\traise InvalidArgument('Sender is not the sender for payload.')\n\tif entity_handle!= payload['Entity']['guid']:\n\t\traise InvalidArgument('Entity is not the entity for payload.')\n\t\n\n\ndef check_valid_handle_list(payload_json, valid_handle_list):\n\t\"\"\"\n\t\tEnsure that a list of handle handles match each other.\n\t\n\t\tFor example a valid handle list of:\n\t\t\t['A', 'X',...]\n\t\tis valid because handle 1: A"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tverify_sender_and_entity_handle(sender_handle, entity_handle)\n\n\n@pytest.fixture\ndef logdir():\n\t\"\"\"\n\t\tSetup fixture for temp/ directory used to test logs\n\t\tThis is temporary fix for the case we use the global enviroment variable for configuration\n\n\tReturns\n\t\"\"\"\n\treturn os.path.join(os.environ.get('TEMP_DIR'), 'Diaspora.log')\n\n\n@pytest.fixture\ndef log_level():\n\t\"\"\"\n\t\tSetup fixture for global setting. Tests will set"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle!= entity_handle:\n\treturn False\n\treturn True\n\n/**\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle.user:\n\t\tif entity_handle.user and sender_handle.user!= entity_handle.user:\n\t\t\traise ValueError(\"Cannot send entity with different user than sender.\")\n\t\tif sender_handle.key:\n\t\t\tif entity_handle.key!= sender_handle.key:\n\t\t\t\traise ValueError(\"Cannot send entity with different key than sender.\")\n\t\t\tif sender_handle.key == entity_handle.key:\n\t\t\t\treturn True\n\telse:\n\t\tif sender_handle.user == entity_handle.user:\n\t\t"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle == entity_handle:\n\t\tsender = sender_handle\n\t\tif sender_handle == \"Diaspora\":\n\t\t\tsender = None\n\t\telse:\n\t\t\tsender_name = get_message_body_string(sender, encoding=\"utf-8\", remove_html=True)\n\t\tif not isinstance(sender_name, str):\n\t\t\traise TypeError('Could not retrieve sender name: %r type %s'\n\t\t\t\t\t\t   % (sender_name, type(sender_name)))\n\telif sender_handle:\n\t\t"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tassert sender_handle.__class__ == type(entity_handle), (sender_handle, entity_handle)\n\n\tif not entity_handle.is_valid():\n\t\treturn\n\t\t\n\tentity_handle_recipient_map = dict((r.entity_id, r.recipient_id) for r in entity_handle.recipients())\n\t\t\n\tsender_handle_recipient_map, sender_handle_entity_ids = get_sender_address_to_recipient_id_map(sender_handle)\n\t\n\tassert sender_handle_recipient_map, \"Sender's address"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif entity_handle in sender_handle:\t# NOTE: the sender is the recipient here\n\t\tif sender_handle < entity_handle:\n\t\t\treturn False\n\telif sender_handle > entity_handle:\n\t\treturn False\n\treturn True\n\ndef check_sender_and_entity_handle_contain(sender_handle, entity_handle):\n\t\"\"\"\n\tEnsure that sender and entity handles do not contain a given entity handle.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle!= entity_handle:\n\t\treturn False\n\treturn True\n\n\ndef create_sender_object(sender_handle, payload, sender_role=sender_handle_data_role, data_entity=data_handler_entity, entity_handle_key=ENTITY_DATA_KEY):\n\t\"\"\"\n\t\tGenerate an AdobeSender object for the supplied HandleData objects.\n\t\tFor the special case for the DiasporaHandler this creates a self-signed Sender object with a data author for\n\t\tthe entity, in that case.\n\t\t\n\t\t"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tsender = None\n\tif sender_handle:\n\t\tsender = get_requester_person(0)\n\t\tif sender == sender_handle:\n\t\t\treturn True, sender_handle\n\telse:\n\t\tsender = 0\n\t\tfor row in db_requester_get_first_for_person(sender_handle):\n\t\t\tsender_handle = row['user_handle']\n\t\t\tif not sender:\n\t\t\t\tif sender_handle == entity_handle:\n\t\t\t\t\treturn True, entity_handle\n\t\t\telse:\n\t\t\t\treturn False,"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle!= entity_handle:\n\t\traise NotMatchingError(\n\t\tu\"Unexpected reply. Expected the sender \" + sender_handle + \".\" +\n\t\tu\" Received \" + entity_handle + u\".\"\n\t\t)\n\n@author_context\ndef _parse_headers(message):\n\t\"\"\"\n\tParse out the headers and check the message payload type.\n\t@param message:\n\t\"\"\"\n\theaders = {\n\t\t'Accept': 'application/json',\n\t\t'Authorization': 'Bearer {token}',\n\t\t'User-"}]}
{"task_id": "630629b952e177c0ba46a043", "prompt": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t# Check url argument\n\tif not url.startswith(\"https:// \") or not url.endswith(\"/\"):\n\t\traise ValueError(\"Invalid URL: %s\" % url)\n\t\n\t# Generate a NodeInfo.well-known document\n\tnodeinfo_document = {\"nodes\" : {\"nodes\": {}}, \"status\": True, \"error\" : {}, \"debug\" : {}, \"nodes\": {} }\n\t\n\t# Parse node info url\n\ttry:\n\t\tdocument = botocore.create_client((url, 0, \"GET\"))\n\texcept botocore.exceptions.Client"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t# First request wether a document exists in the system so we know to\n\t# return an empty value. This allows us to re-render without having\n\t# to use the entire http client\n\tif document_path:\n\t\tresponse = NodeInfoDocumentClient.get_by_path(url, document_path)\nelse:\n\tresponse = NodeInfoDocumentClient.get_by_url(url)\n\n\t# Check for errors, e.g. 404 Not Found\n\tif response.json()[\"error\"] == \"not_found\":\n\t\treturn {}\n\n"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\treturn _parse('document/%s' % url, document_path, 'nodeinfo well known document')\n\ndef get_web_session_info(params, url=None):\n\t\"\"\"\n\t\tRequest HTTP Authentication details for a session and get session details.\n\t\tSee also HTTP Authentication for a simple example, and a more complete example of authentication.\n\t\t\n\t\t:arg params: GET parameters\n\t\t:arg url: An optional user supplied full url\n\t\t:returns: dict\n\t\"\"\"\n\turl, user_id, _ = _parse('session/%s' %"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\timport re\n\timport urllib2\n\timport urllib.request\n\timport binascii\n\timport struct\n\n\tnodeinfo_url = urllib.urlnorm(url)\n\tif not isinstance(url, str):\n\t\traise Exception('Not a.well-known URL: %s' % url)\n\tparsed_url = urlparse.urlparse(nodeinfo_url)\n\tif type(document_path) == str:\n\t\ttry:\n\t\t\tdocument_path = URLToPath(document_path)\n\t\texcept ValueError:\n\t\t\traise Exception"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t# TODO:\n\t\n\t'''\n\tThis does not follow the RFC.  That is, there are also cases where the\n\tdocument does not exist, thus we can say \"404 not found\" when.well-known\n\tis defined - and the \"404 response\" when document exists.  One of these\n\terrors might have happened in the middle of the document search.  If the\n\tresponse is not returned, we just assume it's the same error message as\n\twe got in the request, but the actual document response might contain that\n\terror.\n\t'\n\t'''\n\t\n"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tif url == \"\" or url.startswith(\"/\"):\n\t\t_log.error(\"The url parameter of the method 'get_nodeinfo_well_known_document()' needs to be a URL.\")\n\t\treturn False\n\t\n\tif document_path == None or os.path.isabs(document_path) == False:\n\t\t_log.error(\"The parameter 'document_path' of the method 'get_nodeinfo_well_known_document()' needs to be a path to the document as a relative or an absolute path.\")\n\t\treturn False\n\t\n\ttry:\n"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tfrom scipy import ndimage\n\tfrom subprocess import call\n\n\tdef create_filename(filename):\n\t\tdef repl(fstring):\n\t\t        \"\"\"\n\t\t        Replace underscores in filename to colons.\n\t\t        \"\"\"\n\t\t        return f\"\\xd_\\\\\" + fstring.replace(\"_\", \"_-\")\n\t        filename = filename.replace(\".\", \"_\")\n\t        filename = filename.replace(\"-\", \"_\")\n\t   "}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tfrom xml.etree import ElementTree\n\tfrom urllib.error import URLError\n\t\n\tdocument_path = document_path or document_url or conf.url()\n\n\t# Parse the document URL\n\tpath, path_parts = u'/' + document_path.lstrip('/') + '/'\n\tparsed = urlparse(document_url)\n\tdocument_url = parsed.netloc + path + parsed.query\n\tpath = document_url.split('/')[-1]\n\t\n\t# We expect a <well-known-xml> element\n\tif not path.endswith"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tfrom.base import Base, NodeInfo\n\tif document_path is None:\n\t\tdocument_path = os.path.join(os.getcwd(), 'example')\n\tdocument_path = os.path.abspath(document_path)\n\turl = os.path.abspath(os.sep.join([document_path, url.split(os.sep)[-1]]))\n\tdoc = Base(url)\n\tdoc['filename'] = os.path.splitext(os.path.basename(url))[0]\n\treturn doc\n\npackage com.github."}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tresponse = {}\n\tparams = {\n\t\t'action' : 'NodeInfoGetWellKnownDocument',\n\t\t'url' : url,\n\t\t'path' : document_path\n\t}\n\n\tr = requests.post(\n\t\turl=url, data=json.dumps(params),\n\t\theaders={\n\t\t\t'content-type': 'application/json'\n\t\t}\n\t)\n\tif r.status_code == 200:\n\t\tresponse = r.json()\n\n\treturn response\n\n\ndef get_nodeinfo("}]}
{"task_id": "630629d052e177c0ba46a0a1", "prompt": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t# Get the signed-signature:\n\txpath_xpath = (\n\t\t\t\t'%s/signatures/signature/text()'\n\t\t\t\t'%s' % (schema_path, relay_xsd_path)\n\t\t\t\t)\n\t\t\tif signature == b'signature' else\n\t\t\txpath_xpath = (\n\t\t\t\t'%s/signatures/%s/signature/text()'\n\t\t\t\t'%s' % (schema_path,\n\t\t\t\trelay_xsd_path,\n\t\t\t\tsignature)\n\t\t\t\t)\n\n\t# Execute the xml:\n\txpath_xpath = xpath_xpath"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tif len(doc) < 3700:\n\t\treturn False\n\t# Try this 3 years, but don't give up if we don't get a good signature\n\tnewest = min(doc[-1][\"nsecs\"], doc[-1][\"stamp\"])\n\tif (\n\t\tnewest < 21000 \n\t):\n\t\treturn False\n\telse:\n\t\tfound = False\n\t\tfor x in range(1, len(doc)):\n\t\t\tsignature = doc[x][\"sig\"]\n\t\t\tif (\n\t\t\t\tdoc[x][\"nsecs\"] < 10800"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\txml = ET.fromstring(doc.encode('utf-8'))\n\tsignature_doc = xml.find('Signature')\n\tassert len(signature_doc) > 0, \"Error in the generated certificate\"\n\tverify_result = verify_signature(public_key, signature_doc)\n\tif verify_result.result == 0:\n\t\treturn True\n\telse:\n\t\treturn False\n\n\ndef generate_keypair(name, public_key, issuer, private_key, expiry):\n\tpubkey_key = EC.pubkey_to_eckey(private_"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\txml = '<xml xmlns=\"%s\"' % xml_prefix\n\txml += '%s \"Signature\"=\"%s' % (xml_prefix, signature)\n\txml += '%s'% xml_prefix\n\n\tpublic_key_base64 = base64.encodestring(private_key)\n\tpublic_key_base64 = 'The quick brown fox jumps over the lazy dog'\n\tpublic_key_str = '%s\\n\\n'.join(\n\t\tpublic_key_base64.split(', ')[:9])\n\tpublic_key_str = public"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tverified = None\n\tfor pubidstr in [public_key.get('pkid')]:\n\t\t# This is not really a proof because it has a public key, not\n\t\t# a private key. That isn't done in the signature verification.\n\t\t# Therefore we add the signature to verify before continuing.\n\t\tverified = signer.verify(pubidstr, signature, doc)\n\t\tif verified is not None:\n\t\t\tbreak\n\tassert verified is not None  # lint-like\n\n<?php\n/**\n * Copyright \u00a9 Bold Brand Commerce"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tfrom cryptography.hazmat.primitives import sign\n\tdoc = sign.loads(doc)\n\n\treturn public_key.verify(doc, signature)\n\ndef _verify_signature(cert_tree, signature, key, cert_name):\n\t\"\"\"\n\tVerify a signature by claiming that the claimed author will produce\n\tsigned XML elements.\n\t\"\"\"\n\tpublic_key = cert_tree.find('x509/{}'.format(cert_name))\n\tif public_key is None:\n\t\traise Exception('Can\\'t find certificate for key {}"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tsignature = doc['signatures']\n\tfor sig in signature:\n\t\tif sig['hashAlgorithm'] == public_key.sign(sig['hash']):\n\t\t\treturn signature\n\n# ################################################################################################################################\n\ndef _test_pubkeys_author_signature(signer, public_key, signature):\n\t\"\"\"\n\t\tVerify the signed XML elements to have confidence that the claimed\n\t\tauthor and that claim can help the system author sign this message.\n\t\"\"\"\n\tdoc = signer.sign(public_key, doc=signature)\n\tauthor = public_key"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tif doc:\n\t\tassert(\"signed\" in doc )\n\telse:\n\t\tassert(\"signed\" not in doc )\n\n\tassert(\"<signed>\" in signature )\n\n\ndef verify_relayable_signature_with_signature(public_key, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tassert (\"signature\" in signature )\n\tverify_relayable_signature(public_key, signature['signature'], signature['signature'])\n\tassert (\"version\" in signature['signature"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tfor signature_element in doc.xpath(\"//Signature\"):\n\t\tif signature_element.get(\"Reference\") == public_key.to_x509_ref_string():\n\t\t\treturn True\n\treturn False\n\ndef generate_doc(public_key, signature):\n\tdoc = etree.Element(\"doc\")\n\tfor signature_element in signature:\n\t\tif signature_element.get(\"SignatureReference\") == public_key.to_x509_ref_string():\n\t\t\tif signature_element.get(\"SignatureDigestValue\", {}).get(\"Algorithm\", \"\") =="}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\ttry:\n\t    verify_cert(public_key, doc, True)\n\texcept OpenSSL.crypto.Error:\n\t    log.critical('Error: Signature verification failed '\n\t\t'digital output could not be decrypted')\n\t    log.debug('XML element: %s', xml)\n\t    log.debug('XML Signature: %s', signature)\n\telse:\n\t    sig_ok = False\n\t    sig = signature\n\treturn sig_ok\n#############################################################################\n#\n#       Verifying signatures:\n"}]}
{"task_id": "630629e052e177c0ba46a0c4", "prompt": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tdiaspora_webfinger = {}\n\tif \"d\" in document:\n\t\ttokens_d = document.split()\n\t\tfor token in tokens_d[::-1]:\n\t\t\ttry:\n\t\t\t\tdiaspora_id, command = token.split(\"_\")\n\t\t\t\tdiaspora_webfinger[diaspora_id] = command\n\t\t\texcept ValueError:\n\t\t\t\tcontinue\n\tif \"f\" in document:\n\t\ttokens_f = document.split()\n\t\tfor token in tokens_f[::-1"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tjson_doc = json.loads(document)\n\tdiaspora = json.loads(json_doc[\"diaspora\"])\n\t\n\tdiaspora[\"webfinger_url\"] = (\n\t\tdiaspora[\"url\"] + \"?\" + diaspora[\"webfinger\"]\n\t)\n\n\treturn diaspora\n\n\n@logger\ndef parse_diaspora_webfinger_list(document: str) -> List:\n\t\"\"\"\n\tParse list of Diaspora webfinger which is XRD (old) or HTTP ("}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\twith open('/tmp/diaspora_federation/webfinger.json') as f:\n\t\tdata = json.load(f)\n\t\tresults = parse_json_results(data)\n\t\tif results is None:\n\t\t\treturn {}\n\t\twebfinger = {}\n\t\tfor result in results.values():\n\t\t\twebfinger[result['uid']] = Webfinger(result['webfinger'])\n\t\treturn webfinger\n\nusing System;\nusing System.Collections.Generic;\nusing System.IO;\nusing System.Linq;\nusing System"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tresult = {}\n\tif 'event' in document:\n\t\tfor element in document.get(\"event\",[])[\"entry\"]:\n\t\t\t\tif element[\"event\"] == 'diaspora_federation_v01_webfinger_create':\n\t\t\t\t\tresult['event'] = \"webfinger_create\"\n\t\t\t\t\tbreak\n\n\tif 'event' in document or 'error' in document:\n\t\treturn result\n\n\tif \"error\" in document.get('event',[]):\n\t\treturn result\n\t\n\treturn result + parse_diaspora_fed"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tapi_result = []\n\t# JSON\n\tif os.path.isdir(os.path.join(diaspora_json_folder, 'diaspora', '.json')):\n\t\tapi_result.extend(diaspora_json.read_diaspora_json(document))\n\t# XRD\n\tif os.path.isdir(os.path.join(diaspora_xrd_folder, 'diaspora', '.xrdb')):\n\t\tapi_result.extend(parse_xrdb(document"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tfor line in document.split('\\n'):\n\t\tif line.startswith('['):\n\t\t\tif ','.join(line[1:].split()) in ['new', 'old']:\n\t\t\t\twebfinger = dict()\n\t\t\t\tbreak\n\t\telif '[#]' in line:\n\t\t\twebfinger['url'] = line[:line.index(']') + 1] # remove '#'\n\t\t\twebfinger['type'] = line[line.index('#'):line.index('#'] + 1] # remove '#'\n\n\treturn webfinger\n\ndef parse_diaspor"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tdiaspora_json_str = str(document)\n\tdiaspora_diaspora_json = json.loads(diaspora_json_str)\n\t\n\tdiaspora_json_dict = {}\n\tdiaspora_diaspora_dict = {}\n\t\n\tdiaspora_json = diaspora_diaspora_json\n\tdiaspora_json.update(diaspora_diaspora_json)\n\tdiaspora_diaspora_dict = {di"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tdiaspora_webfinger = {}\n\tif not document:\n\t\treturn diaspora_webfinger\n\tif \"webfinger\" in document:\n\t\tif isinstance(document[\"webfinger\"], list):\n\t\t\tfor item in document[\"webfinger\"]:\n\t\t\t\tdiaspora_webfinger[item[\"name\"]] = Webfinger(item) if \"name\" in item else item\n\t\telse:\n\t\t\tdiaspora_webfinger[document[\"webfinger\"]] = Webfinger(document[\n\t\t\t\t\"webfinger\"]) if \"web"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tdiaspora_federation = {\n\t\t\"type\": \"Webfinger\"\n\t}\n\n\tif \"data\" in document:\n\t\tdocument_body = document[\"data\"]\n\n\t\tcontent = extract(\n\t\t\tdocument_body,\n\t\t\t\"source.file.content_link\",\n\t\t\t\"source.file.webfinger_id\"\n\t\t)\n\n\t\twebfinger_id = content[\"source.file.webfinger_id\"].get(\"value\", None)\n\t\ttype = content[\"type\"].get(\"value\", None)"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\telements = document.split(',')\n\n\tdiasPoraWebfinger = {\n\t\t'@context': 'https://diaspora.github.io/diaspora_federation/discovery/webfinger',\n\t\t'time_updated': None,\n\t\t'info': {'@type': 'WebfingerInfo'},\n\t\t'status': 0,\n\t\t'discovered': False,\n\t\t'items': []\n\t}\n\n\tfor element in elements:\n\t\ttry:\n\t\t\titem = DiasporaFederatedId"}]}
{"task_id": "630629e152e177c0ba46a0d1", "prompt": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\treturn webfinger.get(handle)\n\npackage v1\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/operator-framework/operator-sdk/pkg/k8sutil\"\n\n\tapiErrors \"github.com/operator-framework/operator-sdk/pkg/api/errors\"\n\t\"github.com/operator-framework/operator-sdk/pkg/k8sclient\"\n)\n\n// +kubebuilder:rbac:groups=apps,resources=ingresses,verbs=get;list;watch;create;patch;"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\treturn w3af.core.data.get('https://raw.github.com/{target}/{id}'.format(target=handle, id=handle))\n\n\n@dataclasses.dataclass\nclass Webfinger:\n\t@property\n\tdef id(self) -> str:\n\t\t\"\"\"\n\t\tUnique ID of this object\n\t\t\"\"\"\n\t\treturn '{id}'.format(id=self.handle)\n\n\t# We ignore the target, because it's assumed this does not\n\t# have any valid URLs for them.\n\t# The only reason it wouldn"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\tclient = urllib2.urlopen(handle)\n\t\tdoc = client.read();\n\t\tclient.close()\n\n\t\t# Looking for webfinger/file mime type on the webfinger page:\n\t\ttry:\n\t\t\tmime_type_parts = doc.split(';\\\\s*content-type=\"text/\"')[1].split('\\\\')\n\t\t\twebfinger_mime_type = mime_type_parts[0]\n\t\t\treturn webfinger_mime_type\n\t\texcept ValueError:\n\t\t\t# Not in the content"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\twebfinger = WebFinger(log, handle)\n\treturn webfinger.get_webfinger_document()\n\n\"\"\"An event handler for the document being opened in a webfinger\"\"\"\ndef on_webfinger_handle_open_in_webfinger(self):\n\tlog.debug(\"webfinger opened in a webfinger document\")\n\tfrom PIL import Image, ImageFilter\n\tpass\n\"\"\"An event handler for the document being closed in a webfinger\"\"\"\ndef on_webfinger_handle_open_in_webfinger_close(self):\n\tfrom PIL import Image,"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\t# In case of error, raise an exception\n\t\tfrappe.db.sql(\"\"\"UPDATE w_settings\n\t\t\tSET fk_content = 'webfinger',\n\t\t\t\tfk_subject = 'WebFinger'\"\"\")\n\t\tw = frappe.get_doc('WebFinger', {'subject': 'WebFinger'})\n\t\tfrappe.db.sql(\"\"\"delete from `tab{type}` where `to` = 'WebFinger'  and `tab{id}` = 'webfinger' and `type` not in ('User Group"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\twebfinger_client = webfinger_client(handle)\n\tif websfinger_client.webfinger_document_id is None:\n\t\tpass\n\telif websfinger_client.retrieve_webfinger() is True:\n\t\treturn websfinger_client.webfinger_doc_path\n\telse:\n\t\tpass\n\treturn None\n\n<?php\n\nfinal class PhabricatorCalendarEventEngine\n  extends PhabricatorFulltextEngine {\n\n  private $repository;\n  private $objectPHID;\n  private $property;\n  private $isProperty"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\tif str(handle) == \"https\" and WEBFNFINGER_URL:\n\t\t\treturn handle  # RFC7033 document URLs are case-insensitive\n\t\twith http.client as http:\n\t\t\t(content, code) = (await http.send_head(handle))\n\t\t\tparsed = urlparse(content[1])\n\t\t\treturn parsed.geturl() if parsed and content else None\n\texcept HTTPError as e:\n\t\tif e.code == 402 and WEBFNFINGER_URL:\n\t\t\t\treturn handle # handle points to"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\timport json\n\tdata, status = pywebfinger.post(\n\t\t\"text/json\", None, \"file\", handle,\n\t\theaders={\"Content-Type\": \"application/json\"},\n\t )\n\tif status!= 200:  # We have a problem\n\t\treturn None\n\treturn json.loads(data)[\"body\"]\n\n\ndef extract_webfinger_pdf(path: str) -> Optional[str]:\n\t\"\"\"\n\tExtracts the document PDF file in a temp directory.\n\t\"\"\"\n\timport shutil\n\timport tempfile\n\timport zipfile\n\t"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\toutput = demisto.executeCommand('webfinger.document', {'webfinger_document': handle})\n\tsane_output = output.get('sane', None)\n\treturn sane_output\n\ndef try_retrieve_pofilelink(handle: str) -> Optional[str]:\n\t\"\"\"\n\tReturs the pofilelink field of the webfinger document. Does not raise if it fails.\n\t\"\"\"\n\toutput = demisto.executeCommand('webfinger.pofilelink', {'webfinger_document': handle})\n\tif 'pofilelink' in output"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\treturn parse_rfc7033(try_retrieve_webfinger_document_raw(handle))\n\ndef try_retrieve_webfinger_document_raw(handle: str) -> Optional[str]:\n\t\"\"\"\n\tReturn the content of a webfinger document as a raw string.\n\t\"\"\"\n\tcontent = requests.put(\n\t\turls.webfinger.WEB_FORM_FETCH_URL + \"file\",\n\t)\n\tif content is None or not content.ok:\n\t\tmsg = (\n\t\t\t\"Can't get content of file from URL '%s'\" %"}]}
{"task_id": "630629e152e177c0ba46a0d2", "prompt": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tdiaspora_dict = None\n\ttry:\n\t\tdiaspora_handle = remote_diaspora_api_v3.search_diaspora_webfinger(handle)\n\t\tif diaspora_handle['status'] == None:\n\t\t\tprint error_message(\"Failed to get Diaspora WebFinger document.\")\n\t\t\tdiaspora_dict = {\n\t\t\t\t'rendered_content': '',\n\t\t\t\t'rendered_title' : '',\n\t\t\t\t'rendered_date' : '',\n"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tdoc = handle.retrieve(diaspora_url + 'diasporaWebfinger.html')\n\treturn doc\n\nnamespace Microsoft.Azure.PowerShell.Cmdlets.Migrate.Models.Api20210210\n{\n    using Microsoft.Azure.PowerShell.Cmdlets.Migrate.Runtime.PowerShell;\n\n    /// <summary>\n    /// A PowerShell PSTypeConverter to support converting to an instance of <see cref=\"InMageRcmProtectedDiskDetailsProperties\" />\n    /// </summary>\n    public partial class InMageRcmProtectedDisk"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\treturn handle.retrieve_value('diaspora-webfinger-content')\n\ndef retrieve_and_parse_diaspora_url(handle, query):\n\t\n\t\"\"\"\n\tRetrieve a url and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:arg query: query that defines the output format to retrieve the\n\t:returns: dict\n\t\"\"\"\n\t\n\t# Create an object to hold and store our retrieved Diaspora webfinger\n\tdiaspora_webfinger = retrieve_and_"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tprint ('''Retrieve a remote Diaspora webfinger document.\n------------------------\nRemote: {h}\n''' + \\\n\t\tdisplayable_handle (handle) + '''\n---------------------------------------------------------------------------------------\n-----------------------------\nTo extract:\n{r}\"\"'.format)\t#,handle)\n\t\n\tif not remote_device (handle):\n\t\traise ValueError ('Invalid handle: {h}')\t#,handle)\n\t\n\ttry:\n\t\twebfinger_dict = cwebfinger (device=handle)\n\texcept:\n\t\traise ValueError ('Invalid handle')\n\t\n"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tdoc = urllib2.urlopen(handle)\n\twith doc:\n\t\turl = doc.read().decode('utf-8')\n\ttry:\n\t\tdiasporaapi = DiasporaDiaskaAPI({})\n\t\tdiaspora = diasporaapi._request('diaspora', 'get', '/diaspora?type='+url, headers={})\n\texcept:\n\t\traise handle.error()\n\tdata = diaspora\n\treturn data\n\ndef retrieve_and_parse_diaspora_"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tr = sstruct.pack2(REQUEST_DICT_REQUEST_MAGIC, handle, 8)\n\tm = sstruct.unpack1(REQUEST_DICT_RESPONSE_MAGIC, r)\n\treturn m\n\n\nget_diaspora_webfinger = retrieve_and_parse_diaspora_webfinger\n\n\ndef retrieve_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tr = s"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tprint('Retrieving Diaspora...')\n\t\t\t\t\n\t#Retrieve a Diaspora URL\n\ttry:\n\t\tresponse = urllib.urlopen(os.path.abspath(diaspora.app.config['DIASPORA_URL']))\n\texcept:\n\t\tprint('DIASPORA_URL is not available.')\n\t\treturn False\n\tprint('Retrieved Diaspora URL.')\n\tdiaspora = json.loads( response.read() )\n\n\t#Parse the response\n\ttry:\n\t\tpayload = '"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\ttry:\n\t\tresponse = requests.get(\"https://webfinger.io/api/v2/diaspora/webfinger.io/{}\".format(handle))\n\t\tdiaspora_webfinger = response.json() \n\t\treturn diaspora_webfinger\n\texcept(requests.exceptions.ConnectionError,requests.exceptions.ReadTimeoutException):\n\t\treturn None\n\n\n# Get diaspora info from local. Only use the Diaspora webfinger if the handle exists.\ndef get_diaspora_info(handle"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tr = requests.get('https://diaspora.com/api/v1/diaspora/webfinger', headers = Headers(user_agent=\"Diaspora\"))\n\treturn r.json()\n\n# def send_to_diaspora_contacts(handle=''):\n# \t\"\"\"\n# \t:arg handle: The Diaspora Contacts handle.\n#  :returns: None\n#  \"\"\"\n# \tr = send_to_diaspora()\n# \tr.json()\n# \tprint(\"\\"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\treturn Diarefier.Diarefier.retrieve(handle).get_xml()\n\npackage com.huawei.nb.model.collectencrypt;\n\nimport android.database.Cursor;\nimport com.huawei.odmf.database.Statement;\nimport com.huawei.odmf.model.AEntityHelper;\nimport java.util.Date;\n\npublic class MetaCollectencryptWeatherInfoHelper extends AEntityHelper<MetaCollectencryptWeatherInfo> {\n    private static final MetaCollectencryptWeatherInfoHelper INSTANCE = new MetaCollectencryptWeatherInfoHelper();\n\n"}]}
{"task_id": "630629e252e177c0ba46a0d6", "prompt": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn core.Doc(host.host.Meta())\n\ndef retrieve_diaspora_host_version(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn core.Ver(host.version)\n\ndef retrieve_diaspora_host_version_json(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document,\n\tgenerating a json formatted version string.\n\t\n\t:"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn doc.retrieve_diaspora_host_meta(host)\n\t\n\t\n\t\ndef retrieve_service(url, timeout):\n\t\"\"\"\n\tRetrieve information about a remote service.\n\t\n\t:arg url: URL to retrieve information from\n\t:arg timeout: Maximum time, in seconds, to wait for document being available\n\t:returns: ``DIASPORA:Service`` instance\n\t\"\"\"\n\treturn doc.fetch_service(url, None, timeout)\n\t\n\t\n\t\ndef retrieve_service_meta(host, url):\n\t\"\"\"\n\tRetrieve a remote"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\tif not host or host.is_remote():\n\t\treturn None\n\tif diaspora_config[host.config_name] is None:\n\t\trequest = XRD(uri=\"%s/meta\" % host.config_name, request=True)\n\t\tdiaspora_config[host.config_name] = request.doc\n\treturn diaspora_config[host.config_name]\n\nusing System;\nusing System.Collections.Concurrent;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Threading;"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn diaspora_host_meta.getDiasporaHostMeta(host)\n\ndef retrieve_diaspora_host_metadata(host, key):\n\t\"\"\"\n\tRetrieve a Diaspora server's metadata from\n\t\t\n\t:arg host: Host to retrieve the data from\n\t:arg key: Key to retrieve\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn diaspora_host_metadata.getDiasporaHostMetadata(host, key)\n\ndef retrieve_diaspora_host_metadata_key(host,"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\tf = open(host)\n\tif f is None or f.read():\n\t\tf = open(host)\n\treturn f.read()\n\n/*\n * Copyright (C) Sistina Software, Inc.  1997-2003 All rights reserved.\n * Copyright (C) 2004-2007 Red Hat, Inc.  All rights reserved.\n *\n * This copyrighted material is made available to anyone wishing to use,\n * modify, copy, or redistribute it subject to the terms and conditions\n * of the GNU General Public License version 2.\n */\n\n/*\n * Qu"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t# Check whether the host is reachable via IPv6\n\t_h = socket.inet_ntoa(host.get(\"ip\"))\n\tip_type = socket.inet_aton(_h)\n\t# Check whether the host is reachable via IPv4\n\ttry:\tip_type = socket.inet_aton(ip_type)\n\texcept:\treturn None\n\ttry:\n\t\tudp = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n\t\treturn udp.connect_ex((ip_type, 0))\n\texcept OSError"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn get_remote_diaspora_host(host).retrieve_hostmeta_document()\n\ndef get_server(servername):\n\t\"\"\"\n\tRetrieve a remote Diaspora server or all servers from the Diaspora\n\tlibrary.\n\t\n\t:type servername: ``Server Name``\n\t:arg servername: Name of a Diaspora server.\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn _host.get_server(hostname=servername)\n\ndef retrieve_servers_for_host(h):\n\t"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn retrieve_diaspora_host_meta_doc(d.url(host=host))\n\n\ndef retrieve_host_meta():\n\t\"\"\"\n\tRetrieve a remote local host-meta Diaspora document as ``XRD``.\n\t\t\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\td = XRD(d.url_to_xrd(xrd_type=\"host-meta\"), json=True, headers={\"x-opaque\": \"1\"})\n\treturn d\n\n\ndef retrieve_ip_address_host():\n\t\"\"\"\n\tRetrieve"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn await call(doc=host, doc='meta', service=XRD)\n\n#include <windows.h>\n#include \"defines.h\"\n\n/***  Function Name  : SetFileAttribU\n*\n*  Function Description:\n*\n*  SetFileAttribU  Sets file creation attribute for the given user specified file in the specified directory.\n*\n*  Parameters:\n*\n*               FileName -\n*                   "}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn Xrd(host)\n\"\"\"\nEnd interface\n\"\"\"\n\n/*\n * Copyright 2014 The Error Prone Authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES"}]}
{"task_id": "630629e752e177c0ba46a0fb", "prompt": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tdata = url if url else {}\n\theaders = {}\n\theaders.update(kwargs)\n\tr = requests.post(url, data=data, headers=headers, timeout=timeout, method=method, *args, **kwargs)\n\tif r.status_code == 200:\n\t\treturn r.json()\n\telse:\n\t\traise requests.exceptions.ResponseError(r.status_code, r.text)\n\n\nclass HTTPRequestError (requests.RequestException):\n\tmsg = \"Request error\"\n\n\nclass HTTPError (RequestError):\n\t__slots"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\turl = parse.urlparse(url)\n\t\n\t# Check to see if we can pull a body from the URL\n\tresponse = None\n\theaders = _check_response(url, method=('GET', 'HEAD'))\n\tif response is None:\n\t\tbody = data\n\telse:\n\t\tbody = data if hasattr(data, \"read\") else data.read()\n\t\n\t# Pull headers\n\tif isinstance(headers, dict):\n\t\theaders = headers.items()\n\t\n\thttp_timeout = timeout\n\t\n\t# If data is a file, send the"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tif isinstance(url, bytes):\n\t\turl = url.decode(\"utf8\")\n\tif not isinstance(data, (list, tuple)):\n\t\tdata = [data]\n\tif not isinstance(timeout, int): timeout = 10\n\tresponse = requests.post(\n\t\turl,\n\t\tmethod=method,\n\t\theaders={\n\t\t\t\"Content-Type\": \"text/html; charset=utf-8\",\n\t\t\t\"Authorization\": \"Bearer {}\".format(\n\t\t\t\trequests.utils.get_basic_auth_header(\n\t\t\t\t\tbase_"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tkwargs[\"timeout\"] = timeout\n\tkwargs.setdefault(\"headers\", {})\n\tif method == \"get\":\n\t\tkwargs[\"headers\"][\"Accept\"] = \"application/x-www-form-urlencoded\"\n\telif method == \"put\":\n\t\tkwargs.setdefault(\"headers\", {})[\"Content-Type\"] = \"application/x-www-form-urlencoded\"\n\t\t\n\t# This function does the work of sending the body\n\t# it then decodes it to a dict (dicts are decoded) and returns it\n\t# note that this will get bytes or just return the dict\n\t#"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tmethod = method.lower()\n\tkwargs.setdefault(\"verify\", True)\n\turllib3.util.validate_bearer(token=kwargs.pop('token', None))\n\trequests.packages.urllib3.disable_warnings()\n\turllib3.disable_warnings(InsecureRequestWarning)\n\n\trequest, urlpath = urlparse(url)\n\turlpath = path.strip_netloc(urlpath)\n\n\tresponse = requests.request(method, urlpath, data=data, headers=headers, cookies=cookies, redirect=True, ca=cacert_file,"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tfrom PIL import Image, ImageDraw, ImageFilter, ImageOps\n\timport requests\n\t\n\tresult = None\n\t\n\ttry:\n\t\turllib.request.url = url\n\t\turllib.request.session = requests.Session()\n\t\turllib.request.get_method = getattr(requests, method.upper())\n\t\tresponse = urllib.request.urlopen(method=method.upper(), data=data)\n\texcept Exception as e:\n\t\tresult = (False, e)\n\telse:\n\t\tif timeout!= 10:\n\t\t\traise RuntimeError(\""}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\theaders = {'Content-Type': 'text/plain; charset=utf-8'}\n\tif 'X-Body' in data:\n\t\treturn send_document_content(url, data, timeout, **kwargs)\n\trequest = requests.prepare(method, url, data)\n\tif timeout is None:\n\t\ttimeout = 10\n\tif len(data):\n\t\trequest.get(timeout=timeout, headers=headers)\n\telse:\n\t\trequest.post(method, url, data)\n\t\tresponse = response_from_requests(request)\n"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\theaders = {\n\t\t'User-Agent': settings.USER_AGENT\n\t}\n\t# Send with specific protocol\n\tresponse = requests.post(\n\t\turl,\n\t\tdata=data,\n\t\theaders=headers,\n\t\tparams=kwargs,\n\t\ttimeout = timeout,\n\t\tverify=settings.SSL_VERIFY_CERTIFICATE or (\n\t\t\tsettings.VERIFY_SERVER and settings.SSL_VERIFY_HOSTNAME or (\n\t\t\t\tsettings.VERIFY_CLIENT and settings.SSL_VERIFY_HOSTNAME))\n\t)\n\n\t# Read response and parse error"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t_params = {'url': url, 'timeout': timeout, 'data': data,'method':method}\n\tif args: # we expect POST data to be a tuple\n\t\t_params[\"data\"] = data\n\treturn _send(_request(*args, **_params))\n\ndef unescapeHTML(response):\n\treturn re.sub('<[^>]*>\\s*(<[^>]*>)', '\\1\\3', response)\n\nclass ResponseError(Exception): pass\n\ndef default_request(method=\"GET\", **params):\n\t\"\"\"\n\tHelper method to call a given"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t# We allow sending data via POST since we know all the things we need to send.\n\t# This is useful for e.g. file uploads for posting to a file. We also use the headers\n\t# to create the post data.\n\treturn requests.post(url, data=data, timeout=timeout, headers=headers, files=files, *args, **kwargs)\n\n\ndef send_post_and_get_result(url, request, *args, **kwargs):\n\t\"\"\"\n\tWrapper around the requests.post() methods that allows for async requests.\n"}]}
{"task_id": "6306091073426c38ae68acac", "prompt": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t# check if the current key matches\n\tif key not in keys:\n\t\t# check if the value is a dict\n\t\tif isinstance(val, dict):\n\t\t\t# check the size of the value\n\t\t\tif len(val)!= len(keys):\n\t\t\t\t# raise an error if an inner dict's size!= the size of any\n\t\t\t\t# key in the outer dictionary\n\t\t\t\traise TypeError('inserting non-dict value')\n\t\t\tnew_dic = dict()  # new key value to insert\n\t\t\tfor key in keys:\n\t\t\t\tif not"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tkey = str(key)\n\tfor key in keys:\n\t\tdic[key] = val\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\treturn dic[key]\n\ndef dict_merge(dict_in, dict_out, *keys):\n\t\"\"\"\n\tmerge two dictionaries and return a new merged dictionary\n\t\n\tto merge two dictionaries, both dictionaries should have the same number of\n\tkeys\n\t\n\tto merge, return a copy of the target\n\t\n\texample:\n\tmerged = dict_merge({"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif not keys:\n\t\treturn\n\tvalue = dic.get(key, val)\n\tif value:\n\t\traise exception.InvalidValue(val, key)\n\tdic[key] = val\n\n@utils.positional(1, 'a,b')\ndef dict_delete(dic, key, *keys):\n\t\"\"\"\n\tremove a value of a nested key from the dic dictionary\n\t\n\tto remove value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_delete({}, 'val"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\ttry:\n\t\tkey = next(keys)\n\texcept StopIteration:\n\t\treturn\n\tkeys.append(key)\n\tdic[key] = val\n\tif hasattr(val, 'get'):\n\t\tval['key'] = None\n\tfor k in keys[1:]:\n\t\tif hasattr(dic[k], 'get'):\n\t\t\tdic[k]['key'] = dic[k].get(key)\n\t\telse:\n\t\t\tdic[k]['key'] = dic[k][key]\n\treturn dic\n\n\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif not dic: return\n\tpos = dic.get(key)\n\t#print \"key %s\" % key, \"value %s\" % val\n\n\tif not pos: return\n\n\tpos[len(keys)] = (val,) + keys\n\n\ndef dict_replace(dic, old_value, new_value, remove_key=None):\n\t\"\"\"\n\treplace a value of a nested key into a dictionary\n\t\n\tto replace value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t "}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif (len(dic) > 1): dic.setdefault('dict', {})[key] = []\n\t\n\tif (isinstance(key, basestring)):\n\t\tkey = [key]\n\telse:\n\t\tkey = list(key)\n\t\n\tif (isinstance(key, basestring)):\n\t\tfor i in range(0, len(key)):\n\t\t\tif (key[i] in keys):\n\t\t\t\tif (isinstance(keys[i], basestring)):\n\t\t\t\t\tkey[i] = [keys[i]].append(val.format(**keys["}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tres_key = []\n\tres_val = []\n\t\n\tdef _str_find_idx(curr, start):\n\t\tfor child_idx, child in enumerate(curr):\n\t\t\tcurr2 = curr[child_idx]\n\t\t\t\n\t\t\tif child == start:\n\t\t\t\tprint(\"{} => {}\".format(curr2, [key for key, value in\n\t\t\t\t\tdic[key][1].items() if key not in\n\t\t\t\t\tres_key]), file = output_file)\n\t\t\t\tprint(\"\\n\", file = output_file)\n\t\t\t\tbreak\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif not keys:\n\t\treturn\n\tkeylist = []\n\twhile len(keys):\n\t\tkeylist.append(keys[0])\n\t\tkeys = keys[1:]\n\tif len(keylist) < 1:\n\t\traise ValueError(\"key list is empty\")\n\tkeylist = list(map(lambda x: x[0], keylist))\n\t\n\tfor key in keylist:\n\t\t_set_value(dic, key, val)\n\n\n\ndef dict_has_key(dic, key):\n\t\"\"\"\n\tcheck if"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif not isinstance(keys, Sequence):\n\t\tkeys = [keys]\n\t\n\tassert isinstance(dic, dict)\n\tfor subkey in keys:\n\t\tassert subkey.startswith('.')\n\t\tkey = subkey[1:]\n\t\tval = dic.get(key, {})\n\t\tif key == key + '.':\n\t\t\tif type(val) == list and len(val) > 0:\n\t\t\t\tval.extend(val)\n\t\t\telse:\n\t\t\t\tdic[key + '.'] = val\n\t\telse:\n\t\t\t"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif not isinstance(key, tuple):\n\t\tkey = (key,)\n\t\t\n\tfor i, k in enumerate(keys):\n\t\tif isinstance(k, str):\n\t\t\tk = (k,)\n\t\tif isinstance(val, str):\n\t\t\tval = (val,)\n\t\tdic.setdefault(k, {})\n\t\tif isinstance(key, tuple):\n\t\t\tdic[k][key[0].split('.') + key[1:]].update(val)\n\t\t\treturn\n\t\tif '__parent__' in k:\n\t\t\t"}]}
{"task_id": "6306091a73426c38ae68acc8", "prompt": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t# add the options at the bottom...\n\toptions = [\n\t\t'#', '#', '#', '#',\n\t\t'title', '=title', '',\n\t\t'help',\n\t]\n\toptions_string += \"\\n\".join(options)\n\t# create this a list_of_files or complex_file\n\t# get the real files\n\tfiles_list = get_files(settings_dirs, spec_option, ignore_extra_args=['--list-dir'])\n\n\tfor file_name in files_list:\n\t\t# skip directories (and"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tfor basedir, dirs, file_names in os.walk(settings_dirs):\n\t\tname = file_names[-1]\n\t\ttry:\n\t\t\tbase_path = os.path.dirname(os.path.realpath(basedir)) \n\t\t\tname_path = os.path.join(base_path, name)\n\t\t\tfile_path = base_path + \"/\" + name\n\t\t\tspec_setting = {'name': os.path.realpath(name_path)}\n\t\texcept BaseException as ex:\n\t\t\tLOGGER.error(\"Error while getting"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tfor file_name in sorted(settings_dirs):    \n\t\tif spec_option == \"spec\":\n\t\t\tprint(\"creating configuration file [%s]\" % file_name)\n\t\t\twith open(file_name, 'w') as config_file:\n\t\t\t\tcreate_config_file(file_name, config_file)\n\t\telse:\n\t\t\tprint(\"deleting configuration file [%s]\" % file_name)\n\t\t\twith open(file_name, 'r') as config_file:\n\t\t\t\tdel_config_file(file_name"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t# in case of an empty option we still return a reference to itself\n\tif not spec_option:\n\t\treturn IniType(parent=this, **settings_dirs)\n\t\n\t# add our extra options\n\toptions = this.get_common_options(spec_option)\n\textra_options_names = []\n\tfor option in settings_dirs:\n\t\textra_options = option.additional_options.split()\n\t\tif len (extra_options) > 0:\n\t\t\textra_options_names.append(extra_options[0])\n\t"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tiniParam = IniType()\n\tfor dirpath, dirnames, filenames in settings_dirs:\n\t\tfor filename in filenames:\n\t\t\tout = iniParam.readSetting(dirpath, filename)\n\t\t\tif out['type']!= spec_option:\n\t\t\t\tiniParam.setSetting({'type':spec_option, 'value':out['value']})\n\treturn iniParam\n\n#ifndef IVEC2VEC\n#define IVEC2VEC\n\n/*******************************************************\n * @Description : Vector2\n * @author      : \u6881\u5c71"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tfile_type_list = []\n\tfor f in settings_dirs:\n\t\tdir_path = os.path.join(f, spec_option)\n\t\t# Ini files are stored under a subdirectory __file\n\t\tdir_path_test = os.path.join(dir_path, '__file')\n\t\tif os.path.exists(dir_path):\n\t\t\tfile_type_test = os.path.splitext(dir_path)[1].replace('\\\\','__')[:-3]\n\t\t\tfile_type_list.append(file"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tsettings_dict = {\n\t\t'indentation': 2,\n\t\t'line_length': 75\n\t}\n\tdefault_settings_lines = {\n\t\t'indentation': 2,\n\t\t'line_length': 75\n\t}\n\tnew_settings = list()\n\twith override_settings(default_settings_lines):\n\t\twith override_settings(settings_dict):\n\t\t\twith open(spec_option, 'r') as spec_file:\n\t\t\t\tfor line in spec_file.read().split('\\n'):\n\t\t\t\t\tif line.startswith"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t# Get the name for each file which will be in the list\n\t# Add the extension for the file, because that's how the names in the directory are named in the file.\n\t# To get only the base names without the extension, we can use the \".\" as the filename part.\n\tfile_list_names = []\n\tfile_list_names = [f for f in os.listdir(settings_dirs[0]) if f.endswith('.ini')]\n\t# If the file doesn't have a name, we are in the parent folder, so don't put the \".\" at the"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\treturn [os.path.basename(x) for x in os.listdir(settings_dirs['common']) if 'IniType' in x]\n\n# -----------------------------------------------------------------------------\nclass IniType(object):\n\t\"\"\"\n\t\tThis class stores the different types of ini files, such as\n\t\t'common' directory, file of each type, or directory of each type\n\t\tin which we read our ini file.\n\t\"\"\"\n\tcommon = {}\n\n\tdef __init__(self, settings_dirs, spec_option):\n\t\t\"\"\"\n\t\t\tInstantiate the settings dirs"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tfor config in settings_dirs:\n\t\tmake_list_of_file_names(config, spec_option)\n\ndef make_list_of_file_names(config_path, spec_option):\n\tfor config in config_path:\n\t\tif os.path.isfile(os.path.join(config_path, config)):\n\t\t\twith open(os.path.join(config_path, config)) as config_file:\n\t\t\t\tnew_config = []\n\t\t\t\tfor idx, line in enumerate(config_file):\n\t\t\t\t\tif"}]}
{"task_id": "6306091b73426c38ae68acd7", "prompt": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn importlib.import_module('ansible_collections.config.manager')\n\ndef ansiblecli(args):\n\t\"\"\"\n\t\trun ansible cli\n\t\"\"\"\n\tusage = \"Usage: %prog [options] <command>...\"\n\tparser = argparse.ArgumentParser(prog=sys.argv[0], formatter_class=argparse.ArgumentDefaultsHelpFormatter, add_help=False)\n\tparser.add_argument('-v', '--version', action=\"version\", version=__version__)\n\tparser.add_argument(\"-p\",  '--config', action"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn __import__('ansible.core.config', fromlist=['core.config'])\n\n@util.memoized\ndef ansible_log():\n\t\"\"\"\n\tGets the ansible log.\n\t\"\"\"\n\traise ImportError(__name__)\n\n# vim:ts=4:sw=4:et\n\n/*\n * Copyright (c) 2019, 2020, Oracle and/or its affiliates.\n *\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without modification, are\n * permitted provided that the following conditions are met:\n *\n"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t__virtualname__ = cls.__name__\n\tconfig_files_list = [\"puppetlabs_ansible_config.cfg\"]\n\n\tdef __init__(self, command=\"run\", name=None, config=None, **kwargs):\n\t\t\"\"\"\n\t\tThis is a base command for all ansible commands.\n\t\t\"\"\"\n\t\tcls.__parent__.type_mapping = None\n\t\tsuper().__init__(command=command, name=name, **kwargs)\n\t\tif not config:\n\t\t\tconfig = self.config_manager\n\n\t\tself."}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tglobal get_ansible_config as ansible_config\n\treturn get_ansible_config()\n\n\n# ansible-runner.utils.default_env\ndef default_env():\n\t\"\"\"\n\tGets the default environment variable.\n\t\"\"\"\n\tglobal global_config.env\n\treturn global_config.env\n\n\n# ansible-runner.utils.process\ndef process(\n\tcommand_line,\n\tcommand_line_options,\n\tsubprocess_env,\n\tdefault_env=default_env,\n\tenv_var=\"\",\n\tallow_"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tansible_config_manager = ansible_connection(connection_param=' ansible_config_url', verbose=True)\n\tansible_config_manager.load()\n\treturn ansible_config_manager\n\ndef get_ansible_config_manager_version(cls):\n\"\"\"\n\tGets the ansible version.\n\t\"\"\"\n\tcls.__ansible_config_manager_version_param = None\n\tansible_config_manager = get_ansible_config_manager(cls)\n\tansible_config_manager_version = ansible_config_"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tif hasattr(cls, 'config_class'):\n\t\tcls.__config_manager__ = cls.config_class.Config()\n\t\tcls.__config_manager__.load(cls.config_class)\n\t\treturn cls\n\telse:\n\t\treturn False\n\ndef ansible_config_manager_get_config(cls, section):\n\t\"\"\"\n\tReturns the current ansible_config_manager config as a dictionary. If\n\tget_config is True, then the dict contains the values set by ansible_config_manager.\n\t\"\"\"\n\tconfig = getattr"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tif cls._ansible_config_manager is None:\n\t\ttry:\n\t\t\tfrom ansible.module_utils.ansible_config_manager import AnsibleConfigManager\n\t\t\tcls._ansible_config_manager = AnsibleConfigManager()\n\t\texcept:\n\t\t\traise ImportError(\"\"\"\n\t\t\t\tERROR: ansible.config.manager is not available. Please install \n\t\t\t\tit or re-install ansible.\n\t\t\t\"\"\")\n\t\tif 'ansible_command_prefix' in cls._module_args[0]:\n\t\t\tcls"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn cls._get_manager()\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t# Get the ansible config manager\n\tcls.cls_ansible_conf_manager = get_ansible_config_manager()\n\treturn cls.cls_ansible_conf_manager\n\n\ndef get_ansible_config_manager():\n\t# We use 'boto' module instead of \"boto3\"\n\tboto = import_dependency(settings.CONFIG[\"boto\"])\n\tboto.configure()\n\n\t\"\"\"\n\t\tFind our ansible config file\n\t\tIt needs to be in a specific directory where our config is.\n\t\"\"\"\n"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tansible_config_manager = (\n\t    ConfigManager(\n\t        get_ansible_config_manager(__name__),\n\t        __source__,\n\t    )\n\t    if __name__ in (\"tests\",)\n\t    else get_ansible_config_manager()\n\t    if isinstance(sys.modules[__name__], type)\n\t    else sys.modules[__name__]\n\t)\n\treturn ansible_config_manager\n\nnamespace MassTransit.Azure.ServiceBus."}]}
{"task_id": "6306091b73426c38ae68acd9", "prompt": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\tassert cls._manager is None, 'Can\\'t get workspace manager'\n\tcls._manager = WorkspaceManager()\n\n\tcls._workspace_manager = cls._manager.workspace()\n\n\treturn cls._manager\n\n\ndef assert_workspace_in_control(workspace_name=None, control_name=None):\n\t\"\"\"\n\tChecks whether there are multiple active workspaces or a control workspace.\n\t\"\"\"\n\tif not control_name and not workspace_name:\n\t\treturn\n\tworkspaces = Workspaces().get_all_workspaces()\n\tif workspace_"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn inspect.currentframe().f_back.f_globals.get('workspaceManager', None)\n\ndef workspace_manager_workspace_handle_manager(cls):\n\t\"\"\"\n\t\tReturns the workspace handle manager.\n\t\"\"\"\n\treturn inspect.currentframe().f_back.f_globals.get('workspaceHandleManager', None)\n\ndef workspace_manager(cls=None):\n\t\"\"\"\n\t\tGets the workspace manager.\n\t\"\"\"\n\tif cls is None:\n\t\treturn workspace_manager()\n\telse:\n\t\treturn workspace_manager(workspace"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn cls._workspace_manager\n\ndef init():\n\tglobal _workspace_manager\n\tif _workspace_manager is None:\n\t\t# Get the workspace manager\n\t\t_workspace_manager = WorkspaceManager()\n\t# Register the workspace manager\n\t_workspace_manager.RegisterWorkspace()\n\ndef dispose():\n\tglobal _workspace_manager, _memory_dbms\n\tif _workspace_manager is not None:\n\t\t_workspace_manager.DeregisterWorkspace()\n\n# A global global variable\n_memory_dbms = {}\n\n\nclass WorkspaceManager(object"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn cls._get_workspace()\n\ndef workspace_manager_2(cls):\n\t\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn lambda workspace: cls._get_workspace(workspace)\n\n#######################################\n######## WORKSPACE VARIABLES ########\n#######################################\n\n@workspace_manager\ndef create_workspace(new_workspace_folder):\n\t\"\"\"\n\tCreates a workspace with the given name.\n\n\t@param new_workspace_folder the directory to create the workspace in.\n\t\"\"\"\n\n\tname = new_workspace_folder.split(os"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn workspace_manager()\n\nclass _WorkspaceManager:\n\t\"\"\"\n\t\tWorkspaceManager constructor.\n\t\"\"\"\n\tdef __init__(self):\n\t\t\"\"\" Initialize WorkspaceManager.\n\t\t\"\"\"\n\t\tworkspace = self.workspace_manager(workspace_conf)\n\t\t# The workspace is set for the duration of the execution\n\t\tself.set_workspace(workspace)\n\t\t# For debugging purposes only\n\t\tself.start_time = time.time()\n\t\t# A dictionary of all loaded files.\n\t\tself.files = {}\n\tdef"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn workspace.manager\n\n/*\n * Copyright (c) 2015, Oracle and/or its affiliates. All rights reserved.\n * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n *\n * This code is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License version 2 only, as\n * published by the Free Software Foundation.\n *\n * This code is distributed in the hope that it will be useful, but WITHOUT\n * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn global_var.workspace_manager\n\n\n#\n# class_data\n#\n\n\nclass ClassData:\n\t\"\"\"\n\tThe ClassData object is used to hold data about a class within the class\n\thierarchy.  A new ClassData object should be created for each ClassData\n\tinstance and filled with fields. It must be associated with a class.\n\n\tThe ClassData instance is automatically deleted when the class data is\n\tdestroyed.\n\t\"\"\"\n\n\t__slots__ = [\"cls\", \"module\", \"class_\", \"data_type\",\n\t\t\t\t\t\"field_"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\tfrom.workspace import WorkSpace\n\treturn cls\n\ndef workspace_options(cls):\n\tfrom.workspace import WorkspaceOptions\n\treturn cls\n\ndef workspace_factory(cls):\n\tfrom.workspace import WorkSpace\n\treturn cls\n\ndef workspace_group(cls):\n\tfrom.workspace import WorkSpaceGroup\n\treturn cls\n\ndef workspace_group_options(cls):\n\tfrom.workspace import WorkspaceOptions, WorkspaceGroupOptions, WorkspaceGroupProperties\n\treturn cls\n\ndef workspace_resource_group(cls):\n\tfrom.workspace import WorkSpaceResourceGroup\n\treturn"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\tprint 'Loading workspace manager file.'\n\twith open( 'workspaceManager.txt', 'rb' ) as fp:\n\t\tmanager = pickle.load( fp )\n\t\tfp.seek(0)\t\t\t\n\t\treturn cls( manager )\n\ndef main ():\n\t\"\"\"\n\t\n\tBuilds the command line parser.\"\"\"\n\tparser = argparse.ArgumentParser( description='Builds the command line parser.' )\n\tparser.add_argument( '-d', '--debug', type=int, dest='debug', nargs='?', default=0, help='Set level of command line parsing debugging"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn WorkspaceManager(self.workspace_handler)\n\nclass WorkspaceManager(object):\n\t\"\"\"\n\t\tA class that takes an workspace as input and wraps it.\n\t\"\"\"\n\n\tdef __init__(self, workspace_handler):\n\t\t\"\"\"\n\t\t\tConstructor.\n\n\t\t\t@param workspace_handler: The CWS handler.\n\t\t\"\"\"\n\t\tself.workspace_handler = workspace_handler\n\t\tself._root_workspace = None\n\n\tdef _connect_to_workspace(self, handler, name):\n\t\t\"\"\"\n\t\t\tConnect the"}]}
{"task_id": "6306091b73426c38ae68acda", "prompt": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tif cls.instances[\"plugins\"]:\n\t\treturn cls.instances[\"plugins\"][\"module\"]\n\treturn {}\n\n@classmethod\nclass ConfigModule(BasePlugin):\n\t\"\"\"\n\tDefines the ConfigModule.\n\t\"\"\"\n\tNAME = \"config\"\n\n\tdef __init__(self, parent, config, *args, **kwargs):\n\t\tsuper(ConfigModule, self).__init__(parent)\n\t\tself.config = config\n\t\tself._name = self.config.getString(self.NAME)\n\t\tif not self._name:\n\t\t\tself._"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn cls.pluginmgr\n\n\n\"\"\"\n\tclass _BaseInstanceWrapper(object):\n\t\t\"\"\"\n\t\tA wrapper for an object with all the required methods, methods that must be\n\t\timplemented in the new object, except `__init__` and `new_proxy`.\n\t\t\"\"\"\n\n\t\tdef _create_object(self, object, classname=None, args=(), kwargs=None, instance=None,\n\t\t\t\t\t\t\t__name__=None):\n\t\t\t\"\"\"\n\t\tCreates an instance of the class specified.\n\t\t\t\"\"\"\n\t\t\treturn object\n"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn cls._plugin_manager\n\n\ndef get_plugins():\n\t\"\"\"\n\t\tGet the plugins list.\n\t\tThe method should be used exclusively on a plugin manager.\n\t\"\"\"\n\tplugins_names = get_available_plugins_names('plugins')\n\treturn [get_plugin_obj('plugins', plugin_name) for plugin_name in plugins_names]\n\n\ndef get_plugin_obj(plugin_name, plugin_module):\n\t\"\"\"\n\tGets a particular plugin object.\n\t\"\"\"\n\tassert plugin_name in list(plugin_dict"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn cls._PluginManager\n\n#ifndef _WINGDI_MAIN_C_\n#define _WINGDI_MAIN_C_\n\nULONG\nWINAPI\nGdiQueryDwordInfo(\n    _In_ DHPDEV dhpdev,\n    _In_ DWORD i)\n{\n\tULONG dWord;\n\n\tif (dWord = GdiGetDword(dhpdev, i, GDI_QUERY_DWORD_TYPE),\n\t    dWord!= 0x00000000) {\n\t\treturn dWord;\n\t}\n\t\n\tswitch"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn cls.engine.plugins_manager()\n\ndef main(argv=None):\n\t\"\"\"\n\tExecutes the application.\n\n\t:param argv (list): list of arguments (default: sys.argv).\n\t:param appname (str): application name\n\t:param name (str): main function name\n\t:param version (str): version attribute\n\t:param description (str): description attribute\n\t:param author (str): author attribute\n\t:param copyright (str): copyright attribute\n\t\"\"\"\n\tif not argv:\n\t\targv = sys.argv\n\t"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tcls._plugins = []\n\tplugin_classes = [x.__class__ for x in dir(cls) if callable(x)]\n\tfor cls in plugin_classes:\n\t\tcls.plugins_manager = PluginType(cls)\n\t\tif hasattr(cls, 'plugins'):\n\t\t\tcls.plugins_manager._plugins.append(cls)\n\treturn cls._plugins\n\ndef load_class_from_module(cls_name):\n\t\"\"\"\n\t\tloads a class from a module.\n\n\t\tthis function gets called from both the load_class_from_"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t_plugins = getattr(cls, 'plugins', {})\n\treturn [p for p in _plugins.values() if p.active]\n\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn inspect.getmembers(cls, inspect.isclass)\n\n\ndef has_plugins(clinic):\n\t\"\"\"\n\tGets a list of whether has plugins.\n\t\"\"\"\n\treturn (hasattr(clinic, 'plugins')\n\t    and any(hasattr(_cls, \"plugins\" for _cls in clinic.__coclass__.__bases__)))\n\n\ndef get_plugin_manager(clinic):\n\t\"\"\"\n\tGets a plugin manager.\n\n\tIf the class has plugins the return is an instance of ``PluginLoader``.\n\tIf the class has"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tif cls._plugin_manager is None and cls._plugin_manager is not None:\n\t\tcls._plugin_manager = cls._plugin_manager(name=cls.name(), description=cls.description())\n\treturn cls._plugin_manager\n\n\nclass Plugin(object):\n\t\"\"\"\n\tBase class for types of plugins.\n\n\tPlugins are named without a :class:`~kivy.uix.popup.Popup` class, a UI component, a\n\t:class:`~kivy.uix.button.Button`, etc.\n\t\"\"\"\n\n\tdef __"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn next(getattr(cls, '__plugins_manager__', False))\n\nregister_plugin_handler(plugins_manager.PLUGINS_MANAGER_KEY,\n\tplugins_manager)\n\n\n# A plugin_info object, that contains information on a currently\n# registered plugin (either a plugin, an alias, or a function,\n# depending on the plugin manager, e.g. register_plugin_handlers).\n#\n# The plugin is loaded as soon as possible, e.g. it is necessary to\n# instantiate a module for the plugin to register. Therefore, plugins\n# must be"}]}
{"task_id": "6306091c73426c38ae68acdc", "prompt": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tdata = {\n\t\t'url': '',\n\t\t'version': '',\n\t\t'name': '',\n\t\t'description': '',\n\t\t'language': '',\n\t\t'author': '',\n\t\t'mapabilities': ''\n\t}\n\t\n\terrors = []\n\tif spec_content is not None:\n\t\tdata = yaml.safe_load(spec_content)\n\t\n\tfor key in list(data.keys()):\n\t\tdata[key] = data[key][0]\n\t\t\n\tfor key in list(data.keys()):\n"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tres = _load_yaml_file(spec_content, \"spec_info\")\n\treturn validate_from_spec(cls, res)\n\n\n#==============================================================================#\n#                                                                            #\n#          "}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif spec_content is None:\n\t\treturn {}\n\telif is_yaml_file(spec_content):\n\t\treturn yaml_load_json(spec_content)\n\telse:\n\t\traise IRValidatorException(\"Invalid spec provided file\")\n\ndef validate_from_file(cls, spec_file=None, **kwargs):\n\t\"\"\"\n\tValidates and loads a spec file\n\t\n\t:param spec_file: full spec file path\n\t:param kwargs: optional spec arguments\n\t:return: Dictionary with data loaded from a spec file\n\t\"\"\"\n\t"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec = cls.load(spec_content)\n\treturn cls.validate_from_schema(spec)\n\n\nclass IRValidatorException(Exception):\n\t\"\"\"\n\tException for IR validator exceptions\"\"\"\n\tpass\n\n\nclass NotAnIRValidatorException(IRValidatorException):\n\t\"\"\"\n\tException for IR validation with no IR_Validator class specified,\n\tand IR_Validator has methods which not implemented by base class\"\"\"\n\n\tpass\n\n\nclass IrValidator(object):\n\n\t@staticmethod\n\tdef __validate_from_schema(spec):\n\t\tself ="}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif spec_content is None:\n\t\treturn None\n\tif isinstance(spec_content, dict):\n\t\treturn spec_content\n\telse:\n\t\traise IRValidatorException(\n\t\t\tu\"spec is supposed to be string, but was specified\\n\"\n\t\t\tu\"spec_content was neither a string nor a dict type: %s\\n\"\n\t\t\tu\"The spec was assumed to be string but the spec\\n\"\n\t\t\tu\"was empty\\n\"\n\t\t\tu\"%s\" % (spec_content, spec_content),\n"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\trequired_keys = []\n\tif not spec_content:\n\t\treturn None\n\n\tfor key, section in spec.sections():\n\t\tif section.name in spec_content and key not in required_keys:\n\t\t\traise IRValidatorException(\"Key '%s' is not present in spec file.\" % key)\n\t\trequired_keys.append(key)\n\n\treturn spec.load_data(spec_content)\n\ndef validate_from_xml(xml_content):\n\tif xml_content.startswith(\"<?xml \"):\n\t\treturn validate_"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\treturn _validate_from_content(\"\", \"\", spec_content)\n\n\ndef _validate_from_content(indent, name, spec_content):\n\t\"\"\"\n\t\t:param indent: indentation string for validating output\n\t\t:param name: name of the field, which is not required anymore with\n\t\t    a field with a default value\n\t\t:param spec_content: content of spec file\n\t\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\twith open(spec_content) as spec:\n\t\tdata = yaml.load"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif spec_content is not None: ## TODO: check here spec file is really a spec yaml file\n\t\tspec_content = file_str2bytes(spec_content)\n\tcontent = yaml.safe_load(spec_content) ## should not raise exception because 'content' should not be a str\n\t\t\n\treturn content\n\n\n\nif __name__ == \"__main__\":\n\t# Parse input parameters to construct API client\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"-c\", action=\"store_true\", help=\"Read input yaml content.\")\n\targs"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tcheck = False\n\tif spec_content is None:\n\t\treturn\n\tif spec_content.strip():\n\t\tspec_config = yaml.safe_load(utils.load_file(spec_content))()\n\t\tfor key in spec_config.keys():\n\t\t\tif not key.endswith('config'):\n\t\t\t\traise IRValidatorException(\n\t\t\t\t\t'config entry is missing in config file:'+ key +'in spec:'+ spec_content)\n\t\tcheck = True\n\treturn check\n\ndef parse_config_definition(config_contents"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tdata = {}\n\n\tif spec_content is not None:\n\t\tyaml_spec = yaml.load(spec_content, Loader=lambda f: Loader(f))\n\t\tfor section_name in yaml_spec:\n\t\t\trequired_parameters = yaml_spec[section_name].get('mandatory_parameters', [])\n\t\t\tfor mandatory_parameter in required_parameters:\n\n\t\t\t\tdata.setdefault(mandatory_parameter, [])\n\n\t\t\t\tdata[mandatory_parameter] = validate_from_content(\n\t\t\t\t\tmandatory_parameter,\n\t\t\t\t\tmandatory_parameter_"}]}
{"task_id": "6306091c73426c38ae68acdd", "prompt": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tdata = cls.load_from_file(yaml_file)\n\treturn cls.validate_from_dict(data)\n\n#------------------------------------------------------------------------------------\n# Imports\n#------------------------------------------------------------------------------------\n\nfrom distutils.version import LooseVersion\nimport os, sys, inspect\n\nis_pydot = inspect.isgeneratorfunction(sys.argv[0])\nif not is_pydot:\n\tsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\t# This will get removed in python2"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\ttry:\n\t\tif yaml_file is not None:\n\t\t\twith open(yaml_file) as f:\n\t\t\t\tdata = yaml.load(f, Loader=SafeLoader)\n\t\t\treturn data\n\texcept ImportError, e:\n\t\traise IRValidatorException(\"Cannot parse yaml file. No file provided.\")\n\n\n\n\"\"\"\n\tClass for loading data from other files with a single\n\tload()  method\t\n\t\n\t:ivar data: Dictionary containing data loaded from file or other data input\n\t\t\tif data not required and file specified,\n     "}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\twith open(yaml_file, 'r') as fd:\n\t\twith yaml.reader(fd) as reader:  # reader() function is deprecated in yaml 1.4 and will be removed in yaml 1.5\n\t\t\treturn cls.validate(reader)\n\ndef validate_from_string(cls, obj_str):\n\t\"\"\"\n\t\tLoads & validates a YAML string\n\t\n\t\t:param obj_str: String to find and load in YAML\n\n\t\t:raise IRValidatorException: if string isn't representable\n\t\t:return: Dictionary with data loaded"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif yaml_file is None:\n\t\traise IRValidatorException(\"'data_file' missing. Please specify a valid yaml file\")\n\treturn ir_data_validator.read_json_data(yaml_file)\n\n\nclass IRValidatorException(Exception):\n\tdef __init__(self, msg):\n\t\tself.msg = msg\n\n\nclass IRCodeValidatorException(IRValidatorException):\n\tdef __str__(self):\n\t\treturn self.msg.format('IR Code Validator')\n\n\nclass IRValidation(object):\n\t\"\"\"\n\t\tA class to"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\trequired_fields = ['name', 'type', 'options', 'value']\n\tparser = None\n\tif not yaml_file:\n\t\traise IRValidatorException('No YAML file defined for validator, it\\'s mandatory')\n\t\n\tcls.log.debug('Validating YAML file %s', yaml_file)\n\tparser = yamllint.loadyml.Yamllint(yaml_file)\n\tcls.log.debug('Loaded YAML file %s', yaml_file)\n\tself = cls()\n\tself.yaml_spec_validator = yaml_file\n"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif not yaml_file:\n\t\traise ValueError(\"You didn't provide a path to your YAML file\")\n\t\n\tdata_dict = yaml.load(open(yaml_file), Loader=yaml.Loader)\n\trequired_data_dict_keys = list(set(required_data_dict.keys()))\n\tif len(required_data_dict_keys)!= len(data_dict):\n\t\traise KeyError(\"Missing optional data in file: %s\"%(yaml_file))\n\t\n\treturn data_dict\n\n\ndef validate_from_file_and_dict"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\ttry:\n\t\twith open(yaml_file) as input_file:\n\t\t\tdata = yaml.load(input_file, Loader=yaml.Loader)\n\texcept yaml.exception.YAMLError as exc:\n\t\traise IRValidatorException(exc)\n\texcept (OSError, IOError, TypeError) as exc:\n\t\traise IRValidatorException(exc)\n\treturn data\n\ndef validate_from_dir(cls, yaml_path=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tfrom..definitions.validator import Validator\n\tfrom..definitions.utils import Utils\n\tyaml_file = file_relative_path(yaml_file)\n\t#\tFirst, check that data is not present\n\ttry:\n\t\tassert not yaml.safe_load.check(yaml_file), \"YAML must be valid YAML\"\n\texcept:\n\t\traise IRValidatorException(\"Invalid YAML file: \" + yaml_file + \"\\nCheck that YAML file is valid YAML\")\n\t\t\n\tif not os.path.isfile(yaml_file):\n\t\traise IRValidatorException(\""}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif not isinstance(yaml_file, Path):\n\t\traise(ValueError(\"Expecting a Path\"))\n\t\n\tdoc = yaml_file.read_document()\n\tyml = yamlify(doc,\n\t\tobject_hook=dict,\n\t\tmapping_callback=dict,\n\t\tsequence_items=yaml.MappingNode,\n\t\tstrict_order=True,\n\t\tsort_keys=True,\n\t\tduplicate_keys_allowed=False)\n\treturn yml\n\n\ndef load_from_string(yaml_str):\n\tdoc = yaml.load"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif yaml_file is None:\n\t\tyaml_file = open(os.path.join(os.getcwd(), \"config\", \"default_config_directory\", \"config_files\", \"example_settings.yml\"), \"r\")\t\t\n\tdata_dict = yaml.safe_load(yaml_file)\n\tmeta_data_dict = None\n\tif isinstance(data_dict, list):\n\t\tfor element in data_dict:\n\t\t\tfor sub_key, value in element.items():\n\t\t\t\tmeta_data_dict = {\n\t\t\t\t\t\"name\": value"}]}
{"task_id": "6306091d73426c38ae68ace5", "prompt": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfor includeDict in parser_dict.get('include', {}).get('group', []):\n\t\tparser_obj = includes_parser.ParseConfig(includeDict['config'])\n\t\tparser_obj.resolve_includes(includeDict.get('spec', {}))\n\n\nclass _include_group_directive:\n\tdef __init__(self, name):\n\t\tself.name = name\n\n\tdef __call__(self, parser_dict):\n\t\tparser_obj = parser_dict.get('include', {}).get('group', []).get(self."}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfor line in parser_dict[\"spec\"]:\n\t\t#print(\"  line\", line)\n\t\t#print('INCLUDE GROUPS {} \\n\\n'.format(self.include_groups))\n\t\tif not line:\n\t\t\tcontinue\n\n\t\tself.include_groups += (line,)\n\treturn\n\n\ndef _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include strucution directive in the spec files.\n\t\"\"\"\n\tfor line in parser_dict[\"spec\"]:\n\t\t#print(\"  line\", line"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tparser_dict['include'] = { \n\t\t'__file__': self.project_config['include'],\n\t\t'group': self.project_config['include_group'],\n\t\t'use': not self.project_config['compile_dir']\n\t} \n\tself.project_config['include_group'] = None\n\treturn parser_dict\n\npackage com.github.megatronking.svg.iconlibs;\n\nimport android.content.Context;\nimport android.graphics.Canvas;\nimport android.graphics.ColorFilter;\nimport android.graphics."}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfor item in parser_dict.values():\n\t\tif isinstance(item, tuple):\n\t\t\titem = self._resolve_include_tuple(item)\n\t\telif isinstance(item, list):\n\t\t\titem = self._resolve_include_list(item)\n\t\telif isinstance(item, set):\n\t\t\titem = self._resolve_include_set(item)\n\t\telif isinstance(item, dict):\n\t\t\titem = self._resolve_include_dict(item)\n\t\telif isinstance(item, (list, set, dict)):\n\t\t\titem"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\twith util.filecontext(parser_dict['filename']) as context:\n\t\tself.__include_groups = context.resolve(parser_dict['args'][-1][0:-4],\n\t\t\tinclude_groups=self.__include_groups)\n\n\n\ndef _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include groups argument, or a list of argument\n\t\"\"\"\n\tif parser_dict['has_include_groups']:\n\t\tself.__includegroups = parser_dict['include_groups']\n\telse:\n\t\tself.__"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfor full_name, module_name, _ in parser_dict.items():\n\t\tif not hasattr(self, '__'+module_name+'_include') or len(getattr(self, '__'+module_name+'_include')) > 1:\n\t\t\tcontinue# Skip if we have already included this module or there are some circular\n\t\tincluded_name = self.__module_dict[full_name]['module']\n\t\tif included_name not in self.__module_dict :\n\t\t\twith self.__file_handle.open('r') as fp:\n\t\t\t\tincluded_"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t# handle group definitions first\n\tif not (self.include_group or self.group or self.optional or\n\t        self.required):\n\t\treturn None\n\n\tdef is_keyword(tokens):\n\t\tif tokens.type == 'include':\n\t\t\treturn True\n\t\telif tokens.type == 'group':\n\t\t\treturn False\n\t\telse:\n\t\t\traise ValueError('')\n\n\t# get include file\n\tinclude = None\n\tfor token in parser_dict['include']:\n\t\tif isinstance(token, NameToken):"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tself.resolve_include_group_dict()\n\tdef_config = ConfigFactory.from_dict(self.settings)\n\tparser_dict = self.add_directives(parser_dict,\n\t\t\t\t\t\t\t\tself.settings.get(\"include_group\", None))\n\twith self:\n\t\tif self.settings.get(\"spec\", self.settings.get(\"specs\", {})) == {}:\n\t\t\tparser_dict = self.resolve_directive_groups(parser_dict)\n\t\t\tparser_dict = self.resolve_groups_presets(parser_dict)\n"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tparser_dict[self.resolve_include_dict] = True\n\nclass _spec_file_directives_def(object):\n\t\"\"\"\n\t\tThe include files list.\n\t\"\"\"\n\tdef __init__(self):\n\t\t# self._include = None\n\n\t\t# # 1st pass the include files and a dict with:\n\t\t# #\n\t\t# #    source (string)\n\t\t# #      The name of the include file\n\t\t# #\n\t\t# #    define (bool)\n\t"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfor item in parser_dict:\n\t\tif item[0] in (\"include\", \"group\"):\n\t\t\tgroup_attr = item[1]\n\t\t\tif group_attr[-1] == \".\":\n\t\t\t\tgroup_attr = group_attr[:-1]\n\t\t\tif group_attr == \"\":\n\t\t\t\tgroup_attr = None\n\t\t\tif group_attr and not self.module.has_key(group_attr):\n\t\t\t\tparser_msg = \"Error: invalid group: %r\" % group_attr\n\t\t\t\traise KeyError(parser_msg)"}]}
{"task_id": "6306092373426c38ae68acfa", "prompt": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tdef set_defaults(self, default):\n\t\tvalues = default.keys()\n\t\tself.values = []\n\t\tfor key in values:\n\t\t\tparam = self.set_from_arg(param, values[key])\n\t\tself.init_defaults(default)\n\n\tdef set_defaults(self, data):\n\t\tself.values = data\n\n\tdef set_from_arg(self, arg, value):\n\t\treturn value\n\n\tdef set_to_arg(self, arg, value):\n\t\treturn value\n\n"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tself.set_params()\n\tself.set_source_defaults()\n\tself.merge_source_defaults()\n\tself._process_args([ARG_ID, ARG_OPTIONS])\n\tself.add_spec_defaults()\n\tself._do_source_defaults()\n\nclass Option(object):\n\t\"\"\"\n\tCreate options with default option values.\n\t\"\"\"\n\tdef __init__(self, opt_spec, opt_value):\n\t\tself.opt_spec = opt_spec\n\t\tself._value = opt_value\n\t\tself._process"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tspecs = self.config\n\tspecs = None\n\tfor key in sorted(self.config.keys()):\n\t\tif isinstance(self.config[key], list):\n\t\t\tself.config[key] = {}\n\t\t\tfor item in self.config[key]:\n\t\t\t\tif key!= 'project_dir':\n\t\t\t\t\tself.config[key][item] = None\n\t\t\tif 'project_dir' in self.config[key]:\n\t\t\t\t# Getting the full path of the project from the\n\t\t\t\t# configuration file, but not the 'projects_dir"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\targs = self.options['spec']['args']\n\tvals = self.options['spec']['values']\n\tfor k,v in self.args.iteritems():\n\t\tif hasattr(v,'__call__') and v not in vals:\n\t\t\tvals[k] = self._default_value(v)\n\treturn vals\n\ndef _get_options(self):\n\toptions = {}\n\tif self.spec:\n\t\tspec_defaults = self.get_spec_defaults()\n\t\tfor k,v in self.spec.iteritems():\n\t\t"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\treturn self._resolve_args(self._spec_defaults, self._conf_get)\n\ndef _resolve_args(args, fgetval=None):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tif args is None:\n\t\treturn\n\tif len(args) < 1 and not fgetval:\n\t\treturn args\n\tdefaults = {'name' : None, 'default' : None, 'type': None, 'group': None, 'description': None,'required': None}\n\tif len(args) =="}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tpass\n\n\n\n\ndef check_spec_defaults(self):\n\t\"\"\"\n\tCalled by function spec_defaults.\n\tChecks argument type match and value count.\n\t\"\"\"\n\tpass\n\n\n\n\ndef get_spec_file(self, spec):\n\t\"\"\"\n\tCalled to return a description of a spec file (as a string).\n\t\"\"\"\n\tpass\n\n\n\n\ndef get_spec_file_in_directory(self, rootpath, spec):\n\t\"\"\"\n\tReturn a description of a spec file (as a string) in a directory."}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tspec = self.current_spec()\n\tcurrent_values = spec.settings.getlist(\"system/current_values\")\n\tif \"extra-options\" in current_values:\n\t\t# resolve the list of values\n\t\textra_options = spec.command_line(\"resolve\",\n\t\t\t\tfile_relative=current_values[\"extra-options\"],\n\t\t\t\tnoisy_mode=self.noisy_mode)\n\t\t# filter options, if needed\n\t\t# not needed:\n\t\t# extra_options = [x for x in extra_options if not"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tspec = read_config_file(self.config)\n\tspec.update(read_conf(self.spec))\n\tspec.update(read_conf(self[\"spec\"]))\n\tspec.update(read_script(self[\"spec\"]))\n\treturn spec\n\n// Copyright (c) Microsoft Corporation. All rights reserved.\n// Licensed under the MIT license.\n\n#pragma once\n\n#include <cstdint>\n#include <iostream>\n#include <memory>\n#include <vector>\n#include <chrono>\n#include <thread>\n\n#include \"boost"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tfor a in arguments:\n\t\ttry:\n\t\t\tdest = getattr(config, a)\n\t\texcept AttributeError as e:\n\t\t\tif e.args[0]:\n\t\t\t\traise ConfigError(\"Unable to resolve argument %s: %s\\n%s\"%(a,e.args[0],e.file))\n\t\telse:\n\t\t\ttry:\n\t\t\t\tval = dest()\n\t\t\texcept (NameError, ValueError ):\n\t\t\t\tval = dest.__dict__[''.__dict__['default']__]()\n\t\t\tif isinstance(dest, Config"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t# If default value for \"key\":\n\tif \"key\" in self.get_config_vars():\n\t\treturn self.get_config_vars().get(\"key\")\n\n\t# If default value for \"key1\":\n\tif \"key1\" in self.get_config_vars():\n\t\treturn self.get_config_vars().get(\"key1\")\n\n\t# else return first value as it is\n\treturn self.get_config_var(\"key\")\n\n\n#--------------------------------------------------------------------------------\n\ndef get_config_variables(variables=None):\n"}]}
{"task_id": "6306092973426c38ae68ad01", "prompt": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\treturn { 'deprecated' : self.options.get_deprecated(\"deprecated\") }\n\n\"\"\"\n\tDefining the help\n\"\"\"\nclass CommandLineHelp(CommandLine):\n\t\"\"\"\n\t\tDefine a class for displaying help of classes\n\t\"\"\"\n\n\tdef __init__(self, module, config):\n\t\tself.module = module\n\t\tself.config = config\n\n\tdef do_main(self):\n\t\t# Define command help\n\t\tself.module.exit = self.do_exit\n\n\tdef do_exit(self):\n\t\ttry:\n"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\tresult = {}\n\n\tfor name in self.args_list:\n\t\tif name == \"__argnames\":\n\t\t\treturn {}\n\t\tresult[name] = self.args_list[name].copy()\n\n\tfor name in self.deprecations_list:\n\t\tif name == \"__deprecated__\":\n\t\t\tresult[\"DEPRECATED\"] = self.deprecations_list[name][:2]\n\t\t\tresult[\"OLD\"] = self.deprecations_list[name][2]\n\t\telse:\n\t\t\tresult[name] = self.deprec"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\toptions = [\n\t\t('no-auto-import',\n\t\t\"\"\"By default pybind11 will automatically import all modules as static.\n\t\t\t\t    This is very convenient to enable. Use this option when you have pybind11 modules, without having to install them manually.\n\t\t\tThis option enables you to choose between auto or manual imports.\n\t\t\t\"\"\",\n\t\tdefault=False,\n\t\ttype=bool),\n\t]\n\treturn vars(self.opts) + options\n\n\"\"\"\n\tCode to automatically import modules in Pybind11.\n\t\"\"\"\n\tdef"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\treturn {\n\t\t'id': {\n\t\t\t\t'name' : 'the id which is depricated'\n\t\t},\n\n\t\t'name': {\n\t\t\t\t'name' : 'the name which is depricated'\n\t\t},\n\n\t\t'category': {\n\t\t\t\t'categoryID': {\n\t\t\t\t\t\t'name' : 'the category of the record'\n\t\t\t\t}\n\t\t},\n\n\t\t'notes': {\n\t\t\t\t'notes': {\n\t\t\t\t\t\t'description' : 'the original notes for the record'\n\t\t\t"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\treturn {'func': self.get_args, 'kwargs': self.get_kwargs}\n\n  def get_args(self):\n\t\"\"\"\n\t\treturn a tuple of the argument name and option info.\n\t\tIt's a tuple and can contain duplicate names.\n\t\"\"\"\n\tself.argument_names = list(_argument_names)\n\tif len(self.argument_names) == 0:\n\t\tsys.stderr.write(\"\\n  Error: no argument names\\n\\n\")\n\t\tsys.stderr.write(\"Available argument names:\\n\")\n\t"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t# Return None if deprecated_by or deprecated_args is None\n\tif self.deprecated_by:\n\t\treturn None\n\t# Return dictionary with deprecated args\n\treturn self.deprecated_args\n\ndef check_deprecated(deprecated_by):\n\t\"\"\"\n\t\tCheck if deprecated_by is in the list of deprecated options\n\t\"\"\"\n\toptions = set(DEPRECATED_OPTIONS)\n\t# Remove deprecated option if there are no deprecated flag\n\tif len(deprecated_by) and deprecated_by[:len(DEPRECATED_OPTIONS)] == []:\n\t\tdeprecated_by = deprecated"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\td = dict()\n\t\n\td['log'] = self.gluon.log\n\td['niter'] = self.gluon.niter\n\td['random_seed'] = randomutils.random_init_seed()\n\td['verbosity'] = self.gluon.verbosity\n\td['solver'] = self.gluon.solver\n\n\tif self.verbose:\n\t\td['verbose' ] = self.gluon.verbose\n\n\td['recommender'] = self.recommender\n\td['recommender_"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\treturn {\n\t\t: ['-s_debug', '--debug'], # backward compatibility for new default setting\n\t\t: [],\n\t}\n\n\n@Command.register()\nclass TidyOptions:\n\n\t\"\"\"\n\tThis command uses the commandline parser to identify any commandline arguments which are\n\tnot arguments to a command (like those mentioned in :help command), and which are not\n\tcommands themselves (like those of an option).\n\n\tArguments like ``--help, --version`` are ignored in this usage. You need to use the command\n\tparser to make sure the first"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\tret={}\n\tfor arg in self.deprecated_args:\n\t\tif arg in self.args:\n\t\t\tret[arg] = self.args[arg]\n\tfor arg in self.args:\n\t\tif arg in ret and arg not in self.deprecated_args:\n\t\t\tret[arg] = self.args[arg]\n\treturn ret\n\n\ndef get_args(args_filename):\n\targs = {}\n\tfor line in args_filename:\n\t\tfor arg in re.split(r'[^\\s]+', line):\n\t\t\t"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\treturn {\n\t\t'--deprecated_args': (\n\t\t\tu'<string_list>',\n\t\t\tu'<param1>%s</param1>...<paramN>' % self.VERSION_SHORT,\n\t\t\tu'<param1>%s</param1>...<paramN>' % self.VERSION_LONG,\n\t\t),\n\t\t'--deprecated_args_with_doc': (\n\t\t\tu'<string_list>',\n\t\t\tu'<param1>%s</param1>...<paramN>' % doc_docs2short(),\n\t\t\tu'<"}]}
{"task_id": "6306092c73426c38ae68ad02", "prompt": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tself.print_args_deprecation(cli_args, answer_file_args)\n\tlog.logger.critical(\"The arguments in %s for your application are now deprecated. We will continue \"\n\t                     \"including it in the final bundle. You may edit these for your usecase to continue \"\n\t                     \"to the newest version.\\n\\n\".\n\t             "}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tfor name, arg in answer_file_args.items():\n\t\tif not hasattr(self, name):\n\t\t\t# pylint:disable=not-callable\n\t\t\traise Exception(\"Argument for '%s' '%s' is not a valid argument for the command '%s'\" % (key, name, str(value)))\n\t\telif not isinstance(arg, type):\n\t\t\t# pylint:disable=not-callable\n\t\t\traise Exception(\"Argument for '%s' %s is not a valid Python type\" % (key, type(arg).__name__))\n\telse:\n\t"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tif dict_has_key(answer_file_args, \"name\"):\n\t\tname = answer_file_args[\"name\"]\n\t\tdeprecations = dict_has_key(cli_args, \"dry-run\") and cli_args[\"dry-run\"]\n\t\tif deprecations:\n\t\t\traise ConfigError(\"Deprecated arg name %r\" % (name,))\n\tif answer_file_args is not None:\n\t\tfor key in sorted(answer_file_args.keys()):\n\t\t\tif key in self.ALLOWED_ARG_DEPRECATIONS"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\twarnings = {}\n\n\tnew_arg_dict = {}    # argument dict\n\tfor new_arg, arg in list(cli_args.items()):\n\t\tif arg in answer_file_args:\n\t\t\tcontinue      # skip argument in answer file\n\t\tnew_arg_dict[new_arg] = []\n\t\tfor x in cli_args[new_arg].split(','):\n\t\t\tif x.startswith('-'):\n\t\t\t\tnew_arg_dict[new_arg] += cli_args[new_arg].split("}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tif not answer_file_args:\n\t\tanswer_file_args = {}\n\n\tdeprecation = argparse.SUPPRESS\n\tif not cli_args:\n\t\tdeprecation = argparse.SUPPRESS\n\telse:\n\t\tfor arg in cli_args:\n\t\t\tif arg in self.DEPRECATED_DEPRECATIONS:\n\t\t\t\tdeprecation = argparse.SUPPRESS\n\t\t\t\tbreak\n\t\t\telif arg in self.DEPRECATED_DEFAULT_VALUES:\n\t\t\t\tdeprecation = argparse.SUPPRESS\n\t\t\t\tbreak\n\t\t\t\n\tif not deprecation:\n\t\twarnings.warn(\"\"\"\n\t\tSome"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tfor arg, valid in self.DEPRECATED_ARG_NAMES:\n\t\tif arg in cli_args and not answer_file_args:\n\t\t\tprint(\"\\n\\n\" + ERRORS.keys()[1][\"warning\"], \"\\n    This argument is deprecated and will be removed in future versions of FuelSense.\")\n\t\t\tbreak\n\t\telif cli_args and arg in cli_args:\n\t\t\tprint(\"\\n\\n    Argument with name {} is deprecated and will be removed in future versions of FuelSense.\".format(arg), \"\\n\")\n\t\t\tbreak"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tif \"use-config\" in cli_args and not answer_file_args[\"use-config\"]:\n\t\traise FatalError(\"Use of config file is deprecated.  Please use the --config option to provide an override.\")\n\tif not validate_config(cli_args[\"use-config\"]): return\n\tif answer_file_args[\"use-config\"]:\n\t\tdefault_config = answer_file_args[\"use-config\"].split(\"|\")\n\t\tuse_config = default_config.pop()\n\t\tif use_config in (None, \"\").split():\n\t\t"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t# TODO:\n\tprint(\"# WARNING: This program is now deprecated. You may get broken\")\n\n\t# validate file args and arg names\n\tif not answer_file_args:\n\t\tprint(\"No file given, please provide arguments file.\", file=sys.stderr)\n\t\tsys.exit(1)\n\n\tfor arg_name in answer_file_args:\n\t\tif arg_name and arg_name in cli_args:\n\t\t\tparser_class = _get_parser_class_from_name(arg_name)\n\t\t\tif parser_class"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tcli_args[\"__doc__\"] = \"Deprecated arguments, please use the --config option\"\n\n\ttry:\n\t\tself.validate_args(cli_args, answer_file_args)\n\texcept ValidationError:\n\t\tself.echo(f\"WARNING: {cli_args['file_name']} is not using arguments that were deprecated\")\n\t\t\ndef validate_arg_file(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidate and print the arguments given on the command line.\n\t\"\"\"\n\tself.validate_args(cli"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tif not args.check_deprecated_arguments(self):\n\t\tprint(\"You have disabled the deprecated arguments.\")\n\t\treturn False\n\t\n\treturn True\n\n#################################################################\n#                                                                 #\n#           Copyright (C) 2007-2012  Volker R"}]}
{"task_id": "6306092d73426c38ae68ad04", "prompt": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tparser = OptionParser(\n\t\tusage=SUPPRESS_USAGE\n\t\t\t.format(self.__class__.__name__))\n\t\n\tparser.add_option(\n\t\t'-c', dest='command', type=str, default='main',\n\t\thelp=\"The command to execute to get the options\")\n\t\n\t# This is a temporary option, since it needs some time...\n\tparser.add_option(\n\t\t'-e', dest='command_args', metavar='',\n\t\tdefault='--verbose=1',\n\t\thelp=\"Specify which arguments and options are allowed by"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tparser = self.getparser()\n\ttry:\n\t\tparser.optionxform = (lambda c: c.optionxform)\n\t\treturn parser.optionxform(command_name)[1]\n\texcept:\n\t\treturn None\n\n\ndef get_parser_option_values(self, command_name):\n\tparser = self.getparser()\n\ttry:\n\t\tparser.optionxform = (lambda c: c.optionxform)\n\t\toption_values = parser.optionxform(command_name)[1]\n\texcept:\n\t\treturn None\n"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tif self.is_command_set(self.command_info[command_name]['set_name']):\n\t\treturn self.command_info[command_name]['set_options']\n\treturn []\n\ndef get_parser(parser_name=None): # Get parser by name\n\tif parser_name is None:\n\t\tparser_name = sys.argv[0].split(\"parser\")[-1]\n\t\tprint(f\"Got parser: {parser_name}\")\n\treturn getattr(parsers, parser_name)\n\ndef get_parser_command_"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tparser = OptionHandler.get_parser(self, command_name)\n\tparser.add_argument(\"command\", nargs='?')\n\treturn parser.parse_args()\n\n\ndef handle_help(self, options):\n\t\"\"\"\n\tHandle the various help output\n\t\n\t:param options: the Commandline options\n\t\"\"\"\n\tprint \"\\nCommand Line Options\\n------------------------------\"\n\tprint \"----------------------------------------\"\n\tfor name, option in sorted(options.items()):\n\t\tprint \"%-30s - %s\\n\" % (name, option['help"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\targs = self.parser.parse_args(command_name)\n\tparser_options = OrderedDict()\n\n\tfor group_name, group in self.parser.groups.items():\n\t\tfor command_name, option in group.items():\n\t\t\t# only collect if the specified options are in the group\n\t\t\tif group_name in args[command_name]:\n\t\t\t\tparser_options[command_name] = option\n\t\t\t# break if a new option was detected\n\t\t\tif command_name not in self.parser.options:\n\t\t\t\tbreak\n\n\t"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tresult = []\n\t\n\tif self.debug:\n\t\tprint(\"*** Command Parser ***\")\n\n\ttry:\n\t\tusage = self.parser.usage(command_name, 1)\n\t\tprint(\"Command Line: {}\", command_name)\n\texcept Exception, e:\n\t\tprint(\"Error:\", repr(e))\n\t\treturn []\n\n\treturn CommandOption.parser_parser.get_option_info(\"command_parser.py\")(\n\t\tself, \n\t\tresult, \n\t\tcommand_name, \n\t\toptions=usage,\n"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tparser = self.get_parser(command_name)\n\tparser.add_argument(\"-p\", \"--plugin-path\", type=str, default=_get_plugin_path(), metavar=\"PLUGIN_PATH\")\n\tparser.set_defaults(command=command_name)\n\n\t# the option --no-progress disables progress\n\tparser.add_argument(\"--no-progress\", action=\"store_true\", default=False, help=\"do not show GUI\")\n\tparser.add_argument(\"-o\", \"--output-path\", type=\"path\", default=_get_output_path(),"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tparser = self._get_parser(command_name)\n\t# Get the command-level aliasing options.\n\talias = self._get_alias_commands(command_name)\n\t# Get a list with all the options for the alias for the\n\t# command.\n\t# These options exist only in the \"alias\" list\n\t# They are not command-specific\n\toptions = parser.get_alias_options()\n\toptions += self._get_parser_option_specs(command_name)\n\t# Get a list with all the options for the command.\n\t"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\treturn self._commands[command_name]['short_opts']\n\n\t\ndef get_parser_command_help(self, command_name):\n#\t\"\"\"\n#\t\tGets all the short and long help for the specified command\n#\t\t:param command_name: the command name (main, virsh, ospd, etc...)\n#\t\t:return: the list of all command options\n#\t\"\"\"\n#\treturn self._commands[command_name]['long_help']\n\n\t\n\t\ndef get_parser_commands(self, command_name):\n\t"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tparser = self.get_parser_object()\n\toptions = parser.get_option_table().show(command_name)\n\tif len(options) == 0:\n\t\tlog.error(f'Unknown command \"{command_name}\"')\n\treturn options\n\n\t#for name in self.get_parser_option_specs(\"show\"):\n\t#\toptions = self.show(command=name[0],\n\t#\t\t\ttitle=name[1])\n\t#\tfor d in options:\n\t#\t\tself.parser.parser_info.add"}]}
{"task_id": "6306092d73426c38ae68ad05", "prompt": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tassert len(self.options) > 0, \"No options defined\"\n\n\targument_name = self.get_argument_name(argument_name)\n\targument_spec_list = [argument_spec for argument_spec in argument_spec_list]\n\n\tfor option_spec in list(self.options.values()):\n\n\t\tif option_name == option_spec.name:\n\t\t\tif check_argument_type(argument_name, option_spec):\n\t\t\t\tassert len(self.option_parameters[option_spec.id_or_name_"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn self.spec_option(command_name, argument_name, self.optionParser.get_option)\n\ndef get_command_spec(self, command_name):\n\t\"\"\"\n\t\tGets the specification for a complete command.\n\t\"\"\"\n\treturn self.spec_command(command_name, self.commandParser.get_command)\n\ndef get_command_option_spec(self, command_name, argument_name) :\n\t\"\"\"\n\t\tGets specification for the value of a command-line option and\n\t\treturns None if the option does not"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn self.specifications.CommandOptions[command_name].OptionSpec.GetOptionSpec(argument_name)\n\ndef process(self):\n\tfor child in self.Get_Children():\n\t\tnew_child_object = CliChildObjects().GetChild(child)\n\t\tnew_child_object.Process(self)\n\tself.SetFlag(ControlTypes.PROCESS_STATUS)\n\ndef get_help_for_command(self, command_name):\n\t\"\"\"\n\t\tGet the help text for a command.\n\t\"\"\"\n\n\tcommand = self"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn self[\"optspec\"][command_name][argument_name]\n\ndef getarg_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the next specified option name.\n\t\"\"\"\n\tcommand_name = command_name[0]\n\treturn self[\"optspec\"][command_name][argument_name]\n\n/****************************************************************************\n *\n * ViSP, open source Visual Servoing Platform software.\n * Copyright (C) 2005 - 2019 by Inria. All rights reserved.\n *\n * This software is free software; you can"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tresult=commands_lib.get_option_spec(self.app_state.commands, self.cur_option_spec_, command_name, argument_name)\n\treturn(result)\t\t\n\ndef get_sub_commands_definition(self):\n\t\"\"\"Builds a dictionary of sub commands definition.\"\"\"\n\tif self.sub_commands_definition_!= None:\n\t\treturn self.sub_commands_definition_\n\tif self.app_state.option_dict.get(\"sub_commands_definition_\"):\n\t\tdef sub_commands_definition_factory_(self"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn get_option(self, command_name, argument_name, False, 'Option', False)\n\n\t\ndef get_subcommand_option_spec(self, command_name, argument_name, option_class):\n\"\"\"\n\tGets the specification for the specified subcommand option.\n\t\"\"\"\n\treturn get_option(self, command_name, argument_name, False, 'Subcommand Option', False, option_class)\n\n\t\ndef get_subcommand_option_spec_long(self, command_name, argument_name):\n\"\"\"\n\tGets"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn OptionGetSpec(self, _option_registry, command_name, argument_name)\n\n\nclass OptionGetSpec(object):\n\t\"\"\"\n\tUsed by :py:meth:`PoptOption.GetSpec`.\n\t\"\"\"\n\tdef __init__(self, opt_registry, command_name, argument_name):\n\t\tself.__command_name = command_name\n\t\tself.__argument_name = argument_name\n\t\tself.__value = None\n\t\tself.__aliases = {}\n\t\tself.__desc = None\n\t\tself.__option_type"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn self.call(command_name, argument_name)\n\ndef get_all_options(self):\n\t\"\"\" Gets all options, in a dict keyed by command name.\n\tArguments are the same as in __get_all_commands.\n\tKeys must be in 'list' unless they are prefixed with\n\t'python.'\n\t\"\"\"\n\tresult = {}\n\tfor cmd_obj in self.__get_all_commands():\n\t\tinfo = cmd_obj.get_command_details()\n\t\tresult[cmd_obj.command_name] = info\n\t"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tcommand_name = _command_names.get(command_name, command_name)\n\tcommand_option_spec = self.command_options[command_name]\n\tif command_option_spec is None:\n\t\treturn None\n\treturn command_option_spec.get_spec_property(argument_name)\n\n\n#------------------------------------------------------------------------------\n\n\nclass CommandOptionList(CommandOptionBase):\n\t\"A CommandOption that can be stored in a list.\"\n\tcommand_options = None\n\n\tdef set_command_option(self, command_name, argument"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\toptions = self.Get_options()\n\t# print 'Getting options %r.'%(options)\n\tfor option in options:\n\t\t# print 'Option '%(option)s'\n\t\tif option.name == command_name and option.argument_name == argument_name:\n\t\t\t# print 'option is found.'\n\n\t#print 'option found.'\n\t#print'returning spec:'+ self.option_name(command_name, argument_name)\n\treturn self.option_name(command_name, argument_name)\n\n"}]}
{"task_id": "6306092d73426c38ae68ad06", "prompt": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\treturn args[1:] if args[0] == \"--silenced\" else args\n\nclass SilencedArgument(ArgumentProxy):\n\tdef get_silent_args(self, _):\n\t\t\"\"\"\n\t\tReturn the silent arguments in the command line\n\t\targuments.\n\t\t\"\"\"\n\t\targs = self.get_silent_args()\n\t\targs.extend([\"-s\", \"-s\"])\n\t\treturn args\n\n\tdef get_silent_arguments(self, args):\n\t\t\"\"\"\n\t\tReturn the silent arguments or return an empty dictionary if no\n"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tsilenced_args = []\n\t\n\tfrom corehq.apps.app_manager.models import Application\n\tif not args:\n\t\treturn silenced_args\n\t\n\t# if a list of argument is provided, list all and process for that argument\n\t# if a not list is provided, return that list\n\tif isinstance(args, (list, tuple)):\n\t\tsilenced_args = []\n\t\tfor entry in args:\n\t\t\tif isinstance(entry, tuple):\n\t\t\t\tentry = (entry[0], entry[1])\n\t\t\tsilenced_args"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tfor idx in args:\n\t\tif isinstance(args[idx], SilencedArgument):\n\t\t\targs[idx.name] = get_silent_args(args[idx], silent=True)\n\treturn args\n\n\ndef parse_sourceset(self, sourceset, context=None):\n\t\"\"\"\n\tParameters:\n\tsourceset - The sourceset to be parsed\n\t\n\t:param sourceset: the sourceset\n\t:type sourceset: SourceSet\n\t\"\"\"\n\t# sourceset is set to a function in Python that looks for a\n\t# __slots__"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tret_val = []\n\tif len(args) == 0:\n\t\tpass\n\telif len(args)!= 1:\n\t\tlog_interactive.error(\"Silenced arguments only supports \"\n\t\t\"one argument, got %s.\" % str(args))\n\t\tret_val = []\n\t\treturn list(reversed(ret_val))\n\targ = args[0]\n\tif arg in SILENCED_ARGS:\n\t\treturn SILENCED_ARGS[arg]\n\telse:\n\t\tfor k in SILENCED_ARGS:\n\t\t\tif"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tsilenced = []\n\tfor s in args:\n\t\tif callable(s):\n\t\t\tsilenced.append(None)\n\t\telif isinstance(s, str):\n\t\t\tsilenced.append(s)\n\t\telif len(s) == 3:\n\t\t\tsilenced.append(s[0])\n\t\t\tsilenced.append(s[1])\n\t\telif len(s) > 3:\n\t\t\tfor i in range(len(s)):\n\t\t\t\tif isinstance(s[i], str):\n\t\t\t\t\tsilenced.append(s"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\targ_silenced = []\n\tif args.silent_arg_prefix:\n\t\targ_index = args.silent_arg_prefix.rfind(args.silent_arg_suffix)\n\t\targ_length = len(args.silent_arg_prefix)\n\t\twhile (arg_index!= -1):\n\t\t\targ_silenced +=  [args.silent_arg_prefix[i:i+arg_length] for i in\n\t\t\t\t\t\t\t\t\t\t\t\t\trange(arg_index, arg_index+arg_length)]\n\t\t\targ_index = args."}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tsilenced_args = []\n\tfor arg in args:\n\t\tif arg.get(\"silenced\") is False:\n\t\t\tsilenced_args.append(arg[\"name\"])\n\t\n\treturn silenced_args\n\ndef check_bad_value(arg, values=['str','bool']):\n\t\"\"\"\n\tchecks if a given value is not'str' or 'bool'\n\t\n\t:param arg: the current argument\n\t:param values: possible values of arg\n\t:return: bool,\n\t\"\"\"\n\t\n\treturn arg.get(values[0])"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tif not args: return []\n\t# remove any spurious parameters\n\tfor a in args:\n\t\tassert len(a) >= 2, 'args contain at least two parameters, %s provided'%a\n\t\tif a[-2] == '.':\n\t\t\ta = a[:-1]\n\t\t# convert to underscore with dot (no underscore at beginning)\n\t\ta = a.replace('-', '_').replace('.', '_')\t\t\n\t\treturn [a] + list(a[1:])\n\n\nclass LogInfo:\n\tdef __init__(self,args):"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tsilenced_args = []\n\n\tfor i in range( len(args) ):\n\t\targ_type = args[i].type\n\t\tif arg_type is None:\n\t\t\tcontinue\n\t\tif isinstance( arg_type, type ) == False:\n\t\t\tcontinue\n\n\t\tsilenced_args.append( args[i].name )\n\n\treturn silenced_args\n\ndef read_log( filename, **kwargs ):\n\t\"\"\"\n\tReads each line line from a log file.\n\tIf any arguments are given, the function will return a"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tsilenced_args = list()\n\t\n\t# get the argument value\n\tcur_arg = args[0]\n\tif cur_arg.name.startswith('silenced_arg_name_') or cur_arg.name =='silenced_arg_name_':\n\t\tsilenced_arg_list = cur_arg.value.split('/')\n\t\tsilenced_args = silenced_arg_list\n\t\n\t# get the rest of the arguments\n\tother_args = list()\n\tfor i, arg in enumerate(cur_arg.value.split('/'"}]}
{"task_id": "6306092d73426c38ae68ad07", "prompt": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif len(args) > 0 and args[0] is None:\n\t\traise TypeError(\"A valid positional argument must be provided!\")\n\treturn\n\n\ndef print_list_as_formatted_string(lst, max_cols=80):\n\t\"\"\"\n\t\tPrints the list to a string.\n\t\"\"\"\n\t#  List Formatting:\n\t#  - -:                                 -\n\t#      Type"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tfor arg in args:\n\t\tif arg not in self.required:\n\t\t\tself.raise_exception(\n\t\t\t\t'%r is not present in this version of %r' % (\n\t\t\t\t\targ, self.name),\n\t\t\t'not present', self.required)\n\n\nclass RequirementsFileParser(Parser):\n\n\tdef __init__(self, *args):\n\t\tsuper(RequirementsFileParser, self).__init__('requirements', *args)\n\t\tself.requires_args = args\n\t\tif len(args) == 2:\n\t\t\t"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t# Check if there are args\n\trequired_args = self.check_requires_args()\n\tif required_args is not None:\n\t\tif args == required_args:\n\t\t\tres.log(MSG['INFO'])\n\t\t\treturn True\n\treturn False\n\ndef validate_requires_args_if(self, args, default=False):\n\trequired_args = self.check_requires_args()\n\tif required_args is None:\n\t\tif default:\n\t\t\trequired_args = []\n\t\telse:\n\t\t\trequired_args ="}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tfor arg in args:\n\t\tif hasattr(self.cmd, arg):\n\t\t\tcontinue\n\t\traise AttributeError(\"arg {} not provided\".format(arg))\n\ndef validate(self):\n\t\"\"\"\n\t\tValidate input parameters\n\n\t\t1. Validate required arguments and check if all the provided arguments\n\t\t   have been provided.\n\t\t2. Validate the output_data and print_results and exit.\n\t\"\"\"\n\tself.validate_requires_args(self.required_args)\n\n\tself.validate_common_arguments(self.optional_args)"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tmissing_args = self.get_missing_args(args)\n\tif len(missing_args) > 0:\n\t\tself._error(missing_flag)\n\ndef validate_requires_kwargs(self, kwargs):\n\t\"\"\"\n\t\tCheck if all the required keyword arguments have been provided.\n\t\"\"\"\n\tmissing_args = self.get_missing_args(kwargs)\n\tif len(missing_args) > 0:\n\t\tself._error(missing_kwarg)\n\ndef validate_requires_kwarg(self, kwargs):\n\t\"\"\"\n"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\targs_missing = set(args) - {self.name}\n\tfor arg in args_missing:\n\t\traise RuntimeError(\"Missing argument '%s'.\" % arg)\n\ndef validate_arg_count(self, arg_names):\n\t\"\"\"\n\tCheck that arg_names is equal to the number of args in the class\n\t\"\"\"\n\tfor arg in arg_names[1:]:\n\t\tif arg in self.get_args():\n\t\t\treturn\n\traise RuntimeError(\"The number of arguments for '%s' should equal to the number of present args.\" % ("}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired = set(args) - self.required_parameters\n\tfor param_idx, param in enumerate(args):\n\t\targument = getattr(param,'requires_args', {})\n\t\tif required.issuperset(set(argument.keys())):\n\t\t\tself.errors.append(\"The provided parameter {} is not allowed.\".format(param_idx))\n\n\nclass CliValidator:\n\t\"\"\"\n\t\tCreate a validator that does the validation for a specific command function.\n\t\tOnly callable from CliCommand class\n\t\t\"\"\"\n\tdef __init__(self"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tfor arg in args:\n\t\tif not self.check_argument(arg):\n\t\t\traise argparse.ArgumentError(\n\t\t\t\t\t\t\t\"%s is required\" % arg\n\t\t\t\t\t\t)\n\t\t\t\ndef check_required_arg(parent):\n\t\"\"\"\n\tEnsure all required arguments have been provided, then call parent check function.\n\n\t:param parent: parent object on which argument validation has to be run.\n\t\"\"\"\n\tfor arg in re.finditer(r'^\\((.*?)\\)', parent.parser.arguments):\n\t\targs_string = arg.group()\n"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tfor arg in dir(self):\n\t\tif (not args.get(arg) or arg not in args.keys()) and arg not in [\"-h\",\"-help\",\"-v\",\"--version\"]:\n\t\t\traise Exception(\"--%s argument (should be: '%s') is missing.\" % (arg, \" \".join([str(a) for a in args.keys() if a!= arg])) )\n\n@defvalidate_require(\"requires\")\ndef validate_requires(self, requires):\n\"\"\"\n\tValidate all the required arguments\n\"\"\"\n\tif len(requires) == 0"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired_args = args['requires']\n\trequires_args = self.get_argument_names(required_args)\n\n\tfor name in requires_args:\n\t\trequired_name = required_args[name]\n\t\tnot_required_name = required_args[name]\n\t\tif name in args['exclude']:\n\t\t\traise AttributeError('{} must not be set as required'\n\t\t\t\t.format(name))  # pragma: no cover\n\t\telif required_name is not not_required_name:\n\t\t\traise AttributeError('{} must be set as"}]}
{"task_id": "6306092d73426c38ae68ad08", "prompt": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\tmatch_spec = _get_options_match_spec_for_required_args(command_name,\n\t\toptions_spec, args)\n\tcommand_name = command_name.split('.')[0]\n\targs = args\n\tmatching_commands = []\n\twhile len(args):\n\t\tif not self._match_options_spec(args[0], match_spec):\n\t\t\tbreak\n\t\tmatching_commands.append(args[0])\n\t\targs = [arg for arg in args[1:] if not arg == None]\n\n\t#"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\tcommand_name = command_name.lower()\n\tif options_spec and command_name in options_spec:\n\t\treturn tuple([arg for arg in self.get_args()\n\t\t\tif arg and arg[0].lower()!= command_name])\n\treturn tuple(arg for arg in self.get_args() if arg and any(\n\t\tcondition for condition in arg[1:] if isinstance(condition, str)))\n\n\"\"\"\n\tget a list of conditional arguments, where a condition that matches at least\n\tone command is removed at the end of the list."}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\targs = {}\n\t\n\tfor index, option in enumerate(command_name.options) \\\n\t\t\tif option.dest() == 'option' and option.name()!= '-option' \\\n\t\t\tand isinstance(option.dest(), str) \\\n\t\t\tand option.value() not in [\"\", \"*\", \"+\", \"*\", \"=\", \"-\"]:\n\t\t\t\t\t\targ_name = option_spec.split(':')[-1].strip(\"\\\"\")\n\t\t\t\t\t\tif arg_name in options_spec:\n\t\t\t\t\t\t\targ_name = \"\".join([arg_name]\n\t\t\t\t\t\t\t\t\t\t\t\t + [opt"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\tcmdspec = self.get_argspec(args[0], options_spec, \"cmdspec.id\")\n\tcmd_name = command_name.lower()\n\tself.log.debug(\"_get_conditionally_required_args args=%r, cmd_name='%s', cmdspec=%r\",\n\t\targs[0:10], cmd_name, cmdspec)\n\tout = cmdspec.get('name').lower()\n\trequired_when = self.spec.get('required_when', [])\n\tmatched_name = []\n\tfor n in required_when:"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\ttry:\n\t\t# parse command arguments\n\t\targs = self.__parse_command_args(command_name, options_spec)\n\t\t# parse command options\n\t\targs = self.__parse_options(command_name, options_spec, args)\n\t\tkwargs = self.__parse_kwargs(command_name, options_spec, args)\n\t\tcommand = self.__commands[command_name]\n\t\treturn kwargs, args, command\n\texcept BaseException:\n\t\t# catch command error\n\t\tself.__error_handler.handle(command_name"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\tfrom.base import ArgGroup\n\treturn ArgGroup(\n\t\targ_parsers=self.__arg_parsers,\n\t\trequired_when=True, metavar=False, cmdclass=self\n\t)\n\tsetattr(options_spec,'required_when', required_when)\n\n\n\t\ndef _get_optionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``allow_unless`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\tcommand_option = options_spec.options.options.get(command_name, ()):\n\t\traise AttributeError(f'{command_name!r} is not defined')\n\t\t\n\trequired_when_value = options_spec.args_required_when\n\ttry:\n\t\targs = [arg for arg in args if arg[required_when_value] not in ('', None)]\n\texcept IndexError:\n\t\treturn []\n\telse:\n\t\treturn [(o[required_when_value], o) for o in args]\n\t\n\t\n__commands__"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\toptions = self.options.get(command_name, options_spec)\n\tif not options:\n\t\traise KaresansuiException(\"No options matched '%s' - %s\" %\n\t\t\t\t\t\t\t\t\t\tcommand_name % (options_spec, list(options)))\n\t\t\n\trequired_args = []\n\tfor arg in options['option_args']:\n\t\trequired_args.append(os.path.basename(arg))\n\treturn required_args\n\t\n\n\n################################################################################\ndef _setup_command(self, command_name):\n\t\"\"\"\n\tCreate an command"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\tif options_spec == True:\n\t\toptions = list()\n\telif options_spec == False:\n\t\toptions = list()\n\t\tfor name, value in self.options.unparsed[command_name].items():\n\t\t\tif value == False:\n\t\t\t\toptions.append(name)\n\t\treturn options, []\n\telse:\n\t\traise TypeError(\n\t\t\tf\"OptionsSpec '{options_spec}' is not expected. \"\n\t\t\tf\"Got {options_spec} as a dictionary.\"\n\t\t)\n\ndef _get_conditionally"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\targs_list, required_when = self._get_condition_inputs(\n\t\tcommand_name=command_name, args=options_spec,\n\t\trequired_when=commands.RequiresRequiredArgs)\n\treturn args_list\n\n\n\ndef _get_command_params(self, command, options_spec, allowed_params,\n\thas_output=False, command_prefix):\n\t\"\"\"\n\tParses command with parsed required arguments.\n\n\tCan handle command or options_spec object, if any\n\n\t:param command: command info.\n\t:"}]}
{"task_id": "6306092e73426c38ae68ad09", "prompt": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif self.length > len(args):\n\t\tprint args\n\t\traise Exception(\"Too long argument (> {}).\".format(self.length))\n\n\nclass CheckInputLength(object):\n\t\"\"\"\n\tCheck that string value of argument(s) that is longer than length specified.\n\tThis check is applied if length of argument(s) is not equal to length input.\n\tThis check depends on data type of argument(s).\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tdef __init__(self, args):\n\t\tself.length"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\targ_strs = [str(arg) for arg in args]\n\tif len(arg_strs) > args['length']:\n\t\terror_template = ERROR_LENGTH_LENGTH_IS_LARGER_THAN_STR_LENGTH.format(len(args), args['length'])\n\t\targs['error'] = error_template\n\t\treturn self.async_confirm('confirm_length', args)\n\telse:\n\t\treturn args\n\n\ndef validate_length_choices(self, args):\n\t# Check length\n#\tlength_check_template = \""}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif args.min is None or args.max is None:\n\t\traise ValueError(\"Minimum or maximum length is required!\")\n\t\t\n\n\t\n\n#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.\nfrom aug import add_args, optimizer, param\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom fvcore.common.file_io import PathManager\nfrom fvcore.optim.lr_scheduler import WarmupMultiStepLR, WarmupMultiStepMomentum\n"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tlength = len(args)\n\tif length <= len(self.field.validators): return\n\traise TaurusConfigError(\"Invalid length: The field [%s]'s value [%s] must be less than size [%s]\" % (self.field_name, self.field.validators, length))\n\n##########################################################################\n# Value of type \"string\"\n##########################################################################\nclass StringField(Field):\n\t\"\"\"\n\tTakes a single string value.\n\t\n\t:param str: The string that is being parsed.\n\t\"\"\"\n\tdef __init__(self,"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif len(args) > self.max_string:\n\t\traise ValueError(\"String argument longer than given length\")\n\tif not isinstance(args, list):\n\t\traise TypeError(\"Arguments of list not\")\n\tif len(args)!= len(self.values_by_name.get(\"text\")):\n\t\traise ValueError(\n\t\t\t'Value of the argument has unexpected length. Expected: '\n\t\t\t+ str(len(self.values_by_name.get(\"text\"))) +'Actual: '\n\t\t\t+ str(len(args))\n\t\t"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif len(args)!= len(self.args):\n\t\traise Exception(\"%s received length is out of range, it must be a list with \"+\n\t\t\t\"length of %s, got len=%s\" % ( self.name, len(self.args), len(args)))\n\t\n\tfor key in self.args:\n\t\tif key not in args:\n\t\t\traise Exception(\"%s received value %s of the following key is not in\" % ( self.name, key ))\n\t\telse:\n\t\t\treturn\n\treturn\n\t\n\t\n@register("}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif len(args) > len(self._args):\n\t\traise ValueError(\"Too many arguments.\")  # pragma: no cover\n\t\n\tif args:\n\t\targ_length = len(args)\n\t\tfor idx, arg in enumerate(args):\n\t\t\tif len(arg)!= arg_length:\n\t\t\t\traise ValueError(\"Wrong length. Received {} but expected {}\".format(\n\t\t\t\t\t\t\t\tidx,  arg_length))  # pragma: no cover\n\t\n\treturn args\n\n\n#\n#               "}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif args[0] <= len(args[1]):  \n\t\treturn \"This command was not understood by this device.\"\n\telse:\n\t\treturn None  \n\n#include \"CondTools/Ecal/interface/EcalIntercalibConstants.h\"\n#include \"DataFormats/EcalDetId/interface/EBDetId.h\"\n#include \"DataFormats/EcalDetId/interface/EEDetId.h\"\n#include \"DataFormats/EcalDetId/interface/ESDetId.h\"\n#include \"DataFormats/E"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tfor argIndex in range(len(args)):\n\t\targ = args[argIndex]['value']\n\t\tlength_validation = self.check_arg_length(args[argIndex])\n\t\tif length_validation is None:\n\t\t\treturn False\n\t\tif len(length_validation) <= argIndex:\n\t\t\treturn False\n\t\n\treturn True\n\ndef check_for_unique_arg(args):\n\t\"\"\"\n\tCheck if args have duplicated argument.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\targ_name_map ="}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tassert(not len(args.values[0])) #length not specified\n\tassert(not len(args.values[1])) #length not specified\n\t\n\treturn\n\"\"\"\n\treturn\n\"\"\"\n\treturn\n\"\"\"\n\tself.print_debug_msg(\"Validate arguments\")\n\"\"\"\n\"\"\"\n\treturn\n\t\n\n\"\"\"\ndef validate_value_and_length(self, value, length):\n\"\"\"\n\tCheck the arguments and the length for validity.\n\t\n\t:param value: The received arguments.\n\t:param length: The received length.\n\t:return: Return"}]}
{"task_id": "6306092e73426c38ae68ad0a", "prompt": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tpass\n\n##############################################################################\n# Check if number of arguments is one of the available choices.\n#\n# :param args: The received arguments.\n# :type args: ArgSpec\n#\ndef validate_choices_num_args(self, args):\n\tpass\n\n##############################################################################\n# Check if all choices' arguments are present in options.\n#\n# :param args: The received arguments.\n# :type args: ArgSpec\n#\ndef validate_choices_all_args(self, args):\n\tpass\n\n##############################################################################\n# Check if there are argument(s)"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\targchoices = set(args.keys()) - {'help'}\n\tif argchoices!= self.arg_choices:\n\t\traise Exception('Invalid choice arguments.'+\n\t\t\t\t'Choices: {}.'.format(argchoices) +\n\t\t\t\t'Only allowed choices: {}.'.format(self.arg_choices) +\n\t\t\t\t'Either remove any of the choices or add them to the'+\n\t\t\t\t'form-specific arguments as arguments to the {} section'.format(self.__name__))\n\n\tif args.get('help'):\n\t\tself.help_msg"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tfor choice in self.choices_for_value:\n\t\tif choice in args:\n\t\t\treturn\n\t\n\tif self.choices_for_value:\n\t\tself.choices_for_value.remove(self.choices)\n\tself.choices_for_value.append(args)\n\n/**\n * Marlin 3D Printer Firmware\n * Copyright (C) 2016 MarlinFirmware [https://github.com/MarlinFirmware/Marlin]\n *\n * Based on Sprinter and grbl.\n * Copyright (C) 2011 Camiel Gubbels / Erik van der Zalm"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tchoices = [\n\t\t'default', \n\t\t'yes', \n\t\t'no', \n\t\t'cancel', \n\t\t'double', \n\t\t'transparent', \n\t\t'minimal', \n\t\t'all',\n\t]\n\ttry:\n\t\tvalue = choice(args.items())\n\texcept ValueError:\n\t\tprint('Usage: {} <value>'.format(self.prog))\n\t\texit(2)\n\t\n\tif value not in choices:\n\t\traise Exception('Not a valid choice')\n\telif value == 'default':"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif ('all' in args or 'none' in args):\n\t\treturn True\n\n\tcurrent = getattr(args, 'choice_value', None)\n\t\n\tif current is None:\n\t\targs_choices = set()\n\t\textra_choices = {}\n\t\t\n\t\targs_choices.add('all')\n\t\targs_choices.add('none')\n\t\tfor i in getattr(args, 'choices', []):\n\t\t\tif i not in extra_choices:\n\t\t\t\targs_choices.add(i)\n\t\treturn False if not extra_"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tchoices = args.get_choice_args()\n\tchoices = [choice['value'] for choice in choices]\n\n\tdef list_validation(choice, allowed):\n\t\tkey = str(choice['value'])\n\t\tif key not in allowed:\n\t\t\tself.logger.error('List error:'+ key)\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\n\tallowed = set(choices)\n\tvalid = ['no', 'yes']\n\n\tif list_validation(args.get_choice_arg(), allowed):\n\t\treturn"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tchoices = {\n\t\t'saveasfile':\t\t'Save as %s?' % (os.path.sep.join([('Open file', 'S%s' % i)}) if i > 0 else '')\n\t}\n\n\treturn args.option_type in choices\n\t\n\t\ndef validate_file_type(self, args):\n\t\"\"\"\n\tCheck if file type is a certain type.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tfile_type = ''\n\tfor i in ['s8','s16','s32',"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\treturn [] if args[2] and args[3] in self._choices:\n\t\t# The option was already selected\n\t\tif args[0] == args[2]:\n\t\t\treturn args[3]\n\t\t# The option may not be selected \n\t\telse:\n\t\t\treturn []\n\t\t\n\ndef validate_bool_args(self, args):\n\t\"\"\"\n    Check if all of bool arguments are valid.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t# Ensure there are at least 1 choice\n\tif len(args)"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif args.command == \"get_choice\":\n\n\t\ttry:\n\t\t\tsel = args.args[0].data\n\t\texcept:\n\t\t\tif self.parent.is_hidden:\n\t\t\t\treturn  # Can't pass this command when the parent is hidden, so ignore it.\n\t\telse:\n\t\t\ttry :\n\t\t\t\tchoice = args.args[1].data\n\t\t\texcept:\n\t\t\t\treturn  # Can't pass this command when the choice is not provided.\n\n\telif args.command == \"choice_help\":\n\t\tsel"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tfor arg in args:\n\t\tif arg == 'choice':\n\t\t\tchoice = args['choice']\n\t\t\tif choice in _VALID_CHOICES and choice not in ['yes', 'y', 'true', 't']:\n\t\t\t\traise GloVeValidationError('Choice must be one of the following:'+ _VALID_CHOICES)\n\t\n\ndef _validate_argument(opt, argument):\n\t\"\"\"\n\tValidates an argument of an Option, by checking that all\n\targuments are of type GloVeOptType\n\t\n\t:param opt: The option that we are"}]}
{"task_id": "6306092e73426c38ae68ad0b", "prompt": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tfor k, v in args.items():\n\t\tif not v >= min(min(v, 1), 2) or not v <= max(max(v, 1), 2):\n\t\t\traise Exception('The arguments given by the user must be between'+\n\t\t\t\t\t\t  'minimum-maximum values, min <= max, min >= max')\n\nclass SetParameter(Script):\n\t\"\"\"\n\tSet parameter for this script.\n\tA script needs to have a parameter with some name.\n\tA script should return the value to pass to the user.\n\t\n\t::\n\n\t"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tmin_val = float(args.get('i', False))\n\tmax_val = float(args.get('j', False))\n\t\n\tif min_val <= max_val:\n\t\treturn True\n\telse:\n\t\tprint(\"Argument range exceeded the upper upper limit ({:0.2f} - {:0.2f})\".format(min_val, max_val))\n\t\treturn False\n\t\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif( args.get(\"min\") is None and args.get(\"max\").is_string ):\n\t\targs.fatal('min/max must be integer, string, or None' )\n\tif( args.get(\"min\") is not None and args.get(\"max\").is_string ):\n\t\tif( args.get(\"min_inclusive\") is None and args.get(\"max_inclusive\").is_int ):\n\t\t\targs.fatal('min_inclusive and max_inclusive can not be defined at the same time' )\n\t\tif( args.get(\""}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tself._check_not_empty(args, \"min-max args\")\n\tself._check_min_max_value(args, \"min-max args\")\n\t\n\tself.min = args.get(\"min\", float(\"Inf\"))\n\tself.max = args.get(\"max\", float(\"-Inf\"))\n\t\n\tself.min_, self.max_ = list(self.min), list(self.max)\n\tself.min_ = self.min if self.min_ < self.min_:\n\t\tself.min_, self.max_ = self.min"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif args:\n\t\tif args[0][0].isdigit():\n\t\t\tval = args[0][0]\n\n\t\telif args[0]['type'] == 'Float':\n\t\t\tval = args[0]['val']\n\n\t\telif args[0]['type'] == 'Int':\n\t\t\tval = int(args[0]['val'])\n\telse:\n\n\t\tif len(self.args) == 1 and self._valid_number:\n\t\t\tval = args[0]\n\t\telif len(self.args) == 1:\n\t\t"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif args.val is None:\n\t\traise Exception(\"Argument missing value\")\n\n\tif not all([args.min_type is None, int(args.min_type) is None]):\n\t\traise Exception(\"Invalid argument type for min\")\n\n\tif not all([args.max_type is None, int(args.max_type) is None]):\n\t\traise Exception(\"Invalid argument type for max\")\n\n\tif args.min_type is not None:\n\t\tif args.min_type!= 'integer':\n\t\t\traise Exception(\"Invalid argument type"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif args.arg_minval is None:\n\t\tprint ('Min value is not given because it is mandatory.')\n\t\tprint ('' if args.arg_minval == 0 else 'Min value is set to'+ str(args.arg_minval).rjust(5,'') +'s')\n\tif args.arg_maxval is None:\n\t\tprint ('Max value is not given because it is mandatory.')\n\t\tprint ('' if args.arg_maxval == 0 else 'Max value is set to'+ str(args.arg_maxval"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\trequired_kwargs = set()\n\tmin_required_kwargs = set()\n\tfor arg in args:\n\t\trequired_kwargs |= arg_args(arg).required_kwargs\n\t\tmin_required_kwargs |= arg_args(arg).min_required_kwargs\n\tif required_kwargs!= min_required_kwargs:\n\t\traise ValueError('Only {} is required. Received {}.'.format(\n\t\t\trequired_kwargs, min_required_kwargs))\n\npackage io.cucumber.core.runtime;\n\nimport java.nio.charset.StandardCharsets;\n\n"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tfor arg_name in args:\n\t\tfor arg_type in (float, str):\n\t\t\tif not arg_type.is_possible_value(args[arg_name]):\n\t\t\t\tself.error(f\"The argument: {args[arg_name]} of {arg_name} is not between the minimum and maximum of: {arg_type}.\")\n\t\t\tif arg_type == float:\n\t\t\t\tcontinue\n\t\t\tif arg_name =='min' and'min' not in args:\n\t\t\t\tself.error(f\"The argument: min has"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tassert(self.valid_min_max_args_args(args))\n\ndef validate_arg(self, arg):\n\t\"\"\"\n\tCheck if item with the given argument exists in the given range.\n\t\"\"\"\n\tassert(isinstance(arg, str))\n\tassert(len(arg)>=7)\n\tassert(arg[3] in ['0', '1', '2', '3'])\n\ndef validate_arg_set(self, arg_set):\n\t\"\"\"\n\tCheck if item with the given argument is set for the given range.\n\t\"\"\"\n\t"}]}
{"task_id": "6306092e73426c38ae68ad0d", "prompt": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tif subcommand not in self.__class__._option_type_list:\n\t\tself._print(__import__('pkg_resources').__name__, \"ERROR\",\n\t\t\t\"Unknown subcommand %s\" % subcommand)\n\t\treturn None\n\treturn self.__class__._sub_command(subcommand, self._print,\n\t\toption_name, spec_option)\n\ndef build(_):\n\tfor module, spec in __all__.items():\n\t\tclass_ = module.replace('.', '')\n\t\tfor option, param_name in spec:\n\t\t"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t)\n\treturn self._parser.create_complex_argumet_type(subcommand, type_name,\n\"\"\"\n\tConstruct a complex type instance\n\t:param subcommand: the command\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: the parent option's specifications\n\t\"\"\"\n\t, option_name,spec_option\n)\n\tfrom galaxy import util\n\tresult = None\n\ttry:\n\t\tresult = util.get_type(subcommand, type_name)"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tif subcommand == 'argumet':\n\t\tself.add_argumet_type(type_name)\n\t\tself.add_option_argumet(option_name, spec_option)\n\t\treturn self.get_complex_type(subcommand)\n\n\traise NotImplementedError(\"%s is not implemented\" % self.name)\n\n\nclass create_cmd_generator(cmd_generator.CmdGeneratorBase):\n\t\"\"\"\n\tCreates a command generator used for generating the CMD_* (not the 'cmd' part like).\n\t\"\"\"\n\n\tcommand_"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\toptions = list(self.options.values())\n\toptions = [x for x in options if x.startswith('complex_')]\n\n\t(options, type, type_value, type_default) = get_complex_type_defaults(\n\t\tsubcommand=subcommand,\n\t\ttype_name=type_name,\n\t\toption_name=option_name,\n\t\tspec_option=spec_option,\n\t\tcommand=self.command,\n\t\tname=type_name,\n\t\tdefaults=options,\n\t\terror=False)\n"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tcomplex_argumet_type_name = type_name[len(options.NAME_PREFIX):]\n\tif (self.option_list is None or subcommand not in self.option_list):\n\t\tprint(\"The argument type needs a subcommand.\")\n\t\tsys.exit(1)\n\tcommand_option   = self.command_list[subcommand]\n\tcommand_option.remove_option_type(complex_argumet_type_name)\n\tsubcommand_argumer = command_option.get_type_option(complex_arg"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\ttry:\n\t\tfor x in [\n\t\t\tname for name, value in self.COMPLEX_TYPES if value!= [] and x in value.keys()\n\t\t\tif name!= option_name\n\t\t]:\n\t\t\traise ValueError('invalid subcommand %s' % subcommand)\n\t\tresult = self.COMPLEX_TYPES[x][option_name]\n\texcept KeyError as e:\n\t\traise ValueError('complex option %s not valid in command %s'%(option_name,subcommand))\n\telif len(self.COMPLEX_TYPES[x]) == 1:"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tsubcommand = subcommands.get(subcommand, subcommand)\n\ttype = create_type(subcommand, type_name)\n\ttype = common_subcommands_type_check(\n\t\tself, type, subcommand, option_name, options=spec_option\n\t)\n\ttype = type_with_value(type, subcommand, None)\n\tspec_option = self._spec_option_for_subcommand(subcommand, option_name)\n\tcomplex_type = complex_types.COMPLEX_TYPE_PREFIX + self._value_for_complex"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tif self.__class__._config_flags[self.OPT_COMPLEX_ARGS_DEFAULT]:\n\t\topt = self._extract_complex_type_option(\"--default\", spec_option=option_name)\n\n\t\tif not opt:\n\t\t\tself._out.error(\"Error: please specify a global option.\\n\\n\" + self._format_args())\n\t\t\tself._exit(2)\n\n\tself._complex_argumet[opt.opt_def][type_name] = self._make_argument_type(type_name,\n\t\tcomplex"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tresult_type = None\n\tif option_name in self.subcommands[subcommand]['spec_options']:\n\t\tresult_type = self.subcommands[subcommand]['spec_options'][option_name]['value']\n\t\n\treturn getcomplex(result_type)\n\n# This is a list of all the valid subcommands and their command names.\n# If you create your own commands, make sure your subcommands use the\n# 'command_name' key to refer to the subcommand. This is used for\n# specifying the `Commander instance`.\n# For example, if"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\ttry:\n\t\ttype_definition, option = option_manager._load_command_option(subcommand, option_name)\n\texcept:\n\t\tpass\n\n\treturn complex_generator_manager.ComplexTypeManager.build(self, subcommand, type_name, option, spec_option)\n\ndef create_complex_union_type(self, command, subcommand, type_name, option_name, option_spec=None,\n        type_spec=None, is_union_type=None, options=None, **kwargs):"}]}
{"task_id": "6306092e73426c38ae68ad0f", "prompt": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tdef merge_args(args):\n\t\tfor var in ['control_args', 'nested_args', 'custom_args']:\n\t\t\td = getattr(var,'split', None)\n\t\t\tif args.get(var):\n\t\t\t\tif hasattr(args[var],'split'):\n\t\t\t\t\targs[var].extend(args[var].split(';') + d)\n\t\t\t\telse:\n\t\t\t\t\targs[var] = d + split_comma_plus(args[var])\n\t\treturn args\n\t\n\tcontrol_args = merge_args(args)"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t# If no variables defined in the spec, we return empty dict.\n\tif not self.args or not self.args_set_up:\n\t\treturn {}, {}\n\t# If control_args = [], we are the control target,\n\t# we will use control_args to build the dict to return\n\tif 'control_args' in self.args:\n\t\tcontrol_args_keys = sorted(x.keys() for x in self.args['control_args'])\n\t\tcontrol_args = {}\n\t\tfor key in control_args_keys:\n\t\t"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t# Add the control arguments to the spec. The value here will be put\n\t# into the specification yml file\n\tcontrols = []\n\tcustom = {}\n\tif args:\n\t\tif 'control' in args:\n\t\t\tcontrols = args['control']\n\t\t\tcontinue\n\t\tif 'nested' in args:\n\t\t\tnested = args['nested']\n\t\t\tassert nested.count('spec') == 1, nested\n\t\t\tnested_spec = nested.pop('spec')\n\t\t\tassert nested_spec.count('spec') == 1, nested_spec\n\t\t"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\t\n\tfor arg in args:\n\t\tif not isinstance(arg, dict):\n\t\t\tnested_args[arg] = arg\n\t\telse:\n\t\t\tif arg.get(\"args\") == {}:\n\t\t\t\tfor key, value in arg.get(\"kwargs\", {}).items():\n\t\t\t\t\tif \"args\" in key:\n\t\t\t\t\t\tnested_args[key] = {\"args\": [], \"kwargs\": value}\n\t\t\t\t\telse:\n\t\t\t\t\t\tnested_args[key]"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tbase = ['nested:control_args:custom_args', 'ansible:custom_args:custom_args']\n\tresult = {}\n\t\n\tfor path, subpath, key in zip(args, base, ['control_args', 'nested_args', 'custom_args']):\n\t\tsubpath += '_{}'.format(key)\n\t\tpath_obj = os.path.join(self.config.test_path, path)\n\t\tsubpath_obj = os.path.join(path_obj, subpath)\n\t\t\n\t\tif os."}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\toutargslist = [['control_args', 'on', '\"\"'], ['nested_args', 'on', ''] ]\n\n\tfor arg in args:\n\t\tin_val, in_list = self.split_arg(arg)\n\t\tin_list.insert(0, in_val)\n\n\t\ttry:\n\t\t\toutlist = get_args_to_map(in_list)\n\t\t\toutlist.pop(0)\n\t\t\toutlist.insert(0, '{}.{}'.format(*in_list))\n\n\t\t\tif len("}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\targs_dict = {}\n\tmeta_arg_map = {}\n\targ_map = {}\n\tself.logger.info(\"Perform Ansible playbook.get_nested_custom_and_control_args...\")\n\tfor arg in args:\n\t\tif isinstance(arg, dict) or isinstance(arg, list):\n\t\t\tfor item in arg: \n\t\t\t\tif isinstance(item, dict) or isinstance(item, list):\n\t\t\t\t\targ_map[item[\"name\"]] = item\n\t\t\t\telif isinstance(item, str):\n\t\t\t\t\targ_map["}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tfrom ansible.module_utils.basic import AnsibleModule\n\tfrom ansible_collections.ansible.playbook.plugins.module_utils.common.to_spec import (\n\t\t_split_control_args_spec_dict,\n\t)\n\t\n\t# We need to use `spec` as the key for our nested arguments and \n\t# `control_args` as the value for our control arguments\n\tnested_args = self.params[\"args\"].get(\"nested\", {}) \n\tcontrol_args = self.params[\"args\"].get(\"control\", {}) \n\n"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\targs_flattened = flatten(argssplit(args, ['=']))\n\n\tcontrols = argssplit(args, ['control'])\n\tcombined = argssplit(controls, value=True)\n\textra_controls = [value for value in combined if value]\n\t\n\tnested = argssplit(args, [':nested'])\n\tnested_args = argssplit(nested, value=True)\n\n\tcontrols = argssplit(controls, ':')\n\tcontrols = args_flattened + [':' + control if len(arg) > 0 else control + ':' +"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tif not args:\n\t\traise YamlConfigError(\"Unable to read list of args: {}\".format(self))\n\tcontrol_args = args[0]\n\tnested_args = args[1]\n\t# create a temporary dict to store information about the args.\n\tcontrol_args_dict = {\"ansible_playbook_dir\": args[1].get(\"ansible_playbook_dir\")]}\n\tnested_args_dict = {}\n\t\n\t# process the nested arguments first\n\t# control_args will be passed in to the Ansible playbooks via command"}]}
{"task_id": "6306092e73426c38ae68ad11", "prompt": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\treturn vars(vars_dict)\n\ndef is_func_or_method(val, name):\n\ttry:\n\t\tfunc = val.__name__\n\texcept AttributeError:\n\t\treturn False\n\tif func == name:\n\t\treturn True\n\treturn False\n\ndef get_func_or_method(name):\n\ttry:\n\t\treturn globals()[name].__func__\n\texcept KeyError:\n\t\treturn globals()[name].__name__\n\n\ndef is_generator_function(val):\n\ttry:\n\t\treturn next(generator"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t# if no extra_vars exist: just add them\n\tif extra_vars is None:\n\t\treturn vars_dict\n\telse:\n\t\t# convert vars list to OrderedDict, then fill in in list of vars\n\t\tvars = OrderedDict(*vars_dict)\n\t\t# add those vars that should not be merged\n\t\textra_vars.extend(vars)\n\t\treturn vars\n\ndef read_version_file(filename):\n\t\"\"\" Reads version file to store version\"\"\"\n\twith open(filename, 'r') as fh:\n\t\tv_string ="}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tfor key, var in vars_dict.items():\n\t\tif key in extra_vars:\n\t\t\tvars_dict[key] += ',' + extra_vars[key]\n\t\t\n\t\telif key in variables.__all__:\n\t\t\tvars_dict[key] = extra_vars[key]\n\n\treturn vars_dict\n\ndef has_nonlocal_vars():\n\treturn has_nonlocal_params()\ndef has_nonlocal_params():\n\treturn self.additional_params!= None\n\ndef save():\n\tif not self.is_temp_file"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tfor key in set(vars_dict.keys()) - set(extra_vars):\n\t\tvars_dict[key] = vars_dict[key] + extra_vars[key]\n\n\treturn vars_dict\n\n\nclass FasterEncoder:\n\t\"\"\"\n\tA generic interface for the different models to the encoder\n\t\"\"\"\n\n\tdef __init__(self, args):\n\t\tself.args = args\n\t\t# initialize data structures\n\t\tself.embeddings = [self._initialize_variable(x['word_idx']) for x in args.pretrained_embeddings"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is not None:\n\t\tvars_dict.update(extra_vars)\n\ndef merge_params(variables=None, params=None):\n\t\"\"\"\n\tMerge all the parameters in ``variables`` and ``params`` into\n\ta single dictionary that is produced by ``param()``\n\t\n\t:param variables: Optional dictionary of variables to be merged into\n\t:param params: Optional dictionary of parameters to be combined into the\n\t:return: Variables and parameters merged together into a single dict/dictionary\n\t\"\"\"\n\tvariables = variables or []\n\tparams = params"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is None:\n\t\treturn vars_dict\n\tif isinstance(extra_vars, list):\n\t\textra_vars += vars_dict\n\telse:\n\t\textra_vars_copy = copy(extra_vars)\n\t\textra_vars_copy.update(vars_dict)\n\t\tvars_dict = extra_vars_copy\n\tnew_vars_dict = {key: value for key, value in vars_dict.items() if key not in extra_vars}\n\t# Update extra_vars\n\tif isinstance(extra_vars,"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif not extra_vars:\n\t\textra_vars = vars_dict.keys()\n\tnew_dict = {}\n\tfor k in extra_vars:\n\t\tnew_dict[k] = vars_dict[k]\n\tvars_dict = new_dict\n\t\t\n\treturn vars_dict\n\n/*  Copyright (C) 2019-2021 Daniel Dakhno\n\n    This file is part of Gadgetbridge.\n\n    Gadgetbridge is free software: you can redistribute it and/or modify\n    it under the terms of the GNU"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tfor name in vars_dict:\n\t\tif name in extra_vars: continue\n\t\tvars_dict[name] = getattr(vars_dict[name], 'attr', vars_dict[name])\n\n\treturn vars_dict\n\n\ndef get_default_settings_config(app_obj):\n\t\"\"\"\n\tGet the ``settings.conf`` file\n\t\n\t:param app_obj: Application object\n\t\"\"\"\n\tif config.get('settings', 'conf_file_path') is not None:\n\t\treturn config.get('settings', 'conf_"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tfor name, val in vars_dict.items():\n\t\tif type(val) is dict:\n\t\t\tfor var_name in vars_dict:\n\t\t\t\tif hasattr(val, var_name):\n\t\t\t\t\t# Get value by variable name.\n\t\t\t\t\tvalue = val.get(var_name, \"\")\n\t\t\t\t\tvar_val = vars_dict[var_name]\n\t\t\t\t\t# Make sure it isn't a valid JSON.\n\t\t\t\t\tif not isinstance(value, str) and not value:\t\t\t\n\t\t\t\t\t\tvalue = value.encode()\n\t\t\t\telse:\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tglobal __config__, __log__, __cache__, __main__\n\tglobal __version__, __config__, __log__, __cache__, __main__, __extra_vars__, __vars__, __meta__, __cache_vars__, __config__\n\tif __version__:\n\t\tvars_dict = __vars__\n\tfor var in vars_dict.keys():\n\t\tif var.startswith('__') or not var.endswith('.py'):\n\t\t\tcontinue\n\t\tif var not in __extra_vars__:\n\t\t\tif __main__: __extra_vars__[var] = vars_"}]}
{"task_id": "6306092f73426c38ae68ad13", "prompt": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t# TODO should the playbook be wrapped with ansible-playbook?\n\tif playbook_path is None:\n\t\traise Exception(\"No playbook_path given!\")\n\tif not os.path.isabs(playbook_path):\n\t\tplaybook_path = os.path.dirname(os.path.abspath(playbook_path))\n\tif not os.path.isfile(playbook_path):\n\t\traise Exception(\"Cannot find the playbook at: %s\"\n\t\t                %"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\tverbose = verbose or True\n\n\tif os.path.isdir(playbook_path):\n\t\tmsg = 'Using \"{}\" as play target'.format(playbook_path)\n\t\tif ansible_debug:\n\t\t\tprint msg\n\t\treturn playbook_path, ansible_playbook.main(ir_plugin, playbook_path,\n\t\t\tverbose=verbose, extra_vars=extra_vars,\n\t\t\tansible_args=ansible_args)\n\telse:\n\t\tmsg = 'Running \"{}\" as play target'.format("}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\tpass\n\n//===----------------------------------------------------------------------===//\n//\n//                     The LLVM Compiler Infrastructure\n//\n// This file is dual licensed under the MIT and the University of Illinois Open\n// Source Licenses. See LICENSE.TXT for details.\n//\n//===----------------------------------------------------------------------===//\n\n// UNSUPPORTED: c++98, c++03\n\n// <experimental/coroutine>\n\n// void wait();\n\n#include <experimental/coroutine>\n#include <type_traits>\n#include <exception>\n#include <cassert"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\tif not os.path.isdir(playbook_path):\n\t\traise CLIError('Invalid directory specified, directory should '\n\t\t               'already be under %s' % os.getcwd())\n\n\twork_dir = os.path.realpath(ir_workspace)\n\tlogger.info(\"Working directory: %s\", work_dir)\n\n\tlog = ir_plugin.get_log()\n\tlogger.debug(f\"Working directory is: {log.logdir}\")\n\tpath = os.path.join"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t# get the path of the plugin and playbook (if available)\n\tplaybook_file_stem = None\n\tif os.path.isabs(playbook_path):\n\t\tir_workspace.root = playbook_path\n\t\tplaybook = os.path.abspath(playbook_path)\n\t\tplaybook_file_stem = os.path.basename(playbook)\n\telse:\n\t\ttry:\n\t\t\tplaybook_file = os.path.normpath(playbook_path)\n\t\texcept os.error:\n\t\t\t"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\tansid=None\n\targs=None\n\tcmd=None\n\targswitches=[]\n\t\n\tif extra_vars['ansible_playbook_argument_overrides'].has_key('playbook'):\n\t\tplaybook_path= extra_vars['ansible_playbook_argument_overrides'].get('playbook').replace('/', '\\\\')\n\t\n\tif playbook_path in extra_vars['ansible_playbook_arguments'].keys():\n\t\t# Use the path to the playbook (which could be in a config file) as the argument\n\t\targs=extra"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\tansible_args = dict()\n\t\n\tansible_args.update(ir_workspace.plugin_args())\n\tansible_args.update(ir_workspace.run_args())\n\tansible_args.update(ir_workspace.extra_vars())\n\tansible_args.update(ir_workspace.run_env)\n\tansible_args.update(ir_workspace.ansible_args())\n\t\n\tif verbose is None:\n\t\tverbose = ir_workspace.api.ansible.verbose\n\t\t\n\t\t\n\tif verbose:\n\t\tlog"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\twith open(playbook_path, 'r') as f:\n\t\tir_plugin.register_extra_args(f, extra_vars=ansible_args)\n\n\t\tplaybook_object = Playbook(ir_workspace, playbook_path, verbose=verbose)\n\t\tplaybook_object.invoke()\n\n/*\n * Copyright (C) 2012 Open Source Robotics Foundation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\timport os\n\t\n\t# Setup InfraredPlugin object\n\tplugin = InfraredPlugin(ir_plugin=ir_plugin,\n\t\tbase_plugin=ir_workspace.get_base_plugin(),\n\t\tplaybook_path=playbook_path,\n\t)\n\t\n\t# Setup InfraredPlugin as a wrapper\n\twrapper = AnsibleCommandline(plugin, playbook_type='ansible-playbook',\n\t\tverbose=verbose, extra_vars=extra_vars, ansible_playbook=True)\n\t\n\t# Call Ansible"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t)\n\traise RuntimeError(f\"error running ansible-playbook: {playbook_path} - should be removed\")\n\n\ndef ansible_playbook_run(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:"}]}
{"task_id": "6306093273426c38ae68ad15", "prompt": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t# vars_dict must be ordered\n\tvars_dict = OrderedDict([(\"ansible_run\", True)])\n\t# Get active workspace\n\tif cli_args is not None:\n\t\tworkspace = ir_workspace.get_workspace(cli_args[0])\n\telse:\n\t\tworkspace = ir_workspace.get_workspace(\"unknown_ir\")\n\t# get a list of all commands to run\n\targs = []\n\tfor i in range(2):\n\t\tif workspace is not None:\n\t\t\targs.append(workspace.get_command_by_"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tansible_args_list = []\n\ttry:\n\t\tos_list, cmd_lists = get_ansiblelist(vars_dict)\n\t\tcmd_list = cmd_lists\n\texcept Exception as e:\n\t\tprint(e)\n\n\tfor os in os_list:\n\t\tif os in cmd_list:\n\t\t\tansible_command = cmd_list.split('/')[-1]\n\t\t\tansible_args_list.append(vars_dict.get(ansible_command))\n\t\tprint('Running playbook command {}'.format"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\targs = vars_dict.get('_vars_')\n\tif args is None:\n\t\targs = []\n\tvars = vars_dict.get('ansible_vars')\n\tif isinstance(ir_workspace, tuple):\n\t\tworkspace = ir_workspace[0]\n\telse:\n\t\tworkspace = ir_workspace\n\tvariables = []\n\tfor var in vars:\n\t\tlog.debug(\"Adding extra var %s\", var)\n\t\tif var not in workspace.var_list:\n\t\t\tworkspace.var_list[var] = { }\n"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t# create ansibility and run it\n\tvars = vars_dict\n\tansible_dict = vars.get('ansible_dict') or {}\n\tansible_vars = vars.get('ansible_vars') or {}\n\tif ansible_vars!= None and ansible_dict!= None:\n\t\tansible_vars.update(ansible_dict)\n\tconfig = vars.get('ansible_config', None) or {}\n\tplugin = vars.get('ansible_plugin', None) or {}\n\tconfig[\"plugin\"] = plugin\n\tif vars"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tir_workspace = ir_workspace or InfraredWorkspace()\n\tir_plugin = ir_plugin or InfraredPlugin()\n\tlogger.info(\"--- Running playbook. \"\n\t            \"Current workspace: %s -- Workspace: %s -----\"\n\t            % (os.getcwd(), ir_workspace.get_current_workspace_path()))\n\n\tplaybook_args = cli_args[0]\n\tvars_as_json = cli_args[-1]\n\tvars_as"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tfrom ansible.cli import CliHandler\n\treturn_results, cli_logger = CliHandler.init_ansible_cli_handler(cli_args)\n\tif cli_logger.getEffectiveLevel()!= logging.INFO:\n\t\tcli_logger.setLevel(logging.INFO)\n\n\t# print out debug information\n\tif logger is None:\n\t\tlogger_logger = logging.getLogger('ansible')\n\t\tlogger_logger.setLevel(logging.DEBUG)\n\n\n\t# print the command line and arguments\n\n\t# get a dictionary of"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tvars_dict = vars_dict[\"ansible_extra_vars\"]\n\trun_playbook(cli_args, vars_dict, ir_workspace, ir_plugin)\n\n\ndef run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\t:param cli_args: the list  of command line arguments\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param ir_workspace: An Infrared Workspace object represents the active workspace\n\t:param ir_plugin: An"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\treturn get_ansible_cli_result(_run_ansible_playbook(cli_args, vars_dict, ir_workspace, ir_plugin))\n\n\ndef get_ansible_cli_result(cli_func):\n\t\"\"\"\n\tReturn the results of an cli func using a cli_args dict\n\t\n\t:param cli_func: The function that will be run using cli_args\n\t\"\"\"\n\tdef cli_result(cli_args=None, *variables_dict, **extra_vars):\n\t\t\"\"\"\n\t\tA utility function that returns ansible"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tpasslib.logger.debug(\"Starting Playbook for CLI\")\n\t\n\tansible_vars = vars_dict.get(\"ansible_vars\")\n\t# Convert variables to string and add '=' if any are present\n\tansible_vars = ''.join(ansible_vars.split('=')) \\\n\t               +'[' + '=' +']' \n\t\n\t# Run playbook\n\tansible_action = \"ansible playbook --vars={} ansible_vars {} \\\n        "}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tparser = vars_dict[\"parser\"]\n\tvar_manager_name = vars_dict[\"var_manager_name\"]\n\tvars_manager_names = vars_dict[\"vars_manager_names\"]\n\tvars_manager_vars = vars_dict[\"vars_manager_vars\"]\n\tansible_vars_manager = _get_ansible_vars_manager(vars_manager_name, vars_manager_vars, var_manager_names)\n\n\t# setup default settings for infractor\n\tdefault_settings = vars_dict[\"settings\"].get(\"ansible_plugins"}]}
{"task_id": "63060ada73426c38ae68ad31", "prompt": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t# pylint: disable=too-many-lines\n\t# pylint: disable=too-many-locals\n\tvalues_dict['parser_module'] = parser_name\n\tvalues_dict = dict(values_dict)\n\t\n\t# Remove arguments that start with --\n\t# the command name can be made at least as long as the rest\n\tvalues_dict = list(values_dict.items())\n\t\n\t# pylint: disable=too-many-locals\n\t# pylint: disable=too-many-locals\n\t# pylint: disable=too-many-statements\n\tvalues"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tfor argument in values_dict.values():\n\t\t# the argument is a list... go to next\n\t\tif isinstance(argument, list):\n\t\t\t# go to next argument if possible,\n\t\t\t# otherwise we know if it's a single value or the same\n\t\t\t# format as the parser_name\n\t\t\tif len(parser_name + arguments) > 1:\n\t\t\t\treturn argument\n\t\t\treturn self.convert_arg_list_to_type(argument, arguments)\n\telse:\n\t\tprint(f'Command: {parser_name} has"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tvalues_dict.setdefault('description', '[Virshur] %s - %s' % (self.name, parser_name))\n\tvalues_dict.setdefault('category', self.name)\n\tvalues_dict.setdefault('subcommand', 'convert')\n\ndef convert(args):\n\t\"\"\"\n\tConvert is a command which performs a conversion\n\tof one of the available source files, into another.\n\t\n\t:param args: Positional arguments\n\t\"\"\"\n\n\t# print(args)\n\tparser_name = args.get('parser', 'python')\n"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tparser_opts = dict()\n\tfor key, val in values_dict.items():\n\t\tif not hasattr(val, \"lower\"):\n\t\t\tval = val.lower()\n\n\t\tparser_opts[key.replace('main.', '')] = val\n\n\treturn parser_name, parser_opts\n\n//! This file was generated automatically by the Snowball to C compiler\n\n#pragma warning disable 0169\nnamespace std {}\nusing std::abs;\nusing std::rand;\nusing std::srand;\nusing std::size_t;\nusing std::"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tvalues_dict.update(parser_name,values_dict)\n\n\n# __main__.py functions...\n\nif __name__ == '__main__':\n    parser, values_dict = _convert_cli_args()\n    _convert_non_cli_args(parser, values_dict)\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache license, Version 2."}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tfor a in self.args:\n\t\tcommand, name = a[1].lower(), a[0]\n\t\targs_val = values_dict.get(name)\n\t\t\n\t\tif args_val is None:\n\t\t\targs_val = u''\n\t\t\n\t\tfor a in self.args:\n\t\t\tif not a[1].lower() in value_set:\n\t\t\t\tlogger.debug(\"No value for: %s\", a[1])\n\t\t\t\n\t\t\targs_val = args_val + a[2]\n\t\t\t\n\t\tvalues_dict["}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tparser_name = self._remove_quotes_and_whitespaces(parser_name)\n\t\t\n\tparser_class, parser_method = self._get_parser_class(parser_name)\n\tif not parser_class:\n\t\t# If the parser_name did not match a known type, then it should be a dict.\n\t\t# No such parser exists...\n\t\traise self.usage.unknown_parser_error\n\tif not hasattr(parser_class, parser_method):\n\t\t# If the parser_method does not exist.\n\t\traise self."}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tfor key, value in values_dict.items():\n\t\tif value is None:\n\t\t\tvalues_dict[key] = False\n\t\telif value in [True, \"true\", \"True\"]:\n\t\t\tvalues_dict[key] = True\n\t\telif value in [False, \"false\", \"False\"]:\n\t\t\tvalues_dict[key] = False\n\t\telse:\n\t\t\tvalues_dict[key] = str(value)\n\t\n\tif type(values_dict) == dict:\n\t\tfor k, v in values_dict.items():"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tassert isinstance(parser_name, str)\n\tassert isinstance(values_dict, dict)\n\tfor v in values_dict:\n\t\tassert isinstance(v, (str, unicode))\n\t\tif isinstance(values_dict[v], (str, unicode)):\n\t\t\tvalues_dict[v] = values_dict[v].replace('#', '')\n\t\n\tresult = dict([(k, v) for k,v in values_dict.items()])\n\treturn result\n\ndef _get_argument(self, parser_name, args_dict, default=None):"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tparser_name = self.get_parser_name(parser_name)\n\tfor val_type, val_name in self.value_types[parser_name]:\n\t\t\n\t\t# Check if it is allowed to be converted to given type\n\t\ttry:\n\t\t\tval = eval(val_name)\n\t\texcept:\n\t\t\tpass\n\t\telse:\n\t\t\t\n\t\t\t# Convert val[v] into the appropriate type\n\t\t\ttry:\n\t\t\t\tval = self._convert_to(val, val_type)\n\t\t\texcept KeyError:\n\t\t\t\t"}]}
{"task_id": "63060b1a73426c38ae68ad3e", "prompt": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tplugin_dir = os.path.realpath(plugin_dir)\n\tfor property in spec_properties:\n\t\tsub_dir = plugin_dir + os.sep + property\n\t\tif os.path.isdir(sub_dir):\n\t\t\treturn get_plugin_spec_flatten_dict(sub_dir)\n\treturn {}\n\ndef get_plugin_spec_flattened_data(plugin_dir, sub_dir, prop_name):\n\t\"\"\"\n\t\tGiven a flat dictionary from the plugin spec, returns the flattened data.\n\t\n\t\t:param"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t# Load the plugin yaml and get the plugin info\n\tplugin_yaml = pygame.yaml.load(plugin_dir)\n\tinfo = plugin_yaml['spec']['info']\n\n\t# Get a unique key\n\tkey = info['labels'].get('alias')\n\n\t# Extract the labels\n\tlabels = info['labels']\n\n\tresult = {}\n\tif labels:\n\t\treturn {key: get_labels(labels)}\n\treturn result\n\n\ndef get_plugin_spec_plugin_dir(plugin_dir):\n\t\"\"\"\n\tReturn the path"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tplugin_desc = plugin_dir.name\n\tplugin_dir_path = plugin_dir\n\t\n\tplugin_list = [entry for entry in plugin_desc.split('.') if entry not in _PLUGINS]\n\tplugin_dict = {}\n\t\n\tfor plugin in plugin_list:\n\t\tplugin_dict[str(plugin)] = [str(sub_plugin) for sub_plugin in plugin_list[plugin]]\n\n\treturn plugin_dict\n\n/*\n * Copyright 2020 OmniSci, Inc.\n *\n * Licensed under the Apache License, Version 2."}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\troot = Path(plugin_dir)\n\tflattened_dict = {}\n\t\t\n\tfor root, directories, files in os.walk(root):\n\t\tfor f in files:\n\t\t\tflattened_dict['/%s/%s' % (root, f)] = os.path.join(root, f)\n\treturn flattened_dict\n\n\n\ndef get_plugin_spec_hash(plugin_hash=False):\n\tif plugin_hash:\n\t\treturn plugin_hash\n\telse:\n\t\treturn hashlib.sha256(open('spec.py"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tout = {}\n\tfor key, spec in plugin_dir.items():\n\t\tout[key] = flatten_spec(spec)\n\treturn out\n\n\ndef get_plugin_spec_default(default_path):\n\t\"\"\"\n\t\tExtracts the default plugin spec from the plugin dir\n\t\tIf the path is to a plugin file, it returns None\n\t\tOtherwise the returned spec as a dictionary contains the properties\n\t\"\"\"\n\tplugin_dir = get_folder(default_path)\n\treturn get_plugin_spec_flatten_dict(plugin_dir"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tplugin_spec = {}\n\tfor k,v in sorted(parse_plugin_spec(plugin_dir).items()):\n\t\tplugin_spec[k.split(\":\")[1]] = v\n\treturn plugin_spec\n\ndef get_plugin_source_config_list(plugin_dir):\n\t\"\"\"\n\tConstruct a list of plugin sources. Will be used for the generation of the\n\tprofile.\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A list of list of profile names in order added by default\n\t\"\"\"\n\tall_"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t# plugins dir should be a path to the plugin's dir\n\tplugins_dir = os.path.abspath(os.path.join(plugin_dir, \"plugins\"))\n\tp =  plugins_dir + \"/\" + os.path.join(\"plugin\", \"plugin.json\")\n\t# p should be a path to the plugin's dir\n\tfiles = os.listdir(p)\n\tif len(files) < 9:\n\t\treturn {}\n\tplugin_file_dir = \", \".join(files[8:])\n\treturn _flatten_dict(\n\t\t\""}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tplugin_spec = os.path.join(plugin_dir, PLUGIN_SPEC)\n\treturn plugin_spec_helper.get_plugin_spec_flatten_dict(plugin_spec)\n\ndef get_module_spec_id(module_name, plugin_name):\n\t\"\"\"\n\tTries to extract the module_name from the plugin_id. If that fails,\n\tthe plugin ID is used to locate the module_name in case a valid\n\tplugin_name has been given\n\t\n\t:param module_name: The plugin's module name\n\t:param plugin_"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\ttmp = {}\n\t# Add the python plugin dir if required\n\tif plugin_dir is None:\n\t\tplugin_dir = os.path.dirname(os.path.dirname(os.path.dirname(sys.argv[0])))\n\tbasedict = os.path.join(plugin_dir, \"base.py\")\n\tdict = {}\n\twith open(str(basedict)) as f:\n\t\tfor line in f:\n\t\t\tif line.startswith(\"self.plugins\"):\n\t\t\t\ttmp[kld.utils.safe_name(line)] ="}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tplugin_path = plugin_dir + os.sep + \"plugin.yaml\"\n\tplugin_dir_path = os.path.join(plugin_path)\n\tspec_dir_path = os.path.join(plugin_path, \"config\")\n\t\n\twith open(plugin_path) as pluginyaml:\n\t\tpluginconfig = open(pluginyaml)\n\t\tpluginconfig_yaml = pluginconfig.read()\n\t\tpluginconfig_yaml = pluginconfig_yaml.splitlines()\n\t\tpluginconfig_yaml = \" \".join(pluginconfig_yaml)\n"}]}
{"task_id": "63060b1b73426c38ae68ad42", "prompt": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tself.config = EnvironmentConfig({})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n}\n\n\n#--Class function implementation --------------------------------\n\ndef load_config(settings, cfg_path=None, cfg_override=None):\n\t\"\"\"\n\tLoads a configuration into a dictionary/dictionary.\n\tIf any configuration was overridden via the environment key\n\t``config_path``, also overrides it.\n\t\"\"\"\n\tfrom. import configuration as config\n\t"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif self.data[CONF_ENV_PATH] is None:\n\t\tself.data[CONF_ENV_PATH] = ''\n\n\n\"\"\"\n\tHook into config.py.\n\n\tIf we're not using ConfigBuilder, then we need to check for\n\tthat the config is not set to the default, and call set.\n\n\tThe ConfigBuilder object is what we want to do for us: so\n\tdon't have to wait for ourself to set the default\n\t\"\"\"\n\tif not self.config_builder.__class__.is_a('ConfigBuilder'):\n"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif not self.c_config:\n\t\treturn\n\tenv_path = os.getenv(self.c_config)\n\tif env_path:\n\t\tenv_path = os.path.expanduser(env_path)\n\t\t# In case that the config file does not have a user specific prefix.\n\t\tself.c_config = env_path + self.c_config\n\tself.environ = dict(os.environ)\n\tself.environ.update(env_path)\n\n#-----------------------------------------------------------------------------\n# Public methods:\n#-----------------------------------------------------------------------------\n"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif self.config!= None and os.path.exists(self.config):\n\t\t# We've already set the config_file property, nothing to do.\n\t\treturn\n\n\t# Set the path\n\tself.config = os.path.abspath(os.path.expanduser(\n\t\tos.path.join(os.path.expanduser('~'),\n\t\tos.environ['HOME'] + '/.local/share/conf.py')))\n\n\ttry:\n\t\t# Get conf.py (it exists if there is a config file)\n\t"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif not os.path.exists(self.config_path):\n\t\tself.conf_file = os.path.abspath('/etc/mynetd.conf')\n\t\tif not os.path.exists(self.conf_file):\n\t\t\tself.config_file = '/etc/mynetd.conf'\n\t\t\tif not os.path.exists(self.config_file):\n\t\t\t\tself.config_file = '/etc/mynetd.conf.d/mynet.conf'\n\t\telse:\n\t\t\tself.config_file ="}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tself.config_path = get_config_path()\n\tif not self.config_path:\n\t\tif self.home is None:\n\t\t\tself.home = get_home()\n\t\telse:\n\t\t\tpathlib = pathlib.Path(self.home)\n\t\t\tself.home = pathlib.Path(self.config_path).resolve()\n\tself.config_dir = os.path.join(self.home, 'Library', 'Config')\n\treturn self.config_dir\n\n\ndef get_config_path(env_name"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tenv_var_config_path = os.environ['CONFIG_PATH']\n\tif env_var_config_path == '':\n\t\tenv_var_config_path = None\n\tif env_var_config_path:\n\t\tself.config_path = env_var_config_path\n\n\ttry:\n\t\tself.config_path = config_path()\n\texcept ValueError:\n\t\tpass\n\n\tif self.config_path:\n\t\tself.config = config.Config()\n\t\tself.config.open_section(self"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif os.environ.get('CONFIGPATH') is None:\n\t\tos.environ['CONFIGPATH'] = os.path.join(os.path.expanduser('~'), '.config')\n\treturn 1\n\ndef get_default_path(default_path):\n\t\"\"\"\n\t\tGet the default path to use, if specified, by the user during startup.\n\t\tThis is done by expanding $HOME. Most things go\n\t\tto the user's home directory.\n\n\t\tReturn None if the default path has not been set.\n\t\"\"\"\n\texpand_"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tconfig_file = os.path.join(current_dir, 'conf/conf.json')\n\tif os.path.isfile(config_file):\n\t\twith open(config_file) as fd:\n\t\t\twith open(os.path.join('~', os.path.abspath(config_file)), 'r') as fd2:\n\t\t\t\tconfig = json.load(fd2)\n\t\t\tif '.' not in config['local']:\n\t\t\t\tconfig['local'] = config['local'].replace('_', '/')\n\t\t\tif config['public_key']"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\ttry:\n\t\tif self.use_config_path is not False:\n\t\t\traise Exception(\"Configuration in use, \"\n\t\t\t                \"please specify the path, not config.\")\n\n\t\tself.use_config_path = self.config_path\n\texcept Exception, e:\n\t\traise Exception(str(e))\n\nclass Option(object):\n\t\"\"\"\n\tThe base class all option objects must implement.\n\n\tParameters\n\t   + name (+ optional: str): name of option; see"}]}
{"task_id": "63060b1b73426c38ae68ad43", "prompt": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tprint(\"\\nOptions:extend-cli(subparser):\")\n\tsubparsers = {u: self(root_subparsers) for u in sorted(self.entry_points)}\n\tsubparser_names = [name for name,obj in subparsers.items()]\n\toption_parser = make_option_parser(subparsers)\n\toption_parser.update(root_subparsers)\n\tfor name in option_parser.names:\n\t\tname = uncapitalize(name)\n\t\tmain_name = u\"opt \" + u'|'.join(subparsers"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparser = super(ExtendCli, self).subparser\n\t\n\tdef add_cli(self, subparsers, spec_root_dir):\n\t\tself.add_cli_arguments(subparsers, spec_root_dir)\n\t\t\n\t\t# get out parameters\n\t\targuments = spec_root_dir.get_cli_parser_args()\n\t\t\n\t\tif arguments.no_spec_file:\n\t\t\tlogger.critical('Parameter \"no_spec_file\" will be overwritten in the subparser with'+ self.cli_parser.cli_name)\n\t\t\n"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tdef _extend_cli_option(self, option, argument):\n\t\t\n\t\tfor cli_option in ALL_CLI_OPTIONS:\n\t\t\t\n\t\t\tif option == cli_option:\n\t\t\t\targs = argument\n\t\t\t\t\n\t\t\t\treturn parser.add_argument(*args)\n\n\t\treturn self._add_cli_subparser(option, argument)\n\t\t\n\tparser.add_argument(\"-r\", \"--root\", default=False, action=\"store_true\", help=\"Use root subparser\")\n\tparser.add_argument(\"-a\", \"--all\", default=False, action=\"store"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\troot_parser = root_subparsers.add_parser('cli')\n\troot_parser.add_argument(\n\t\t\"-c\", \"--config\", help=\"Path to the Cobbler config file of the Cobbler server.\", required=True)\n\troot_parser.add_argument(\n\t\t\"-i\", \"--info\",\n\t\thelp=\"Displays in <info> format about the Cobbler server and its \"\n\t\t\t\"capabilities.  The info format is used for version 1.x, \"\n\t\t\t\"versions 0.x, and older.  C"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\timport sys\n\timport getopt\n\timport os\n\timport glob, tempfile\t\t# make sure it's a function\n\t# for command line utilities like -C\n\tfrom collections import OrderedDict\n\t\n\tbase_args = []\n\tbase_opt = []\n\tbase_cmds = []\n\textra_args = []\n\textra_opts = []\n\tbase_help = []\n\t\n\tfor i in base_subparsers:\n\t\tbase_args += i.values()\n\t\t\n\tif is_windows():\n\t\tif sys.version_info < (3, 3"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tfor cli_opt in self.cli_options:\n\t\tkwargs = {}\n\t\tfor sub_opt in cli_opt:\n\t\t\tkwargs[sub_opt] = cli_opt[sub_opt]\n\t\tsubparser = root_subparsers.add_parser(\"cli_options\", **kwargs)\n\t\tsubparser.add_argument(\"-c\", \"--config\", required=True, help=\"spec yaml config file\")\n\t\tsubparser.add_argument('--output', required=True, help='spec yaml output file')\n\t\n\tdef parse_args(self):"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparsers = self._get_subparsers(root=True, required=[self.spec_key])\n\t\n\tsubparsers.append(self.spec_cli)\n\t\n\treturn subparsers\n\n\ndef extend_spec_cli(self, root_subparsers):\n\tsubparsers = self._get_subparsers(root=True, required=[self.spec_key])\n\t\n\tsubparsers.append(self.spec_cli)\n\t\n\treturn subparsers\n\n\ndef _get_subparsers(root=False, required=None, required_only=False, **"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsuper(Cli, self).extend_cli(root_subparsers)\n\tself._add_spec_cli(self._get_spec_common_cli(root_subparsers=False))\n\t\ndef add_cli(self):\n\tfrom.spec_cli import (validate_config, validate_client, validate_env, validate_args, validate_subcommands, validate_args_no_client, validate_args_no_env, validate_args_no_args)\n\tfor c in self.subparsers:\n\t\tc._orig_validate_args = getattr"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsub_args = root_subparsers.add_parser(\"spec\", help=HELP_TEXT,\n\t\tdest=os.getcwd())\n\tif self.default_subparser_name:\n\t\treturn sub_args\n\tsub_args.add_argument(\n\t\t\"--include\", dest=\"includes\",\n\t\taction=\"append\", type=str,\n\t\thelp=\"extra spec file(s) to include.\", metavar=\"FILE\",\n\t\tdefault=[],\n\t)\n\tsub_args.add_argument(\n\t\t\"--exclude\", dest=\"excludes\","}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tdesc = \"%(name)s - %(func)s\\n\\n\"\n\tdesc += \"Options for:\\n\\n\"\n\tdesc += \"  %(name)s(help_txt=False) - Specifies this is help for the %(name)s action.\\n\"\n\tdesc += \"  %(name)s(help_txt=self.help) - Specifies an advanced help string for the %(name)s action.\\n\"\n\tdesc += \"- %(name)s(description) - Specifies a longer description for the %(name)s action.\\n\"\n\tdesc += \"-"}]}
{"task_id": "62ece4982e6aefcf4aabbd64", "prompt": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif path == root:\n\t\te = RValueError('Invalid path; path cannot be the root directory.')\n\t\traise e\n\ttry:\n\t\troot.remove(path)\n\texcept OSError as e:\n\t\tif e.errno!= errno.ENOENT:\n\t\t\traise\n\t\treturn\n\traise KeyError('Path not found in root.')\n\ndef safe_glob(pattern):\n\t\"\"\"\n\tReturns true if glob matches pattern in a sequence of paths.\n\tReally just tests for matches with glob, and avoids the resulting\n\tobject creation unnecessarily"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif not isinstance(path, Path):\n\t\traise AttributeError(\"`path` should be `Path` instance\")\n\tif path.is_absolute() and len(path.resolve()) < 1:\n\t\ttry:\n\t\t\tPath(path).remove()\n\t\t\treturn\n\t\texcept AttributeError:\n\t\t\t# the path is not absolute and does not refer to an absolute path\n\t\t\t# but the path refers to a relative path not on the path itself\n\t\t\t# (e.g. 'path/to/file.txt' => 'path')\n\t\t\treturn\n"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif root:  # Can be empty string (root == path or root is root of zipfile)\n\t\tif sys.platform[:3] == 'win' and root.startswith('\\\\\\\\'):\n\t\t\troot = root[1:]\n\n\t\tpath = root + path\n\t\tif not os.path.exists(path):\n\t\t\traise RuntimeError(\"%s not found or path does not exist\" % path)\n\n\t\tif not path.startswith(root):\n\t\t\traise RuntimeError(\"path must starts with root of zipfile\")\n\n\t\tstart_index = 1\n\t\t"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif root:\n\t\tparts = path.split('/')\n\t\tif '.' not in path and parts and parts[-1] == '':\n\t\t\tparts.pop()\n\t\twhile parts and parts[-1] == '':\n\t\t\tparts.pop()\n\t\tif not parts:\n\t\t\traise KeyError(name)\n\t\treturn '/'.join(parts)\n\treturn path\n\ndef split_prefix(name):\n\t\"\"\"Split path from name and return prefix, raising IOError on failure\"\"\"\n\tname = strip_root(name, '')\n\tprefix ="}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tf = Path.Dir(path)\n\t_path = f.joinpath(root)\n\tf.removedir()\n\tif Path.Dir(f.joinpath('..')).is_file:\n\t\traise RuntimeError(\"Unable to remove {0:s}.\".format(path))\n\n\ndef to_bool(x):\n\tif not isinstance(x, bool):\n\t\traise RuntimeError(f\"{type(x).__name__!r} is not a bool\")\n\treturn x\n\n\ndef ensure_directory(path):\n\t\"\"\"\n\tMake path directory"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tpath = os.path.normpath(path)\n\tpath = os.path.join(path, root)\n\n\tif not os.path.isdir(path):\n\t\treturn path\n\n\ttry:\n\t\tif os.path.isdir(path):\n\t\t\tos.rmdir(path)\n\n\t\tif os.path.isfile(path):\n\t\t\tos.remove(path)\n\n\texcept WindowsError as e:\n\t\tif e.errno == 2:\n\t\t\traise NoSuchPath\n\n\texcept Exception as e:\n"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tpath = join(root, striproot(path))\n\tif not exists(path):\n\t\traise IOError('%r does not exist' % root)\n\tif has_magic(path):\n\t\tpath = get_magic(path)\n\t\tfor name in path.split('/'):\n\t\t\tpath = join(root, name)\n\t\treturn get_magic(path)\n\telse:\n\t\treturn path\n\ndef get_magic(path):\n\t\"\"\"\n\tReturn magic from any path (str or file-like path).\n\t\"\"\"\n\t"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tpath = path.expanduser().replace('\\\\','/')\n\tif not path.startswith(root) or not path.endswith(os.path.sep):\n\t\traise ValueError(\"path must start with %r and not end with %r\" % (root, path))\n\tif six.PY2:\n\t\treturn path.replace(os.extsep, '/')\n\telse:\n\t\treturn path.replace('//', '/')\n\nclass RemovableDevice(object):\n\n\tdef __init__(self, path, remove_device=True):\n\t\tif six."}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif not path:\n\t\treturn\n\troot = os.path.abspath(root)\n\tif '.' not in root:\n\t\traise Exception(\"root '%s' must be an absolute path, not root of package\" % root)\n\tif path == root:\n\t\treturn ''\n\n\tos.makedirs(os.path.dirname(path), exist_ok=True) # create parent directory\n\tfor e in os.listdir(path):\n\t\ts = os.path.join(path, e)\n\t\ttry:\n\t\t\tif os"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\trel_root_path = os.path.relto(path)\n\trel_root = os.path.join(rel_root_path, root)\n\tif os.path.isdir(rel_root):\n\t\treturn\n\tif os.name == 'nt':\n\t\tif os.path.commonprefix ([rel_root_path, '']) == [':', '..', '']:\n\t\t\traise EnvironmentError(\"Path '%s' is not absolute and cannot be a root directory\" % (os.path.abspath(path)))\n\t\treturn\n\ttry"}]}
