{"task_id": "62e60f43d76274f8a4026e28", "prompt": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\t# handle python version\n\t\tif not isinstance(nanoseconds, Time):\n\t\t\ttry:\n\t\t\t\tnanoseconds = timedelta(seconds=nanoseconds)\n\t\t\texcept (OverflowError, ValueError):\n\t\t\t\tnanoseconds = timedelta(seconds=0)\n\n\t\t\tif isinstance(tz, Time):\n\t\t\t\tnanoseconds = tz - nanoseconds\n\n\tseconds = int(nanoseconds.total_seconds())\n\tmicroseconds = int(nanoseconds.total_microseconds())\n\tuseconds = nanoseconds.total_seconds() * 1000000 + nanoseconds.total_"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tutc_nano = timegm(nanosec(nanoseconds))\n\treturn Time(date(1970, 1, 1), utc_nano, tz)\n\ndef datetime_for_utc_time(dt, tz=None):\n\ttry:\n\t\treturn dt - now_utc(utc=True).localtime()\n\texcept AttributeError:\n\t\treturn dt\n\ndef epoch(dt, tz=None):\n\t\"\"\"\n\tExtracts unix timestamp from string-like `dt`.\n\n:param dt:\n:param tz:\n:return: int\n\t\"\"\"\n\t"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tif tz is None:\n\t\tret = (Time.now + Timedelta(value=nanoseconds)).astimezone(tz=None)\n\t\treturn ret\n\telse:\n\t\tret = (Time.now + Timedelta(value=nanoseconds)).astimezone(tz)\n\n\t# Round if we've been asked to, and return if so.\n\tif ret.tzinfo is None:\n\t\treturn Time(ret.replace(tzinfo=tz))\n\n\t# We've been asked to. Now check to see if the time zone has\n\t#"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tif tz is None: return get_time_epoch()\n\telse: return tz.fromtimestamp(get_time_epoch())\n\n\ndef convert_secs2time(secs, tz=None):\t\t\"\"\"convert integer seconds to local time\"\"\"\n\tsecs += get_time_offset()\n\ttime = nanoseconds_to_time(secs, tz=tz)\n\treturn LocalTime.fromtimestamp(time)\n\n\ndef convert_time2secs(time, tz=None):\t\t\"\"\"convert time to sec and fractional seconds \"\"\"\n\thours = time / SECPERHOUR"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t# NOTE: Since Time is immutable, calling Time.mktime will not take effect\n\t# because Time.fromtimestamp may raise NotACurrentTimeError.\n\ttime = Time.mktime(nanoseconds)\n\tif tz not in (None, \"UTC\"):\n\t\treturn time.astimezone()\n\telse:\n\t\treturn time\n\n\ndef _validate_localtime(nanoseconds):\n\tlocaltime = lambda: _time_from_localtime(nanoseconds)\n\treturn localtime\n\n\ndef _time_from_localtime(nanoseconds):\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tif tz is None:\n\t\ttz = (None, None)\n\treturn calendar.timegm(nanoseconds, tz)\n\n# -----------------------------------------------------------------------------\n# format for `Text` and `Date` objects\n\ndef format_string(obj, format_str=''):\t#, dayfirst=True, numeric=False)\n\t\"\"\" Format a `Text` and `Date` object using `format_string`\n\t(see also :func:`format_list`)\n\tor a `Text`, `DateGroup` object, in either case, as a tuple\n\n\t.. deprecated:: 0"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tif nanos is None:\n\t\treturn None\n\n\tsec = float(nanoseconds)\n\n\tif isinstance(nanoseconds, timedelta):\n\n\t\thour = sec / 60. / 60.\n\n\t\tminute = hour * 60. - sec\n\n\t\tsecond = minute * 60 + sec\n\n\t\tmilli = sec - (hour * 60. * 60 + minute * 60)\n\n\t\treturn Time(hour, minute, second, milli)\n\n\tif tz is None:\n\t\treturn LocalTime(nanoseconds)\n\n\ttz"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tdef utime(self, tz=None):\n\t\tassert self.tzinfo is not None\n\t\tresult = (self.value // NANOSECONDS * NANOSECONDS) % (timedelta(1) * NANOSECONDS)\n\t\treturn timedelta(0, result * NANOSECONDS)\n\tdef istime(self, tz=None):\t\"\"\"\n\t\tImitate istime() from timelib's clock.\n\t\t:return: timedelta\n\t\t\"\"\"\n\t\tassert self.tzinfo is not None\n\t\treturn timedelta(self.value"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t# Returning Time objects would be much more efficient, but the\n\t# alternative would be to generate the `LocalTime` objects directly.\n\t# For backward compatability, we're moving back into a `Time` and\n\t# extracting the local timezone offset from it, allowing both to be\n\t# queried at the same time.\n\tt = time()\n\t# Extract the time zone from TZ if not set manually\n\tif tz is None:\n\t\tzone = pytz.timezone(t)\n\telse:\n\t\tzone = tz\n\ttime_obj = tz"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tif not isinstance(nanoseconds, (timedelta, decimal.Decimal)):\n\t\tnanoseconds = datetime.timedelta(0, nanoseconds)\n\tif tz is None:\n\t\t# Get a local time zone (from time.tzset)\n\t\ttz = self.__tz\n\tlocal_tz = datetime.timezone(tz)\n\tdatetime_local_timezone = local_tz.localize(datetime.time(0, 0, 0))\n\tdatetime_utc_timezone_no_dst = datetime.timezone(tz).localize(\n\t\tdatetime.time(0, 0,"}]}
{"task_id": "62e60f3bd76274f8a4026e10", "prompt": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\treturn timedelta(value)\n\n@cache_control\ndef daterange_startrange(value, dayfirst=True):\n\t\"\"\"\n\tA function which will format the first non-overlapping value in a range.\n\n\t.. versionadded:: 2016.3.1\n\n\t:param value:\n\t:param dayfirst: whether the first of the range is dayfirst, if True\n\t:return:\n\t:rtype: datetime.Range or string / Date\n\t:raises:\n\t:Raises ValueOutOfRange\n\n:example:\n\n.. raw:: html\n\n"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\treturn int(round(value.total_seconds()))\n\n\ndef _str_timedelta(timedelta):\n\tret = f\"{timedelta.seconds}:{timedelta.microseconds}:{timedelta.days}:{timedelta.hours}:{timedelta.minutes}:{timedelta.seconds_fraction}\"\n\treturn ret\n\n\ndef format_timedelta(seconds):\n\t\"\"\"\n\tFormat a timedelta, used in the\n\t:attr:`~pandas.DataFrame.timedelta` attribute in ``pandas.Series``.\n\n\t:param seconds: datetime/timestamp,"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tif PY3 and isinstance(value, (str, bytes, unicode)):\n    \treturn u(value)\n\telif PY2 and isinstance(value, datetime):\n        return datetime.fromisoformat(value.toordinal())\n\ndef dehydrate_tuple_of_timedelta64(value):\n\t\"\"\"Returns a dict of values parsed from ``value`` as timedelta64 or ISO formatted string.\n\n:param value:\n:type value: Tuple[int, int, int]\n:return:\tA dict of time (seconds,"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tif value is None:\n\t\treturn\n\treturn timedelta(seconds=value.total_seconds())\n\n\ndef get_default_cache_timeout(config):\n\t\"\"\"\n\tGet the default value for the \"cache_timeout\" config key in the config\n\tdictionary.\n\t\"\"\"\n    def default_value():\n\t\treturn config.get(\"cache_timeout\")\n\treturn def.get(config, defaults.DEFAULT_TIMEOUT, default_value)\n\n\ndef convert_date_range_to_absolute_urls(date_range):\n\t\"\"\""}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\treturn (value.days * (60 * 60 * 24)) + (value.seconds * 60 * 60 * 24)\n\n\ndef dehydrate_int(value):\n\t\"\"\"\n\tDehydrator for non-negative integers.\n\n\t:param value:\n\t:type value: int\n\t:return:\t\"\"\"\n\traise InvalidDataError(\"Integer values are not supported by this hydrator.\")\n\n\ndef dehydrate_list(value):\n\t\"\"\"\n\tDehydrator for list values.\n\n\t:param value:\n\t:type value"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\treturn timedelta(value)\n\n\ndef dehydrate_timedelta_list(value):\n\t\"\"\"\n\t  Dehydrator for `timedelta_list` values.\n\n:param value:\n:type value: dict or timedelta\n:return:\n    value for `list` types as dehydrated\n\n:Example: |\n\n.. code-block:: python\n\n\t>>> for dehydrated in dehydrate_timedelta_list(1):\n\t...\t\tprint(dehydrated)\n\n\t   # do something\n"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\treturn timedelta(seconds=value.total_seconds())\n\n# =============================================================================\nclass QueryType(Base):\n\t\"\"\"\n\t:return:\tAn :py:class:`SqliteOperation` object for indexing.\n\n:note:\n\n:example:\n\n.. code-block:: sql\n\n    SELECT person.id, person.name, person.age, people_to_friends.person.id, people_to_friends.person.name, people_to_friends.person.age, friends_to_people.person.id, friends_to"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tif isinstance(value, timedelta):\n\t\tvalue = value.hour * int(value.minute) * int(value.second) +\n\t\t\t\tint(value.micro) + (1 if value.tzinfo is None else value.tzinfo.utcoffset())\n\treturn value\n\n@dehydrate_db\ndef dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\t\n\tIn database.\n\t\n\t:param value:\tthe input `datetime`\n\t:type value:\ta :class:`dateutil"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tif not isinstance(value, timedelta):\n\t\traise TypeError(inspect.formatannotation(v))\n\treturn value.__class__(value)\n\n\ndef dehydrate_date(value):\t\t\t\t\t\t\n\t\"\"\"\n\tDehydrator for datetime.\n\n:param value:\n:type value: datetime.date\n:return:\t\"\"\"\n\treturn datetime.date(value.year, value.month, value.day)\n\n\ndef dehydrate_datetime(value):\t\t\t\t\t\t\n\t\"\"\"\n\tDehydrator for datetimes.\n\n:param value:\n"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tif value == \"NaT\":\n\t\t# This could be an epoch second (1s/second/...).\n\t\treturn NaT\n\tif value == \"NaT\":\n\t\t# This could be a UNIX timestamp (seconds since 1 January 1970 UTC).\n\t\treturn datetime.fromtimestamp(\n\t\t\t\t\t\t\tint(\n\t\t\t\t\t\t\t\tvalue.split('/')[0]\n\t\t\t\t\t\t\t\t\t+\n\t\t\t\t\t\t\t\t\tvalue.split('/').pop().split('.')[0] *\n\t\t\t\t\t\t\t\t\t10000)\n\t\t\t\t\t\t\t)\n\t\t# This is equivalent to timedelta(None).\n\t\t#"}]}
{"task_id": "62e60f37d76274f8a4026dfd", "prompt": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tif value is None:\n\t\treturn None\n\treturn ducktools.date2num(value)\n\n@dehydrate.register\ndef dehydrate_year(value):\n\t\"\"\"\tAdd the year only, if missing.\"\"\"\n\tif not value:\n\t\treturn None\n\tvalue = int(value)\n\ttry:\n\t\tassert int(value) <= self.year_max, repr(value)\n\t\treturn value\n\texcept:\n\t\traise ValidationError(\"Year exceeds range: %s\" % value)\n\n@dehydrate."}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\ttime_object = None\n\tif value is None:\n\t\treturn value\n\ttry:\n\t\tvalue = time.strptime(value, \"%b %d %H:%M %Z\")\n\texcept (ValueError, KeyError):\n\t\tpass\n\ttry:\n\t\t# if time is strptime, split on '.'\n\t\tvalue_parts = value.split('.')\n\t\tif len(value_parts) == 2:\n\t\t\t# if it's an int time, check and return seconds\n\t\t\tvalue = int(value_parts[0])\n\t"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tif value is None:\n\t\tdate_time = None\n\telse:\n\t\tif isinstance(value, Date):\n\t\t\tdate_time = value.time()\n\t\t\tif date_time is not None:\n\t\t\t\treturn date_time\n\t\t\t\terror_msg = _(u\"Time value must be in a correct format!\")\n\t\t\telse:\n\t\t\t\tdate_time = value.isoformat().replace('T', '')\n\t\telif isinstance(value, Time):\n\t\t\treturn value\n\t\telse:\n\t\t\tdate_time = value.isoformat"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\treturn time.parse(time.decode(value, \"strict\"))\n\n\nclass TimeProperty(Property):\n\t# Hack to get around a pycodestyle issue where `Time` does not convert to\n\t# `Date` when the `value` is a string value\n\tdef __init__(self, name, default=None, hint=None):\n\t\tif isinstance(default, date) and isinstance(hint, str):\n\t\t\tself._hint = hint\n\t\tif isinstance(default, str):\n\t\t\tself._hint = default\n\t\tself._default = default"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tif not isinstance(value, Date):\n\t\traise TypeError(f\"Please pass object of type {type(value)} to dehydrate.time\")\n\treturn value.strftime('%Y-%m-%d %H:%M:%S.%f')\n\n\ndef dehydrate_datetime(value):\t\"\"\"\n\tDehydrator for `datetime` values.\n\n:param value:\n:type value: datetime\n:return:\t\"\"\"\n\tif not isinstance(value, Date):\n\t\traise TypeError(f\"Please pass object of type {type(value)} to de"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t# First of all, this will make sure we're dealing with an\n\t# `Time`, even though the code won't have access to its\n\t# `datetime.time` member. :(\n\tif value.tzinfo is not None:\n\t\treturn value.tzinfo.localize(value._tz)\n\telse:\n\t\treturn value\n\n\ndef normalize_time(value):\n\t\"\"\"\n\tNormalize the provided value into a `Time`,\n\tas if it was a regular `datetime.time` object (not a `time.Time`).\n\n:param value"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\traise ValueError('Cannot dehydrate \"time\" value')\n\n\n@def dehydrate_time_delta(value):\t\"\"\"\n\tDehydrator for `time_delta` values.\n\n:param value:\n:type value: TimeDelta\n:return:\t\"\"\"\n\traise ValueError('Cannot dehydrate \"time_delta\" value')\n\n// Copyright \u00a9 2012-2021 VLINGO LABS. All rights reserved.\n//\n// This Source Code Form is subject to the terms of the\n// Mozilla Public License, v. 2.0. If a copy of the"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\treturn time.mktime(value.timetuple())\n\n/*\n * Copyright 2015 Netflix, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tassert(isinstance(value, Time))\n\treturn dehydrate_simple(value)\n\n\ndef dehydrate_time_delta(value):\t\"\"\"\n\tDehydrator for `datetime` values.\n\n:param value:\n:type value: Timestamp\n:return:\t\"\"\"\n\tassert(isinstance(value, Timestamp))\n\treturn dehydrate_simple(value)\n\n\ndef dehydrate_simple(value):\t\"\"\"\n\tDecode simple values from `string` types.\n\n:param value:\n:type value: unicode\n:return:\t"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\treturn value\n\ndef to_time(value):\n\t\"\"\"\n\tRepresents a value as a time with the time zone setting.\n\n:param value:\n:type value: Time\n:return: Time of `value` as specified by `timezone`\n:raises ValueError: if `value` is `None`\n:raises TypeError: if `value` is not a `Time` representation such as `str`\n:raises Exception: if `value` is an `Int` or any `str` or an `Int`/`Float`\n:raises Exception: if `value` cannot"}]}
{"task_id": "62e60f33d76274f8a4026de9", "prompt": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\tdef get_raw_value(value):\n\t\treturn tuple(value)\n\n\treturn dehydrate_point(dehydrate_point_impl(get_raw_value(point)))\n\ndef dehydrate_point_impl(value):\n\t\"\"\"\n\tDehydrate Point value.\n\n:param value:\n:type value: (Point, Point)\n:return:\n\t\"\"\"\n\treturn {\n\t\t\t\"x\": ((value[0], value[1]) * 2.0),\n\t\t\t\"z\": (value[2] * 2."}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\treturn value\n\n\n# Dehydrator for single precision floats\ndef dehydrate_float(value):\t\"\"\"\n\tDehydrator for floating-point values.\n\n:param value:\n:type value: float or long\n:return:\t\"\"\"\n\ttry:\tvalue\t = float(value)\n\texcept OverflowError:\traise\tDehydratorError(dehydrate_point, value, \"int\")\n\texcept ValueError:\traise\tDehydratorError(dehydrate_point, value, \"float\")\n\texcept TypeError:\traise"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\treturn Point(value[\"x\"])\n\ndef dehydrate_line(value):\t\"\"\"\n\tDehydrator for Line data.\n\n:param value:\n:type value: Line\n:return:\t\"\"\"\n\treturn Line(value[\"x0\"], value[\"y0\"])\n\ndef dehydrate_polygon(value):\t\"\"\"\n\tDehydrator for Polygon data.\n\n:param value:\n:type value: Polygon\n:return:\t\"\"\"\n\treturn Polygon(value[\"xlist\"])\n\ndef dehydrate_area(value):\t\"\"\"\n"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\tif type(value)!= types._Point:\n\t\tvalue = Point(value)\n\tif value.x!= None:\n\t\treturn pmath.Vector2(value.x, value.y)\n\n\ndehydrate_size = dict((size, dehydrate_size) for size in dehydration_sizes)\ndehydrate_points = dict(dehydrate_point, *dehydration_sizes)\n\n###\n### Plist\n###\n\ntry:\n\tplist_name \t= name.replace('.plist','')\n\t"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\tif not value.isValid():\n\t\treturn False, \"Error, corrupt Value: \" + value.toString()\n\telif value.isInteger():\n\t\treturn value.integer, \"Integer: \" + str(value)\n\telif value.isNumber():\n\t\treturn value.int, \"Real: \" +str(value.real) +\" ( \" +str(value.imaginary) +\")\"\n\telif value.isMatrix():\n\t\treturn _dehydrate_matrix(value), \"Matrix\"\n\telif value.isPoint():\n\t\treturn _de"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\treturn Point(value['x'], value['y'])\n\n/*\n * Copyright (c) 2018 THL A29 Limited, a Tencent company. All Rights Reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\tfrom. import Dehydrator\n\tdehydrator = Dehydrator()\n\treturn dehydrator.dehydrate_point(value)\n\ndef decompose_dehydrated_values(values):\n\tfrom. import Dehydrator\n\tdehydrator = Dehydrator()\n\treturn dehydrator.dehydrate_dehydrated_values(values)\n\ndef dehydrate_dict(dict):\n\tfrom. import Dehydrator\n\tdehydrator = Dehydrator()\n\treturn de"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t# Point to Dehydrator\n\tdehydrator_cls = dehydrator_classes.Point()\n\n\t# Dehydrate Point class\n\tdehydrator_cls.initialize(value)\n\n\treturn dehydrator_cls\n\n/*\n * Copyright (C) Scott Cranton, Jakub Korab, and Christian Posta\n * https://github.com/CamelCookbook\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\tif value is None:\n\t\treturn None\n\tp_list = [float(value.X), float(value.Y)]\n\treturn Point(*p_list)\n\ndef dehydrate_path(value):\t\"\"\"\n\tDehydrator for path data.\n\n:param value:\n:type value: Path\n:return:\t\"\"\"\n\tif value is None:\n\t\treturn None\n\tp_list = [float(value.X), float(value.Y)]\n\tp_list += [float(value.LeftWidth), float(value."}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\treturn Point(value)\n\ndef dehydrate_rect(value):\t\"\"\"\n\tDehydrator for Rect data.\n\n:param value:\n:type value: Rect\n:return:\t\"\"\"\n\treturn Rect(value)\n\ndef dehydrate_list(value):\n\t\"\"\"\n\tDehydrator for Arrays.\n\n:param value:\n:type value: List[Any]\n:return:\t\"\"\"\n\treturn [x.dehydrate() for x in value]\n\n\ndef dehydrate_dict(value):\n\t\"\"\"\n"}]}
{"task_id": "62e60ed4d76274f8a4026da0", "prompt": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t", "suggestions": [{"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn self.items()\n\ndef setdefault(self,name,value):\n\t\"\"\"\nIf a given key was not previously in the record, add it.\nIf the record had already been initialized with a key, use this method.\n\"\"\"\n\tif self._keys_for_default:\n\t\tself.keys()[-1] = []\n\t\tself._keys_for_default[name] = value\n\t\t# If a new key was added, keep a reference to it\n\t\tif name in self._keys_for_default:\n\t\t\tself._keys_"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn self._keys\n\ndef get_value(self, name):\n\t\"\"\"\n\tGet a value from the record and return it as a list of strings.\n\n\t:param name: name of key\n\t:return: list of key values corresponding to name\n\t:rtype: list\n\n\tUsage:\n\t\tget_value(self, key)\n\t\tfor k in keys(self): get_value(self, k)\n\t\t...\n\t\"\"\"\n\ttry:\n\t\treturn self[name]\n\texcept KeyError:\n\t\treturn []\n\n"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\tres = []\n\tfor key in self.db_keys():\n\t\tres.append(key)\n\treturn res\n\n\n#####################################\n#                             #\n# RULEs /                      #\n#                             #\n#####################################\n"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn self.key_names\n\n\tdef get_key(self):\t\"\"\"\n\tReturn the key at the given index of the record\n\n:param int _index_: an integer identifying the place in the list of\n  keys of the recieved record\n\n:return: The corresponding key value\t\"\"\"\n\tglobal _key_names\n\n\t_key_names.sort()\n\n\treturn _key_names[_index_]\n\nclass KRecord(Record):\n\tdef __init__(self, master_key_list, key=None, value=None):\n\t\t"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\tresult = self['keyname']\n\treturn result\n\ndef values(self):\t\"\"\"\n\tReturn the values of the keyname.\n\n:return: list of key values\t\"\"\"\n\tresult = ''.join(value for index, value in enumerate(self['keyvalue']))\n\treturn result\n\ndef add_record(self, data):\n\t\"\"\"\n\tAdds the record data to the registry.\n\n:param data: data to be added to the registry\n:return: None\n\t\"\"\"\n\tif not data:\t\treturn\t\t# Nothing to do\n\t"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn [x[0] for x in self.d]\n\n#-------------------------------------------------------------------------\ndef values(self):\t\"\"\"\n\tReturn the values of the record.\n\n:return: list of values\t\"\"\"\n\treturn [x[1] if self.header else x[0] for x in self.d]\n\n#-------------------------------------------------------------------------------\nclass FileLine(_data):\n\t\"\"\"\n\tA line in a (named) text file.\n\n\tThe format of text file is defined in \"File Format Specification V1.2\"\n\n\tAn optional comment has to consist of several lines"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn keys(self._records)\n\n\n# ----------------------------------------------------------------------\n# class DictRecord\n#   An interface for a dictionary record.\n\n\nclass DictRecord(object):\n\t\"\"\"\n\tAn interface for a dictionary record.\n\t\"\"\"\n\n\t# ----------------------------------------------------------------------\n\t# class DictKey\n\n\t# ------------------------------------------------------------------------\n\t# dict item methods\n\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\tCheck if the entry exists.\n\n\t\tReturns the key.\n\n\t\t:param str key: The key.\n\t\t:rtype: str\n\t\t"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\tb = self.data\n\ts = []\n\tkeys = b.keys()\n\tfor i in range(len(keys)):\n\t\ts = s + keys[i]\n\treturn s\n\t\n\tdef iterkeys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\tb = self.data\n\ts = []\n\titerkeys = b.iterkeys()\n\tfor i in range(len(iterkeys)):\n\t\ts = s + iterkeys[i].keys()\n\treturn s\n"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn self.get_attr('keys')\n\n// Copyright (c) 2018-present Baidu, Inc. All Rights Reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn [ key for key in self._db ]\n\ndef delete_keys(self, keys):\n\t\"\"\"\n\tRemove the keys.\n\n:param keys: list of key names\n:return the total number of deleted rows\n:note: it is the responsibility of the ``Session`` class to get rid of that\nand all the ``PreparedStatement`` instances.\n\t\"\"\"\n\tself._db.delete(keys)\n\treturn self._db.commit()\n\ndef remove_key(self, key):\n\t\"\"\"\n\tRemove the specified ``Key`` from the record.\n\n:"}]}
{"task_id": "62e60ecfd76274f8a4026d6a", "prompt": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif not protocol_version:\n\t\tprotocol_version = (3, 5)\n\tversion_mapping = {}\n\tversion = 0\n\tfor version, handler in cls.protocol_map.items():\n\t\tif not protocol_version[0]:\n\t\t\tversion_mapping[version] = handler\n\t\telse:\n\t\t\tif cls.is_supported(*version_mapping.get(version, [])):\n\t\t\t\tversion_mapping[version] = handler\n\treturn version_mapping\n\n#\n#\n\ndef protocol_name(version):\t\t\t# e."}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_version = ()\t# empty handler\n\tif protocol_version is not None:\n\t\ttry:\t# try to get the version from the protocol version\n\t\t\tprotocol_version = tuple(protocol_version)\t# make a tuple\n\t\texcept TypeError:\t# if protocol version is not supported\n\t\t\tprotocol_version = (0,)\t# report an exception for the first\n\t\t# verify the version is supported: get a module-level attribute\n\t\ttry:\n\t\t\t# This is the only supported version in B"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, (tuple, list)):\n\t\t\tprotocol_version = tuple(protocol_version)\n\t\tfor major, minor in protocol_version:\n\t\t\tif minor and not isinstance(minor, int):\n\t\t\t\traise TypeError(\"Expected protocol versions to \"\n\t\t\t\t    \"be integers, but got %r\" % protocol_version)\n\t\t\tcls.protocol_handlers[major][minor] = ProtocolHandler\n\n\treturn cls.protocol_handlers\n\nfrom. import utils\n\nclass ProtocolHandler"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\ttry:\n\t\tif protocol_version:\n\t\t\tcls['version'] = int(protocol_version[0])\n\t\telse:\n\t\t\tcls['version'] = 0\n\texcept:\n\t\traise TypeError(\"protocol version must be an integer \"\n\t\t\t\"version tuple or None\")\n\tv = 0\n\tif 1 <= cls['version']:\n\t\tif protocol_version:\n\t\t\tv = cls['version'] + 1\n\telse:\n\t\tv = 0\n\treturn cls\n\n\f\nclass NodeBase(DBBase):\n\t\"A"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tprotocols = list(set(cls.versions))\n\tif protocol_version is not None:\n\t\tprotocols.remove(protocol_version)\n\tprotocols.sort()\n\tprotocols_stringified = \",\".join(protocols)\n\tif protocol_version:\n\t\tif not (protocol_version > 0):\n\t\t\traise TypeError(\n\t\t\t\t\"protocol_version must be provided as \"\n\t\t\t\t\"a protocol version when creating a protocol \"\n\t\t\t\t\"with an explicit protocol version\" )\n\t\treturn {\"eth\": ETHHandler, \"tcp\": TCPHandler,\n\t\t "}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\thandler_dict = {}\n\tif protocol_version is None:\n\t\tprotocol_version = version.ProtocolVersions()\n\tfor version in protocol_version:\n\t\tif version > protocol_version[0]:\n\t\t\tbreak\n\t\ttry:\n\t\t\tversion_number = int(version)\n\t\t\thandler = cls.VERSION_MAP.get(version_number)\n\t\t\tif handler is None:\n\t\t\t\traise ValueError('unknown version in protocol_version: {!r}'.format(version_number))\n\t\t\thandler_dict[version_number] = handler\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\n\t\treturn default_protocol_handlers\n\telse:\n\t\treturn protocol_handlers[protocol_version]\n\n\ndef _make_middleware_object(func):\n\t\"\"\"Helper function to create a new function with\n\ta new middleware callable as an attribute.\"\"\"\n\t# This will allow this functions to directly connect to\n\t# an endpoint when a'middleware' (or callable) block is defined.\n\tdef wrapper_with_middleware(self, *args):\n\t\tcls = self.__class__\n\t\tmiddleware = func(self,"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_version = (0, )\n\t#\n\tprotocols = cls.PROTOCOL.items()\n\tfor ver_tuple in protocol_version:\n\t\tver, ok = _decode_ver_tuple(ver_tuple)\n\t\tif not ok:\n\t\t\traise TypeError(\"Unrecognized protocol version {!r}\".format(ver_tuple))\n\t\tver_type = ver.TYPE\n\t\tif ver_type == \"BoltProtocol\":\n\t\t\tprotocols[ver_tuple[0]].append(ver_type)\n\t#"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_version = tuple(map(int, _PROTOCOL_VERSIONS))\n\tassert version == 0 or len(version) == 2, \\\n\t    \"Unsupported protocol version %s\" % version\n\treturn dict(\n\t\t# All versions, supported or otherwise\n\t\t# No, Bolt3D\n\t\t(0, Bolt3D),\n\t\tversion[0],\n\t\tversion[1],\n\t\tprotocol_version[0],\n\t\tprotocol_version[1],\n\t)\n\n__"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\thandler_map = {'bolt_protocol_version': cls.__version__,\n\t\t'bolt_protocol': cls.__protocol__,\n\t\t'bolt_supported_protocols': {x:cls.__version__ for x in cls.__protocols__}}\n\tif protocol_version:\n\t\thandler_map['bolt_protocol_version'] = (\n\t\t\tprotocol_version, )\n\treturn handler_map\n\n\n#\n# Bolt Transport related methods\n#\n\n\n@implementer(ITransport)\nclass BoltTransport(object):\n\t\"\"\"Bolt Protocol Transport interface"}]}
{"task_id": "62e60e49d76274f8a4026d25", "prompt": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 210.00 MiB (GPU 0; 22.16 GiB total capacity; 20.09 GiB already allocated; 147.88 MiB free; 21.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62e60e05d76274f8a4026cfd", "prompt": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t", "suggestions": [{"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\tindex = self.__keys.index(key)\n\tif index >= 0:\n\t\treturn index\n\telse:\tself.value = ''\n\treturn -1\n\n\n####################################################################################################\n#### INTERNAL METHODS ####\n####################################################################################################\n\n# def merge(self, key, value):\n# \t\"\"\"\n# \tMerge any of the given arguments into the database entry\n\n# \t:param key: the key\n# \t:param value: the value\n# \t:exception ValueError: the key cannot be inserted into the database\n# \t:exception Overflow"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn sorted(self.map)[key]\n\n\t\n#----------------------------\n#                                      \n#                                      \n#                    Basic utilities  \n#     "}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn self.dict[key]\n\n\ndef remove(self, key):\t\"\"\"\n\tRemove key in the index.\n\n:param key: a key\n:rtype: int\t\"\"\"\n\tself.dict.pop(key)\n\ndef keys(self):\t\"\"\"\n\tThe list of items\n\n:rtype: list\n\"\"\"\n\treturn list(self.dict.keys())\n\ndef values(self):\t\"\"\"\n\tReturn a list of the names of all items\n\n:rtype: list          []  the names\n\"\"\"\n"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn self.data[str(key)[::-1]]\n\ndef load_from_json_path(self, json_path):\n\tload_json(self.data, json_path)\n\ndef load_from_json_path_files(self, json_path_files):\n\tfor json_path in json_path_files:\n\t\tload_from_json_path(self, json_path)\n\ndef make_json_path(self):\n\tjson_path = os.path.join(BASE_DIR, \"json\")\n\treturn"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\tif not self[key]: return -1\n\treturn self[key].index(key)\n\ndef last(self):\t\t\t\"\"\"\n\tReturn the last item.\n\n:return: last item\n:rtype: T.type\n\"\"\"\n\treturn chain(*(e for e in self._elts))\n\ndef map(self, mapper):\n\t\"\"\"\n\tMap each elt to the given mapper.\n\n:type mapper: callable(elt,elt) -> elt\n:return: \n\tA lazy iterator which will automatically iterate over each item in the\n\tdictionary, yielding the"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn self._key_to_index.get(key, None)\n\ndef keys(self):\t\t\"\"\"\n\tReturn a list of all keys, in ascending order.\n\n:returns: keys\n\t\"\"\"\n\treturn self._keys\n\n# This is called by the base class, but the user is informed here with the new set of keys.\n_update_cache = None\ndef user_changed(obj, new):\n\t\"\"\"\n\tCall this for the user once all items have been updated.\n\t\"\"\"\n\t\"\"\"\n\tIf the object itself has changed, reset"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\tassert(isinstance(key,str))\n\tfor i in self.get_data(key):\n\t\tif i.has_index():\n\t\t\treturn i.index()\n\t\telse:\n\t\t\treturn i.has_children() \\\n\t\t\tand self.index(i.children()[0])\n\ndef keys(self):\n\t\"\"\"\n\tReturn all keys of the given dictionary.\n\n:return: a list of keys\n:rtype: [str]\n\t\"\"\"\n\treturn [key.key for key in self.values()]\n\ndef index_sorted"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn self._items.index(key)\n\n\n# ----         dictionary methods           ------------------------------\n\n\ndef keys(self):\t\t\"\"\"\n\tReturn a list of the keys.\n\n:return: list of keys\n:rtype: list of int\n\t\"\"\"\n\treturn self._keys\n\n\ndef values(self):\t\t\"\"\"\n\tReturn a list of the values.\n\n:return: list of values\n:rtype: list of int\n\t\"\"\"\n\treturn self._values\n\n\n#"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn self.map[key]\n\n\ndef map_values(self):\n\t\"\"\"\n\tReturn an iterator over the mappings represented by the container.\n\n:returns: A iterator containing pairs of values.\n:rtype: iterator over mappings.MapValue\n\t\"\"\"\n\n\treturn self._map_values()\n\n\ndef _set_value(value, key, key_value):\n\t\"\"\"\n\tSet the value of the given key, if available.\n\n:param value: The actual value.\n:param key: The key of the value, whose existence will be checked in this"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\tself._db.close()\t\t# closes the database if exists\n\tindex = 0\n\ttry:\n\t\tindex = self._db.index_by(f\"(key, item_type, value)\", \"(?i)id\", key = key)\n\texcept:\n\t\ttry:\n\t\t\tindex = self._db.index_by(f\"(key, type)\", \"(?i)id\", key = key)\n\t\t\tfor i in range(len(self._db.all_keys)):\n\t\t\t\tself._db.index_del(f\"(key, type"}]}
{"task_id": "62e60da4d76274f8a4026cf1", "prompt": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t", "suggestions": [{"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tif len(keys) == 0:\n\t\tvalues = self.kv[self.idx]\n\t\treturn values or []\n\tkeys = [x for x in keys if x[:2] == 'keys']\n\tvalues = []\n\tfor v in self.kv[self.idx]:\n\t\tif not v['key'].endswith('-*'):\t# match a key\n\t\t\tknames = [x[2:] for x in keys]\n\t\t\tvalues.extend(v[knames] if knames else [])\n\t\telse:\n\t\t\tif keys and list"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tl = []\n\tfor k, v in self.fields.iteritems():\n\t\tvstr = [y for y in v]\n\t\tif self.filter(k) & vstr:\n\t\t\tif len(v) == len(vstr):\n\t\t\t\tl.append((k, vstr))\n\tif keys is not None:\n\t\treturn tuple(l)\n\treturn l\n\ndef set(self, key, value, *keys):\n\t\"\"\"\n\tSet a value in a key field\n\n:param key: key of the record\n:"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tresults = []\n\t\tfor k in keys:\n\t\t\tresults.append(self[k])\n\t\treturn results\n\n\t# -----------------------------------------------------------------------------\n\t#                    Methods for adding records to the\n\t#                    database\n\t# -----------------------------------------------------------------------------\n\t#\n\t#      add_item\n\t#\n\t#              Adds a new record to the database"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tfvalue\t= self._f[0]\t\t\t\t\t\t\t# return current record\n\tif len(keys) > 0:\n\t\tf = []\n\t\tfor i in range(len(self._f)):\n\t\t\tif keys[i] >= 0 and self[fvalue+keys[i]]!= self._f[i]:\n\t\t\t\tf.append(self._f[i])\n\t\t\telse:\n\t\t\t\tf.append(-keyvalue(self[fvalue+keys[i]]))\n\t\treturn f\n\telse:\n\t\treturn map(self."}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tif keys is None:\tkeys=self.ids\n\n\tif isinstance(keys, int):\tkeys = [keys]\n\tm = self.items()\n\tids = m.keys_by_index()\n\tlst= dict()\n\n\tfor v in self.by_key(keys):\n\t\tif v in m or v in ids: continue\n\t\tlst[v] = m[v]\n\n\treturn lst\n\n\t\ndef items(self): return self.by_index()\n\ndef __build_rows(self, keys, default=None"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tresult = []\n\tfor key, item in self.key_items().iteritems():\n\t\tif item.keys().intersection(*keys):\n\t\t\tresult.append(item.value(key=True))\n\treturn result\n\ndef __all():\n\tresult = __all_records()\n\tfor key, value in list(dict.keys(result)):\n\t\tif not isinstance(value, baseclass):\n\t\t\tvalue = type(value)(value)\n\t\tresult.setdefault(key, value)\n\treturn result\n\n__all_records = dict("}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tres=None\n\n\tif keys:\n\t\tret=[]\n\n\t\tfor key in keys:\n\t\t\ttry:\n\t\t\t\tret.append(self[key])\n\t\t\texcept KeyError:\n\t\t\t\t# the key does not exist in the record\n\t\t\t\tif self.get(key) is None: return []\n\t\t\t\telif key not in self: return []\n\t\t\t\telif key in self: return [self[key]]\n\t\t\texcept (KeyError, TypeError, ValueError):\n\t\t\t\t# some other error\n\t\t\t\treturn ret\n\telse:\n\t\t"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tif keys:\n\t\tif isinstance(keys, numbers.Integral):\n\t\t\tkeys = [keys]\n\t\tkeys = list(keys)\n\n\t\tfor p in keys:\n\t\t\tif p not in self.records:\n\t\t\t\tself.append(None)\n\t\t\tself[self.records[p]] = p\n\telse:\n\t\tself.append(None)\n\n\tself.sort_records(key=lambda x: x.keyname, reverse=True)\n\treturn self.records\n\n#---------------------------------------------------------------------------\n# Main code\n#----------------------------------------------------------------"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tif isinstance(keys, six.string_types):\n\t\tkeys = _index_and_key(self.handle.client_version, keys)\n\n\treturn self.query(self.handle, keys, \"values\").get(\"values\")\n\n// Generated by the protocol buffer compiler.  DO NOT EDIT!\n// source: google/protobuf/compiler/plugin.proto\n\n#ifndef PROTOBUF_google_2fprotobuf_2fcompiler_2fplugin_2eproto__INCLUDED\n#define PROTOBUF_google_2fprotobuf_2fcompiler_2fplugin_2eproto__"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tdb = get_db()\n\ttry:\n\t\treturn  [self[n] for n in keys if key(self[n]) in keys]\n\texcept KeyError:\n\t\treturn  [self[n] for n in keys]\n\n/*\n * Copyright 2016 Red Hat, Inc. and/or its affiliates.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n *\n *      http://www.apache.org/licenses/LICENSE-2"}]}
{"task_id": "62e60b10d76274f8a4026ccd", "prompt": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t", "suggestions": [{"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\treturn dict((getattr(self, field), value)\n                 for field, value in zip(self._info['fields'], *keys)\n\t\t )\n\n\t\ndef datadict(self, *keys, **defaultdict):\t\"\"\"Return the data in a (name[index], val[key]) pair,\nautomatically applying missing keys and\noptionally applying the defaultdict argument\nfor the missing keys. If self is a container object,\nself.__dict__ will be modified as if it were a sequence.\n\n:param"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\titems = (y for y in self[\"fields\"][\"@items\"])\n\tkeys = frozenset(keys)\n\tdef __get_item(i): return i if i >= 0 else None\n\telse:\n\t\ti = len(keys)\n\t\tif i_exists(items, i):\n\t\t\treturn defdict(items[i])\n\t\telse: return \"None\"\n\nclass ListField(Field):\n\t\"\"\"\n\tGeneric ``list`` field\n\t\"\"\"\n\tdef __init__(self, **kwargs):\n\t\tself[\"fields\"] = {}\n"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\tif len(self) == 0:\n\t\traise IndexError(\"You must include at least one field.\")\n\tindex = iter(self)\n\tout_string = \", \".join([str(item) for item in sorted(index)])\n\tif len(keys) == 0:\n\t\treturn dict(zip(self._fields, out_string))\n\telse:\n\t\treturn dict((key, out_string) for key in keys)\n\n// Copyright 2004-present Facebook. All Rights Reserved.\n\n#include \"FlowerFilter.h\"\n#include <algorithm"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\tf = self[field_to_idx]\n\tret = {}\n\tfor index in range(len(self)):\n\t\tif index < len(keys):\n\t\t\tif self[index][field_to_idx] is f:\n\t\t\t\tret.update(keys[index])\n\t\t\telse:\n\t\t\t\tret[keys[index]] =  self[index][field_to_idx]\n\treturn ret\n\ndef items(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a list,\noptionally including only certain values by index"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\tif isinstance(keys, Integral): keys = [keys]\n\tfor key in keys:\n\t\titem = self.item(*key)\n\t\tif key >= len(self):\t\t\t# out of bounds: insert new item\n\t\t\tself.insert(0, item)\n\t\telse:\t\t\t\t\t# in bounds: append new item\n\t\t\tself[-1].append(item)\n\treturn self\n\n\ndef append(self, item):\n\tself.update(item)\t\t\t# write out\n\n\n# _________________________________________________________________\n\n\ndef find"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\tfor k, v in items(copy_all_keys=True, **self):\n\t\tif type(v).__name__ == 'field':\n\t\t\tv = data(v, *keys)\n\t\telse:\n\t\t\tassert type(v) is not list  # or tuple\n\t\t\tv = [v]\n\t\tif k in keys:\n\t\t\tv.append(data(self[k], *keys[k]))\n\t\tself[k] = v\n\treturn self\n\ndef copy_all_keys:\n\treturn dict([(k,"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\tkwargs = {const.KEY_VALUE_KEY: None}\n\tvalues = self.dict().get(keys)\n\tif values:\n\t\tlog.debug(\"Found duplicate keys {0}\", keys)\n\t\tfor k in values:\n\t\t\tif k not in keys:\n\t\t\t\tlog.debug(\"Removing duplicate field {0}/{1}\", k,\n\t\t\t\t\tkeys)\n\t\t\t\tself.dict()[k] = None\n\t\t\telse:\n\t\t\t\t# Make sure the key isn't included\n\t\t\t\tkey_keys_set = set(self.dict()[:,"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\tresults = {}\n\tfor key in keys:\n\t\tresults[key] = self[key]\n\treturn results\n\t\n\t\n\t\ndef data_set_dicts(self, keys, values=True):\n\t\"\"\"\n\tAssign data (as a dictionary) to the record; values are converted\n\tusing :func:`dict_from_data_keys_strings` as dict keys.\n\tIf :attr:`copy` is ``False``, values at the source\n\trecord are added to the specified :attr:`values` list.\n\tKeys with a value of :const:`None"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\traise NotImplementedError('super().__setitem__ not defined for %s classes' % type(self).__name__)\n\ndef items(self):\t\"\"\"\n\tReturn the values of this record, in list form for data()\n                returns; in sets, returns a list\n                which is sorted by field index in Python 3.8+\n\n:rtype: list(ty)\"\"\"\n\traise NotImplementedError('super().items() not defined for %s classes' % type(self"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\ttry:\n\t\tvkeys = [_get_value(self,key) for key in keys]\n\texcept TypeError:\n\t\tvkeys = keys\n\ttry:\n\t\treturn {self:getattr(self, key):value for key, value in vkeys}\n\texcept AttributeError:\n\t\treturn {key:value.value for key, value in vkeys}\n\ndef __contains__(records, *keys):\n\t\"\"\"\n\tTest existence of record with string index or key.\n\tReturns True (True) if record is found in records or all records are missing \"\"\""}]}
{"task_id": "62e6087bd76274f8a4026bfa", "prompt": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\ta,b = self._data[:][::2]\n\tif b == self._data:\n\t\t# Whole buffer contains just the last byte.\n\t\tif len(self._data) > 1:\n\t\t\tself._data = self._data[1:]\n\t\telse:\n\t\t\tself._data = None\n\t\treturn struct.unpack(\"<H\", a)[0]\n\n\twhile b!= self._data:\n\t\tdata = b[3:7]\n\t\tself._data = self._data + self._header + data + self._data"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn (self[:2] + self[2:6])[::-1]\n\ndef pop_u32(self):\t\"\"\"\n\tRemove the last four bytes of data, returning them as a big-endian\n32-bit unsigned integer.\t\"\"\"\n\treturn self[:4] + struct.pack('>I', self[4])\n\ndef pop_u64(self):\t\"\"\"\n\tRemove the last eight bytes of data, returning them as a big-endian\n64-bit unsigned integer.\t\"\"\"\n\treturn struct.unpack('>Q', self[:8])[0]"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn struct.unpack_from('<I',self.data,\n\t                         len(self.data)-2)[0]\n\n\n\t\ndef dump_u16(self,value):\n\t# The (very similarly named) dump function as used for integer\n\tb = struct.pack('<i',value) # Put the big-endian 16-bit value on the\n\t\t# front of this buffer. This will then be used by the\n\t\t# debugging tools to print the"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn self.__pop_uint16()\n\n#\tdef pop_u32(self):\t\"\"\"\n\tRemove the last four bytes of data, returning them as a big-endian\n32-bit unsigned integer.\t\"\"\"\n\treturn self.__pop_uint32()\n\n#\tdef pop_u64(self):\t\"\"\"\n\tRemove the last eight bytes of data, returning them as a big-endian\n64-bit unsigned integer.\t\"\"\"\n\treturn self.__pop_uint64()\n\n#\tdef pop_s8(self):\t\"\"\"\n\tReturn the last ("}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn unpack(\">H\", self.data[:2])[0]\n\n\t\ndef unpack_array(self, data):\t\n\treturn array.array(\"i\", data)\n\n\npack = {\n\t1:     \"<1>\",\n\t2:     \"<2>\",\n\t_1:    \"\\\\x01\"\n\n}\n\ndef pack_str(self):\t\n\treturn \"%d-%d\" % (self.value, self.value + self.size)\n\n\ndef pack_bytes(self): \t\n\tout  = \"\""}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn unpack_u16(self[s+2:])\n\ndef pop_u32(self):\t\"\"\"\n\tRemove the last 4 bytes of data, returning them as a big-endian\n32-bit unsigned integer.\t\"\"\"\n\treturn unpack_u32(self[4:])\n\n\nclass Packet(PacketBase):\n\tdef __init__(self, message = None, **kw):\n\t\tself._cache = {}\n\t\t\t\n\t\t #\t@param message: message class\n\t\t #\t@type message: subclass of glib.Message or None\n\t\t self.message = message"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\tself.read = int(self.pop_data, False)\t# don't pop last data byte\n\tn = self.read\n\tself.read = self.read & 0x00ff\n\tself.read = self.read << 8\n\tself.read = self.read | n\n\treturn self.read\n\n\ndef pop_u24(self):\t\"\"\"\n\tRemove the last three bytes of data, returning them as a big-endian\n24-bit unsigned integer.\t\"\"\"\n\tself.read = int(self.pop_data, False)\t#"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\tself['f2-0'].pop()\n\tself.set_size(2)\n\tfor v in ('f2-1', None):\tself.append(v)\n\treturn self\n\ndef pop_u32(self):\t\"\"\"\n\tRemove the last 4 bytes of data, returning them as a big-endian\n32-bit unsigned integer.\t\"\"\"\n\tself['f2-0'].pop()\n\tself.set_size(4)\n\tfor v in ('f2-1', None):\tself.append(v)\n\treturn self\n\n"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn (ord(self.read(2)),ord(self.read(1)))\n\ndef pop_s16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit signed integer.\t\"\"\"\n\treturn(ord(self.read(2)),ord(self.read(1)))\n\n# End of pop_*.py interface\n\n\n# Beginning of pop_c*.py interface\nclass c_char_p(object):\n    \"\"\"\n    Abstract base class.\n    \"\"\"\n"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn struct.unpack('>H', self.read16())[0] << 16 >> 16\n\n\ndef b16(x)\t\t\t:\treturn (struct.unpack('>B', x))[0]\ndef be16(x)\t\t\t:\treturn (struct.unpack('<B', x))[0]\n\ndef pop_u32(self):\t\"\"\"\n\tRemove the last four bytes of data, returning them as a signed 32-bit\ninteger value.\t\"\"\"\n\treturn struct.unpack('>I', self.read32())[0] << 24\n\ndef"}]}
{"task_id": "62e6087ad76274f8a4026bf2", "prompt": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 22.16 GiB total capacity; 20.65 GiB already allocated; 5.88 MiB free; 21.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62e60879d76274f8a4026bec", "prompt": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 22.16 GiB total capacity; 21.05 GiB already allocated; 19.88 MiB free; 21.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62e60723d76274f8a4026b75", "prompt": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\tif n > 0:\n\t\treturn n\n\telif 2 * n > 4:\n\t\treturn n // 2\n\telif n > 2:\n\t\traise TypeError(\"n too large\")\n\telse:\n\t\treturn 1\n\n@dec.register(dict)\ndef dec_round_half_to_even(n):\n\t\"\"\"\"Round n to the nearest multiple of 2 (not 2 more than before)\"\"\"\n\tif n > 0 and n % 2!= 0:\n\t\tn += 2 \n\tif n > 0 and n%2 == 0:\n"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t_round_half_to_even(n)\n\n#==============================================================================\n#                                   PRINT FRIENDLY MODULE COMMANDS\n#==============================================================================\n\ndef print_friends(friend_string):\n    \"\"\"Prints friend information. This function takes a friend string argument.\n\n    :Parameters:\n     - friend_string :   The friend string to be printed.\n    \"\"\"\n "}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\tif ((n <= 0)):\n\t\treturn int(n * -1)\n\t\n\ndef __add__(a, b, op):\treturn op(add(a, b))\ndef __sub__(a, b, op):\treturn op(subtract(a, b))\ndef __mul__(a, b, op):\treturn op(mul(a, b))\ndef __div__(a, b, op):\treturn op(div(a, b))\ndef __truediv__(a, b, op):\treturn op(truediv(a, b))"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\treturn int(math.floor(math.log10(n)))+1\n\n\ndef nearest_even(num):\n\t\"\"\"Returns nearest num from zero to n in [-1, 1]\"\"\"\n\tn = int(math.log10(num))\n\tnum = int(math.ceil(num / 1.3))\n\tif(num > 5000):\n\t\traise ValueError\n\tif n < 5:\n\t\treturn num\n\telse:\n\t\treturn num-1\n\n\ndef round_half_to_even_n(num, n):\n\t"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\ttry:\n\t\tif (n+0.5)==n+0.5 and n%2==1:\n\t\t\treturn (n//2)\n\t\treturn (n//2+1)(n%2)\n\texcept:\n\t\tpass\n\n\traise ValueError(\"cannot interpret the number\")\n\n\ndef round_half_to_odd(n):\t\"\"\"\n\t>>> round_half_to_odd(3)\n    3\n    >>> round_half_to_odd(3.2)\n    3\n  "}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\tif n == 0: return 0\n\t# n rounded down\n\tp = 2*n\n\twhile p <= n and round_to_even(p):\n\t\tp+=\"1\"\n\treturn p\n\ndef convert_short_to_long(val):\n\treturn (\"0x%s\" % hex(val))[2:-1]\ndef format_hex(val):\n\n\tval = val.upper()\n\tif val[_l] not in (\"0X\", \"X\"):\n\t\traise argparse.ArgumentError(\n\t\t\t\"The following characters %"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\treturn int(pow(_math.floor(math.log(float(n), 10))+1, 2))\n\ndef floor_to_float32(n, bit_size=32):\n\t\"\"\"A flooring version of float32.\n\n\t:param n:\n\t:param bit_size:\n\t:return:\n\t\"\"\"\n\tif not len(n) and bit_size:\n\t\treturn n\n\tif 0 <= n < (1 << (bit_size-1)):\n\t\treturn float32(n) ** (1. / (1 << bit"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\treturn int(ceil(n))\n\ndef even_down(n):\n    \"\"\"\n\tSimple:\n\tIf n is even, return floor(n/2) else return floor(n/2)+1.\n\t:param n:\n\t:return:\t\"\"\"\n\tif not n % 2 == 1:\n\t\treturn int(ceil(n / 2))\n\treturn int(floor(n / 2) + 1)\n\n\n# ________________________________\n# ________________________________\n\n\ndef convert_text_field_value(evalue, f"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\tif n < 0:\n\t\traise ValueError('Round half to even n < 0')\n\treturn int(math.floor(math.log(n, 2) + 1))\n\ndef round_half_to_even(n, precision=1):\n\t\"\"\"\n\t:param n:\n\t:param precision:\n\t:return:\n\t:raises ValueError: if n exceeds precision, rounded\n\t\"\"\"\n\tn = int(n)\n\tif n == 0:\n\t\treturn 0\n\telse:\n\t\treturn round_half_to_even(n"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\treturn int(round(float(n)))\n\ndef min(x):\n\t\"\"\"\n\t:param n:\n\t:return:\n\t:rtype: float\n\t\"\"\"\n\treturn x if x < 0 else 0.0\n\ndef max(x):\n\t\"\"\"\n\t:param n:\n\t:return:\n\t:rtype: float\n\t\"\"\"\n\treturn x if x > 0 else 0.0\n\ndef sign(x):\n\t\"\"\"\n\t:param n:\n\t:return:\n\t:rtype: float\n\t\"\"\"\n\tif x <"}]}
{"task_id": "62e60707d76274f8a4026b69", "prompt": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tpoint_class = Point\n\tif name.startswith('point'):\n\t\treturn Point\n\tif name.startswith('base'):\n\t\tname = name[3:]\n\tif name.startswith('edge'):\n        name = name[4:]\n\tif 'point2d' == name and point_type.__name__.find(\"Point\")!=-1:\n\t\tname=points.name\n\telse:\n\t\tname=points.id\n\tpoint_class = type(name, \n\t\t\t\t(Point,),\n\t\t\t\tbases = (Point"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\toptions = ['Point', None]\n\tif srid_map:\n\t\traise NotImplementedError(\"Cannot handle srid_map %s.\" % srid_map)\n\n\tclass Point(bases.XForm, bases.XFormField):\n\t\t@property\n\t\tdef default(self):\n\t\t\treturn NotImplemented\n\t\n\t\tdef as_sql(self, compiler, connection):\n\t\t\tsql, value = super(Point, self).as_sql(compiler, connection)\n\t\t\tpoint = sql.get_bind()\n\t\t\tx = sql.col_x"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tclass Point(object):\n\t\tdef __init__(self, srid=None, **kwargs):\n\t\t\tself.srid = srid\n\t\t\tself._update_fields(fields)\n\t\t\tself.fields = self._create_geom_fields(self.srid)\n\t\t\tself.add_geom_attrs()\n\n\t\tdef _add_geom_attrs(self):\n\t\t\tfor field in self._geom_fields(self.srid):\n\t\t\t\tif hasattr(field, '_geom_attrs'):\n\t\t\t\t\tsetattr(self, field._geom"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tdef new(self, *args, **kwds):\n\t\t\"\"\"Generate a new Dynamically-created Point instance.\"\"\"\n\t\treturn Point(self, *args, **kwds)\n\tif not hasattr(point_type, '__subclasscheck__'):\n\t\t# Point is not subclassable, so return a plain Point\n\t\treturn point_type(name, fields, srid_map)\n\tpoint_classes = [point_type.__subclasscheck__(*args, **kwds) for args, kwds in (\n\t\t\targs_ordered(point_type.__args"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\ttry:\n\t\tfields = [field[2] for field in fields]\n\texcept:\n\tfail('Unexpected field values in subclass __init__: %s' % fields)\n\treturn Point('%s.x/%s' % (name, ','.join(fields)), srid_map)\n\ndef point_type_as_dict(point_type, name, fields):\n\t\"\"\"\n\tDynamically create a dict subclass.\t\"\"\"\n\ttry:\n\t\tfields = [field[1] for field in fields]\n\texcept:\n\t\tfail('Unexpected field"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tcls = Point\n\tif name == 'Point': return cls\n\tpt = types.new_class('Point', POINT2DPYTYPES)\n\tpt.__name__ = name\n\tpt.fields = fields\n\tpt.srid_map = srid_map\n\n\tdef setcoords(self, newp):\t\"\"\"\n\t\tSet coordinates for this Point.\tThe srid of newp is preserved so\n\t\tfor point sets, all geometry objects with the same srid will have\n\t\tthe same coordinates.\n\t\t\"\"\"\n\t\tpt = self\n\t"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tPOINT = cls([ (name, fields[0], fields[1], fields[2], fields[3], True, True, False), \n\t\t\t\t\t(name, fields[4], fields[5], fields[6], fields[7], True, True, True) ])\n\tPOINTS = cls([ (name, fields[0:2], fields[2:5], fields[6:8], fields[8:11], fields[11:], True, False),\n\t(name, fields[12], fields[13], fields[14], True, True, True, False) ])\n\tsr"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tfrom math import sqrt\n\tfrom warnings import warn\n\tfrom mmtf.lib import PointTypes\n\n\tPoint = PointTypes.Point\n\n\tPoint = _make_class(Point, Point, doc='''\n\n\tThis class can be used to represent an Point of unknown type,\n\tsuch as a field, or to represent a point with an unknown type and\n\tas a value.\n\t''', mclass='Point type')\n\n\n\n\tPointType = Point.point_type\n\tPoint = PointType.base\n\t_get_type = PointType._get"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tclass Point(object):\n\t\t__slots__ = ['name', 'id']\n\n\t\tdef __get_id__(self):\n\t\t\treturn (self.name, self.id)\n\n\t\tdef __set_srid__(self, name, srid):\n\t\t\t\"Create a DynamicSubclass of a Point subclass.\"\n\t\t\tself.name, self.id = name, srid\n\n\t\tdef __init__(self, name, id=None, **fields):\n\t\t\t\"Copy the value of a dynamically-created Point class instance.\"\n\t\t\tif"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tp = Point()\n\tp.name = name\n\tp.fields = dict(fields)\n\tp.points = {} # The points created in this class\n\tp.srid = srid_map\n\tp.size = 0 # Size of one point\n\tp.id = 0 # Unique identifier\n\n\tdef make_point(**kw):\n\t\tif 'X=...' in kw.keys():\n\t\t\tp.x = kw['X']\n\t\tif 'Y=...' in kw.keys():\n\t\t\tp.y = kw['Y']"}]}
{"task_id": "62e5dc9ed76274f8a4026b5b", "prompt": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\tif not message:\n\t\treturn \"Use 'foo' instead.\"\n        warning(\"There was a deprecated function: \\n\" + message)\n\tdef _deprecated(msg):\n\t    warn(\"There was a deprecated method: \\n\" + msg)\n    _Deprecated = deprecated\n\nThe following exception objects represent the exceptions raised during an\nexecution of a task.  These do not necessarily propagate:\n\n::\n\n    try:\t\t# doctest\n        task = Task()\n    "}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\tif __doc__:\n\t\treturn \"\"\n\treturn DeprecationWarning(message).replace('\\n','').replace('\\t','')\n\n\nimport re\n\n\ndef main():\n\timport doctest, inspect, codecs\n\timport unittest, re, os\n\timport warnings\n\twarnings.filterwarnings('ignore', '', DeprecationWarning)\n\n\tdef _deprecate_args(function, deprecated_methods):\n\t\t\"\"\"\n\t\tFunction with the additional parameters\n\t\t\"\"\"\n\t\ts = unittest.TestLoader().loadTestsFromTestCase(function)\n"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\"\"\"\n\tDecorating function foo in the deprecated module.\n    \"\"\"\n\n\tif 'deprecate' in globals():\n\t\tmsg = message % globals()[ 'deprecate' ]\n\t\tif not msg:\n\t\t\tdeprecation(message)\n\telse:\n\t\tmsg = message % globals()\n\t\tdeprecation(message)\n\n/*\n * (C) Copyright 2006-2016 Nuxeo SA (http://nuxeo.com/) and others.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\toldwarning = current_warnings\n\ttry:\n\t\t\tcurrent_warnings.pop()\n\texcept AttributeError:\n\t\tpass\n\n\tdef f():\n\t\tprint('Welcome to a Python %s.' % sysname, file=sys.stderr)\n\n\t@deprecated('A method has been deprecated and will be removed.')\n\tdef g(x):\n\t\tx = deprecated(\"'g' has been deprecated in favour of 'y'\")\n\t\t# You would use these, but this line triggers deprecation warning.\n\t\tprint('Welcome to a Python %s"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\tpass\n\n\n\ndef isDeprecated(func=None):\n\t\"\"\"\n\t::\n\n\t\tdef isDeprecated(func):\t\t# returns bool\n\t\t\treturn True if 'func' is deprecated\n\n\tIs deprecated if the function is deprecated.\n\n        When invoked on an instance, ``__class__`` is always ignored.\n\n        When invoked from a function, ``__module__`` is always ignored.\n\n        When invoked with keyword arguments,\n        the"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\tif not __debug__:\n\t\tassert deprecated(\"message\")\n\n\tdef warn(x):\n\t\tif not __debug__:\n\t\t\tassert deprecated(\"message\")\n\n\t\tf = sys._getframe()\n\t\tf.f_globals['msg'] = message # noqa\n\t\tf.f_locals['msg'] = message # noqa\n\n\t\ttry:\n\t\t\treturn x+1\n\t\texcept ValueError:\n\t\t\treturn warn(1)\t\"\"\"\n\n\tdef deprecated_warn(msg=None):\n\t\tdef wrapper(fn):\n"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\tdef bar(x):\n\t\tpass\t\"\"\"\n\n\tdel deprecated\n\n\n\"\"\"\n\n# Note: all these functions are now part of the Python 2.5+\n# interface (so they cannot import outside the class!), but as\n# long as the function isn't called from \"from. import foo\"\n\n\n__all__ = [\n    \"Function\", \"Method\", \"FunctionDef\", \"ClassDef\", \"Alias\", \"File\",\n    \"Module\", \"Interactive\", \"Experimental\", \"ExperimentalStatement\",\n    \"Statement\", \"Global\","}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\tdef __init__(self,*args,**kw):\n\t\tsuper(deprecated,self).__init__(*args,**kw)\n\t@abc.abstractmethod\n\tdef foo(self):\n\t\t\"\"\"foo in a class (deprecated)\"\"\"\n\t@deprecated(\"'foo' has been deprecated in favour of 'bar' (Python 3)\")\n\tdef bar(self):\n\t\tpass\t\"\"\"\n\n\tdef baz(self):\n\t\tfoo(),foo()\n\t\t\"\"\"\n\n\tdef f(self):\n\t\tbar(),bar()\n\t\t\"\"\"\n\t"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\twarnings.warn(\"DEPRECATION: Deprecated function 'deprecated'\")\n\treturn\n\"\"\"\n\n//  Boost string_algo library trim.hpp header file  ---------------------------//\n\n//  Copyright Pavol Droba 2002-2003.\n//\n// Distributed under the Boost Software License, Version 1.0.\n//    (See accompanying file LICENSE_1_0.txt or copy at\n//          http://www.boost.org/LICENSE_1_0.txt)\n\n//  See http://www.boost.org/ for updates"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\treturn deprecator(deprecated.__doc__)\n\nDEPRECATION_WARNING = deprecator(deprecated.__doc__)\n\n\nclass CliError(Exception):\n\n\t\"\"\"Base for CLI exceptions\"\"\"\n\n\nclass CliRuntimeError(CliError):\n# XXX: should be subclass of Exception\n\t\"\"\"Customized from CliError\"\"\"\n\n\tdef __init__(self, msg):\n\t\tself.msg = msg\n\t\tsuper(CliRuntimeError, self).__init__(msg)\n\n\nclass CliUsageError(CliError):\n\n\t\"\"\"The same as `Cli"}]}
{"task_id": "62e4fc3c85ea98643089041e", "prompt": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\tif env_var_map not in os.environ:\n\t\traise ValueError(\n\t\t\tf\"Couldn't find environment variable {env_var_map} in r_app's environment. Please make sure that it is defined in your.env\"\n\t\t)\n\ttry:\n\t\tout = str(code)\n\t\tout = re.sub(\n\t\t\tr\"\\D+\",\n\t\t\tr\"R \",\n\t\t\tout,\n\t\t\tcount=1,\n\t\t)\n\t\treplace_in_string = r\"env:REPLACE_IN_STRING"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\tif code == 'test' or (code == 'test' and env_vars['USEV']):\n\t\t# test runs in this scope\n\t\treturn code\n\telif not env_vars['USEV']:\n\t\t# normal run of the program\n\t\treturn code\n\telif env_vars[\"USEV\"] in (\"r\", \"v\", \"x\", \"z\"):\n\t\t# special run, it is the one being passed to the environment\n\t\treturn \"env:{}\".format(env_vars[\"USING\"])\n\telse:\n\t\t# the last possible option"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\tglobal R_FLAGS\n\tR_FLAGS = {\n       'verbosity:'                      : 1,\n        'help'                            : 1,\n    }\n\tR_OPTIONS = R_FLAGS\n\tR_OPTIONS['r_log_dir'] = \"./logs/r\"\n\tR_OPTIONS['r_server"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\toptions = {str(k) for k in os.environ if k.startswith(code)}\n\treturn _inline_opt_setup(options)\n\ndef _inline_opt_setup(options: str) -> str:\t\"\"\"\n\tParse the output of `env.py --setup`, in order to make a simple setup code for R.  \t\t\"\"\"\n\tsetup = options.split(\"=\")\n\tr_opt, r_opts = \" \".join(setup).split('-')\n\tif len(options) > 1:\n\t\traise Exception(\"Only one extra word"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\tcode = _cleanup_and_escape_args(code)\n\treturn r\"\"\"\n\t# Make sure R never runs with debug enabled.\n\tfrom __future__ import print_function\n\tfrom r2.lib.cli import get_commands\n\tprint_function()\n\tset_r_debug()\n\t\"\"\"\n\nclass _inline_r_setup_default(*_inline_r_setup)\t# type: ignore\t# noqa\t# can't find any code after class\n\n// Copyright 2016 Google Inc. All Rights Reserved.\n//\n// Licensed under the Apache License,"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\twith env(\"R_OPTS_HERE\") as R_OPTS_HERE:\n\t\tR_OPTS_HERE.append(code)\n\treturn \"\\n{}\".format(code)\n\ndef _inline_r_cleanup(code: str) -> str:\t\"\"\"\n\tR clean up.\n\tcode is not run, but must be written to the right section of the R config options.\n\t\"\"\"\n\twith env(\"R_OPTS_HERE\") as R_OPTS_HERE:\n\t\tfor i in range(len(R_OPTS_HERE)):\n\t\t\tif ("}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\tassert code in _inline, \"code should be in {}\".format(_inline)\n\tcode = code.strip()\n\n\treturn \"{}\\n\".format(code)\n\n\ndef _r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\tassert code in _r, \"code should be in {}\".format(_r)\n\tcode = code.strip()\n\n\treturn \"{}\\n\".format(code)\n\n"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t#\n\t# if (r\"${CMAKE_TARGET_DIR}.*\" in r\"{\\${CMAKE_DEFINITIONS_DIR}:${CMAKE_GLOBAL_DIR}\", r\"[$]Error:.*${CMAKE_DEFINITIONS_DIR}.*\")\n\t# else\n\t# \treturn \"\"\n\t#\n\tr_opts = [\"%{CMAKE_TARGET_DIR:${TARGET_DIR}}/${TARGET_NAME}\"]\n\tr_opts.extend([\"set(\\${CMAKE_GLOBAL_DIR}${global_dir}) %{USE_R_"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\tenv._R_enabled = True\n\treturn code\n\n\n_inline_r_setup_str = _inline_r_setup\n_inline_r_setup_inline = _inline_r_setup\n\n\ndef _inline_r_start_inline():\n\t\"\"\"\n\tStart R via inline R execution, if R is enabled. This requires R to\n\tbe in the command line.\n\t\"\"\"\n\tif hasattr(sys, \"_set_executable\") and sys._set_executable!= \"0\":\n\t\treturn 0\n\n\tif getattr(sys, \"_R_"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\tif code == \"install\" and \"R_HOME\" not in os.environ:\n\t\tos.environ[\"R_HOME\"] = environ.os_getenv(\"HOME\", default_home=environ.global_get(\"R_HOME\", default_home))\n\t\tenviron._replace_or_push_pop(\"R_HOME\", environ.os_getenv(\"HOME\", \"\"))\n\n\treturn code\n\n\ndef _inline_r_main(code: str, _args: strTuple, _kwargs: Dict) -> int:\n\t\"\"\"\n\tCode goes to _main by"}]}
{"task_id": "62e4fbda85ea986430890405", "prompt": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\"\")\n\n\tdef run(self):\n\t\tself.config.xargs.color = self.args.color\n\t\tself.config.xargs.target_concurrency = \\\n\t\t\tself.config.xargs.target_concurrency\n\n#include <cassert>\n#include <cstdio>\n#include <cstdlib>\n#include <cstring>\n#include <algorithm>\n#include <iostream>\n#include <vector>\n#include <queue>\n#include <list>\n#include <set>\n#include <map>\n\n#define inf"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t)\n\t_usage = \"\"\"[-h] [options] {program | --script}\nExecutes the program, using the input as parameters.  Options are:\n\t-v | --verbose\n\t-C | --color\n\t-T | --target-concurrency\nThe output is an array of the output of the program; if the program\nerrors, it should be printed but not exit.\"\"\"\n\n\nclass Parallel(BaseCommand):\n\t'''Runs multiple programs concurrently'''\n\t_cmdline_ = ['parallel']\n\tdef run(self, args):\n\t\tfrom docopt import"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t)\n\n\n\n#: Argument specification\ndef argument_spec(\n\targspec: types.ArgumentTypeList\n\t) -> specs.ArgumentTypeSpecBase:\n\t\"\"\"Type-specific arguments\"\"\"\n\tpass\n\n\n\n#: Argument type specifier\ndef argument_type_spec(\n\tdeclarator: specs.SpecParam,\n\targument_name: str,\n\ttype_: specs.TypeSpecifier,\n\tdefaults: Optional[Dict[str,Any]]= None\n) -> Optional[specs.FunctionParamSpec]:\n\t\"\"\"Create new instance of function argument-spec with optional defaults"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t.args(\n\t\t(\"-s\", \"--ssh\",)\n\t\t.metavar(\"ARG...\").default(1),\n\t\t(\"-t\", \"--tty\",)\n\t\t.metavar(\"TERM...\").default(0),\n\t\t(\"-q\", \"--quiet\",)\n\t\t.metavar(\"QUIET...\").default(0).hide(),\n\t\t(\"--no-color\",)\n\t\t.action(self.color)\n\t\t.metavar(\"COLORS...\")\n\t\t.default(0)\n\t\t.hide()\n\t)\n\t.positional_"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\t# this code is a cropped version of it!     if target_concurrency is 0 then\n\t\t# use it!  This code is a pain to have, but it breaks some functions that are\n\t\t# not working with pty (like spawn and os.spawnv)! -pgr\n\n\t\timport sys, pty\n\n\t\ttarget_concurrency = sys.platform.startswith('win') +'-P'\n\t\n\t\t# if we can be sure that we've got as much space, increase the target for more threads\n\t\tif"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t)\n\n\tdef __init__(self, **kwargs):\n\t\tsuper().__init__(**kwargs)\n\n\tdef parseArgs(self):\n\t\tself.opts['color'] = self.opts['color'] and not self.opts['color'] == 'True'\n\n\tdef run(self):\n\t\tif self.opts['color']:\n\t\t\treturn Color(log_level=3, color_type=self.opts['color']).main()\n\t\treturn 0\n\n\n# a simple container utility for managing a list of partition id\nclass IdList(object):"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\targuments = [\n\t\t\"cat $XARGS_SOURCE | tee $XARGS_TARGET \"\n\t]\n\n\t# On all platforms, if a target number bigger than\n\t# one the test may fail.\n\targuments = [\"sh \" + target_name()] + arguments\n\n\tcolor = False\n\ncolor: Make tests with $COLOR\n\"\"\"\n\tcolor = True\n\tXARGS_TARGET = target_concurrency()\n\n\n\ndef xargs_options():\n\t\"\"\"\n\tOptions for xargs.\"\"\"\n\toptions = (\n\t\t\"xargs-flags\","}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t# This sets the system wide environment variable\n\n        if 'color' not in list(os.environ):\n            os.environ['color'] = 'always'\n        if 'color' not in list(os.environ):\n                return -2\n        if \"COLOR\" in os.environ['color']:\n                return os."}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t)\n\nxargs.parseopt(\n\tusage=\"Usage: %prog [options] <command>...\"\n)\nxargs.print_help = xargs.print_help\n\nif xargs.parse:\n\t# we already parsed this arg, just continue\n\targs = xargs.parse(sys.argv)\nelse:\n\tf = open(XARGS_FILE.encode(sys.getfilesystemencoding() or sys.getdefaultencoding()), \"r\")\n\tf.encoding = sys.getdefaultencoding()\n\tf.encoding = \"ascii\" # for"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t):\n\t'xargs'-compatible implementation for xargs. Takes no args.'\n\treturn xargs(color=False, target_concurrency=False, env=None)\n\n\ndef xlog(level=u'ERROR'):\n\t'''Simple logger. This is just for purity because the xlog functions\n\twill be called more often.\n\ncolor: Fancy color.\ntarget_concurrency: Set as many processes.\nenv: The environment, or tuple of environment variables. Passed to xlog for\n\tformatting and other details as per xget(2), xget(2"}]}
{"task_id": "62e4fbda85ea986430890403", "prompt": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\tif len(seq) == 0:\t\t\t\t# Empty str in a list\n\t\treturn []\n\tassert seq > \" \"\t\t\t# Empty list\n\tres = []\n\tcur = []\n\tlast = False\t\t\t\t# For speed\n\tfor i in seq:\n\t\twhile last and not i:\n\t\t\tcur.append(i)\n\t\t\tlast = False\n\t\tcur.append(i)\n\treturn res\n\ndef _shuffle_helper(x: str, y: str) -> str:\n\t\"\"\"\n\tHelper function for _shuffled"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\tresult = []\n\tfor _ in shuffler:\n\t\ttry:\n\t\t\tresults = _shuffled_seq(seq)\n\t\texcept IndexError:\n\t\t\tpass\n\t\telse:\tresult += results\n\treturn result\n\n\ndef _unshuffled_seq(seq: Sequence[str]) -> list[str]:\t\t\t\"\"\"\n\tUnshuffled.\n\t \"\"\"\n\tif not isinstance(seq, list):\n\t\tseq = [seq]\n\tnew_seq = []\n\tfor s in seq:\n\t\tidx ="}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t# seq[i] := i + 1 & 0\n\tn = len(seq)\n\tret = []\n\tfor i in range(n):\n\t\tret.append(seq[i])\n\t\tfor j in range(1, i):\n\t\t\tj += 1\n\t\t\tret.append(ret[j-1])\n\treturn ret\n\n\n#\n#  I can't explain my life to this solution... because I don't have the proper\n#  definition of a list yet.\n#\n#  _print(map(list, _shuffled"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\treturn list(shuffle(seq))\ndef is_iterable(obj) -> bool:\t\t\t\"\"\"Determine if an object is an instance of any type of object.\"\"\"\n\treturn issubclass(obj, Iterable)\ndef is_string(val: any) -> bool:\t\t\t\t\"\"\"Determine if provided object is None, string, or int, unicode, or any object providing string representation as implementation of `str.__format__`.\"\"\"\n\treturn hasattr(val,'format') and not callable(val.format)\ndef is_iterable_subclass(type_instance: Type[Iterable]) -> bool:"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\tif not seq:\n\t\treturn []\n\tlist = list()\n\tfor item in seq:\n\t\tseed = seed * len(item)\n\t\tidx  = random.randint(0, len(list) - 1)\n\t\tlist[idx] = list[idx].swapcase()\n\t\tlist[:idx] += list[idx:].swapcase()\n\treturn list\n\n@functools.lru_cache(maxsize=None)\ndef _build_tree(tree_type: str, input_size: int,\n\t\t\t\tdepth: int, seq"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\tn = len(seq)\n\tshuffled = []\n\n\tfor i in range(n):\n\t\ttemp = seq[i]\n\t\tshuffled.append(\"\".join(random.choices(string.ascii_letters, k=n)) if(int(temp)==int(seq[i])) else temp)\n\t\tshuffled.append(random.choice(string.ascii_letters)) if(int(temp)==int(seq[i])) else temp # remove the letter from shuffled\n\n\treturn shuffled\n\n# ------------------------------------------------------------------------------\n"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\tshuffled = list(seq)\n\tn = len(shuffled)\n\n\tdef shuffle_func(i:int):\n\t\tshuffled[i] = shuffled[i * ((n - i) // 2) : (i + 1) // 2]\n\n\tshuffle_func(0)\n\tshuffle_func(1)\n\tassert all(a == a for a in shuffled[1:])\n\n\tshuffled[0:0] = shuffled[n - 1:]\n\tshuffled[0:0] ="}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t# print(\"shuffled %s\" % seq)\n\tseed, arr = int(seq[-1]), seq\n\tcnt = int(1)\n\tif(len(seq) < 3):\n\t\tprint(\"Not enough elements for shuffled %s\" % seq)\n\telse:\n\t\tfor i in range(len(seq)):\n\t\t\tseq[i] = \"...\"\n\t\tfor i in range(3, len(seq)):\n\t\t\tif(cnt % 3 == 0):\n\t\t\t\tarr = [seq[j] + seq[j +"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\tfor i, j\t in enumerate(seq):\n\t\tj = seq[i]\n\t\t# if seq[i]!= seq[j]:\n\t\t# \t# i comes after j\n\t\tseq[i], seq[j] = seq[j], seq[i]\n\treturn list(seq)\n\n\ndef _unique(seq: \tlist = [])\t():\t\"\"\"\n\tReturn a unique list\"\"\"\n\tif not \tseq\t:\treturn []\n\treturn \t[: len(\tseq)\t]\n\n/*\n * Licensed to"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\tfor i in range(len(seq)):\n\t\tc = seq[i]\n\t\tfor j in range(i-1, -1, -1):\n\t\t\tr = seq[j]\t\t# sort [a,c,b,c,b,c,a,f,g]\n\t\t\tseq[j], seq[i-1], seq[i], seq[j-1] = r, seq[j], r, c\n\t\tseq[i] = seq[i-1]\t# remove [f,c,g]\n\treturn"}]}
{"task_id": "62e4fb6585ea98643089032b", "prompt": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tv1 = s.lower()\n\tv2 = s #@\n\t\n\t# if the first two are numbers  @\n\tv3 = { 3 : 1, 4 : 2 }\t# or {1 : 1, 3 : 2} in the list, it's a number\n\t\n\tif v1 == '1':\n\t\treturn v2,v3\n\telif v2 == '3':\n\t\treturn v1,v3\n\telif v1 == '4':\n\t\treturn v2,v3\n\telif v1 == '2':\n\t\t"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tif s == \"0.1\":\treturn (0, 0, 1)\n\tif s[:2] in (\"0\", \"1\"):\n\t\t_, _, number = s[2:].partition(\".\")\n\t\treturn int(number), s\n\tversion_matches = [\n\t\t\"12\", \"12.1\", \"12.2\", \"12.3\", \"12.4\", \"13\", \"13.1\", \"13.2\", \"13.2.1\",\n\t\t\"13.2.2\", \"13.3\", \"13.4\","}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\ttry:\n\t\tv = tuple(map(int, s.split(\".\")[:2]))\n\texcept ValueError:\n\t\tpos = s.find(\".\")\n\t\tif pos!= -1:\n\t\t\tv = (s[:pos], int(s[pos:]))\n\treturn v\n\n\ndef _parse_version(args=None) -> Tuple[int,...]:\n\t\"\"\"\n\tdesc:           Parse and version data from a manifest file\n\tauthor:         Trevor Chpat"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\ti, e = 0, len(s)\n\twhile i < e :\n\t\tm = s.count(_version_re.match(s[i]))\n\t\tif m == 3 :\n\t\t\tif s[i - 1] == s[-1] : i = i - 1\n\t\telif m == 2 :\n\t\t\tif s[i - 1] == \"rc\" : i = i - 1\n\t\telse :\n\t\t\ti = i - 1\n\treturn i + 1, s[i:i + 1]\n\ndef read_data("}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tfrom. import __version__\n\n\t# Check in all major version numbers\n\tpossibilities = [\n\t\t# Major is 1, Minor is 0, Build number is 9876\n\t\t(1, 2, 3),\n\n\t\t# Major is 1, Minor is 2, Build number is 8, Build date is \"January 26, 2013 00:00:00 MST\"\n\t\t(1, 2, 3, 9876),\n\n\t\t# Major is 1, Minor is 3, Build number is 7, Build date is \"January 27, 2013 00:00:00"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\ttuple = (int(s[i:i+2]), tuple(i+2 for i in range(int(s[i:i+2]))))\n\treturn tuple\n\ndef parse_url(url=None, baseurl=None, userinfo=None, host=None):\t\"\"\"\n\tparse the URL and set various options\n\turl: full address of the URL\n\tbaseurl: the base address of the URL, in case we need to make it absolute\n\tuserinfo: (opt) set userinfo as an URL.  this must be an ``url.UserForm"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tif s in (\"1.0\", \"0.99\"):\t# 1.0 was a special case, 2.0 should not\n\t\treturn tuple(0,2,0)\n\tif is_number(s) and int(s)>=0:\n\t\treturn (1,s)\n\telif len(s)>1:\n\t\treturn (1,int(s))\n\treturn tuple(0,None)\n\ndef is_number(s: str) -> bool:\t\"\"\"\n\tReturns True/False from strings as integers or floats\"\"\"\n\ttry:\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tparts = s.split(\"=\")\n\t#print(f\"{s}\")\n\t_1 = int(parts[0]) - 2**16\n\t_2 = int.from_bytes(s, byteorder=\"big\")\n\tif 1 < _2 < _1:\n\t\treturn _1, _2\n\telse:\n\t\treturn _2, _1\n\ndef get_platform() -> str:\n\treturn \"linux-ppc32\"\n\ndef get_arch() -> str:\n\t# this would be best if all the architectures are supported.\n\t"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tfrom. import __version__\n\ttry:\n\t\treturn int(float(s)), None\n\texcept ValueError:\n\t\treturn None, \"unable to parse {} (version)\"\n\n@conf\ndef load(\n\tconfig_name: str,\n\tcheck_path: Optional[str] = None,\n\tcheck_fn: Optional[Callable[[str], None]] = None,\n\tstrict: bool = False,\n\tversion: Optional[Tuple[int,...]] = None,\n) -> Tuple[Mapping[str, int], Dict[str, float]]:\n\t"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\ttry:\n\t\t_, v = s[0].isdigit()\n\texcept:\n\t\treturn int(s), tuple(s)\n\tversion_pieces = VCS_VERSION.split(\".\")\n\ti = 0\n\twhile i < len(version_pieces):\n\t\tif int(version_pieces[i])!= int(v):\n\t\t\tbreak\n\t\ti += 1\n\treturn int(s[0:i]), s[i :], s[i + 1 :]\n\n\ndef _get_version_from_file(filename) -> tuple[int"}]}
{"task_id": "62e4fb4d85ea9864308902e7", "prompt": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\ttry: import winreg  # @UnresolvedImport\n\texcept ImportError: print('Warning:  unable to import winreg')\n\texcept: pass\n\timport nikola\n\n\tif isinstance(cmd, str):\n\t\treturn cmd\n\n\ttry:\n\t\timport nikola_py  # @UnresolvedImport\n\texcept ImportError:\n\t\tprint('Warning:  nikola is not installed,  using normal nikola')\n\t\timport nikola\n\n\tfor modname in [\n\t\t\t\"niKOL_PY\",\n\t\t\t\"n"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tif isinstance(cmd[0], str):\n\t\tif '/' in cmd[0]:\t# https://bugs.python.org/issue8553\n\t\t\treturn cmd, [], cmd[1:]\n\t\t# https://bugs.python.org/issue8553\n\t\treturn cmd\n\telse:\n\t\tcmd = cmd[0]\n\tshell_type, shell_command = shell_prefix(cmd)\n\t# for windows cmd is like: 'cmd /c echo Hello /S/world'\n\t# and for shell type is like: /C 'echo Hello world"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tout: tuple[str,...] = cmd\n\tfor i in range(1, cmd.count(b'(') + 1):\n\t\tif cmd[i] == b'#':\n\t\t\tcontinue\n\t\tj = i\n\t\twhile j < cmd.count(b'('):\n\t\t\tif cmd[j] == b'#':\n\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\tj += 1\n\t\tnew = j + 1\n\t\tout[new] = b' '.join(cmd[i:j]).strip()\n\t\tout"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tpath_no_quotes = (cmd[0], cmd[1].replace(\"'\", \"'\\\\''\"))\n\tpath = os.getenv(\"PATH\", \"\")\n\tpath = os.path.expandvars(os.environ.get(path_no_quotes))\n\treturn cmd if not path else (cmd[0], \"{}{}\".format(*path.split()))\n\n\nclass _ShellHelper(WindowsCommand):\n\n\tdef __init__(self, cmd, **kwargs):\n\t\tsuper().__init__()\n\t\tself.cmd = cmd\n\t\tself.kwargs = kwargs\n\n"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tdef normalize_shebang(shebang):\n\t\t\"\"\"This fixes the issue #9\"\"\"\n\t\tif shebang:\n\t\t\tif shlex.isidentifier(shebang):\n\t\t\t\treturn shebang, ''\n\t\t\telse:\n\t\t\t\treturn '', shebang\n\t\telse:\n\t\t\treturn '', ''\n\n\t# Normalize first by removing all whitespace\n\tcmd = tuple([remove_unicode_whitespace(x) for x in cmd])\n\n\t# This replaces whsp with \\\\. If it is still \\. This just prevents\n\t# that from being parsed as"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tif path.isfile(cmd[0]):\n\t\tcmd = cmd[1:]\n\telif cmd[-1] == \"(\":\n\t\tcmd = cmd[1:]\n\treturn path.normcase(cmd).split(os.path.pathsep)\n\n\ndef is_path(pathname: str) -> bool:\n\tif not pathname or pathname[-1]!= os.sep:\n\t\treturn False\n\treturn pathname[0:1].lower() in [os.path.altsep, os.path.sep]\n\n\ndef is_dir(pathname:"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t# This works only on platforms that do not support parsing for\n\t# shell=False (and vice versa on Python 2), but it's not perfect because\n\t# only certain os/2 versions exist.\n\t# https://github.com/the-universe-of-users/universe/issues/17\n\tcmd = list(cmd)\n\t# Note there is no path to a \"shlex\" or other magic-argument that\n\t# will expand to just a single argument on Windows.  If python's\n\t# built-in path.expanduser() just doesn't expand back"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t# Check that the command is a single path\n\t# if not we have no path, so we return a tuple\n\tif cmd[0] not in ['.', '?'] or cmd[-1]!= os.path.sep:\n\t\treturn cmd\n\t# else return the command\n\treturn (cmd[1:-1], cmd[-1])\n\n\ndef normalize_path(path: str) -> str:\n\t\"\"\"Normalize a path\n\tThis will take the path (without. nor \\ on windows!)\n\tSo in order to normalize to something similar, we will also replace a"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tcmd = cmd[1] if len(cmd) >= 6 else cmd\n\tcmd = \" \".join(cmd[i+1:]).lower() if len(cmd) > 1 else cmd[1:]\n\tcmd = re.sub(r\"[^a-z]\", \" \", cmd, re.I)\n\tcmd = re.sub(r\"[^\\S ]\", \"\", cmd)\n\treturn cmd\n\ndef fix_paths_if_in_bin():\n\tdef check_bin_filename(bin_filename: str) -> bool:\n\t\t\"\"\"\n\t\tFixes"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tif sys.platform == \"win32\":\n\t\treturn normalize_windows_shebang(cmd)\n\telif hasattr(cmd[0], \"rsplit\"):\n\t\treturn tuple(cmd[0].rsplit(\" \", 1))\n\telse:\n\t\treturn cmd[0], cmd[1:]\n\n# Adapted from https://github.com/rakur/python-prompt\ndef normalize_windows_shebang(cmd: tuple[str,...]) -> tuple[str,...]:\n\t\"\"\"Returns normalized command after shell-like quoting removed\"\"\"\n\tif sys"}]}
{"task_id": "62b8d27a48ba5a41d1c3f4c6", "prompt": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tdef cached_decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tmemo = None\n\t\t\tkey = func.__name__ + '_' + str(hash(args)) + '_' + str(kwargs)\n\t\t\tif key in cache:\n\t\t\t\tcached = cache[key]\n\t\t\telse:\n\t\t\t\tcached = cache[key] = memo = func(*args, **kwargs)\n\t\t\t\tlock = False\n\t\treturn wrapper\n\treturn cached_decorator\n\ndef uncached(cache, key=hashkey"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tif cache is not None and cache.has_key(key):\n\t\treturn cache[key]\n\tif not hasattr(cache, 'lock'):\n\t\tif lock is None:\n\t\t\tcache.lock = Lock()\n\t\t\tcache.lock.acquire()\n\t\tcache.has_key(key)\n\t\treturn cache[key]\n\tf = memoized(cache, key, cache.add, lock)\n\tf.__wrapped__ = cache\n\treturn f\n\n\ndef memoized(cache, key, wrapper, lock=None):\n\t\"\"\"\n"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\treturn _cached(cache, lambda f: f, key, lock)\n\n\ndef _cached(cache, f, key, lock):\n\tif not cache:\n\t\treturn f\n\tval = cache.get(key)\n\tif val is not None:\n\t\tf = val\n\n\tif f is None:\n\t\treturn _cached(cache_no_key, lambda: None, key, lock)\n\telse:\n\t\treturn f\n\n\ndef _get(keygen, key):\n\tif not hasattr(keygen, \"cache\"):\n"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\treturn _cached(cache, key, _defaultcache, lock)\n\n\ndef _defaultcache(\n\tfkey=None, fval=None, expire=None, use_lock=True, maxsize=None\n):\n\t\"\"\"\n\tdecorator to cache the result of a function. Has 2 parameters fkey\n\tand fval.  If given, key is used for caching instead of the function\n\tname.\n\n\tIf fval is given, the result is stored in a cache.\n\n\tIf both fkey and fval are None the cache is created.\n\n"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t@wraps(cache)\n\tdef wrapper(func):\t\t# check lockable\n\t\targs = [func, lock]\n\n\t\t# cache result if necessary\n\t\tif (cache or\n\t\t\t(cache and key is not None and key in cache) or\n\t\t\tcache and cache[key] and key in cache):\n\t\t\t\targs += cache[key]\n\t\t\treturn func(*args)\n\t\treturn wrapper.__wrapped__(func)\n\treturn wrapper\n\n\ndef memoize(cache = {}, key=hashkey, lock=None):\t#\n"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t# This will run without the cache if called repeatedly\n\tdef inner(func):\n\t\tcache.setdefault(key(func),[]).append(func)\n\t\treturn func\n\treturn _wrap(\n\t\tinner,\n\t\tfunc.__name__,\n\t\tfunc.__doc__)\n\ndef lru(cache, key=hashkey, lock=None):\n\tdef inner(func):\n\t\treturn functools.lru_cache(cache)(key=key, func=func)\n\treturn _wrap(inner,\n\t\tfunc.__name__,\n\t\tfunc.__"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t@functools.wraps(lock)\n\tdef _cache(func, *args, **kwds):\n\t\twhile True:\n\t\t\tresults = cache.get(key, (func, *args, **kwds))\n\t\t\tif results == b'' and len(results) == 4:\t# pragma: no cover\n\t\t\t\tcache_hit_key = weakref.ref(\n\t\t\t\t\tcache[key, (func, *args, **kwds)],\n\t\t\t\t\ttimeout=cache_timeout)\n\t\t\t\tif cache_hit_key is not None:\n\t\t\t\t"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tdef decorator(f):\n\t\t@wraps(f)\n\t\tdef wrapper(*args, **kw):\n\t\t\tkey = hashkey((id(f),) + args)\n\t\t\tsave = cache.get(key, None)\n\t\t\tif save is None: save = save_result(f(*args, **kw))\n\t\t\tif save is False:\n\t\t\t\tcache[key] = save\n\t\t\t\treturn False\n\t\t\telse:\n\t\t\t\tcache[key] = save\n\t\t\t\treturn save\n\t\treturn wrapper\n\treturn decorator\n\ndef"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tdef cache_memoized(self, *args, **kw):\n\t\tc_name = 'cached(%s)' % self.func\n\t\tif not lock:\n\t\t\tlock = Lock()\n\t\t\tassert not lock.owned()\n\t\tc = cache[c_name] # cache will be created in decorators\n\t\t# check to see if we had a cache miss...\n\t\tif c is not None:\tc.__dict__ = kw\n\t\treturn c(self.func, *args, **kw)\n\treturn cache_memoized\n\ndef memoize"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\"Make the function's call-back thread-safe (reentrant call-back)\"\n\t\"with a memoizing cache.\"\n\tnargs = len(args)\n\tif nargs not in (args):\n\t\tpass\n\telse:\n\t\tcache = _CacheDecorator(cache)\n\t\tif inspect.ismethod(args[nargs - 1]):\n\t\t\tfunction=args[nargs - 1]\n\t\t\targs[nargs-1] = cached(cache, key=function, lock=lock)\n\t\telif inspect.ismethod(args[nargs-2]):\n\t\t"}]}
{"task_id": "62b8d24048ba5a41d1c3f49f", "prompt": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tdef __init__(self, cache_size, ttl, timer=time.monotonic, typed=False):\n\t\tif not callable(cache_size) or isinstance(cache_size.func, types.FunctionType):\n\t\t\traise TypeError(\"ttl must be a callable of\"\n\t\t\t\t\" the form function (arg): \"\n\t\t\t\t\"function.cache_size, \"\n\t\t\t\t\"arg.cache_size (type: function, default: LRU)\")\n\t\tself.cache_size = cache_size\n\t\tself.ttl = ttl\n\t\tif not typed:\n"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tdef wrapper(func, *args, **kwargs):\t\t\t# @Hack: do not use closures in decorators\n\t\t# note: using (v * 2) is actually cheaper than (v * 3),\n\t\t#       while the other will slow down the computation\n\t\tdef wrapped(*args, **kwargs):\n\t\t\t_cached = False\n\t\t\tkey = args[0]\n\t\t\tmaxsize = maxsize if maxsize > 0 else int(_cached_cache_maxsize(key))\n\t\t\tif ttl is not None and ttl >"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tfrom pyo import memoize\n\tf = memoize(lambda x: x or -1, timeout=maxsize * 2)\n\treturn functools.wraps(f, timeout=timeout, timer=timer, type=typed)\n\ndef get_instance():\n\treturn G.app.get_base_instance()\n\ndef stop():\n\t\"\"\"Stops the Gevent-Pygeet daemon.  Use this function instead of\n\t\"python -m pygee\" at the beginning to temporarily stop the\n\tGEOSEvent-Pygeet daemon. \"\"\"\n\ttry:"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t__doc__ = \"\"\"\n\n\t'''%locals()'''\n\tdef _memoized_memoize(func):\n \n\t\tclass M(object): \n\t\t\tlast_used = 0\n \n\t\t\tdef __call__(self, *args):\n\t\t\t\tif M.last_used < M.last_used+1 and len(args)>0:\n\t\t\t\t\tmemo = args[M.last_used]\n\t\t\t\t\tmemo[1] = M.on_result(memo[1])\n\t\t\t\t\treturn memo[1]\n\t\t\t\tif not args:\n\t\t\t\t\tmemo"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t_memoize_lru = lru # back door to python-lru in Python 3.3\n\tdef __init__(self, memoize, maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\t# We use the same algorithm as a built-in decorator of the\n\t\t# following pattern:\n\t\t#\n\t\t#\n\t\t#   @lru_cache(maxsize=100*1024)\n\t\t#   def test(self):\n\t\t#      ...\n\t\t#\n\t\t"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tdef __init__(self, f, *args, **kwargs):\n\t\tif maxsize<=0:\n\t\t\traise ValueError, \"Cache size must be at least 1\"\n\t\tself.maxsize = maxsize\n\t\tself.item_count = 1\n\t\tself.timeout = timer()\n\t\tself.max_cache_size = min(maxsize, INT_MAX)\n\t\tself.timeout_cache = None\n\t\tself.typed = typed\n\t\tself.cache_type = kwargs.get(\"cache_type\", \"list\")\n\t\t#print"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t__slots__ = 'cache','maxsize', 'ttl', 'timer', 'typed'\n\t_slots_cache = {}\n\t_cache_lock = RLock()\n\tdef __init__(self, f, *args, **kwargs):\n\t\tif not callable(f):\n\t\t\traise TypeError(\"Expected func to be a callable\")\n\t\tself.cache, self.maxsize, self.ttl, self.timer, self.typed = f(*args, **kwargs)\n\tdef __call__(self, f, *args, **kwargs):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tdef wrapper(func):\n\t\tif typed:\n\t\t\t# Typed time-to-live has a limitation, it needs to be a\n\t\t\t# callable that can be passed in to memoize if it is more\n\t\t\t# efficient\n\t\t\treturn lambda: func(timer)  # this does a best effort to call function\n\t\t# Must either do a memoise decorator, or a better function to\n\t\t# cache, if no callback\n\t\telif not cache:\n\t\t\tf = memoize(func)\t# memoize the function\n\t\t\tf = LRU("}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tdef decor(func):\n\t\tLRUClass = collections.UserDict()\n\t\tTTLClass = collections.UserDict()\n\n\t\t@functools.wraps(func)\n\t\tdef wrapper(self, *args, **kw):\n\t\t\tlru = LRUClass()\n\t\t\tttl = TTLClass()\n\t\t\tt = timer()\n\n\t\t\t__t0 = t\n\t\t\t__ret = 0\n\t\t\t__i = 0\n\n\t\t\tfor result in gen_lru(lru, ttl, cache=maxsize, typed=typed, t=t):\n"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t_ttl, _ttl_max = ttl, ttl\n\tdef new_fun(func, arg):\n\t\t_args = ('returning', ) + tuple([arg] * (_ttl_max - len(_ttl) - 1)) + \\\n\t\t      ('memoizing', _ttl_max)\n\t\treturn lru_memoize(_args, func, self)\n\treturn new_fun, ((arg,) + (tuple(map(new_fun, func.args)),))\n\n\nclass lru_memoize(CacheDecorator):\n\n\tdef __init__(self"}]}
{"task_id": "62b8d23b48ba5a41d1c3f49a", "prompt": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tif typed:\n\t    def __cached__(self, *args, **kwds):\n\t\treturn self._cache[(0, (size(self),),) + args + kwds]\n\telse:\n\t    def __cached__(self, *args, **kwds):\n\t\trv = self._cache[(0, (size(self),),) + args]\n\t\treturn rv[-1] if rv else None\n\t\n\treturn _MemoizedFunctionDecorator(self, typed)\n\n\n\n\n_cached_cache = cached_memoize(_cached)"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t@wraps(func)\n\tdef wrapper(*args, **kwargs): return MostRecentMemoized(*args, **kwargs, func)\n\treturn wrapper\n\n\ndef most_recent(maxsize=128):\n\t\"\"\"Returns a decorator to mark a function as the most recent so that the arguments it takes will be the least recently used.\"\"\"\n\treturn MostRecentMemoized(mru_cache, maxsize, False)\n\n\ndef cache(func=most_recent, maxsize=128):\n\n\t\"\"\"The memoization decorator that caches the result of `func(args, kwargs,"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef decorator (func):\n\t\treturn cache(maxsize, func, typed)\n\treturn decorator\n\ndef cache (maxsize, key, typed):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing named function\n\tthat saves up to `maxsize` results for key.\t If typed is True,\n\tthe key will be treated as a typed array, as if typed=True.\n\n\tWhen typed=True the key and func are the output of a function\n\tthat caches the result, like functools.lru.\n\t\"\"\"\n\tdef inner (*arg, **kw):"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef wrapper(f):\n\t\t@wraps(f)\n\t\tdef wrap(self, *args, **kwargs):\n\t\t\tmaxresults = self.maxsize\n\t\t\tif maxresults > 0:\n\t\t\t\tresult = f(self, *args, **kwargs)\n\t\t\t\tif (not isinstance(result[0], tuple) or\n\t\t\t\t    len(result) > maxresults):\n\t\t\t\t\ttotal = len(result)\n\t\t\t\t\tfor resultlist in (result[0][i:i+maxresults]\n\t\t\t\t\t\t\t for i in range(0, len("}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef __init__(self, func):\n\t\t\"\"\"\n\t\tFunction to decorate with `mru_cache`.\n\n\t\tParameters\n\t\t----------\n\t\tfunc : callable\n\t\t\tCallable function to decorate with `mru_cache`.\n\n\t\tKeyword arguments\n\t\t----------\n\t\tmaxsize : integer\t\tMaximal number of results to remember.\n\t\ttyped : boolean\t\t\tWhether the decorated function\n\t\t\t\t\t\treturns a typed result.\n\t\t\"\"\"\n\t\targnums = func.__code__.co_argcount\t\t# # # argnums\n"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\treturn _MRUListCacheDecorator(maxsize, typed=typed)\n\n\n### Internals ###\ndef _MRUListCacheDecorator(maxsize, typed=False):\n\t\"\"\"\n\tA decorator that saves N**2 values into a list, if possible.\n\tIt uses the built-in lru if available or the built in mru, if\n\tnot.\n\t\"\"\"\n\t_MRUCache = lrulib.LRUCache(maxsize, typed=typed)\n\t_MRUListCache = [_MRUCache(maxsize * (i + 1))\n\t\t\t\t\tfor i in"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t_wrapper = wrap(lambda f: getattr(f, 'cache', {}).update, typed=typed)(_cache)\n\treturn _wrapper\n\n#include <linux/string.h>\n#include <sys/mman.h>\n#include <asm/io.h>\n#include <asm/io_apic.h>\n\n#include <linux/console.h>\n#include <linux/tty.h>\n\nstatic void __iomem *ioend;\n\n#if __LINUX_CONSOLE_IO0_OFFSET!= IOMEM_OFFSET\n#error \"__LINUX_"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef decor(func):\n\t\tif typed:\n\t\t\t__wrapped_memoizations__.append((func, None))\n\t\treturn MRUDecorator(func)\n\tdef decorate(func):\n\t\tif isinstance(func, types.FunctionType):\n\t\t\treturn decor(func)\n\t\tif typed:\n\t\t\tcache = __wrapped_memoizations__\n\t\t\t__wrapped_memoizations__ = []\n\t\t\treturn __cache_decorator_cls(\n\t\t\t\tcache, maxsize, func, cache.__name__)\n\t\tcache = __wrapped_memoizations__"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tf=lambda x:-1\n\n\tclass Memoable(Memoable):\n\t\t\"\"\"A memoable decorator that saves up to `maxsize` results to a new\ninstance, each getting it's own copy (without reference counting).\n\n\t\tIf passed a non-callable ``__init__()``, it will only run the lambda\n\t\t`f` once when the memoizer is retrieved.\"\"\"\n\n\t\tdef __init__(self,f,maxsize=128,typed=False):\n\t\t\tif f is None:\n\t\t\t\treturn\n\n\t\t\tself.f=f\n\t\t"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef wrapper(func):\n\t\tif typed:\n\t\t\tfunc = lambda maxsize: cache_typed(maxsize, None, func)\n\t\telse:\n\t\t\tfunc = lambda maxsize: cache_untyped(maxsize, None, func)\n\t\t@wraps(func)\n\t\tdef wrapper_call(*args, **kargs):\n\t\t\t\"\"\"\n\t\t\tDecorator for cacheing a function with a\nmost recent item until the result is found.\n\n\t\t\tParameters\n\t\t\t----------\n\t\t\tmaxsize:  integer (default: 128)\n\t\t\tfunc"}]}
{"task_id": "62b8d23948ba5a41d1c3f498", "prompt": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tdef __init__(self, func, memo=None):\n\t\tself.maxsize = maxsize\n\t\tif memo is None:\n\t\t\t__weakref__ = self\n\t\tmemo[len(self)] = self\n\t\tself.cache = {}\n\t\tif not typed:\n\t\t\tfunc = (lambda k,v=self:memo[k] || (not isinstance(v, (list,dict,set)) and len(v)<maxsize))\n\t\telse:\n\t\t\tname = type(func).__name__\n\t\t\tfunc = wrap_namedt"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tif not memcached: raise Exception('requires memcached module')\n\n\t_ = typed\n\tdef _decorated(func):\n\t\tif not memcache: return func\n\n\t\tdef memoized(key, initial=None):\t\t# @UnusedVariable\n\t\t\treturn func(key, initial)\n\t\tmemoized.func = func\n\t\treturn memoized\n\treturn _decorated\n\n\n#\n# Decorator to limit the amount of time to which we will wait (with\n# realtime) before calling func().\n#\n\n\ndef timeout(seconds=0"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tdef wrapper(func):\n\t\twrapper = self._cache[(maxsize, typed)] = lru_cache(maxsize, typed)()\n\t\tlast_key = None\n\t\twrapped = wrapped_func = None\n\t\twrapped_func = wrap_decorator(wrapped_func, func)\n\t\twhile wrapped_func is not None and wrapped_func() is not None:\n\t\t\tfunc = wrapped_func\n\t\t\twrapped = wrapped_func = wrapped_func.__wrapped__\n\t\t\tself._cache[(maxsize, typed)] = lru_cache(maxsize, typed)()\n\t\t\tlast"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t# from future.builtins import lru_cache\n\t# @lru_cache()\n\tdef wrapper(*args, **kwargs):\n\t\tm = (maxsize, typed)\n\t\tcache = lru_cache(maxsize=maxsize)\n\t\ttry:\n\t\t\tfor e in cache(func=args[0], keyfunc=args[1], valuefunc=args[2]):\n\t\t\t\tyield (m(m, e, args[2]), args[1], args[2], e)\n\t\texcept:\n\t\t\treturn args[0]()\n\n\treturn wrapper\n\n"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tMAX_LRU = 50\n\tdef __init__(self, func):\n\t\t# Making sure the cache is never full\n\t\tlifocache.checkcache()\n\t\tcache = []\n\t\tif typed:\n\t\t\tcache = cache_typed(func)\n\t\telse:\n\t\t\tcache = default_cache\n\t\tsitems, streamp = lru_ext_stats[func.__name__]\n\t\tsitems.sort()\n\t\tsitems.reverse()\n\t\tif args <= 0:\n\t\t\tself._cached_function = func"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\treturn MemoizeFunct(\n\t\t\t\tMemCache(maxsize, typed)\t\t,\n\t\t\t\t'__builtin__'\t\t\t,\n\t\t\t\ttyped=typed,\n\t\t\t)\n\n\nclass Cache(BaseCache):\n\t\"\"\"Cache decorator to work with a cache object.\"\"\"\n\tdef __init__(self, *largs, **kwarg):\n\t\tkwarg['maxsize'] = maxsize = kwarg.get('maxsize', 0)\n\t\tself.maxsize = maxsize\n\t\tself.expires = kwarg.get('expires', expiry)\n\t\t"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tdef wrapper(func):\n\t\tdef wrapper_wrapper(self, *args, **kwargs):\n\t\t\tkeys = list(self.keys())\n\t\t\tif typed:\n\t\t\t\tlru_cache = {key:self[key]._lru_cache for key in keys[:4]}\n\t\t\t\tself_cache = {key:lru_cache[key] for key in keys[4:]: self[key]._lru_cache}\n\t\t\t\tnew_cache = {key:lru_cache[key] for key in keys[:4]}\n\t\t\t\tif len(keys) == 4:"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tif typed:\n\t\treturn lambda f, *a, **kw: memoized(lambda *x: memoize_lru(f)(*x, **kw))\n\telse:\n\t\treturn memoized\n\n\ndef memoize_lru(f):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tf = _coerce_memoize_to_lazily(f)\n\n\t# Wrap with None because the caller just needs to keep"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t# decorator is also available under Python 3.1 and removed to be\n\t# compatible with Python2.4.\n\n\tdef __init__(self, memoizer=util.lru_cache,\n\t\t     cache_size=0, typed=False):\n\t\t\"\"\"\n\t\tinitialize a callable with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU) algorithm.\n\n\t\tmemoizer is a callable function with the following signature:\n\t\t(*args, **kwds) -> result...\n\n\t\tThe"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tmemoize = functools.lru_cache(maxsize=maxsize)\n\tif isinstance(maxsize, _int_types):\n\t\treturn functools.partial(memoize, maxsize=maxsize)\n\tif typed:\n\t\treturn functools.partial(memoize, typed=False)\n\telse\t# for typed caching we don't want to mess with memoize\n\t\treturn functools.partial(memoize)\n\nclass LRUItem(object):\n\t\"\"\"A function that takes a value and returns an index\n\tto that value.  Optionally also adds the index to"}]}
{"task_id": "62b8d23748ba5a41d1c3f496", "prompt": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t# This decorator should be applied if memoizing functions\n\t# returns a dictionary.\n\tdef __init__(self, wrapped, maxsize=128, typed=False):\n\t\tself.wrapped = wrapped\n\t\tif typed: self.valuefunc = self.typed\n\t\telse: self.valuefunc = self.value\n\t\tself.maxsize = maxsize\n\t\tself.__cache = {}\n\n\t@property\n\tdef value(self):\n\t\tvalue = self.wrapped(*self.valuefunc())\n\t\tif self.valuefunc.__name__ == 'lf"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tcache = {}\n\t__cache = {}\n\tdef __init__(self, f, maxsize=128, typed=False):\n\t\t\"\"\"\n\t\tConstruct a memoizing callable with a given `f`.\n\n\t\t:arg f: A function with `f` as its first argument.  The\n\t\tfirst argument will be memoized by the function.\n\n\t\t:arg maxsize: Set the maximum size of the cache.\n\n\t\t:arg typed: If the cache is a dictionary of tuples of keys\n\t\tand length respectively, the values of the tuples are"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tmemo = {}\n\tdef _cache(func):\n\t\tif typed: func(memo)\n\t\tmemo[func.__name__] = func\n\t\treturn func\n\treturn _cache\n\ndef lrucache(*args, **kwargs):\n\t\"\"\"\n\tDecorator to limit the amount of items that can be cached.\n\tUsage:\n\t\t@lrucache(*args, **kwargs)\n\t\"\"\"\n#\tmemo = {}\n#\tfor _, arg in sorted(kwargs.items()):\n#\t\tmemo[arg] = 1\n#\t\tlrucache.put("}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tcached_data = []\n\tdef __memcache_cached_init_func(*args, **kwargs):\n\t\t# Save some data\n\t\tif maxsize<=0:\n\t\t\treturn None\n\t\tif not typed:\n\t\t\tf = pickle.dumps(cached_data, protocol=2)\n\t\telse:\n\t\t\tf = cached_data\n\t\tf.append(args)\n\t\tf.append(kwargs)\n\t\treturn f\n\t#return cached_data\n\treturn __memcache_cached_init_func\n\n##\ndef"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tif typed:\n\t\treturn memoize_typed(lfu, (maxsize,), {})\n\telse:\n\t\treturn memoize(lfu, (maxsize,), {})\n\ndef memoize_dyn_tree(memo, tree_type):\n\tif hasattr(tree_type, \"get_cache\"):\t\t# Python 3\n\t\treturn memoize(tree_type.get_cache, (), {})\n\telse:\n\t\t# Else, just get the cache directly.\n\t\t__cache = memo.get(tree_type, {}).__get_cache__()\t\t\t\t\t\t\t\t"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tif (isinstance(LFu, type) and not isinstance(lfu, types.FunctionType) and\n\t\tLFu is typed and type(LFu)() is Callable_type):\n\t\treturn lfu\n\telse:\n\t\treturn functools.lru_cache(maxsize)(LFU, typed)\n\ndef maxdict(*args, **kwds):\n\t\"\"\"\n\tmaxdict(dictionary, max_size) computes the maximum number a dictionary\ncan possibly have, and it returns a nested dictionary with at most\nmax_size entries, where each entry is of form (key,"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\treturn FunctionDecorator(lambda f,a=maxsize,b=typed: lfu(*a,**b))(f)\n\ndef _lfu_cache(func):\n\t\"\"\"Returns a function that saves results in cache by reducing the\ncache size and storing the function. The cache reduces a function in\nO(lg k) over N calls to `func`.\"\"\"\n\n\tdef f(memo=Memoize):\n\t\tfor o,l,k in eval(func).items(): # pragma: no cover\n\t\t\tif isinstance(k,List):\n\t\t\t\tfor i,"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\treturn functools.wraps(lambda f: memoize(lambda x: f, typed), (memoize))\n\ndef get_current_cache():\n\t\"\"\"\n\tA function that retrieves the value of @cache.  The cache must be saved\n\tbefore any function is called, so the cache does not need to be cleared.\n\n\tReturn None if function is not set.\n\t\"\"\"\n\tif 'lfu.cache' in globals():\n\t\treturn globals()['lfu.cache']\n\ndef set_current_cache(cache):\n\t\"\"\"\n\tA function"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tif not typed and len(functools.lru_cache)>maxsize * 2:\n\t\treturn None\n\tf = functools.lru_cache.get(maxlen=maxsize)\n\tif not f:\n\t\tf = functools.lru_cache.cachekeys('_lfu_cache__').pop(1)\n\treturn f\nlbf = functools.lru_cache.LruHandleFunCache\nlfuw = functools.lru_cache.LruUnicodeVersionWrapper(r)\nllf = functools.lru_cache.LRUHandleFunCache\n\n\nclass"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t__wrapped__ = cache(maxsize=maxsize, typed=typed)\n\treturn __wrapped__\n\n/*\n * Copyright (C) 1998-2018 ALPS Collaboration. See COPYRIGHT.TXT\n * All rights reserved. Use is subject to license terms. See LICENSE.TXT\n * For use in publications, see ACKNOWLEDGE.TXT\n */\n\n/** @file test.cpp\n    @brief this file contains the unit test of the pomdp client module\n*/\n\n#include \"alps/settings.hpp\"\n#include \"alps/options.hpp\""}]}
{"task_id": "62b8d22f48ba5a41d1c3f488", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\ttry:\n\t\tfor key in self.items:\n\t\t\tvalue = self[key]\n\t\t\tnext (key,value)\n\texcept KeyError:\n\t\tself.items\n\t\tprint(\"pop: KeyError! popitem(0):%s\"%(key))\n\treturn key, value\n\ndef iteritems(self):\n\tkeys = self.keys()\n\tfor key in keys:\n\t\tyield key, self[key]\n\ndef iterkeys(self):\n\tfor key in self.keys():\n\t\tyield key\n\ndef iteritems_"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\tif self._stack is not None:\n\t\treturn self._stack.pop()\n\telse:\n\t\treturn None\n\n\tdef pprint(self, stream = sys.stdout):\n\t\tif self._stack:\n\t\t\tstream.write((\"Key: %3d = %s, Value:\\n\" % (self._key, self.dict[self._key]))+)\n\t\t\tfor key, value in self._stack:\n\t\t\t\tstream.write((\"%3d %s=%s\\n\" % (key, safestr(value), safestr(self.dict"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\tif self.firstkey < 0:\n\t\t\tself._len, self.firstkey = self._len, len(self.data)\n\t\tname = self.keyat(self.firstkey)\n\t\tif name in self._items:\n\t\t\tself._items.pop(name)\n\t\treturn name, self.data.pop(self._len)\n\tdef __delitem__(self, name):\n\t\tname, value = self._items.pop(name)\n\t\tif not self.data:\n\t\t\tself._len -= 1\n\t\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\twhile True:\n\t\ttry:\n\t\t\treturn self.dict['__popitem__'][1]\n\t\texcept KeyError:\n\t\t\tbreak\nself.dict['__popitem__'][1] = None\nreturn key, value\n\ndef pop(*args):\n\t\"\"\"\n\tRemove and return all the items that match the given criteria, in\n\tkey, value, and with the given order.\n\t\"\"\"\n\treturn chain((k, v) for k,v in popitems(args, order=True))\n  \n\n\nclass File:\n\t\"\"\"\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\ttry:\n\t\tidx = self.index[self.index.next_to_key(*self.index.keys())]\n\texcept StopIteration:\n\t\tidx = self.index[self.index.next_to_key(*self.index.keys())+1]\n\tself.list.remove(idx)\n\tself.length = self.length - 1\n\tself.mod = self.mod - self.length\n\treturn idx, self.mod, None\n\n\ndef _extend(m1, m2):\n\t\"\"\"\n\tExtend a map for"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\treturn (k for k in list(self) if k!= '_')\n\n\n#---- docstrings\n\n_type_str = \"\"\"\ntype (%s)\n%s\n\"\"\"\n\n_func_str = \"\"\"\ndef %s(%s):\t%s\n\"\"\"\n\n_method_str = \"\"\"\ndef %s(%s):\n%s\n%s\n\"\"\"\n\n_type_str_doc = \"\"\"\nType-specific docstring for %s.%s.\n\nThe docstring of %s should look something like::\n\n    >>> print type(a"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\tmydict = self.dict = newdict()\n\tfor (key, value) in self.dict.items():\n\t\tif key in self.dict:\n\t\t\tdel self.dict[key]\n\treturn (mydict[0], mydict[1])\n\t\n# dict_test:\ndict_test = dict()\ndict_test[\"this\"] = \"a\"\ndict_test[\"this\"][\"two\"] = 2\ndict_test[\"this\"][\"three\"] = 3\ndict_test[\"this\"][\"four\"] = 4\ndict_test[\"this\"][\"five\"] ="}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\tlst = [k for k, v in self.__dict__.items() if not k.startswith(\"_\")]\n\tdel(self)\n\treturn lst[0] if isinstance(lst, dict) else lst\n\n/* -*- c++ -*- */\n/*\n * Copyright 2012-2015 Free Software Foundation, Inc.\n *\n * This file is part of GNU Radio.\n *\n * SPDX-License-Identifier: GPL-3.0-or-later\n *\n */\n\n#ifdef HAVE_CONFIG_H\n#include \"config.h\"\n#endif\n\n#include \"agc."}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\ttry:\n\t\tself.update()\n\t\treturn (self._pairs.get(self.count()), ) + self._pairs\n\texcept:\n\t\treturn\n\n\tdef clear(self):\n\t\tself._pairs = {}\n\t\tself._count = 0\n\n\tdef update(self, values=None):\n\t\t\"\"\"Adds/replaces/removes a (key, value) pair on top of the stack.\n\t\t\"\"\"\n\t\tself._pairs.update(values or {})\n\n\tdef pop(self, key=None, default=None"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\tif len(self.dict)==0:  \t\treturn ([], None)\n\tkey=self.dict.popitem()\n\treturn self.dict.copy(), self.dict.popitem()\n\n\nif __name__ == '__main__':\n\t\"\"\"\n\tCreate a dictionary with three attributes: a dictionary, an iterator, and a key.\n\tIn this example, both attributes are called 'x' and 'y'.\n\t\"\"\"\n\tprint key=\"xy\"\n\tprint \"x\"\n\tprint \"y\"\n\tprint \"x\"\n\tprint key\n\t"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47e", "prompt": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t", "suggestions": [{"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tif default is not None:\n\t\tD.setdefault(key, default)\n\n\nclass _DictMixin:\n\n\t@staticmethod\n\tdef dictupdate(d,**kwds):\n\t\tif _use_setdict(d):\n\t\t\tD.update(d)\n\t\telse:\n\t\t\tdict_update2(kwds, d)\n\n\t@staticmethod\n\tdef update(d,**kwds):\n\t\tif _use_setdict(d):\n\t\t\tD.update(d)\n\t\telse:\n\t\t\tdict_"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tres = dict()\n\tfor k,v in self.__dict__.items():\n\t\tif v is not None and k!= key:\n\t\t\tres[k] = v\n\t\telif default is not None:\n\t\t\tres[k] = default\n\tif res == {}:\n\t\tself.__dict__.update(res)\n\treturn res\n\ndef dictsort_key(t, k):\n\tr = t.copy()\n\tr.pop(k)\n\tfor x in t:\n\t\tfor y in t:\n\t\t\tif k =="}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tself.defaults = dict()\n\tself.defaults[key] = default\n\tvalue = self._get_default(key)\n\tif value is None:\n\t\ttry:\n\t\t\tvalue = self._default_values[key]()\n\t\texcept:\n\t\t\tvalue = copy(default)\n\t\tself._set_default(key, value)\n\treturn value\nw.setdefault = C.setdefault = setdefault\n\nw.define_method(\"define_alias\", w.define_function, \"alias('name', 'value')\"))\nw.define"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\treturn self.__dict__.setdefault(key, default)\n\ndef setitems(D, items):\n\t\"\"\"Dictionary constructor.\n\tSet items in dictionary D.\n\n\t\tD = {'foo','bar','baz','quux'}\t\t\t-> result = {'foo'='bar', 'baz'='quux'}\n\t\"\"\"\n\tfor k, v in items:\n\t\tif isinstance(v, D):\n\t\t\tD[k]=v\n\t\telse:\n\t\t\tD.setdefault(k, v)\n\ndef dict_copy(d):\n\t\"\"\" Returns"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\treturn  None\n\n\ndef copyto(x):\n\t\"\"\"Helper for dictionary.copyto()\n\t\"\"\"\n\twith\tgcref(x):\t# delete the reference\n\t\twith\tc.lock():\t# make a copy of the table for myself\n\t\t\tc.pop()\t\t# and do a pop on it\n\t\twith\tc.lock():\t# and then take over it, or gc.collect()\n\t\t\tr=copy.copy(c)\t# and overwrite itself\n\t\t\tr.__delattr__(\"_lock\")\n\t\ttry"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tself[key] = default\n\n\ndef getset(self, key, default=None):\t\"\"\"Returns the value of the key if it exists, otherwise set it to default.\"\"\"\n\ttry: return D.get(key,default)\n\texcept: self[key] = default\n\treturn default\n\n\ndef __add__(self, key,*args):\t# For backward compatibility: args[2:] -> args[0], args[1:]\n\tif hasattr(self,\"__sets\"):\n\t\td = self.__dict__\n\t\tself._update_d(d)"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tsetattr(self, key, default)\n\ndef keys(self):\t\t\"\"\"List of the keys of D\"\"\"\n\treturn list(self.keys())\n\ndef has_key(self, key):\t\"\"\"Has key?\"\"\"\n\treturn key in self.keys()\n\ndef keys_iter(self):\t\t\"\"\"Get all keys\"\"\"\n\tkeys = list(self.keys())\n\twhile 1:\tyield keys\n\ndef setdefault_iter(self, *keys):\n\tfor key in keys:\n\t\tsetattr(self, key, self.getdefault("}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\ttry:\n\t\tv = self[key]\n\texcept:\n\t\tv = default\n\t\tif self._is_deleted(key+self.SUFFIX):\n\t\t\traise IndexError(\"Key is not found: key=%s\" % key)\n\tself._set(key, v)\n\n\treturn v\n\ndef getitem(keys, default=None):\n\tif not keys:\n\t\tif default is None:\n\t\t\ttry:\n\t\t\t\treturn next(iter(self))\n\t\t\texcept StopIteration:\n\t\t\t\treturn default\n\t\telse"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\ttry:\n\t\tval = self.__getitem__(key)\n\texcept:\n\t\tval = default\n\tD.__setitem__(self, key, val)\n\treturn val\n\ndef __getattribute__(self, attrname):\n\twhile attrname in cls.__dict__:\n\t\t# print(\"%-12.15e%-18.15e %-18.15e %-15.15e\" %(self.__dict__[classname], attrname, cls.__dict__[classname][attrname],))\n\t\tself = cls.__dict__[classname][attrname]()\n\treturn self\n\n\n#----------------"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\ttry:\tself[key] = default\n\texcept:\tself[key] = default\t\n\n\t# Add or change the default keyword\n\tif not hasattr(self.default, key):\n\t\tself.defaults.append(key)\n\n\t# Create new table if needed\n\tself.key.append(key)\n\n\t# Return the default keyword\n\treturn default\n\ndef getdefault(self, key, default=None):\t\"\"\"\n\tD.get(k,d) -> (P if k in D) and (d else P), also set D["}]}
{"task_id": "62b8d22948ba5a41d1c3f47c", "prompt": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\treturn self.__dict__.get(key,default)\n\n\n\ndef setdefault(k,d=None):\n\t\"\"\"\n\tD.setdefault(k[,d]) -> D[k] if k in D, else d.\n\n        If `d` is None, the default value for `k` will be returned.\n        \"\"\"\n\treturn self.__dict__.setdefault(k, d)\n\n\n\ndef iterkeys(d=None):\t\"\"\"Keys in dict\"\"\"\n\treturn d.iterkeys()\n\n"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\tif key in self: return self[key]\n\tif default is None: return self.defaults[key]\n\treturn default\n\n\t# Override dict methods\n\n\tdef __contains__(self, key):\n\t\tk = key\n\t\tif isinstance(key, basestring): k = key.lower()\n\t\treturn k in self.keys()\n\tdef __delitem__(self, key): del self[key]\n\tdef __getitem__(self, key):\n\t\tk = key\n\t\tif isinstance(key, basestring): k = key.lower()"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\tif default is None:\n\t\td = self.__dict__.get(key, None)\n\t\tif d is None: return default\n\t\tif not isinstance(d,DObject): d = DObject(d)\n\t\treturn d.get(key)\n\n\tif str(key).startswith(\"G_\"):\n\t\tfrom.. import log\n\t\tlog.debug(f\"G_[{key}] == {default}\")\n\t\treturn self.get(str(key).lstrip(\"_\"))\n\telse:\n\t\treturn self.__dict__[key]\n"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\treturn self.__getitem__(key, d=default)\t\n\t\t\n\tdef __getslice__(self, i, j):      #@modified 20171028\n\t\n\t\n\treturn self\n\n\n\t@property\n\tdef __setslice__(self, i, j, val):\n\t\tself.__setitem__(Key(i), val)\n\t\n\tdef append(self, a):\n\t\tself.extend(a)\n\t\n\t@property\n\tdef __delslice__(self, i, j):\n\t\t# this is not strictly required: it would be good"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\ttry:\n\t\treturn D[key]\t# already a member...\n\texcept KeyError:\t\t# key is not in hash table...\n\t\tD[key] = d\t\t# d defaults to None\n\t\treturn d\t\t\t#... in class __init__\n\ndef __missing__(key,default=None):\n\t\"\"\"\n\tHelper function to find a data member\n\n\t:param key: The data member key.\t\n\"\"\"\n\ttry:\n\t\tD[key]\n\t\treturn default\n\texcept KeyError:\n\t\treturn default\n\n"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\treturn self.data.get(key, dget(default,self))\n\n\tdef __getitem__(self, key):\t# this is only called if key in D\n\t\treturn self.get(key)\n\n\tdef _setkeys(self, k):\t\t# this is only called if keys in D\n\t\tself.data[k]=dset(dget(self.data,self,default=self.default))\n\n\t_hashids = dhash_ids = dhash = [\n\t\t(\"C\", \"_C\"),\n\t\t(\"E"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\treturn self.__getitem__(key, default)\n\n@_coerce_args('U', 'C')\ndef update(self, mapping):\n\t\"\"\"Update dictionary/list/set dictionaries mapping.\"\"\"\n\tfor k, v in mapping.items():\n\t\tif v is S.Null:\n\t\t\tself.__delitem__(k)\n\t\telse:\n\t\t\tself[k] = v\n\treturn self#XXX\n\n\n@_coerce_args('U', 'F')\ndef __iter__():\t\t#XXX: do something\n\ttry:\n\t\t"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\ttry:\n\t\treturn self[ key ]\n\texcept KeyError:\n\t\tpass\n\treturn default\n\ndef enumElements(objType, objKey, factory, keys, *args, **kwargs):\n\tif objType == 'dictionary':\n\t\tresult = {k+':'+v for ( k, v ) in objects(key_format=None, key_separator=None, key_separator_args=None, objKey=key_format=None, *args)+objects(key_format=None, key_separator=None, key_separator_args=None,"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\tif default is None:\n\t\treturn self.__dict__.get(str(key),self.default)\n\treturn self.__dict__.get(str(key),default)\n\n\t\ndef get_first(self, key, default=None):\t\"\"\"\n\tD.get_first(k[,d]) -> (k, d) if k in D, else d.\n\t\"\"\"\n\tif default is None:\n\t\treturn (self.__dict__.get(key, self.default), )\n\treturn (self.__dict__.get(key, default), )\n\n"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\tself.checkvalidkey(key)\n\t\tself._validate_value_type(key, 'key')\n\t\tself._validate_value_type(default, 'default')\n\t\tres = None\n\t\tself[key] = res = self._data_adapter.get(self.encodekey(key), self._data_adapter.decode(self.decodevalue(default)))['d']\n\t\treturn res\n\n\tdef iteritems(self):\t\"\"\"\n\t\tD.iteritems() -> list of D's items.\n\t\"\"\"\n\t\tres"}]}
{"task_id": "62b8d22548ba5a41d1c3f472", "prompt": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t__wrapped__ = cache.__array_wrap__(*args, **kwds)\n\treturn __wrapped__\n\ndef __enter__(self):\t\"\"\"\n\tSave self in the instance dictionary if __enter__ is defined.\n\tThis is automatically done by __exit__() so not much code is\n\tdone in this method.\n\n\tRaises:\n\t\tAttributeError - if self is an atom and has no 'cached' attribute.\n\t\"\"\"\n\tif hasattr(self, '_m___dict__') and self.__class__.__dict__.get('_m_'):\n\t\tif self"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tdef cachemethod(c):\n\t\t_memoized_dict = dict(cache)\n\t\tif lock:\n\t\t\tlock.acquire()\n\t\ttry:\n\t\t\tresult = c.__get__(key, _memoized_dict)\n\t\t\tif cache is not _memoized_dict:\n\t\t\t\tcache.clear()\n\t\t\t_memoized_dict.update(result)\n\t\tfinally:\n\t\t\tif lock:\n\t\t\t\tlock.release()\n\t\t\tif not result:\n\t\t\t\treturn\n\t\treturn _memoized_dict[hashkey](c)\n\t"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tclass CacheMemoizingMethod(object):\n\t\t\"\"\"\n\t\tThis class allows one to define caching methods for all objects.\n\t\tEach cache will contain a hashable object that can be used to\n\t\tretrieve data from the cache.  It can access the cache as well\n\t\tas the cache itself.\n\n\t\tThe cache object that created us gets a reference to this class\n\t\tafter the callable returns.\n\n\t\tParameters\n\t\t----------\n\t\tcache : :class:`object`\n\t\t\tTHe cache object to wrap.\n\t\tkey : :class"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t# The class or instance method can be either a class-method or\n\t# staticmethod decorator.\n\tdef __get__(self, instance, owner):\n\t\treturn self\n\tdef __call__(self, *args, **kwargs):\n\t\tsaved_cache = kwargs.get('saved_cache') or kwargs.get('saved_cache_at_call') or None\n\t\tfn = __cacheattr__\n\t\tif cache is True:\n\t\t\tfn = functools.partial(memoize(cachedmethod, key=fn.__name__), name=self.__class__.__name"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tdef __init__(self, instance=None, method=None):\n\t\tif instance is None:\n\t\t\tself.cache = {}\n\t\telse:\n\t\t\tself.cache = instance.cache\n\t\tself.cache[key] = self._memoise(cache, method or method, lock)\n\n\tdef __call__(self, *a, **kw):\n\t\treturn self.cache[hashkey(*a, **kw)]\n\n\tdef __reduce__(self):\n\t\treturn (cls, key)\n\tdef clear(self):\n\t\ttry"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tif hasattr(cache.cache, key):\n\t\treturn cache.cache.__getitem__\n\tcache.cache.__getitem__ = make_wrapped(\n\t\t\tkey, \n\t\t\tobject.__getattribute__, __cache_get,\n\t\t\tobject.__setitem__, \n\t\t\tobject.__delitem__, cache, key)\n\npackage org.tron.common.crypto.zksnark;\n\nimport lombok.extern.slf4j.Slf4j;\nimport org.tron.crypto.zksnark.Proposal;\nimport org.tron.protobuf.Network;\n\n@"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t#\n\t#\tArguments:\n\t#\n\t#\t\tcache(cache class or instance): cache method to be wrapped.\n\t#\t\tkey(key for this item in the cache): key of the item to be cached.\n\t#\t\tlock(lock object for this item in the cache): lock object for\n\t#\t\t\tthe cache.\n\t#\n\t#\tReturns:\n\t#\n\t#\t\tMethod wrapping the item on the cache.\n\t#\n\t#\tExample usage:\n\t#\n\t#\t\t@cachedmethod\n\t#\t"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\treturn cache.memoize(__func__, cache._cache, key, cache.memoize_args_cache, cache, lock)\n\ndef cachedmethod_with_instance_method_cache(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\treturn cache.memoize_instance(cachedmethod, cache._cache, key, cache, lock)\n\t\n\t\n\t\n\t\n\t\n \n\t\n\t\n\nclass cachedmethod2(function_wrapper):\n\t\"\"\"\n\tDecorator to"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tif lock is None: lock = _defaultlock\n\treturn _cache(cache, fget=lambda _caller: _caller, fset=lambda _caller: _caller, key=key, lock=lock)(cache)\n\ndef _defaultlock(cache, fget, fset=None, key=None, lock=None):\t\"\"\"Cache decorator - caching is a feature. To achieve performance this function is called on all\n\tclasses and instances that have an __init__() method attached to them as well as classes who have\n\ta __set__() method. As well this function can be safely"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tif not (hasattr(cache, '__func__') or\n\t\thasattr(cache, '__self__')):\n\t\tcache_name = getattr(cache, '__name__',\n\t\t\tcache.__class__.__name__)\n\t\tif not (cache_name.startswith('MemcachedCache') and\n\t\t\thasattr(cache, '__func__')):\n\t\t\treturn None\n\t\tsetattr(cache, '__self__', repr(cache))\n\t\tgetattr(cache, '__func__')(**cache.__dict__)\n\treturn cache.__call__(key, **_cache_args"}]}
{"task_id": "62b8c517e0d34b282c18122e", "prompt": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\tmsg, filename, lineno, func_name, reraise, py3K = 1, \"py\", \"3\", \\\n\t\t'except %s as ex:\t# you can make this a warning or a'\\\n\t\t\t\t\t'critical exception\\n'\n\treturn _str_exception(cls, e, max_level, max_path_level, msg, filename, lineno, \\\n\t\tfunc_name, reraise, py3K, (ex.__name__+'.').upper(), str(ex))\n\nclass CDeferrableExc(Exception):\n"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\ttry:\n\t\tif hasattr(e, '__name__'):\n\t\t\te = type(e).__name__\n\t\tmsg = e\n\texcept AttributeError:\n\t\t# e is not a subclass of Exception, so we can use the module name\n\t\t# if we can extract the module name. If the message is str, then it's\n\t\t# a function to call (e.g. RuntimeError might be a different\n\t\t# class of TypeError).\n\t\tn = repr(e)\n\t\tif issubclass(type(e), ImportError) or n[:len"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\tif issubclass(cls, type):\n\t\treturn \"class {0}\".format(cls.__name__)\n\telif issubclass(cls.__class__, type):\n\t\texc_name = cls.__class__.__name__\n\t\texc_attr = \"__name__\" if hasattr(cls, \"__name__\") else \"\"\n\t\texc_ex = \"exception\" if isinstance(e, type) else \"{0} (type {1})\".format(e.__class__.__name__, exc_name+exc_attr)\n\t\treturn \"Exception: {0} (type {1})"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t# Get the exception's full description text.\n\tdesc = e.args[0].__traceback__.tb_frame.f_code.co_name + \\\n\t\t\"_ex_get\"  # type: ignore\n\tif hasattr(e, \"args\"):\n\t\t# If there's an old-style class's doc-string, we want to show a\n\t\t# stack trace since that's the most clear reason why the doc\n\t\t# itself might be lost. To do this, we use the 'oldstyle' class\n\t\t# name, if available, as"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\tmsg = None\n\tif e.__class__ is Exception:\n\t\ttry:\n\t\t\tpath = e.__traceback__()[1]\n\t\texcept Exception as e:\n\t\t\tmsg = str(e)\n\t\t\tif isinstance(e, UnboundLocalError):\n\t\t\t\tmsg = \"unbound local error\"\n\telse:\n\t\tpath = e.__module__ + \".\" + func_name(e.__function__)\n\t\tif e.__traceback__:\n\t\t\tpath += \":\" + e.__traceback__()[1][:- 1]\n\treturn \"%s(%"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\tdef _exception_(cls, e):\n\t\tassert type(e) is tuple\n\t\tif len(e) == 2:\n\t\t\te = e[0]\n\t\tif e and hasattr(cls, e) and hasattr(cls, \"ext\"):\n\t\t\text = getattr(cls, \"ext\")\n\t\t\tret = []\n\t\t\tif hasattr(cls, \"raise_\" + e):\n\t\t\t\texc = cls.raise_ + e\n\t\t\telse:\n\t\t\t\texc = cls.throw_ + e\n\t\t\tfor exc_, n, p, a"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\ttry:\n\t\te.__cause__.__suppress_context__()\t# supress stack-level messages\n\texcept AttributeError:\n\t\treturn str(e)\n\ts = str(e)\n\tif max_path_level:\n\t\ts = s%(\" [stack: \" + str(max_path_level))\n\ts = \"[%s]\"%s % (s)\n\tif max_level:\n\t\ts = s%(\" [traceback: \" + str(max_level))\n\ts += \"%s in %s\"\t\t\t\t# print originating"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\tfilename, lineno, offset, description = None, None, None, None\n\tif e.args:\n\t\tpath = \"%s:%s\" % (e.filename, e.lineno)\n\t\tdescription = 'in function '+path.replace('\"', '\\\\\"')\n\t\tformatted_tb = traceback.format_exception(e, e.args, description)\n\telse:\n\t\tformatted_tb = traceback.format_exc(e, None)\n\n\treturn formatted_tb.strip('\"\\n')\n\n/*\n * This file is part of the GROMA"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\tdef estr(errorexception):\n\t\t\"\"\" Format an exception.\n:param e: Any exception instance. \"\"\"\n\t\tif errorexception is None:\n\t\t\treturn \"Undefined\"\n\t\telif errorexception is not None:\n\t\t\treturn 'Errorexception: %s' % (errorexception.getMessage(), )\n\t\telse:\n\t\t\treturn str(errorexception)\n\ndef extract(path, base_name=''):\n\t\"\"\" Extract file from zip archive.\n:param path: Path to extracted archive.\n"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\tlines= []\n\texc_code= 1\t# 1 Exception was thrown, which is not a user-visible subclass\n\tif e is not None:\n\t\tlines.append('------------------------' + str(e.__class__) + str(e.__bases__[0].__name__) + '---------------')\n\t\texc_file_path, exc_line, exc_exc_type, exc_orig_line = sys.exc_info()\n\t\texc_file_name, exc_line_no, exc_func, exc_msg = trace_exc_info(exc_"}]}
{"task_id": "62b8bbbfe0d34b282c181210", "prompt": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t", "suggestions": [{"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tif not _open_file(file_name):\n\t\treturn 0\n\t\n    if PY3:\n        text_buffer = text_buffer.translate(os.linesep)\n\t\n\t# check for special encoding and apply it\n\tif encoding == \"utf_8\" and _encoding_utf_8 == \"\":\n\t\t# if there is little or big endian, use either utf8 or utf_8\n\t\tencoding = \"utf_8\"\n\tif encoding == \"latin_1\" and _encoding_latin1_to_"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tfrom cStringIO import StringIO\n\tfrom glob import glob\n\t# check that the buffer is long enough.\n\tif text_buffer.maxlen < int(len(text_buffer)) + 2:\n\t\treturn 0\n\t# get the list of files in the current directory\n\tfiles = glob(StringIO(\"*.\"+file_name+\".\"+os.sep+\".\"+os.sep), recursive=0) # recursive=0 returns all files, recursive=1 also returns directories\n\t# sort the files by time.\n\tfiles.sort()\n\t# write the text data to disk"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t# If the file doesn't exist, return 0.\n\tfile_path = find_path(file_name, True)\n\tif not file_path: return 0\n\n\t# Load the text from that file into the text buffer by creating a new memory buffer using an append mode of append.\n\t# The new memory buffer will contain the concatenated text.\n\told_buffer = get_text_buffer(file_path, file_name, False, True)\n\tnewbuffer = bytearray(1024*1024 * 2)\n\t\n\t# Get new data by seeking to the start"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tif not overwrite:\n\t\tif not os.rename(file_name,file_name+\".backup\"):\n\t\t\tlog.error(\"Error renaming {} to {}\".format(file_name,file_name+\".backup\"))\n\t\t\treturn 0\n\t\telse:\n\t\t\tlog.debug(\"Renamed {}\".format(file_name))\n\t\t\tif not os.rename(file_name+\".backup\", file_name):\n\t\t\t\tlog.error(\"Error resetting {} to {}\".format(file_name,file_name+\".backup\"))\n\t\t\t\treturn 0\n\telse:\n"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tif os.access(file_name, os.R_OK):\n\t\tf = open(file_name,  \"at\")\t# File opened.\n\t\tif getattr(f.mode, 'r', None) == 'b':\n\t\t\tf.write(text_buffer)\n\t\telse:\n\t\t\tf.write(text_buffer[:-1] + text_buffer[-1:])\n\t\tf.close()\n\t\treturn len(text_buffer[:-1])\n\telse:\n\t\treturn 0\n\ndef append_binary_to_file"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t#print('>append_text_to_file> %s %b' % (file_name, text_buffer), end='')\n\tif isinstance(text_buffer, basestring):\n\t\t# Create the file.\n\t\tf = open(file_name, 'a')\n\t\tf.writelines(text_buffer)\n\t\tf.close()\n\t\t# Return the bytes written.\n\t\treturn text_buffer.tell()\n\telse:\n\t\tif isinstance(text_buffer, bytes):\n\t\t\t# If the buffer is bytes and"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tif encoding!= 'utf-8': encoding = 'utf8'\n\ttry:\n\t\twith open(file_name, 'rb') as input_file:\n\t\t\tif len(text_buffer) + len(input_file) >= 1024:\n\t\t\t\tinput_file.seek(0)\n\t\t\t\tinput_file.write(text_buffer.encode(encoding))\n\t\t\telse:\n\t\t\t\tinput_file.write(text_buffer)\n\t\treturn len(input_file)\n\texcept:\n\t\tprint('-e error - %s' %"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\treturn file_util.append_text_to_file(file_name, text_buffer, encoding, overwrite)\n\n####################################################################################################\n\n##############################################\n#   Pure Python Library Version #\n##############################################\n\n##############################################\n#   Main Functions #\n##############################################\n\n###############################################################################\n\n####################################################################################################\ndef version():\n\t\"\"\"Get the current version.\n:return: Version information (str).\n:rtype: str\n:return: Current version (str).\"\"\"\n\treturn __version__\n\n###############################################################################\n#############\n#   "}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t# Get buffer size\n\tlen_buffer = len(text_buffer)\n\t# Open file\n\twriter = open(file_name, 'w', encoding=encoding)\n\t# Do work\n\tif len_buffer > 0:\n\t\t# Append to existing file\n\t\tfile_size = len(filename)\n\t\tfp = open(filename, 'ab', 0)\n\t\tfp.write(text_buffer)\n\t\tfp.flush()\n\t\tfp.close()\n\t\t# Get actual size\n\t\tsize = len(filename)\n\t"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tfh = open(file_name, \"r+b\")\n\tfilesize = fh.tell()\n\tif file_name.endswith(\".txt\"):\n\t\ttext_buffer += \"\\r\\n\"\n\twhile text_buffer and bytes_in_read_buffer(text_buffer, fh):\n\t\ttext_buffer = text_buffer.replace(b'\\r\\n', b'\\n')\n\t\tif encoding!= os.system(decode_path(file_name)).split('/')[-1].lower() or encoding not in (\"UTF-8\", \"utf-16\"):\n"}]}
{"task_id": "62b8bbbfe0d34b282c18120f", "prompt": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t", "suggestions": [{"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\tif not os.path.exists(file_name):\n\t\traise IOError(_(\"File %s does not exists!\") % (file_name))\n\n\tfile_contents = None\n\ttry:\n\t\twith open(file_name, 'rb') as fh:\n\t\t\tfile_contents = fh.read()\n\texcept IOError :\n\t\tpass\n\n\ttry:\n\t\tfile_contents = file_contents.decode(encoding).encode('utf-8')\n\texcept UnicodeDecodeError:\n\t\tpass\n\texcept (UnicodeEncodeError, UnicodeError"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t# Read the file into a string, using a temporary buffer.\n\tf_buff = file_data(file_name, encoding)\n\tif f_buff is None: return None\n\t\n\t# Build the buffer from the string.\n\tbuff = c_char * len(f_buff)\n\tif not buff or len(f_buff)!= buff_length(f_buff): \n\t\tbuffer_error(\"Buffer to string conversion is not ok\")\n\t\treturn buffer_get_string_from_buff(buff)\n\t\n\t# Perform conversion (i.e."}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\timport ctypes\n\treturn wtap_call_c_get_filename(file_name, -1, encoding)\n\n\ndef file_textbuffer_to_stream(file_name, encoding, stream_name=None):\n\t\"\"\"\n\tSave a file in file to a text buffer, using the specify encoding while reading.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:param stream_name: File name to write.\n\t:type stream_"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\treturn read_file_utf8(file_name, 'rt', encoding)\n\nclass XMLReader:\n\tdef __init__(self, stream):\n\t\tself.stream = stream\n\t\tself.node = None\n\t\tself.root = None\n\n\tdef __read():\n\t\tstream = self.stream\n\t\tif stream:\n\t\t\ttree = XMLTree(stream)\n\t\t\t\n\t\t\t# Get the root\n\t\t\troot = self.get_root(self.root)\n\n\t\t\t# Load the tree\n\t\t\tfor name in tree.get"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\tresult_set = csv(file_name, 'utf-8', True, encoding)\n\tif len(result_set) == 0: self.log_error('Empty file \"'+file_name+'\"')\n\treturn result_set[0]\n\ndef string_to_string_list(string_list):\n\treturn [x[0] for x in string_list]\n\n/* libFLAC - Free Lossless Audio Codec library\n * Copyright (C) 2009-2010  Xiph.org Foundation\n *\n * Redistribution and use in source and binary forms, with or without"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\treturn file_to_textbuffer(file_name, \"utf_8\")\n#-------------------------\n\n\ndef make_dirs(dirname):\t\"\"\"\n\tMake all directories in dirname. Does not follow symlinks!!!\nCAUTION :\n\tDirectory already exists.\n\n\tIf you specify a string with a different separator than the default, you have to prepend it with '\\\\'.\n\t\"\"\"\n\tfor separator, sublevel in listdir(dirname):\n\t\tif separator ~=\":/\" or separator == \"\\\\\" :\n\t\t\tcontinue\n\n\t\twith open(join(dirname,"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\tpass\n\n/* Copyright (C) 2013-2016, The Regents of The University of Michigan.\nAll rights reserved.\n\nThis software was developed in the APRIL Robotics Lab under the\ndirection of Edwin Olson, ebolson@umich.edu. This software may be\navailable under alternative licensing terms; contact the address above.\n\nThis library is free software; you can redistribute it and/or\nmodify it under the terms of the GNU Lesser General Public\nLicense as published by the Free Software Foundation; either\nversion 2.1 of the License, or (at your option"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\"\"\"\n\tfile = file_name\n\tif hasattr(file,'read'):\n\t\tfile = file.read()\n\tif isinstance(file, str):\n\t\tfile = codecs.open(file, 'r', encoding)\n\telse:\n\t\tfile = codecs.open(file, encoding)\n\tbuffer = file.read()\n\tfile.close()\n\treturn buffer\n\n\ndef text_to_file(filename, encoding = \"UTF-8\", data = None):\n\n\tglobal File_Openers, File_Encodings\n\t\"\"\"\n\t"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t# File exists in path, see if its text\n\treturn b\"\".join(map(chr, open(file_name).readlines())).decode(encoding)\n\n\n# Returns the absolute path to the given file\ndef to_abs_path(file_name):\n\t\"\"\"\n\tConvert a filepath (file ending with.py or.co) to a path relative to the given working directory.\n\tThis process is very useful if we want to find out the relative path of a given file.\n\t:param file_name: File name.\n\t:type file_name: str"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t# Read file in memory, then return the data as a string.\n\t# First try to read the entire file in memory.\n\t# If this fails, read all the data until the end.\n\t# The size of the entire file is returned.\n\t# If a error occurs, we assume a file size of 100MB.\n\ttry:\n\t\tdata = file(file_name, 'rb')\n\t\tif data.readlines() == []:\n\t\t\tassert data.next() == b'', 'Not found line in file.'\n\t\tdata.name = file"}]}
{"task_id": "62b8bbbce0d34b282c18120d", "prompt": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tcheck_if_path_exist(file_name)\n\ttry_dir(file_name, create_check=True)\n\treturn(os.path.exists(file_name))\n\n\ndef is_path_exist(path_or_file):\t\"\"\"\n\tCheck if path or file exist.\n:param path_or_file: Path or file name\n:type path_or_file: str\n:return: Return true (exist), false (do not exist, or invalid path or file name)\n:rtype bool\t\"\"\"\n\tcheck_if_path_"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\treturn os.path.exists(file_name)\n\ndef create_link_dir(src, dst):\n\t\"\"\"\n\tCreate a link and dir for new source and dest\n\t:param src: Source dir path\n\t:param dst: Destination dir path\n\t:type src: str\n\t:type dst: str\n\t:return: True or False\n\t:rtype bool\n\t\"\"\"\n\tos.makedirs(os.path.dirname(dst) if os.path.isdir(dst) else dst)\n\tfp = op.join(dst, src)"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\tstatinfo['b_type'].append(type(os.path.realpath(filename)))\n\t\treturn os.path.isfile(file_name)\n\ndef del_file(file_name):\t\t\"\"\"\n\tDelete selected file.\"\"\"\n\t#print \"\\n***** delete_file ****\"\n\tstatinfo['b_deleted'] = 1\n\tos.remove(file_name)\n\ndef mkdir(dir_name):\t\t\"\"\"\n\tCreate a directory.\"\"\"\n\t#print \"\\n***** mkdir ****\"\n\tif os.path.isdir("}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tif file_name == \"\":\n\t\treturn False\n\ttry:\n\t\tif os.access(file_name, os.R_OK|os.W_OK|os.X_OK):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n\n@app.route(\"/file_list\", methods=[\"GET\"])\ndef file_list():\n\tfile_dict = defaultdict(set)\n\tfile_list_file = app.config[\"file_list_file\"]\n\tif file_list_file"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tresult_code = os_api.check_if_filename_exist(file_name,\n\t\t                                                   True)\n\t# print \"check_if_filename_exist\", result_code\t\t#DEBUG\n\treturn result_code\n\n\ndef is_valid(fname):\n\t\"\"\"\n\tChecks if a file is a valid one.\n:param fname:"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\ttry:\n\t\tif os.path.exists(file_name):\n\t\t\treturn True\n\t\t\t#return False\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n\ndef check_existed_folder(file_path, folder):\n\t\"\"\"\n\tCheck if file_path exist.  Return True if exist, else False.\n:param file_path: File path\n:type file_path: str\n:param folder: Sub-folder names.\n:type folder: str\n:return: Return true if exist, else False."}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\treturn (exists(file_name)!= False)\n\n\ndef existsFile(file_name):\n\t\"\"\"\n\tCheck if the provided file name is exist in the given directory.\n\t:param file_name: File name\n\t:type file_name: str\n\t:return: Return True (exist), False (do not exist, or invalid file name)\n\t:rtype bool\t\"\"\"\n\treturn exists(os.path.join(os.path.dirname(file_name), file_name))\n\n#include \"Game/AI/Action/actionPlayRippleAnim"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tif os.path.exists(file_name) == True:\n\t\treturn True\n\telse:\n\t\treturn False\n# def file_get_list(str):\t\"\"\"\n# Get all the files in the folder\n# in this example, every file in the folder is listed like this:\n#    [filename.png] [filename2.png]...\n# Here file_name is 'filename'\n# You can also get all the files in the folder in the folders:\n#    [filename.png] [filename.jpg]...\n# where"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\treturn os.path.isfile(file_name) and os.path.isfile(file_name)\n\ndef read_file(fname):\n\t\"\"\"\n\tRead a File.\n\t:param fname: File Name.\n\t:type fname: str\n\t:return: Data.\n\t:rtype: str\n\t\"\"\"\n\twith open(fname, \"r\") as f:\n\t\treturn f.read()\n\ndef is_file(fname):\n\t\"\"\"\n\tCheck if file name is file path.\n:param fname: File name.\n:type"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tif not file_name:\n\t\treturn False\n\t#print(\"file_name = %s\" % file_name)\n\tif os.path.exists(file_name):\n\t\treturn True\n\telse:\n\t\terror_message = \"file_name[%s] not exist\" % (file_name)\n\t\tlog.error(error_message)\n\t\traise Exception(error_message)\n\n\ndef is_file_exist_if_exists(file_name, directory_name, file_root_dir=None):\n\t\"\"\"\n"}]}
{"task_id": "62b8b99de0d34b282c1811f8", "prompt": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tglobal _LOGGING_DISABLED\n\tcls._LOGGING_DISABLED = 0\n\npackage org.jboss.windup.config.parser;\n\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertNotNull;\n\nimport java.util.Collections;\nimport java.util.List;\n\nimport org.jboss.arquillian.container.test.api.Deployment;\nimport org.jboss.arquillian.junit.Arquillian;\nimport org.jboss.forge.arquillian.AddonDependencies;\nimport org.jboss.forge.arquill"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tcls._logger = cls._setup_logging()\n\t\n\tfor meth in [cls.log_debug, cls.log_info, cls.log_warning, cls.log_error]:\n\t\tmeth(_reset_logging)\n\t\n\tcls._setup_logging()\n\t\n\treturn\n\t\n\t\ndef _set_debug_logging(method) :\n\t\"\"\"\n\tSet debug logging\n\t\"\"\"\n\t\"\"\" \n\tDebug mode\n\t\"\"\"\n\t#print \"DEBUG: \" + repr(method)\n\tcls._logger.setLevel( debug )\n\t\n\t\ndef"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tfrom.core.logger import logger\t# importing logger here allows us to get a logger object to use\n\tlog_type = cls.log_type  # type for the default logging of a logger\n\tif log_type == \"default\":\t\n\t\t# Using the default logging module, or simply logging.basicConfig(...)\t\n\t\tif not hasattr(__module__.split('.logger.basic'), 'default'):\n\t\t\tlogger = logging.getLogger(__package__)\n\tif log_type == 'basic':\n\t\tif not hasattr(logger, 'basicConfig'):\n\t\t\t#logger."}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t# log a message\n\timport json\n\tresult = {}\n\tfor log in ['debug', 'info', 'warning', 'error', 'critical', '.']:\n\t\tif int(log) > 1: log = log[0]\n\t\tfor level in ['trace', 'debug', 'info', 'warning', 'error', 'critical']:\n\t\t\tif int(log) <= 6 and log >= level: result[log] = level\n\t\telse: result[log] = 'debug'\n\t# log a message\n\timport traceback\n\tfor message in traceback.format"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tfrom. import logger\n\tlog = logger.Logger(__name__, 'DEBUG')\n\tlog = logger.Logger('test_logging', 'DEBUG')\n\treturn log\n\n\n@pytest.fixture\ndef logfile():\n\t\"\"\"\n\tfile log file\n\t\"\"\"\n\treturn Path(__file__).parent / 'test_output.log'\n\n\n@pytest.fixture\ndef temp_logfile():\n\t\"\"\"\n\tTemp file log file\n\t\"\"\"\n\ttemp_file = Path(__file__).parent / 'test_output.log'\n\ttemp_file.replace(temp"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\told_logger_filters = logging.get_loggers()\n\tlogging.reset_loggers()\n\treturn old_logger_filters\n\n\n@contextmanager\ndef _lock():\n\t\"\"\"\n\tA context manager to temporarily aquire the lock.\n\n\tThis allows the loggers to be locked (if the lock is currently acquired)\n\tand prevents an unexpected logging output (when an output handler is\n\tunregistered).\n\t\"\"\"\n\twith _lock_:\n\t\tyield\n\n\n@contextmanager\ndef _lock_(name=None):\n\t\"\"\"\n\tContext"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tcls._LOG_MESSAGES =  []\n\tcls._DEBUG = False\n\ndef disable_logger_level(level):\n\t\"\"\"\n\tDisable\tlevel. Only used if logging cannot be stopped\n\t\"\"\"\n\tassert level in LOG_LEVELS\n\tcls = LOGGER_MAP[level]\n\tcls.setlevel(0)\n\n\nclass BoltLogger(LOGGER):\n    def __init__(self,\n                 filename,\t\t\t# filename,\n#          "}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\twith open(_logger_path, \"at\") as logger_file:\n\t\tlogger_file.seek(0, os.SEEK_SET)\n\t\tlogger_file.truncate()\n\t\tlogger_file.write('#%s.%s\\n' % (_module_name,_method_name))\n\n\t\tdef write_at(filename, line_no):\n\t\t\tlogger_file.write('%s# %s %s\\n' %(filename,line_no,method_name))\n\twrite_at(_logger_path,_logger_path['_line_no"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tcls._logger = None \t\tcls._logger = None #Reset here!!!!!??\n\tfor a in dir(cls): \t\tif a.startswith('__'): pass\n\tdel cls.logger\t\t\n\nclass DebugLogger (Logger):\n\t\"\"\"\n\t\"\"\"\n\t#pylint: disable=superfluous-parens\n\tdef __init__(self, *arg, **kwargs):\n\t\t\"\"\"Init\n\t\t\"\"\"\n\t\tif kwargs.get('debug', False): debug = True; self.debug = True\n\t\telse: debug = False; self.debug"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tlogging.basicConfig(\n\t\tfilename=cls._log_file(),\n\t\tlevel=cls._log_level(),\n\t\tformat=\"%(host)s:%(user)s:%(time)s:%(levelname)s: %(message)s\")\n\n\tcls.logger.setLevel(cls._log_level())\n\tcls.logger.setLevel(cls.GLOBAL_LOGGING_LEVEL)\n\tcls.logger.info(\"Logging started\")\n\n\ttry:\n\t\tif cls._LOG_DIRECTORY is not None:\n\t\t\tclass_map = {}"}]}
{"task_id": "62b8b59feb7e40a82d2d1291", "prompt": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn _impl_module\n\nclass TargetBase(object):\n\t\"\"\"Subclass of TargetBase to override the _getTargetClass function to handle the case\n\tof a single class.  This is not a part of the API, and will probably break at a later\n\tversion.\"\"\"\n\n\t__slots__ = ()\t\t# keep _impl_module set to the class we've provided\n\t\n\tdef _getTargetClass(self):\n\t\treturn self.__class__\n\t\n\tdef _setTargetClass(self, cls):\n\t\t# can't do it earlier, because there shouldn't be"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t#\tif __doc__ is not None:\n\t#\t\treturn self.__doc__\n\t#\telse:\n\t#\t\treturn self.targetClass\n\treturn str(self.__doc__)\n\ndef _get_module_name(cls):\t\"\"\"\n\tCreate a relative path from the class name.\n\t\"\"\"\n\tmodname = cls.__name__\n\tif modname.endswith('.'):\n\t\tmodname = modname[:-1]\n\t#\tthe first segment is the name part\n\t#\t... strip off the name from the class name and"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn self.__class__.__name__\n\n#-----------------------------------------------------------------------------\n# Dev API\n#-----------------------------------------------------------------------------\nclass TraceModule(object):\n\t\"\"\"\n\tThe TraceModule provides an interface for reading the traced files,\n\tusing Trace.GetModule(), Trace.GetModuleNames() to obtain a string\n\trepresentation of the module names that should be traced.  It can be\n\tused to gather information about the current load and save state, and\n\tto examine the full stack of modules.  These strings are returned\n\tas-is.\n\n\tThe TraceModule can be constructed from"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\tif \"class_name\" in self.implementation:\n\t\treturn self.implementation\n\n\treturn _Py_GetImplementation().find(\"class_\" + self.implementation)\n\n\n############################################################\n##              _PyGetImplementation()                ##\n############################################################\n\ndef _PyGetImplementation():\n\t\"\"\"\n\tReturn the \"implementation\" as a string.\n\t\"\"\"\n\treturn getattr(sys.modules[\"__main__\"], \"implementation\", None)\n\n"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn self.__pypy__\n\n\t_make_methods(IPythonPyyIPythonPang)\n\n\n\n_IPythonPyyIPythonPangIPythonPangPang = _PyPyyIPythonPangPang.IPythonPyyIPythonPangIPythonPangPang = _IPythonPyyIPythonPangPang\n\n#ifndef _LINUX_TRACE_EVENT_H\n#define _LINUX_TRACE_EVENT_H\n\n/*\n * Kernel Tracepoint header definitions\n *\n"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn _class_from_module(self.__module__)\n\n\tpass\n\nclass _AttributeAccessWrapper(object):\n\n\tdef __init__(self, attribute):\n\t\tself.attribute = attribute\n\n\tdef __get__(self, object, attribute):\n\t\treturn getattr(self.attribute, object, attribute)\n\n\tdef __set__(self, object, attribute, value):\n\t\tsetattr(object, attribute, value)\n\nclass _WrapperAttributeAccess(object):\n\n\tdef _getTargetClass(self):\n\t\treturn _AttributeAccessWrapper\n\n"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn (sys._getframe(1).f_globals.get('__name__')).split('.')[-1]\n\ndef find_module(module_name):\n\treturn None\n\n#def _find_module(module_name):\n#\tmodule_name = module_name.replace('/', '.').replace('.','/')\n#\troot, ext = os.path.splitext(module_name)\n#\tif ext!= '':\n#\t\tmodule_name += ext\n#\t\text = '.' + ext\n#\t\tmodule_name += '.'.join"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn self.implementationClass\n\nclass _FallbackError(PyObjCToolsError):\n    \"\"\" Base class for errors thrown by the fallback generator when\n        using the 'Py' or 'Fallback' class method. \"\"\"\n    def __init__(self, name, bases, dict):\n        \"\"\" Create a new exception with the name 'name', the set of\n            bases 'bases', and the dictionary of class attributes\n            '{..."}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn c_type_info\n\ndef _setTargetPackage(self, package):\n\t\"\"\"\n\tSet the module_search_paths and module_name attributes of this\n\tmodule and return it.\n\t\"\"\"\n\tself.module_search_paths = [package]\n\tresult = module_search_paths\n\t# Get the name & import all modules contained in target_package\n\tfor i in self.target_package:\n\t\tresult.append(i + '/')\n\tmodules = self.getModules(result)\n\treturn modules\n\n@setTargetPackage\ndef"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn sys.implementation.name\n\ndef get_class_from_object(obj, modname = None):\n\tif obj is None:\n\t\treturn None\n\t# Get the module name and object type by walking the parents up\n\ttry:\n\t\tmodnames = _get_module_names_with_modname(obj)\n\texcept:\n\t\treturn None\n\tif modname not in modnames:\n\t\ttry:\n\t\t\tmodobjs = _get_module_objects_with_modname(obj, modname)\n\t\t\tmod ="}]}
{"task_id": "62b8b590eb7e40a82d2d1275", "prompt": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t_index2 = 0\n\tfor i, name in enumerate(sorted(orderings)):\n\t\t_index2 = _index2 + 1\n\t\tif i == _index2:\n\t\t\tif name in orders:\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tdel orders[i]\n\t\t\t\tdel orders[i - 1]\n\treturn _index2, orderings, orders\n\ndef _flattenOrderings(orderings):\n\tfor i, name in enumerate(sorted(orderings)):\n\t\torderings[i] = name\n\ndef _normalize("}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\tfor list_ in orderings:\n\t\tfor i, o in enumerate(list_):\n\t\t\tlist_[i] = o.split(\".\")\n\treturn list(list_[0] + list[1:])\n\ndef _legacy_merge(otherlist):\n\tif otherlist:\n\t\t# merge two lists\n\t\tif type(otherlist[0]) == type(LIST):\n\t\t\tl = [o.split(\".\") for o in otherlist]\n\t\t\tfor i, o in enumerate(l):\n\t\t\t\tl[i] = o.split(\"/\")"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\tif len(orderings) < 2: raise ValueError(\"Must have at least 2 orderings.\")\n\tif len(orderings) % 2: raise ValueError(\"Must have an even number of orderings.\")\n\tfirst_orderings = reversed(set(orderings[:-1]))\n\tlast_orderings = reversed(set(orderings[1:] + [len(orderings) - 1]))\n\tfor ordering in first_orderings:\n\t\torderings.insert(0, ordering)\n\tfor ordering in last_orderings:\n\t\torderings.append(ordering)\n"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\tif not len(orderings):\n\t\treturn orderings\n\treturn [o for o in orderings if o > 0]\n\ndef _addSingle(key, obj, single, defv=\"\"):\n\t\"\"\"\n\tAdd a new key/value pair to a dictionary.\n\n\tIf the key already exists in the dictionary, raise a KeyError.\n\n\tIf the key becomes a list (so is a list or tuple), rather than\n\tappending an element, keep the element list in order and append\n\tit at the end.\n\n\tIf the key matches the key of"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\tfor idx, ordering in enumerate(orderings):\n\t\tif idx % 2 == 0:\n\t\t\torderings[idx] = ['q'*(idx + 1)]\n\t\telse:\n\t\t\torderings[idx] = ['q', 'z']\n\treturn orderings\n\ndef _sortOrderings(obj):\n\t# sort the orderings by their name\n\torderings = []\n\tfor name, obj in iteritems(obj):\n\t\torderings.append({\"name\":name, \"obj\":obj})\n\treturn orderings\n\ndef _sortMerge"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\tfor index, o in enumerate(orderings):\n\t\tif o in orderings[index + 1]:\n\t\t\tcontinue\n\t\tnewOrderings = [o]\n\t\tfor j, j2 in enumerate(o):\n\t\t\tnewOrderings[j + 1] = j + 1\n\t\t\tif j2 in orderings[index]:\n\t\t\t\tnewOrderings.insert(0, j2)\n\t\torderings = newOrderings\n\nOrderings are constrained in such a way that there exists already an object\nin this list and no object appears in the other."}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\tif len(orderings) <= 1:\n\t\treturn list(orderings)\n\tnew_orders = []\n\told_length = len(orderings)\n\tnew_length = len(list(orderings))\n\twhile len(new_orders) < new_length:\n\t\to2 = old_length - 1\n\t\twhile o2 >= 0 and sorted(new_orders)[o2] > orderings[o2]:\n\t\t\to2 -= 1\n\t\t\tif o2 < 0:\t\t# No more orderings\n\t\t\t\tbreak\n\t"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\tfor o in orderings:\n\t\tfor i in o:\n\t\t\to[i] = i\n\n\"\"\"\n\n\tdef _orderings(orderings):\n\t\tret = []\n\t\tfor o in orderings:\n\t\t\tfor o in o:\t\t# iterate through list\n\t\t\t\tret.append(o)\n\t\treturn ret\n\n# ========================================================================================\ndef _checkIndexList(indexList):\n\t\"\"\"\n\tCheck if the indexList is consistent with a list of pairs.\n\n\t:param indexList: the input to be validated."}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\tnew = []\n\tfor (s1, s2) in orderings:\n\t\tif (len(s1) == 2) and (s1[0] == s2[0]): continue\t\t# 1 and <empty> must NOT be added\n\t\tif (len(s1) == 1) and (s1[0] == s2[0]): continue\t\t# 1 and <empty> must NOT be added\n\t\tif (len(s1) == 2) and (s1[0]+s1[1]) == s2: continue # 1-"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\tfor order in orderings:\n\t\torig := order[:]\n\t\tfor i, order_ in enumerate(order):\n\t\t\tif i < len(order):\n\t\t\t\torig[:i] = 'x'\n\t\t\tif i >= len(order):\n\t\t\t\torig[i:] = 'q'\n\t\t\tif i < len(order) - 1:\n\t\t\t\torig[i:] = 'z'\n\t\t\torder_ = orig + order\n\t\treturn [order_ for order_ in order]\n\n\t# This was the old order, not a new order"}]}
{"task_id": "62b8b58deb7e40a82d2d1269", "prompt": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\treturn interfaces.IDeclaration\n\n@implementer(interfaces.ITypeDefinition)\nclass ITypeDefinition(val self, val base):\n\t\"\"\"Implements ITypeDefinition.\n\tImplementations should provide a property descriptor\n\timplementation for a class or type or value in properties.\n\tA ITypeDefinition should not be created directly.  An ITypeDefinition\n\tshould be returned unless created via a factory method defined here.  If\n\tthe factory method is not given, must be declared in the class or\n\tinterface it is attached to.  \"\"\"\n\t_providedBy = indirectlyProvidedBy\n\n\tdef"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\tvalue = zope.interface.interfaces.IDeclaration.implementation(object) # pylint:disable=no-member\n\tvalue.interfaces = object.interfaces\n\tvalue.classes = object.classes\n\treturn value\n\ndef directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\treturn object is indirectlyProvidedBy(object) or indirectlyProvidedBy(object.parent)\n\ndef indirectlyProvidedBy(object):\t  # lint-amnesty, pylint:disable=missing-function-docstring\t# pylint:disable=no-member\n\t\"\"\""}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\tif type(object) is not type(()):\n\t\treturn None\n\t\n\treturn CdeclInterface(object)\n\nif __name__ == \"__main__\":\n\timport sys\n\tsys.exit(main())\n\n/*\n    Copyright (C) 2013 Mike Hansen\n\n    This file is part of FLINT.\n\n    FLINT is free software: you can redistribute it and/or modify it under\n    the terms of the GNU Lesser General Public License (LGPL) as published\n    by the Free Software Foundation; either version"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\treturn IDeclaration, object,\n\n@asynchronously(synchronous=False)\ndef directProvideInterface(interfaceName, object, value): # pylint:disable=redefined-builtin\t# pragma: no cover\n\t\"\"\"\n\tGiven the given interface name and object, return an iterator over\n\tit's instances.\n\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\treturn interfaceProvideInterface(interfaceName, object, value)[interfaceName]\n\n@synchronously(synchronous=True)\ndef asynchronousInterface(interface"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\treturn interface.interfaces.IDeclaration(\n\t\tobjects=object.__class__,\n\t\t__module__=object.__class__.__module__)\n\ndef directlyProvidedByList(objects): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn all interfaces directly provided by the given objects\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\treturn interface.interfaces.IDeclaration(\n\t\tobjects=objects,\n\t\t__module__=objects.__class__.__module__)\n\ndef implementsAny(interfaces): # pylint:"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\tfor entry in object.entries():\n\t\tif isinstance(entry, interfaces.IDeclaration):\n\t\t\treturn True\n\treturn False\n\ndef directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\treturn True # return just in case.\n\ndef directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\treturn True # return just in case.\n\ndef directlyProvidedBy(object, classDefinition):\t# pylint:disable=redefined-builtin\t\"\"\"\n\tfor entry in object.entries():\n\t\t"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\tpass\n\ndef _addMethod(m, name):\n\tdef method(a, b): return m()\n\treturn property(method, (a, b))\n\ndef _addMethod2(m, name, method):\n\tdef method(a, b): return m(a, b)\n\treturn property(method, (a, b))\n\ndef methodsFor(object, types):\n\treturn list(m for m, t in zip(object.methods, types) if not types.isAbstract(t))\n\ndef functionsFor(object, types):\n\t"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\treturn IDeclaration\n\n\ndef reflectedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\treturn IDReflectedBy(object)\n\n\ndef declaredBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\treturn IDDeclaredBy(object)\n\n\ndef __implementedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\treturn IDisposer(object)\n\n/*\n * This file is part of libFirm.\n * Copyright (C) 2017 University of Karlsruhe.\n */\n\n"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\tmodule = sys.modules[__name__]\n\n\t# Find the IDeclaration type\n\tmodule.IDeclaration = getIDeclaration(object)\n\n/*\n * Copyright 2017 Google Inc.\n *\n * Use of this source code is governed by a BSD-style license that can be\n * found in the LICENSE file.\n */\n//\n//  Implementation of some useful data structures and helper functions\n//  for building/loading/manipulating Pango graph objects.\n//\n\n#include \"include/core/SkGraphics.h\"\n#include \"include/"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\tif isinstance(object, interfaces.IImplementsConstraint): return object\n\n\t@implementer(IImplementsConstraint)\n\tdef directlyProvidedBy(constr):\n\t\t\"\"\"The interface that is required by this constraint\n\t\t\"\"\"\n\t\tif isinstance(constr, interfaces.IImplementsConstraint):\n\t\t\treturn constr.interfaces[0]\n\t\tif isinstance(constr, interfaces.IConformingConstraint):\n\t\t\tfor iface in constr.interfaces:\n\t\t\t\tif iface.providesInterface(interfaces.IImplementsConstraint[IImplementsConstraint]):\n\t\t\t\t\treturn iface\n\t\t\t\ttry:\n"}]}
{"task_id": "62b8b559eb7e40a82d2d11f8", "prompt": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t", "suggestions": [{"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t#print classes\n\tordered = [\n\t\te for e in classes if e not in classes\n\t]\n\tfor e in ordered:\n\t\tif e not in classes:\n\t\t\tordered.remove(e)\t\t# remove class\t\t\"\"\"\n\t\t\t# print e\n\t\t\t# print classes\n\treturn sorted(ordered)\n\ndef maxOverlapRatio(setR, bases):\n\t\"\"\"calculate the max overlap ratio between two or less\"\"\"\n\ttotalOverlap1 = sum(b[1] for b in setR)\n\ttotalOverlap2 = sum(b"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tmin=classes[0]\t\n\tfor c in classes[1:]:\n\t\tif (min is None) and issubclass(c, MinimalType) is False:\n\t\t\tmin=c\t\n\treturn min\n\ndef minimalType(ttype):\t\"\"\"\n\tReduce a minimal subclass of a basic type to its basic subclass and\n\treturn that minimal type as a new basictype\n\t\"\"\"\n\ttt =(ttype, ttype)\n\tr=ttype.__new__(tt)\n\tif (ttype is not ttype.type()):\n\t\traise Pycc"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tif not classes:\n\t\treturn []\n\tresult = set()\n\tclasses.reverse()\n\tfor i, c in enumerate(classes):\n\t\tif isinstance(c, int): continue\n\t\t# We don't want to sort classes that have a method < 0 to make them not part\n\t\t# of the default (e.g. in the order in which they appear in an.ini file).\n\t\t# We skip them here - they must be first in a list!\t\n\t\tif c > 0:\n\t\t\tfor j, a in enumerate(classes):\n"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tordered = []\n\tfor item in classes:\n\t\tfor item2 in ordered:\n\t\t\tif item == item2: continue\n\t\t\tif not item2 in classSet: continue\n\t\t\titem2 += \"_c\"\n\t\t\tif classSet[item2] == item: continue\n\t\t\tif not item2 in classSet and classSet[item2] - 1 in itemSet:\n\t\t\t\titem2 = item2.rpartition(\"-\")[0]\n\t\t\t\tordered.append(item2)\n\t\t\t\tif item2[1:].isdigit():\n\t\t\t\t\tcontinue"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tbases = list()\n\tfor k in classes:\tbases.append(tuple([c for c in classes if c is not k]))\n\tbases.sort()\n\tfor k in classes:\tbases.remove(k)\n\treturn bases\n\n\ndef makeBaseList(bases, baseClass=None, startAtLast=False):\n    \"\"\"\n\tGet the list of all base classes in this list of bases which match class k,\n\tin order.\n\n\tIf startAtLast is true, then only bases ending in last class whose order is baseClass\n\tare"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tif classes == None:\n\t\treturn set([cls for cls in classes ])\n\tres = set(cls for cls in classes if cls is not None)\n\tres.update((r for r in classes if not r is None) )\n\treturn res\n\n\ndef isString(c):\n\tif isinstance(c, str):\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef isNotNone(value):\n\tif value is None: return False\n\tif not isinstance(value, basestringtypes): return True\n\treturn False\n\n\ndef"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tif isinstance(classes, str) : classList = classes.split(\",\")\n\telse : classList = classes.tolist()\n\n\tfor i in range(len(classList)):\n\t\tclassIndex = classList[i].index(\"\")\n\t\tminBases[classIndex] = min(minBases[classIndex], classList[i])\n\tif (len(classList) == 2):\n\t\tminBases[1] = minBases[1] + 1\n\n\treturn minBases\n\n\n\"\"\"\ndef getSubset(subset, list = None, n"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tcls = []\n\tfor c in classes:\n\t\tcls.append(c)\n\tfor i in range(len(cls)-1,-1,-1):\t# For last element\n\t\tif cls[i] == cls[i+1]:\n\t\t\tcls[i], cls[i+1] = cls[i+1], cls[i]\t# Order them lexicographically\n\treturn cls\n\ndef basesToLabels(baseClasses, classes, labels):\n\t\"\"\"\n\tConverts an n-ordered list of classes in a class array to a list of"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tdef minimal(baseclass):\n\t\t\"\"\"\n\t\tRecursive implementation of the algorithm\n\t\tof a minimal subset of the classes\n\n\t \t@param baseclass: Reference to the class with ancestors (parent)\t\"\"\"\n\t \tdef find(baseclass):\n\t\t \tfor i in range(len(baseclass.baseClasses)):\n\t\t\t \tprint \"[Class %s is a %s]\" % (baseclass.__class__.__name__, i+1)\n\t\t\t\ttry:\n\t\t\t\t\tif baseclass is baseclass.parentClasses[i]:\t\tbreak\n\t\t\t \texcept:"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tfor base in classes:\n\t\tyield base\n\t\tmin = base\n\t\tfor base in minimalBases(base+2):\n\t\t\tmin = min.__class__\n\t\tyield min\n#@-<<  end minimalBases >>\n\n\n#@debug\n##def printOrderedAlphabet(alphabet, output=sys.stdout):\n##\tif output is None: output = sys.stdout\n##\tall, used, inAlphabet, not inAlphabet = []\n##\tfor base in alphabet:\n##\t\tif base[0]!= '0' and inAlphabet:\n##"}]}
{"task_id": "62b8b4b9eb7e40a82d2d1134", "prompt": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\tfrom.attr import is_attr_deprecated\n\ts = \"Name/Descriptor\"\n\tif not all:\n\t   s = self._name or s\n\treturn s\n\ndef attr_deprecated(self, *a): # pylint:disable=redefined-builtin\n\t\"\"\"For a given attribute.\t\"\"\"\n\tfrom.attr import deprecated\n\twarn_msg =     \"%s is a deprecated attribute\" % self.__class__.__name__ +\\\n\t\t\t\t  \"  Use attr::getattr\"\n\tif is_attr_deprecated(a):\n\t    warn"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t# pylint:enable=redefined-builtin\n\tif all:\n\t\treturn [(name, description) for (id, name, description)\n\t\t\t\tin zip(self.getUuidList(), self.getNameList(), self.getDescriptionList())\n\t\t\t\tif id]\n\telse:\n\t\treturn [(key, description) for key, (id, name, description)\n\t\t\t\tin zip(\n\t\t\t\t\tself.getUuidList(), self.getNameList(), self.getDescriptionList())\n\t\t\t\tif name == key and id in map(int, self.getUuidList())]"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\tglobal _interfaceNamesAndDescriptions\n\trv = \"## \" + str(_interfaceNamesAndDescriptions)\n\tif len(all):\n\t\tfor i,name in enumerate(_interfaceNamesAndDescriptions):\n\t\t\tif all or self._interfaceNamesAndDescriptions[name][0]:\n\t\t\t\trv += \"[\" + str(i+1) + \"] \\\"\" + str(name) + \"\\\"\"\n\t\t\t\tif hasattr(self, name + \"_description\"): \n\t\t\t\t\trv += \" - \" + str(getattr(self, name + \"_description\"))\n\n\treturn rv or \"(no attributes defined)\""}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\tif not all is None and self is None: raise AttributeError(\"'attribute' is not an attribute but a method'\")\n\treturn [ name for (name, val) in self.__dict__.values() if (f.doc_attribute() is None and hasattr(val, \"doc_attribute\") and val.doc_attribute() is not None) or (val is None and f.doc_attribute() is not None) ]\n\n@property\n@doc_attribute(doc_type='Attribute instance', exclude=True)\ndef atts(self, *args, **kwargs): # pylint:disable="}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\treturn [a[0].lower() + '.' + a[1].lower() for a in _attributes_map[self.interface] if (not all or not a) ]\n\n\nclass _InterfaceWrapper(object):\n\n\tdef __init__(self, interface, interface_dict): self.interface = interface # string\n\tdef __repr__(self): return self.interface + '(' +''.join([_type(e) for e in interface_dict[self.interface]] + ')') # object\n\n\tdef __getitem__(self, *args): return _interface_wrapper("}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\treturn getattr(super(BaseElement, self),'namesAndDescriptions', [])\n\t\t\n\t\nclass Element(BaseElement):\n\t# Element base class that allows a class-level access to the class-level attributes defined by Elements\n\t# that has been defined by this class.\n\t\n\t\n\tname = 'Element'\n\t\n\t# List of names and descriptions defined by classAttributes.\n\tnamesAndDescriptions = namesAndDescriptions()\n\t\n\t# Whether the element is enabled.\n\tenabled = False\n\t\n\t# Attributes defined by Element base class.\n\tclassAttributes = [\"name\", \"label"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\treturn getattr(self, 'namesAndDescriptions', ())\n\n\ndef _namesAndDescriptions(self, all=None): # pylint:disable=unused-argument\n\t\"\"\"\n\tReturn attribute names and descriptions defined by attributes.\n\n\t:rtype: tuple\n\t  -  (attribute names, attribute descriptions)\n\t:return:\n\t\t-  (attribute names, attribute descriptions)\n\t\"\"\"\n\t# no point to cache\n\tif self.cache.nameAndDescriptions is None:\n\t\tself.cache.nameAndDescriptions = (getattr(self, 'namesAndDescriptions', ())"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\tif len(self.__class__.interfaces) > 0:\n\t\treturn self._getInterfaceNamesAndDescriptions(all)\n\tfor name, interface in self.__class__.interfaces:\n\t\tif name not in self._interfaceNames or name in self.__allNames:\n\t\t\tlogger.debug(u\"Interface '%s' defined\" % (name,))\n\t\t\tcontinue\n\t\tif interface is not None:\n\t\t\tself._info[name] = (interface, interface.__doc__)\n\treturn [(name, info) for name, info in self._info.items()]\n\n"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\tif all:\n\t\treturn self._attributeNames(), self._attributeDescriptions()\n\telse:\n\t\treturn self._attributeNames(), self._attributeDescriptions, self._descriptorNames()\n\n\tdef get_attribute(self, attrName, attrType, default=None, noReturnDefault=False, onlyInScope=False): # pylint:disable=redefined-builtin,no-self-use,redefined-variable\n\t\tif attrName in self:\n\t\t\treturn self.__getattribute__(attrName, attrType, onlyInScope=onlyInScope)\n\t\telse"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\treturn(self.get_attribute_names_and_descriptions())\n\n\tdef get_attribute_descriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\"\"\"\n\tReturn attribute descriptions defined for this object.\t\"\"\"\n\treturn(self.get_attribute_descriptions())\n\n\tdef get_description(self, attr_name=None): # pylint:disable=redefined-builtin\t\"\"\"\n\t\"\"\"\n\tPurpose:\n\t\tGet description of an attribute.\n\t\"\"\"\n\treturn(self.get_attribute"}]}
{"task_id": "62b8b416eb7e40a82d2d1129", "prompt": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\tfrom..configobj import get_config\n\n\tconfig = get_config(master=self)\n\treturn type(self)._attrs_to_names(config, all)\n\n\tdef _attrs_to_names(self, config, all): # pylint:disable=invalid-name\n\twhile config._attrs_to_names:\n\t\tnames = config._attrs_to_names[all]\n\t\tconfig = config._attrs_to_names.pop()\n\t\tfor name in names:\n\t\t\tyield name, getattr(self, name)\n\t\tif"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\tsetattr(self, 'names', [])\n\n\tif all:\n\t\tself.names = self.attribute_names\n\telse:\n\t\tself.names = [x for x in self.attributes if hasattr(x, \"name\")]\n\t\tassert self.name not in self.names, (self.name, self.names)\n\n\tdel self.attribute_names  # Avoid gc cycles (not really necessary)\n\treturn self.names\n\ndef _copy_attrs_from(self, source_module, attrs_dest):\n\t\"\"\"Make all attributes from source"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\twith self._name_context(all) as context:\n\t\tnames = self._attributes.keys()\n\t\tnames = [name for name, _ in names if name not in\n\t\t         self._attributes.keys() and not all]\n\t\treturn names\n\ndef attributes(self, exclude=None, all=False): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tReturn an attribute dictionary.\n\t\n\tThe attributes are extracted from the interface.\n\t\"\"\"\n\twith self._name_context(all) as context:\n\t"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\tall_interfaces = self._get_all_interfaces()\n\tself._attributes.keys()\n\tfor name in self._attributes:\n\t\tif all or not isinstance(self._attributes[name], type):\n\t\t\tself._attributes[name] = globals()[name]\n\treturn self._attributes\n\n# frozen_string_literal: true\n\nrequire \"active_job/connection_adapters/sql_configuration\"\nrequire \"active_job/connection_adapters/sql_schema\"\nrequire \"database_cleaner\"\nrequire \"support/test_order_schema\"\n\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\treturn _inspect.getmembers(sys, _AttributeNames, all=all)\n\n# -----------------------------------------------------------------------------\n\nclass _AttributeAttributes(_AttributeClass):\n\t\"\"\"A descriptor that is a tuple with all the attributes defined by a class.\t\t\n\t\"\"\"\n\t_attribute_map = {}\n\t_field_names = ( '__init__', )\n\n\tdef __new__(cls, name, bases, attributes):\n\t\tif name in cls._attribute_map:\n\t\t\tobj = cls._attribute_map[name]\n\t\telse:\n\t\t\tobj = object.__new__(cls,"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\tif not all:\n\t\treturn list(self.attribute_names())\n\telse:\n\t\treturn tuple(sorted(self.attribute_names()))\n\t\ndef data(self): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tReturn the element information as a dict of lists with column names sorted\n\tin order of description.\tThe element_info will be a tuple (name,\n\tdescription, array dimension, array dimension) with the name being\n\tthe column name and the description (each type of array element).\n\t\"\"\"\n\treturn self.element_info"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\treturn [c for c in dir(self) if hasattr(c, 'name')]\n\n\ndef _create_attributes(parent, interfaces,\n\t\t\t\t\t\t name=None,\n\t\t\t\t\t\t exclude=None,\n\t\t\t\t\t\t _only=False,\n\t\t\t\t\t\t _init=False):\n\tfor c in interfaces:\n\t\tif exclude is None:\n\t\t\tif c.name not in exclude:\n\t\t\t\tcontinue\n\t\tif c.name == exclude:\n\t\t\tcontinue\n\n\t\tif exclude and c.name not in exclude and c.name!= name:\n\t\t\tcontinue\n\n\t"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\tif all is False:\n\t\treturn self.__all_attribute_names\n\telse:\n\t\treturn [ a.name for a in self.__attributers ]\n\n#include \"CondFormats/PPSObjects/interface/PPSPixelVertexingPS.h\"\n#include \"FWCore/Utilities/interface/typelookup.h\"\n\nTYPELOOKUP_DATA_REG(PPSPixelVertexingPS);\n\n<?php\n/* Copyright (c) 1998-2010 ILIAS open source, Extended GPL, see docs/LICENSE */\n\ninclude_once(\"./Modules/Scorm2004"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\tif self._is_class:\n\t\treturn dict(self._attrs)\n\telse:\n\t\treturn {}\n\n\nclass _NameInfo(metaclass=ABCMeta):\n\n\t# FIXME: The name_index field is used instead of names because\n\t# there seems to be a bug (https://bugs.python.org/issue2574).\n\t# In the Py3k this is called '_name_index', and if for some reason\n\t# the Python 3 code isn't loaded, the field may not exist and the\n\t# type of \"_name_index"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\treturn self.__dict__.keys()\n\n\nclass Property:\n\tdef __init__(self, object, name):\n\t\t# pylint:disable=redefined-builtin\n\t\tself._value = object.getProperty(name)\n\t\t#\n\t\tself.attr_name = (object.getName() if hasattr(object, 'getName') else \"\") + name\n\t\t#\n\t\tself._object = object\n\t\t#\n\t\tself._set_type()\n\t\tself._get_type_info()\n\t\tself._get_default_value()\n"}]}
{"task_id": "62b8b3d6eb7e40a82d2d111c", "prompt": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tif len(sequence)==0: return None\n\n\tfor i in range(len(sequence)):\n\t\tif sequence[i]=='@':\n\t\t\ttry:\tsequence[i]=('id', sequence[i].upper(), sequence[i+1])\n\t\t\t\texcept:\tpass\n\n\treturn sequence\n\n\ndef _build_class(target, interfaces, constructor):\n\t\"\"\"Construct a class from the interfaces and arguments \"\"\"\n\tassert len(target._types)>2\t# a class cannot be an interface\n\tif interfaces: interfaces.clear()\n\tif constructor:"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tif sequence == []:\t\t\t\treturn \"\"\t\t\t\t# empty input?\n\tfor elt in sequence:\n\t\tif isinstance(elt,str):\t\telt = str(elt)\n\t\tif isinstance(elt,(tuple,list)):\n\t\t\telt = _normalizeargs(elt, output=output)\n\t\tif isinstance(elt,interfacetypes):\n\t\t\telt = elt.getdecl()\n\t\tif isinstance(elt,interfacetypes):\n\t\t\telt = elt.getdecl()\n\t\tif isinstance(elt,interfacetypes):\n\t\t\telt = elt."}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tfrom collections import defaultdict\n\tfrom itertools import groupby\n\timport collections\n\n\t# Check for invalid type argument\n\tif hasattr(sequence[0], '__call__'):\n\t\treturn sequence\n\n\t# Determine which specs correspond exactly\n\tdef _findmatch(items, what):\n\t\tfor item in items:\n\t\t\tif item is what:\n\t\t\t\treturn item\n\n\t# Create the new collection\n\targs = collections.defaultdict(list)\n\n\tfor item in sequence:\n\t\titem['_normalize'] = _normalize\n\t\tif isinstance("}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tif not sequence.args:\n\t\treturn\n\tfor item in sequence.args[:]:\n\t\tmatch = re.match(r'\\s*([A-Z][A-Z_0-9]+?)\\s*=>\\s*(.*)\\s*', item)\n\t\tif match:\n\t\t\titem = match.group(2)\n\t\t\tif len(item)>0:\n\t\t\t\treturn item +'='+ _normalizeargs(item[4:-2])\n\t\t\t\tbreak\n\treturn (len(sequence.args), sequence.args[-len(sequence"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tfor d in sequence[:]:\t\t\t\t# make copies of all, remove nested interfaces\n\t\tif isinstance(d, tuple):\t\t# don't bother in empty tuples\n\t\t\td = d[0]\n\t\tif isinstance(d, pyzo.structures.class_mixin.InterfaceClass):\n\t\t\tif output is None:\t\t\t\t\t\t# output is None when only expanding interfaces\n\t\t\t\tsequence = [pyzo.structures.copy_and_expand(d) + d.members()]\n\t\t\telse:\t\t\t\t\t\t\t\t\t# output only if expanding interfaces\n\t\t\t\tsequence = ["}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tfor item in sequence:\n\t\tif isinstance(item, (tuple, Declarions)):\n\t\t\t# Normalization will expand tuples, but not in this case\n\t\t\t# (and not expand anything)\n\t\t\tcontinue\n\t\tif isinstance(item, (Interfaces, TraitObjects)):\n\t\t\t# Interface spec does not need to be added as a component\n\t\t\t# but as a normal argument.\n\t\t\tnormalized = item.names()\n\t\telse:\n\t\t\t# Normalizer should never add other normalizations\n\t\t\t# (and not expand anything)\n\t\t\t"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tif isinstance(sequence, Declarators):\n\t\tfor d in sequence:\n\t\t\t# We want to make sure there are not references\n\t\t\t# to the declarations (such as: module::func1, module::func2;)\n\t\t\t# and this is how Expander.expand() would handle them.\n\t\t\td._check_declaration()\n\n\tif isinstance(sequence, BaseDeclarations):\n\t\tfor d in sequence:\n\t\t\t# BaseDeclarations require both the interface of the declaration\n\t\t\t# and the declaration itself to have a BaseDeclaration (so, for\n"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tif sequence.__class__ is declarator:  \"\"\"\n\t\t# Is the declaration part a declaration declaration?\n\t\tif (hasattr(sequence, 'value') and not\n\t\t\t\tisinstance(sequence.value, (declarator, declaratorList, \n\t\t\t\t                     declaratorList2, declaratorItem))):\n\t\t\tname = ''\t# Leave as is.\n\t\telse:\t\t\n\t\t\tname = sequence.id.name\n\telse:\n\t\tname = ''\n\n\tif"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t_normalize = _default_normalize\n\n\tdef _callmethod(self, obj, methodname, method, *positional,\n\t\t\t\t\tout):\n\t\t\"\"\"\n\t\tCall function using new object and methodname and return concatenated result (possibly shortening).\n\t\tIf only one interface argument is given, the methodname is used.\n\t\tThis can be given more than once. Multiple outputs can be supplied if desired.\n\t\tout is the output of the call.\n\t\t\"\"\"\n\t\targs = list(self.args)\n\t\tnarg = len("}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t# Convert list of Interface objects to list of\n\t# tuple (interface-name, default-values).\n\tinterfaces = {}\n\tfor i, paramdef in enumerate(sequence):\n\t\tif paramdef[2] == 'I':\n\t\t\tname = paramdef[0]\n\t\t\ttry:\n\t\t\t\tdefaults = paramdef[1].split(':')\n\t\t\texcept ValueError:\n\t\t\t\t# paramdef[3] is for a default values specification\n\t\t\t\t# This isn't valid, it's just the empty defaults dict\n\t\t\t\t# We can't do anything with it"}]}
{"task_id": "62b8b3d5eb7e40a82d2d1110", "prompt": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\treturn _opt_module_available\n\n// Copyright (c) Microsoft.  All Rights Reserved.  Licensed under the Apache License, Version 2.0.  See License.txt in the project root for license information.\n\nusing System.Collections.Immutable;\nusing System.Collections.Generic;\nusing System.Diagnostics;\nusing System.Diagnostics.CodeAnalysis;\nusing System.IO;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Microsoft.CodeAnalysis;\nusing Microsoft.CodeAnalysis.Text;\nusing Microsoft.CodeAnalysis.Text.Classification;\nusing Microsoft.CodeAnalysis.Text"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\tfrom.cffi import cffi_enabled, cc_warn_c_file_in_path\n\t# check for C and C++ files\n\tif cffi_enabled:\n\t\ttry:\n\t\t\tfrom. import capi_ext, c_source_file_ext, c_ext, c_cpp_ext\n\t\texcept ImportError:\n\t\t\treturn False\n\t\t# Check if c is present, if not raise ImportError\n\t\tif not hasattr(c_cpp_ext, 'c_file_found'):\n\t\t\t# c_file_found might not be necessary"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\tpath = c_libdir_path()\n\tif os.path.exists(path):\n\t\twith open(path+'.pyc', 'r', encoding='utf-8') as fp:\n\t\t\ttry:\n\t\t\t\timports = __import__('.pyc', fromlist=[''], fromlist_cooked=(), level=0)\n\t\t\t\tif 'lib3to2' in sys.modules:\n\t\t\t\t\tdel sys.modules['lib3to2']\n\t\t\t\treturn imports\n\t\t\texcept:\n\t\t\t\treturn False\n\t\treturn False\n\telse:"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\treturn (sys.modules[__name__]!=None)\n\ndef _c_warnings():\t\"\"\"\n\tReturn the C warnings module, if available, otherwise\na false value.\n\"\"\"\n\treturn (sys.modules[__name__].__warningregistry__!=None)\n\ndef _c_config_dict():\t\"\"\"\n\tReturn a copy of the configuration dictionary.\n\nThis is used to provide access to the configuration parameter\nfor the CCompiler, in the same way it does for the CCompiler.\nIt may be useful to provide, e.g.,\nit can be used to build"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\tif not _optimizations_available:\n\t\traise ImportError(\"the C optimizations are not \"\n\t\t\"available on this system\")\n\n\treturn (True, None)\n\nif _optimizations_available:\n\tdef _c_optimizations():\n\t\treturn _optimizations\nelse:\n\tdef _c_optimizations():\n\t\t# XXX Not sure if this is even necessary, if not available,\n\t\t# we could just use the None optimizer?\n\t\t_optimizations = None\n\t\treturn _optimizations\n\n# _optimizations_cache should"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\ttry:\n\t\t# _optimizes is a method in os, but in some\n\t\t# environments that can be slow to look at. So we\n\t\t# try the heuristic\n\t\t_optimizes = sys.modules['__main__'].__optimize__\n\texcept AttributeError:\n\t\t_optimizes = None\n\n\ttry:\n\t\t_optimizes = _optimizes[1] # the default __optimize__\n\texcept IndexError:\n\t\tpass\n\n\ttry:\n\t\toptimizer = _optimizes['fmax']\n\texcept AttributeError:\n"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\toptim_module = os.environ.get('PYSPARK_OPTIMIZATION_MODULE')\n\tif optim_module is not None:\n\t\treturn optim_module\n\n\ttry:\n\t\treturn sys.modules['opt']\n\texcept KeyError:\n\t\treturn None\n\nclass UnavailablePySpark():\n\t\"\"\"\n\tA module to indicate that a PySpark session is unavailable.\n\n\tUsed to indicate an error when a PySpark session is unavailable.\n\t\"\"\"\n\n\tdef __init__(self, message=None, is_unavailable=False):\n\t\tif"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\ttry:\n\t\treturn hasattr(optimizations, \"get_c_opt_level\")\n\texcept AttributeError:\n\t\treturn False\n\ndef _optimize_available(optim, do_fast=False, do_c_optimizations=False):\n    \"\"\"Do a single optimization pass.\n    \n    Optimization may be performed immediately without being\n    saved in the global optimizer state.  Returns 0 if all runs\n    of the optimizer succeeded, or the number of optimization\n    passes left to do before an error occured.\n\n "}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\tc = set()\n\tc.update(_c_optimizations)\n\treturn c\n\ndef _set_optimizations(func):\n    \"\"\"\n    Set function to be optimised.  This function is called\n    (if appropriate) one or more times, with three arguments:\n    * the context (as _c_context).\n    * the function whose argument is the argument with which\n    this function was called.  For example, the function\n    _set_optimizations(set_optimize_int) may"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\tif _c_optimizations_available.used:\n\t\treturn _c_optimizations_available.ret\n\n\t_c_optimizations_available.used = True\n\ttry:\n\t\t_c_optimizations_available()\n\texcept RuntimeError:\n\t\tsys.stderr.write(\"importing the C optimizations might fail.\\n\")\n\t\t_c_optimizations_available.ret = False\n\n\treturn _c_optimizations_available.ret\n\n_available._always_b = True\n\ndef _module_loader(path):\n\t\"\"\"\n"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110f", "prompt": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t#\tThere are three basic places where we need to look.\n\t#   1) _use_c_impl - use the \"optimization level\" (>=O3)\n\t#\t2) O3 - force no optimization\n\t#   3) c_optimize - try to use some experimental optimization level (>=0)\n\tif sys.platform[:5] == 'py' or (\n#\t    O3 and PYTHONO3[1] == \"c\" or PYTHONO3[1] == \"3\"\n\t    sys.platform[0]"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\treturn _use_c_impl in ('none', bool) and \\\n            'c' not in ('none', bool) and \\\n            'py' in ('none', bool)\n\n#\n# C-compatibility support\n#\n\n_c_impl_compatible = True\n\ndef _get_c_impl_compatible():\n\t\"\"\"\n\tReturn the name of the C function for which the function should be\n\tcompiled.\n\t\"\"\"\n\tif not _c_impl_compatible"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\treturn get_compiler_in_compiler_flags(compiler, 'CFLAGS')\n\n_use_c_impl = get_compiler_in_compiler_flags('Py_ENABLE_SHARED', 'CFLAGS', 'IMPLIB')\nif _should_attempt_c_optimizations():\n\timport imp\n\nelif sys.platform == 'win32':\n\tfrom.warnings import ImpLibError as ImpLibWarning\n\timport impwin32\nelif sys.platform.startswith('linux'):\n\timport imp\n\timport fcntl\n\timport os\n\timport sysconfig\n\timport tempfile\n"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\tif self._use_c_impl:\n\t\tif self._c_optimizations == \"ON\":\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\telif self._pure_python:\n\t\treturn self._c_optimizations == \"ON\"\n\telse:\n\t\t# If we can't figure a way to know if we should perform C\n\t\t# optimizations, just use the value of the environment\n\t\treturn self._c_optimizations\n\n\n\nclass _Internals:\n\t\"\"\"\n\tInternal helpers for the :class:`_"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\tif '_USE_C_IMPL' in os.environ:\n\t\treturn 1\n\telse:\n\t\treturn 0\n\n_use_c_impl = 1\n_use_c_optimizations = _should_attempt_c_optimizations\n\ndef _override_c_impl(value):\n\t\"\"\"Call this from Python's _PyUnicode implementation to override\n\t``PURE_PYTHON_C``.  This changes value of\n\t`_use_c_impl` to '0' or '1' (and '0'/'1'/'off'/'false'). "}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t# First try to detect PyPy, without using it to do everything.\n\ttry:\n\t\tdetect_cxx = check_environ(environ, check=False)\n\texcept RuntimeError as e:\n\t\tif e.args[0] == '-c/pure_python':\n\t\t\twarn_with_traceback(e)\n\t\t\treturn False\n\tif detect_cxx:\n\t\treturn True\n\t# If it's not in Python, attempt to use C's optimizations.\n\t# This only runs if the C libraries are available.  If your system\n\t# has"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t_use_c_impl = False\n\tif sys.platform == 'win32':\n\t\tfrom _set_import_handle import (\n\t\t\tset_import_handle, SetImportHandleA)\n\t\tif SetImportHandleA(None):\n\t\t\t_use_c_impl = True\n\treturn _use_c_impl\n\ndef _create_compiler(use_stdlib=True):\n\t#\n        from. import gc\n\n\t# XXX This is a duplicate of a bunch of functions from libm\"\"\"\n\timport libm"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\tuse_c_impl = get_bool_from_config('pure_python_use_c_impl')\n\t# If either are not present, or the above function call doesn't\n\t# return anything useful, we'll fall back to the old behavior.\n\treturn use_c_impl or get_config_option('pure_python_disable_old_C_impl')\n\ndef _can_run_c_optimizations():\n\t\"\"\"Determine whether PurePython can successfully run C optimizations.\"\"\"\n\tif _is_windows():\n\t\treturn 0\n\treturn ("}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\treturn get_option('USE_PY_C_ONLY')\n\ndef _validate_optimization():\n    \"\"\"Perform the actual validity checks on our options\"\"\"\n\n    # This is the default for _optimizer, but there is a small chance\n    # it might be missing, so we check for it.\n    solver_name = get_option('OPTIMIZER')\n    if solver_name is not None:\n        assert not solver_name in {'default', None}, \\\n         "}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\tif self.interpreter.compiler_directives is not None:\n\t\t# C is enabled.\n\t\treturn True\n\tif self.interpreter.compiler_directives is not None:\n\t\t# On PyPy, if we're on PyPy, the value of the\n\t\t# ``PURE_PYPY`` variable is defined on PyPy.\n\t\tif sys.version_info >= (3, 4):\n\t\t\treturn True\n\treturn False\n\ndef _make_optimize(code_str, interpreter=None, verbose=None, options=None,\n    "}]}
{"task_id": "62b8b3d4eb7e40a82d2d110e", "prompt": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\tpass\n\npackage com.walmartlabs.concord.server.security.authentication;\n\n/*-\n * *****\n * Concord\n * -----\n * Copyright (C) 2017 - 2018 Walmart Inc.\n * -----\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n * \n *      http://www.apache.org/licenses/LICENSE-2.0\n * \n * Unless required by applicable law or"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t# For the moment, the reasoning behind this is that\n\t# there are more than likely to be problems with\n\t# `opt.get_value_from_config()` at one time.  For example,\n\t# suppose an optimizer (in `opt(optimizer.cfg)`)\n\t# adds a new option called `optimizer.max_grad_norm.\n\tfrom torch import nn\t# noqa\n\n\ndef _c_optimizations_required():\t\"\"\"\n\tThe opposite of `_c_optimizations_ignored`.\t\"\"\"\n\t# Check the backwards compatibility from\n\t"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\tif hasattr(self, \"_ignoreOptimize\") and self._ignoreOptimize: return\n\t\tif not self._c_optimizations_required(locals()):\n\t\t\treturn\n\t\tself._ignoreOptimize = True\n\t\treturn\n\n\tdef _c_optimizations_required(self, locals()):\n\t\t\"C implementation version of required optimization of function parameters, ignoring function parameters not used for the purpose of optimizing.\"\n\t\t\"\"\"\n\t\ttry:\n\t\t\tfor variable in self.variables:\n\t\t\t\tif variable in locals():\n\t\t\t\t\traise Exception(\n\t\t\t\t\t\t"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\tc = xrange(5)\t\t# Including the first and last\n\td = [1,0,2,3]\t\t# Within the first, middle, and last\n\n\tfor c, d, i in (\n\t\t\t[1,0,2],\n\t\t\t[0,1,3],\n\t\t\t[2,3,0],\n\t\t):\n\t\tfor d in _c._c_optimizations_required(c, i):\n\t\t\tpass\n\tprint d\n\ndef _c_optimizations_ignored():\n\tc"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\tpass\n\ndef _c_optimizations_required():\n\t\"\"\"\n\tIgnore culling and depth testing.\n\t\"\"\"\n\tpass\n\n# Local Variables:\n# tab-width:4\n# indent-tabs-mode:nil\n# End:\n# vim: set expandtab tabstop=4 shiftwidth=4:\n\nusing System;\n\nnamespace Volo.Abp.BlobStoring\n{\n    public class SimpleBlob\n    {\n        public string Key { get; }\n\n       "}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\treturn [(\"numpy\", \"1.2.0\")]\n\ndef _c_optimizations_required(name, required):\n    if \".\" in name:\n        # handle '.[opt]', '.opt[opt]', where '.' are optional\n        opt = name.split('.')[0]\n        # special case: 'opt[opt]' => always ignored\n        if opt == 'opt' and \".[opt]\" not in name:\n       "}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\tc_optimizes = [(i, None) for i in xrange(1000)]\n\tfor i in xrange(1000):\n\t\tc_optimizes[0][i] = lambda i: None\n\treturn tuple(c_optimizes)\n\n\"\"\"\n\n\ndef main():\n\tfrom matplotlib.testing.decorators import check_figures_equal\n\timport doctest, inspect\n\timport string\n\timport tempfile\n\n\n# test_case = test_doctest + test_figures + test_random_seed_args\ntest_case = [\n\tdict"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t#if \"b_\" in op_names:\n\t#\treturn True\n\treturn False\n\n\n#---------------------------------------------------------------------------\ndef _c_convergence_msg():\n\tmsg = \" \".join(c_convergence_msg)\n\tif msg!= \"\":\n\t\timport warnings\n\t\t\n\t\tif len(warnings) > 0:\n\t\t\timport sys\n\t\t\timport subprocess\n\t\t\texec(warnings.format(warnings.pop(0)), [], sys.argv)\n\treturn msg\n\n\n#---------------------------------------------------------------------------\ndef _c_convergence_msg2():\n\t"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\tpass\n\n\ndef _c_optimizations_required(self):\n\t\"\"\"\n\t:return list: a list of :class:`optimizer` names. These can be used to add :attr:`optimizer.name` and :attr:`optimizer.params` to model's parameter list.\n\t\"\"\"\n\tpass\n\n\n@_c_optimizations_ignored()\ndef _c_optimizations_for_new_param(self, param):\n\t\"\"\"\n\tTries to add only new parameter options.\n\n\t:param param: the :class:`Param` object to add the"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\tfrom. import ops\n\toptimizer_names = sorted(set(ops.list.keys()))\n\tfor op_name_filter in optimizer_names:\n\t\top_name_filter.optimize_module_names()\n\ndef _c_optimizations_allowed(c_optimizations, force=False):\n\t\"\"\"\n\tDetermine what operations can be optimized for a given configuration,\n\tgiven a list of possible optimizations.\n\n\t@param c_optimizations: An ``op.__dict__`` of the supported configuration.\n\t@param force: Preserve the optimization rules in"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110d", "prompt": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\tif not config.pure_python:\n\t\treturn True\n\t# If we have not yet configured for pure python, do our best\n\t# and bail if they don't exist\n\tif config.optimization!= \"none\":\n\t\tos.environ[config.optimization] = \"1\"\n\t\treturn True\n\t# and just return the value in that env var\n\tos.environ[config.optimization] = \"1\"\n\treturn True\n\n\n@_deprecate_positional_args\ndef _c_optimizations_required(obj, opts, p):"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn os.getenv('PURE_PYTHON', 'True') == 'True'\n\n\ndef _get_c_file(fname, src_path=None, c_impl='text'):\n\t\"\"\"\n\tReturn the value for a C file that provides the functionality of the given file.\n\tWhen C files do not provide the functionality (ie. the C functions not part of the C file), use\n\tthe ``pure_python`` variable instead.\n\n\tThe given C file is assumed to be part of the src_path.\n\n\tIf src_path is not provided it returns the"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\tvalue = None\n\tif USE_C:\n\t\tif USE_PYTHON:\n\t\t\t# the C optimizer doesn't need any options to be compiled\n\t\t\tvalue = True\n\treturn value\n\n# \t@property\n# \tdef _use_c_lib(self):\n# \t\treturn self.USE_C_LIB\n\t\t\n\t\n\tdef _use_c_impl_for_cpp_lib(self):\n\t\t# We don't want to use any C optimizations from a different compilation unit.\n\t\tif self._use_c_lib:\n"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn pep425_use_c_impl_true(\"_use_c_impl\")\n\n\ndef _make_optimization_dir(path):\n\t\"\"\"\n\tCreate a directory in the given path recursively.\n\n\tSee :func:`_make_pythonlib`.\n\t\"\"\"\n\tif not os.path.exists(path):\n\t\tos.makedirs(path)\n\n\tpypath = os.path.join(path, \"src\", \"opt\", \"Pep425\")\n\n\twith open(pypath + \".py\", \"w\") as"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn int(not os.environ.get('PURE_PYTHON'))\n\n#--------------------------------------------------------------------------------------------\n# _get_available_modules_impl()\n#--------------------------------------------------------------------------------------------\ndef _get_available_modules_impl():\t\n\t\"Get the available modules without importing them.\"\n\treturn _modnames_and_deps\n\t\t\t\t\n\n#--------------------------------------------------------------------------------------------\n# _c_impl()\n#\n#\tLook for ``_c_`` subdirectory.\n#\n\ndef _c_impl():\t\t\t\t\t\t\t# For Cython 2.0\n\n\timport _use_python_impl as"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn _PURE_PYTHON\n\n\ndef _c_enable_optimizations():\n\t\"\"\"\n\tEnable C optimizations.\n\t\"\"\"\n\tfor mod in _get_module_names():\n\t\ttry:\n\t\t\tmodule = __import__(mod, globals(), globals(), ['__name__'])\n\t\texcept ImportError:\n\t\t\tcontinue\n\t\ttry:\n\t\t\tsetattr(module, 'enable_optimizations', 1)\n\t\texcept AttributeError:\n\t\t\tpass\n\t\tfor k in _get_module_names():\n\t\t\ttry:\n\t\t\t\tmodule ="}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\tuse_c_impl = os.environ.get('PURE_PYTHON') is not None\n\treturn (use_c_impl and (not use_c_impl.endswith('0'))  or\n\t\t(isinstance(use_c_impl, (list, tuple)) and\n\t\t (len(use_c_impl) > 1) and\n\t\t (not use_c_impl.startswith('0'))))\n\n/*\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n *"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn \"PURE_PYTHON\" in os.environ\n\ndef _c_api_supported():\n\t\"\"\"\n\tReturn True if the C API is supported for C-execution\n\n\tThis uses C header files to see if any functionality is present on\n\tthe ``cmake`` executable.\t\"\"\"\n\treturn _c_impl_required() and (not _c_api_impl)\n\ndef _use_c_impl(impl=None):\n\t\"\"\"\n\tReturn True if C-API code is provided for a C target build\n\n\tThis uses either the ``C_API"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn C_OPTIMIZATION_REQUIRED\n\ndef _c_lib_is_available(_name_suffix=None):\t\"\"\"\n    Return true if the C extension is available.\n\nThis function returns True in the case with a C compilation\nlibrary which contains C or Fortran.  A C compilation library,\ncalled ``_lib_name``, is available and can be obtained from\n:mod:`ctypes` as provided by the Python ``pymalloc`` package,\nor from the registry.\n\nParameters\n----------\nname_suffix : str\n    Specifies the suffix of the C extension"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\tfrom numpy.core.umath import _USE_C_IMPL\n\t__PURE_PYTHON__ = True\n\treturn _USE_C_IMPL()\n\n\ndef _force_array_wraparound(arr):\t# Forced array arr is a numpy.ndarray wrapped in arrays\n\tif hasattr(arr, 'ndim') and arr.ndim == 1:\n\t\tarr = np.wrap(arr, dtype='object')\n\treturn Ndarray(arr)\n\n\ndef _c_get_lib_name():\n\t# From https://www.numpy.org/dev"}]}
{"task_id": "62b87b989a0c4fa8b80b35ee", "prompt": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tself.__dict__ = dict()\n\n\n\nclass Histogram (object):\n\t\"\"\" A wrapper of a histogram object\n\nThis class is intended for use by a ``tablib.Histogram`` object\ndirector to access individual histogram elements.\n\nTo avoid overhead, these objects are provided \"owning\"\nclass ``Histogram`` objects.  This class contains all\nthese methods that should not be called directly, but\nshould be directly accessed: ``get_bins`` or ``set_bins``.\nThese methods should not contain any \"return\" statements\nor \"exceptions\".  See these documentation in the"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tif self:\t\t\t\t\t\t# if we are defined, then do it :\n\t\tself.name\t\t\t\t= ''\t\t\t\t# this is a pointer to the name you requested\n\t\tself.make_bins\t\t\t()\t\t\t\t# this is a pointer to a function to make the bins.\n\t\tself.total_bins\t\t\t= 0\t\t\t\t# this is a pointer to the total number of bins\n\t\tself.counts\t\t\t\t= 0\t\t\t\t# the number of bins the count can have\n\t\tself.initial_value\t\t= 0\t\t\t\t# initial number for the"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tself.clear_bin_count()\n\tself._count_bins(True)\n\n/*\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * See the NOTICE file distributed with this work for additional\n * information regarding copyright ownership.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\"\"\"\n\t# first check the user settings:\n\tif not self.settings:\t# check for default user settings\n\t\tself.settings['histogram']=None\n\tif self.settings['histogram']:\t# initialize the user settings\n\t\tself.settings['histogram']=(len(self.hist)-1,1)\n\t\t# reset bins with last one in this range\n\t\tself.bins=[[] for i in range(len(self.hist))]\n\t# make the bins by the user setting and initialize bins\n\tif self.settings['histogram']:\n\t\tfor"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tself._dict = {}\n\tself._initial_value = self.make_bins()\n\n# -----------------------------------------------------------------------------\n#  init the histogram\nhist = Histogram()\n# -----------------------------------------------------------------------------\n\ndef histogram (hist_name):\n\t\"\"\"\n\tMake a new histogram\n\n\tArgs:\n\t\thist_name: the name of the histogram to be created\n\n\tReturns:\n\t\tthe created histogram.\n\n\t\"\"\"\n\treturn hist\n\t# hist = Histogram (hist_name, hist_id=None, min_count=0,\n\t# \tmax_count=None"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tself.__docstring = __docstring\n\n#--------------------------------------------------------------------------------#\n\t# get info on how often a particular bin is split\n\n\tdef get_initial_value(self, context):\n\t\t# return a list as context is used, so when\n\t\t# context.pop() is done during the get_max_width() method\n\t\t# we'll call context.pop(), then pop() again to get the value\n\t\t# and reassign it\n\t\treturn context.pop()\n\n\n\tdef get_max_length(self, context):\n\t\tif"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tif self._bins!= {}:\n\t\tself._bins = {}\n\t# reset all values\n\tself._bins[self._value] = {}\n\tself._value = None\n\tself._range = (np.linspace(\n\t\t\t\t\t\tself.min_value,\n\t\t\t\t\t\tself.max_value,\n\t\t\t\t\t\tself.num_bins,\n\t\t\t\t\t\tnp.log2),\n\t\t\t\t\t\t) + self.bins_uniform_range.step_uniform)\n\n\t# for uniform, uniform bins are set as 0.5.\n\t# set to 0 again because"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tself.__dict__ = {}\n\n\ndef make_bins(hist=None, base=3, minval=1, npts=100, \n\thistval=None, bin_size=0):\n\t\"\"\"\n\tInitialize histogram bins from initial values.\n\n\tArguments:\n\t\t\thist (list): A histogram from which to compute the histogram.\n\t\t\t\n\t\t\tbase (int): The number of decimal places to add to the \n\t\t\thistogram bins during computation.  If it is not specified, \n\t\t\tthe full binning will be computed.\n\n\t\t\tnpts ("}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tself.bins = {}\n\tself.bins_key = {'first':0, 'previous':1, 'last':2, 'next': 3,'mid':4}\n\tself.bins_keyname = {'left': 'left', 'right': 'right', 'center': 'center','mid':'mid', 'last': 'last', 'next': 'next'}\n\tself.bin_names = ['first', 'previous', 'last', 'next','mid', 'last.last', 'next.right', 'next.mid', 'next.first']\n\t"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t# self.__context is reset to the corresponding instance, if any\n\tself.__context = {} if self.__context is None else self.__context\n\t# initialize dicts with all entries equal to their counterparts\n\tself.__context['hist'] = {}\t\t\t# histogram of value counts\n\t# initial value for *a* count\n\tself.__initial_value = self.initial_value\n\t# list all bins, keyed by bin name\n\tself.__bins = self.make_bins()\n\n# __add__ = _add_to_a_count = _add_to_"}]}
{"task_id": "62b87b859a0c4fa8b80b35d7", "prompt": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tif self.is_multidimensional():\n\t\tdef __to_csv__(self, data_matrix=None, separator=\"\\t\"):\n\t\t\tif len(self.points)==0:\n\t\t\t\treturn None\n\t\t\tpoints = []\n\t\t\tfor d in range(len(self.points)):\n\t\t\t\tfor i in range(len(data_matrix)):\n\t\t\t\t\tval = data_matrix[i, d][0]\n\t\t\t\t\tpoints.append(val)\n\t\t\treturn separator.join(points)\n\tif header:\n\t\treturn None\n\t"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tif isinstance(self.nodes, tuple):\n\t    to_csv = self.to_table\n\telse:\tto_csv = self.to_table_csv\n\tvalues = getattr(self, \"values\", None)\n\tif values:\n\t    return values.applyfunc(to_csv)\n\telse:\n\t    return to_csv(separator, header)\n\n\nclass FromCSV(object):\n\t\"\"\" Converts a sequence of strings into a table, with\n\ta special case for specifying the separator character,\n\twhen the header has not yet"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tif len(self) == 0:\t\t\t\t\t\t\treturn \"No data detected.\"\n\tif len(self) == 1:\t\t\t\t\t\t\treturn \"First point found.\"\n\tvalues = list(self.values())\n\tif len(values) < 2 or len(values) > 3 or (len(values) == 2 and values[0] is None):\n\t\treturn \"Invalid size of data array.\"\n\treturn csv.writer(csv.writer(sys.stdout, dialect=csv.excel), delimiter=separator)\n\t\ndef print(self, sep='|', file=sys."}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tif header is None:\t\theader = lena._lena_defn_separator\n\tself = self.copy()\n\tfor name, point in self.nodes.items():\n\t\tstrval = point.as_str(header)\n\t\tstrval = strval.replace(separator, \"\")\n\t\tself.add_node(self.nodes[name], strval)\n\t\tstrval = \"\"\n\tself.export(self.nodes, header=header)\n\treturn self.save()\n\n\nclass Graph(BaseObj, _BaseItem):\n"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from collections import defaultdict\n\n\tdef _do_not_do_it_again(obj, in_dict=False):\n\t\tif not in_dict:\n\t\t\treturn [obj]\n\t\telif isinstance(obj, Graph):\n\t\t\t_do_not_do_it_again(obj.coord, in_dict)\n\t\t\t_do_not_do_it_again(obj.points, in_dict)\n\t\telse:\n\t\t\treturn [obj]\n\t\n\tfor (coord, in_dict"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tparser = ToCSV.parser\n\tparser.add_argument('-s', '--separator?', action=\"store\", type=str,help=\"\")\n\targs = parser.parse_args(argz = (self,))\n\n\treturn args\n\ndef to_hdf5(self, path=''):\t\"\"\"Convenience to :func:`pydot.graph.to_hdf5 <pydot.graph.ToHDF5>`.\n\nConvert graph's points to hdf5 with optional extension (only if present).\n\n*path*, if not ``'']`` then the file name is"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tt = self.ToCSV(separator) + \"\\n\"\n\tfor p in self:\n\t\tself._p = p\n\t\tself.AddPoint()\n\t\tself.data.WriteList(t)\n\t\tt = t[:-1] + \"\\n\"\n\tself.data.__del__()\n\n\tif header!= None:\n\t\tt = header\n\tself.AddPoint()\n\tself.data.WriteList(t)\n\tt = t[:-1] + \"\\n\"\n\tif p!= self.head:\n\t\tself.Add"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tpass\n\n\nclass ToTensor(ToCSV):\n\t\"\"\"\n\tConvert graph's points to tensor, with support for saving and loading.\n\n\tIn the following format, every node, every edge, and\n  \tevery node in a directed graph (i.e., nodes in which edges\n  \toverlap) are converted to tensors.  For example, if a graph such\n  \tas the following is constructed, the *Save* function may be used,\n  \tbut the format will be:\n\n\tnode_x   node_y   edge_x  "}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tif ((isinstance(self, str) and\n\t\t len(self) == 0) or\n\t\t (isinstance(self, Iterable) and\n\t\t len(self) > 0) or\n\t\t type(self) is not Iterable):\n\t\traise AttributeError(\"not writable: %r\" % self)\n\tfor i, v in enumerate(self):\n\t\tif v is None:\n\t\t\tif self.header is not None:\n\t\t\t\t(header,) = self.header\n\t\t\telse:\n\t\t\t\theader = None\n\t\t\tself[i] = header\n\t\telse"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tif header is None:\n\t\theader = None\n\tlinesep = separator.replace(\",\", \" \")\n\treturn [str(p.coor).replace(\"\\r\", \"\").replace(\"\\n\", \"\\\\n\").replace(l, l.replace('\\\\', l))+linesep +v for v,l in self.nodes.items()} if self.nodes else []\n\n\tdef to_csv(self) :\n\t\t\"\"\"\n\tConvert graph's points to CSV.\n\n*separator*, if not ``\"\\n\"```, is the first string of the output\n(new line is added automatically"}]}
{"task_id": "62b87b839a0c4fa8b80b35cb", "prompt": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tif coord_name is None: return None\t# self is empty\n\tn_errors = 0\n\tfor node in self.nodes:\t\t\t\t\t\t# Loop over nodes\n\t\tn = len(node)\n\t\twhile node:\t\t\t\t\t\t\t# Move to end of n\n\t\t\tk = node.key(coord_name)\n\t\t\tif not k:# If the coordinates match\n\t\t\t\tbreak\t\t\t\t# break out of loops\n\t\t\tnode = self.nodes[k]\t\t\t# Now node = list(self.nodes['nodename'])[(coordinates==nodename)^(2"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tres = self._call(_error_method, coord_name)\n\treturn_info = _res_list(res[0], \"return_info\")\n\treturn return_info[0]\n\ndef _list_of_coords(self, **kwargs):\n\tkwargs['coords'] = self.coords.T\n\treturn _return_self(kwargs)\n\nclass CoordSystemError(CoordTypeError):\n\tpass\n\n\nclass CoordSystem(object):\n\tr'''Coord system class, represents the data of a coordinate system\n\t(coordinate axes data structures and a coordinate array"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tcoord = self._get_coord(coord_name)\n\tif coord is None: return []\n\terr_ind = []\n\tfor a in coord.data.shape[1:]:\n\t\terr_ind.append(a - self._err_start_idx)\n\treturn err_ind\n\n\tdef _get_coord(self, coord_name):\n\t\tcoord_name = self.spec.get_coord_name(coord_name)\n\t\tif coord_name is not None:\n\t\t\treturn self.spec.coord_dict[coord_"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tresult = {}\n\tdim = self.dim()\n\tncoords = coord_name.count('.') + 1\n\tfor axis in range(dim):\n\t    try:\tcoord_name = coord_name[:-len(self.prefix_ + str(axis))]\n\t\tindices = [i for i in dim ]\n\t\tindices.sort()\n\t\tif axis == 0:\n\t\t    coord_name = coord_name[:-1]\n\t\telse:\n\t\t    indices.reverse()\n\t\t    indices.reverse()\n\t\tcount = 1"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tif coord_name[0:4] == 'coord/i': # I-coordinate\n\t\tret = []\n\t\tif coord_name[8:12] == 'coord/i':#-H- coordinate\n\t\t\tret.append(0)\n\t\telif coord_name[8:12] == 'coord/ip':#-p- coordinates\n\t\t\tret.append(4)\n\t\t\tif coord_name[12] == 'coord/i':#j,k,l,m,... coordinates\n\t\t\t\tret.append(8)\n\t\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tfrom EMAN2 import eman2_cppwrap\n\teris = eman2_cppwrap.eman2_to_tf(self.im, self.ctf['nx'], self.ctf['ny'], self.ctf['nz'], ecv=self.ecvecs)\n\t# This is assuming the first half of the coordinates are the origin\n\teris -= ((2*self.coord_offset)//2,)\n\teris[self.coord_name] = 0\n\teris[self.coord_name+1] = self.nx\n\teris["}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\terr_coord = [self.list_err_index.get(coord_name) for coord_name in self.coordList]\n\treturn err_coord\n\ndef _save_coord(coord_name, coords):\n\tf = open(\"coord\",\"w\")\n\tif \"coord\" in coords:\n\t\tf.write(coords[coord_name].c_str(orient=self.coord_orientation[coord_name]))\n\telse:\n\t\tf.write(\"0.0\")\n\tf.close()\n\ndef _read_coords(params):"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tif coord_name == \"None\" :\n\t\tcoord_index = []\n\t\tx_err = []\n\t\ty_err = []\n\t\tz_err = []\n\t\tu_err = []\n\t\tv_err = []\n\n\t\treturn coord_index, x_err, y_err, z_err, u_err, v_err\n\t\tdie(\"Error: Coordinate '%s' does not exist.\" %(coord_name))\n\telse :\n\t\tcoord = self.master_mesh.coord_name_to_coors"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\ttry:\n\t\treturn self._coord_dict[coord_name]\n\texcept KeyError:\n\t\traise ValueError(\"Coordinate '%s' not found.\" % (coord_name,))\n\n\n# ____________________________________________________________\n#\n# _____________________________________________________________\n#\n# Basic class\nclass CoordinateVector(object):\n\t\"\"\"A 2D array-like coordinate matrix, for efficient indexing\"\"\"\n\tdef __init__(self):\n\t\t\"\"\"Create an empty CoordinateVector\"\"\"\n\t\tself.coords = []\n\n\tdef __getitem__(self, coord):\n\t\t\"\"\""}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tif self.use_vwg:\n\t\terrors_dict = self.vwg.errors_dict\n\telse:\n\t\terrors_dict = self.errors_dict\n\t\n\tif not self.use_vwg:\n\t\t# Get a subset of the errors\n\t\terrors = [coord_name]\n\t\tcoords = []\n\t\tnum_coords = len(errors)\n\t\tfor i in range(num_coords):\n\t\t\tcoords.append(errors[i])\n\t\terrors = coords\n\t\n\treturn np.array(errors, nd"}]}
{"task_id": "62b87b7e9a0c4fa8b80b35bc", "prompt": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tfor name in (\"error\", \"error_\"):\n\t\tif name.find(\"x_\") == 0:\t# if \"x_\"\n\t\t\tif name == \"x\" :\t\tname = \"t\"\t# we use t to represent errors\n\t\t\tif hasattr(context, name) or name in context:\n\t\t\t\tif hasattr(context, name):\n\t\t\t\t\tdelattr(context, name)\n\t\t\t\telse:\n\t\t\t\t\tdel context.dealias(name)\n\n\t# Append error/error_low subcontext.\n\tfor name in context:\n\t"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tfrom pmy.graph import ToCSV\n\tcontext_set = set(context)\n\tif len(context_set) > 1:\t# just some context\n\t\tself.error = ToCSV()\t# assume a single field (the \"E\")\n\t\tfor context in context_set:\n\t\t\tif len(context.values) > 1:\n\t\t\t\t# if \"error_E_low\" exists, convert it to a field of \"error_E_\",\n\t\t\t\t# using the \"z\" index (which is zero-indexed)\n\t\t\t\tlow_index"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\twith errors.catch():\n\t\tfor name, n in self.structure.items():\n\t\t\tif isinstance(self.nodes[n], list): n = _unsort(self.nodes[n], -1)\n\t\t\telif isinstance(self.nodes[n], tuple): n = _sort(self.nodes[n], -1)\n\t\t\ttry:\n\t\t\t\tself.graph.update_context({name: n}, self.nodes[n], **context)\n\t\t\texcept errors.ValidationError: pass\n\n@deprecate()\ndef _sort(self,"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tfor e in dir(self):\n\t\tif self.get(e, {}).get(\"__name__\", \"Unknown_name\") == self._name:\n\t\t\t#print \"*update* e\", repr(e)\n\t\t\ttry:\n\t\t\t\tfor k,v in self.properties(e):\n\t\t\t\t\tv.update_context(self[e.__name__][k])\n\t\t\texcept:\n#\t\t\tnew = self.to_dict(exclude=(e.__name__, k))\n#\t\t\t\tif self[e.__name__][k] is not None:"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t_check_context_valid(context)\n\n\tfor v in context.vars:\n\t\tif v.name in self.variables:\n\t\t\tname = self.variables[v.name]\n\t\telse:\n\t\t\tname = v.name\n\n\t\t_check_varinfo(v)\n\t\tv.name = v.destructure(name)\n\n\t_check_varinfo(context.destructure(v.type.field.name))\n\n\tcontext.name = context.destructure(context.name)\t# XXX: name is not"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tself.context = context\n\tif self.context is None:\n\t\t# empty graph! remove all subcontexts:\n\t\tself._context = None\n\nclass GraphSet(GraphCollection, GraphFlows):\n\t\"\"\"Abstract class for graphs with a set of edges.\n\n*Graph sets* are graphs objects used to represent data\nas well as edges.  They are created from scratch,\nand are used as an efficient way to store *graph* data.\n\nExample usage::\n\ngraph = mdi.GraphSet()\ngraph.create_edges(10, 3)\t"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tp.clear(*self['context'], deep=True)\n\n\tfor i in range(self['context'].fields[\"E\"].nlevels):\n\t\tif i > 0 and i!= self['context'].fields[\"E\"].nlevels - 1:\n\t\t\tfor idx in range(p.shape[1]):  # 1-based\n\t\t\t\tif idx in p:\n\t\t\t\t\tidx = len(p[idx]) - 1\n\t\t\t\telse:\n\t\t\t\t\tidx = -1\n\t\t\tp = p[:i][idx]\n\n\t# Convert data\n\tfor j,"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tcontext_dict = {}\n\tcontext_dict['values'] = []\n\tcontext_dict['errors'] = []\n\tcontext_dict['nit'] = []\n\tcontext_dict['errors_E_low'] = []\n\tcontext_dict['t'] = []\n\t\n\tfor i in self.vertices.values():\t\tcontext_dict['vertices_E_low_'+str(i) ] = []\n\tfor i in self.parents.values():\t\tcontext_dict['parents_E_low_'+str(i) ] = []\n\tfor i in"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tif context.error!= None:\t\t\t\t\t# append with indices\n\t\tfor idx, x in enumerate(context.error):\n\t\t\tcontext.loc[self] = self.loc[self] + (x, self.y[idx])\n\t\tfor idx, x in enumerate(context.value):\n\t\t\tfor i in range(len(self.loc)) : # remove items that are equal\n\t\t\t\tif x[i] is None:\n\t\t\t\t\tself.loc[self] = self.loc[self].replace(None, None, None), None\n\t\t\t\telse"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tfor k in list(context): contexts[self.XFIELD_MAPPING[k+\"\"]] = k\n\nclass Data_Set(object):\t\t# {{{\n\tdef __init__(self,\n\t             X):\t\t# {{{\n\t\tself.X = X\n\n\tdef as_str(self):\t\t\t# {{{\n\t\t\"\"\"Return the data in this set as a single string of comma-\n\t\tseparated decimal values.\n\t\tSee also :class:`io.StringIO` for the details.\"\"\"\n\t\t"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3580", "prompt": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\tif not edges: return 1.\n\tb, e = max(edges)*2-1\n\treturn float(b)*max(edges-b)/edges\n\n\n@_api.deprecated(\"3.2\", alternative=\"integrate\")\n@_api.method\ndef integral(bins, edges, min_=0., max_=1.):\n\n\tpass\n\n\n@_api.deprecated(\"3\", alternative=\"distribute\")\ndef divergence(values, degrees=None, weights=None, min_=0, max_=1):\n\t\"\"\"\n\tReturn a density of"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\tself = hvector(name=\"integral\", bins = bins, edges = edges)\n\tself.parent_collections.append(self.parent().parent())\n\treturn self\n\nimport pandas as pd\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import confusion_matrix, adjusted_rand_score\nfrom sklearn.calibration import CalibratedClassifierCV\n\n#\n# Function\n#\ndef poly_features_score(X, y, sample_weight = None, n_splits = 1, n_features=None, random_state=None):\n "}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from spconv import bintools\n\tpass#IMPORTIMPORTIMPORT from spconv import scale_array\n\tpass#IMPORTIMPORTIMPORT from weakref import ref\n\tpass#IMPORTIMPORTIMPORT from numpy import linalg\n\tpass#IMPORTIMPORTIMPORT from array import array\n\n\t# Use bintools functions\n\treturn bintools.integrate(_scale(bins, edges), return_bins=True)\n\n\ndef integral_2D(nsegs, nsides, x0, x"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\tfrom copy import deepcopy\n\tif isinstance(binnings, str):\tfrom.histogram import *\n\tif not bins: return [[0 for i in range(edges)]]\n\tif binnings < 0:\n\t\traise ValueError('bins must be nonnegative')\n\ti = len(bins) - 1\n\tm = copy(edges)\n\tfor k in range(bins):\n\t\tedges.append(edges[-1] + bins[k]*((i-k)/bins[-1]))\n\treturn deepcopy(edges[0:edges[-1] + 1])\n"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\tif (isinstance(bins, numbers.Integral)):\n\t\tfrom..libpy.sp_utilities import bincount\n\t\treturn (list(bincount(bins)))\n\telse:\n\t\treturn integral_nonlinear(bins)\n\ndef integral_nonlinear(bins, n=3):\n\t\"\"\"\n\tCompute integral of a sub-scale nonlinear with binsize\n\tprovided by *n*.\n\n\t*bins* -- a 1-d array of values, representing the\n\tmesh used in the integration.\n\t\"\"\"\n\tfrom random import random\n\tseed=random"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\treturn [0.] + [float(edges[i]) for i in range(bins[0])]\n\n/*\nCopyright (c) 2003-2020, CKSource - Frederico Knabben. All rights reserved.\nFor licensing, see LICENSE.md or https://ckeditor.com/legal/ckeditor-oss-license\n*/\nCKEDITOR.plugins.setLang( 'preview', 'id', {\n\tpreview: 'Pr\u00e9-visualizar'\n} );\n\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\n\nnamespace UniGLTF"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from sp_statistics    import bin\n\tpass#IMPORTIMPORTIMPORT from sp_utilities    import bernoulli, inverse_sqrt\n\tpass#IMPORTIMPORTIMPORT from math        import sqrt\n\tpass#IMPORTIMPORTIMPORT from matplotlib    import pyplot, axis, title, color, \\\n\t                                   "}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\tpass\n\ndef box(center, r):\n\t\"\"\"Box (ellipsoid)\"\"\"\n\treturn EuclideanProjection(center, r)\n\ndef disk1d(x, *args):\n    \"\"\"Distance between two coordinates\"\"\"\n    return np.power((0.5*np.mean(np.diff(x))), 0.5).sum()\n\ndef disk2d(x, y, *args):\n    \"\"\"Distance between two 2D coordinates\"\"\"\n    return np.power((np.mean(np.diff(x))), 0"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\tdel del edges\n\tif (bins[-1] < 0):\n\t\treturn\n\telif bins[0] == 0:\n\t\treturn  1.0\n\treturn sum(x**2 for x in xrange(bins[1], bins[0] + 1, bins[0])) \\\n\t\t/ bins[:-1]\n\n\nif __name__ == '__main__':\n\t\"\"\"\n\tmain program\n\t\"\"\"\n\n\n\t#################################################################\n\t# set up input parameters\n\t#################################################################\n\tparser = argparse.ArgumentParser(\n\t\tdescription=__doc"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\tif bins.__class__!= type(bins.data()):\n\t\tbinmin, binwidth = bins.data(), bins.width()\n\telse:\n\t\tbinmin, binwidth = bins, 1\n\tbindata = bins.min(bins.data())+1\n\tbinwidth = max(binwidth, 1)\n\tbindata, binwidth = [int(d) for d in bindata], max(binwidth, 1)\n\tbinmask = binmin + binwidth * (2*binm) - 1\n\tedges = numpy.floor(edges)\n"}]}
{"task_id": "62b87b199a0c4fa8b80b354e", "prompt": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tif seq is None:\n\t\treturn False\n\tif isinstance(seq, FillRequest):\n\t\treturn all(is_fill_request_seq(p) for p in seq.seq)\n\treturn any(is_source_sequence(x) for x in seq)\n\ndef sequence(seq):\n\t\"\"\"\n\tConvert a SourceSeq (or just a single source element) to a\n\tSequence (an empty sequence if it is None)\n\n\tWARNING: when it is possible to have just a single source\n\telement ('') that is not a fill request seq, you"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tif hasattr(seq, \"source_seq\"):\n\t\treturn True\n\telse:\n\t\treturn not isinstance(seq, Set)\n\n\ndef find_missing_fill_request_ids(seq):\n\t\"\"\"\n\tFind sequences that don't have a FillRequest element in their seq.seq_index()\n\n\tReturns an empty sequence on error.\n\tSee issue #3567#.\n\n\tDoes not use seq.source_seq() and seq.source_seq in the results but\n\tif an id is found, sets the member of *seq* to a Fill"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tif isinstance(seq, FillRequest):\n\t\treturn False\n\treturn any(s.is_fill_request_seq() for s in seq)\n\ndef _check_type_against_list(element, list_type):\n\tassert (isinstance(element, list_type))\n\tfor x in element:\n\t\tassert (not isinstance(x, list_type))\n\t\tassert (not isinstance(x, str))\n\ndef _element_to_str(element):\n\t# return element.name if it is of String type\n\t# otherwise return element"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tfor element in seq:\n\t\t# Check for \"Source\"\n\t\tif isinstance(element, Source):\n\t\t\treturn False\n\t\t# See if it is a FillRequest\n\t\tif element.has_content and len(element.children) == 0:\n\t\t\treturn True # element and all child elements can be of type FillRequest\n\treturn False\n\ndef get_fill_request_seq(seq, index, index_offset):\n\tidx1, idx2 = index[index_offset:]\n\tstart_index = idx1 + 1\n\tif (start"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tif isinstance(seq, list):\n\t\tfor x in seq:\n\t\t\tif not isinstance(x, FillRequest):\n\t\t\t\treturn False\n\t\t\tif not is_fill_request_seq(x):\n\t\t\t\treturn False\n\t\treturn True\n\telif not isinstance(seq, FillRequest):\n\t\treturn False\n\treturn is_source_seq(seq)\n\ndef as_fill_request(req):\t\t\"\"\"\n\tConvert *req* to a FillRequest.\n\nRaises IndexError if *req* is not a FillRequest.\n\nIf *"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\treturn _element_type(seq) is _FillRequestSeq\n\ndef is_data_request_seq(seq):\t\"\"\"\n\tTest if *seq* can be converted to a DataRequestSeq.\n\nTrue only if it is a DataRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\treturn _element_type(seq) is _DataRequestSeq\n\n# Test for Sequence with one Source element\n#\ndef is_source_sequence(sequence):\t\"\"\"\n\tTest whether *sequence* is a SourceSequence.\n\nTrue"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tif isinstance(seq, six.string_types) and seq:\n\t\tif seq.strip()[0] == '>':\n\t\t\treturn True\n\t\treturn False\n\tif not seq:\n\t\treturn False\n\treturn True\n\ndef is_stix2_container_request_seq(seq):\n\t\"\"\"\n\tTest whether *seq* can be converted to a Stix2ContainerRequestSeq.\n\nTrue only if it is a Stix2ContainerElementSeq or contains at least one such,\nand it is not a Source sequence.\n\nIf *seq* is"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tfor element in seq:\n\t\tif isinstance(element, FillRequest):\n\t\t\tif element.hasSourceItems():\n\t\t\t\tlog.error('cannot convert %s to FillRequestSeq' % (element,))\n\t\t\telif element.hasTag(tag.SEQUENCE) and element.hasTag(tag.EXPLICIT):\n\t\t\t\tfor s in element.getSources():\n\t\t\t\t\tif s.hasTag(tag.EXPLICIT) and not s.hasTag(tag.PRESENT):\n\t\t\t\t\t\tlog.error('expected source tag %s, not presented' % (s,))\n\n\t\t"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tfor x in seq:\n\t\tif (x.tag!= FillRequest._tag or len(x.source.value) == 0) \\\n\t\tor (len(x.source) > 0 and not isinstance(x.source, SourceValue)) \\\n\t\tor isinstance(x.source, SourceValue):\n\t\t\treturn False\n\treturn True\n\ndef fill_request_seq_from_req(request):\n\t\"\"\"Given a :class:`FillRequest <XForms.FillRequest>` object,\n\treturn the source and content part of that FillRequest.\t\"\"\"\n"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tif hasattr(seq,'seq') and not isinstance(seq.elem, FillRequest):\n\t\treturn False\n\tfor seq in seq.seq:\n\t\tif not is_source_seq(seq) and not is_fill_request_seq(seq):\n\t\t\treturn False\n\treturn True\n\ndef get_fill_requests_for_source_seq(seq):\n\t\"\"\"\n\tGet the sources of the FillRequestSeq sequence *seq*.\n\n\tThis function is mainly for use by a factory function that takes a\n\t<FILL_REQUESTS>, not a <"}]}
{"task_id": "62b87b099a0c4fa8b80b3538", "prompt": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\treturn hasattr(obj, \"fill_request_el\") and callable(obj.fill_request_el)\n\ndef is_color_picker_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'color_picker' and'request'.\"\"\"\n\treturn hasattr(obj, \"color_picker_request_el\") and callable(obj.color_picker_request_el)\n\ndef is_color_picker_el(obj):\t\"\"\"\n\tObject contains executable methods 'color_picker' and 'fill'.\"\"\"\n\treturn hasattr(obj, \"color_picker_"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tif _len(obj.methods) == 0:\n\t\treturn False\n\treturn all(is_method(obj.methods[0].name, \"fill\")\n\t\t   or is_method(obj.methods[0].name, \"request\"))\n\ndef is_text_request_el(obj):\t\"\"\"\n\tObject contains executable texts 'text-request' and 'text-request'-\n\t\"\"\"\n\tif _len(obj.texts[0].method.names) == 0:\n\t\treturn False\n\treturn all(is_method(obj.texts[0"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tif getattr(obj, '_fill_el', None) and isinstance(obj._fill_el, _base):\n\t\tif id(obj._fill_el) in _id_fill_els:\n\t\t\treturn True\n\treturn False\n\ndef fill_req(obj):\n\t\"\"\"return True if the object is a filled request\"\"\"\n\tfor f in [fill_req, fill_el_req, _base]:\n\t\tif is_fill_request_el(obj) and f(obj):\n\t\t\treturn True\n\treturn False\n\ndef request(obj"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tif not isinstance(obj, _method_wrapper_t):\n\t\treturn False\t\t\n\tres = 0\n\tfor m in obj.methods:\n\t\tif obj.method_string == m[0]:\n\t\t\tres |= m[1]\n\treturn res!= 0\n\ndef is_fill_request_p(obj):\n\t\"\"\"\n\tObject contains executable methods '__p__fill' and'request'.\t\"\"\"\n\treturn hasattr(obj, '__p__') and obj.__p__ \n\ndef is_fill_request_i(obj):\n\t\"\"\"\n\t"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\treturn isinstance(obj, (base_classes.FillRequestModel, base_classes.RequestModel))\n\n\n# For requests, we want the model name\nrequest_model_name = \"request\"\n\n\ndef request_model_from_dict(obj, namespace_name, fields,\n                            model_name):\n    \"\"\"\n    Convert dictionary into a model for a request\n\n    `obj`\n       "}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\treturn 1\n\n\nclass _set_fill_default(object):\n\n\tdef __init__(self, default_fill):\n\t\tfrom.default_fill import *\n\t\tself.default_fill = default_fill\n\n\tdef __call__(self, *args):\n\t\ttry:\n\t\t\tif self.default_fill is None:\n\t\t\t\treturn\n\t\t\tf = args[0]\n\t\t\targs = list(args)\n\t\t\targs[0] = getattr(f, self.default_fill)\n\t\t\tdelattr(f, self"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tif isinstance(obj, _base.WrapperDescriptorType):\n\t\tobj = obj.get_wrapped_object()\n\tif obj is not None and obj._type_ is descriptor._METHOD:\n\t\tobj = obj.get_wrapped_object()\n\tassert obj is not None, 'bad type for is_fill_request_el'\n\treturn _base.MethodDescriptorType(obj, None) == descriptor._METHOD and obj.python_type =='struct' and obj.return_type == descriptor._RETURN_VALUE\n\n\ndef __getstate__():\n\t\"\"\"\n\tHelper"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tassert isinstance(obj, Elm)\n\treturn is_any_method(obj, \"fill\", \"request\")\n\n# Elias Mantas\t\t19-Jul-2002\n\n# Python implementation of L{ILM}\nclass ilm(Object):\n    \"\"\"\n    Abstract L{ILM} class.\n    \"\"\"\n    \n    def request(self):\n        \"\"\"\n        Make an I{request} request.\n        @"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tfrom floyd.tasks.utils.base import (\n\t\trequestmethod,\n\t\ttask_is_available_from_target,\n\t)\n\n\t# TODO: Remove on FYI\n\tif task_is_available_from_target('floyd.app.task.application') is False:\n\t\treturn False\n\n\t'''\n\timport inspect\n\timport re, traceback\n\timport requests\n\n\ttry:\n\t\tif inspect.ismethod(obj.fill):\n\t\t\treturn obj.fill is requests.post\n\n\t\tif"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\treturn hasattr(obj, \"_fill_\") and callable(getattr(obj, \"_fill_\"))\n\n\ndef is_select_method(obj):\n    \"\"\"Returns True if the given object is a method of :class:`Field`\"\"\"\n    return hasattr(obj, \"select\")\n\ndef is_query_method(obj):\n    \"\"\"Returns True if the object is a method of :class:`Query`\"\"\"\n    assert is_query_method(obj)\n    return hasattr(obj, \"query\")\n\ndef is_update_"}]}
{"task_id": "62b87af99a0c4fa8b80b3524", "prompt": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\treturn hasattr(obj, '_el_run')\n\ndef is_hook(obj):\t\"\"\"\n\tObject contains executable method 'hook'.\"\"\"\n\treturn hasattr(obj, '_el_hook')\n\ndef is_test(obj):\t\"\"\"\n\tObject contains executable method 'test'.\"\"\"\n\treturn hasattr(obj, '_el_test')\n\ndef get_hooks(module, hooknames):\n\tif hooknames is None:\n\t\treturn module._el_hooks\n\treturn [hook for hook in module._el_hooks if hook['name'] == hooknames]\n"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\tif isinstance(obj, types.MethodType):\n\t\treturn True\n\n\tif obj is None:\n\t\treturn False\n\n\tif hasattr(obj, '__call__'):\n\t\treturn True\n\n\treturn False\n\ndef is_method(obj):\t\"\"\"\n\tObject is a classmethod or a staticmethod.\t(e.g. is method of cls\n\tor of an instance method.)\t\"\"\"\n\tif isinstance(obj, types.MethodType):\n\t\treturn True\n\n\tif isinstance(obj, types.FunctionType):\n\t\treturn False\n\n\t"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\treturn isinstance(obj, Executable) and obj.run\n\n/*\n * Copyright (c) 2014. Real Time Genomics Limited.\n *\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are\n * met:\n *\n * 1. Redistributions of source code must retain the above copyright\n *    notice, this list of conditions and the following disclaimer.\n *\n * 2. Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\tif obj:\n\t\tif hasattr(obj, '__call__'):\n\t\t\t# it's a normal method\n\t\t\treturn True\n\t\telif isinstance(obj, (tuple, list)):\n\t\t\t# it a class method\n\t\t\treturn False\n\t\telif isinstance(obj, MethodType):\n\t\t\t# it's a classmethod\n\t\t\treturn False\n\t\telif isclass(obj):\n\t\t\t# it's a class\n\t\t\treturn False\n\t\telif ismodule(obj):\n\t\t\t# it's a module\n\t\t\treturn False\n\t\telif isinstance"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\tif obj is None :\n\t\treturn False\n\t\"\"\"\n\n\tdef __is_not_el(obj):\n\t\t\"\"\"\n\t\tObject must have executable method 'run'  and not be deleted.\n\t\t\"\"\"\n\t\tif obj is None :\n\t\t\treturn False\n\t\t\"\"\"\n\n\t\tdef __is_not_el(obj):\n\t\t\t\"\"\"\n\t\t\tObject must have executable method 'run', but be deleted.\n\t\t\t\"\"\"\n\t\t\treturn False\n\t\t\n\t\treturn not obj.__self__.__dict__['run']\n\t\t\n"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\tobj_sig = inspect.signature(obj)\n\n\t# check the args for validity\n\targs, _ = inspect.getargspec(obj)\n\tassert len(args) == 1\n\n\t# the method must take no args\n\tassert args[0].args is None\n\n\t# only methods with a '*args' keyword have 'run' signature\n\tassert obj_sig.parameters().__len__() == 0\n\n\t# only methods that return tuples (e.g., class methods) have a'return'\n\t# statement; if they are not specified, the function has"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\tif sys.version_info[0] == 2 and isinstance(obj, types.MethodType):\n\t\tobj = six.ensure_text(obj)\n\n\treturn obj in _RUN_EL.keys()\n\ndef _eval(obj, globals, locals):\n\tif isinstance(obj, types.MethodType):\n\t\tobj = six.ensure_text(obj)\n\n\tresult = eval(obj, globals, locals)\n\n\tif isinstance(result, str):\n\t\treturn eval(result)\n\n\treturn result\n\ndef run(func, args, kwargs"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\treturn hasattr(obj, \"run\") and callable(getattr(obj, \"run\", None))\n\n\n####################\n#\n# Code execution module\n#\n####################\n\nclass CodeExecutionError(Exception):\n\n\t\"\"\"The exception class\"\"\"\n\tdef __init__( self, message= None, stackTrace= None ):\n\t\tException.__init__(self,message,stackTrace)\n\t\t#print(self.message )\n\nclass CodeExecution( object ): # Code Execution object\n\tdef print_( *args ):\n\t\t\"\"\"If __str__ is used, it returns"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\treturn (bool(getattr(obj, name, None)))\n\n\ndef is_run_in(cls: type) -> bool:\n\t\"\"\"\n\tShould be defined for classes that implement the run() attribute.\n\tTrue for builtin classes.\n\t\"\"\"\n\treturn getattr(cls, 'run', None) is not None\n\n\ndef call_builtin_method(cls: type, name: str) -> list:\n\t\"\"\"\n\tTry to call a Python function with the same name and signature.\n\tReturn the results as a list of the return values.\n\t\"\"\"\n\t"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\"\"\"\n\t>>> from.. import _ast\n\t>>> e = _ast.Run(1, obj.a)\n\t>>> e\n\trun(1, a.Expr(obj.a))\n\t\"\"\"\n\treturn obj.isinstance(ExecutableBase)\n\ndef test_run_el(obj):\t\"\"\"\n\tObject contains test method 'run'.\t\"\"\"\n\t\"\"\"\n\t>>> from.. import _ast\n\t>>> e = _ast.Run(2, obj.a)\n\t>>> e\n\trun(2, a.Expr(obj.a))\n\t>>>"}]}
{"task_id": "62b87af69a0c4fa8b80b351a", "prompt": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\tif not obj or not inspect.isfunction(obj) or sys._getframe().f_globals['__name__']==\"__main__\":\n\t\treturn False\n\tfrom. import el_list\n\tif isinstance(obj, el_list.list):\n\t\tfor element in obj:\n\t\t\tif not is_fill_compute_el(element):\n\t\t\t\treturn False\n\treturn True\n\nusing OpenTabletDriver.Plugin.Tablet;\n\nnamespace OpenTabletDriver.UX\n{\n    public partial class Tablet\n    {\n    "}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\tif inspect.isclass(obj) and ismethod(obj):\n\t\treturn (hasattr(obj, 'fill') and callable(obj.fill)\n\t\t\tand hasattr(obj, 'compute') and callable(obj.compute))\n\treturn False\n\n@pytest.fixture(scope='module')\ndef fill_exe(obj, fixture_obj, init_func):\n\t\"\"\"\n\tFixture object 'f' fixture object to have executable set.\n\t\"\"\"\n\tfor name in dir(obj):\n\t\tif name.startswith('fill_'):\n\t\t\tinit"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\treturn _core.is_class(obj, _core.PyCapsule)\n\ndef is_scalar_fill_el(obj):\t\"\"\"\n\tObject contains 'fill' methods'scalar' et'scalar2' (all subclasses).\"\"\"\n\treturn _core.is_class(obj, _core.PyScalar)\n\ndef is_scalar_fill_elx(obj):\t\"\"\"\n\tObject contains 'fill' and'scalar2' methods (all subclasses).\"\"\"\n\treturn _core.is_class(obj, _core.PyScalar2)\n\ndef _as"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\tif not isinstance(obj, _dmatrix):\n\t\treturn False\n\treturn is_el(obj.mattype)\n\ndef is_el(obj):\t\t\"\"\"\n\tReturns True if the object is a standard matrix object.\n\tAssumes it is a real python matrix object, i.e. does not have a size attribute.\n\t\"\"\"\n\tif not obj.flags:\n\t\treturn False\n\tif obj.flags & 1:\t\t# matrix of real s*t numbers\n\t\tif obj.ndim!= 1:\t# matrix has only one dimension\n"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\treturn hasattr(obj,'_fill_compute_el') and callable(obj._fill_compute_el)\n\ndef is_reduce_operation(obj):\n\t\"\"\"Object contains a function'reduce_op' and an optional arguments 'comm_size'.\n\tIf both arguments are specified, the function is called with two arguments,\n\tthe first of which is the number of elements, and the second is the\n\tnumber of components of array_to_reduce.\"\"\"\n\treturn hasattr(obj,'_reduce_op') and callable(obj._reduce_op) \\\n\t     "}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\tif not isinstance(obj, ndarray):\n\t\traise TypeError('obj must be an nd-array')\n\treturn len((obj.fill, obj.compute(val, 1), obj)) % 2 == 1\n\ndef is_argminmax_1(arg):\n\t\"\"\"\n\tTest whether 'arg' is an 1-D ndarray with integer values\n\tinside the range (-oo, oo) or (0, oo). Note this must\n\treturn False for any input array, but return True for any object\n\tthat is not of type 'arg' -- except for special cases."}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\treturn len(inspect.getmembers(obj, lambda obj, name, default: is_fill(obj) or is_compute(obj))) > 0\n\n\ndef is_fill_computer(obj):\t\t\"\"\"\n\tClass has 'fill' and 'computer' attributes.\t\"\"\"\n\tif not (is_network(obj) or is_trainer(obj)) and not (is_network(obj) and is_filler(obj)):\n\t\treturn True\n\treturn False\n\n\ndef is_update(obj):\t\t\t\"\"\"\n\tReturn True for methods '"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\treturn type(obj) == types.MethodType and \\\n\t       obj.__flags__ & COX_METHOD_TYPES_MASK_FINAL and \\\n\t       (obj.__flags__ & COX_METHOD_TYPES_MASK_NONKEY_VAL_DATA)\n#------------------------------------------------------------------------------\n\nclass FillAttribute(AttributeWithMeta):\n    \"\"\"\n    Class used to add non-key attribute to class.\n    \"\"\"\n\n    def __init__(self, attr_name, attr_value, allow_none = False,"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\treturn (\n\t\thasattr(obj, \"n\") and callable(obj.n)\n\t\tand obj.kind!= \"comprehension\"\n\t\tand obj.n >= 0\n\t)\n\ndef is_fill_compute_array_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill', 'compute','slice', 'fill_buffer' and 'to_list' (see 'fill_buffer'),'size' and 'fill(None)'\n\t(which are also supported in numpy_to_ragged_arrays).\"\"\"\n\treturn (\n\t"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\treturn _object_is_callable(obj, \"fill_compute_el\")\n\ndef is_any_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'any_fill' and 'compute'.\t\"\"\"\n\treturn _object_is_callable(obj, \"any_fill_compute_el\")\n\ndef is_any_fill_from_object_to_object(obj):\t\"\"\"\n\t...object->object\n\t...object->object\n\t...object_in->object\n\t...object_in->object\n\t...object_out->"}]}
{"task_id": "62b87af19a0c4fa8b80b34f7", "prompt": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tr = {}\n\tfor key, value in d1.items(): r[key] = value\n\tfor k in d2.keys(): r[k] = d2[k]\n\tif level!= -1: n = 0 # no recursive? go deep\n\telse: if d2 and not d2.has_key('__copy__'): # no recursion? go shallow\n\t\tr.__copy__ = d2\n\t\tn = 1 # for normal shallow assignment, or recursion?\n\telse:\t\t# recursion? go full\n\t\tif d1: n"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tfrom sympy.utilities import default_sort_key\n\tfrom copy import deepcopy\n\tfrom collections import defaultdict\n\tlevel =  2 if len(d1) == 0 else -1\n\tif len(d2) == 0:\n\t\tif level > 0:\n\t\t\treturn defaultdict(dict)\n\t\treturn deepcopy(d1)\n\tfor key, value1 in d1.iteritems():\n\t\tif key in d2:\n\t\t\tif level > 0:\n\t\t\t\tvalue2 = deepcopy(d2[key])\n\t\t\telif len("}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t# from.grammar import grammar\n\t# from.grammer import grammer\n\t# from.differ import differ\n\t# gram=grammar\n\t# grammer=grammer\n\t# from.util import flatten, reduce_dict_items, flatten_keys\n\t# # Do this on the unmodified subdicts just to be sure that only\n\t# ## one thing will add them to the output after each key-value pair\n\t# # is encountered.\n\t# newsubdicts=merge_dicts( gram._unmodified_subdicts, grammer._"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\treturn _diff_recursion(_difflib.ndiff, d1, d2, level)\n\ndef subword(s, i, starti=None, endi=None):\n\t\"\"\"Return a dictionary with items from *s* corresponding\n  to words in specified range of *i*.\"\"\"\n\tif starti is None and endi is None:\n\t\ts0 = s\n\t\tf = lambda a: a.__getitem__(i)\n\telse:\n\t\tassert starti >= 0 and endi >= starti\n\t\tassert i <= endi-starti"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tfor k_k, k_k_v in zip(d1, d2):\n\t\tk = k_k\n\t\tif k not in d:\n\t\t\td[k] = {k_k or k_k_v:k_k_v}\n\t\telse:\n\t\t\td[k] = d[k] | set([k_k or k_k_v] + [k_k_v])\n\tif level < -1: return d\n\treturn d.difference(d2)\n\ndef __eq__(a, b"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tfrom..util import copydict, copy_nested\n\tfrom..compat import PY2, string_types\n\titems = list(d1.items())&list(d2.items())\n\tif not (len(items)<(*level)):\t\treturn {}\t\t# nothing to do\n\titems = [key for key,value in items]\t\t# strip unnecessary keys\n\titems = [k for k,v in items]\t\t# get rid of all Nones\n\titems = [dict(v) for k,v in items]\n\treturn dict(items)\n"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tif len(d1) == 0 or len(d2) == 0: return {}\n\tif not d1 or not d2:\n\t\treturn {}\n\tresult = {}\n\tif len(d1) == 1 and len(d2) == 1:\n\t\tk = d1\n\telif len(d1) == 1:\n\t\tk = d1[0]\n\telif len(d1) == 2:\n\t\tk = d1[1]\n\telif len(d2) == 1:\n\t\tk = d2\n\t"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tif len(d1) == len(d2):\t\treturn\n\tif level < 0:\t\t\t\t\tlevel = len(d2)-1\n\tif len(d1) < len(d2):\n\t\tn = len(d1)\n\t\tm = len(d2)\n\t\td2 = d1\n\t\td1 = d2[:n] + d2[m:]\n\t\tt = list(d1)\n\telse:\n\t\tt = list(d1) + list(d2)[:m]\n\treturn intersection"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tif not d1: return d2\n\tif not d2: return d1\n\td1 = sorted(set(d1))\n\td2 = sorted(set(d2))\n\tresult = {}\n\tfor k in d1:\n\t\ttry:\n\t\t\tv = d2.pop(k)\n\t\texcept KeyError:\n\t\t\tresult[k] = d1.pop(k)\t# remove all entries not present\n\t\telse:\n\t\t\tresult[k] = d1.pop(k)\t# remove specific keys\n\t\t"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tresult={}\n\t\n\t\n\tfor key,value in both_dictionary.items():\n\t\tresult[value] = key\n\t\tif isinstance(d1[key],dict):\n\t\t\telif isinstance(d2[key],dict):\n\t\t\t\tdel result[value]\n\t\t\t\tdifference(**d1,**d2,level=level+1)\n\t\telif value!= d2[key]:\n\t\t\tresult[value] = d2[key]\n\t\telif value!= d1[key]:\n\t\t\tresult[value] = dict([(key, value"}]}
{"task_id": "62b87af09a0c4fa8b80b34f1", "prompt": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tif np.sum(coord) == 0: return\n\tself._data[:,...].fill(coord,weight=weight)\n\tcoord -= self._min\n\tx = 0\n\tfor col in self._data[0,...]:\t\tx += coord[col]\n\tself._n += x \n\treturn\n\ndef add_hist(self, src, n, start, step = 0.0, weight=1): #@ReservedAssignment\n\tpass#IMPORTIMPORTIMPORT from sp_utilities  import compose_transform3\n\tpass#IMPORTIMPORTIMP"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tself.data.fill(self.offset + self.size, self.origin + coord, weight=weight)\n\t#self.data.fill(self.offset + self.size, coord,weight)\n\tself.size += 1\n\ndef set_fill(self, fill):\t\t\t\t\"\"\"\n\tMark *fill* as filled\n\t\"\"\"\n\tself.data.fill(\n\t\tself.offset, _zeros\n\t\tfor _ in range(len(fill._data._data))\n\t)\n#end class histogram\n\t\n\t\n##########################################################################################\n###"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tself.mean /= self.nmembers\n\tself.dev /= self.nmembers\n\tself.stddevs /= self.nmembers\n\tself.sum_squares /= self.nmembers\n\n\tfor i in range(coord.ndim):\n\t\tdata_offset = self.start_offset + coord.shape[i] * self.stride\n\t\tfor j in range(coord.shape[i]):\n\t\t\toffset = data_offset + j * self.itemsize\n\t\t\tdata = self.data[offset:offset+self.itemsize]\n\t\t\ts"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tself.hist = histogram(coord, weight=weight)\n\tself.hist.reset()\n\n\nclass histogram(object):\n\n\t\"\"\"\n\tHistogram class, designed to work with data in either 0- or 1-D space.\n\tHistograms are generated using a *weight* property.\n\t\"\"\"\n\tdef __init__(self, coord, weight=1):\t\"\"\"\n\tConstructor.\n\n\tFor 1-D space:\n\t\t1. Construct the histogram with empty contents.\n\t\t2. Add data elements that falls outside of the histogram.\n\t\t3"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tif weight <= 0:\t\t\t\t# No change to be done.\n\t\treturn\n\tx = coord[0]\t\t# First bin\n\tn = coord[-1]\t\t# Number of bins\n\tif x == n-1:\t\t\t# All values are 0, skip a bin at a\n\t\tself.hist(0)\n\telse:\n\t\tindex = x/n\t\t# Convert from bin coordinates to\n\t\tbins[index] += weight\t\t\t# bins\n\tind = x%n\t\t\t\t\t\t\t\t\t\t# bin index\n\tself.hist(bins"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from sp_utilities  import  read_text_row, get_image, rotate_alpha, mirror_like_to_3d, model_circle, model_gauss\n\tpass#IMPORTIMPORTIMPORT from EMAN2db import db_open_dict, db_close_dict\n\tself.list_hist = [0] * self.Nx\n\t#self.list_hist = [0] * self.Nx\n#\tif len(coord) == 2 and self.verbose:\n#\t\tprint \"DEBUG: ("}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tself.coord_counts[np.seterr(np.NINF),coord] += 1\n\tx = np.arange(-self.windowsize/2.,self.windowsize/2.,-1)\n\tr = np.diff(x)\t# r is the relative weight by which to decrease x\n\ty = x * r + coord\n\tnp.place(weights, weights == -self.windowsize/2., weights <= coord)\n\tweights[np.where(weights == -self.windowsize/2.),weights < coord] = r\n\tweights = weights /"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tself.hist.clear()\n\tself.history.clear()\n\thist = self._history.get(coord)\n\tif hist is not None:\t\t# is a valid histogram object\n\t\t#  we get the weights and put them scaled to the last one \n\t\tif hist is None:\t# the coordinate is invalid\n\t\t\treturn\t#.. nothing we must do\n\n\t\thist = self._hist_cache.get(hist)\t\t# do the conversion here\n\t\thist=hist.get(0.)\t\t\t\t# it's the first one\n\n"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from sp_fundamentals import angle\n\tpass#IMPORTIMPORTIMPORT from sp_morphology import binarize\n\tpass#IMPORTIMPORTIMPORT from cell_methods import CELL_CENTERS # new method\n\tpass#IMPORTIMPORTIMPORT from sp_utilities import reduce_EMData_to_root, pad, model_blank, set_params_proj\n\tpass#IMPORTIMPORTIMPORT from mpi import mpi_comm_split, mpi_barrier, mpi_reduce, mpi_bcast\n\tpass#IMP"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from sp_fundamentals import rot_avg_3D\n    \n\t\n\tif( self.n_bins ==1):\n\t\tpass#IMPORTIMPORTIMPORT from sp_utilities import c_map3d_MPI\n\t\t\n\t\tnvec = len(coord)\n\t\tif( nvec!=len(coord[0]) ): \\\n\t\t\traise ValueError('The dimensionality of the data set must be 2, 3, or 4. Received (%d) and (%d).' % (nvec,len(coord["}]}
{"task_id": "62b86aa3b4d922cb0e688d36", "prompt": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 364.00 MiB (GPU 0; 22.16 GiB total capacity; 19.88 GiB already allocated; 219.88 MiB free; 21.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b86a9eb4d922cb0e688d25", "prompt": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\tregex_pattern = [re.compile(r'\\w+'), ]\n\tkrake_app_prefix = '_~'\n\tkrake_app_name_regex = re.compile(r'^[a-z\\d+]' + \\\n\t                                                 '[a-z0-9 _~]+')\n\tfor prefix in [\"krake_app\", \"app\"]:"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\treturn re.compile(\"^Kraketv/.*\")\n\ndef _remove_namespace_prefix(namespace, prefix):\n\ttry:\n\t\ts = str_to_bytes(namespace + '/')\n\texcept UnicodeEncodeError:\n\t\treturn ''\n\treturn prefix + s[len(s) - 2:(len(s) - 1)]\n\n/*\n * Copyright 2016 Google Inc. All Rights Reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\tregex = re.compile(\n\t\tr'^[a-z0-9]+(?=(?:\"[^\"]*\"|\\'\\'(?:\"|\"[^\"]*\")))' +\n\t\tr'\\d{6}(?:[a-z\\.0-9]+)*(?:[a-z\\.0-9]+)*[a-z0-9]$|^[a-z' +\n\t\tr'0-9]+[a-z' +\n\t\tr'0-9]+:[a-z0-9]+\\.(?=:)|$' +\n\t"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\tre_resource_name = re.compile(\n\t\tr\"^([\\S\\s]+):(.*)$\")\n\treturn re_resource_name\n\n\ndef _format_resources(resources):\n\t\"\"\"Format the resources of krake.\n\n\t:param resources: a dictionary mapping resource names to resource\n\tnames.\n\t:returns: a list of error messages, empty list if no errors detected\n\t\"\"\"\n\tif not _has_krake_api:\n\t\treturn []\n\n\treturn [\n\t\t(\n\t\t\t\"error\",\n\t\t\t\""}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\tr = re.compile(r\"^[\\w\\._]+$\")\n\n\treturn r\n\ndef check_valid_krake_name(krake_name, resource_name):\n\t\"\"\"\n\tTest the name of the krake resource by checking regular expression\n\t`resource_name`, if exists, returns 1 else returns 0.\n\n\t>>> check_valid_krake_name('mykrake_name','my_resource_id')\n\t1\n\t>>> check_valid_krake_name('naked_kraine_name', 'naked"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\tRE_NAME = r'^[a-z_][a-z0-9_-]*'\n\tRE_PATH = re.compile(r'([^/]+)/([^/]+)')\n\treturn re.compile(\n\t\tr'^(?P<resource>[\\w_\\-]+):'.center(\n\t\t\t','.join((RE_NAME, RE_PATH)))),  # name\n\t\tre.W\n\t)\n\n\ndef _check_name_match(name, regex):\n\t\"\"\"\n\tSimple wrapper around the built-in validation.\n\t\"\"\"\n"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\tr = re.compile(  r'^{(\\w|_)resource name (\\w|\\w|)}$', re.VERBOSE)\n\treturn r\n\ndef _generate_rdoc(module, method, name, docstring=None):\n\t\"\"\"Returns a documentation string for the class or module.\"\"\"\n\tdef _get_rdoc():\n\t\tif docstring is None:\n\t\t\tdocstring = _get_documentation(method)\n\t\treturn _escape_html(docstring)\n\n\treturn '<%s %s=\"%s\">' % (module,name,_get_"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\tre_name_reg = re.compile(r\"resource_name\")\n\tre_name_pattern = re_name_reg.fullmatch\n\n\treturn re_name_pattern\n\ndef _get_resource_name(domain, resource_name):\n\t\"\"\"Build or return the regular expressions that are used to validate\n\tthe name of the resources.\n\n\tArgs:\n\t\tdomain: a string representing the domain of the resource.\n\t\tresource_name: a string representing the Kraken resource name,\n\t\t\tor the resource_name_regex pattern.\n\n"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\treturn (\n\t\tre.compile(r\"\\b(.*)\\b.*\") |\n\t\tre.compile(r\"\\b(.*)\\b.*\\z\") |\n\t\tre.compile(r\"^[^\\w\\d\\-_]+(\\.\\w{1,3})?\") |\n\t\tre.compile(r\"^[-.\\\\-\\(\\)\\{\\}\\|\\*\\$\\?.\\d\\1\\2,;\\s\\(\\)$]+\")\n\t)\n\n\ndef _get_client_id_regex():\n\treturn (\n\t\tre.compile("}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\tm = re.match(fr'(?P<resource>[a-zA-Z0-9_-]+)|(?:(?:(?P<name>[a-zA-Z0-9_-]+)_(?P<version>[-]?[0-9]+))-(?P<prerelease>[a-zA-Z0-9_-]+)|(?:(?P<buildver>[a-zA-Z0-9_-]+))-(?P<build[0-9]+[a-zA-Z_-])*)|(c\\d+\\.\\d+\\.\\d+))',\n\t\t("}]}
{"task_id": "62b86a4fb4d922cb0e688cf8", "prompt": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tregex = re.compile(r\"^[a-z0-9-]+$\")\n\n\tif value not in regex.pattern:\n\t\traise ValidationError(\"Given value has an invalid characters.\")\n\n\n# ----\nclass Text(Field):\n\t\"\"\"\n\tA Text field. The type of the text field can be given as string and have it the\n\tfollowing attributes that are passed to a Text Field's constructor.\n\n\tExample:\n\n\t>>> tf = Text(\"Hello\", \"World\")\n\t>>> tf.value  # The value of the Text.\n\n\t>>>"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tre.compile(value)\n\n// Copyright 2019 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tif value not in REGEX_VALS:\n\t\traise ValidationError(\"The value %r is not valid\" % value)\n\n<?php\n// This file is part of Moodle - http://moodle.org/\n//\n// Moodle is free software: you can redistribute it and/or modify\n// it under the terms of the GNU General Public License as published by\n// the Free Software Foundation, either version 3 of the License, or\n// (at your option) any later version.\n//\n// Moodle is distributed in the hope that it will be useful,\n// but WITHOUT ANY WARRANTY"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tif not value:\n\t\treturn \n\n\tif _regex is not None and not re.match(REG_RE, value):\n\t\t_regex.fail(value)\n\n\treturn\n\ndef validate_ip_range(str_value, allow_empty = False,\n\t\t\t\t\t\tallow_list = False,\n\t\t\t\t\t\tallow_set = False,\n\t\t\t\t\t\tallow_set_default = True):\n\t\"\"\"\n\tValidate the given string containing an IP range (e.g. for DNS requests).\n\nArgs:\n\tstr_value: string to validate\n\t"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tpass\n\ndef validate_bool(value):\t\"\"\"\n\tValidate if the parameter is a boolean value.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the parameter is not conform to the regular expression.\n\"\"\"\n\tpass\n\ndef validate_dict_parameter(value):\t\"\"\"\n\tValidate if the parameter does not contain anything but a dictionary.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the parameter is not a dicument containing a dictionary"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\treg_list = re.compile(\"\\[(.*):([0-9]+)\\]\", re.I)\n\tif reg_list.match(value) is None:\n\t\tval = error(\n\t\t\t'should not convert value {} to integer value.\\n\\nValid values:\\n{}'.format(\n\t\t\t\tvalue, ', '.join(reg_list.split(','))\n\t\t\t)\n\t\t)\n\t\treturn val\n\treturn True\n\n// SPDX-License-Identifier: GPL-2.0\n/*\n * This is a cute hack to get all"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tif not re.match(r\"`[[:alnum:]]+(\\S+)`\", value):\n\t\traise ValidationError(u'Value \\'%s\\' does not conform to regular expression' % value)\n\n// This is a generated file from running the \"createIcons\" script. This file should not be updated manually.\nimport React, { forwardRef } from \"react\";\n\nimport { FontIcon, FontIconProps } from \"@react-md/icon\";\n\nexport const FilterOneToneFontIcon = forwardRef<HTMLElement, FontIconProps>(\n  function FilterOneToneFontIcon"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\ttry:\n\t\timport re\n\t\tregex = re.compile(value)\n\texcept:\n\t\tregex = None\n\n\tif regex is None:\n\t\traise ValidationError(\"Invalid value.\")\n\n\nclass BooleanChoices:\n\t\"\"\"\n\tA collection of boolean choices.\n\t\"\"\"\n\tdef __init__(self, value, default):\n\t\tself.value = value\n\t\tself.default = default\n\n\tdef __str__(self):\n\t\treturn self.value\n\n\ndef validate_bool_choice(value):\n\tvalue = value"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\timport re as rx\n\trx.raise_regex(re.compile(value))\n\ndef validate_str(str_value):\n\ttry:\n\t\t_validate_str(str_value)\n\texcept ValidationError:\n\t\tlogger.exception('Validation error for the provided value (%s).', str_value)\n\ndef _validate_str(str_value):\n\ttry:\n\t\ttry:\n\t\t\trx.str_validate(str_value.strip(),'str_validate_error')\n\t\texcept Exception:\n\t\t\tlogger.exception('"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tresult = re.findall(value, str(value))\n\tif len(result) < 1:\n\t\traise ValidationError\t(\"\"%s\" is not a valid string.\")\n\t\t\t\n\treturn result\n\t'''\n\n/******************************************************************************\n * drivers/wireless/spirit/lib/spirit_d11.c\n *\n *   Copyright(c) 2015 STMicroelectronics\n *   Author: VMA division - AMS\n *   Version 3.2.2 08-July-2015\n *\n *   Author: William Roskemeyer\n *   Author"}]}
{"task_id": "62b86a4fb4d922cb0e688cf7", "prompt": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tif re.match(PATTERN_RE, key, re.VERBOSE)!= None:\n\t\treturn key\n\telse:\n\t\traise ValidationError, \"Invalid key %s\" % key\n\n\ndef validate_regex(regex):\t\"\"\"\n\tValidates the regular expression pattern against a string.\n\nArgs:\n    regex: the string to validate\n\nRaises:\n    ValidationError: if the given regular expression is not conform to the defined regular expression.\n\nSee also validate_string().\n\"\"\"\n\t# Validating based on the regular expression\n\treturn"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tregexp = r'[\\w]+'\n\tre = r'(?P<key>[\\w ]+)(\\w|\\*)?(?P<invalid_pattern>$)'\n\tmatch = re.search(key)\n\tif match:\n\t\ttry:\n\t\t\tregex = r'{0!s}(\\d+)'.format(match.group('key'))\n\t\t\traise ValidationError(re, regex, 'nonconformant regular expression')\n\t\texcept ValueError:\n\t\t\tpass\n\tif re.search(key):\n\t\tif re.search(regexp):\n"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\ttry: \ttext = urllib.quote(key)\n\texcept:\n\t\traise ValidationError(\"Invalid URL\") \n# End of file: key_validator.py\n\n/* \nGeoGebra - Dynamic Mathematics for Everyone\nhttp://www.geogebra.org\n\nThis file is part of GeoGebra.\n\nThis program is free software; you can redistribute it and/or modify it \nunder the terms of the GNU General Public License as published by \nthe Free Software Foundation.\n\n */\n\npackage org.geogebra.common.kernel"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tif not regex.match(key):\n\t\traise ValidationError('Invalid format with name \\'%s\\'. Should conform to the format \\'%s\\'.' % (key, regex.pattern))\n\ndef validate_secret(secret):\t\"\"\"\n\tValidate the given secret against the corresponding regular expression.\n\nArgs:\n    secret: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tif not regex.match(secret):\n\t\traise ValidationError('Invalid format with \\'%s"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tif not key.match(r\"\"):\n\t\tmsg = \"keys must consist of alphanumeric ASCII characters only.\"\n\t\traise ValidationError(msg)\n\n@nba_deprecated(replacement=validate_email)\ndef validate_email(value):\n\tif not isinstance(value, six.string_types):\t\n\t\traise ValueError(\"email must be a string, not %s\" % type(value))\n\tif value.find('@') == -1:\n\t\tmsg = \"`@` must not appear in user names, \"\n\t\tmsg += \"as it does"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tm = re.match(key, key[0])\n#\t\t\tif not m:              # check for invalid regular expressions\n#\t\t\t\traise ValidationError  # raise\n\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n#"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tpattern = re.compile(key)\n\tif not pattern.match(os.environ['KIMI_DEBUG_VAR']):\n\t\traise ValidationError(\"Invalid format in Kimi debug output: %s\" % key)\n\n/*\n *  Copyright (C) 2016 Red Hat, Inc.\n *   All Rights Reserved.\n *\n *   Author: Matthew Garrett\n *\n *  This library is free software; you can redistribute it and/or\n *  modify it under the terms of the GNU Lesser General Public\n *  License as published by the"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\ttry:\n\t\tRE_PATTERN.match(key)\n\texcept Exception:\n\t\traise ValidationError('Illegal regexp.')\n\n#include \"StdAfx.h\"\n#ifndef ACL_PREPARE_COMPILE\n\n#include \"stdlib/acl_define.h\"\n#include <stdlib.h>\n#include <sys/stat.h>\n\n#ifdef ACL_BCB_COMPILER\n#pragma hdrstop\n#endif\n\n#include \"stdlib/acl_msg.h\"\n#include \"stdlib/acl_define.h\"\n\n#ifndef ACL_USER"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t# We make a copy of the key to avoid modifying it.\n\tkey = key.lower()\n\twhile '%' in key:\t#\t<\n\t\tkey = key.replace('%', '')\n\n\t# Validate the regex in the beginning and end of the string.\n\ttry:\n\t\tre.match(key, key.replace(\"_\", \"\\\\\\\\\").lower())\n\texcept re.error:\n\t\traise ValidationError\n\t# Done\n\n\t# We have a valid string. Let's see if it contains only white space characters.\n\tif key:\n\t\t"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tif not re.match(key[:-4], key, re.UNICODE):\n\t\tprint(\"Key must conform to regular expression\")\n\t\tsys.exit(1)\n\nfrom gazette.spiders.base.fecam import FecamGazetteSpider\n\n\nclass ScRigorantaSpider(FecamGazetteSpider):\n    name = \"sc_rigoranta\"\n    FECAM_QUERY = \"cod_entidade:120\"\n    TERRITORY_ID = \"4285"}]}
{"task_id": "62b86a01b4d922cb0e688ccc", "prompt": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t# NOTE: This should be moved to some future mechanism and done as part of #6260.\n\t# At the time of writing, no need to handle these cases since they'll be handled\n\t# by the future ones.\n\tnew_manifest_info = dict()\n\tif manifest_dict:\n\t\tfor k, v in manifest_dict.items():\n\t\t\tfor key, new_value in v:\n\t\t\t\tif key == \"kind\":\n\t\t\t\t\tif new_value not in (\"\", None):\n\t\t\t\t\t\tnew_manifest_info[new_value] = manifest"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tif first_level:\n\t\tobserver_schema = None\n\telse:\n\t\tobserver_schema = deepcopy(manifest_dict)\n\tfor resource_name in manifest_dict['resources']:\n\t\tfor key, value in manifest_dict['resources'][resource_name].items():\n\t\t\tif 'name' not in key:\n\t\t\t\tobserver_schema = value\n\t\t\t# if the value is a list, recursively call this function with new_item\n\t\t\telse:\n\t\t\t\tnew_item = generate_default_observer_schema_list(value)\n\t\t\t\t"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tglobal_schema = generate_default_observer_schema(manifest_dict, first_level)\n\tif first_level:\n\t\treturn global_schema\n\tif not hasattr(manifest_dict['resources'],'resource_definitions'): # Not a resource object\n\t\tmanifest_dict['resources'].resource_definitions = {}\n\treturn generate_default_observer_schema_dict( manifest_dict['resources'] )\n\ndef generate_default_observer_schema(manifest_list, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tif first_level is True:\n\t\t# Create empty dictionary for this object\n\t\tdict = {}\n\t\tdict['version'] = manifest_dict['version']\n\t\tfor key, value in list(manifest_dict.items()):\n\t\t\tif key not in ['versions']:\n\t\t\t\tdict[key] = None\n\t\t\telse:\n\t\t\t\tobj = generate_default_observer_schema(value)\n\t\t\t\tdict[key] = obj\n\t\treturn dict\n\telse:\n\t\treturn generate_default_manifest_schema(manifest_dict)\n\n"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tif not manifest_dict:\n\t\treturn None\n\tobserver_schema = None\n\n\tif isinstance(manifest_dict, dict) and not first_level:\n\t\t# Convert the dictionary into a string\n\t\ts_dict = manifest_dict.to_dict(copy=False)\n\n\t\t# Collect the keys for the resulting dict and sort the keys by their\n\t\t# respective value (not the value itself).\n\t\tnames = [k for _, k in sorted(v.items())[:len(s_dict)]]\n\n\t\tfor key in names"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tif first_level and not isinstance(manifest_dict,dict):\n        raise Exception('first_level must be a dict or True')\n\tnew_dict = dict()\n\tfor d in manifest_dict:\n\t\tnew_dict.update(generate_default_observer_schema_dict(d, first_level))\n\t\tnew_dict.update(d)\n\treturn new_dict\n\n\nclass ResourceObserverSchema(resource_types.Resource):\n\tdef __init__(self, manifest_dict,\n\t\t\t\tmanifest_list = [],"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\ttry:\n\t\tmanifest = merge_dicts(manifest_dict, manifest_list=True)\n\texcept KeyError:\n\t\tfor key in manifest_dict:\n\t\t\tif is_string_like(key):\n\t\t\t\tif not isinstance(manifest_dict[key], str):\n\t\t\t\t\traise\n\n\t\tobserver_schema_dict = generate_default_observer_schema_dict(manifest_dict, first_level=False)\n\t\tif is_list_like(observer_schema_dict):\n\t\t\tobserver_schema_list = []\n\n\t\t\t"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tif first_level and not manifest_dict:\t# empty dict\n\t\t# Generate default empty dict and skip the field from the first stage\n\t\tmanifest_dict = copy.deepcopy(DEFAULT_KUBERNETES_SCHEMA)\n\t\tfor p in manifest_dict:\n\t\t\tp['key'] = p.pop('value')\n\tif not manifest_dict:\t\t\t\t\t# empty dict\n\t\treturn None\n\t# Generate dictionary for identifying field types\n\tmanifest_dict.update(DEFAULT_KUBERNETES_SCHEMA) # update it with original values"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tresult_dict = {}\n\tfor key, field in list(manifest_dict.items()):\n\t\t# if a non-dict or non-list field, the original observer schema will\n\t\t# be copied by definition\t\n\t\tif isinstance(field, list):\n\t\t\t# do nothing, just append field to dict for this field\n\t\t\tresult_dict[key] = field.copy()\n\t\telif not is_resource(field) and not isinstance(field, str):\n\t\t\tresult_dict[key] = None\n\treturn result_dict\n\ndef convert_"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tif not first_level:\n\t\tif not isinstance(manifest_dict, dict):\t\t# If dictionary is a list, then it must contain non-list data\n\t\t\tif not isinstance(manifest_dict, list):\t# If it isn't list, then it must be a dict\n\t\t\t\tmanifest_dict = list(manifest_dict)\n\t\t\telse:\n\t\t\t\traise ValueError('Empty dictionary detected in manifest file')\n\t\tfor k in manifest_list:\t\t# Collect nested values and their respective names\n\t\t\tif not isinstance(manifest_dict[k],\n"}]}
{"task_id": "62b869ebb4d922cb0e688cc6", "prompt": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\n\tdef get_object(self, data_id, log=log):\n\t\tif log: log.debug('retrieve object %s from %s' % (data_id, self.id))\n\t\treturn self.data.get(data_id)\n\n\tdef _get_last_applied_manifest_list(self, log):\n\t\twith log_prefix('retrieving last applied manifest list'):\n\t\t\tfor data_dict in self.data:\n\t\t\t\tif log: log.debug(data_dict)\n\t\t\t\tif data_dict"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tself.update_last_applied_manifest_dict_from_resp(last_applied_manifest=last_applied_manifest,\n\t\t\t\t\t\t\t\t\t\t\t\t\t   observer_schema=observer_schema)\n\nclass LastAppliedManifestList(object):\n\t\"\"\"\n\tClass to keep last_applied_manifest of a response from the Kubernetes API.\n\tThis class is called once the HTTP request for pod template creation has\n\tbeen finished.\n\tIt updates last_applied_manifest\n\n\tArgs:\n\twebhook_status (bool): whether this instance has processed webhooks,\n\t\t\t\t\t\t\t\t   "}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\n\ndef update_last_applied_manifest_dict_from_resp(resp):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest_dict``\n\tfrom a partial Kubernetes response\n\n\tArgs:\n\t    last_applied_manifest_dict (dict): partial ``last_applied_manifest_dict`` being\n\t updated\n\t\tdict {\n\t\t\t\tlast_applied_manifest (dict),\n\t\t\t\tlast"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\tlast_applied_manifest_list = {\n\t\t\"create_time\": None,\n\t\t\"labels\": {\n\t\t\t\"app\": {\n\t\t\t\t\"name\": \"kubeapps\",\n\t\t\t\t\"value\": {\n\t\t\t\t\t\"env\": {\n\t\t\t\t\t\t\"NAME\": \"test_app\",\n\t\t\t\t\t\t\"default\": {\n\t\t\t\t\t\t\t\"name\": \"my_app\"\n\t\t\t\t\t\t},\n\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"instance_type\": {\n\t\t\t\t\"name\": \"my-app-type\","}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tlast_applied_manifest = []\n\tjson_dict_for_update = {\n\t\t'apiVersion':      \"manifest.cluster.k8s.io/v1beta1\",\n\t\t'kind': 'Manifest',\n\t\t'items':           last_applied_manifest,\n\t\t'page': 1,\n\t\t'pageSize': 100\n\t}\n\tresponse = api_query(api_path + '/v1beta1/manifests', json_dict_for_update,\n\t\t\t\t\t\t   None"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t) {\n\t\t\tlast_applied_manifest = [e.get('last_applied_manifest') for e in last_applied_manifest]\n\t\t\t\t# initialize them from the last response\n\t\t\tfor cur_last_applied in last_applied_manifest:\n\t\t\t\tif (len(cur_last_applied.get('schema_version'))) < 1:\n\t\t\t\t\tif cur_last_applied.get('api_version'): continue\t# ignore if the version is empty\n\t\t\t\tif cur_last_applied.get('last_appeared'):\tcur_last_"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tif len(last_applied_manifest) > 0:\n\t\tassert len(last_applied_manifest) == 1, \\\n\t\t\t'You passed in a list as the last_applied_manifest, but you\\'re passing'\\\n\t\t\t'+ a single list as last_applied_manifest.'\n\t\tlast_applied_manifest[0] = last_applied_manifest[0].copy()\n\n\tfor key, value in six.iteritems(response):\n\t\tif key.startswith(('kube_last_applied_manifest', 'kube_last_applied_manifest"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tglobal last_applied_manifest\n\tif not last_applied_manifest:\n\t\tlast_applied_manifest = [0]*64\n\n\t# If an observer schema exists for our observer_schema, update\n\tif observer_schema:\n\t\tif not \"last_applied_manifest\" in response:\n\t\t\treturn\n\t\telif last_applied_manifest:\n\t\t\tresponse[\"last_applied_manifest\"][-1] = last_applied_manifest[-1] + 1\n\n\tif \"version\" in response:\n\t\tlast_applied_manifest[0] ="}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t);\n\n\tdef update_manifest_list_from_resp(\t\"\"\"\n\tRecursively calls itself.\n\tThis function is called recursively to update a partial ``manifest``\n\tfrom a partial Kubernetes response\n\nArgs:\n    manifest_list (list): partial ``manifest`` being updated\n    response (list): partial response from the Kubernetes API.\n\nThis function goes through all observed fields, and initialized their value in\nmanifest_list if they are not yet present\t\"\"\"\n\t);\n\tdef update_volume_types_from_resp(\t\"\"\"\n\tRe"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\t):\n\tglobal last_applied_manifest\n\tlast_applied_manifest = []\n\t# update the current_last_applied_manifest and apply the new value\n\tif last_applied_manifest:\n\t\tlast_applied_manifest = unicodify(last_applied_manifest)\n\t\tupdate_last_applied_manifest_dict_from_resp(last_applied_manifest, self, response)\n\n\ndef update_last_applied_manifest_dict_from_resp(\tlast_applied_manifest,\tself, response):\t# update_last"}]}
{"task_id": "62b869eab4d922cb0e688cc5", "prompt": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\tprint \"\\n########################################################################################\"\n\t\tprint '###########################################################'\n\t\tprint '### Last APP applied manifest information is not properly present for this resource ###'\n\t\tprint '###########################################################################################'\n\t\tprint '###########################################################'\n\npackage test_test\n\nimport (\n\t\"errors\"\n\t\"os\"\n\t\"os/signal\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/go-log/log\"\n\tuuid \"github.com/satori/go.uuid\"\n\t\""}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t) -> None:\n\n\t\tfor k in observed_schema:\n\t\t\tm = observed_schema.get(k)\n\t\t\tif m is None:\n\t\t\t\tcontinue\n\t\t\tif 'last_applied_manifest_list' not in m:\n\t\t\t\tlast_applied_manifest[k] = m['last_applied_manifest_list']\n\t\t\t\tlog.warning(\"The last_applied_manifest_list entry \" \\\n\t\t\t\t            \"did *not* exist in the observed schema. \" \\\n\t\t\t\t   "}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t)\t\t:\n\t\tself.last_applied_manifest = json.loads(\tself.response.get(\"manifest\", {}).get(\"items\", {})\n\t\t\t\t\t\t\t\t\t[self._get_pod_name_in_dict(self.last_applied_manifest)])\n\t\tself.last_applied_manifest.pop(\"__typename\")\n\t\tself.last_applied_manifest.setdefault(\"labels\", {})\n\n\t\tself.last_applied_manifest = {\n\t\t\tk:\tv[self._get_pod_name_in_dict(self.last_"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t)\n\ndef update_last_applied_manifest_dict_from_list(    \t\t\"\"\"\n\tUpdate ``last_applied_manifest`` from a list of items that is returned\n\t\n\tThis function is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    - last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t\t- observer_schema (dict): partial ``observer_schema``\n\t    - response (array"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t)\n\t\tlast_applied_manifest\t\t= None\n\t\tlast_applied_manifest\t\t= get_last_applied_manifest()\n\t\tresponse\t\t\t\t\t= get_resp()\n\t\tfor k, v in last_applied_manifest.items():\t\t# Loop through all the observed fields\n\t\t\tif k in response.data.items():\n\t\t\t\tdel response.data.items()[\"%s\" % k]\t# Clear all fields from the response\n\t\t\t\tif response.data.items()[\"%s\" % k]:\n\t\t\t\t\tlast_"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t, last_applied_manifest_dict_to_update\n\t, last_applied_manifest\n\t, observer_schema\n\t, response\n\t\t)\n\n\t\tlast_applied_manifest_dict_to_update[\n\t\t\"last_applied_manifest.resource_version\"],\n\tlast_applied_manifest_dict_to_update[\n\t\t\"last_applied_manifest.manifest.content_hash\"],\n\tlast_applied_manifest_dict_to_update[\"last_applied_manifest.manifest.content_hash\"]\n\t\t\t\t,\n\t\"last"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t)\n\ndef update_schema_from_resp(\t\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``schema``\nfrom a partial Kubernetes response, after all\n\nArgs:\n    observed (dict): partial ``schema`` being updated\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nIn this case, `observed` contains fields that do not exist in `last_applied_manifest`,\n\nIn order to update"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\tfor k,v in last_applied_manifest.items():\n\t\t\t\n\t\t\t\tif k in observed and 'last_applied' in observed[k]:\n\t\t\t\t\t\tobserved[k]['last_applied'] = observed[k]['last_applied']\n\t\t\t\t\n#\tupdate_last_applied_manifest_list_from_api_v2()\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\tif 'last_applied' in  observed and 'last_applied' in response and response['last_applied'] : # if the response is a version of the last_applied field\n\t\t\t\tfor k,v"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t) {}\n\n\ndef update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tWe first call this function to update all ``last_applied_manifest`` fields\n\tfrom the ``response``, and then pass the new ``last_applied_manifest``\n\twhich contains all observed fields into the new ``last_applied_manifest``\n\t\"\"\"\n\n\t\t) {}\n\n\n# ====================================================================================\n# This function is called from the K8S client to update the resource type -----------\n# ====================================================================================\n\ndef get_all_types_from_"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\treturn _update_last_applied_manifest_dict_from_resp(\n\t\t\t\tresponse, last_applied_manifest, observer_schema)\n\n\ndef update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n    "}]}
{"task_id": "62b869eab4d922cb0e688cbf", "prompt": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tresources = set([app.manifest[\"spec\"][\"pod\"], app.manifest[\"spec\"][\"service_account_name\"]])\n\tresources.add(app.manifest[\"spec\"][\"service_name\"])\n\tfor r in resources:\n\t\tif not r.get(\"serviceaccount\"):\n\t\t\tr[\"serviceaccount\"] = \"sa\" + r.get(\"label\", \"\")\n\t\ts = {\"metadata\": {} }\n\t\ts[\"apiVersion\"] = \"apps/v1\"\n\t\ts[\"spec\"] = {\"template\": {\"spec\": {\"strategy\": \"Template\",\n\t\t   "}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\treturn [app]\n\ndef generate_custom_observer(app, observer_type=KubernetesWebhook):\n\t\"\"\"Generate a list of custom observer for each Kubernetes resource present in\n``spec.manifest`` for given ``app`` and ``observer_type``, respectively.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        custom observer schema\t\"\"\"\n\treturn [(app, observer_type)]\n\ndef generate_custom_observer_schema(app, observer_type=KubernetesWebhook):\n\t\"\"\""}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tif not app.metadata['kubernetes']:\n\t\traise ValueError(\"Kubernetes resource 'application' has not been loaded\")\n\tif app.is_active_resource('kubernetes.apps'):\t# this app is currently active\n\t\tk8s_object_name = generate_default_observer_schema_object(app)\n\t\tif k8s_object_name in generate_default_observer_schema_object_map_file:\n\t\t\topen(generate_default_observer_schema_object_map_file[k8s_object_name], \"rb\").close()\n"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tfor r in app.list_resources():\n\t\tr.generate_default_observers(app)\n\n\tfor r in app.list_resources():\n\t\t# Generate the default observer with the custom observer schema, if any\n\t\to = r.get_default_observer()\n\t\tif o is not None:\n\t\t\tr.generate_default_observers(app, o)\n\napp_regen_config = {\n\t# Configure the application to provide the \"default observer\" custom\n\t# observer schema\n\t\"default_observers\": [\n"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\toutput = \"{%s}\"\n\toutput_string = \"\"\n\tfor r in app.resource_dict.values():\n\t\tresult = generate_default_observer_schema_for_resource(r)\n\t\tif result:\n\t\t\toutput_string += result\n\n\tif output_string:\n\t\toutput_string += \" {\\n\"\n\t\toutput_string += output + \"}\\n\"\n\n\treturn output_string\n\n\ndef generate_default_observer_schema_for_resource(resource):\n\t\"\"\"Generate the default observer schema for `resource`.\n"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tapp_default_observer_schema = _getDefaultObserverSchema()\n\tfor resource_pod in app.get_items('resource'):\n\t\tif resource_pod.get_kind() == 'Pod':\n\t\t\tif'spec' in resource_pod.get_metadata().get('labels', {}):\n\t\t\t\tapp_pod_labels = resource_pod.get_metadata('labels')\n\n\t\t\t\t# if no new-style labels are registered for this kind of app,\n\t\t\t\t# add default label\n\t\t\t\tif app_pod_labels is None:\n\t\t\t\t\tapp_"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t# If no default observer schema has been specified for this application, set\n\t# the default to a simple list\n\tif 'default_observer_schema' not in app.data:\n\t\tif 'default' in app.data:\n\t\t\tapp.data['default_observer_schema'] = [app.data['default']]\n\t\tif'resources' not in app.data:\n\t\t\tapp.data['resources'] = {}\n    return app.data['resources']['default_observer_schema']\n\ndef generate_required_observer_schema(app):\n\t"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tif not (app.spec.manifest.get('apiVersion') \\\n\t   and app.spec.manifest.get('kind') \\\n\t   and app.spec.manifest.get('metadata') \\\n\t   and app.spec.manifest.get('spec') \\\n\t   and app.spec.manifest.get('template') \\\n\t   and app.spec.manifest.get('status')).get('custom_resource_mapping') \\\n\t   and app.spec.get('kuberesource', None) and app.metadata.get('name', None):\n\t\treturn"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tif app.spec.config.kubedn.webhook.config.enabled:\n\t\tfrom app.tools.kubedn_server import generate_default_cluster_config_deployment\n\n\t\tif app.spec.config.kubedn.webhook.config.webhook_url:\n\t\t\tapp.spec.yaml_file_name = '{}-{}-webhook.yaml'.format(app.spec.config.kubedn.webhook.config.webhook_project, app.spec.config.kubedn.webhook.config.webhook_name)\n\t\t\tapp.spec"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tfor obj in app.objects_by_path(\"spec.manifest.objects.\"):\n\t\tfor k, v in sorted(obj.attributes.items()):\n\t\t\t# Don't add attributes that would interfere with\n\t\t\t# the existing behavior (https://github.com/openstack/upgrade\n\t\t\t# will raise an exception when it shouldn't\n\t\t\tif k[0]!= \"attributes\" or v[0]!= \"attributes\":\n\t\t\t\tcontinue\n\t\t\tif k in (\"namespace\", \"creationTimestamp\", \"labels\", \"annotations\", \"labels\",\n\t\t\t\t\t "}]}
{"task_id": "62b43427903eeb48555d3ea5", "prompt": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 22.16 GiB total capacity; 20.79 GiB already allocated; 5.88 MiB free; 21.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b43428903eeb48555d3eaa", "prompt": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 626.00 MiB (GPU 0; 22.16 GiB total capacity; 20.42 GiB already allocated; 39.88 MiB free; 21.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b45df05108cfac7f2109ce", "prompt": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\ttry:\treturn validate(self.__get_path(path))\n\n\texcept NoValidOCFLCObjectException:\n\t\treturn False\n\n# --------------------------------------------------------------------\n\ndef path(name):\n\t\"\"\"\n\tReturn the full path to an OCFile object.\n\n\tThis may be either the actual path or the canonical path.\n\n\tTo open a file, run the path() function, and the second argument\n\tmust be the canonical name (in case the file has relative or absolute path references.)\n\tIf the filename is relative, it is assumed to be relative to the canonical path,\n"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tif path == None:\n\t\treturn True\n\telif os.path.isabs(path):\n\t\treturn True\n\telse:\n\t\treturn self.__get_fs_object(path)\n\t\n\t\t\t\t\n\t\t\t\n\n\n\n##############################################################################\nclass Filesystem(AbstractFSObject):\n\t\"\"\"\n\tFilesystem class.\tUses PyFSCatalog\n\n\tAttributes:\n\t\tfs_object -\tfs_object\n\t\n\tClass attributes:\n\t\tpath\n\t\t\tName the file system object.\n\t\t\n\tExample:\n\t\t>>> fp = Filesystem(\"path\")\n"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tif not os.path.exists(path):\n\t\treturn False\n\ttry:\n\t\to = open(path, 'rb')\n\texcept IOError:\n\t\treturn False\n\texcept:\n\t\treturn False\n\treturn o if o.read(4) == b'*' else None\n\ndef validate_pyfs_tag_tag_id(self, tag_tag_id, tag_id):\t\"\"\"\n\tValidate OCFL object at tag or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\ttry"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tif not osfile_isfile(path) or not pyfs.exists(path):\n\t\treturn False\n\n\tself.validate_info(None, self.get_path(path))\n\n\tfor fileobject in self.files:\n\t\tif not fileobject.exists():\n\t\t\tos.unlink(fileobject.path)\n\t\t\treturn False\n\n\tfor fileobject in self.fileobjects:\n\t\tobjecttype = fileobject.type()  # Get type of object.\n\t\tif fileobject.is_dir() and isinstance(fileobject,"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\troot = self.__resolve_path(path)\n\tif not os.path.isdir(root):\n\t\t# file does not exist\n\t\treturn False\n\n\tvalid= True\n\tif os.path.isfile(root + '/.pyfs_config'):\n\t\t# python script config file exists\n\t\tpyfs_config = root + '/.pyfs_config'\n\t\tconfig = FsConfig(pyfs_config)\n\t\t# check if any Python scripts were run successfully\n\t\tfor script_name in self.__pyfs_scripts:\n"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tself.validate_cache = True\n\t\n\tif self.is_ocfl_valid(path):\n\t\treturn True\n\t\n\t# Check the current directory\n\tself.current_dir = os.path.dirname(__file__)\n\tpath = os.path.realpath(path)\n\tcur_path = os.path.join(self.current_dir, path)\n\t\n\tif os.path.isdir(cur_path) and os.path.isdir(self.root_dir):\n\t\tself.valid_cache = True\n\t\tself."}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tif not os.path.exists(path):\n\t\treturn False\n\tif not os.path.isdir(path):\n\t\treturn False\n\tfn = os.path.join(path, '__init__.py')\n\tif os.path.isfile(fn):  # try loading module\n\t\treturn validate_pyf_objects(fn)\n\treturn True\n\n# ===================================================================\n\n\nclass ModuleInfo:\n\t\"\"\"\n\tInformation for modules\n\t\"\"\"\n\tdef __init__(self):\n\t\tself.names = {}\n\n\tdef add"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tself.validate_pyfs(self.fs.get(path))\n\n\tif path is not None:\n\t\traise oec.exception.PyfsNotFound(path)\n\t\n\t# Get the configuration tree as a string\n\tpath = self.config.get('pyfs_path','modules', 'ocfl')\n\t\n\t# Get the absolute path to the pyfs_path directory\n\tif path is None:\n\t\tpath = '/mnt/pyfs'\n\t\n\t# Validate the ocfl root exists\n\tocfl_root, ocfl_dir"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t# Get the name of the file on sys.path.\n\tpath = os.fspath(path)\n\tif not os.path.isdir(path) and not os.path.isfile(path):\n\t\treturn False\n\n\t# Create full path if not given on command line.\n\tif self.fs_target is None:\n\t\tself.fs_target = fs_target or 'pyfs'\n\t\tself.fs_full_path = os.path.abspath(\n\t\t\tos.fsencode(self.fs_target))\n\n\t"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tif not os.path.exists(path) and os.path.exists(os.path.dirname(path)):\n\t\tself.warning('File %s not accessible' % path)\n\t\treturn False\n\treturn True\n\t\n\t\ndef init_fs(path):\t\t\t# Forces creation of a new file system\n\t\n\t# Make a list of all the possible system commands\n\tcmd_lst = [\n\t\t('mkdir', ''),\n\t\t('mkdir', '-p'),\n\t\t('touch', ''),\n\t\t('touch', '-R'),\n\t\t('"}]}
{"task_id": "62b45df15108cfac7f2109dc", "prompt": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\treturn '{type} \"{value}\" - {prefix}'.format(type=self._type2name(self._status),\n\t\t\t\t\t\t\t\t\t\t\t  value=self._value_str(self._val),\n\t\t\t\t\t\t\t\t\t\t\t  prefix=self._get_prefix_string().translate(self.name) if prefix else'')\n\n\n\n\n\n\n\n\n\n# -- Common Validators -- #\nclass ValidatorMixin(object):\n\t\"\"\"\n\tUtility mixin that defines common validators.\n\t\"\"\"\n\n\tVALIDATORS = ValidationSupport.VALIDATORS\n\tvalidator_type = ValidationSupport.validator_type\n"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tif not self.is_active:\t\t\t\treturn 'not_active'\n\t\tif self.active_version==None:\treturn 'invalid'\n\t\tif self.active_version<0:\t\treturn 'invalid'\n\t\tif self.active_version>self.highest_version:\treturn 'invalid'\n\t\tif self.active_version==0:\t\treturn '%d of %d' %(self.active_version,self.highest_version)\n\t\treturn '%d of %d' %(self.active_version,self.highest_version"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t# pylint: disable=ungrouped-imports\n\n\tdef _status_str(self, value):\n\t\tprefix = '{0} {}'.format(prefix, self.__class__.__name__)\n\t\tif value == SUCCESS:\n\t\t\treturn 'True'\n\t\tif value == FAIL:\n\t\t\treturn 'False'\n\n\t\t# Default values\n\t\tvalue = self._map_data(_value)\n\n\t\treturn '{0} {1}'.format(prefix, str(value))\n\n\t# @cache_data\n\tdef _map_data(self,"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tprefix = '', prefix\n\tcode = prefix + str((self.code, ))\n\tif self.valid or self.tested or not self.error:\n\t\tcode = prefix + str(self.code)\n\treturn code\n\n\tclass ConfigError(Exception):\n\tpass\n\n\tdef get_config_errors():\n\t\t# this function is called on each instance of ValidationBase\n\t\traise NotImplementedError\n\nclass Validated(object):\n\t\"\"\"\n\tValidation class to wrap validation methods and validate data.\n\n\t.. note::\n\t\tThis is the base class"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\tif self.validator:\n\t\t\t# If validator has a status string, return it.\n\t\t\treturn self.validator.__dict__[\"report\"][\"status\"][\"\" if prefix!= \"\" else \"\"]\n\t\telse:\n\t\t\treturn STATUS_UNKNOWN\n\n\tdef is_onelinerary(self, *args):\n\t\t\"\"\"Return boolean of whether validator is onelinerary.\"\"\"\n\t\treturn self.status_str() in self.STATUS_ONELEINERARY\n\n\tdef is_valid(self, *args):\n\t\t\"\"\"Return boolean of whether validator is valid.\"\"\""}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tif not self.status:\n\t\tstatus_str = '<UNKNOWN>'\n\telif self.status.is_OK:\n\t\tstatus_str = 'OK'\n\telif self.status.is_OK(prefix=PREFIX_MSG_OK):\n\t\tstatus_str = 'OK'+ prefix\n\telif self.status.is_ERROR(prefix=PREFIX_MSG_ERROR):\n\t\tstatus_str = 'ERROR'+ prefix\n\telif self.status.is_WARNING(prefix=PREFIX_MSG_WARNING):\n\t\tstatus_str = 'WARNING '"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tstatus = Validator.Validation_OK\n\tif self._status & self.VALID: status |= Validator.Validation_NEED_AUTH\n\tif self._status & self.PENDING: status |= Validator.Validation_NEED_REVIEW\n\tif self._status & self.VALID_PASSCODE: status |= Validator.Validation_NEED_PASSCODE\n\treturn self.VALID_OK_PREFIX + \" \" + prefix + self.STATUS_PREFIX + self.VALID_OK_PREFIX + status\n\ndef get_user(self, nick):\n\t\"\"\"\n\tReturn user that user exists or None otherwise."}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tstatus = 0\n\tfor item in self.validator.status:\n\t\tif len(item[2]) > 0 and item[2][0] == prefix:\n\t\t\tstatus = status | item[1]\n\treturn \"%d (%s) %s\" % (status, status & 0x100?\"not\":\"\", status & 0x1FF?\"is\":\"st\")\n\n# ################################################################################################################################\n\nclass Validator(object):\n\n\tdef __init__(self, *args, **kwdargs):\n\t\tif (args and kwdargs and kwdargs"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tv = self._get_option( prefix )\n\tif v is not None and isinstance( v, int ):\n\t\tstatus = ValidatorStatus(v)\n\t\treturn f'{prefix}{status.name}'\n\telif v is not None and isinstance( v, str ):\n\t\tstatus = ValidatorStatus(v)\n\t\treturn f'{prefix}{status.name}'\n\treturn 'unknown'\n\n\tdef get_description(self, prefix=\"\"):\n\t\tdesc = \"\"\n\t\tv = self._get_option( prefix )\n\t\tif v is not None"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\ttry:\n\t\tstatus = self.status\n\t\treturn '%s %s' % (prefix, (status.code or '???'))\n\texcept ValueError:\n\t\treturn 'error not a known validator status'\n\n\n#\tImplementation notes:\n\n\nclass BaseValidator(object):\n\n\t__slots__ = ('name', 'validator_class','schema', 'errors')\n\t_validators = None\n\n\tdef __init__(self, validator_class=None, schema=None):\n\t\tself.name = validator_class\n\t\tif schema is None"}]}
{"task_id": "62b45df15108cfac7f2109dd", "prompt": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\tif not self.valid: return ['empty']\n\tif not hasattr(self, 'info_str'): return ['empty']\n\tif prefix: prefix +=''\n\tfor key in self.info_str: prefix +='   {}:  {}'.format(key, self.info_str[key])\n\treturn prefix\n\ndef _test():\n\t\"\"\"\n\tExecute unit tests from 'ipalib.frontend.validate' module\n\timport ipalib, sys\n\tipalib.frontend.validate\n\t\"\"\"\n\tprint('Testing validation log format...')\n\tsys"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\treturn\t_(\tself.__doc__)\\\n\t\t+\t(\"\\n\\tValid: %s\" %(\tself.__valid__) \\\n\t\t\t+ \"\\n\\tValid: %s\" %(\tself.__valid_log__) \\\n\t\t) \\\n\t\t.format(prefix=prefix)\n\n\ndef _is_valid_val(val):\n\t\"\"\"\n\tCheck if value is defined.\n\n\t:param val: value, as string or int\n\t:returns bool:\n\t\t- True if valid\n\t\t- False if invalid\n\t:see"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\treturn prefix + \"\\n\" + \" \".join([l.strip() for l in\n\t\tself.log.split(\"\\n\")]) + \"\\n\"\n\nclass LogRecord(BaseModel):\n\t_name = 'logging'\n\t_type_ = 'validation-log'\n\tself_type ='self_type'\n\t_base_data_type_ = 'validation-log'\n\t_base_data_type_value_ = 'validation-log'\n\tself_data_type_ ='self'\n\t_default_value_ = 'logging'"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\tprefix_len = len(prefix)\n\tif (prefix_len < self.STATUS_MAX_LEN):\n\t\tstatus = self.STATUS_PREFIX[self.STATUS_FLAGS[self.STATUS_KEY(self.STATUS_VALUE)]]\n\t\tif (self.STATUS_VALID_FLAG(self.STATUS_VALUE,\n\t\t\t\t\t   self.STATUS_KEY(self.STATUS_VALUE+1-prefix_len),\n\t\t\t\t\t   prefix_len)):\n\t\t\treturn '[%s>%s]' % (self.STATUS_FLAGS[self.STATUS_KEY(status"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\ts = ['%s[%s(val=%s)]' % (prefix, s[3:], val)]\n\tfor key, val in self.items():\n\t\ts.append('    %s=%r' % (key, val))\n\n\treturn '\\n    ;'.join(s)\n\n\nclass Validator:\n\t\"\"\"\n\tAn implementation of the Validator class intended to be used with\n\t:class:`ValidatingDict`.\n\n\tThis class implements a simplified version of the built-in dictionary\n\tclass, but extends Python's validation module to support"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\tif self._status_flags:\n\t\tprefix = '%s'%prefix\n\t\treturn prefix + \" \" + self._status_str()\n\treturn self._status_str()\n\ndef _validations():\n\tstatus_cache = list()\n\twhile True:\n\t\tvalidation = logging.ValidationList()\n\t\tif not validation:\n\t\t\treturn status_cache\n\t\tlogging.root.manager.validation = validation\n\t\tlogging.root.start()\n\t\tif not validation.run():\n\t\t\tlogging.root.stop()\n\t\t\tstatus"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\tself.log = \"\"\n\tfor k, v in self.log.items():\n\t\tself.log += \"[%s] \" % k\n\tself.log += '%s\\n' % self._format_status(self.status)\n\tself.log += ''.join((prefix + \"%s= %s\" % (k, v) for k, v in self.log.items()))\n\treturn self.log\n##\treturn repr(self).replace('\\x1b\\x5b\\x5b', self.status_str())\n\n\n\t\n##"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\tresult = string_as_list(self.status)\n\tif status[0] == 'error':\t# pragma: no cover\n\t\tprefix = 'Error: '\n\t\telif status[1] == 1:\n\t\t\tprefix = 'Skipped: '\n\t\telif status[1] >= 2:\n\t\t\tprefix = 'Failed: '\n\t\telse:\n\t\t\tprefix = 'Warning: '\n\treturn '%s%s' % (' '.join(result), prefix)\n\n\n\n## Class Variables ###########################################################################################\n\nclass ValidationLog(object):"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\treturn\tprefix + 'Validation Status:'+ \\\n\t\t\t\t\tself.status +'-'+ self.severity + \\\n\t\t\t\t\t' #'+ print_r(self.errors, self.verbosity) + \\\n\t\t\t\t\t' - # failures:'+ str(self.failures) + \\\n\t\t\t\t\t' - # warnings:'+str(self.warnings)\n\nclass ValidatingResult(object):\n\tdef __init__(self, msg, status=None):\n\t\tsuper(ValidatingResult, self).__init__()\n\t\tself.status = status\n\t"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\treturn '%s\\n' % (prefix or '') + str(self.__status)\n\n\t\n\t\n###\n## Generic validation helper functions\n###\n\ndef validation_func(func):\n\t\"\"\"\n\tWraps user supplied function to convert it to actual object, and checks \narguments. \n\t\n\tParameters\n\t----------\n\tfunc       : valid callable function, must accept any argument and return\n                                    "}]}
{"task_id": "62b45e135108cfac7f2109f4", "prompt": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n\n/*\n * Copyright (\u00a9) 2021 The webRTC project authors. All Rights Reserved.\n *\n * Use of this source code is governed by a BSD-style license\n * that can be found in the LICENSE file in the root of the source\n * tree. An additional intellectual property rights grant can be found\n * in the file PATENTS.  All contributing project authors may\n * be found in the AUTHORS file in the root of the source tree.\n */\n#ifndef TEST_PC_E2E_CALL_PEER_CONNECTION_H_\n#define TEST_PC_E2E"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n\t\n\t\n# A couple methods to help construct a unique filename, filepath, and fileformat tuple.\nclass uniquefiledict(dict, object):\n\t\"\"\"\n\tA dictionary-like object for an arbitrary key\n\t\"\"\"\n\tdef __init__(self, fspath, fileformat=None):\n\t\tself.fspath = fspath\n\t\tself.fileformat = fileformat\n\t\t\n\tdef __getstate__(self):\n\t\tk = _getk()\n\t\ttry:\n\t\t\tv = _getv(self.fspath)\n\t"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n\n\nclass InMemoryTableBase:\n\tdef __init__(self):\n\t\tself.base_sql = f\"\"\"\nUPDATE or_user_settings_table SET\n\tcreated = NOW(),\n\tmodified = NOW(),\n\tupdated_on = NOW(),\n\tupdated_by = USER_ID()\nWHERE\n\tidentifier = '{self.TABLE_ID.upper()}'\n\"\"\"\n\n\tdef get_sql(self, identifier):  # pylint: disable=unused-argument\n\t\treturn self.base_sql\n\n\tdef create_table"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n\n\t# pylint: disable=unused-argument\t\nclass File(object):\n\t\"\"\"\n\tThis is the base class for storing and managing files within the project. \n\n\t\t>>> import zabbix_embedded.zabbix_embedded_config as zabbix_embedded_config\n\t\t>>> zabbix_embedded_config = zabbix_embedded.ZabbixEmbeddedConfig.get_instance()\n\t\t>>> file_name = ''\n\t\t>>> file_id = zabbix_embedded_config.get('file_identifier')\n\t\t"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\tif not isinstance(identifier, str_cls):\n\t\tidentifier = str_cls(identifier)\n\n\ttry:\n\t\treturn identifier in self.VALID_IDENTIFIER\n\texcept KeyError:\n\t\treturn False\n\n\tdef set_valid(self, identifier, valid):\n\t\t\"\"\"\n\t\tSets the string value from the string, validates the value,\n\t\tand stores the new string value and the new string validity.\n\n\t\t:param identifier: The identifier of the current string, usually a string.\n\t\t:type identifier: str\n\t\t\"\"\"\n\t"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n\n\nclass CleoFileParser(BaseParser):\n\t\"\"\"\n\tBase class for parsing the CLEO file.\n\t\"\"\"\n\t_DEFAULT_PARSER = \"cleo\"\n\n\tdef __init__(self, **kwargs):\n\t\tsuper().__init__(**kwargs)\n\n\t\t# default values, always valid\n\t\tself._metadata = {}\n\t\tself._metadata_version = {\n\t\t\tMETADATA_KEY_VERSION_MAJOR: METADATA_VERSION_MAJOR,\n\t\t\tMETADATA_KEY_VERSION_MINOR: METADATA_VERSION_MINOR"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn self.__is_valid(__identifier)\n\n\tdef __is_valid(self, identifier):  # pylint: disable=unused-argument\n\t\t\"\"\"\n\t\t@returns: True if identifier is in the list, False else\n\t\t\"\"\"\n\t\treturn identifier.find(self.__identifier) == 0\n\n\tdef is_numeric(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\t\t\"\"\"\n\t\t@returns: True if identifier is a numeric identifier in the set,\n\t\tFalse else.\n\t\t\"\"\"\n\t\treturn self"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n\nusing System.Windows.Controls;\nusing System.Windows.Data;\n\nnamespace ZdfFlatUI.Test.UiSpectrum.Duo\n{\n    /// <summary>\n    /// StyleBarControl.xaml \u7684\u4ea4\u4e92\u903b\u8f91\n    /// </summary>\n    public partial class StyleBarControl : UserControl\n    {\n        public StyleBarControl()\n        {\n            InitializeComponent();\n  "}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n\n\n#####################################################################################################################\n#\n# Class to hold a single word or its sub-words and the respective list-strings\n# associated with that word or sub-word\n#\n#####################################################################################################################\nclass SubWordList(object):\n\t\"\"\"\n\tImplements the word list to hold a single Word.\n\t\"\"\"\n\tdef __init__(self, word_list):\n\t\tself.words = word_list\n\t\tself.word_list_list = list(self.words)\n\n\tdef __iadd__(self, s):  #"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n\nclass NoMatchingBaseException(Exception):  # pylint: disable=too-many-instance-attributes\n\tpass\n\n#include <math.h>\n#include <stdio.h>\n#include <stdlib.h>\n\n#include \"libm.h\"\n\ndouble __libm_logf(double x) {\n\treturn __ilogb(x) - 1.0;\n}\n\ndouble logf(double x) {\n\t/* http://functions.wolfram.com/Gamma/Gamma.html */\n\tstatic const double\n\t "}]}
{"task_id": "62b45e145108cfac7f210a07", "prompt": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tfor inv in get_inventory(inventory):\n\t\tif inv.name is None:\n\t\t\tcontinue\n\t\tif inv.type is None:\n\t\t\traise ValueError(\"No type defined on {}.\".format(inv.name))\n\t\t\n\t\tif inv.type[0] == 'T':\t# type is a valid type\n\t\t\tcontinue\n\t\telif inv.type[0] in (\"V\", \"t\", \"C\", \"s\") and inv.type.upper()!= \"V':\t# type is a valid type\n\t\t\tcontinue\n\t\telif inv."}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t# Make sure the inventory is a valid inventory by checking if it is a\n\t# version 1 and an instance of a spec type.\n\tfor i, idef in enumerate(inventory):\n\t\tis_version1 = idef.get(\"type\") == \"version1\"\n\t\tis_instance = idef.get(\"type\") == \"instance\"\n\t\tif not is_version1 and not is_instance:\n\t\t\tcontinue\n\t\t\n\t\tif extract_spec_version and not idef.get(\"spec_version\"):\n\t\t\tcontinue\n\t\t\t\n\t\tif str"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\ttry:\n\t\t# first the base classes\n\t\tfor o in self.__class__.__bases__:\n\t\t\tif not (hasattr(o, 'is_' + self.__class__.__name__) or\n\t\t\t        isinstance(o, (type, type))):\n\t\t\t\tcontinue\n\t\t\to.validate(self, inventory, self.spec_version, extract_spec_version)\n\n\t\t# now the subclasses\n\t\tfor o in self.__class__.__mro__[:-1]:\n\t\t\tif not (hasattr(o, 'is_' +"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tif len(inventory) <= 0: return True # No inventory?\n\n\tfor item in inventory: # Iterate through the inventory.\n\t\tis_a_type = True\n\t\ttry:\n\t\t\tis_a_type = self.ValidateItem(item, extract_spec_version)\n\t\texcept Exception as err:\n\t\t\tif self.verbose: print \"# FAILED: %s: %s\" % (item._debug_info(), err)\n\t\t\ttry:\n\t\t\t\traise\n\t\t\texcept Exception as err:\n\t\t\t\tself.fail(\"Unable to validate"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tif not isinstance(inventory, Inventory):\n\t\treturn False\n\t\t\n\tif not extract_spec_version:\n\t\treturn True\n\t\n\tif not self.spec_version:\n\t\tfor spec in inventory.all_specs():\n\t\t\tversion = spec.get(\"Version\", \"\")\n\t\t\tif version not in self.spec_version:\n\t\t\t\tself.failed_spec(\"Invalid spec version '%s'\" % version)\n\t\t\t\treturn False\n\t\t\n\tif self.spec_version not in inventory.all_specs():\n\t\tself.failed_spec(\"Invalid spec"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tfrom. import parser\n\tfrom. import validators\n\n\tif inventory.type.find('type')!= -1 and not extract_spec_version:\n\t\ttry:\n\t\t\tself.spec_version = parser.types[inventory.type]\n\t\t\tself.spec_version = self.spec_version.spec_version\n\t\texcept KeyError:\n\t\t\tnew_key = \"ERROR: no type found in spec version: %s\" % inventory.type\n\t\t\tif inventory.type == \"image\":\n\t\t\t\tnew_key = \"ERROR: no type found"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tname = self.name\n\n\t# If there is a type value, go through the hierarchy and add to the item names\n\tif self.item_type:\n\t\tself.item_names = add_item_name(inventory, self.item_type, self.item_names)\n\t\tif extract_spec_version:\n\t\t\tself.spec_version = self.spec_verion\n\t\t\tif self.spec_version!= None:\n\t\t\t\tif isinstance(self.spec_version, int):\n\t\t\t\t\tif 0 < self.spec_version <= 255"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif self.check_types:\n\t\t\t\tself.check_types(inventory)\n\n\tdef check_types(self, inventory, version=False):\n\t\tversion_map = {}\n\t\tspec_version = None\n\t\tfor type_name in (str, int, bool, dict):\n\t\t\ttype = self.data.getType(type_name)\n\t\t\tif type is None:\n\t\t\t\ttry:\n\t\t\t\t\tvalue = inventory.get(type_name, type.from_name, type.to_name)\n\t\t\t\texcept:\n\t\t\t\t"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tself.validate_spec_version(inventory, extract_spec_version)\n\n\tfor item in inventory:\n\t\tif item.get_type() == \"item\":\n\t\t\tself.validate_item(item, inventory, extract_spec_version)\n\n\tif not self.spec_version: check_type(inventory, \"spec_version\", str, True)\n\tfor spec in inventory:\n\t\tif not self.spec_version: check_type(spec, \"spec_version\", str, True)\n\n/*-\n * Copyright (c) 2006-"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tif not inventory.type:\n\t\tinventory.createType()\t#\tCreate the type.\n\t\tself.assertIn(\"type\", inventory.getType().keys())\n\t\t\t#\tThe type we're working with is a dictionary\n\n\tif not inventory.getSpec() or not inventory.getSpec().keys():\n\t\t#\tNo spec given, exit now\n\t\tprint(\"Please input a spec file.\")\n\t\tprint(\"Type in the inventory:\")\n\t\tprint(inventory.type)\n\t\texit_with_error(1)\n\n\tif not isinstance("}]}
{"task_id": "62b45e145108cfac7f210a09", "prompt": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor digests_file in digest_files:\n\t\tif digests_file is None:\n\t\t\tself.log.fatal(\n\t\t\t\t\"Error: {} is not valid.\t Digests files must be listed with the --digests-file argument\".format(\n\t\t\t\tfile=digests_file)\n\t\t\t)\n\t\twith open(digests_file, 'r') as fd:\n\t\t\tfor fd_digest in enumerate(fd):\n\t\t\t\tif fd_digest == 5: continue\n\t\t\t\tvalue_name = fd_digest[3].title()"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tif self.debug:\n\t\tprint(\"Checking for Digests used in {}.\".format( manifest_files ))\n\tfor manifest_file in manifest_files:\n\t\tif manifest_file[0] == \"Digests\":\n\t\t\tfiles = glob.glob( manifest_file + \"/*\" )\n\t\t\tfor file in files:\n\t\t\t\tif file[-3:] == \"md5\":\n\t\t\t\t\tmd5 = file[:-3]\n\t\t\t\t\tif md5 == self.digest:\n\t\t\t\t\t\tfiles.remove( file )\n\t\t\t\t\t\t\n\t\t\tfor file in files:"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor i in manifest_files:\n\t\tif not os.path.exists(i):\n\t\t\traise ConfigParser.Error(\"%s does not exist\" % i)\n\n\t\tf = open(i)\n\t\tf.close()\n\t\t\n\t\tself._all_files.add(i)\n\t\tdigests_to_check = [x for x in manifest_files if x[:-4].endswith(digest)]\n\t\tself._check_digests_used(digests_to_check, digests_used)\n\t\t\n\t\tfor index in all_"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tcheck_digests_present(self, manifest_files, digests_used)\n\tcheck_digests_unused(self, manifest_files, digests_used)\n\n\n\tdef check_digests_present(self, manifest_files, digests_used):\n\t\t\"\"\"Checks that all used digests are present in all manifest files.\"\"\"\n\t\tall_digests_included = True\n\t\tfor manifest_file in manifest_files:\n\t\t\twith open(manifest_file, mode='rb') as manifest_file_ptr:\n\t\t\t\tfor digest_"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tself.manifest.check_used_digests(digests_used)\n\tfor manifest_file in manifest_files:\n\t\tself._check_present_digests(manifest_file)\n\n\tfound_unused_files = len(\n\t\t\tself.manifest.get_unused_digests(digests_used))\n\tself.assertTrue(found_unused_files == 0,\n\t\t\t\"Got %d unused digests while expecting %d.\",\n\t\t\tfound_unused_files, 0)\n\n\tself.assertEqual(self.manifest.get_digest_"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tmanifest = manifests(dict.fromkeys(manifest_files), self.__path_to_manifest)\n\n\twith open(manifest=self.__path_to_manifest + manifest, 'w') as f:\n\t\tf.write('<?xml version=\"1.0\" encoding=\"utf-8\"?>')\n\t\tf.write('<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">')\n\t\tf.write('<plist version=\"1.0"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\twith open(manifest_file, encoding='UTF-8') as file_handle:\n\t\t\tmanifest = json.load(file_handle)\n\t\tdigests_in_file = set([h['digest'] for h in manifest])\n\t\tdigests_used_in_file = set([h for h in manifest_files if h['digest'] not in digests_in_file])\n\t\tif digests_used_in_file.issubset(digests_used):\n\t\t\tprint(\"One of the"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\tif self._manifest_files_to_keep_dict[manifest_file]:\n\t\t\tcontinue\n\t\ttry:\n\t\t\tif digests_used.keys():\n\t\t\t\tself.check_manifest_digest(manifest_file, digests_used)\n\t\texcept:\n\t\t\tif self._get_manifest_name_dict(self._get_last_manifest_names_dict_name_list()).count(manifest_file):\n\t\t\t\traise\n\n#pragma once\n\n#include \"envoy/server/"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor f in manifest_files:\n\t\ttry:\n\t\t\twith open(f) as fd:\n\t\t\t\tdigests.check_digests_present_and_used(fd, dict(self.parse_manifest[f]), digests_used)\n\t\t\tif digests_used[f]:\n\t\t\t\traise DigestError(\"No digest files to parse needed for this package\")\n\n\tdef check_package_digests_present_and_used(self, package, digests_used):\n\t\tif not digests_used.get(package.name):\n"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tif self.options.verbose: print (\"Checking manifest_files\")\n\tif self.options.verbose: print ( manifest_files )\n\tfor manifest_file in manifest_files:\n\t\tfp = open( os.path.join( self.options.source_folder, manifest_file), 'rU')\n\t\tmanifest = fp.read()\n\t\tfp.close()\n\n\t\tif self.options.digest:\n\t\t\tdigest = self._find_digest(manifest, self.options.digest)\n\t\t\tif digest is None:\n\t\t\t\tprint (\""}]}
{"task_id": "62b45e165108cfac7f210a16", "prompt": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t#\n\t# We need to validate the versions of the inventory object itself\n\t# before checking against the rest of inventory.  The last possible error\n\t# of the first one is that the input is incompatible with the current\n\t# inventory version. We may end up throwing an exception if the current\n\t# inventory is not compatible with the prior version.\n\t#\n\t# As far as this function is concerned, it never fails and we simply have to\n\t# make sure there are no remaining validators of incompatible version.\n\t#\n\t# We can be sure that all validators which"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tif not self.is_valid_for_object(prior):\n\t\traise WrongInventoryVersionError(\"invalid inventory object, no prior version specified\")\n\tself.validate_prior(prior, \"prior is currently a %s inventory\" % prior.name, prior, True)\n\ndef validate_prior_version(self, prior, val):\n\tif not self.is_valid_for_object(prior):\n\t\traise WrongInventoryVersionError(\"invalid inventory object, got %s as prior version for %s\" \\\n\t\t\t\t\t\t\t\t\t\t% (val, prior.name))\n\n"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tif not hasattr(self, '_version') and not hasattr(prior, '_version'):\n\t\tmsg = _('Inventory object must have been saved.')\n\t\traise UnidentifiedObjectError(msg)\n\tif not isinstance(prior, InventoryValidator):\n\t\tmsg = _('Inventory object must have been generated from a InventoryValidator object.')\n\t\traise UnidentifiedObjectError(msg)\n\tif getattr(self, '_version', None) < getattr(prior, '_version', None):\n\t\tmsg = _('Inventory object must not have been saved prior to current version of inventory object.')\n"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\treturn self.inventories.find_first(_inventories_with_prior(*prior))\n\n############################################################################\n\n# Classes\n\nclass _inventory_validator(InventoryValidator):\n\n\t#-----------------------------------------------------------------------\n\t# Initialization\n\n\t#-----------------------------------------------------------------------\n\tdef __init__(self, inventory_file, settings, item_settings):\n\t\tsuper(_inventory_validator, self).__init__(inventory_file, settings, item_settings)\n\t\tif settings['verify_on_exit']:\n\t\t\tself.validate_inventories()\n\n\t#-----------------------------------------------------------------------\n\t#"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t# XXX This check does not follow any strict rules, and it should be safe\n\t# to run before the method is called.\n\tif not isinstance(prior, InventoryValidator_v1):\n\t\traise Exception('inventories must be a InventoryValidator_v1 object')\n\t# XXX check the classname, and if that doesn't work, call\n\t# self.__class__.name\n\treturn self.get_version() == prior.get_version()\n\n_AS_INVENTORY = 'inventory_validator_v1'\n_AS_INVENTORY_INFO ="}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tif isinstance(prior, InventoryValidator):\t# XXX: may be a ValueError!\n\t\tassert(self.get_as_raw_inventory() == prior.get_as_raw_inventory())\n\t\tassert(self.get_as_inventory() == prior.get_as_inventory())\n\t\tassert(self.get_as_data_map() == prior.get_as_data_map())\n\n\tassert(self.get_prior_version() == prior.get_prior_version())\n\tassert(self.get_prior_version() == self."}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\ttry:\n\t\tversion = self.get_inventory().get_inventory_version()\n\texcept exceptions.InventoryException:\n\t\tversion = None\n\tif not version and isinstance(prior, InventoryValidator):\n\t\treturn prior\t# return if prior instance is an InventoryValidator but it's a Inventory\n\tif version:\n\t\t# validate if prior version is in this inventory\n\t\tversion_dict = {str(v):p for v, p in prior.get_inventory_version_list()}\n\t\t# if 'x' in version_dict then that it is a"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tprior_version = self.prior_version\n\tif prior_version and prior.prior_version!= prior_version:\n\t\traise Exception(\"Incompatible inventory.Prior versions.\")\n\n\tif prior.inventory == self.inventory:\n\t\treturn\t\t\t\t\t# the prior is from the same inventory\n\ttry:\n\t\tif self.inventory!= None and prior.inventory!= None:\t# the prior is from another inventory\n\t\t\tfrom..inventory.InventoryValidator import InventoryConstraintValidator\n\t\t\tfrom..inventory.InventoryValidator import InventoryConstraintValidatorForVersion\n\t\t\t# make sure version 0"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tif isinstance(prior, InventoryValidator) == False:\n\t\traise TypeError('InventoryValidator expected, got %r instead'%type(prior))\n\n\tself._check_valid_priorversion(prior)\n\tself.prior_version= prior._get_priorversion()\n\tself.__class__ = self.__class__ is not None and self.__class__ or InventoryValidator\n\n\t# validate the fields in _get_inventory\n\tself._get_inventory(prior)\t\t# raises a lot of python exceptions\n\n\treturn prior\n\nclass Inventory(object):\n\t\n\tdef"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tmsg = \"Incompatible version number passed to InventoryValidator\"\n\tassert len (prior)>0, msg+\"Prior must be nonempty.\"\n\n\tdef is_valid(var):\t\t\t# Check if variable has valid prior version\n\t\traise NotImplementedError('must be implemented by deriving from '\n\t\t\t'InventoryValidator instead of self.  See InventoryValidator')\n\n\tdef validate(self, var):\t\t# Invoke on the inventory object and return\n\t\tif is_valid (var):\t\t\t\t# Valid prior version\n\t\t\tself.update (var, False)\t\t#"}]}
{"task_id": "62b45e165108cfac7f210a17", "prompt": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tret = {}\n\tfor inventory in self.state:\n\t\tif inventory.version_version < version:\n\t\t\tcontinue\n\t\tif not inventory.content_files:\n\t\t\tcontinue\n\t\tret[inventory.logical_path] = set(inventory.content_files)\n\treturn ret\n\n\ndef write_inventory(inventory):\n\tfor logical_path in inventory.get_logical_path_map(0, 0):\n\t\tfilepath = os.path.join(cfg.DATA_DIR,'state', logical_path, 'inventory.yaml')\n"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\t# We can cache it because we will need it to perform\n\t\t# a search if it is going to take a long time.\n\tdef _get_data_map(inventory, logical_path_map, version_name):\n\t\tfile_path = f\"{version_name}/{inventory}/{logical_path_map}\"\n\t\tdata_map = set()\n\t\t# If it's going to take sometime, get its contents\n\t\t# instead of fetching it every time.\n\t\tif os.path.exists(file_path):\n\t\t\twith open("}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tnew = {}\n\tfor fullpath, fullstate, version in inventory.get_by_state(version, True):\n\t\tfor (logicalpath, contentfile) in map(lambda s: re.match(r'^(.+)/(.*)$', s), inventory.state_dict[fullstate]):\n\t\t\tif contentfile in content_files and contentfile not in 'noarch':\n\t\t\t\tnew[logicalpath] = contentfile\n\treturn new \n\t\ndef save_version(inventory, version, content_files, filename = None):\t\"\"\"\n\tSave a version into"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tlogical_path_map = {}\n\n\tfor component in inventory.values():\n\t\tcontent_files = component[\"content_files\"]\n\t\tlogical_path_in_state = component[\"logical_path_in_state\"]\n\t\tif logical_path_in_state:\n\t\t\tfor logical_content_file in logical_path_in_state:\n\t\t\t\tlogical_path_map[logical_content_file] = content_files.get(logical_content_file)\n\treturn logical_path_map\n\n\ndef check_for_duplicates_"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tversion_str = inventory.get_version_str(version)\n\tif version_str not in inventory.get_logical_path_map():\t#if the version has not been added yet\n\t\treturn {}\n\tlogical_path_in_state = get_logical_path_collection(inventory)\n\tcontent_files = get_content_files(inventory, version)\n\tversion_meta = get_version_metas(inventory, version)\n\treturn get_reference_file_map(logical_path_in_state, version_meta, content_files)\n\n"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tlogical_path_map = {}\n\tfor version in inventory:\n\t\tpaths = set([])\n\t\tfor physical_path in version.physical_path.split('/'):\n\t\t\tfor logical_path in physical_path.split('/'):\n\t\t\t\tif logical_path.startswith(inventory['physical_path']['logical_path']):\n\t\t\t\t\tpaths.add(logical_path)\n\t\tfor logical_path in paths:\n\t\t\tlogical_path_map[version.version][logical_path] = pathlib.Path(version.physical_path)\n\t"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tif inventory is None:\n\t\treturn {}\n\n\tlogical_path_map = {}\n\n\tfor path, pathlist in inventory.items():\n\t\tif 1 in pathlist:\n\t\t\tcontinue\n\n\t\tkey = tuple(x[1] for x in pathlist.itertuples())\n\t\tpathlist.sort()\n\t\tpathlist.reverse()\n\n\t\tfor name, file_list in pathlist:\n\t\t\ttry:\n\t\t\t\tfor entry in file_list:\n\t\t\t\t\tif entry:\n\t\t\t\t\t\tif isinstance(entry,"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tlogical_path_mapping = {}\n\tfor state, version in inventory.keys():\n\t\tif version < version_info.CURRENT_VERSION:\n\t\t\tcontinue\n\t\tstate_file = logical_mapping.get_version_file_for_state(state)\n\t\t# add all state versions for current version, skipping duplicates\n\t\t# (e.g. if current state version is the same as the version to copy it to)\n\t\tlogical_path_mapping[state] = state_file\n\treturn logical_path_mapping\n\ndef get_partial_state"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tif version >= inventory.get_version():\n\t\tversion_path = inventory.get_version_path(version)\n\t\tlogical_path_in_state = {}\n\t\tfor file in inventory.get_file_set(version_path, \"content\"):\n\t\t\tlogical_path_in_state[file.get_path()] = file\n\t\treturn logical_path_in_state\n\telse:\n\t\treturn inventory.get_file_set(version_path)\n\ndef get_logical_path_set(inventory, version, exclude_path):"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\treturn {}\n\ndef write_entity_files(entity, version, inventory, version_metadata):\n\t\"\"\"\n\tWrite the entity files for version from version_metadata to state.\n\n\tArgs:\n\t\tentity: entity dictionary.\n\t\tversion: version this entity corresponds to.\n\t\tinventory: inventory to apply this entity to.\n\n\tRaises an error if 'file_only', 'only_in_data' or'remove_metadata' is True.\n\tTrue means that files will only be present on disk and should be cleaned up.\n\tFalse means that the data"}]}
{"task_id": "62b45e175108cfac7f210a19", "prompt": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tself.fixity = fixity\n\tif fixity.fixity == \"missing\":\n\t\traise ParserError(\"Missing fixity block: Missing\")\n\tif fixity.fixity == \"in_inventory\":\n\t\tmissing = list(fixity.file_list)\n\t\tself.in_inventory = set(manifest_files.keys()) - missing\n\tfor other in (self, manifest_files):\n\t\tif other.fixity!= \"in_inventory\":\n\t\t\tself.fixity.fixity_block = other.fixity\n\ndef get_"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tmanifest_path = os.path.join(self.directory,'manifest.json')\n\tfile_paths = [\n\t\tpath\n\t\tfor path in util.iter_flat_path(manifest_files, os.path.dirname(manifest_path))\n\t\tif os.path.isfile(path)\n\t]\n\n\tself.fail_if_file_path_is_invalid(file_paths)\n\n\tfor path in file_paths:\n\t\tif os.path.isfile(path) and (\t# Only check files mentioned in manifest"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tif manifest_files is not None:\n\t\tfor filename in manifest_files.keys():\n\t\t\tif filename not in fixity or \\\n\t\t\t\tfilename in self.invalid_filenames:\n\t\t\t\treturn\n\t\tfor filename in self.bad_filenames:\n\t\t\tif filename not in fixity:\n\t\t\t\tself.invalid_filenames.append(filename)\n\t\t\t\traise AnsibleError(\"Invalid filename specified: %s\" % filename)\n\t\tself.bad_filenames = []\n\nclass _AFNixit:\n\t\"\"\"Class which emulates how A"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tfor f in manifest_files:\n\t\ttry:\n\t\t\tself.get_raw_file(f)\n\t\t\tverify_manifest_file(f.name, fixity)\n\t\t\tif not self.is_in_manifest(fixity):\n\t\t\t\tself.print_in_missing(f.name)\n\t\t\t\tsys.exit(1)\n\t\texcept NotYetOmittedError:\n\t\t\tself.print_in_not_found(f.name)\n\t\t\tsys.exit(1)\n\n\tself.print_in_ok"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tself.inventory.validate_fixity(fixity)\n\treferenced_files = self.inventory.get_referenced_files(fixity)\n\tif referenced_files:\n\t\t# validate each file entry referenced by this fixity\n\t\tfor fref in referenced_files:\n\t\t\tself.inventory.validate_fixity(\n\t\t\t\tfref[\"file_entry\"])\n\tif manifest_files:\n\t\treferrers = manifest_files\n\t\twhile referrers:\n\t\t\tfref = self.inventory.get_file_ref(referrers"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tif len(manifest_files) > 0 and isinstance(fixity, fixity_registry.Fixity):\n\t\tif not manifest_files:\n\t\t\treturn False\n\t\tfound = False\n\t\tfor file in manifest_files:\n\t\t\tfound = True # check first\n\t\t\tif fixity.is_file_referenced(file):\n\t\t\t\tfound = False\n\t\tif found:\n\t\t\treturn False\n\treturn True\n\ndef validate_fixity_manifest(self, manifest_files):\t\"\"\"\n\tValidate fixity block in the Manifest block.\n"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t# Read in the fixity block.\n\tfixity_block = fixity.encode_fixity()\n\tfixity_block_bytes = common.int32_to_bytes(fixity_block)\n\n\t# Check that the manifest is well formed.\n\tfirst_char = fixity_block[:common.FIXITY_FILE_LENGTH]\n\tif first_char!= \"FIXITY\":\n\t\traise self.INVALID_FIXITY_BLOCK(fixity_block)\n\n\t# Check that all of the listed files are present.\n\tfor fileinfo in manifest"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tresult = False\n\tfor file_path, file_obj in manifest_files.iteritems():\n\t\tif file_obj is None:\n\t\t\t# File was not loaded at all (ie: is a reference) or is empty\n\t\t\tcontinue\n\n\t\tif self.validate_file(fixity.config_file, file_path, fixity.version, self.file_extension, file_obj):\n\t\t\tresult = True\n\n\treturn result\n\n\n# vim:et:sw=4\n\nimport Vue from \"vue\";\nimport Router from \"vue"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tif fixity is None:\n\t\treturn\t-\t'Error: Fixity block required.'\n\t#print \"checking fixity %s\" % fixity\n\t#print \"---\"\n\tif not repair(fixity):\n\t\treturn\t-\t'Error: Failed to repair inventory. (Not a fixity block?)'\n\t#for name in fixity:\n\t#print \"---\"\n\n\t#\tvalidate that fixity references any files that have been found in the inventory?\n\tfiles = dict(iter_files_in_dir(manifest_files,"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t# Check fixity content for any manifest files to be required.\n\tfor manifestfile in manifest_files:\n\t\tif not is_file_included(manifestfile, fixity.location):\n\t\t\tvalidator.add_error(validator.BAD_MANIFEST,\n\t\t\t\t\"Fixity block in \"+manifestfile+\" does not exist.\")\n\n\t\t# Make sure that only files defined in fixity block are referenced.\n\t\tif hasattr(fixity,'ref_file'):\n\t\t\tfor loc in fixity.references:\n\t\t\t\tif hasattr(loc, 'name"}]}
{"task_id": "62b463153879012d19481498", "prompt": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tfor root, dirs, files in os.walk(path):\n\t\tfor f in files:\n\t\t\tf = os.path.join(root, f)\n\t\t\tif os.path.isdir(f) and not f.endswith('__pycache__'):\n\t\t\t\treturn [f]\n\t\t\telif f.endswith('.py'):\n\t\t\t\tfiles.remove(f)\n\treturn []\ndef _load_source_from_root_file(file_name, path):\n\twith open(file_name) as f:\n\t\tcontent = f."}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\treturn [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n\n# -----------------------------------------------------\ndef get_file_from_path(path, file_path=None):\t\t\"\"\"\n\tReturn the given file from the given path, or None\"\"\"\n\tif file_path is None:  file_path = get_path_from_user()\n\tif len(file_path) == 0: return None\n\t# Check if the file inside `file_path` is a file\n\t# and we"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tpath = os.path.expandvars(path)\n\tpath = os.path.normpath(path)\n\tpath = expanduser(path)\n\tpath = expandvars(path)\n\treturn listdir(path)\n\n// SPDX-License-Identifier: (GPL-2.0 OR BSD-2-Clause)\n/*\n * Copyright (C) 2013-2015 Per Lundqvist <plundq@verdiek-u.com>\n */\n\n#include <linux/export.h>\n\n#include \"net_driver.h\"\n#include"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tf_list = []\n\tfor folder in os.listdir(path):\n\t\tprint(folder+'->')\n\t\tf_list.append(folder)\n\treturn f_list\n\ndef get_text(path):\n\tdef text_split(text, tabsize):\n    tokenizer = json.load(open(path))\n\tfor _, t in tqdm(tokenizer):\n\t\tt.pop(\"text\")\n\t\tt = t.copy()\n\t\tt[\"text\"] = text.replace(\"\\n\",\"\")\n    # remove"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\treturn [ fn for fn in os.listdir(path) if os.path.isfile(fn) and os.path.splitext(fn)[-1].startswith('.' ) ]\n\ndef print_listdir(path):\n\tprint('%5s  %7s  %-5s' % ('  File Name',' File Type','    Size'))\n\tfor fn in files_list(path):\n\t\tprint('%-7s %7s %-5s %-10s' % (fn, filetype(fn), int(r.size("}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\treturn glob.glob(os.path.join(path, '*'))\n\ndef open_folder(path):\t\"\"\"\n\tOpen `path` with the list in `folders`\t\"\"\"\n\tpath = os.path.join(path, '*')\n\tfor f in files_list(path):\n\t\ttry:\n\t\t\tif os.path.exists(f):\n\t\t\t\twith open(f, \"rb\") as file:\n\t\t\t\t\tpass\n\t\t\t\telse:\n\t\t\t\t\traise ValueError(\"folder open error: %s\" % os.path.basename(f))"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\ttry:\n\t\tfiles = os.listdir(path)\n\texcept:\n\t\treturn []\n\treturn sorted(files)\n\t\ndef deletefile(filepath):\n\t# check file path\n\tpathFile = path.split(filepath)\n\tpathFile.pop(pathFile.index(pathFile[0]+\"/\"))\n\treturn pathFile\n\t\n\t\n# def mkdir(path):\n#\t\tpath.mkdir(parents=True, exist_ok=True)\n\n\ndef GetDate():\n    def parsedate(datestr):\n "}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\treturn set([i.name for i in find_files(path) if i.read()])\n\ndef check_and_copy(source, dest):\n\t\"\"\"\n\tRun `source` with `dest` and delete the target if they are different\n\t\"\"\"\n\n\tif not os.path.exists(dest) and not os.path.exists(source):\n\t\t#print(\"Check and copy successful\", source, dest)\n\t\treturn\n\t\n\tif os.path.isdir(source):\n\t\t#print(\"source is a directory\", source, 'checking to"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tdef _print_file(f):\t\t\"\"\"\n\t\t\t\t\tOutput a print statement so the user can\n\t\t\t\t\tput file text in a logfile\n\tParameters\n\t\t\t\tf:\tThe file object which will be printed\n\tReturns\n\t\t\t\tNothing\n\t\"\"\"\n\tprint(\"File:  \"<<f.name<<\">\"+f.buffer.decode()+\"\\n\")\n\ttry:\t\t# Check disk space\n\t\tf.seek(0)\n\t\tf.close()\n\texcept:\n\t\t# Disk not found\n\t\tf = open(\"disk.log"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\treturn os.listdir(path)\n\ndef format_size(input, unit='B'):\n\n\t\"\"\"\n\tGiven a string, convert it to a corresponding byte_string.\n\n\tExamples\n\t--------\n\n\t>>> from cpy2 import str2bytes, bytes2str\n\t>>> str(bytes2str(bytes.fromhex(str2bytes('12345678', unit = 'B')), 8))\n        '12345678B'\n\t\"\"\"\n\treturn bytes2str({'B': input.count(unit)[0]})[unit]\n\n"}]}
{"task_id": "62b463153879012d1948149a", "prompt": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\tfor file in files:\n\t\tdata = parse_manifest_file(file, xmls=xml_filename)\n\t\tfiles[file] = data\n\treturn data\n\ndef _flatten_xml_file(xml_filename):\n\t\"\"\"\n\tFlatten XML files.\n\n\tParameters\n\t----------\nxml_filename : str\n    basename of the XML files\n\n\tReturns\n\t-------\nstr\n    xml file content as str\n\"\"\"\n\tif xml_filename==\"\":\treturn \"\"\n\txmls = []\n\tif os.path.is"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\treturn dict([(file.name, file) for file in\n\t\t\t\tfiles if file.name.endswith('.xml')])\n\ndef check_xml_file(source, xmls):\n\t\"\"\"\n\tReturns info about the given xml files or None if no XML files.\n\n\tNote - If the list of files is empty then this function\n        must consider them valid files.\n\t\"\"\"\n\tresult = None\n\tfor i in range(len(xmls)):\n\t\txml_info = xml_info_from_file(xmls[i]."}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\tfiles = list(itertools.chain.from_iterable(files))\n\tfiles_dict = {}\n\tnames = []\n\tfor xml_file in xmls:\n\t\tnames.append(os.path.basename(xml_file))\n\twhile names:\n\t\tfile = files.pop()\n\t\tif os.path.basename(file) in names:\n\t\t\tfilenames = ['']\n\t\t\tcontinue\n\t\tfilenames = [os.path.basename(file)]\n\t\tfiles.append(file)\n\t\twith open(file, 'rb')"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\txmls = re.findall(\"([\\w\\d_-]+[\\.].xml)\", xml_filename)\n\tfor xml in xmls:\n\t\tfilename = os.path.basename(xml)\n\t\tfiles.append(os.path.basename(xml).strip()[0])\n\tgroups = {}\n\tfor filename in files:\n\t\tfname = os.path.basename(filename).strip()[0]\n\t\tbase_filename = os.path.basename(filename).split('.')[0]\n\t\tfname = os.path.join(xmls[0],"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\txmls = sorted(xmls)\n\tfiles = map(lambda f: f.name.split('.')[0], files)\n\treturn dict([ (f.name,f) for f in files ]) \n\n\ndef _group_files_by_xml_name(source, xmls, files):\t\"\"\"\n\tGroup files by their XML filename\n\nGroups files by their XML filename and returns data in dict format.\n\nParameters\n----------\nxml_name : str\n    XML names\nfiles : list\n    list of files in the folder or zipfile\n\nReturns"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\tpkgFile = os.path.basename(xml_filename)[:-3]+'.xml'\n\tfiles = [os.path.join(folder_path, f) for f in files]\n\tdict = {}\n\txmls = list(map(lambda f: f[0],\n\t\t\t   zip(files, [pkgFile, xml_filename])))\n\tfor xml in xmls:\n\t\tpackage_path = os.path.join(folder_path, xml)\n\t\tpackage_dict = {}\n\t\tif xml in dict:\n\t\t\tpackage_dict[pkg"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\txml_files = dict()\n\tfor xml_filename in xmls:\n\t\txml_files.setdefault(os.path.basename(xml_filename),0)\n\treturn xml_files,files\n\ndef file_exists(filename):\n\treturn os.path.exists(filename)\n\ndef _get_path_for_download(filename):\n\t# return the full path of the archive or a relative path\n\tfilename = os.path.expanduser(filename)\n\ttry:\n\t\treturn os.path.join(get_base_path(),filename)"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\txml_files = []\n\tfiles.sort()\n\tfor file in files:\n\t\tsrc = file.split('.')\n\t\txmlfile = '.'.join(src[1:])\n\t\txmlfile = os.path.splitext(xml_filename)[0] + xmlfile\n\t\txml_files.append( xmlfile )\n\t# Now the XML files in the folder\n\txml_files.sort()\n\txml_files_xml = [ ]\n\tfor xml in xml_files:\n\t\txml_file = ''.join("}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\txml_files = [f.name for f in source.iter_files(exclude_dirs=True) \\\n\t    if f.fname.endswith(xmls)]\n\t\n\t# Extract dict.Dict into a dict to get dictionary.\n\tdict = {x[1]: x for x in xml_files}\n\t# Sort the keys in order to make sure that we don't get multiple files with the same name.\n\tdict = sorted(dict.keys())\n\t# Add extra files.\n\tdict.extend(dict.values())\n\n\treturn {'package': dict"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\twith _Lock(lock_fileobject=os.path.dirname(__file__)) as dir_fo:\n\t\txml_files = os.listdir(dir_fo)\n\t\txml_files.sort()\n\t\t# remove.gz suffixes when filetype = pkg\n\t\txml_files = [ele for ele in xml_files if ele.endswith('_gz')]\n\t\tfor ele in xml_files:\n\t\t\tif _filetype_in_folder(os.path.basename(ele), files):\n\t\t\t\tret_dict[ele] = ele + '."}]}
{"task_id": "62b463153879012d1948149b", "prompt": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tif not file_path.startswith(prefix): return False\n\n\treturn True\n\n\n#-----------------------------------------------------------------------\ndef find_all_file_paths_from_package(package_path):\n\t\"\"\"\n\tRetrieve a list of all files in `package_path` whose name starting with the given `prefix`\n\n\tParameters\n\t----------\n\tpackage_path : str\n\tprefix : str\n\tReturns\n\t-------\n\tlist of str\n\t\"\"\"\n\tresult = []\n\tfor file_name_path in glob.iglob(package_path+'/*') if file_"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\treturn prefix == prefix + \".\" + file_path.split(\".\")[-1].lstrip(\".\")\n\n\ndef match_file_by_name(prefix, file_name):\t\"\"\"\n\tIdentify if a `file_name` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_name : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\treturn str.lower(file_path)[:len(prefix)] == prefix\n\ndef extract_package_file(package_path):\n\t\"\"\"\n\tExtract a file in package `package_path` which is in the package\n\n\tParameters\n\t----------\n\tpackage_path: str\n\t    Package path\n\n\tReturns\n\t-------\n\tfile : file\n\t    File object     \"\"\"\n\tbasename = os.path.basename(package_path)\n\treturn file_operations.extract(os.path.join(package_path, basename))\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tsplit_by_dot = file_path.rsplit('.')\n\treturn len(set(split_by_dot)) in (prefix.rsplit('_'))\n\n\ndef extract(extracted_path, filename):\n\t\"\"\"\n\tExtract the contents of a `file_path` into a string and return the path to the extracted file\n\n\tReturns\n\t-------\n\tstr\n\n\tParameters\n\t----------\n\t[in] extracted_path: [description]\n\t[in] filename: str\n\t[out] [description]\n\t\"\"\"\n\tif extracted_"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tfrom docutils.parsers.rst import document_module\n\tbase, ext = os.path.splitext(file_path)\n\tmodule_name = document_module(base)\n\tif module_name is None:\n\t\treturn False\n\treturn module_name.endswith(ext)\n\n// ------------------------------------------------------------------------------\n// Copyright (c) Microsoft Corporation.  All Rights Reserved.  Licensed under the MIT License.  See License in the project root for license information.\n// ------------------------------------------------------------------------------\n\npackage com.microsoft.graph.generated;\n\nimport com.microsoft.graph.concurrency.*;\nimport"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\twith open(file_path, 'r') as f:\n\t\treturn f.read(min(len(f.read()), len(prefix))) == prefix\n\n/*\n * Copyright (C) 2019 The Android Open Source Project\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tpackage = frappe.get_doc(file_path)\n\tif package and not package.get(\"source\"):\n\t\treturn False\n\treturn True\n\n// Copyright 2009 the Sputnik authors.  All rights reserved.\n// This code is governed by the BSD license found in the LICENSE file.\n\n/**\n* @name: S15.8.2.2_A6;\n* @section: 15.8.2.2;\n* @assertion: Punctuating the characters p1 and p2 in property declaration, but \n*   without the"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tprefix_parts = prefix.split('/')\n\tpkg_type = '.'.join(prefix_parts[:-1])\n\n\treturn file_path.endswith(pkg_type + '.pak')\n\n\ndef file_path_for_package(package, ext):\n\t\"\"\"\n\tPath to file in `package`.\n\n\tReturns\n\t-------\n\tstr\n\t     Path in `package`.\n\n\tExamples\n\t--------\n\t>>> import __main__\n\t>>> path = file_path_for_package(__main__.__test__, '.pyc')\n\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tfile_path = os.path.abspath(file_path)\n\tbase_paths = os.listdir(os.path.abspath(prefix))\n\tfor path in base_paths:\n\t\tif os.path.exists(os.path.join(prefix, path)) and \\\n\t\t\tnot os.path.isfile(os.path.join(prefix, path)) and \\\n\t\t\tos.path.isdir(os.path.join(prefix, path)) and \\\n\t\t\tfnmatch.fnmatch(os.path.relpath(file_path,"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\treturn file_path.startswith(prefix) or file_path.endswith(ext_in_prefix(prefix))\n\ndef ext_in_prefix(file_path):\n\t\"\"\"\n\tGenerar a string para todos os extens\u00f5es principais\n\nRetorna `None`\n-------\nstr\n    Extens\u00f5es que pertetas no blanco estar em strings.\n\nParameters\n----------\nfile_path : str\n    File path\nReturns\n-------\nNone\n    File path with no extention de qualquer do se"}]}
{"task_id": "62b463153879012d1948149c", "prompt": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t", "suggestions": [{"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tret = []\n\tif files.has_key(prefix):\n\t\tret = [files.get(prefix)]\n\treturn ret\n\n\ndef save_all_files(files, path, package=''):\n\t\"\"\"\n\tSave all files `files` relative to `path`.\n\n\tParameters\n\t----------\n\t[in]\n\t[in]\n\tpath:\n\t[in]\n\tpackage:\n\t[in]\n\t\"\"\"\n\tfiles_str = []\n\tfor path in files:\n\t\tfiles_str.append(path._get_"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tfrom frappe.core.doctype.utils import get_docname_prefix\n\tif not files:\n\t\tfrom frappe.core.doctype.utils import get_docname_prefix\n\t\tfiles = get_docname_prefix()\n\telse:\n\t\tfor file in files:\n\t\t\tif file[0]startswith(prefix):\n\t\t\t\treturn file[len(prefix):]\n\t\tif len(files) == 0:\n\t\t\treturn sys.argv[0]\n\ndef filebased():\n\tfrom frappe.utils.filemisc"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tpass\n\ndef select_list_of_filenames(files, pattern):\n\t\"\"\"\n\tGet filenames which match the pattern\n\nRetorna Os arquivos os arquivos dos arquivos dentro dos filtros contidos\n\nParameters\n----------\nfiles : str\n    List of files paths\npattern : str\n    Filename regexp\nReturns\n-------\nlist\n    List of filenames matched by the pattern in a list.\n\nM\u00e9todos\n-----------\n\n* `select_filenames_by_pattern`\n\t  "}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tpatt = '/'.join(prefix.split('/')[:-1])\n\tif abs(patt.split('$')[-1]) == patt.split('/')[-1]:\n\t\treturn prefix.split('/')[-1]\n\telif patt in files:\n\t\treturn True, files\n\telse:\n\t\treturn False, files\n\n\ndef build_from_directories(files, prefix, outdir):\n\t# Remove directories which were not built\n\tmissing_prefix = []\n\tfor f in files:\n\t\tif not any(f.endswith(f) for"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\timport string, sys\n\timport numpy as np\n\timport fnmatch as fnmatch\n\tpat = re.compile(r'^'+prefix+'(\\S+)')\n\tret = []\n\tfiles1_raw = [p.strip('\"') for p in files]\n\tfiles2_raw = [p.rstrip('\"') for p in files]\n\tfiles2_nlist = []\n\tfor f in files2_raw:\n\t\tf_raw = re.sub(r'^'+prefix+'(\\S+).*', '', f)\n\t\tf_n"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\treturn files + globbyprefix(prefix)\n\ndef is_package(package, rootdir):\t\"\"\"\n\tIs the package the root package of the specified `rootfolder`, if it\tsucceeds, return true\nParameters\n----------\npackage : str\n    Name of the package\nrootfolder : str\n    The root folder of the specified `package` `packagedir`\nReturns\n-------\nbool\n    True for root package and false for non root package\"\"\"\n\treturn package in get_root_folders(package, rootdir)\n\ndef is_package_"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tfiles_paths = []\n\tprefix = prefix.lower()\n\tfor f in files:\n\t\tfiles_paths.append(f[prefix:].lower() + '.')\n\treturn files_paths\n\ndef select_directories_by_prefix(prefix, directories):\t\"\"\"\n\tGet directories which belongs to a document package.\n\nRetorna os arquivos da lista `directories` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\ndirectories : str list\n    Directories paths\n"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tif not prefix:\n\t\tret = []\n\telse:\n\t\tret = []\n\t\tfor file in files[::-1]:\n\t\t\tret.append(file.rpartition('/')[0] + '/' + prefix)\n\treturn ret\n\n\n\ndef select_filenames_by_extension(splited_basenames, files):\t\"\"\"\n\tGet files which belong to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com\n`splited_basenames`\n\nParameters\n----------"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\treturn list(\n\t\tmap(\n\t\t\tlambda x: x,\n\t\t\tglob(os.path.join(prefix, '%s*' % x) + os.path.sep)\n\t\t)\n\t)\n\n\ndef select_filenames_by_suffix(prefix, files):\t\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tpattern = re.compile(r'%(srcname)s.*%%(ext)s$')\n\tmatches = []\n\n\tfor f in files:\n\t\text = os.path.splitext(os.path.basename(f))[1]\n\t\tif pattern.match(f) and pattern.match(f.replace('.' + ext, '')):\n\t\t\tmatches.append(f)\n\n\treturn matches\n\n#include \"precomp.hpp\"\n\n#include \"wx/combobox.h\"\n\nnamespace quick {\n    namespace {"}]}
{"task_id": "62b463153879012d1948149d", "prompt": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tflist = os.listdir(folder)\n\tdict = {}\n\n\tfor f in flist: # Get info of file\n\t\tif os.path.isdir(os.path.join(folder, f)):\n\t\t\tdict = {}\n\t\t\tfor subf in os.listdir(os.path.join(folder, f)):\n\t\t\t\tfullname = os.path.join(folder, f, subf)\n\t\t\t\tbasename = os.path.basename(subf)\n\t\t\t\t_explore_folder(fullname)\n\t\t\t\ttry:"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tres = {}\n\n\tfor root, dirs, files in os.walk(folder):  # get all subfolders\n\t\tfor fn in files:\n\t\t\tres[os.path.basename(os.path.normpath(fn))] = dirs\n\treturn res\n\n// Licensed to the Apache Software Foundation (ASF) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The ASF licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\twith open(folder, 'r') as file:\n\t\tdata = dict(xml_path_to_dict(file))\n\n\t\t# Remove.zip extension\n\t\tdel data['info']['name']\n\t\tdel data['package']\n\n\t\treturn data\n\ndef _process(package, file_group):\n\t\"\"\"\n\tProcess a package\n\n\tParameters\n\t----------\n\tpackage : str\n\tfile_group : dict\n\t\tContains folder name to package group key\n\n\tReturns\n\t-------\n\tNone\n\t\"\"\"\n\tfor filename in"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tlogger.info('Retrieving data from {} folder'.format(folder))\n\tfolder = _clean_folder(folder)\n\tres = []\n\tfor path in os.listdir(folder):\n\t\tfilename = os.path.join(folder, path)\n\t\tf = open(os.path.join(folder, path), 'r')\n\t\tres.append(get_json_file(f, filename, folder))\n\t\tf.close()\n\t\tif os.path.isdir(os.path.join(folder, path)):\n\t\t"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tfor filepath in Path(folder).rglob(\"*.xml\"):\n\t\t# Open the xml file\n\t\twith open(filepath) as f:\n\t\t\t# Get the data\n\t\t\tdata = f.read().strip()\n\n\t\t# Get the groups names\n\t\tgroups = data.strip().split(\"\\n\")\n\n\t\t# Return groups data\n\t\treturn dict(zip(groups[0::2], groups[2::2]))\n\npackage cmd\n\nimport (\n\t\"github.com/jenkins-zh/jenkins-cli/app/i"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tp1,p5,p9,p10 = {}, {}, {}, {}\n\tp7,p8,p11,p12 = {}, {}, {}\n\tinifilename = os.path.join(folder, 'inifile.ini')\n\tinifile_raw = open(_get_package_file_in_folder(inifilename),'r')\n\tlines = inifile_raw.readlines()\n\tinifile_raw.close()\n\tinifilename = os.path.join(__dirname,'inifile_raw.ini')\n\t"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tf = open(folder + \"META-INF/MANIFEST.MF\", 'rb')\n\tm = xml.dom.minidom.parse(f)\n\t# Create the dict of all the root elements\n\troot = {}\n\tfor key in m[\"manifest\"]:\n\t\tm5 = m[\"manifest\"][key].strip()\n\t\tif m5[0:4] == \"<?xml\" or m5[:4].lower() == \"<?xml\":\n\t\t\tcontinue\n\t\troot[key] = m5\n\tclose(m)\n\treturn root"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tdata = {}\n\tfile = open(_path(folder, 'package.ini'))\n\tfor name in file.read().splitlines():\t\t# read each object\n\t\tpath = os.path.join(folder, name.strip('\"'))\n\t\tif os.path.isfile(name):\t\t\t# If folder is regular file\n\t\t\tdata[\n\t\t\t\tname if (not os.path.isroot(name)):\n\t\t\t\tpath[-4:] + name + '.py',\n\t\t\t\tpath\n\t\t\t] = 1\n\tfile.close()\n\treturn"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tif 'groups.xml' not in os.listdir(folder):\n\t\treturn {}\n\txml2dict = {'packages': {}, 'dependencies': {}}\n\tfor file in os.listdir(folder):\n\t\tif os.path.split(file)[-1][-1] == 'groups.xml':\n\t\t\tfilepath = os.path.join(folder, file)\n\t\t\tfile = open(filepath, 'r').read()\n\t\t\ttry:\n\t\t\t\txml.dom.minidom.parseString(file)\n\t\t\texcept SyntaxError:\n"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tdf = pd.read_xml(folder+'/Package.xml')\n\treturn {'Package': ['Item ID', 'Name', 'Package Type', 'Version']: df.info()['Item'].keys()}\n\n\ndef _package(target, target_version, xml_basename, base_folder):\n\t\"\"\"\n\tBuilds a dictionary of required items\n\n\tArguments\n\t----------\n\ttarget\t\tstr\n\ttarget_version\tstr\n\txml_basename\t\tstr\n\tbase_folder\t\tstr\n\tReturns\n\t-------\n\tPackage object\"\"\"\n\tbase_"}]}
{"task_id": "62b463153879012d1948149f", "prompt": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tpacote = {}\n\tfor f in os.listdir(os.path.join(file_folder, prefix, file_path)):\n\t\tif \"_asset\" not in f:\n\t\t\tcontinue\n\t\to = os.path.join(file_folder, prefix, file_path, f)\n\t\tif \"_rendition\" in f:\n\t\t\to = os.path.join(file_folder, prefix, file_path, f, \"_rendition\")\n\t\to_tmp = os.path.join(prefix, file_path)\n\t"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tif not file_path.endswith('.pkl'):\n\t\treturn {}\n\n\tf = open(file_path)\n\t_data = f.read()\n\t_data = _data[:-1] # remove BOM\n\n\t_id1 = None\n\t_id2 = None\n\n\tm = re.search('name = \\'([^\\'\" ]+\\.(?:ext|firm|pkl))\\'', _data)\n\tif m is not None:\n\t\t_id1 = m.group(1)\n\telse:\n\t\tm = re."}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\txml = open(file_path, 'r')\n\tp = xml.read().split('}')\n\n\tpackages = {}\n\tfor x in p[:int(len(p) / 6)]:\n\t\tp2, p3, p4, p5 = x.split('/')\n\t\tp2 = p2[1:]\n\t\tif p2 == 'packages':\n\t\t\tpackages = p4\n\t\telif p2 =='rendition':\n\t\t\tpackages.append(\"rendition\")\n\t\telif p2 not in packages:\n\t\t\tpackages."}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tresult = {}\n\t\n\tfor file_name in os.listdir(file_folder):\n\t\tfile_path = os.path.join(file_folder, file_name)\n\t\tdir_name  = os.path.join(prefix, file_path)\n\t\t\n\t\tresult[dir_name] = {}\n\t\t\n\t\tif os.path.isdir(dir_name):\n\t\t\t_list = os.listdir(dir_name)\n\t\t\tif file_name in _list: result[dir_name][file_name] = True\n"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\ttmp_file = pkg_resources.resource_filename('plotly.io',\n\t\tprefix + file_path)\n\treturn parse_data_and_get_metadata_in_file(tmp_file, pkg_resources)\n\ndef _get_graph_file_path(graph_filename):\n\t\"\"\"\n\tSeleciona o nome do arquivo XML do arquivo do visualiza\u00e7\u00e3o em tempo de\n\trenderiza\u00e7\u00f5es.\n\n\t\"\"\"\n\trender_filename = graph_filename.replace('%GraphMetadata.xml', '')"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\treturn prefix_xml_dict(prefix, None, None, prefix_asset_dict(prefix,None,file_path,None,file_folder) )\n\ndef _eval_file_dict(prefix,filename,file_folder):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tif file_path.endswith('.xml'):\n\t\tparser = etree.parse(file_path)\n\t\t# return {'package' : {'packages': [package_dict]}\n\t\t#        'rendition': {'renditions': [rendition_dict]}\n\t\treturn {'package' : {'packages': [package_dict],\n\t\t                    'renditions': [rendition_dict]}\n\t\t              "}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tfrom isces.asset.abstract.AbstractAssetDB import AbstractAssetDB\n\tprefix = AbstractAssetDB(prefix).get_prefix()\n\tresult = prefix\n\troot_path =  os.path.split(file_path)[0]\n\tif root_path!= '' and os.path.exists(root_path):\n\t\tfile_folder = os.path.split(file_path)[1]\n\t\tif os.path.exists(os.path.join(root_path, file_folder + '/assets','rendition')) or \\\n\t\t\tos"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tif not os.path.exists(os.path.join(dir_path, file_folder, prefix)): # no esquema\n\t\tpkg = {}\n\t\tdir = os.listdir(file_folder)[0] # certeza de recurso\n\t\tfor f in os.listdir(dir):\n\t\t\tfpath = os.path.join(dir, f)\n\t\t\tif f.endswith('.xml') and is_rendition_file(fpath): # com o formato XML do arquivo do pacote?\n\t\t\t\t"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\treturn util_func.parse(file_path, self.file_folder, prefix)\n\n##\n# Return data\n# -----------------------------------------------------------------------------\n\n\n# def _fetch_asset(self, asset):\n#     asset_data = asset.data\n#     asset_id = asset.asset_id\n#     asset_name = asset_data[0]\n#     asset_data_format = asset_data[1]\n#     asset_info = asset_data[2]\n\n#   "}]}
{"task_id": "62b463153879012d194814a1", "prompt": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\timport os\n\n\tdir_path = tempfile.gettempdir()\n\tfname = \"papers_en.pdf\"\n\tself._save(lang, os.path.join(dir_path, fname), file_path)\n\n\tself.__check_for_papers(lang)\n\tself.remove_all_papers()\n\n\tos.rename(\n\t\tos.path.join(dir_path, fname),\n\t\tos.path.join(dir_path, \"papers_en.pdf\")\n\t)\n\n\tself.remove"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tfile_path = os.path.abspath(file_path)\n\t# TODO: Use only one language.\n\n\t# Make a dictionary of pages to translate.  Also return the full\n\t# path of the resulting PDF (for embedding in PDF via URL).\n\tpages_to_translate = []\n\twith open(file_path + \"_en.pdf\", \"rb\") as f_file:\n\t\tfile_bytes = f_file.read()\n\t\tdoc = pdfmetrics.make_pdf(file_bytes)\n\n\t\tfor page_num in range("}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tself._checkForIncorrectParams()\n\n\tif not file_path.endswith(\".pdf\"):\n\t\tself.log_error(\"PDF files are not supported. Please install the PDF library and run the script %s\" % file_path)\n\t\tself.exit()\n\n\timage = _image_factory(file_path)\n\twidth_inches = self._convert_dpi_to_inches_width(image.width)\n\theight_inches = self._convert_dpi_to_inches_height(image.height)\n\n\tself.log_"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tif not os.path.exists(file_path) or self.verbose:\n        self.file_path = file_path\n        self.out = open('{}/{}.{}'.format(file_path,lang,self.lang),'w+') \n    #\n    # \u88c5\u5165\u8bed\u8a00\n    #\n    #\n    @property\n    def lang(self):   \n        return self._lang    "}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tif lang == \"de-DE\":\n\t\t\tprint(\"Sie haben dich genau, er sie indem das ersten beides k\u00f6nnen.\")\n\t\telse:\n\t\t\tfor s_lang in lang_list:\n\t\t\t\tif s_lang == lang:\n\t\t\t\t\tprint('Zur\u00fcck ge\u00e4ndert das arbeitet: %s' % os.path.split(file_path)[-1])\n\t\t\treturn\n\n\n@timeit\ndef get_path_from_file_paths(path, lang):"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tdef extract_pdf(self):\n\t\tfile_name = os.path.join(root, \"original\", \"_\".join([lang] + [file_name]))\n\t\tfile_path = os.path.join(root, \"en\", file_name)\n\t\twith open(file_path, 'r+b') as new_file:\n\t\t\twith open(file_path, 'rb', encoding='utf-8') as source_file:\n\t\t\t\tdata = source_file.read()\n\t\t\t\tdocument = Pdf.Pdf()\n\t\t\t\tdocument."}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tlogger.info(\"Adding rendition file: %s to: %s\", lang, file_path)\n\tself.pdf_files.append(file_path)\n\n\tself.add_pdf_page(lang)\n\n\nclass Artigo02Generator(GeneratorBase):\n\t@classmethod\n\tdef get_language_by_name(cls, name):\n\t\t\"\"\"\n\t\t        :name: language name\n\t\t\"\"\"\n\t\tname = normalize_language_name(name)\n\n\t\tlanguage = languages.Language.all()[0]"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\twith open(file_path, \"rb\") as f:\n\t\tfor i, line in enumerate(f):\n\t\t\tline_split = line.split(\",\",1)\n\t\t\tif(line_split[0] == lang):\n\t\t\t\tline = line[1:len(lang),:len(line_split)]\n\t\t\t\tbreak\n\n\twith open(file_path, \"wb\") as f:\n\t\tf.write(b''.join([chr(0x41) + i*13 + '|' + s + '|' for i, s in enumerate("}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\treturn __add_pdf(self._path, self.name+\"_Rendition-\"+lang, file_path)\n\ndef add_web(self, lang, file_path):\t\"\"\"\n\turl = \"http://www.artigo.net/artigo02-en.pdf\"\n\t{\n\t\"original\": \"artigo02.pdf\",\n\t\"en\": \"artigo02-en.pdf\",\n}\n\t\t{\n\t\"<page\": {\n\t\t\"layout_text\": \"\",\n\t\t\"language\": \"en\",\n\t\t\"content\":"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tself.create_document(title=\"rendition.pdf\")\n\tself.file_to_pdf(file_path)\n\tself.file_to_pdf([os.path.join(self.workbook_root, self.metadata[\"metadata-set\"])])\n\tself.create_document(title=\"meta.pdf\")\n\tself.file_to_pdf(file_path, lang=lang)\n\n\n##\tdef add_image(self, content_type, src, destination=None,\n##\t\twidth=None, height=None, is_thumbnail"}]}
{"task_id": "62b463163879012d194814a2", "prompt": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tfile_path += \".tiff\"\n\tfile_path += \".jpg\"\n\tfile_path += \".png\"\n\tfile_path += basename\n\treturn file_path\n\n\n\n#\t@unittest.skip(\"not implemented\")\n#\t@unittest.expectedFailure\n#\tdef test_load2_bkgd(self):\n#\t\t\"\"\"test_load2\"\"\"\n#\n#\t\tpass\n\n\"use strict\";\n\nwindow.localStorage.setItem(\"language\", \"en-US\");\nwindow.localStorage.setItem(\"language/code\", \"de"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t# get the file path and the basename\n\tfile_path = os.path.join(self.directory,file_path)\n\tbasename = file_path.split(\"\\\\\")[-1]\n\t# if the file exists already (and is a local texture) then it's probably something else.\n\t# TODO do we need to check first if it's a local texture or not?\n\n\t# ensure the file really exists now so it's added to the cache as a new zip\n\t# file is opened in readonly mode so we have to open the file in append mode before appending;\n\t"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tassert isinstance(basename, str), \"File basename must be a string, not %s\" % type(basename)\n\tf = open(file_path, \"rb\")\n\tim = Image.open(f)\n\tf.close()\n\tdel im\n\n\tasset_list = [basename, image.filename, file_path]\n\tself.asset_list = asset_list\n\n\tself.image_format = \"fits\"\n\tself.width = im.width\n\tself.height = im.height\n\n\ndef create_directory(target, force"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tself.assets[basename] = (file_path, )\n\n\twith open(file_path, 'r+') as f:\n\t\tfor line in f:\n\t\t\tif line.strip() == '!':\n\t\t\t\tself.artigos = []\n\t\t\telif len(line) < 2:\n\t\t\t\tcontinue\n\t\t\tline = line.strip()[1:].split(':')\n\t\t\tfile_info = line[0].strip().lower().strip('*')\n\t\t\tif file_info == 'artigo02-gf03.tiff':\n\t\t\t"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tif file_path:\n\t\tself.file_path_to_index[basename] = file_path\n\n\tif basename in [\n\t\t\"artigo02-gf03.tiff\",\n\t\t\"artigo02-gf03.jpg\",\n\t\t\"artigo02-gf03.png\",\n\t]:\n\t\tfrom artigo_datasets import imagenet_utils\n\t\tself.imagenet_utils.extract_single_art_file(\n\t\t\tself.file_path_to_index[basename],\n\t\t\tbasename,\n\t\t\tfile"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\twith open(basename) as f:\n\t\tcontent = f.read()\n\t\t\n\t\tcontent = content.replace('<base64>', base64.b64decode(urllib2.urlopen(file_path).read()))\n\n\t\tcontent = content.replace('\\'\\\\', '\\\\').replace('\\\"', '\"')\n\n\t\tcontent = re.sub(\"https://artigo02.com/[a-zA-Z0-9\\\\-\\\\.\\\\%]*\", \"http://artigo2.m.artigo.net/artigo02-gf03.png\")\n\n\t"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tif self._asset_name is not None:\n\t\tif basename == self._asset_name:\treturn\tTrue\t\t# Same asset\n\t\tasset = self._ar2cgf.add_asset_file(self._asset_name)\n\t\tself._asset_name = None\t# Unsave the asset\n\t\tself._asset_fname = basename\n\t\treturn\tTrue\t\t\t\t# New asset found\n\telse:\n\t\tself._asset_name = basename\n\t\tself._asset_fname = file_path\n\t\tself._ar2cgf"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t# get the name and the directory to search for the desired file\n\tname = basename\n\ttry:\n\t\tpath = file_path.split(\"/\")[-1] + \"/\" + basename.split(\"/\")[-2]\n\t\tdirname = os.path.basename(path.split(\"/\"))\n\texcept:\n\t\tdirname = basename\n\tdirectorypath = os.path.dirname(file_path)\n\n\tself.asset_path = os.path.join(directorypath, dirname, name)\n\n\t# append additional directories if necessary\n\tfor i in range(\n\t\t"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tif not os.path.exists(file_path):\t\t\t\t\n\t\t\t\twith open(os.path.join(self.rootdir,'assets','assets.json'), 'r') as myfile:\t\t\t\t\t\t\n\t\t\t\t\t\tmyfile.close()\n\n\t\t\tself.assets.append({\n\t\t\t\t'basename': basename,\n\t\t\t\t'file_path_prefix': self.rootdir +basename,\n\t\t\t\t'file_path': os.path.join( self.rootdir, basename )\n\n\t\t\t})\n\n\n\tdef remove_asset(self, basename):\n\t"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tself.logger.debug('Add asset \"{0}\": path=[\"{1}\"]'.format(basename, file_path))\n\tself.asset_db[basename] = file_path\n\tself.art_root = file_path.replace(dirname(self.root_path), \"\")\n\treturn self.asset_db[basename]\n\t\n\t\ndef add_dir_from_str(self, dir, name):\n\t\"\"\"\n\tfile_name = dirname + \"/\" + filename[1:-2]\n\t\"\"\"\n\tself.logger.debug('Add dir from str"}]}
{"task_id": "62b463163879012d194814a4", "prompt": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\ttry:\n\t\tzip = zipfile.ZipFile(zip_path)\n\t\treturn zip.namelist()\n\texcept zipfile.Error:\n\t\treturn None\n\n\ndef _sort(pkg_list):\n\t\"\"\" Sort a list of package information by name\n\n\t   The order of package name list is a little more economical\n\tand is not defined in Zauri API\n\n\t   Each package names list must be given name in string format (name < package1 <\n\tother_package1 < package2 < other_package2 <... other"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\tzip_filename = zipfile.ZipFile(zip_path, 'r')\n\tfiles = glob.glob(zip_filename.name[1:] + '*.xml')\n\tpackages = dict()\n\tfor file in tqdm(files):\n\t\tpackage = dict()\n\t\tpackage[file] = open(file, 'r').read()\n\t\tpackages[str(package['package_id'])] = package\n\treturn packages\n\npackage_id_dict = {\n\t'1b30de2b.zip': ('1be7b2fd-8c"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\tzip_content, = open(zip_path, 'rb')\n\tzip_reader = ZipReader(zip_content)\n\tpackages = []\n\tfor file in zip_reader.namelist():\n\t\tpkg_name, = file.split('/')\n\t\tpackages.append(pkg_name)\n\topen(zip_path + os.sep + 'package.xml', 'w').write(zip_content.read())\n\treturn packages\n\ndef _extract_package_xml(zip_path, xml_path):\t\t\"\"\"\n\tExtract files from zip_path"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\treturn zipfile.ZipFile(zip_path, 'r')\n\n\ndef _finding_manifest(manifest_filename, manifest_filename_start=False):\t# find manifest on this file\n\tmanifest_start = None\n\tmanifest_index = 0\n\tif manifest_filename_start == True:\n\t\tmanifest_filename = 'test-manifest-' + str(manifest_index) + '.xml'\n\tfor file in glob.glob(manifest_filename):\n\t\tif (manifest_start is None) and (not file.endswith('yml')) and (not file."}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\timport zipfile\n\tfhandle = zipfile.ZipFile(zip_path)\n\t\t\n\tfor fname, dirnames, filenames in fhandle.infolist():\n\t\tfor filename in filenames:\n\t\t\tfiledata = parse_zipinfo(fhandle, fhandle.getinfo(filename))\n\t\t\tfilename = os.path.relpath(filename, os.path.abspath('./'))\n\t\t\tdata[filename] = filedata\n\t\t\t\n\treturn data\n\ndef parse_zipinfo(zinfo, finfo):\n\t\"\"\"\n\tParse archive info of zip file by zipinfo."}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t# get xml file of each file in zip file\n    with zipfile.ZipFile(zip_path, 'r') as zipf:\n\t# get packages' files content from zip file\n        packages = zipf.infolist()\n\t# loop through groups in each file\n\tfor name in packages:\t\n\t\t# get xml info for file\n\t\ttry:\n\t\t\tfile_xml = open(name, \"r\").read() \n\t\texcept IOError as e:\t\t# No file, no xml : continue\n\t\t\tpass"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\tf = zipfile.ZipFile(zip_path)\n\tfor item in f.infolist():\t# Extract the data in form of dict\n\t\tif \".xml\" in item.filename:\n\t\t\tcontinue\n\t\ttry:\n\t\t\tdata = f.open(item.filepath).read()\n\t\t\ttmpdict = dict()\n\t\t\ttmpdict['package_name'] = item.pathname[:item.pathname.index(zip_path)]\n\t\t\ttmpdict['package_version'] = item.filename[:item.filename.index(zip_path)]\n\t\t\t"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\tfilename = os.path.basename(zip_path)\n\tzip_file = zipfile.ZipFile(zip_path)\n\tnames_dict = {}\n\tfor zipfile.ZipFile.NameHeader(zip_file.namelist()):\n\t\ttar_basename = zipfile.ZipFile.GetBasename(zip_file.namelist()[0])\n\t\tnames_dict[tar_basename] = zipfile.ZipFile.GetComment(zip_file.namelist()[0])\n\treturn names_dict\n\ndef _load_dir(zip_path, path='/'):"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\tzip_basename = os.path.basename(zip_path)\n\tfor d in os.listdir(zip_path):\n\t\tif '.zip' not in d:\n\t\t\tcontinue\n\t\tsub_basename1 = os.path.splitext(zip_basename)[0]\n\t\tsub_basename2 = os.path.splitext(d)[0]\n\t\tif os.path.splitext(d)[1]!= sub_basename1: continue\n\t\tif sub_basename1!= sub_basename2: continue\n\t\tif os.path."}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\tfrom xml.etree.ElementTree import ParseError\n\tfrom xml.parsers.expat import ExpatError\n\timport re\n\timport os\n\tfrom io import open\n\ttry:\n\t\tzip_file = zipfile.ZipFile(open(zip_path, \"r\"))\n\texcept:\n\t\treturn {\"err\": \"Cannot open zip file.\"}\n\t\traise ExpatError\n\ttry:\n\t\tzip_file.extractall(os.path.join(\"*\"))\n\t\treturn {\"err\": \"Zip file is corrupted. Extracted to data dict.\""}]}
{"task_id": "62b463163879012d194814a6", "prompt": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 34.00 MiB (GPU 0; 22.16 GiB total capacity; 20.82 GiB already allocated; 5.88 MiB free; 21.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b4631b3879012d194814dd", "prompt": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tcontent = content.replace('w:', '')\n\tcontent = content.replace('w:st', 'w-st')\n\tfor line in content.split(b'\\0'):\n\t\tif line and line.startswith(b'w:'):\n\t\t\tline = line.replace(b'w:', b'w-')\n\t\tline = line.strip()\n\t\tif not line: break\n\t\tif not line[0] == b'w' and line[0]!= b'':\n\t\t\tline = b'start w:st=\"' + line\n"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tfor k, v in content.items():\n\t\tif k.startswith('w:'):\n\t\t\tv = 'w-st=\"'\n\treturn content\n\n\ndef format_string_as_xml(s):\n\t\"\"\"\n\tCreate an XML string from an argument using to_xml.\n\tS is str object type\n\t\"\"\"\n\ts = re.sub('/>', '>', s)  # xml close tag < >\n\ts = re.sub('<w:(.*)','w:$1', s)\n\ts = re.sub('xmlns=\"','xmlns"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tnamespace_prefix = content.split('\"', 1)[0].split('\"', 0)[1]\n\tcontent = \"\"\n\tfor name in namespace_prefix.split('\"', 1)[1]:\n\t\tnextchar = content.split('\"', 1)[0]\n\t\tif'' + nextchar in _namespaces:\n\t\t\tnamespace_prefix_idx = nextchar.index(nextchar.split('\"', 1)[0]) + 1\n\t\t\tcontent = content.replace(namespace_prefix[namespace_prefix_idx], '\"%s\"' % name)\n\t\telse:\n\t\t"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t# TODO: this may make the parser non-contiguous, so don't use the reverse\n\t# operator.\n\tfor line in content:\n\t\tif not line.startswith('w:st=\"') and not line.startswith('w-st=\"'):\n\t\t\tline = 'w:'+line\n\t\t\treturn \"\\t\".join(map(lambda x: \"\\t\".join(x.split(':')), [line])), 1\n\ndef fix_namespace_uri(uri):\n\t\"\"\"\n\tFix the URI. This is a little complicated because of problems for URLs that\n\tthe parser"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tcontent = content.replace('w:', 'w-')\n\treturn content\n\npackage com.planet_ink.coffee_mud.Abilities.Songs;\nimport com.planet_ink.coffee_mud.core.interfaces.*;\nimport com.planet_ink.coffee_mud.core.*;\nimport com.planet_ink.coffee_mud.core.collections.*;\nimport com.planet_ink.coffee_mud.Abilities.interfaces.*;\nimport com.planet_ink.coffee_mud.Areas.interfaces.*;\nimport com.planet_ink.coffee_mud.Behaviors."}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tnew_prefix = ''\n\tfor start in ('{','}','<','>',\"''\"):\n\t\tif content[start] == '}':\n\t\t\tnew_prefix += content[start:]\n\tcontent = new_prefix\n\treturn content\n\ndef is_url_protocol(content):\n\t\"\"\"\n\t@param content: Text to check if is a url protocol\n\t@return\n\t\tTrue if this is a url protocol.\n\t\"\"\"\n\tfor start in ('', \"'\", '\"', \"'\", \"\\n\", \"\\r\", \"'s\", \"'t\\r\", \"'e"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tfor part in content:\n\t\ttext = part.strip(\"\\r\\n\")\n\t\twhile part.isspace():\t\t# Remove leading spaces in text by skipping leading and trailing spaces in prefix.\n\t\t\tpart = part.lstrip(\"\\r\\n\")\n\t\t\ttext= text.lstrip(parts)\n\t\t\tif part.find(\"xmlns:w\") > -1:\t# Only process if there are more than x='w'\n\t\t\t\tpart = part[:part.find(\"w='\")]\t# No more than one 'w='\n\t\t\ttext= text."}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\treturn fix_namespace_prefix(content).replace('w:', 'w:st-')\n\n\ndef fix_namespace_prefix(content):\n    \"\"\"\n    Convert text that defaults to 'w:t=\"' to 'w-tag=\"'\n    \"\"\"\n    content = re.sub(r'[\\n\\r]+', r'\\g<w:(st-)?', content)\t# remove empty lines\n    content = re.sub(r'[\\n\\r]+', r'\\g<w-(t-)?', content)\t#"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tif content.startswith('w:t='):\n\t\tcontent = content[2:]\n\tif not content or isinstance(content, text_type):\n\t\treturn text_type(content)\n\treturn content\n\n\n@add_metaclass(SoupStrainerMetaclass)\nclass SoupStrainer(BaseStrainer):\n\tdef _get_name(self):\n\t\treturn self._name\n\n\tdef parse(self, soup):\n\t\t\"\"\"\n\t\tReturn the results of the parse_html method as strings.\n\n\t\tThis is done"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tcontent = content.replace('  ', '~')\n\tcontent = content.replace('.', '~')\n\t# convert from 'w:\" to 'w:\"~'\n\tcontent = content.replace(':', '~')\n\t# convert from 'w:f=\"...\" to 'w-f:\"...\"~'\n\tcontent = content.replace(':', '~~')\n\t# convert 'w:f=\"...\" to 'w:f=\"...\"~'\n\tcontent = content.replace('~', '...')\n\treturn content\n\ndef fix_namespace_prefix(content):\n\t\"\"\"\n"}]}
{"task_id": "62b463283879012d1948153d", "prompt": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\txpath = \".//item\"\n\txpath = \"{0}//item\".format(xpath)\n\n\t#print(node.xpath(xpath))\n\n\t# The xpath in node will return nodename when you search for that node\n\ttry:\n\t\tfor item in node.xpath(xpath):\n\t\t\tfor xpath in pubdate_xpaths:\n\t\t\t\tif item.xpath(xpath):\n\t\t\t\t\t#print(\"Found\")\n\t\t\t\t\treturn item\n\t\t\t\telse: \n\t\t\t\t\t#print(\"Unmatched\")\n\t\t\t\t\tdel item\n\texcept:\n\t\treturn"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tif node.parent == None:\n\t\treturn False\n\tfor root, _, _ in pubdate_xpaths:\n\t\tif (\n\t\t\t\txml.isDescendant(root, node.parent)\n\t\t\t\tand xml.isAttributeValue(root, node.name, \"pubyear\")\n\t\t\t\tand xml.isAttributeValue(root, node.name, \"pubyear\")\n\t\t\t):\n\t\t\t\treturn True\n\treturn False\n\n\ndef add_metadata_keys(d, keys):\n\t\"\"\"helper function\"\"\"\n\tif len(keys) == 0:\n\t"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tif len(node.getchildren()) < 2:\t\t\t\t\t\t\t\treturn None\n\tif node.getchildren()[0].text!= \"xmlns:\":\t\t\t\t\treturn None\n\tif node.getchildren()[1].text is None:\t\t\t\t\t\t\treturn None\n\tnode_parent = node.getparent()\n\tif not node_parent.text == \"\" and node_parent.text!= node.getchildren()[0].text: return None\n\tfor elem in node.getchildren()[2:]:\n\t\tif elem.text == \"pubdate\":\n\t\t\tif elem.getchildren()[0]."}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tfor n in pubdate_xpaths:\n\t\tif n.xpath(node.get(\"uri\")) or n.xpath('./') or n.xpath('.//ns:publisher') or n.xpath('//ns:publish')):\n\t\t\t# print \"found n match\"\n\t\t\treturn n\n\t\telse:\n\t\t\t# print \"\"\n\t\telse:\n\t\t\tprint \"\\t\\t\" + n.toxml()#\" should have already been removed\n\t\t\t#print \"\"\n\n\treturn False\n\ndef match_publish_date(node, pubdate_x"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t# First time in list\n\tif node is None:\n\t\treturn None\n\t# First date in list\n\tnode = node[0]\n\tfor xpath in pubdate_xpath.xpath(node.getnext()):\n\t\tif xpath.text == str(pubdate_xpaths[0]):\n\t\t\treturn node\n\t# If can't find in list\n\tfor xpath in pubdate_no_xpath.xpath(node.getnext()):\n\t\tif xpath.text == str(pubdate_xpaths[0]):\n\t\t\treturn node\n\t# Default date"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tfor i in range(len(pubdate_xpaths)):\n\t\tif all(p in xpaths for xpaths in pubdate_xpaths[i].values() if xpaths):\n\t\t\treturn i\n\ndef _clean_datetime(dt):\t\"\"\"\n\tInternal function used to clean the datetime.\n\t\"\"\"\n\treturn dt.replace(microsecond=0).isoformat()\n\ndef clean_datetime(dt):\t# pragma: no cover\n\t\"\"\"\n\tClean time.\n\n\tParameters:\n\t\tdt: datetime.datetime data type\n\tReturns:"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tif node and not len(node.findall(xpath_node))!= 0: return None\n\tnow = datetime.now()\n\tfor xpath in pubdate_xpaths:\n\t\tfor pubdate in xpath.iterchildren(now):\n\t\t\tif pubdate.data!= now.strftime(pubdate.data): continue\n\t\t\treturn xpath\n\treturn None\n\ndef match_publisher(node):\treturn None\n\ndef match_node(node):\n\tif node and node.tag!= \"publisher\": return None\n\treturn node\n\t\ndef match_content("}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tprint(\"Starting match of node %s at xpath %s %s\" % (node, pubdate_xpaths, node))\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tprint(\"Matching pubdate %s to xpath %s\" % (pubdate_xpath, pubdate_xpath))\n\t\tmatches = False\n\t\tif node.is_element:\n\t\t\tmatches = node.is_element\n\t\telse:\n\t\t\tmatches = node.text.lower() == pubdate_xpath.lower()\n\t\tif matches:\n\t\t\treturn"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tfor idx,xpath in enumerate(pubdate_xpaths):\n\t\tif not(match_xpath(node, xpath):\n\t\t\treturn idx\n\treturn 0\n\ndef match_datetime(node, datetime_xpaths):\t\"\"\"\n\tReturns the first match in the datetime_xpaths list\t\"\"\"\n\tfor idx,xpath in enumerate(datetime_xpaths):\n\t\tif not(match_xpath(node, xpath):\n\t\t\treturn idx\n\treturn 0\n\n# --------------------- Main Node (XML-Tag) Processing ----------------------\n##        "}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tfor xpath, _ in pubdate_xpaths:\n\t\tres = (node.xpath(xpath).extract() or None)\n\t\tif res: return res\n\n\traise ET.ElementTraversalException('XML namespace tag not supported.  Please use the xpath functions under the doc/lib/xml-node-utils package.')\n\n\ndef match_datetime(node, datetime_xpaths):\t\"\"\"\n\tReturns the first match in the datetime_xpaths list\t\"\"\"\n\tfor xpath, value in datetime_xpaths:\n\t\tres = (node.xpath("}]}
{"task_id": "62b463303879012d19481579", "prompt": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\treturn {'number': issue.text,'suppl' :issue.get_attr_by_name('supplementalAttributes:count')}\t# Use regex instead of count() to avoid the exception being thrown in case of invalid format.\n\ndef _extract_date_from_issue_element(issue):\n\tdate_obj = datetime.strptime(issue.get_attr_by_name(\"date\") or issue.get_text(), '%m/%d/%Y %I:%M.%p')\t# Remove the day-of-month date prefix (e.g. 20180126)"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tnumber = None\t# set by the superclass\n\tsuppl = None\t# set by the superclass\n\n\t# extract the number\n\tif \"number\" in issue.xpath(\"//ul[@id='d7']/li\"):\tnumber = issue.xpath(\"//ul[@id='d7']/li/h3/a\").get(\"title\", \"no number\")\n\n\t# extract the suppl\n\ttry:\tsuppl = issue.xpath(\"//ul[@id='d8']/li/strong[@class='s suppl']/following-sibling::ul[2]/following-"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tsuppl = issue.suppl()\n\t\n\tnumber = utils.remove_comments(issue.find(r'''\n\t\t# <supplant numbers=\"1\">\n\t\t\\s*</supplant>\n\t'''\n\t).text), unimodn = utils.remove_comments(issue.find(r'''\n\t\tnum_m.\"\"\n\t''').text)\n\t\n\treturn number, suppl\n\n@utils.multicase()\ndef _find_issue_elements(issue):\t\"\"\"\n\tUse the issue element's find function to"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tissue_number = None\n\tfor element in issue.findall(f'*[@numid=\"{$issue_number}\"]'):\n\t\tif element.has_attr(f'numId') and element.has_attr(f'numDesc'):\n\t\t\tnumber = element.get(f'numId')\n\t\t\tsuppl = element.get(f'numDesc')\n\t\t\tis_numeric = (\n\t\t\t\tnumber.isdigit()\n\t\t\t\tand ((number <= 9) and (number >= 1))\n\t\t\t)\n\t\t\tnumber_elements = number."}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tif len(issue) == 1 and \"number\" in issue:\treturn dict(issue[\"number\"]), True\n\telif \"suppl\" in issue:\t\treturn dict(issue[\"suppl\"]), True\n\telse:\treturn set(), False\n\ndef set_status_and_date_of_issue_status(issue, date=None):\n\t\"\"\"\n\tSet Status and/or Date for issue.\n\t\"\"\"\n\tstatus = issue.get(\"status\")\n\tif status == \"open\":\tstatus = \"closed\"\n\treturn (status, issue.get(\"date"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tnumber = int(issue.childNodes[0].data)\n\tsuppl = issue.childNodes[1]\n\treturn number, suppl.text\n# End of extract_number_and_supplment from issue element\n\n# Start of extract_subjudging_term from issue element\ndef _extract_subjudging_term_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of subjudging term from the contents of issue.\t\"\"\"\n\tterm, number = _extract_number_and_subjudging_term_from_"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tnumber_and_suppl = issue['comment'] if \"comment\" in issue else get_single_element(issue, 'numberAndSupplement')\n\tnumber_and_suppl = number_and_suppl if number_and_suppl else None\n\tif issue.has_key(\"numberSupplement\") and not number_and_suppl:\n\t\tnumber_and_suppl = issue['numberSupplement']\n\treturn number_and_suppl\n\ndef _extract_information_element_from_issue_element(issue):\t\"\"\"\n\tExtract the information"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\treturn {'number': issue['number'],'suppl': None, 'location': None}\n\n\ndef _extract_location_from_issue_number(issue_number):\n\t\"\"\"\n\tExtract the the possible values of location from the contained issue number.\t\"\"\"\t\n\treturn {'number': issue_number}\n\t\ndef _extract_issue_number_and_location_from_issue_element(issue):\n\t\"\"\"\n\tExtract the possible values of issue_number and location from the contents of issue.\t\"\"\"\n\treturn {'number': issue['number'], 'location': None"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tnumber_and_suppl = [_strip_tags(x) for x in list(_extract_number_from_element(issue))]\n\treturn number_and_suppl\n\n\nclass GithubRepoManager(object):\n\n\t'''Manage a git repository for a particular company/domain\n\n\t'''\n\tdef __init__(self):\n\t\t\"\"\"Initialize a new instance of GithubRepoManager.\n\t\tArguments\n\t\t:\n\t\t:\n\t\t:\n\t\t'''\n\t\tself.git = Github(self)\n\t\tself.logger"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tif ('number' == issue.attrib.get('type')):\n\t\treturn {\n\t\t\t# Possible values are 1-5, with the 5th being on the right side of the number\n\t\t\t# (not the letter of the number itself) plus the letter 'o' on the side of the string.\n\t\t\t'number' : _get_number(issue.find(by.tagName('span')).text()) + 'O',\n\t\t\t'supply_number' : _get_supplier_particular_issue_number(issue),\n\t\t\t'type'"}]}
{"task_id": "62b46740d2f69a53b466171a", "prompt": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tf_lst = list(self)\n\tif debug: f_lst.reverse()\n\n\tf_line=1\n\n\tif not self:\n\t\tif debug: f_line += 1\n\t\treturn f\"{indent}Empty node\"\t\n\n\tf_list = []\n\tfor idx,val in enumerate(f_lst):\n\t\tif not val:\n\t\t\tf_list.append(\"%s\" % (str(idx)+\"->\"))\n\t\t\tcontinue\n\t\tf_list.append(\"# %s: %s\" % (str(idx),"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tindent = indent + 2 if debug else 0\n\ttext = IndentedText(indent=indent, initial=\" \", subsequent=[\" \"] * len(self.items))\n\tfor i, k in enumerate(self.keys):\n\t\tif debug:\n\t\t\ttext += k + \" : \"\n\t\t\tk1,v1 = k\n\t\t\tk2,v2 = v1\n\t\t\tprint(\"  \" + k1 + \":\" + pretty(k2))\n\n\t\ttext += self.__dict__[k] + \"\\n\"\n\treturn text\n\n### Caching ###"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tindent = \" \" * 4 * indent\n\tif isinstance(self.data, list):\n\t\toutput = \" * [ \"\n\t\tfor item in self.data:\n\t\t\tif is_numeric(item):\n\t\t\t\tnewStr = \" \" * 4 * indent\n\t\t\t\t# print self.data\n\t\t\t\t##print newStr\n\t\t\t\tprint \"\\t\" + newStr\n\t\t\telse:\n\t\t\t\t# print list items in one line\n\t\t\t\t# print self.data\n\t\t\t\toutput += \"'\"\n\t\t\t\toutput +=''.join([ str(val) for"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tif self.parent is None:\n\t\treturn r\"\"\"\\tab\\t %s\" % (self.name,)\n\telse:\n\t\treturn self.parent.pretty(level=2, print=False, debug=debug) + r\"\\nto \\tab\\t %s\" % (self.name, )\n\n/** @file\n  Comet Lake Platform Architectural Protocol as defined in PI 1.1 Specification\n  Volume 2. It covers both the Capsule Architectural Protocol as defined in PI 1.1\n  Specification and the PI 1.2 Specification"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tindent = \" \" * 4 if indent == 0 else indent\n\treturn indent + \\\n\t       \"Class\\t: %s\\n\"%self.classname\n\n##------------------###-----\n## Type Conversion and Unpickling\n##------------------###-----\ndef read_objstruct(file_name, debug=False, header_only=False):\n\t\"\"\"\n\tRead in an Objective C/C++ class structure; \n\ta separate file for.o files.\n\n\tArguments:\n\t\t-\tinputfile\t--\tFile containing the Objective C"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tfor x in self.__dict__.keys():\n\t\tif isinstance(self.__dict__[x], dict) :\n\t\t\tt = \" \".join(str(k)+' = %r' % v \n\t\t\tfor k,v in self.__dict__[x].items())\n\t\t\tn += '%s%s\\r\\n'%(indent*' ', t)\n\treturn n\n\n#######\n# \n# Grammar\n# ==========\n\n### \n# \nclass Grammar(object):\n\t\"\"\"A grammar, like a grammar rule.\"\"\"\n\t"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tp = \"%s { \" % self.__class__.__name__\t# First line comment\n\t\t+= \"modeltype  \"\t\t# Set model type: i.e. self.modeltype\n\t\t+=\t\"name  \"\t\t\t# Name of model (optional)\n\t\t+ \"num_parameters \"\t# Number of parameters (optional)\n\t\t+ \"parameters \"\t\t# List of parameters (optional)\n\t\t+ \"num_classes \"\t\t# Number of target classes (required)\n\t\t+ \"targets \"\t\t\t# List of target classes (required"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tn = self.__dict__['__repr__']\n\tif isinstance(n, bytes):\n\t\ttry: # 32bit\n\t\t\tn = '>%d' % int(n, base=int)\n\t\texcept TypeError: # Not str, bytes\n\t\t\tpass\n\ttry:\treturn repr(n)\n\texcept:pass\n\tif indent: indent = '\\t' * indent\n\tr = []\n\tfor k,v in sorted(dict.items(self.__dict__), key=lambda x:x[1]):\n\t\tif debug: r."}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tmsg = \"indent %d of %s\\n\"%(indent,self.name)\n\tmsg = msg+self.__doc__\n\tif debug: msg = msg+(\"Debug only; use dbg() to view debug output: %s\"%(self.dbg()))\n\treturn msg\n\ndef to_text(fpath, verbose=None, indent=0):\n\t\"\"\"\n\tSave some basic data to a file, using write() to output the result.\n\t\n\tIf the file already exists, it is not overwritten, regardless of its\n\tcontents.\n\t\n\t>>> fpath"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\ttags = {'tag': self.__class__.__name__}\n\tindent_length = 4\n\tindent = \"   \"*(indent_length)\n\treturn indent + \"###\" + self.__class__.__name__ + \"\\n\" + \\\n\t       indent + self.tags + \"\\n\"\n\n#pragma once\n#include <vector>\n#include <string>\n\nenum class TrieFormat {\n  // 0x7ff is reserved, so it must be <= 0x7ef\n  Hexadecimal = 0xFFFFFF, // 0, 0x"}]}
{"task_id": "62b46746d2f69a53b4661722", "prompt": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t", "suggestions": [{"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import collections\n\torbitals = {}\t\t\t# will hold result:orbitals, orbitals, orbitals...\n\tfor e in args:\t\t# walk through arguments...\n\t\tif len(e) == 2:\t# not a sequence...\n\t\t\torbitals.update( [ (a,b) for (a,b) in iterifneg( [a,b] ) ] )\"\"\"\n\t\telse:\t\t\t\t# just one arg - so it's neither, nor...\n\t\t\torbitals["}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from sp_utilities import model_absorb_MPI\n\tpass#IMPORTIMPORTIMPORT from sp_statistics import binarize\n\tpass#IMPORTIMPORTIMPORT from sp_filter import filt_tanl\n\tpass#IMPORTIMPORTIMPORT from mpi import MPI_COMM_WORLD, mpi_barrier\n\tpass#IMPORTIMPORTIMPORT from mpi import mpi_reduce, MPI_FLOAT, MPI_INT, MPI_SUM, MPI_LOR\n\n\tmpi_barrier(MPI_COMM_WORLD)\n\tif"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tparser.add_argument('--absorption-invert', '--invert')\n# \tparser.parser.add_option('--absorption-invert', '-n', '--no',\n# \t\t\taction=\"store_true\", dest=\"invert\", default=False,\n# \t\t\thelp=\"Invert absorption instead of absorbing all\")\n\tparser.add_argument('-F', '--full-inverse',\n\t\t\thelp=\"Calculate full-inverse, including all\")\n\tparser.add_argument('--neg-full-inverse', action='store_true')\n\tparser.add_"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\treturn\n\n\targs = [e.as_atom() for e in args]\n\tabsorb, _ = get_binop(e_binop.left_op,args)\n\tassert(binop.expr0 is not None and binop.expr1 is not None)\n\tif binop.expr0:\n\t\treturn expr.Binary( binop.expr0,absorb,BINOP_LEFT)\n\tif binop.expr1:\n\t\treturn expr.Binary( binop.expr1,absorb,BINOP_RIGHT)\n\t\n\ndef and"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\treturn list(map(lambda e: e._reduce(), args))\n\n\nclass expr(_match, _name, dict):\n\t\"\"\"\n\tA complete, universal, abstract, expression. It is used to implement the\n\tsyntax given in \"BNF\".\n\n\tTo perform syntax, call the :func:`parse` method.\n\n\t:Parameters: see class :class:`_match`\n\t:Types: see class :class:`expr`\n\n\t:ivar name: name of the expression\n\t:ivar value: content of the expression\n\n\tA list of all the"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tif len(args) == 1:\n\t\treturn args[0], True\n\telse:\n\t\treturn apply_op(args, lambda a : a & -a, True), False\n\nclass Negation(Op):\n\t\"\"\"\n\tOperator representing negation of two symbols. \n\tThe return value will hold the symbol in the list of symbols \n\tin the left-hand side symbol with the value in the right-hand\n\tside symbol and None in the other side symbol.\n\n\tExamples\n\t--------\n\n\t>>> from music21.grand_total import Negation"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tmsg = \"Absorb can be applied only on functions - function signatures supported\"\n\tprint(msg)\n\treturn False\n\n\tdef func(a,b,args):\t\t# Check if arguments are valid\n\t\ttry:\n\t\t\treturn a,b\n\t\texcept:\n\t\t\treturn args,a,b\n\t\treturn False,0,a,b\n\n\tdef check(args):\n\t\tb = args[0]\n\t\tif b.is_Add:\n\t\t\tfor (a,b ) in args[1:]:\n\t\t\t"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tself.funcname = \"%s_absorb\" % self.name\n\trlist = []\n\tfor args in args:\n\t\top, terms = args\n\t\trterms = []\n\t\tfor t in terms:\n\t\t\tif t.func == self.funcname:\n\t\t\t\trterms.append(t)\n\t\t\t\trlist.append(False)\n\t\t\t\tcontinue\n\t\t\n\t\ta = self.args.pop()\n\t\tif op.func == \"==\" and op == op.getarg(0) and op.getarg(1"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tself.arg_count = len(args)\n\tself.arg_count2 = len(args)\n\tself.arg_types = [c.TYPE for c in args]\n\tself.arg_dict = {}\n\tfor idx, arg in enumerate(args):\n\t\tif not isinstance(arg, types.T):\n\t\t\tself.arg_types[idx] = types.T\n\t\telif c == arg:\n\t\t\tself.arg_types[idx] = types.bool\n\t\telse:\n\t\t\tself.arg_types[idx] ="}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t)\n\n# -----------------------------------------------------------------------------\ndef all(self, args):\t\t\"\"\"\n\tAll expression.\n\nSee also::\n\n\tall_expr()\n\tall_list() - all elements of lists, list containers, tuple, dict, string, list, stream, other.\n\nOptional args::\n\n\tnone\n\n\t* list, tuple and string arguments::\n\t\tAll elements of lists, tuples.\n\n\t* stream arguments::\n\t\tStream expressions to be applied as arguments.\n\n\t* list arguments::\n\t\tList expressions to be applied as arguments.\n\n"}]}
{"task_id": "62b86707b4d922cb0e688c2a", "prompt": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\tdef decorator(fn):\n\t\thook_key = hook.__name__\n\t\tif hook_key in registry:\n\t\t\tkey = registry[hook_key]\n\t\t\tif key in sys.modules:\n\t\t\t\tfor mod in sys.modules[key].values():\n\t\t\t\t\tif mod.__name__ == hook_key and mod() is not None:\n\t\t\t\t\t\t# If a wrapper isn't specified don't try to add the callback\n\t\t\t\t\t\t# as this would result in circular imports.\n\t\t\t\t\t\treturn fn(key)\n\t\telse:\n\t\t\tif not len"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\treturn hook\n\n\nclass BaseHook (Hooks):\n\t\"\"\"\n\tBase class for hook definitions.\n\t\"\"\"\n\t_hook_type = 'hook'\n\n\tdef __init__(self):\n\t\tHooks.__init__(self)\n\t\tself.actions = []\n\n\tdef register(self, action):\n\t\t\"\"\"Register a new hook.\n\n\t\tHooks can return a callback function or a list of callbacks.\n\n\t\t\tclass BaseHook(BaseHook()):\n\t\t\tdef __init__(self, action):\n\t\t\t\"\"\"\n\n\t\tRegistering"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\tif self.hooks.has_key(self.handle):\n\t\tself.hooks[self.handle].append(hook)\n\t\tif not self.hooks.has_key(hook.handle):\n\t\t\tself.hooks[hook.handle] = sorted(self.hooks[hook.handle], key=lambda h: -h.handle)\n\t\tprint(\"Registered listener: {0}.\".format(hook.name))\n\t\thook.add_listener(self, self.handle)\n\telse:\n\t\t print(\"Could not register. Hook not found: {0"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\tfn = getattr(HookRegistry, hook, None)\n\tif fn:\n\t\treturn fn\n\telse:\n\t\treturn register_hook(hook, hook)\n\n\nclass HookRegistry(object):\n\t\"\"\"Base class for a registry of registered hooks.\n\n\tA HookRegistry acts as a \"root\", for example when invoking\n\t:meth:`add` of some hook::\n\n\t\tfrom pyglossary import HookRegistry\n\t\tfrom pyglossary.compat import basestring\n\n\t\thregistry = HookRegistry(basestring('hello!'))\n\t\th"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\tdef register(handler):\n\t\tif hook in self._handlers:\n\t\t\treturn self._handlers[hook]\n\t\tself._handlers[hook] = handler\n\t\treturn handler\n\treturn register\n\n\n#\n#  Registry class for registering (and handling) new callbacks and\n#  registering events for a particular hook.\n#\nclass Registry:\n\n\tdef __init__(self, hooks=()):\n\t\t\"\"\"\n\t\tInitialize the registry with all of hooks' metadata.\n\t\tDo *not* call this constructor directly.\n\t\t\"\"\"\n\t\tself"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\thook.register()\n\n@decorator(on)\nclass HandlerRegistry(object):\t\t\t# pylint: disable=invalid-name\n\t'''A decorator that automatically adds handlers to a registry\n\n\t\t\t\t@param hook: Name of the hook to register all handlers for\n\t\t\t\t\t\t'''\n\t_registrations = WeakKeyDictionary()\n\n\tdef __init__(self, hook=None):\n\t\tif hook:\n\t\t\tself._add_to_registrations(hook)\n\t\telse:\n\t\t\tself._add_handlers()\n\n\tdef _add_handlers(self"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\treturn lambda **kw: self.handler.on(hook, **kw)\n\ndef off(self, hook):\t\t\"\"\"Decorator function to remove a handler.\"\"\"\n\treturn lambda **kw: self.handler.off(hook, **kw)\n\n/*\n * Copyright 2008-2009 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http://www.apache."}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\told_handlers = self.registry\n\told_handlers_hook = getattr(old_handlers, hook.__name__)\n\tregistry = {}\n\tfor hook_type in hook.__bases__:\n\t\tregistry[hook_type] = getattr(old_handlers_hook, hook_type)\n\tself.registry = registry\n\tfor event in hook.events:\n\t\tself.add_handler(event.__name__, event, registry)\n\treturn self.hook\t# Return the original function\n\ndef add_handler(name, event, registry=None):\t#\tFunction to"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\treturn func.decoratewrapper(hook, handler)\n\nclass HookDecorator(Decorator):\n    \"\"\"Decorator that calls the specified handler only once.\"\"\"\n    def __init__(self, hook, handler):\n        \"\"\"Decorator constructor.\n\nArgs:\n    hook (HookType): An object that encapsulates the hook that\n\t\t\tresponds to the decorated handler.\n\n    If you don't define a function named hook(), it defaults to the\n\t\t'on' handler.\n\n    On a decorator registered by the"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\treturn decorator.on(hook)(self)\n\n/*\n * Copyright 2018-present MongoDB, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY"}]}
{"task_id": "62b86729b4d922cb0e688c2f", "prompt": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\tconfig = {\"name\": \"base-config\",\n\t          \"user\": user,\n\t          \"hosts\": [etcd_host, \"localhost\", 3306],\n\t          \"ports\": [etcd_port, 3307, 3306],\n\t          \"enable_ipv6\": False,\n\t          \"cert_file\": \"\",\n\t          \"key_file\": \"\",\n\t    "}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\tdb_config = {\n\t\t\"user\": user,\n\t\t\"email\": \"foo@example.com\",\n\t\t\"name\": \"admin\",\n\t\t\"password\": \"abc\",\n\t}\n\tetcd_config = {\n\t\t\"host\": etcd_host,\n\t\t\"port\": etcd_port,\n\t\t\"timeout\": \"5s\",\n\t\t\"key\": \"blah\",\n\t\t\"cert\": \"blah\",\n\t}\n\treturn db_config, etcd_config\n\ndef get_static_user():\n\treturn {"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\tbase_config = {}\n\tif user is None:\n\t\tif etcd_host == \"localhost\":\n\t\t\tuser = None\n\t\t\t_log.debug(\"Creating %s configuration\", USER_NAME)\n\t\t\tuser = getpass.getuser()\n\t\t\tif user is None:\n\t\t\t\t_log.error(\"User is not valid for static configuration, use --user and set it through the user configuration\")\n\t\t\t#\tuser = __salt__['user.create'](name=USER_NAME, value=\"root\")\n\t\t\t_log.debug(\"Configured user:"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\treturn {\n\t\t\"user\": user,\n\t\t\"etcdHost\": etcd_host,\n\t\t\"etcdPort\": etcd_port\n\t}\n\ndef admin(user, password):\n\tpass\n\ndef main():\n\t# Initialize the database and database configuration\n\t# This example uses one instance of the database\n\t# (main.bak), and has a custom configuration\n\t# To learn more about database configuration,\n\t# please see the example https://github.com/ultralytics/yolov5/wiki/Configuration-Guide\n\tdb = config."}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\tapp = get_app()\n\n\t# Define the basic environment.\n\t# User and cluster environment variables can be set on the application.\n\tenv_variables = [\"%s=%%s\" % (key, value) for key, value in app.env.items() if key.startswith(\"BASE_ENV\") and value.startswith(\"%\") and key!= \"BASE_ENV\"]\n\tenv_variables = \"\".join(env_variables)\n\t# Set the default value - the configuration file.\n\tos.environ[\"BASE_ENV\"] = env_variables\n\n\treturn {}\n"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\twith open(static_configs_path, \"w\", encoding=\"UTF-8\") as f:\n\t\tf.write(SIMPLE_CONFIG_TEMPLATE.format(user=\"\",\n\t\t                                       etcd_host=etcd_port))\n\t\treturn {\"user\": user, \"etcd_host\": etcd_host, \"etcd_port\": etcd_port}\n\n\n#-----------------------------------------------------------------------------\n#        "}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\treturn {\"etcd_config\": \"{}.yml\".format(user)}\n\n\nclass StaticDataServerDatabaseBackend:\n\n\t# This must be a class to get the backend's __init__ method\n\tdef __init__(self, name, backend, **options):\n\t\t\"\"\"\n\t\tArguments:\n\t\t\tname: a name of the backend\n\t\t\tbackend (base_database): the backend for the database\n\t\t\t**options: the options for the class\n\t\t\"\"\"\n\t\tself.name = name\n\t\tself.backend = backend\n\t\tself.database_"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\tuser_conf = {\n\t\t'NAME' : user,\n\t\t'CONF_NAME' : user,\n\t\t'CONF_HOST' : '127.0.0.1',\n\t\t'CONF_PORT' : str(etcd_port)\n\t}\n\treturn json.dumps(user_conf)\n\n\n@pytest.fixture(scope=\"function\")\ndef auth_config(base_config):\n\treturn {\n\t\t'NAME' : 'demo',\n\t\t'TYPE' : 'kerberos',\n\t\t'HOST' : '127"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t# Set default DB parameters\n\tif user and user!= \"root\":\n\t\tuser = \"/usr/local/sbin/user/\" + user\n\n\tserver = {\n\t\t'host':     etcd_host,\n\t\t'port':     int(etcd_port),\n\t\t'user':     user,\n\t\t'group':    \"\",\n\t\t'auth_db':  \"rbac\",\n\t\t'auth_read': False,\n\t}\n\n\tdb_params = {\n\t\t'db"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t# make a new config object\n\tconfig = {}\n\t\n\t# the user's credentials file\n\tfilename = settings[\"users\"].get(\"session\", \"\").replace('\\\\', '/')\n\t# the config section name\n\tpath = os.path.join(settings[\"modules\"][\"config\"][\"paths\"][\"config\"], 'auth_default_conf.cfg')\n\t# a flag to indicate if a new config has been created\n\tconfig_created = False\n\t# the config file to read\n\tconfig_fh = open(filename, 'r')\n\t# this is the configuration for the user\n\tconfig"}]}
{"task_id": "62b8a4a4755ee91dce50a3d2", "prompt": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t# We assume that this will return :class:`datetime.date` if we are in\n\t# the first timezone that we've identified as non-ambiguous.\n\tif not all (dt.tzinfo is None for dt in self):\n\t\tfold = lambda dt: tz.fold(dt)\n\t\tnow = datetime.datetime.combine(dt.time(), dt.tzinfo.tzname(dt))\n\t\tfor day in range(1, 7):\n\t\t\tdt1 = self.unambiguous(now, fold)\n\t\t\tdt2 = now.replace("}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tnewdt = dt.replace(tzinfo=None)\n\t# If we can't find a match in this timezone, we can't be ambig;\n\t# if we can't find a match in the timezone's new timezone, we\n\t# can't be ambiguous.\n\tmatch = self._ambig_re.match(newdt.astimezone(self))\n\tif match:\n\t\t# We had a problem. Set some other date.\n\t\tif str(newdt) == match.group(0):\n\t\t\traise AssertionError(\n\t\t\t\t\"Ambiguous"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\ttz = self._ambiguous_zone(dt.tzname())\n\ttz = self._combine_times(tz.replace(tzinfo=tzinfo(dt))) + tzinfo(dt)\n\treturn tz\n\ndef _convert(dt, errors='raise', tz=None, fold=None):\n\t\"\"\"\n\tConvert a timezone aware or naive datetime to the correct timezone\n\tin context of the tzname.\n\n\t:param dt:\n\t    A timezone-aware datetime, in which timezone information is\n\t\tinappropriate.\n\t:param errors:\n"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tself.tzinfo=dt.tzinfo\n\tself.utcoffset=dt.utcoffset(self.tzinfo)\n\n\t# In order to be able to set other fields as necessary, we need to\n\t# know the timezone offset and the year, both of which will determine\n\t# if and how the result is ambiguous.\n\tassert(not self.utcoffset.tzinfo)\n\n\tif self.utcoffset.utcoffset(dt.tzinfo):\n\t\tn=self.utcoffset.utcoffset(dt.tzinfo)-"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t# The following are the only cases where a datetime is ambiguous:\n\t#  - When a naive datetime falls in an ambiguous field...\n\t#  - When a naive datetime is ambiguous...\n\t#  - When a datetime in the past is ambiguous...\n\t#  - When a datetime in the future is ambiguous...\n\t#  - When a naive datetime is ambiguous...\n\t#  - When a naive datetime is ambiguous, but with an offset > 0.\n\t#  - When a naive datetime does not fall in any ambiguous field...\n\t# There are several amb"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tif not hasattr(DT, 'tzinfo'):\n\t\traise AttributeError(\"DT.tzinfo is missing\")\n\tutc = dt.astimezone(DT.tzinfo)\n\tif utc is None:\n\t\treturn dt\n\tdt2 = dt.replace(tzinfo=utc)\n\ttry:\n\t\tdt = dt2._tzrule.replace(tzinfo=utc)\n\texcept AttributeError:\n\t\tutc = DT\n\t\ttry:\n\t\t\tdt = dt2._tzrule.replace(tzinfo=utc)\n\t\texcept AttributeError:\n\t\t"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tfrom datetime import datetime\n\ttz = timezone.tzlocal()\n\n\toffset = timedelta(0)\t\t# UTC (Coordinated Universal Time) = GMT\n\tis_ambiguous = False\t\t\t\t# whether timezone info is ambiguous\n\tlast_transition_time = _tzinfo(dt)[0]\t# last time this transition occurred\n\twhile last_transition_time < datetime(dt.year, offset, tzinfo=tz):\n\t\tfrom_zone = not datetime(dt.year, offset, tzinfo=tz).tzinfo\t\t\t\t\t# get the datetime in tz"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tif not isinstance(dt.tzinfo, time.tzinfo):\n\t\traise TypeError('Not a time.struct_time_as object.')\n\t# we will try to work out the timezone offset for each local\n\t# timezone\n\tlocaloffset = dt - self._tzinfo_to_local_offset_time(dt.tzinfo)\n\treturn dt + localoffset\n\ndef _get_weekday_offset(year, month, day):\n    \"\"\" Given the year, month, and day of the year (00 to 99) in a\n    Gregorian calendar"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tassert isinstance(dt, (\n\t\tdatetime.datetime,\n\t\tdatetime.date,\n\t\tdatetime.datetime,\n\t\tdatetime.timedelta,\n\t\tdatetime.time,\n\t)), 'Expected datetime'\n\n\t# Normalize to our current \"fold\" state before starting.\n\tdt_in_fold = dt.fold(self.fold)\n\n\t# Create an ambiguous datetime from the other parts.\n\t# Don't use the standard \"from_date(...)\" implementation here;\n\t# we explicitly need it because we'd risk a misuse (and I\n"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tif not isinstance(dt, datetime):\n\t\traise TypeError, \"'dt' must be a datetime object\"\t\n\tif dt.tzinfo == self:\n\t\treturn dt.replace(tzinfo=self)\n\telse:\n\t\treturn ambiguity_time_normalize(dt, self)\n\ndef ambiguity_time_normalize(dt, timezonename=None):\n\t\"\"\"\n\tGiven a :class:`datetime.datetime`, which in the case of unambiguous\n\tdatetimes, attempts to normalize the time so that it is compatible with\n\tthe given time"}]}
{"task_id": "62b8982f755ee91dce50a241", "prompt": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn relativedelta(\n\t\t\tyears=self._years,\n\t\t\tmonths=self._months,\n\t\t\tdays=self._days,\n\t\t\thours=self._hours,\n\t\t\tminutes=self._minutes,\n\t\t\tseconds=self._seconds,\n\t\t\tmicroseconds=self._microseconds,\n\t\t\tmilliseconds=self._milliseconds,\n\t\t\tuse_timestamps=self._use_timestamps\n\t\t)\n\n#include \"stdafx.h\"\n#include \"ui_viewport.h\"\n\nvoid CUIWndViewport :: DoEvent( TInputEvent"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\treturn self._asobject(self._absolutedelta(self, True, True)[0], True, False)\n\n\tdef isoweekly(self):\t\"\"\"\n\t\tReturns a :class:`dateutil.relativedelta.relativedelta` object.\n\n\t\tIf a ``relativedelta`` object is available, returns a\n\t\t:class:`dateutil.relativedelta.relativedelta` object.\n\n\t\tIf not, returns a :class:`dateutil.relativedelta.relativedelta`\n\t\tobject with at least ``year``, ``month``, ``day``, ``hour"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn relativedelta(\n\t\tdays=int(self._weekday),\n\t\thours=int(self._start_hour),\n\t\tminutes=int(self._start_minute),\n\t\tseconds=int(self._start_second))\n\nRelativeDelta.__doc__ = (\n\t\"\"\"\n.. _tutorial-relative-delta:\n\nRelative changes of a :class:`dateutil.relativedelta.RelativeDelta`.\n\nThe ``RelativeDelta`` class provides methods for creating an\ninstance of a `RelativeDelta` object that encapsulates a\nrelative change. The first function to"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn RelativeDelta(self)\n\n#\n# Class-level methods\n#\n\n@python_2_unicode_compatible\nclass Relativedelta(object):\n\t\"\"\"\n\tA relativedelta can be used to represent, for example, a date or time\n\tinterval in a human-readable format.\n\n\t:param days: Day-part of the relative delta. As an integer, 0 indicates\n\tthe whole day, and 1 indicates the month.  Negative integers are disallowed.\n\t:param hours: Hour-part of the relative delta. In the case of a time\n\tinterval"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\treturn self.for_type(RelativeDelta)\n\n/*\n * This file is subject to the terms and conditions of the GNU General Public\n * License.  See the file \"COPYING\" in the main directory of this archive\n * for more details.\n *\n * Copyright (C) 2000,2002 Ralf Baechle (ralf@gnu.org)\n */\n#ifndef _ASM_IO_H\n#define _ASM_IO_H\n\n\n#include <asm/byteorder.h>\n#include <asm/addrspace.h>\n#include <asm/"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn self.value\n\ndef normalizeddelta(days=1.5, hours=2, minutes=3*24*60*60, seconds=10**6*60,\n\tmicroseconds=10**3*24**int(time.time()-timedelta(days=days)*3600)):\n\t\"\"\"\n\tReturn a version of this object represented exactly using\nthe :class:`dateutil.relativedelta.relativedelta` objects for the\ndifference between each two fields.\n\n>>> relativedelta(days=1, hours=2, minutes=3*24*60, seconds=10**"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn dateutil.__version__.relativedelta\n\nabsolute = (1, 0)\n\ndef absolute(self):\n\treturn dateutil.relativedelta(self)\n\n\n#---------------------------------------------------------------------------\n\nclass day(_datetimebase):\n\t\"\"\"\n\tBasic day-of-week model.\n\n\tGenerally, this holds some (but not all) day-of-week values in a range\n\tof days from 0 to 99, where an edge condition for each day-of-week\n\tis \"before\" or \"after\".\n\n\tThis month model has a start end range of 3"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn self.relativedelta\n\n\n#---------------------------------------------------------------------------\n\n# Standard date/time library\n\nclass date(dateutil.parser.base):\n\tmonth = None\n\tday = None\n\n\tdef __init__(self, value, dayofmonth=None):\n\t\t\"\"\"\n\t\t  Initialize a :class:`dateutil.parser.base.base` parser object.\n\n\t\t  Note that this does not parse the string value.  It merely\n\t\t  initializes a date object from the string.\n\t\t\"\"\"\n\t\tdateutil.parser.base.base.__init__(self)"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn relativedelta.relativedelta(self.years,\n                                        self.months)\n\nclass ibase:\n\t\"\"\"\n\tThis class defines the set of functions to apply\n    to dates.  There is probably one instance of\n    this class per class, but I want to make our life\n    simpler.\n\n\t\"\"\"\n\tmonths = 6\n    yeardays"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\tif not isinstance(self, dateutil.relativedelta):\n\t\traise TypeError('%s object is not a dateutil.relativedelta' %\n\t\t               type(self).__name__)\n\tif self.days < 0:\n\t\traise ValueError('days must be >= 0')\n\tif self.hours < 0:\n\t\traise ValueError('hours must be >= 0')\n\n\t# We can avoid the special processing for hours/minutes because of the\n\t# additional constraints we want to cover the years.\n\tif self.hours"}]}
{"task_id": "62b89640755ee91dce50a114", "prompt": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t", "suggestions": [{"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tencoding = locale.getpreferredencoding()\n\ttzname = getattr(time, 'tzname', None)\n\tif tzname is not None:\n\t\treturn tzname(namefunc)\n\tname = namefunc(encoding)\n\tif name[0].isdigit():\n\t\t# name is already unicode\n\t\tparts = name.split('.')\n\t\treturn ''.join([part.strip().lower() for part in parts])\n\telse:\n\t\treturn name\n\n# This module contains tests for bytestring.strip() and bytestring.lstrip(),"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tif PY3:\n\t\treturn namefunc\n\telse:\n\t\treturn unicode\n\n\n# Convert all of the unicode stuff to bytes (for Python 2)\nif PY3:\n\tdef unicode_to_bytes(source):\t\"\"\"\n\tConvert unicode stuff into bytes\n\n\tConvert unicode stuff into bytes when it's in Python 3.\n\n\tIt can return bytes of length 2 (ie. \"\u00c2\") if all is well and\n\tthere are no Unicode code points less than \\u0000 excluding\n\toverlong ones and spaces.\n\n\tTo force the BOM ("}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\ttry:\n\t\tfrom importlib import import_module\n\texcept ImportError:\n\t\t# It might not be in the same Python version, import_module\n\t\t# only checks the first one.\n\t\ttry:\n\t\t\timport_module('tzname')\n\t\texcept ImportError:\n\t\t\tbasestr = str\n\t\t\tif sys.platform not in 'linux':\n\t\t\t\tbasestr = base64\n\t\t\ttimestr = time.strftime('%m')\n\t\t\tnamestr = \"NaT\" # AET-style tzname\n\t\telse:\n\t\t"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tif not isinstance(namefunc, bytes):\n\t\tnamefunc = str(namefunc)\n\t\tif sys.version_info[0] == 2:\n\t\t\tnamefunc = namefunc.decode(\"utf8\")\n\t\tnamefunc = namefunc.encode(\"utf8\")\n\tfunc = lambda x: getattr(x, namefunc)(), 0\n\tres = namefunc(\"%d\" % func())\n\n\tif isinstance(res, str):\n\t\treturn unicode(res[0], \"utf-8\")\n\telse:\n\t\treturn res\n\ndef"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tif namefunc() <= 0:\n\t\traise ValueError, 'tzname()'\n\t#\n\t_len = max(len(namefunc() if hasattr(namefunc,'__len__') else namefunc()),2)*2\n\t_name = lambda x:x*2+namefunc()\t#.translate(u'').encode('unicode-escape')\n\tZ = [(0,_name(_s,_len),None,None) for _s,_len in zip(s,z)]\n\treturn tuple(Z)\n\nget_territory_by_code = python_"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tif not PY3: return\n\tif sys.implementation.name == \"IronPython\":\n\t\treturn namefunc()\n\treturn namefunc.__name__\n\ndef unidecode(iterable):\n\t\"\"\"Return a list containing items that might cause ValueError because of\na bytestring encoding problems.\"\"\"\n\treturn [encode(o) if isinstance(o, bytes) else o for o in iterable]\n\n\nclass _LazyConfigObj(object):\n\t'The class used to lazily load ConfigParser values.'\n\tdef __init__(self, default=None):\n\t\t'''Initialize"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t#if namefunc in unicode, return\n\tif isinstance(namefunc, str):\n\t\treturn namefunc.encode(\"utf-8\")\n\telse:\n\t\treturn None\n# I'm in python 2, but maybe you have a string (not unicode). Python 2 accepts that too.\ndef tzname_function(name_function):\n\tif name_function in ('timezone', 'timezones'):\n\t\t# if timezone is in the name of some datetime function (not just local)\n\t\t# return a string containing such information\n\t\tif callable(name_function):"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tif PY3:\t\t\t# pragma: no cover\n\t\treturn namefunc(namefunc.__self__)\n\telse:\n\t\t# This does the right thing\n\t\treturn namefunc(namefunc.__self__.encode('utf-8'))\n\ndef test_gettz():\n\t# This test ensures that timezone functions do what they return\n\n\t# Create a list of two TZONES_ARE_EQUAL in the order in which they'll\n\t# be parsed.  If the first is \"UTC\", it should be handled as UTC\n\t# since UTC has no time component."}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tdef f(tzdata):\n\t\treturn tzdata.name.decode(\"utf-8\")\n\treturn f\n\n\n\ndef _gettzoffset(offset, *basecomps):\n    if offset==0:\n        return 0\n    if offset<0:\n        offset=-offset #-1440: subtract DST offset\n    if offset>1440:\n        offset=offset%1440\n    offset=timedelta(seconds=-abs(offset))\n   "}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tnamefunc = compat_str(namefunc)\n\tname = str(namefunc)\n\n\tif sys.version_info[:2] > (3, 4):\n\t\tdef _name(*args, **kwds):\n\t\t\tnamefunc = args and (namefunc.__name__ or\n\t\t\t\tnamefunc.__unicode__ or _p).__name__\n\t\t\treturn name.encode(namefunc, *args, **kwds)\n\telse:\n\t\tdef _name(*args, **kwds):\n            args ="}]}
{"task_id": "62b87d24d292efb640a55670", "prompt": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\ttry:\n\t\twith open(VERSION_FILE, \"r\") as f:\n\t\t\treturn parse_version_file(f.read())\n\texcept IOError:\n\t\treturn \"unknown\"\n\nimport React from'react';\nimport { shallow } from 'enzyme';\n\nimport { ActionPanel } from './ActionPanel';\n\nfunction wrapComponent(Component: React.ComponentType<any>, props = {}) {\n  return shallow(<Component {...props} />);\n}\n\nconst props = {\n  description: 'foo description',\n};\n\ndescribe('ActionPanel"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\ttry:     ver = requests.get(URL_DEV_VERSION).text \n\texcept:     ver = 'undefined VERSION'\n\treturn int(ver)\n\n# Main routine\nif __name__ == '__main__':\n\t\n\tprint(BANNER)\n\ttry:\n\t\tprint(sys.argv[1])\n\t\turl=URL_DEV_VERSION\n\t\tp = GetVersion(url)\n\t\tif p==None:\t\t#print('NO VERSION')\n\t\t\tprint('Unknown version')\n\t\t\tsys.exit()"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\ttry:\n\t\t# This will raise GithubAPIException if the underlying data is no longer available.\n\t\tdata = github_api.get(\"repos/%s/releases\" % owner, params={\"per_page\":10}, auth=github_api.get_github())\n\t\tif \"assets\" in data:\n\t\t\tassets = data[\"assets\"]\n\t\t\treturn {\"assets\":assets, \"versions\":assets[0][\"versions\"]}\n\t\telse:\n\t\t\treturn {\"assets\":None, \"versions\":None}\n\texcept GithubAPIException:\n\t\t"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\ttry:\n\t\treturn os.stat('.').st_mtime\n\texcept OSError:\n\t\tpass\n\treturn get_default_version()\n\ndef safe_glob(glob, recursive=False):\n\tfor item in glob:\n\t\ttry:\n\t\t\tfile_exists = os.path.isfile(item)\n\t\texcept OSError:\n\t\t\tif recursive:\n\t\t\t\tfile_exists = os.path.isdir(item)\n\t\t\telse:\n\t\t\t\tfile_exists = False\n\t\tif file_exists: continue\n\t\t"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\ttry:\n\t\t__import__('pkg_resources').declare_namespace(__name__)\n\texcept ImportError:\n\t\treturn {'version': 'unknown',\n\t\t\t\t 'full-revisionid': 'unknown',\n\t\t\t\t 'dirty': 'unknown',\n\t\t\t\t 'error': 'unknown',\n\t\t\t\t 'date': 'unknown'}\n\t__version__ = pkg_resources.get_distribution(__name__).version\n\treturn {'version': __version__,\n\t\t\t 'full-revisionid': pkg_resources.get_distribution(__name__).version,\n\t\t\t 'dirty': pkg_resources.get_distribution(__"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\timport fnmatch\n\timport os\n\timport sys\n\tif hasattr(sys, 'gettotalrefcount'):\n\t\treturn sys.gettotalrefcount(), os.path.join(os.getcwd(), \"VERSIONS\")\n\there = os.path.abspath(__file__)\n\twith open(os.path.join(here, os.pardir, os.pardir, \"VERSIONS\"), \"r\") as f:\n\t\tfor line in f:\n\t\t\tif line.startswith(\"git_tag =\"):\n\t\t\t\treturn line.split(\" = \")[1].strip()"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\ttry:\n\t\twith open(path.join(current_path,'versions.json'),'r') as f:\n\t\t\treturn json.load(f)\n\t\treturn None\n\texcept FileNotFoundError:\n\t\ttry:\n\t\t\twith open(default_version,'r') as f:\n\t\t\t\treturn json.load(f)\n\t\texcept FileNotFoundError:\n\t\t\treturn None\n\texcept Exception as e:\n\t\t# Log the error but stay quiet for now...\n\t\tif not silent:\n\t\t\tsys.stderr.write(\"ERROR loading"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\ttry:\n\t\tf_version\t= open(VERSION_FILE,'r')\n\t\tfor line in f_version:\n\t\t\t#print line.rstrip('\\n')\n\t\t\t tokens\t= line.split()\n\t\t\tif not tokens or len(tokens)<2: continue\n\t\t\tif tokens[0]=='-' and tokens[1]!='':\n\t\t\t\ttry:\n\t\t\t\t\t(a,b)= map(int,tokens[1].split('.'))\n\t\t\t\texcept:\n\t\t\t\t\ttry:\n\t\t\t\t\t\t(a,b)= map(str,tokens"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\timport sys\n\tif sys.version_info[0] == 2:\n\t\treturn '2.7'\n\tif sys.version_info[0] == 3:\n\t\treturn '3.4'\n\tif sys.version_info[0:2] == (3, 4):\n\t\treturn '3.5'\n\tif sys.version_info[:2] >= (2, 7):\n\t\treturn '3.6'\n\treturn '3.7'\nversion_info = get_versions()\n\n/**\n * @license Copyright ("}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\ttry: version = open(os.path.join(here, \"VERSION\")).read().strip()\n\texcept: version=\"\"\n\treturn version\n\n#     def main(args):\n\n\nif __name__ == \"__main__\":\n\tmain()\n\n/* linux/drivers/scsi/mpt3sas/fwfault.h\n *\n * Header file for linux/drivers/scsi/mpt3sas/fwfault.c\n *\n * Copyright (C) 2003-2004 Matthew Wilcox <matthew@wil.cx>\n * Copyright (C)"}]}
{"task_id": "62b87d24d292efb640a5566f", "prompt": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tif pieces[\"error\"]:\n\t\traise ValueError(\"Invalid git_ref name: %s\" % pieces[\"error\"])\n\n\tname = pieces[\"name\"]\n\trepository = pieces.get(\"repository\")\n\tif repository is None:\n\t\trepository = repository_from_url(pieces[\"url\"])\n\n\t# if we have a full ref, let's just get it\n\tif pieces[\"full\"]!= \"\":\n\t\ttry:\n\t\t\tresolve_ref(pieces, repository)\n\t\texcept NotThisMethod(\"git not found\"):\n\t\t\tpass\n\t\treturn"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tfor piece in pieces:\n\t\tif piece[0] == 'S':\n\t\t\tpieces[0] ='s'\n\t\tpieces[0] = renderpiece(pieces[0], style)\n\t\tif pieces[-1][0] =='':\n\t\t\tbreak\n\treturn ''.join(pieces)\n\n\n# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\nclass GitError(Exception):\n\tcode = None\n\n\n# Helper functions for git_cmd_run().\n#\n# XXX: This is a"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tif style == \"pep440\":\n\t\treturn render_pep440(pieces)\n\telif style == \"pep440-dark\":\n\t\treturn render_pep440_dark(pieces)\n\n\ndef get_versions():\n\t\"\"\"Try to determine the version of the module using regexp.\"\"\"\n\t# Get the version from the __version__ variable.\n\t_version_re = re.compile(r'__version__ = ['\n\t\tr'\\\"]([^\\\"]*)\"')\n\t_version = _version_re.search(version).group(1"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tfor piece in pieces:\n\t\tif piece[0] =='':\n\t\t\tr = piece[1:]\n\t\t\tif len(r) >= 2 and r[-2] == '>' and r[-1] == '>':\n\t\t\t\tr = r[0:-1]\n\t\t\tpieces.insert(i, r+'\\n')\n\t\t\ti += 1\n\treturn ''.join(pieces)\n\ndef latex_to_html(in_string):\n\t\"\"\"Converts a LaTeX-compatible string into HTML.\"\"\"\n\tfrom collections import namedtuple\n\t"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\ttry:\n\t\tstyle = str(style) + \" \"\n\texcept: pass\n\treturn render_version(pieces)\n\n\ndef render_version(pieces):\n\tversion = pieces[\"version\"]\n\tif \"dev\" in version:\n\t\treturn \"Development-\" + version.split(\"dev\")[1]\n\telse:\n\t\treturn version\n\n//-------------------------------------------------------------------------------------------------------\n// Copyright (C) Microsoft. All rights reserved.\n// Licensed under the MIT license. See LICENSE.txt file in the project root for full license information.\n//-------------------------------------------------------------------------------------------------------\n\nfunction test0"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tif pieces[\"error\"]:\n\t\traise ValueError(\"Error rendering %s: %s\" % (style, pieces[\"error\"]))\n\n\tversion = pieces[\"version\"]\n\tif not version or version == \"0+untagged\":\n\t\t# render a v0.1.2+ tag\n\t\tpieces[\"version\"] = \"0+tagged\"\n\t\tif style == \"git\":\n\t\t\tpieces[\"full\"] = render_git_tag(pieces)\n\t\telif style in (\"git-describe\", \"git-describe.long\"):\n\t\t\tpieces[\"full\"] ="}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\treturn render_to(style, \"\", pieces)\n\n\ndef render_text(pieces, style):\n\t\"\"\"Render a text string, but avoid using heuristics to predict the\n\tlanguage of a piece of text.\"\"\"\n\treturn render_to(style, \"text\", pieces)\n\ndef render_unicode(pieces, style):\n\t\"\"\"Render a unicode string, but avoid using an heuristic to decide\n\thow to get a language, or whatnot.\"\"\"\n\treturn render_to(style, \"unicode\", pieces)\n\nclass Token(object):\n\tdef __init__(self"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tassert isinstance(pieces, (list, tuple))\n\tassert len(pieces) >= 1\n\n\tstyleStr = ''\n\n\tfor piece in pieces:\n\t\tif isinstance(piece, str):\n\t\t\tstyleStr += piece + ':'\n\t\telif isinstance(piece, (int, float)) and piece >= 0:\n\t\t\tstyleStr += str(piece) +''\n\t\telse:\n\t\t\traise Exception('Unknown piece: %r' % piece)\n\n\treturn styleStr\n\n\nclass FormatStyleError(Exception):\n\tpass\n\n\nclass No"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tpass\n\n\ndef safe_version(release):\n\t\"\"\"\n\tGiven a version string, try to determine if the version is valid.\n\tThe string could be a git-describe or git-describe.vxxx thing.\n\tGiven anything that is not a version, return False.  It is a\n\tspecial case: if git-describe.v00.y has a bogus version, we will\n\tsimply treat that as a new package and try to publish it as\n\tthe new package.  Otherwise we will treat it as a new package\n\tfor when it comes to versioning"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\ttags = [\"bison\"]\n\tfor piece in pieces:\n\t\tpieces[pieces.index(piece)+1] = render(piece, style)\n\treturn ''.join(pieces)\n\n\n# Map a path name to a module name\n_name_to_module = {}\n\ndef map_path(path):\n    \"\"\"\n    Given a module and a path name, returns the corresponding module name.\n    If the module doesn't exist, or if the path is a directory, returns the \n    empty string.\n    \"\"\""}]}
{"task_id": "62b87d24d292efb640a5566d", "prompt": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tdo_it = not bool(pieces)\n\tif do_it:\n\t\ttry:\n\t\t\treturn dot_mod_two(pieces)\n\t\texcept ValueError:\n\t\t\tpass\n\treturn ''\n\ndef delete_chars(s, chars):\n\t\"\"\"Delete the passed string characters from the end of all but first.\"\"\"\n\t# these rules have the least strict to the order imposed on least\n\t# string matching\n\tif chars == '':\n\t\treturn ''\n\telif chars == 'fgoth0z':\n\t\treturn '\\f'\n\t"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tfor piece in pieces:\n\t\tif piece[0] == '.':\n\t\t\treturn piece\n\treturn ''\n\ndef main():\n\tversionpat = re.compile(r\"^__version__ = ['\\\"]([^'\\\"]*)['\\\"]\", re.M)\n\tabout = {}\n\twith open('README.txt') as f:\n\t\tfor line in f.readlines():\n\t\t\tline = line.strip()\n\t\t\tif not line or line.startswith('#'):\n\t\t\t\tcontinue\n\t\t\tif line.startswith('usage'):\n\t\t\t\tcontinue\n"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tfor piece in pieces:\n\t\tif piece[0] == PLUS: return piece[1:]\n\treturn pieces\n\n\ndef main():\n\twith open(sys.argv[1], 'r', encoding='utf-8') as infile:\n\t\tlong_file = infile.read().splitlines()\n\n\n\t\t# if there are no arguments presently, just print out the\n\t\t# file without parsing. (it's assumed that sys.argv is ordered\n\t\t# decreasing in the file, which is the main reason we did this)\n\t\t"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif \"+\" in pieces:\n\t\treturn pieces\n\treturn \"plus\" + pieces\n\n\ndef main():\n\tSCRIPT_NAME = os.path.basename(sys.argv[0])\n\tsys.path.insert(0, os.path.dirname(SCRIPT_NAME) + \"/..\")\n\n\timport subprocess, os\n\n\tsys.stdout = subprocess.makePIPE()  # type: ignore\n\tos.environ[\"READTHEDOCS\"] = \"1\" if len(sys.argv)>1 else \"0\"\n\tos.environ[\"READTHEDOC"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif pieces[0] == '.':\n\t\treturn pieces\n\treturn pieces[0] + '.' + pieces[1:]\n\n\ndef render_git_long_version(pieces):\n\t\"\"\"TAG-VER-ID-MICROVERSION\"\"\"\n\tshort = pieces[0]\n\tlong = pieces[1:].strip()\n\tif not (len(long) == 0 or pieces[1] ==''):\n\t\tshort = pieces[0] + '-' + pieces[1:]\n\t\tlong = pieces[1:].strip() + '-' + pieces[2:]."}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\talternates = '()[]{}'.format\n\n\tdef f(b):\n\t\tif b.isupper():\n\t\t\treturn 'i'\n\t\tb1 = b.split()\n\t\twhile len(b1) == 2:\n\t\t\tb1.append('')\t\t#.* to.*, otherwise get rid of a trailing *\n\t\tb1.reverse()\n\t\treturn alternates.join(b1)\n\n\tif pieces[0] == '.':\t\t# Either an empty string or a.* was specified, never mind\n"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\treturn _lazy_list(find_dotall(pieces))\n\n\ndef find_dotall(pieces):\n\t\"\"\"Find all occurrences of '.' in a piece-wise way.\n\n\tIt returns a list of strings.\n\t\"\"\"\n\ttry:\n\t\treturn findall(r'\\1.\\2', piecewise(pieces))\n\texcept ValueError:\n\t\treturn []\n\n\n# Should be provided by the user.\n\ndef find_version():\n\t\"\"\"Find the version of this package in the standard Python path.\"\"\"\n\treturn findfile('python3.4/"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\ttry:\n\t\treturn pieces.pop() + '.'\n\texcept IndexError:\n\t\treturn '.'\n\n\ndef normalize_name(name):\n\t\"\"\"Convert a module name to a standard distribution name\n\tUsed by build_wheel.\n\t\"\"\"\n\tparts = name.split('.')\n\treturn ''.join(c for c in parts if c not in (\"_\",))\n\n\ndef safe_name(name):\n\t\"\"\"Convert a string to a standard distribution name\n\tUsed by build_wheel, and also used in Distribution.name to avoid\n\trepetitive"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tfor piece in pieces:\n\t\tif piece.startswith('+'):\n\t\t\treturn pieces\n\treturn pieces + '.'\n\ndef main(argv):\n\t\"\"\"\n\tThis is the entry point from the command line.  It does some\n\tstuff and then exits.\n\t\"\"\"\n\t# Do the thing\n\tprogname = argv[0]\n\tusage = progname + \" [options] file.py...\"\n\tparser = OptionParser(usage, version=PROGRAM_VERSION)\n\tparser.add_option(\"-c\", \"--check\", dest=\"filecheck\", help=\""}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tfor piece in pieces:\n\t\tif piece == '.':\n\t\t\treturn '.'\n\treturn plus_or_dot(pieces)\n\ndef three_state_transition_1(prev0, prev1, prev2):\n    # The 2nd state is always in line with the first state (prev1).\n    if prev1 in [prev0, prev2]:\n        return True\n    # prev1 is not in the first state for 2nd state\n    if not state_equal(prev0, prev1):"}]}
{"task_id": "62b87d23d292efb640a5566b", "prompt": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tif cwd is None:\n\t\tcwd = os.getcwd()\n\tprint(\"Working directory: %s\"%cwd)\n\tos.chdir(cwd)\n\tif env is not None:\n\t\t#print('using environment variables: %s'%env)\n\t\tfor key,val in env.items():\n\t\t\tif hasattr(args[key],\"value\"):\n\t\t\t\targs[key].value = str(val)\n\t\t\telse:\n\t\t\t\targs[key] = str(val)\n\n\tprint(commands, '->',''.join(args), file"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\timport __main__\n\tstatus = 0\n\tfull_args = []\n\ttry:\n\t\tif not args: # No command given.\n\t\t\targs = $_['args']\n\t\telse:\n\t\t\targs = shlex.split(args)\n\t\tif not progdir: # No $PROGRAM_NAME given.\n\t\t\tprogdir = os.path.dirname(os.path.realpath(__file__))\n\t\tfor a in args:\n\t\t\tfull_args.append(shlex.split(a, keepends=True))\n\texcept Exception,"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\treturn __import__(_pkg_resources[\"boto_ssmaccount\"][\"__package__\"].rsplit(\".\", 1))[0](commands, args, cwd, verbose, hide_stderr, env)\n\ndef get_version(*args):\n\treturn run_command([\n\t\t\"get_version\", \"-s\", \"-T\", \"-T\", \"-c\"], cwd=\".\", env=get_env())\n\n/*=========================================================================\n *\n *  Copyright NumFOCUS\n *\n *  Licensed under the Apache License, Version 2.0 (the \"License\");\n *  you may not use this file"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tassert isinstance(commands, (list, tuple))\n\ttry:\n\t\tpcmd = commands[0]\n\t\tassert isinstance(pcmd, str)\n\n\t\t# On Windows, use os.popen() if the command causes trouble.\n\t\t# We won't ever get here (or at least won't be printing an error)\n\t\t#  with pcmd=, which is equivalent to os.system(pcmd), so we\n\t\t#  can short-circuit here and return immediately.\n\t\tif sys.platform == 'win32':\n\t\t\tout, err"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tshort_help = 'build a wheel or extract archive from one or more archives'\n\tlong_help = 'Build or extract the archive from one or more archives\\n' + short_help\n\n\tdef call(self, *args, **options):\n\t\t''' Make a shell command and invoke it. '''\n\t\tcmd  =''.join(args)\n\t\t# We will put our data-files to run_dir\n\t\tif cwd is None:\n\t\t\tcwd  = self.command.__globals__[args[0]]\n\t\t\t# This is where"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\tfor i in range(len(commands)):\n\t\ttry:\n\t\t\tif verbose: print(commands[i])\n\t\t\tproc = Popen(commands[i], cwd=cwd, env=env, stdout=PIPE, stderr=PIPE)\n\t\t\tstdout, stderr = proc.communicate(args)\n\t\t\treturn proc.returncode\n\t\texcept OSError as e:\n\t\t\tif str(e).find('No such file')!= -1:\n\t\t\t\tpass\n\t\t\t"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tshell(commands, cwd=cwd, env=env or {}, verbose=verbose)\n\ndef shell(commands, cwd=None, env=None, verbose=False, hide_stderr=False):\n\t\"\"\"\n\tRun commands in context of the given process.\n\t\"\"\"\n\tif env and 'GUNICORN_TESTING' not in env:\t\n\t\tprint_debug(\"Running command(s).\")\n\t\tfor (command,cwd,env) in commands:\n\t\t\tprint_debug(\"Working directory: [%s].\"%cwd)\n\t\t\tprint_debug(\"Execution environment"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\timport mcomix.config.setup\t# so we can pass it to the setup process\n\tdef find_executable(sdir, env):\n\t\texe, myname = os.path.split(sdir)\n\t\tif not os.path.isfile(exe):\n\t\t\treturn None\n\t\texe = os.path.normpath(exe).replace('\\\\', '/')\n\t\tif myname=='':\n\t\t\tmyname = exe\n\t\texe = exe.lower()\n\t\tif exe[-4:] == \".exe\":\n\t\t\texe = exe[:-"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tcmd = commands.get(args) if args in commands else None\n\tassert isinstance(cmd, Callable), 'Command \"%s\" must be a callable.' % args\n\tret = None\n\ttry:\n\t\tif verbose: loggit.set_threshold('DEBUG')\n\t\tretcode = cmd(*args, cwd=cwd, env=env)\n\texcept CalledProcessError as err:\n\t\tretcode = err.returncode\n\t\tif err.stdout and err.stderr:\n\t\t\terr.stdout = err.stdout.strip() + '\\n' + err.stdout."}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tshort_version = 'python %s' % __version__\n\t# I hate python2, but then py3k seems to be one of the most popular options\n\tif env is None:\n\t\tenv = dict(os.environ)\n\n\tif hide_stderr:\n\t\tstderr = None\n\telse:\n\t\tstderr = open(os.devnull, 'w')\n\ttry:\n\t\tcode, out, err = commands\n\t\tif out:\n\t\t\tstdout = out\n\t\t\terr = err.decode('utf-8')\n"}]}
{"task_id": "62b87d23d292efb640a55668", "prompt": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\tclass_dir = os.path.dirname(sys.modules[__name__].__file__)\n\tversionfile_long = os.path.join(class_dir, 'long_version.py')\n\texec(versionfile_long)\n\tconfig = VersioneerConfig()\n\t# This specifies the scheme we should use.\n\tconfig.SCHEME = 'taglet'\n\t# This specifies the basic version of the package we're installing.\n\tconfig.VERBOSITY = 1\n\t# Show which fields the user has specified.\n\tconfig.SHOW_UNKNOWN_FIELDS ="}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\tclass VersioneerConfig:\n\t\tfile_logger = None\n\t\tpackagers_settings = None\n\t\tgit_revision_strategy = \"GO\"\n\t\tgit_revision_order = \"local\"\n\t\tgit_repository = None\n\t\thash_strategy = \"archive\"\n\t\tdefault_branch = \"main\"\n\t\trelease = None\n\t\tmax_bad_files_size = 3\n\n\tdef load_config(self):\n\t\t\"\"\"\n\t\tLoad the VersioneerConfig() object based on the contents of the\n\t\t#V"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\tc = VersioneerConfig()\n\tc.load()\n\treturn c\n\n\ndef get_version():\n\t\"\"\" Get the semantic version string \"\"\"\n\tset_style_provider(style_provider)\n\twith open('index.py','r') as f:\n\t\tfor line in f:\n\t\t\tif line.startswith('__version__'):\n\t\t\t\tver = [x.strip() for x in line.strip().split('=')]\n\t\t\t\tif ver and ver[0]!= '':\n\t\t\t\t\tif not any(x in v for v in (\"dev"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\tVersioneerConfig.__init__(self)\n\tself.versionfile_source = None\n\tself.pardir_prefix = 'pardir'\n\tself.parfile_source = None\n\tself.build_backend = False\n\n\t# Find version from 'pardir_prefix'.pardir_prefix_foo is defined but not 'pardir_prefix'.foo\n\tself.pardir_prefix_foo = os.path.join(\n\t\tos.path.dirname(pardir_prefix.__file__),\n\t\tpardir_prefix.pardir_prefix_foo)"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\tabout = {}\n\tif sys.argv[1] == 'test':\n\t\tunversioned = True\n\t\tconfig = VersioneerConfig(about=about)\n\t\ttry:\n\t\t\texecfile(os.path.join(os.path.dirname(__file__),'..','binaries','test_versioneer.py'),config.parse_args(sys.argv[2+len(sys.argv):]))\n\t\texcept OSError,why:\n\t\t\tif os.path.basename(why) == 'No suitable versioneer found':\n\t\t\t\tsys"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\timport imp\n\tversioneer_module_dir = os.path.dirname(imp.find_module(__name__))\n\tconfig_file = os.path.join(versioneer_module_dir,'versioneer.conf')\n\n\t# read in the config file(s)\n\ttry:\n\t\twith open(config_file, 'r') as f:\n\t\t\tconfig_text=f.read()\n\texcept IOError:\n\t\tconfig_text = ''\n\n\t# replace keys that contain a '.' with '-'\n\tdot_keys = []\n\t"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\trv = VersioneerConfig()\n\trv.token = None\n\tstack = rv.parser.get_parser_stack()\n\tfor t, v in trace_parser.parse(stack[1]):\n\t\tsetattr(rv, t, v)\n\treturn rv\n\n\nclass VersioneerConfig:\n\tdef __init__(self):\n\t\tself.show_stderr = False\n\t\tself.debug = False\n\t\tself.tokens_to_show = 200\n\t\tself.show_temp = False\n\t\tself.show_version = True\n\t\t"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\tclass VersioneerConfig:\n\t\t\"\"\"Container for Versioneer configuration parameters.\"\"\"\n\n\t\tsigning_secret = os.getenv('VERSERAISE_SIGNING_SECRET')\n\t\ttime_taken = 300\n\t\tlog_level = os.getenv('VERSERAISE_LOG_LEVEL', 3)\n\n\t\tdef __init__(self, **kwargs):\n\t\t\tsuper().__init__(domain='versera.com', **kwargs)\n\n\t\t@staticmethod\n\t\tdef defaults(**kwargs):\n\t\t\t\"\"\"Container for Versioneer configuration defaults.\"\"\"\n\t\t\td"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\tclass VersioneerConfig:\n\t\t__slots__ = (\"prefix\",)\n\t\t\n\t\tdef __init__(self):\n\t\t\ttry:\n\t\t\t\timport configparser\n\t\t\texcept ImportError:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tself.original_config_files = os.listdir(configparser.getini_filepath())\n\t\t\t\texcept OSError as e:\n\t\t\t\t\tif e.errno!= errno.ENOENT:\n\t\t\t\t\t\traise\n\t\t\t\tself.config_files = []\n\t\t\t\tfor file in self.original_config_"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\tclass VersioneerConfig:\n\t\tdef __init__(self):\n\t\t\ttry: config = _versioneer_cfg_from_file()\n\t\t\texcept EnvironmentError:\n\t\t\t\tprint(\"unable to read in versioneer.cfg file\")\n\t\t\t\tsys.exit(1)\n\t\t\tself.cfg = config\n\n\t\tdef load_cfg(self, cfg_name):\n\t\t\tif '@' in cfg_name:\n\t\t\t\t# dynamically load a named configuration\n\t\t\t\tcfg_name = cfg_name.split('@')[0]\n\t\t\t\ttry:\n\t\t\t\t\t"}]}
{"task_id": "62b87d23d292efb640a55667", "prompt": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tvcs_handler_list = method.split('.')\n\tif not vcs_handler_list[0][:1] in ['get', 'post', ]:\n\t\traise ValueError('Invalid vcs method %s.'% vcs_handler_list[0].split('.')[-1])\n\tmethods = (\n\t\t'stash', 'delete', 'head', 'put', 'patch',\n\t\t)\n\tif vcs_handler_list[-1][0] in methods:\n\t\treturn\n\telif vcs_handler_list[-1][0]"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t@functools.wraps(method)\n\tdef wrapper(*args, **kwargs):\n\t\tresult = getattr(method,'method_' + _vcs_handler_name(*args), None)\n\t\tif result is None:\n\t\t\treturn method(*args, **kwargs)\n\t\telse:\n\t\t\treturn result\n\treturn wrapper\n\n\ndef register_vcs_handler_class(vcs, handler_name, handler):  # decorator\t\"\"\"\n\tCreate decorator the handler_name for a decorator registered with VCS.  \"\"\"\n\t@functools.wraps("}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tdef decorator(f): # @private\n\t\t@wraps(f)\n\t\tdef new_f(*args, **kwargs):\n\t\t\t'''\n\t\t\t\tMarks a method as the handler of a particular VCS.\n\t\t\t\t@param f: reference to the method being wrapped.\n\t\t\t\t@param args: signature of the original method\n\t\t\t\t\t\t@param kwargs: a dictionary of the args and varibales\n\t\t\t\t\t\t'''\n\t\t\tf[vcs.lower()] = method\n\t\t\treturn f\n\t\treturn new_f\n\treturn decorator\n\n# -----------\n"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tdef decorated(f):\n\t\t@functools.wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tassert len(vcs) <= 1\n\t\t\tif not hasattr(f,'register_' + vcs[-1]):\n\t\t\t\ttry:\n\t\t\t\t\t__import__\n\t\t\t\t\timport json\n\t\t\t\t\tsys.modules[vcs[-1]] = json.loads(f.__name__)\n\t\t\t\texcept ImportError:\n\t\t\t\t\tsys.modules[vcs[-1]] = {}\n\t\t\ttry:\n\t\t\t\treturn getattr(f, vcs[-1"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t# XXX add some convenience methods if the method already exists?\n\n\tdef register(f):\n\t\tif isinstance(f, types.MethodType):\n\t\t\treturn getattr(f, 'wrap', f)\n\t\telse:\n\t\t\treturn f\n\n\tdef get_handler(method):\n\t\tmy_method = {\n\t\t\t'vcs': vcs.__name__,\n\t\t\t'method': method.__name__,\n\t\t\t}\n\n\t\t@register(method)\n\t\tdef wrapped(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\t\tDecorator taking the"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tdef register_handler(cls):  # decorator\t\"\"\"\n\t\tdef decorator(obj):  # obj\t: An object to be decorated.\n\t\t\tif hasattr(obj, \"add_callback\"):  # class\t: Decorator\n\t\t\t\tdef callback(*exc, func):\n\t\t\t\t\t\tcallback(*(obj, func))\n\t\t\t\t\treturn cls(obj, callback)\n\t\t\t\t\tobj.add_callback = None\n\t\t\telse:  # function: Decorator\n\t\t\t\t@wraps(cls)\n\t\t\t\tdef wrapped(*param, **key):\n\t\t\t\t\t\treturn cls.add"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tdef decorator(f):\n\t\t@functools.wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\t\thandler:  function applied to a VCS\n\t\t\t\tname:  class name of the handler\n\t\t\t\tkwargs:  any additional arguments to pass to method\n\t\t\"\"\"\n\t\twrapper.__name__ = f.__name__\n\t\twrapper.__doc__ = \"{} {}\".format(vcs.__name__, method.__name__) \\\n\t\t\t+ (\"(with {} {})\".format(f.__name__, vcs.__module__) if\n"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t@functools.wraps(getattr(vcs.__class__, method)())\n\tdef wrapper(*args, **kwargs):\n\t\tif hasattr(vcs, method):\n\t\t\treturn getattr(vcs, method)(*args, **kwargs)\n\t\treturn None\n\n\ndef install_vcs_handler(handler=None, class_name=None, version=None):\n\t\"\"\"Register a VCS handler as an install handler.\n\n\tTakes care of wrapping a class, so there must be a class_name argument;\n\twe can't just pass the class_name explicitly because it"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tdef register(cls):\n\t\thandler_name = handler_map[cls]\n\n\t\tif hasattr(cls, 'get_handlers'):\n\t\t\thandler = cls.get_handlers[handler_name]\n\t\t\tif handler is None:\n\t\t\t\thandler = cls.get_handler(handler_name)\n\t\t\t# We ignore vcs argument when the class is registered as\n\t\t\t# a VCS\n\t\t\tif kiwi_class and handler.vcs_name!= kiwi_class.name:\n\t\t\t\t# We can return early because we register only"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tdef decorator(fn):\n\t\ttry:  # Try to use @vcs decorated function\n\t\t\treturn vcs.register_handler(fn, method)\n\t\texcept VcsInitializationError as err:\n\t\t\t# Return error\n\t\t\treturn str(err)\n\t\treturn fn\n\treturn decorator\n\nuse cpython::import_::import;\nuse structopt::StructOpt;\n\nfn main() -> Result<(), std::io::Error> {\n    std::env::set_var(\"RUST_BACKTRACE\", \"1\");\n\n    let"}]}
{"task_id": "62b45e945108cfac7f210a4a", "prompt": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from sp_applications import flash_pix\n\tpass#IMPORTIMPORTIMPORT from sp_utilities import model_circle\n\tpass#IMPORTIMPORTIMPORT from sp_utilities import generate_ctf\n\tpass#IMPORTIMPORTIMPORT from sp_utilities import set_params_proj\n\tpass#IMPORTIMPORTIMPORT import os\n\tpass#IMPORTIMPORTIMPORT import types\n\n\tif validate_objects:\n\t\tvalidate_objs\t= [0,1,2]\n\ttry:\n\t\tcheck_"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tglobal valid_hierarchy, valid_hierarchy_cache\n\n    if self.check_hierarchy(check_digests=check_digests, show_warnings=show_warnings):\n\t\tvalid_hierarchy = valid_hierarchy_cache[self.cid]\n\t\tvalid_hierarchy_cache[self.cid] = -1\n\t\tvalid_hierarchy_cache[self.cid] += 1\n    return valid_hierarchy\n\n\t# ---------------------------------------------------------------------------------------------\n\t# --  Set path --\n\t# ---------------------------------------------------------------------------------------------\n    def set_puppet_path(self"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tnum_objects, good_objects = 0, 0\n\tobjs, digests, hashes, objects_from_folder, folder_names = (None, None, None, None, None, None)\n\tfor obj in self.hierarchy:\n\t\tif obj.location in (0, 1, 2, 3, 4, 5, 6): continue\t\t\t\t\t\t\t\n\t\tif not check_digests:\t\t\t\t\t\t\t\n\t\t\tif digests is not None and obj.digest() in digests:\t\\\n\t\t\t\tcontinue\t\t\t\t\t\t\t\t\t\t\t\n\t\t\thashes.append(obj.sha1())\n\t\t\t\t\t\t\t\t\t\t\n"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tif validate_objects:\t\n\t\tself.validate_objects()\n\tif check_digests and check_digests_from_root():\n\t\tfor object_digest_path in self.object_names_digest_paths.values():\n\t\t\ttry:\n\t\t\t\tself.validate_digest(object_digest_path, None, self.digest_base_file)\n\t\t\texcept:\n\t\t\t\tpass\n\treturn self.objects_valid_cache\n\n\n\n\n\t\ndef validate_digest(object_digest_path, object_digest, object_digest_"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tnum_objects = self.find_objects(check_digests=check_digests)\n\tbad_objects = self.find_bad_objects(check_digests=check_digests)\n\tnum_objects = len(self.objects)\n\tif show_warnings:\n\t\tprint(\"Found %i object%s and %i other objects%s. They are: %i of %i object%s and %i of %i others.\" % (\n\t  num_objects, \"\", num_objects-bad_objects, \"\", num_objects-num_objects, \"\","}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\troot_object = self.main_object['path']\n\t# Determine the total number of objects at this level (if specified)\n\tnum_objects = 0\n\tif validate_objects:\t# Validity of objects specified via parameters\n\t\tif root_object.startswith(self.target_object['path']):\n\t\t\t# No change on target path\n\t\telse:\n\t\t\tif os.path.isdir(root_object):\n\t\t\t\tnum_objects = len(listdir(root_object))\n\t\t\tif root_object.endswith('.gz'):\n"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\thierarchy_name = 'Storage Root File'\n\tif self.get_value('StorageRootDir'):\thierarchy_name = 'Storage Root Directory'\n\tlogging.info('Validating: %s [%s]', hierarchy_name, self.get_value('StorageRootDir'))\n\n\t# Iterate over all active and valid objects and look for any valid objects which are present for a storage container\n\t# and have an active digest reference and can be validated by using a valid digest type\n\tinvalid_object_count = 0\t\t\t# number of objects found to be non-valid to validate (but"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tlogging.debug('validate_storage_root_hierarchy: %s %s' % (self._object_data_model.__name__, check_digests))\n\n\tfile_name = Storage.objects_digest_from_location(self._object_data_model.location)\n\tnum_objects = 0\n\n\n# -----------------------------------------------------------------------------\n\nclass MetadataStorage(Storage):\n\t\"\"\"\n\tMetadata storage class. Handles all objects found in DB storage.\n\n\tParameters:\n\t\tstorage_cls (str): name of the storage class to use.\n\t\tobject_type (str"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tself.root_nodes_storage_checkers = check_digests\n\treturn validate_objects and self.validate_hierarchy_or_return_num_objects(num_objects=True, object_list=self.object_list, validate_objects=validate_objects, show_warnings=True if not show_warnings else not(show_warnings))\n\n\n\t\ndef get_digest(self, name, recursive = True, **kwargs):\n\t\"\"\"Digest function based on name, if specified (not to include full path) it returns a fully qualified name\"\"\"\n\tif name is"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tobjects_info = self.get_hierarchy()\n\tnum_objects = len(objects_info)\n\n\tif not objects_info or self.hierarchy_is_valid:\n\t\tif validate_objects:\n\t\t\tself.objects_info = []\n\t\t\tself.hierarchy_is_valid = False\n\t\t\tlogging.info('Performing validation for object storage')\n\t\telse:\n\t\t\tself.return_code = -1\n\t\t\tlogging.info(\"No valid objects were found\")\n\t\t\treturn 0\n\n\tif not self.is_root_"}]}
{"task_id": "62b45e515108cfac7f210a3c", "prompt": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tself.root = StorageRoot_v2(self)\n\tself.root.storage_options.create_file_system(\n\t\t\"tmp_test_file_system\", {})\n\treturn\n\nclass StorageRoot_v2 (StorageRoot):\n\t\"\"\"\n\tA storage root in the VirtualFileSystem. The root has a storage\n\tbackend set with the name of its backend, such as \"root\",\n\t\"filesystem_root_v2\" and \"vhd_root_v2\".\n\t\"\"\"\n\tdef __init__(self, vsm=None, parent="}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tself.root = ROOT\n\tself.open_root(self.root, path) # TODO use the actual root?\n\tself.mounts = {}\t\t\t\t\t\t# a dict of open file handle to path\n\tself.path_index = path_index(list(self.mounts.keys()))\n\t\n\tself.handle_read_path()\n\t\n\tif self.file_open_error:\n\t\traise OCFLError(self, \"file is open\") \t\t# TODO: why?\n\tif self.mount_open_error:\n\t\traise O"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tself.root = OCFLRoot()\n\n\n\tif len(self.options) == 0:\n\t\tprint \"\\n     No config file specified.\\n     Run `ocfl_init ${file}`.\\n\"\n\t\tsys.exit()\n\telse:\n\t\tself.config = open(self.options[0])\n\t\tself.options = list(self.config.readlines())\n\t\tself.options = dict(self.options)\n\t\t# initialize the options\n\t\tself._initialize_options()\n\t"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tself._root = dict()\n\tself._root[OSV_DATA_INFO] = _info\n\tself._root[OSV_META_INFO] = _info\n\tself._root[OSV_META] = dict()\n\tself._root[OSV_VOL_ATTR] = dict()\n\tself._root[OSV_RECORD] = dict()\n\n\tosv = self._root.setdefault(OSV_ID, {})\n\tosv[OSV_ID] = 'Root ID'\n\tosv[OSV_CREATED] = mktime"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tself.root = r.root.new_ocfl_storage(self.header, self.meta)\n\tself.meta.write(self.root)\n\tself.root.set_version(ocfl_version.OCFL_VERSION)\n\n\tocf_dict = OrderedDict( [\n\t\t( \"root\", self.root ),\n\t\t( \"hash\", self.header.as_string(32) ),\n\t\t( \"len\", len( self.get_hashes() ) )\n\t] )\n\tself.add_ocf( \"ROOT\","}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tself.root = (os.path.splitdrive(i[0])[1][:-1])\n\treturn\n\n\ttry:\n\t\ttry:\n\t\t\tos.mkdir(\"c:\\\\\")\n\t\t\tos.mkdir(\"d:\\\\\")\n\t\t\tos.mkdir(\"o:\\\\\")\n\t\t\tos.mkdir(\"i:\\\\\")\n\t\t\tself.getroot()\n\t\texcept OSError:\n\t\t\treturn\n\t\tself.getroot()\n\texcept (OSError, IOError):\n\t\tself.cleanup()\n\t\treturn\n\n\tself.write"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tself.root = self.new_node(node_type.root)\n\tself.root.root = self.root\n\tself.root.id = b\"\"\n\tself.root.storage_type = node_type.root\t\n\t\n\t\n\tself.all_nodes = { self.root: { } }\t\n\t\n\t\n\t\nclass Storage:\n\t\"\"\"\n\tThe simple storage class, which stores files in OCFL.\t\t#\n\t\t#\n\t\t#\t\t#\t\t#\n\t\t#\t\t#\t\t#\n\t\t#\t"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t# if not self.get_root_path():\t\t# If we haven't already created the root\n\t# \t# Create a dummy file that has the same name as the original file, but an empty\n\t#\t    # file so it does not get compressed to get written to disk\n\t# \tself.dummy_fname = open(self.rootPath + \".ocfl.tmp\", \"rb+\"):\n\t# \tself.ocfllist = []\n\t# \tself.count = 0\n\t# \tself.nextfile_number = 0"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tself.root = HNode(\n\t\t\"root\", 0, \n\t\t[\n\t\t\tHAttr(\n\t\t\t\tHAttr(\"root\"), \n\t\t\t\tself.path,\n\t\t\t)\n\t\t]\n\t)\n\tself.init()\n\n\treturn\n\n\n\t\"\"\"\n\tAdd a node to the hierarchy.\n\t@params path:  Name of the node\n\t@param node: The node\n\t@return 1 on success, 0 on failure.\n\t\"\"\"\n\tdef add_node(self, path, node):\n\t\tself.node"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tocflroot = OCFLRoot()\n\tocflroot.initialize(self)\n\treturn ocflroot\n\n\t\ndef new_ocfl_root_from_dict(self, dict, prefix='OCFLArchive'):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root with the dictionary specified.\n\tIf `prefix` is '', it is stripped from the text before creation.\n\t\"\"\"\n\tnew_dict = self.__get_dict()\n\tnew_dict.update(dict)\n\tocflroot = OCFLRoot()\n\toc"}]}
{"task_id": "62b45e2eb89c9fd354170232", "prompt": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tif version == \"0001\":\n\t\tversion = \"0101\"\n\tif version == \"0002\":\n\t\tversion = \"0102\"\n\treturn version\n\ndef is_old_version(version):\n    \"\"\"Check whether the version string is in the correct format.\"\"\"\n    return version[0:6] == \"010100\"\n\n\nclass BadVersionError(BuildError):\n    pass\n\ndef get_next_version() :\n\treturn dict(\n\t\tnext_version = next_version,\n        )"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tif version[0] not in numeric_version:\n\t\traise ValueError, \"'%s' is not a valid numeric version identifier\" % version\n\n\tver = numeric_version[version[0]]\n\tif ver.startswith(numeric_version_delimiter):\n\t\tver = numeric_version[version[0] + 1]\n\treturn ver\n\nclass Dependency_ListError(RuntimeError):\n\t\"\"\"An error with a list-like interface describing the dependency issues.\"\"\"\n\t\n\tdef __init__(self, *deps):\n\t\tsuper(Dependency_ListError, self).__init"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tif len(version)==0:\n\t\treturn 1\n\telif version == 1:\n\t\treturn 2\n\telif version < 3:\n\t\treturn 3\n\telif version < 4:\n\t\treturn 4\n\telif len(version)<2:\n\t\treturn 5\n\treturn 1\n\n_version_regexp = re.compile(\"([A-Za-z0-9]+)(\\.[A-Za-z0-9]+)*\", re.I)\n_version_identifier_pattern = re.compile(\n\t\t\t\t\t\t\t\tr\"^[A-Za-z"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tif version == '0':\n\t\treturn 0\n\telse:\n\t\tparts = version.split('.')\n\t\tv = 0\n\t\tif len(parts) > 0:\n\t\t\ttry:\n\t\t\t\tv = int(parts[-1])\n\t\t\texcept ValueError:\n\t\t\t\tparts.pop()\n\t\treturn v\n\n\nclass _VersionMap:\n\tdef __init__(self):\n\t\t# version numbers\n\t\tself.__version = 0\n\t\tself.__mapping = {}\n\t\tself.__default_revision = 1\n\n\t#"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tresult = ''\n\tresult = result + str(version)\n\tresult = result + str(next(version.lower() - 1))\n\tif version == '0':\n\t\tresult = result + '00'\n\t\tprint('Found zero-prefixed version...')\n\t\tsys.exit(0)\n\t\tprint(next(-1) +'is the current version.')\n\tprint('Version number ='+ result)\n\nusing System;\nusing System.Runtime.InteropServices;\n\nnamespace NetFabric.Hyperlinq\n{\n    public static partial class"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tmatch = re.match(VERSION_IDENTIFIER_RE, version)\n\tif not match:\n\t\traise ValueError(\"Invalid version number: %s\" % version)\n\tprefix = match.group(1)\n\tif len( prefix ) > 0:\n\t\tmajor = int( match.group(2) )\n\t\tminor = int(match.group(3) or 0) - 1\n\t\tif minor < 0:\n\t\t\traise ValueError(\"Invalid version number: %s\" % version)\n\t\treturn get_version(prefix) + (major,minor)"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tif version is None:\n\t\treturn None\n\tif not isinstance(version, tuple):\n\t\treturn None\n\ti = 0\n\twhile True:\n\t\tif version[i] == '0s':\n\t\t\treturn ('', '0')\n\t\ti += 1\n\treturn ('', '')\n\n\n##\n## Module-level help.\n\n_config_vars = None\n\ndef __getattr__(name, loc=None, cls=Undefined, GLOBAL=False):\n\tif _config_vars is None:\n\t\t_config_vars = config_"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tnext_number = -1.0\n\tnext_number_lower = -1.0\n\twhile True:\n\t\ttry:\n\t\t\t# Try to recover the next version.  The following line was\n\t\t\t# added by Dave Thornon:\n\t\t\tversion = version[1:]\n\t\t\tbreak\n\t\texcept ValueError:\n\t\t\t# An unknown version was requested:\n\t\t\tpass\n\t\telif not version.startswith('0.'):\n\t\t\t# It's a non-zero-prefixed version:\n\t\t\tnext_number = float(version)\n"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tif version == 0:\t\t\t\t\t\treturn VERSION_0_1\n\t# check for non-zero-prefixed version\n\tif version == VERSION_0_1:\t\t\treturn VERSION_0\n\t# check for zero-prefixed or non-zero-prefixed version\n\tif version == 0 or version == VERSION_1:\treturn version\n\traise ValueError('Unsupported version identifier: {}'.format(version))\n\ndef toInt(x):\n\tif x is None:\n\t\treturn None\n\telif isinstance(x, bytes or six.text_type):\n\t\ttry:\n\t\t"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tif (version.count('.') > 0):\n\t\tversion = '.'.join(version)\n\t\tnew, old = None, None\n\t\twhile True:\n\t\t\tif (not old) and (version.startswith(new)):\n\t\t\t\treturn new\n\t\t\tnew, old = new, old + version[:len(old)]\n\telse:\n\t\treturn None\n\treturn None\n\ndef _remove_dots(value):\n\t\"\"\" Removes dots from 'value' for any non-dots found. 'value' may be a\n\tstring, in which case"}]}
{"task_id": "62b45e23e0d4551b0392c90a", "prompt": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tif len(version_dirs) == 0 or len(version_dirs) % 4!= 0:\n\t\traise SystemExit(\"This version requires an inventory sequence (1..).\")\n\tfor v in version_dirs:\n\t\tif not os.path.exists(v):\n\t\t\traise SystemExit(\"Inventory sequence out of sync: %s\" % v)\n\t\t\n\tfiles = []\n\tfor v in version_dirs:\n\t\tassert os.path.isdir(v)\n\t\tfiles.append(v)\n\t\tfor i in range(4):\n"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tif len(version_dirs)!=len(VERSION_LIST):\n\t\traise Exception(\"Did not understand array of version numbers.\")\n\t\n\tversion = 1\n\tcontent_digests = []\n\tfor v_info in version_dirs:\n\t\tversion += 10\n\t\t# Add 1 for the version number to allow for future versions too\n\t\tif v_info>version:\n\t\t\tversion += 1\n\t\tdirectory = os.path.join(*([v_info-1] + version_dirs))\n\t\tif not os.path.exists(directory):\n\t\t"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tversion_sequence = []\n\tfor root_dir in version_dirs:\n\t\tnew_vds = 0\n\t\tfor vdf in get_data_files_list(\"virtual-data/virtual-data-{}\".format(root_dir)):\n\t\t\told_vds = get_data_files_list(\"virtual-data/virtual-data-{}\".format(version_dir_to_root(root_dir)))\n\n\t\t\tif old_vds == []:\n\t\t\t\tnew_vds += 1\n\t\t\t\tcontinue\n\t\t\tif new_v"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tfor root_dir, _, hierarchy, filenames in os.walk(version_dirs):\n\t\tfor filename, dirnames, filenames in os.walk(root_dir):\n\t\t\tfor filename in filenames:\n\t\t\t\tif \".sha1\" in filename:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tshasum = filename[filename.rfind(\"\\\\\")+1:filename.rfind(\"/\")]\n\t\t\t\t\t\tshasum = shasum.replace(\"-\", \"\") # strip out extension\n\t\t\t\t\t\tshasum = sha1(shasum.encode('utf-8')).hexdigest()"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t# This is to test the case where an SHF has a higher parent than an\n\t# SHF that is part of a higher version number. The former case\n\t# should generate a warning, but the later case should generate a\n\t# success message.\n\t_parent_inventory_in_a_shallow_parent = {\n\t\t'5.1.0.0a2-1.5.0': '0'\n\t}\n\t_version_number_seq_of_parent = {\n\t\t'5.1.0.0b1-rc."}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t# validate_version_inventories should have done nothing if version_dirs is\n\t# a non-empty array\n\tassert version_dirs\n\tif version_dirs:\t# if v is an  array then we have an inventory\n\t\tfor version_dir in version_dirs:\n\t\t\twith version_dir.open(\"r\") as version_file:\n\t\t\t\tv = version_file.read()\n\n\t\t\tif \"Inventory\" in v:\t# if the version file has the inventory\n\t\t\t\tself.validate(v[\"Inventory\"], version_file)\n\t"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tfor i in range(len(version_dirs)):\n\t\tfor version in version_dirs[i]:\n\t\t\tself.validate_version_inventories_from_dir(version)\n\n\treturn None\n\n\ndef validate_version_inventories_from_dir(version_dir):\n\t\"\"\"\n\tRecursive function that takes in a version directory path and checks it if\n\tit is a valid version inventories.\n\t\"\"\"\n\tversion_dirs = os.listdir(version_dir)\n\tif not version_dirs:\n\t\treturn True\n"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tself.lock.acquire()\n\tfor version_dir in version_dirs:\n\t\tassert version_dir!= \"\"\n\t\tif version_dir in self.version_map:\n\t\t\traise RuntimeError, 'duplicate version numbers: %s' % version_dir\n\t\t# version_dir now contains the version number of an entry in the inventory\n\t\tself.version_map[version_dir] = 0\n\tself.log_summary('initial version is %d' % self.version)\n\tself.lock.release()\n\n\tself.is_root = True"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tfor pver in version_dirs:\n\t\t# For now, only check for content hashes in each entry in the\n\t\t# version directory, but make the logic more tidy in this way,\n\t\t# so that, if the version string is out of date, then no\n\t\t# comparison is needed - we might want to support\n\t\t# versioning as a directory\n\t\tpverstr = pver[1]\n\t\tif os.path.isfile ( pverstr ) :\n\t\t\thashes = set ( list ( read_file ( pverstr )"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tif not isinstance(version_dirs, list):\n\t\traise UnrecognizedVersion(version_dirs, type(version_dirs), 1, 3)\n\n\t# Get the root inventory\n\troot = self.root_inv\n\n\t# Find any version that is greater than or equal to the version we want\n\tfor index, version_dir in enumerate(version_dirs):\n\t\tif re.match(\"^%d$\" % index, path.basename(root.path)) is None:\n\t\t\t# Found content with a later version\n\t\t\tversion_dirs[index] ="}]}
{"task_id": "62b45e21e0d4551b0392c8ed", "prompt": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tm = re.match(r\"^[\\w\\d]+([\\-\\s]+)$\", path)\n\tif not m:\t# OCFL\n\t\treturn 'root'\n\telif '0=*' in m.groups():\n\t\treturn m.groups()[0]\n\treturn 'object'\n\ndef make_string(s):\n\tdef make_text_safe(s,*l):\n\t\tt = {}\n\t\tfor k, v in sorted(l, key=get_sort_key):\n\t\t\tif s.startswith(k):\n"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\treturn \"root\"\n\n\ndef find_object(path):\n\tfrom ocflide import ocflparser\n\n\treturn ocflparser.parseobject(os.path.join(path, \"0\"))\n\n\ndef remove_object_dir(path):\n\tfrom ocflide import ocflparser\n\n\tfor item in os.listdir(path):\n\t\tif os.path.isdir(path+item):\n\t\t\tos.remove(path+item)\n\treturn\n\n\ndef remove_object(path):\n\tfrom"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\ttry:\n\t\t# Get the root\n\t\tvol = _open_volume(volume='0=*', path=path)\n\t\tself.vol = vol[1]\n\tfinally:\n\t\t_close_volume(vol)\n\treturn self.vol.get_info().get_type()\n\ndef find_file_info(path, start_index, end_index):\n\t\"\"\"\n\tFind file info for a given start (inclusive) and end (inclusive) indexes\n\n\tReturn a tuple where start is an integer (>=0) and end is an integer"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tocl_object = ocl_parse_path(path)\n\ttype = 'root'\t# Default is 'object' if ocl_object is None\n\ttry:\n\t\tif ocl_object:\n\t\t\t# Looking only at \"0=*\" Namaste files to determine the directory type\n\t\t\tfor directory, _ in ocl_object:\n\t\t\t\tif not directory.startswith('.'):\n\t\t\t\t\t# Found a directory\n\t\telse:\n\t\t\t# We are in an ocl object, not a folder, but instead a file.  We"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tpath = os.path.abspath(path)\n\troot = os.path.abspath('\\\\').split('\\\\')[-1]\n\tpath_list = os.listdir(root)\n\tif len(path_list)==0:\n\t\treturn \"root\"\n\tif len(path_list)==1:\n\t\treturn \"object\", path_list[0]\n\tpath_dir = path_list[-1]\n\tfor path_file in path_dir.split(os.sep):\n\t\tif path_file == 'file':\n\t\treturn path_"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tif re.search(\"0\\:?\", path):\n\t\treturn \"root\"\n\tif re.search(\"0\\:?$\", path):\n\t\treturn \"object\"\n\tif path[0] == \"0\":\t\t\n\t\treturn \"0=*\"\n\tif path[0] == \"0:\":\n\t\treturn \"0=*\"\n\tif re.match(\".*:[0-9]\", path):\n\t\treturn \"0=*\"\n\tif re.match(\".*:[0-9]+\", path):\n\t\treturn \"0=*\"\n\tif re."}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tpath_base = 'root' ;\n    \n\tpath_parts = path.split('/')\n\tdir_path = ''\n\tpath_type = ''\n\tif len(path_parts) > 0:\n\t\tdir_path = '0=1'\n\t\tpath_type = path_parts[0]\n\t\t\n\tif path_type == '0=0/1':\n\t\tpath_base += '/'\n\t\tpath_type = ''\n\t\t\n\tif len(dir_path) > 0 and path_type!= '':\n\t\tif dir"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tif path.endswith('.nc'):\n\t\treturn 'root'\n\telif path.endswith('.ns'):\n\t\treturn 'object'\n\telif path.endswith('.o'):\n\t\treturn 'file'\n\treturn 'UNKNOWN'\n\n######### Storage Directory Constants #################\n\n# Storage Type constants\nTYPE_DIR_OBJECT = 0\nTYPE_DIR_FILE = 1\nTYPE_DIR_NEWS = 2\n\nTYPE_STORE_NEWS = 3\nTYPE_STORE_FILES = 4\nTYPE_STORE_OBJECTS = 5\nTYPE_STORE_GROUPS = 6\n\n#"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tif not path.endswith('/'):\n\t\tpath += '/'\n\tif path.find('=*') >= 0:\n\t\treturn 'file'\n\telse:\n\t\treturn 'object'\n\n\n# Namaste allows to search within both directories from a filesystem and\n# to store file extensions. To specify a path in a Namaste directory,\n# put this in the 'filename' dictionary.\n#\n# NOTE: We check for paths with \"**\" somewhere inside to see if they\n#\t\tare namaste's files, since this is the only one"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\treturn 'root' if path=='.0.meta/0storage/' else 'object'\n\n\ndef get_file_path(path, directory):\n\t\"\"\"Looks at `path` the filename given by its OCFL type\n\tand possibly an error message to be informed if it is not\n\t\"\"\"\n\ttry:\n\t\tif find_path_type(dirname(path)) == 'file':\n\t\t\tfilename = dirname(path)\n\t\telse:\n\t\t\tfilename = path\n\t\t\n\t\t# if given type is file, assume it is Namaste"}]}
{"task_id": "62b45b396decaeff903e1001", "prompt": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t", "suggestions": [{"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\treturn( self.amend_bzparams_with_name_and_id( params, self.make_name_id( bug_ids ) ) )\n\t#def amend_bzparams_with_name_and_id(params, name_id):\n\n\t#FIXME: this is a hack to keep the bzflag not be displayed on a page\n\t#def make_bzflag( name, description, comment = None ):\n\n\t#FIXME: This function is supposed, of course, to be obsolete, however, I want to keep the bzflag\n\t#def make"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tfor id in bug_ids: \t\t\t\"\"\"\n\t\tparams = amend_bzparams(self.config.base, params, id)\n\t\t\"\"\"\n\t\tThis looks bad, but there may be some bugs where the params are already present in a bug_list.\n\t\tOther bugs should have already been done on that List.\n\n\t\tThe old parameters would be kept around for a while, and so it would be a lot of work.\n\t\t\"\"\"\n\n\treturn params\n\n\n\nclass BIZ(BaseModel):\n\t\"\"\"\n\tThis class is deprecated"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\ttry:\n\t\tbug_id = int(params['id'])\n\texcept:\n\t\tself.z_dict['bug_list']  = []\n\t\tself.z_dict['bug_list_info'] = []\n\t\tamend_bzparams_setstatus(self, 'amended')\n\t# Bugzilla bug has been amended\t\t\"\"\"\n\twhile bug_ids and bug_ids[0]:\n\t\tif params['id'] == bug_ids[0]: break\n\t\tbug_ids.pop(0)\n\t\tbug_"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tself._update_attrs()\n\n\tif bug_ids:\n\t\tbug_ids = list(bug_ids)\n\t\tself.set_bug_ids(bug_ids)\n\t\tparams['amendment_ids'] = ','.join(bug_ids)\n\t\tparams['update_bug_id'] = 'updated'\n\n\treturn self._send_request('BugzillaUpdate', params)\n\n\ndef amend_comment(self, params, bug_ids):\n\tamend_comment = self._get_bug_option('amendcomment"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\ttry: issue_revisions_of_bugs(params, bug_ids, 'update', {})\n\texcept: pass\n\n\t# TODO: This is a workaround for #2444.\n\t# Make sure not to overwrite the list of problem revisions\n\t# since the bug_ids may have a problem subdomain.\n\tfor bad_part in product(bug_ids[:1], bug_ids[1:]):\n\t\ttry: bug_id = params['revision_id'].replace(\n\t\t\tr'', bad_part).replace('-', '').lower()\n\t\texcept"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tbugid = str(bug_ids[0])\n\t# print(bugid)\n\tif bugid == 'None':\n\t\treturn None\n\t# bugid = str(bug_ids[0])\n\t# print(bugid)\n\t# if bugid == 'None':\n\t# \treturn None\n\t# else:\n\t# \tself.bugid = bugid\n\t# \tf = bz_open(self.bugid)\n\t# \tself.params = bz_get_params(f)\n\t# \tbz"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tparams.update({ 'project': self['project'], 'bug_id': bug_ids[0] })\n\tparams.update({ 'name': self['name'], 'description': self['description'] })\n\tparams.update({ 'project_id': self['project'] })\n\treturn params\n\n\ndef bugzilla_parameters(self, params, bug_ids):\n\t\"\"\"\n\tGet the parameters for the params for the particular bug.\n\t\"\"\"\n\tparams['bugnote_id'] = bug_ids[0]\n\tparams['description'] = str(bug_ids"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tbparams= self.params.copy()\n\tfor id, aparami in params['issue_id'].items():\n\t\tif aparami == BugzillaParam.BUG_ID and id not in bug_ids: \n\t\t\tbparams.pop(id)\n\t\n\t#print('Amend', self.bug_num_prefix, len(self.bparams))\n\tself.bparams.update(bparams)\n\tself.save()\n\treturn self\n\t\n#######################################\ndef main():\n\tparser = argparse.ArgumentParser(description=\"Bugzilla"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tparams[\"bug_ids\"] = bug_ids\t\n\treturn params\n\ndef amend_bzparams_postponed_by_postponed(self, params, bug_ids):\t\"\"\"\n\tAmend the Postponed params\"\"\"\n\tparams[\"postponed_bug_ids\"]= bug_ids\t\n\treturn params\t\ndef amend_bzparams_summary(self, params, bug_ids):\t\"\"\"Amend the summary of a bug based on the postponed bug IDs\"\"\"\n\ttemp_dict = dict()\n\ttemp_list = []\n\n\tfor"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tdbfield_name = \"Bugzilla\"\n\tdbfield_value = []\n\tbugzilla = []\n\tfor field_name, field_value in zip(sorted(DEBUG_FIELD_ORDER), params):\n\t\tdbfield_name += \"-\" + field_name\n\t\tdbfield_value.append(field_value)\n\t\n\tfor bug_id in bug_ids:\n\t\tif not bug_id in bugzilla:\n\t\t\tbugzilla.append(bug_id)\n\t\n\tbugzilla_sort = sorted(bugzilla,\n\t\tkey=lambda"}]}
{"task_id": "62b4567ed7d32e5b55cc83d9", "prompt": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 1.40 GiB (GPU 0; 22.16 GiB total capacity; 19.06 GiB already allocated; 955.88 MiB free; 20.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b4567ad7d32e5b55cc83af", "prompt": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(description='Use Python\\'s argument completion tools to generate completions.')\n\t(options, args) = parser.parse_args(*arguments)\n\n\tif args:\n\t\tparser.error(\"You must specify a script file if you want to use any other method.\")\n\t\tparser.error(\"For details and a list of available methods, see the online help page in the online documentation. For basic help on how to\"\n\t\t\"use your program please see {script_name}\")\n\t\tparser.exit()\n\n\twith open(options.filename) as in"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(name = os.path.basename (sys.argv [0]), description = sys.argv [1])\n\t\n\t# We don't want to print to stdout like we should, so we add our own custom print function \n\tdef usage(self, *args):\n\t\t# If called with no arguments, print usage message and terminate\n\t\tif len(args) == 0:\n\t\t\tprint(\"usage : \" + self.description + \"\\n\")\n\t\t\tprint(\n\t\t\t\t\"  -c, --config       Read configuration from config"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\targument_parser = ArgumentParser(prog=__name__, description=__doc__, epilog=__epilog__)\n\targument_parser.add_argument('files', nargs='*', help='command file paths/filenames (strings)')\n\targument_parser.add_argument('files_wildcard', nargs='*', help='command files with wildcards (strings)')\n\targument_parser.add_argument('version', action='store_true', help='show program version')\n\targument_parser.add_argument('output_path', type=str, help='Directory to which dumps will be"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\targument_parser = ArgumentParser(add_help = False)\n\n\t# Set up the basic parser\n\targuments_string = ''\n\targuments_string =''.join(arguments)\n\n\t# Parse out the options and arguments\n\tif len(arguments) > 0:\n\t\targuments_string = arguments_string.replace(' ', '')\n\t\targuments_string = arguments_string.replace('\\n', '')\n\t\targuments_string = arguments_string.replace('\\t', '')\n\t\targuments_string = arguments_string.replace('\\r', '')\n\n\t\t"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t# Default argument parser\n\tparser = ArgumentParser(formatter_class=RawTextHelpFormatter, add_help_option=False, prog=\"wsd-snmp\", description=\"\"\"Show SNMP objects from SNMP DNEs\\n\\n\"\"\")\n\n\t# Set up subparsers\n\tparser_smi = subparsers.add_parser('smi', help='Dump SMI')\n\tparser_smi.add_argument('smi-name-or-oid', nargs='?', default=DEFAULT_SMI_NAME_OR_OID,\n\t\t\t\t\t\t\t\t\tmetavar='O', help='SNMP object identifier')\n"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(usage=__usage__,\n\t\tdescription=__doc__,\n\t\tadd_help=True,\n\t\t formatter_class=RawDescriptionHelpFormatter)\n\tparser.add_argument(*arguments,help=__help__)\n\ttry:\n\t\t# In theory this test could fail if the argument parser is invoked by\n\t\t# argparse.SUPPRESS with a non-None parse_known_args.  However with that\n\t\t# test it should not happen there.\n\t\targs = parser.parse_known_args()[0]\n\t\tassert args"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(usage=\"usage: python %s\" % sys.argv[0])\n\tparser.add_argument(\"--profile\", action=\"store_true\", help=\"Profile the python version in the logfile\")\n\tparser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug level messages in the logfile\")\n\tparser.add_argument(\"--config\", default=None, help=\"Configuration file (or directory) to use in the script (default %s)\")\n\tparser.add_argument(\"--log\", default=None, help=\"Log level for the script ("}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(\n\t\tusage\t= 'python %prog [options] [arguments]',\n\t\tformatter_class=RawTextHelpFormatter,\n\t\tadd_help_option=True,\n\t\tdescription=__doc__)\n\n\ttry:\n\t\tglobal _argument_group_count\n\t\tparser.add_argument_group(command_line_argument_group)\n\t\tglobal _options\n\t\t_options = parser._get_default_options()[1]\t# pylint: disable=C0103\n\t\tglobal _arguments\n\tfinally:\n\t"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(usage=USAGE, description=DESCRIPTION)\n\t\n\tgroup_flags = parser.add_argument_group(\"General flags\")\n\tgroup_flags.add_argument(\n\t\t\"--version\",\n\t\taction=\"version\",\n\t\tversion=\"{} {}\".format(__version__, VERSION),\n\t)\n\n\tgroup_flags = parser.add_argument_group(\"Generic Flags\")\n\tgroup_flags.add_argument(\n\t\t\"-p\",\n\t\tmetavar=\"PATH\",\n\t\ttype=str,\n\t\thelp=\"Specify a path to"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(prog=os.path.basename(sys.argv[0]), description=__description__, epilog=epirelog, formatter_class=RawTextHelpFormatter, add_help_option=False, add_version_option=False)\n\tparser.add_argument('command', metavar='COMMAND', nargs='?', type=str, help='The command that should be executed')\n\tparser.add_argument('--version', action='version', version='%(prog)s {}'.format(__version__))\n\n\t# add any additional arguments as normal\n\tfor argument in"}]}
{"task_id": "62b45679d7d32e5b55cc83a9", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn \" \".join(parser.dest.flags)\n\n\ndef has_option_like(option_type):\n\t\"\"\"\n\tReturn True if the option_type argument is a type that has a get_XXX() method, and False\n\tif it's a string argument type.\n\t\"\"\"\n\treturn hasattr(option_type, '__get__')\n\n\ndef is_argument_like(cls):\n\t\"\"\"\n\tGiven a class instance, ensure that its __init__() was called.\n\t\"\"\"\n\tflag = has_option_like(cls.__init__) or inspect"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn parser.add_argument + \"--\" + parser.process_group_args\n\n# -*- coding: utf-8 -*-\n\nimport os\nimport sys\n\n# -- Python 3/E-book configuration -----------------------------------------------------\n\nif sys.version_info < (3, ):\n    from email.utils import sendmail\n    from email import settings\nelse:\n    # In Python 2 onwards, default to html emails, and the other way around.\n    try:\n        import email.utils\n    except"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tparser.add_argument('--noinput',\taction='store_true',\t\n\t\t\t\tmetavar=('NO', 'FALSE', 'NONE')) # NOTE: NO|FALSE|NONE is the same as None\n\tparser.add_argument('-O', '--output', metavar = ('OUTPUT', 'FILE'),\n\t\t\t\t\thelp = \"Output filename (default: %(default)s)\")\n\tparser.add_argument('-H', '--histogram', type = str,\n                        metavar = ('INTERVAL_"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tf = functools.partial(f2py_ast.default_argument_parsers[parser.dest].get_flags_string,\n\t\t\t\t\tparser.action)\n\treturn ''.join(map(lambda x:''+ x, f(parser)))\n\n\ndef get_args(parser, arg, defaults=None, var=None, default=None,\n\t\t\t\trequired=False, nargs=None,\n\t\t\t\tnamespace=None, type=None,\n\t\t\t\tvalue=None, varname=None):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn parser_flags(parser.prog)\ndef parser_flags(prog, progdict=None):\n\tif not progdict:\n\t\tprogdict = Program(prog)\n\tflags = {'-': 0}\n\tflags['*'] |= opt.flag_no_default | opt.flag_version | opt.flag_quiet\n\tflags['-f'] = flags['*-'] |= opt.flag_verbose\n\tif opt.flag_debug: flags['-v'] |= opt.flag_verbose\n\tif opt.flag_dry_run: flags['-n']"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = \"\"\n\tfor option, argument in vars(parser.option_list).iteritems():\n\t\tif argument:\n\t\t\tflags += parser_flags(option.dest) if hasattr(option, 'dest') else '\\n\\t*'\n\t\telse:\n\t\t\tif hasattr(option,'short_alt'):\n\t\t\t\tflags +='(no arg for %s)'% option.short_alt\n\t\t\tif not options[option]:\n\t\t\t\tflags += '???'\n\treturn flags\n\n\ndef cli_options_for_file(option_"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tparser.add_argument( '--verbose', '-v', metavar='LEVEL', action='count',\n\t\t\t\tmetavar='NUMBER', dest='verbosity', default=logging.INFO,\n\t\t\t\thelp='log '\n\t\t\t\t'level (default: %(default)s) '\n\t\t\t\t'specify log level')\n\tparser.add_argument('--log-file',\n\t\t\t\tdefault=None,\n\t\t\t\taction=ActionLogger,\n\t\t\t\tmetavar='FILENAME',\n\t\t\t\tdest='logfile',\n\t\t\t\thelp='a file for logging output')\n\n\treturn parser\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn parser._prog\n\ndef usage(parser):\n\t\"\"\"\n\tReport the usage of command line arguments.  If called normally on the\nprogram, it returns \"\".  If called from inside argparse.ArgumentParser.parse, it\nreturns \"\".  Each line of output is terminated with a line feed character ('\\\\n').\n\nUsage is intended for the most common usage of the -u/--user flag and to the common\nsystem usage.\n\t\"\"\"\n\tinfo(parser.usageMessage)\n\ndef version(parser):\n\t\"\"\"\n\tReport the version of program being parsed\n\n\t"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tif getattr(parser, 'parser_name', None):\n\t\treturn parser.parser_name,''.join(sorted(getattr(parser, 'parser_name', ['', ]).flags)), parser.default\n\treturn parser.default,''.join(sorted(getattr(parser, 'args', [None])[0].flags)), parser.required\n\n\ndef set_parser_default(*args):\n\t\"\"\"\n\tSet the parser_name attribute of this parser so that it shows up.\n\n\tThis may be necessary if you wish to re-use a parser or parse again."}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn ''.join(parser.option_list)\n\n\ndef main(argv=None):\n\tprogname = os.path.basename(argv[0])\n\tusage = progname +'[options] [<input> <output>]\\n'\n\topts = docopt_help(usage, version=DOCOPT_VERSION)\n\t\n\tif argc == 1:\n\t\tparser = argparse.ArgumentParser(usage=usage, description=DOCOPT_DESCRIPTION)\n\telse:\n\t\tparser = argparse.ArgumentParser(description=DOCOPT_DESCRIPTION)"}]}
{"task_id": "62b45665d7d32e5b55cc8365", "prompt": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\toptions = {}\n\tfor i in range(len(sys.argv)):\n\t\tif sys.argv[i].startswith('-'):\n\t\t\topt, arg = sys.argv[i][1:].split('=',1)\n\t\t\ttry:\n\t\t\t\toptions[opt] = parse_subparser_and_exit(arg, sys.argv[i+1:])\n\t\t\texcept SystemExit as e:\n\t\t\t\tif e.code!= 0:\n\t\t\t\t\tsys.exit(e.code)\n\t\t\telse:\n\t\t\t\tprint(i+1)\n\t\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=__name__, description=\"\"\"\n\tGet help for a specific command.\"\"\"\n\t)\n\tparser.add_argument(\n\t\t\"command\", \n\t\tmetavar=\"<command>\", \n\t\thelp=\"what command to use\", \n\t)\n\tparser.add_argument(\n\t\t\"-f\", \n\t\t\"file\", \n\t\tmetavar=\"<file>\", \n\t\thelp=\"which file to use in the script; required\", \n\t)\n\tparser.add_argument(\n\t\t\"-h\", \n\t\t\"help\","}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser = argparse.ArgumentParser(description='', add_help=False, epilog=help_text)\n\tparser.add_argument('opts', metavar='', help='the command-line options')\n\tfor k, v in unparsed_arguments.items():\n\t\tparser.add_argument('-' + k, '--' + k, metavar='', choices=v, help=help_text.format(k=k))\n\treturn parser.parse_args()\n\ndef main():\n\ttry:\n\t\t(opts, args) = parse_arguments(sys."}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\targuments = {}\n\tposargs = unparsed_arguments.get(\"argument_map\", None)\n\n\tif posargs is not None:\n\t\tposargs = parse_posargs(posargs)\n\n\t\t# if there are no Positionals, use the local default\n\t\tif \":\" not in str(posargs):\n\t\t\t# if there is no default, the argparse will pick up the wrong defaults\n\t\t\t print('ERROR: %s does not have a valid Positionals: %r' % (program_name, posargs))\n\t\telif \":\" in str"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers()\n\n\t# Setup command-line argument choices.\n\tsubparsers.required = False\n\tsubparsers.dest = \"global\"\n\tglobal_arguments = getattr(subparsers, \"global\")\n\tglobal_arguments[global_arguments['help']].add_argument('command')\n\tglobal_arguments[global_arguments['help']].add_argument('-h', '--help', action='help', help=\"show program's help\")\n\tglobal_arguments[global_arguments['help']].add"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser_result = {}\n\t\n\tfor name, value in unparsed_arguments:\n\t\tif name == \"arguments\":\n\t\t\tfor arg in value:\n\t\t\t\targ_parts = arg.split()\t\n\t\t\t\targ, required_arg_num = arg_parts[:2], len(arg_parts) - 1\n\t\t\t\tname_parts = [cmd_name]\n\t\t\t\t#if \"required_arg\" in arg_parts:\n\t\t\t\t#\tname_parts.append(str(required_arg_num))\n\t\t\t\t#else:\n\t\t\t\t#\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=__title__, description=__description__, formatter_class=RawDescriptionHelpFormatter)\n\tparser.parse_args(*unparsed_arguments)\n\treturn parser.namespace\n\ndef parse_common_arguments(args=None):\n\t\"\"\"\n\tParses args and returns the dictionary of config options.\n\t\"\"\"\n\tparser = argparse.ArgumentParser('Dell System Discovery', add_help=False)\n\thelp_str = __title__ +'[--help]'\n\thelp_str += '\\t\\thide-line options:'\n"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\targs = {}\n\tparser = ArgumentParser(description=__doc__)\n\tparser.add_argument(\"command\",\n\t\t\t\t\t\t action=\"store\", \n\t\t\t\t\t\t dest=\"command\",\n\t\t\t\t\t\t help=\"Command to be invoked\")\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\tif subparsers.command =='set':\n\t\targs['set'] = subparsers.add_parser('setting', help=\"set key/value\")\n\telif subparsers.command == 'global':\n\t\targs['global'] = parser.add_argument_group('global')\n\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser = argparse.ArgumentParser(usage=__usage__,add_help=False)\n\tparser.prog = \"setup\"\n\tparser.add_argument( \"-b\", metavar=\"PATH\", help=\"binary directory containing ephy installation directories (see [Install dirs])\" )\n\tparser.add_argument( \"-d\", metavar=\"PATH\", help=\"directory where the setup script files will be written\" )\t\n\tparser.add_argument( \"-u\", metavar=\"URL\", help=\"repository of setup script\" )\n\tparser.add_argument( \"-e\", metavar=\"DIR"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=\"run\")\n\tparser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Enable verbose output on stdout.\")\n\tparser.add_argument(\"-d\", \"--debug\", action=\"store_true\", help=\"Enable debugging output on stdout.\")\n\tparser.add_argument(\"-h\", \"--help\", action=\"store_true\", help=\"Prints this message.\")\n\targuments, unknown = parser.parse_known_args(\n\t\t__doc__\n\t)\n\n\tif arguments.help:\n\t\tparser.print"}]}
{"task_id": "62b45665d7d32e5b55cc8364", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tparser_results = []\n\n\t# Get the set of arguments that have been passed in\n\tsubparsers = {}\n\tfor arg_id, subparser in enumerate(subparsers):\n\t\tif hasattr(subparser, \"add\") and hasattr(subparser, \"parse\"):\n\t\t\tsubparser_results.append((arg_id, subparser))\n\n\tremaining_args = (s for s in unparsed_arguments if not s.startswith('--'))\n\tif not remaining_args:\n\t\treturn (None, [])\n\n\tparser = argparse.ArgumentParser(description"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tif subparsers is None:\n\t\tsubparsers = argparse.ArgumentParser.add_subparsers()\n\tsubparsers.add_parser(subcommand.get_parser(),\n\t\t\t\t\t\t\t   help=subcommand.get_parser_help())\n\t# subparsers.add_parser(SubCommandParser(), action_func=handle_arg\n\tfor name, argument_handler in subparsers:\n\t\tfor argument_arg in argument_handler.parse_args(unparsed_arguments):\n\t\t\targs.append(argument_arg)\n\t\t\t# check if this subcommand's"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tif not unparsed_arguments or not subparsers:\n\t\treturn result, args[:]\n\t\n\t__global_arguments = globals()\n\t\n\tparsed_arguments = {}\n\t__unparsed_subparsers__ = [unparsed_args[0].__class__ for unparsed_args in unparsed_arguments]\n\tparsed_subparsers = []\n\tfor unparsed_arg in __unparsed_subparsers__:\t\n\t\tif unparsed_arg.__module__ == __global_arguments.__module__:\n\t\t\tparsed_subparsers.append(unparsed_arg)\n\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tresult = {}\n\tfor subparser in subparsers.keys():\n\t\tresult.update(command_line_options.__dict__[subparser])\n\t\tresult.update(unparsed_arguments.__dict__[subparser])\n\treturn result\n\n#-------------------------------------------------------------------------------\n\nclass _Subparser:\n\t'''\n\tInternal baseclass for defining a subparser on a line or a command string.  The argument parsing\n\tis done by a call to argument_parser on an argparse object.\n\t'''\n\n\t__metaclass__ = argparse.ArgumentParser\n\n\tdef __init__("}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\taction = subparsers.pop(0)\n\tresult = {}\n\tif 'namespace' in subparser_kwargs:\n\t\tresult.update(subparser_kwargs.pop('namespace'))\n\telif subparser_kwargs:\n\t\tresult.update(subparser_kwargs)\n\treturn result, action_kwargs[action]\n\ndef parse_action(s, subparsers):\t\t# Parse arguments from the following action:\n\ts = s.replace('(subparsers)', '')\n\t# s in form:\n\t#    (<Action>.subparsers.<Action"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tparser = argparse.ArgumentParser(add_help=False, prog=\"subparser\")\n\tparser.add_argument(\"--config\", \"-c\", nargs=\"\", help=\"Name of the file that contains the config file\")\n\tparser.add_argument(\"--repository\", \"-r\", help=\"Repository name to read config from\") \n\tparser.add_argument(\"--no-logging\", action=\"store_true\", default=False, help=\"Don't print log messages\")\n\n\t# Process remaining arguments\n\tresult = OrderedDict()\n\tfor name, arg in subparsers:\n\t\tif hasattr(arg,"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tsubparser_defaults = OrderedDict()\n\tfor _subparser, _subparser_arguments in subparsers.items():\n\t\tsubparser_defaults[_subparser.name] = _subparser_arguments.copy()\n\n\tfor subparser_name, subparser in subparsers.items():\n\t\tfor arg, action in subparser.parse_args(unparsed_arguments):\n\t\t\tif action not in subparser_defaults and arg.name == subparser_name:\n\t\t\t\tsubparsers[action].add_argument(arg.default)\n\t\t\t\tsubparser_defaults"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tparsers = subparsers\n\tkwargs = {}\n\tremaining_non_optional = subparsers\n\tfor i in range(min(len(unparsed_arguments), len(remaining_non_optional))):\n\t\ta = argparse_argument_parser().parse_args(unparsed_arguments[i])\n\t\tremaining_non_optional = a.remaining_non_optional\n\t\tif a.has_arg:\n\t\t\tkwargs[a.name] = a\n\treturn kwargs\n\n\ndef get_subparser(name):\n\t# check for subparsers\n\tif"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tlogger = logging.getLogger(\"hubspot.cli.action\")\n\tlogger.debug(\"In function subparser(): start\")\n\taction = None\n\tresults = []\n\tremaining_arguments = []\n\tfor argument in unparsed_arguments:\n\t\tnext_arg_name = \" \".join(re.escape(x) for x in argument.lower().split(\"=\") if len(x) > 1)\n\t\tnext_arg_name = next_arg_name.strip()\n\n\t\targ_class = subparsers.get(next_arg_name, argparse"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tparser = subparsers.add_parser(shlex.split(unparsed_arguments[0]))\n\tparser.add_argument(\"repo_id\", help=\"Repository [org] to list subsubparsers on\")\n\tparser.add_argument(\"--list-subparsers\", help=\"List subparsers?\")\n\targs = parser.parse_args(unparsed_arguments[2])\n\treturn vars(args), tuple(args[:args.index(\"--\")] if args.index(\"--\") else [])\n\n\ndef get_org_cmd(cmd):\n\t\"\"\"Find the correct subparser to"}]}
{"task_id": "62b45665d7d32e5b55cc8363", "prompt": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparsers = []\n\tparsers.append(parser1())\n\tparsers.append(parser2())\n\tparsers.append(parser3())\n\tparsers.append(parser_infix())\n\tparsers.append(parser_infix2())\n\tparsers.append(parser_infix3())\n\tparsers.append(parser_infix4())\n\treturn (parsers,)\n\t\t\ndef make_parser(p1):\n\treturn _make_parser(p1)\n\ndef _parse_args():\n\t'''Parse command line arguments'''\n\t# Create a parser and"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\ts = Stack()\n\tp = Group(s, \"Parser\")\n\tp = Fiddle(p, s, \"Parser\")\n\tp = Pgrp(p, s, \"Parser\", s.parser_help, \"1\")\n\tp = Fiddle(p, s, \"ParserError\",\n\t\tformatter='pdoc',\n\t\tlonghelp='Python Error Handler',\n\t\tstyle='long',\n\t)\n\tp = Label(p, \"Parser-specific options:\", s)\n\tp = Pgrp(p, s, \"Parser\")\n\tp"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser = argparse.ArgumentParser( add_help=False, description=\"\"\"\n\t\tThe main command parser for all interactive commands.  This is only used for commands that\n\t\tshould be run from interactive programs or from interactive commands that should be executed\n\t\ton startup.\"\"\", prog_name=\"InteractiveCommandParser\")\n\tparser_addsubparsers(parser)\n# \tparser.set_defaults(func=command_func) # this is needed even if command_func is a separate type\n\n\tparser.add_argument(\"-r\", \"--remote\", action=\"store\", default=None"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tglobal parser, subparsers, aliases, option_list\n    parser = argparse.ArgumentParser(\n\t'-'\n\t'--%s %s\\n\\\n\t'%(sys.version[:3]) +\n\t'(1) '.join(['%s [^%s]' % (alias, opt) for opt,alias in option_list])\n\t'\\n\\\n\tusage: %s [options]' % (sys.argv[0], __version__)\n    )\n    subparsers = parser.add_subparsers(metavar='--help',"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Find the wordset, split the wordbook-files into pieces for parsing. \")\n\tparser.add_argument('--n-chunks', '--nb-chunks', required=True, type=int, metavar='int',\n\t\t\t\t\thelp=\"Maximum chunk number (Default value for %default option if not specified) \")\n\treturn parser.parse_args()\n_parsers = make_parsers()\n\n\ndef add_parser_arguments(parser):\n\t\"\"\"Add all subcommand-specific arguments to the parser.\"\"\"\n\tsubparsers = parser"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tprog='gff3',\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t\tdescription='Convert GFF3/GFF3C/GFF3BI GFF3DB to GFF3I format.')\n\n\tparser.add_argument('fasta', type=str)\n\tparser.add_argument('fastq', type=str)\n\n\tparser.add_argument('-s', '--startseq', type=int)\n\tparser.add_argument('-t', '--threads', type=int,"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\ttry:\n\t\tclass ParserClass(object):\n\t\t\tdef __init__(self, **kwargs):\n\t\t\t\tself.__class__.__dict__.update(kwargs)\n\t\treturn ParserClass,ParserClass\n\texcept ValueError:\n\t\traise ValueError(\"Argument must be a dictionary\")\n\ndef find_parser(parser, arg):\n\ttry:\n\t\tparser = find_parser_class(parser)\n\t\tif parser:\n\t\t\treturn parser.find(arg)\n\t\telse:\n\t\t\treturn None\n\texcept ValueError:\n\t\tpass\n\ndef"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t__all__ = []\n\n\t@staticmethod\n\tdef _get_parser(arg=None, short=True, prog=None, noclasses=False):\n\t\t# Get an argument parser.\n\t\tif arg is None:\n\t\t\targ = argument(prog, short=short, **options.get('parser', {}))\n\n\t\tif not arg in parsers:\n\t\t\tif not short or noclasses:\n\t\t\t\tprinter.parser.print_usage()\n\t\t\t\tsys.exit(1)\n\n\t\t\tparsers[arg] = OptionParser(prog)"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser_classes = tuple([cls.__name__  for cls in __subclasses__])\n\tparser_namespace = dict(__subclasses__=[\n\t\t{\n\t\t\t'parser': ('parser',),\n\t\t\t'namespace': 'ParserNamespace'\n\t\t},\n\t\t{\n\t\t\t'parser': ('Parser',)\n\t\t},\n\t\t{\n\t\t\t'parser': ('SimpleXMLParser',)\n\t\t}])\n\tparser_dict = {\n\t\t# standard types\n\t\t'string': parser_classes[1],\n\t\t'int"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tname        = NameParser()\n\tdescription = DescriptionParser()\n\tauthors     = AuthorsParser()\n\tversion     = VersionParser()\n\textensions  = NumpyParser()\n\ttags        = NumpyParser()\n\n\t# The only remaining special parser we keep around\n\textensions.add_parser('parser.parser_for_all_extensions.%s' % (os.pathsep), extensions)\n\n\textensions.set_defaults(\n\t\tfunc=extension_parser, parser=extensions,"}]}
{"task_id": "62b438ba66fea644fe22cca2", "prompt": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 1.40 GiB (GPU 0; 22.16 GiB total capacity; 19.06 GiB already allocated; 955.88 MiB free; 20.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b438b666fea644fe22cc78", "prompt": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(usage='%prog name [options] command-line-arguments', add_help_option=False, description=\"\"\"The simplest way to start a script:\n\n1.\t# start a python script with the name of the script and arguments\n2.\t# call the script executable passing as args, return an instance of the application\n3.\t# start a script using the name of the script using the args as its arguments, and return its instance\n\\ingroup Scripting\n\"\"\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n\tparser.add_argument(\"-"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter, add_help=False)\n\tparser.add_argument(\"--test\", \"-t\", type=int, choices=range(len(sysargs)), help=\"Test number.\")\n\t# --help is default and not overrriden\n\tfor argument in sysargs:\n\t\ttry:\n\t\t\tparser.add_argument(\"--%s\" % argument, type=argument, default=\"0\", help=\"{} argument.\".format(sysargs[argument]))\n\t\texcept:\n\t\t\tparser.add_"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(prog='zrevit-app', description=__doc__)\n\treturn parser.parse_known_args(*arguments)\n\n\nif __name__ == '__main__':\n    main()\n\n/***************************************************************************//**\n * @file\n * @brief EFR32BG1P_AF_H field definitions\n * @version 5.8.0\n *******************************************************************************\n * # License\n * <b>Copyright 2019 Silicon Laboratories Inc. www.silabs.com</b>\n *******************************************************************************\n *\n * SPDX-License-Identifier: Zlib\n *\n"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(formatter_class=RawDescriptionHelpFormatter)\n\tparser.add_argument('--no-version',  action='store_true',  help=_('Not output the \"version\" column')),\n\tparser.add_argument('--version',  action='store_true', help=_('Displays the version column')),\n\targs = parser.parse_args(arguments)\n\n\tif not args.version and not args.no_version:\n\t\tprint('\\nVersion information is not available for this version of Salt')\n\n\tglobal version\n\tversion = [version"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(usage=__doc__, add_help=True, description=__doc__).add_argument\n\tif arguments:\n\t\tgroup_arg = parser.add_argument_group(title='Options')\t\t\t# <option 1> <option 2>...\n\t\tgroup_arg.add_argument('input', help=\"Input file to test: <file names are given before>\", metavar='<file1>,<file2>,...')\n\t\tgroup_arg.add_argument('-v', '--verbose', help='Show verbose debugging info', action='store_true"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(usage=usage)\n\n\t# The following arguments use argparse to parse the given arguments as well as\n\t# the positional arguments required by some methods in the class.\t It is hoped that\n\t# their use would break older scripts and thus don't require changes here.\n\t# However, it could easily introduce incompatible options to support older scripts\n\t# (e.g. specifying \"name=False\" instead of \"name\").\n\n\tparser.add_argument('-V', '--version', action='version', version='Version \"%s\"'\n\t\t\t\t\t\t     "}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(usage=__doc__,add_help_option=True)\n\tfor argument in arguments:\n\t\tparser.add_argument(argument)\n\n\treturn parser\n\ndef main():\n\tArguments(prog_name='run_code')\n\n\tresult = 'Unknown Error'\n\ttry:\n\t\tif not os.path.exists(os.path.join(args.data_dir, args.csv_file)):\n\t\t\t# Create an empty file with only the csv_file name\n\t\t\twith open(args.csv_file, '"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(usage=\"\"\"\nExamples:\n  python -c dog.py <command>\n\n  python -c dog.py -r Bagman -v SirCad -m Mohammad_Cadaveri <job:jobid>\"\"\", version=\"2.0\", error_out=False)\n\n\t# Add each item in the list to the parser once\n\tparser.add_argument(\"-r\", action=\"store\", dest=\"jobids\", type=str, required=True, help=\"job ids to run\")\n\tparser.add_"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(prog=os.path.basename(sys.argv[0]),\n\t\tversion=__version__,\n\t\tdescription=\"Pawn: a python tool for running pawns\")\n\n\t# add options from the argument list, which has the form of an argument:\n\t#\t\t-a\tAll args are arguments.\n\t#\t\t-p\tSpecify a configfile.\n\t#\t\t-h\tPrint command line usage.\n\t#\n\t#        -l\tList arguments.\n\tparser.add_argument"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t# Get the arguments used to specify the arguments to the script\n\targument_parser = ArgumentParser(description='Launch a new game server', epilog='''\n\t\t# We are using the \"-h/--help\" flag for a bit of help!\n\t\t# We are using a slightly modified form of this with respect to the description of the command, and the\n\t\t# first one was added. This way the argument parser can get the actual description, not just the command-line.\n\t\t# You can then append --help to get more help on this command.\n\n\t\t"}]}
{"task_id": "62b438b666fea644fe22cc72", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn parser.flags\n\n\t\nclass TestParser(BaseParser):\n\t\"\"\"\n\tTest-only class for keeping track of options and parameters accepted by the parser\n\t\"\"\"\n\tdef __init__(self, program=None, version=None, description=None, usage=None, options=None, arguments=None):\n\t\t\"\"\"\n\t\tConstructor\n\t\t\"\"\"\n\t\tsuper(TestParser, self).__init__(program, version, description, usage, options, arguments)\n\t\targuments['options'] = options\n\t\tfor name in self.option_keys(): options"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tparser.add_argument(\"-Y\", \"--yes\", action=\"store_true\", dest=\"yes\", default=False,\n\t\thelp=\"display Yes or No.\")\n\tparser.add_argument(\"-n\", \"--no\", action=\"store_true\", dest=\"no\", default=False,\n\t\thelp=\"display Yes or No.\")\n\n\tdef process_options(option, value, parser, _) -> None:\n\t\t\tif option!= '-n' and option!= '--no':\n\t\t\t\traise argparse.ArgumentTypeError(f\"illegal option - expected -n or --no: {option"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = \"\"\n\n\tfor group in parser.option_groups[0]:\n\n\t\tfor name in group.flags:\n\t\t\tflags += f'  --{name.translate(parser.prefix_chars, parser.separator_chars)}  '\n\n\tif parser.description and parser.description.translate(parser.prefix_chars, parser.separator_chars, True):\n\t\tflags += f'--{parser.description.translate(parser.prefix_chars, parser.separator_chars)}'\n\telse:\n\t\tflags += '--without-description'\n\t"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = []\n\tfor i in range(0, 10):\n\t\tflag = parser.add_argument(parser.prog, short_opt, default=i, action='store_const', const=i, nargs='*', const_type='int', help='set action for argument {} by action-number {}'.format(i, parser.action_space.split('[')[-1][:-2]))\n\t\tflags.append(action_flags(parser, flag))\n\treturn '|'.join(flags)\n\ndef action_flags (parser, action):\n\t\"\"\"\n\t"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tif not hasattr(parser,'metavar'):\n\t\traise ValueError('cannot determine flags without a metavar')\n\treturn parser.metavar.re.pattern\n\n\ndefault_parsers = {\n\t'dict': (dict_action_t, dict_action_common_actions_t, parse_struct),\n}\n\n\ndef action_by_name(name):\n\t\"\"\"\n\tGiven a key to an action dictionary, return a function for that action.\n\tOther names returned will be converted to their actual names, so\n\twith the help of a registry.  See"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t# Flags for the main argument parser as a string.\n\tflags = ['-x', '-o', '--option', '--long', '--default']\n\tflags += option_table.cli_flags(parser)\n\t# Flags for the subparsers.\n\tflags += subparser_table.cli_flags(parser)\n\n\t# Add a few common flags to allow more parser arguments and options.\n\tflags += [\n\t\t'-x', '--flag2', '--flag3', '--flag4', '--flag5', '--flag6',\n\t\t`--flag7\n\t\t--"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = list(parser.choices)\n\treturn \" \".join(filter(is_flag, flags))\n\ndef is_flag(flag):\n\treturn (flag.startswith(\"-\") and flag.endswith(\"-\") and flag!= \"no-such-option\")\n\n\ndef arg_help(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return a tuple describing the arguments we accept, parsed,\n\tand flags.\n\n\tThe return value has the same format as the args-defined type with one exception. The values\n\tof the flags attribute have the name of"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = []\n\tfor o, a in group(parser.flags, ''):\n\t\tflags.append(o and '=' + o + (''in o or '-' in o) and a or '--' + a)\n\treturn ';'.join(flags)\n\n_ARG_TYPES = (str, bool, int, float, ParserElement)\n_mutually_exclusive_groups = parser_flags(_arg_group) + (_arg_group_1, _arg_group_2, _arg_group_3, _arg_group_4)\n\ndef _get_"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tparser_type = parser.get_type()\n\tif not parser_type:\t\t# just to be safe\n\t\treturn parser_flags(parser.add_argument)\n\treturn parser_flags(parser) +'' + parser_type\n\n\n#\t\t# def find_first_user_dir():\t# XXX could cache the results for performance, but\n#\t\t# the logic here isn't quite right; it should be cached by the caller...\n#\t\t# def find_system_dir():\t\t# XXX could cache it...\n#\t\t#"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tparser.add_argument('--verbose',    type=int,    default=0,\n\t                    action='count', help='Enable verbose messages')\n\tparser.add_argument('--pidfile', type=str,      choices=('', ''), default='data/plots')\n\tparser.add_argument('--config',    type=str, metavar='PATH', nargs='+',\n\t                  "}]}
{"task_id": "62b438b566fea644fe22cc70", "prompt": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\ttry:\n\t\tif len(sys.argv) == 1:\n\t\t\tusage()\n\t\t\tsys.exit(1)\n\t\tscript_file_name = sys.argv[1]\n\t\ttry:\n\t\t\tscript = open(script_file_name, 'r')\n\t\t\tparser = pluginline.Parser()\n\t\t\tparser.read(script)\n\t\t\tcommand_line = parser.interim_command\n\t\t\tcommand_lines = command_line.split()\n\t\t\tscript_name = 'bash'\n\t\t\tscript_name"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\ttry:\n\t\targspec = inspect.getargspec( __import__('borgmatic').Borg )\n\t\tif len(argspec.args) > 0:\n\t\t\t# If the user specified multiple command-line arguments, then make the most recent one take priority\n\t\t\tif __import__( 'borgmatic').Borg > '' or argspec.args[-1] is not None:\n\t\t\t\treturn __import__( 'borgmatic').Borg.sh_complete(argspec.args[-1:]).output.strip()\n\t\treturn __import__( '"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\treturn [\n\t\t\"bash completion.sh\",\n\t\t\"bash -ex 'b=2:2'\",\n\t\t\"bash completion.sh --version | /usr/bin/bash --version\",\n\t\t\"bash b org\",\n\t\t\"bash b --verbose\",\n\t]\n\n/*\n * Copyright (C) 2015 RoboVM AB\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\tparser = optparse.OptionParser(version=\"4.0\")\n\n\tparser.add(\"--version\", action='version', version='%(prog)s'+ datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\"), version_addendum='\"')\n\tparser.add_option(\"--help\", dest=\"help\", action=\"store_true\", help=\"Print help and exit\")\n\tparser.add_option(\"--no-completion\", action=\"store_false\", dest=\"completion\",\n\t\t\t\t\t\t   help=\"Disable the default bash-completer by setting its enable"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\tparser = argparse.ArgumentParser(description=__doc__[2])\n\tparser.add_argument('--version', action='version', version='v%s' % borg_version, help=__doc__)\n\tparser.add_argument('files', metavar='', nargs='+', action=borg_files_action, help=__doc__.format(borg_files_summary))\n\tparser.add_argument('-v', '--version','--version', action='version', version='v%s' % borg_version, help=__doc__)\n\tparser.add_"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\treturn _bash_completion\n\n\ndef bash_completion_func():\t\"\"\"\nProvide a function for bash completion. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\ttry:\n\t\targparser = argparse.ArgumentParser(\n\t\t\t\t\t\t\tprog=_bash_completion.__name__.split('.')[-1] + name + '.'\n\t\t\t\t\t\t)\n\texcept:\n\t\timport traceback, traceback.print_exc\n\t\tprint(traceback.print_exc())\n\t\traise SystemExit\n\targparser.parse_"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\treturn _borgmatic_completion_script\n\n\ndef _borgmatic_completion_script(argv=None):\n\t\"\"\" Run a borgmatic's completion script.\t\"\"\"\n\tfrom frappe.core.doctype.output import format_docblock\n\n\tcmdlist = get_list(argv)\n\tif cmdlist:\n\t\tout = format_docblock(\"completion\", **cmdlist)\n\telse:\n\t\tout = \"\"\n\n\treturn out\n\n\ndef is_python_script(filename):\n\t\"\"\" Check if the file specified by filename"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\tparser = OptionParser(usage='%prog borgmatic [options]')\n\n\n\tdef get_options():\n\t\tparser.add_option(\"-q\",\"--quiet\", dest=\"quiet\", action=\"store_true\", \n\t\t\t\t\t\t metavar=\"COMMAND\", default=False, help=\"displays non-standard commands\")\n\t\tparser.add_option(\"-d\",\"--daemon\",\"daemonize\", action=\"store_true\", \n\t\t\t\t\t\t metavar=\"COMMAND\", default=False, help=\"start server in background only, not daemonized\")\n\t\tparser.add_option(\"-c\",\"--config\",\""}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\to = OptionParser(usage=\"\"\"%prog bash-completion\n\nThe borgmatic command adds completion for borgmatic command-line commands; borgmatic requires\nall command-line arguments in this borgmatic command to be complete or equivalently introspectable.\n\t\"\"\")\n\to.add_option(\"-q\",\"--quiet\", dest=\"quiet\", is_flag=True, help=\"Suppress completion\")\n\to.add_option(\"-y\",\"--non-interactive\", dest=\"non_interactive\", action=\"store_true\", help=\"Enable completion without asking for confirmation.\")"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\ttry:\n\t\targs = shlex.split( args.scriptLine )\n\texcept ValueError as err:\n\t\traise scriptError( args, err )\n\n\tscriptFilename = args.scriptFilename\n\ttry:\n\t\twith open( scriptFilename, 'r' ) as fp:\n\t\t\tcommandLineParts = fp.read().split('' )\n\texcept OSError, e:\n\t\tif e.errno == 32:\n\t\t\tprint >>file, 'Command-line argument parse error: command not found: ', cmd.command\n\t\t\tprint >>file"}]}
{"task_id": "62b438a266fea644fe22cc2e", "prompt": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tsubparsers = parser.parse_subparsers()\n\tnamespace = parser.add_parser(\"namespace\", help=\"Generate the list of commands.\")\n\n\tfor _, subparser in subcommands:\n\t\tsubparser.add_argument(\"command_name\", help=\"The command's unique name.\")\n\t\tsubparser.add_argument(\"short\",   type=str, help=\"The short description of the command (e.g. 'install'.)\")\n\t\tsubparser.add_argument(\"long\",    type=str, help=\"The long description of the command (e."}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\t\tusage='''\nUsage: %s [script [args]] [global flags]\n\nThis script calls a subparser with arguments it recognizes, and passes the parsed arguments to the\nsubparser's main() function. The main() function's default behavior in Python is to simply execute\nthem, but for complex script arguments, it may also be executed as a single function. It will print the\nresult of the main() function on STDOUT and STDERR as an example, each argument given.\n\nThis script is designed to work with command-line arguments that"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tglobals().update(UNPARSED_ARGS_DICT)\n\treturn UPARSED_ARGS_DICT\n\n#include \"source/extensions/filters/http/cache/no_cache.h\"\n\n#include <string>\n#include <atomic>\n\n#include \"source/common/buffer/buffer_impl.h\"\n#include \"source/common/common/assert.h\"\n#include \"source/common/common/exception.h\"\n#include \"source/common/common/stats.h\"\n#include \"source/common/http/codes.h"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser = argparse.ArgumentParser(usage=__usage__ if __usage__ or \"[global] <command-line>\",\n\t                                 formatter_class=argparse.RawDescriptionHelpFormatter,\n\t                                 description=__doc__)\n\n\tparser.set_defaults(func=parse_arguments)\n\n\tsubcommands"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser = argparse.ArgumentParser(*unparsed_arguments, show_help=False, formatter_class=ArgumentDefaultsHelpFormatter)\n\tparser.parse_known_args()\n\treturn parser.namespace\n\n\ndef add_arguments(requiredArguments, providedArguments):\n\t\"\"\"Given global config files, validate and set up required arguments.\n\tReturn the original file and a dictionary mapping from option groups to argparse.Namespace instances\n\tand the required command-line arguments, suitable for creating new subcommands.\n\t\"\"\"\n\n\trequiredArgumentsDict = {}\n\tprovidedArgumentsDict = {}\n\n\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser_name = os.path.basename(sys.argv[0])\n\tparser = subparsers.ArgumentParser(description=parser_name, add_help=False)\n\tparser.add_argument('--version', action=\"version\", version=\"0.1\")\n\tparser.add_argument('--help', action=\"help\")\n\tsubparsers = parser.add_subparsers(title=\"subcommands\", help=\"\")\n\n\t# check help and version arguments\n\tsubparser_command_help = subparsers.add_parser('commandhelp', help='help')\n\tsubparser"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tglobal parsed_args\n\tparser = argparse.ArgumentParser()\n\tif type(unparsed_arguments) is not list:\n\t\tparser.add_argument(\"-x\", \"--global\", action=\"store_true\", help=\"show global options (default = False) \")\n\telse:\n\t\tparser.add_argument(\"-x\", \"--global\", action=\"store_false\", dest=\"show_global_opts\", help=\"show global options \")\n\t\tglobal parsed_args\n\t# the above line needs to be moved into\n\targs = parser.parse_args(unparsed_arguments"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser = argparse.ArgumentParser(prog='cditms',\n                         description=__doc__\t# $ python -m cditms.cli\n\t\t\t\t\t\t)\n\n\t\t\t\t\t# $ cditms.py <filename> [commandarguments]\n  for line in unparsed_arguments:\n\ttry:\n      tokens = line.split()\n\t  if len(tokens) == 1:\t\t# Empty argument value.\n\t\tcontinue\n    "}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser = argparse.ArgumentParser(usage=USAGE_MAIN, prog=PROG_NAME)\n\tadd_subparsers(parser, globals())\n\tfor subparser_name, subparser in named_subparsers(parser.add_subparsers, subparsers=True):\n\n\t\t# Get the handler for the subparser\n\t\thandler_class = getattr(subparser,'subparser', None)\n\t\tif handler_class is None:\n\t\t\ttry:\n\t\t\t\thandler_class = getattr(subparser, 'handler', None)\n\t\t\texcept AttributeError:\n\t\t\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\toptions = vars(unparsed_arguments)\n\tparsed_args = {}\n\tsubparsers = None\n\tfor name, obj in options.items():\n\t\tif name == 'general':\n\t\t\tparsed_args.update(general_commands(obj))\n\t\telif name == 'parser':\n\t\t\tsubparsers = subparsers or {}\n\t\t\tsubparsers[name] = functools.partial(General_parser, obj=obj)\n\tif opt.help:\n\t\tparser.print_help()\n\t\tsys.exit(0)\n\targs = parser"}]}
{"task_id": "62b438a266fea644fe22cc2d", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t#\n\t# If the parser is called from the command-line args or any other subparser, set that up.\n\t#\n\tparser, unparsed_args_ = subparsers.pop()\n\t#\n\t# If it is one of the cases related to a help message, display it.\n\n\tif parser.usage:\n\t\tif parser.help:\n\t\t\tprint(parser.help)\n\t\telse:\n\t\t\traise RuntimeError(\"Error: unrecognized subcommand \" + parser.name + \" (try `\" + parser.usage + \" [options]`)\")\n"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tparser_arguments_dict = {}\n\n\tif unparsed_arguments is None:\n\t\treturn parser_arguments_dict\n\telse:\n\t\tremaining_parsed_arguments = unparsed_arguments\n\t\tfor key, value in subparsers.items():\n\t\t\tnext_subparser_name = key.split('.')[-1]\n\t\t\tif next_subparser_name not in parser_arguments_dict:\n\t\t\t\tnew_parsers = {}\n\t\t\t\tnew_parsers.setdefault(next_subparser_name, {})\n\t\t\t\tsubparser_args = value("}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\targument_groups = {}\n\tfor subparser_name, cmd_subparsers in subparsers.items():\n\t\tgroup = {'subparsers': []}\n\t\tfor parser_name, cmd_subparser_list in cmd_subparsers.items():\n\t\t\tif parser_name not in argument_groups:\n\t\t\t\targument_groups[parser_name] = []\n\t\t\t\tfor cmd_subparser in cmd_subparser_list:\n\t\t\t\t\tgroup['subparsers'].append(cmd_subparser)\n\t\t\targument_groups[parser_name].append(CmdSub"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tresult = {}\n\tparser_unparsed_arguments = []\n\tadd_extra_args = lambda subparser_name, value_dict:\n\t\tif key.lower() not in result:\n\t\t\tresult[key.lower()] = {}\n\t\tsubparser = subparsers[subparser_name]\n\t\tfor short_help in subparser.get_commands_descriptions():\n\t\t\targ_short = '--' + short_help[0] if len(short_help) > 1 else short_help[0]\n\t\t\tvalue = subparser.get_"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tparser_arguments = {}\n\tactions = subparsers.pop('action', [])\n\n\tfor subparser in subparsers:\n\t\tparser_arguments[subparser.name] = subparser.parse_args(unparsed_arguments)\n\n\tfor group in actions:\n\t\tif not group in parser_arguments:\n\t\t\tparser_arguments[group.key] = list(group.subparser.parse_args(\n\t\t\t\t{k:v.value for k, v in arguments.items()}\n\t\t\t))\n\n\treturn parser_arguments\n\n\ndef _"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tparser_key = parser_keys.Namespace(subparsers)\n\tnew_parser = argparse_registry[parser_key]\n\tsubparser_names = unparsed_arguments.keys()\n\tfor subparser in subparsers:\n\t\tsubparser_name = subparser_names.index(subparser)\n\t\t# print('subparser %s: %s' % (subparser_name, unparsed_arguments[subparser]))\n\t\tif subparser_name and \"help\" in subparser_names[subparser]:\n\t\t\t# print('subparser %s"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tresult = {'parser':None,'remaining_args':None,'remaining_actions':None}\n\tfor subname, arg in subparsers.items():\n\t\tif 'parser' in subname:\n\t\t\tresult['remaining_actions'].append((subname, arg))\n\t\telif'remaining_args' in subname:\n\t\t\tresult[subname] = result.get(subname+'_'+arg.dest, {})\n\t\telse:\n\t\t\t#print \"Could not find parser in parse_subparser_arguments\"\n\t\t\treturn result\n\t#"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tparser = subparsers.add_parser(\n\t\t\t\"subparser\",\n\t\t\tsubparsers._get_opt,\n\t\t\tdescription=f\"Run subparsers against {_NAMESPACE} args with {subparsers._get_longdoc()}.\",\n\t\t\tdisable_interspersed_args=True,\n\t\t\tdoc=parse_subparser_docstring(subparsers, unparsed_arguments[0].strip()),\n\t\t\tusage=SUBPARSES_USAGE,\n\t\t\tsubcommand_epilog=f\"Run subparsers against {_NAMESPACE} args with {subparsers._get_"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t# We iterate over argument, a list of argument names\n\targuments = {}\n\tfor a in subparsers.values():\n\t\targuments.update(a.arguments)\n\t\t# get all subparser name(s) claimed by this subparser\n\t\tfor a1 in a.actions:\n\t\t\t# skip subparser itself\n\t\t\tif isinstance(a1, argparse.Action) and a1.parser == subparser:\n\t\t\t\t# if this argument has been claimed by this subparser don't repeat\n\t\t\t\tcontinue\n\t\t\t# otherwise, check it for claim"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tif (subparsers.is_parsing_action or subparsers.is_resolving_action) and not any(\n\t\tcallable(i) for i in subparsers._internal_parsers):\n\t\t# if only one parser action, then its a call to an external plugin which should never happen,\n\t\t# but we still return an empty dict.\n\t\treturn {}\n\t\n\tfor subparser in subparsers._internal_parsers:\n\t\tdef parse_options():\n\t\t\tfor option in subparsers.subparser.options:\n\t\t\t\tif option.dest in option."}]}
{"task_id": "62b438a266fea644fe22cc2c", "prompt": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\ttop_parsers = {}\n\tsubparsers = {}\n\tfor name, builder in _TOP_PARSERS.items():\n\t\tif not builder.parser_names:\n\t\t\tcontinue\n\t\tfor build_parser in builder.build_parsers:\n\t\t\tif getattr(build_parser, \"is_new\", False):\n\t\t\t\tname = build_parser.parser_type\n\t\t\ttop_parser = BuildY.build_parser(name, build_parser.get_parser_func(), build_parser)\n\t\t\tfor subparser, args in top_parser.parse"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\treturn parserclass.parser_tuple\n\n\n\n#----------------------------------------------------------------------\n# Parser class:\n#\n# A parser is the entry point for the program. It processes the standard\n# input, and calls the appropriate module parser function based on the\n# command-line options.\n#\n# Remember that option processing happens in two steps. The first step,\n# option_list(), processes the command-line arguments, and, if\n# there are no more remaining command-line arguments, calls the default\n# action:\n#\t-f(arduino_filename). This will load all Arduino files in"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser_names = 'option', 'target', 'command', 'description', 'choices', 'options'\n\tparser_class = argparse.ArgumentParser\n\tparser_class.add_argument( '--command',help='Command to run.', dest='command', required=True)\n\targs = parser.parse_args()\n\n\treturn parser_names, args, parser_class\n\n\ndef run_parse( args=None, command=None ):\n\t\"\"\"\n\tRun a top-level parser, and return the output as a string.  If no args\n\tare provided, print"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser = argparse.ArgumentParser\n\tparser.add_argument('-f', '--force', action='store_true',\n\t\thelp = \"force all existing parsers to be reinitialized\")\n\tparser.add_argument('-v', default=0, type=int,\n\t\thelp = \"verbose level [0, 5]\" )\n\tpass#IMPORTIMPORTIMPORT import sys\n\tif len(sys.argv) == 1:\n\t\tsys.argv='cwrf_allparsers.py allparsers'\n\targs = parser.parse_args()\n\treturn args"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tsubparsers = parser.add_subparsers(dest='command')\n\tfor subparser in subparsers:\n\t\tsubparser.set_defaults(func=lambda: parse(\n\t\t\tcommand=subparser.command,\n\t\t\tparse_args=not bool(subparser.option_list)\n\t\t))\n\tfor args in subparsers[0].parse_args:\n\t\tsubparser = subparsers[0]\n\t\tparser = subparser.add_parser('command', parents=[subparser])\n\t\ttry:\n\t\t\tsubparser.parse_args"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\ttop_level_parsers = []\n\tleft_level_parsers = []\n\tfor tag in ('parser','subparser', 'tag', 'none', 'default', 'help'):\n\t\tparser = lambda: None\n\t\tif tag[0] == tag[-1] == '!':\n\t\t\tparser = parse_comment\n\t\telif tag[0] == '!':\n\t\t\tparser = NoOptionError\n\t\telif tag[0] == '{':\n\t\t\tparser = parse_tagline\n\t\telif tag[0] == '[':\n\t\t\tparser ="}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\ttry:\n\t\tfrom pgen2 import parser\n\t\tfrom pgen2 import config\n\t\tparser.parse_file(config.pgen2_config_file)\n\t\t# This is only needed so we can use the parser.parse_file function.\n\t\t# parser.read_file will handle it, even if it's not supposed to, i.e. if \n\t\t# it will fail.\n\t\t#parser.read_file(config.pgen2_config_file)\n\t\treturn parser.subparsers\n\texcept ImportError:\n\t"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t## [#] Need to know parser name and its subparsers.\n\n\tparser_help = \"\"\"\n\tParser for the output files:\n\n\t%% parser name\n\n\t\t%% subparser names\n\n\t\t%% options\n\n\t\t%% usage text\n\n\t\"\"\"\n\n\tparser_help1 = \"\"\"\n%% parser name\n\n\t\\%s\n\n%% subparser names\n\n\t\\%s\n\n%% options\n\n\t\"\"\" % (\n\t\t\" \".join([\n\t\t\t\"   %s\" % s for s in parser_help.splitlines"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tdef _parser(*args, **kwds):\n\t\tparser = ArgParser(*args, **kwds)\n\t\tparser.add_parser(\"action\", HelpAction)\n\t\tparser.add_parser(\"help\", HelpAction)\n\t\treturn parser\n\treturn _parser\n\ndef main(args=None):\n\t\"\"\"Parse args, and return the parser.\"\"\"\n\ttry:\n\t\topts, args = getopt.getopt(args, \"aC:H:O:\", [\"help\", \"version\"])\n\texcept getopt.error, msg:\n\t\tif args:"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers()\n\n\t# List all the groups under <group_abbreviation>...\n\tgroup2 = dict([(x.strip(),k) for x, k in parser.parsers if isinstance(x,str)][0])\n\tgroup2['group'] = group2['group'].strip()\n\tgroup2['arguments'] = group2['arguments'].strip()\n\n\t# List all the subparsers under <subparser_abbreviation>...\n\tsubparser1 = (x.strip() for x in"}]}
{"task_id": "62ece4982e6aefcf4aabbd5f", "prompt": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tif response:\n\t\t# Remove the next link of this WAPI request.\n\t\tnext_link = get_next_link(response)\n\t\t# Loop to get the paginated object.\n\t\tfor item in response:\n\t\t\tyield {\n\t\t\t\t\"pagingId\": item[\"pagingId\"],\n\t\t\t\t\"pagingDetails\": [paging(response=response,\n\t\t\t\t\tmax_results=max_results),\n\t\t\t\t\titem,\n\t\t\t\t],\n\t\t\t\t\"type\": \"paging_response\",\n\t\t\t}\n\telse:\n\t\tyield {\"paging"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tparser = urllib.parse.urlparse(response[0])\n\tif parser.path!= page_url:\n\t\traise ValueError(\"Not the page you are looking for\")\n\tnew_parser = urllib.parse.urlparse(page_url)\n\tnew_page = page_name.replace(page, \"\")\n\tfor pag in new_parser.query.values():\n\t\tfor index, p in enumerate(pag):\n\t\t\tif pag == p:\n\t\t\t\tcontinue\n\t\t\tyield u\"\".join([new_page, f\"{index + 1 if max_"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tfor x in response:\n\t\tyield [x['pagination']['paging'], x['paging']['item'], x['paging']['total_pages']]\n\ndef create_job_name(job_id, app, version):\n\treturn \"%s.job.%s.%s\" % (app, VERSION, version)\n\ndef delete_job(app, pipeline, job_id):\n\t\"\"\"\n\tDelete job\n\nArgs:\n\tapp (string): App name\n\tpipeline (string): Pipeline\n\tjob_id (string): Job ID\n\t\nReturns:\n\tobject with response"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tdef chunk(iterator, chunk_size):\n\t\treturn list(zip(*iterator))\n\n\tpage_number, iterator, chunk = paginate(response, int(max_results))\n\tif page_number == 1:\n\t\titems = chunk(iterator, chunk_size)\n\telse:\n\t\titems = chunk(iterator, chunk_size) if chunk else []\n\treturn response, items\n\n\ndef sort_by(sort_column, key=None, sort_type=None):\t\t\"\"\"Convert to a valid sort by column string type\"\"\"\n\tif"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tfor idx in xrange(0, max_results):\n\t\tyield response[idx]\n\n\ndef wap_find_page_by_search(web_client, web_response, search_type,\n\t\tsearch_string, search_string_length, limit):\n\t\"\"\"\n\tReturns wap pages page by page\n\nArgs:\n    web_client (WapiClient): wapi client\n    web_response (WapiResponse): wap response\n    search_type (search_type): search type\n    search_"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tmax_len = max(20, len(response))\n\titemsPerPage = max_len // max_results + 1\n\tprint (\"\\n%s page\" % (itemsPerPage))\n\toffset = 1\n\tnext_item = False\n\twhile next_item == False:\n\t\tpaginated_response = response[offset]\n\t\tpaginated_response = paginated_response or []\n\t\tif str(paginated_response[0], 'd') == 'list':\n\t\t\tpaginated_response = paginated_response[0]\n\t\tpaginated_"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\ttry:\n\t\treturn paged_obj_finder(response, max_results)(response.items())\n\texcept:\n\t\treturn None\n\ndef paged_obj_finder(data, max_results):\t\t\t\"\"\"\n\tReturns paged object.\n\nArgs:\n    data: WAPI data.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Page object.\n\"\"\"\n\tfor i in range(1, max_results):\n\t\tobj = data[start: end"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t# Get all pages of a specific page.\n\tpage = requests.utils.request_url_page(response, 'page', default='.')\n\n\tfor _ in range(max_results):\n\t\tyield response\n\nif __name__ == '__main__':\n\timport sys\n\tsys.exit(main())\n\n// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * Copyright (c) 2019 Linaro Limited.\n */\n\n#include <linux/module.h>\n#include <linux/init.h>\n#include <"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tpaging_info = {'results': [], 'page': 1, 'count': 0, 'items': [], 'hasMore': False}\n\tfor i in range(max_results):\n\t\tpaging_info['page_info'].append(i)\n\t\tpaging_info['count'] = i+1\n\t\tpaging_info['hasMore'] = bool(i+1)\n\t\n\tpaging_info['first_info'] = paging_info['page_info'][1]\n\tpaging_info['last_info'] = paging_info['page_info'][paging_info['count"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tpage = 1\n\tp_url = 'http://localhost:1087/%(number)s' % {\n\t\t    'number': str(max_results)}\n\tfor x in range(1, max_results+1):\n\t\tp_url = 'http://localhost:1087/%(number)s/'\n\t\tp_url += str(x)\n\t\tyield response, p_url, page\n\ndef wap_get_list_result_json(request_info):\n\treturn json.loads(request_info.response)\n\ndef"}]}
{"task_id": "62ece4982e6aefcf4aabbd60", "prompt": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t", "suggestions": [{"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\tif str(size) == '':\treturn size\n\tif size[0] == 'f':\n\t\tsize = size[1:]\n\telif size[0] == '-':\n\t\tsize = size[1:]\n\telse:\n\t\tsize = size.strip().upper()\n\treturn int(size) * 1024\n\n# ========================================================================================\n# File Manager\n# ========================================================================================\n\nclass Manager(object):\n\t__metaclass__ = ABCMeta\n\n\t_logger            = None\n\t_config"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\tSIZE_TO_BYTES_MAP = {\n\t\t\"B\": 512, \"KB\": 1024, \"MB\": 1048576, \"GB\": 1099511627776,\n\t\t\"TB\": 1073741824,\t# TB - TiB > than 4 gigs\n\t\t\"PB\": 1048576,\t\t# PB - PiB > than 5 gigs\n\t\t\"EB\": 9223372036854775806, \"GB\": 2147483648, \"TB\": 999999999, # EB - EB > than 999 gigs\n\t\t\"YB\": 11111111111111111\t# YB"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\tif size == \"\": return 0\n\ts, e = size.split(\" \")\n\treturn int(e) if e == \"\" else len(s) * s.count(\".\")\n\ndef file_to_human_readable(filename: str) -> str:\t\"\"\"\n\tConvert human readable file size to a nice human readable file.\n\tFor example:\n\n\t    file_to_human_readable(\"1000\") -> \"1K\"\t\"\"\"\n\tif filename == \"\":\n\t\treturn \"No file\"\n\tsize = size_to_bytes(filename)"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\treturn int(float(size) * 1024)\n\n//\n// Copyright(C) 1993-1996 Id Software, Inc.\n// Copyright(C) 2005-2014 Simon Howard\n//\n// This program is free software; you can redistribute it and/or\n// modify it under the terms of the GNU General Public License\n// as published by the Free Software Foundation; either version 2\n// of the License, or (at your option) any later version.\n//\n// This program is distributed in the hope that it will be useful,\n// but WITHOUT ANY WARRANTY; without even the implied warranty"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\tsize = to_bytes(size)\n\n\tpos = size_to_bytes(size, 1)\n\tdiv = pos % 1024\n\tif div > 1024:\n\t\tdiv = round((div/1024), 2 )\n\treturn int(round(size/div)/1024 )\n\n/* Copyright \u00a9 2015-2016 David Valdman */\n\ndefine(function (require, exports, module) {\n    var Base = exports.Base = require('./base')\n                       "}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t# TODO: remove all spaces and other stuff from str to avoid parsing exception\n\tsize = size.strip(u' ').strip(\"-\")\n\n\t# Size must be converted in byte values\n\t# Round up from one\n\tsize = int(size + 0.5)\n\n\t# Return integer value\n\treturn size\n\n\ndef is_bytestr(data: Union[str, bytearray_t]) -> bool:\n\t\"\"\"\n\tCheck if any bytes (or a string) type are in bytearray type\n\nReturn:\n\tTrue: if any bytes type in data"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\treturn size.split('k')[0] * 1048576  # Kilobytes to bytes\n\n\ndef _check_path(path):\n    \"\"\"\n    Raises an exception if the provided path do not adhere to the\n    requirements of psychopy.config.BaseFileFinder.\n    \"\"\"\n    if not os.path.exists(path):\n        raise KeyError(\"{} no longer exists.  Did you remove it from your configuration?\"\n              "}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\tfrom math import floor, log10\n\tif not size:\n\t\treturn None\n\tsizeArray = []\n\tfor char in size:\n\t\tv = float(char)\n\t\tsizeArray.append(v/(math.log10(1024)-log10(1)))\n\tlgth = len(sizeArray)\n\tif lgth == 1:\n\t\treturn None\n\tif lgth*10>=10000:\n\t\treturn None\n\tpow10 = 10000.0**10\n\treturn int(round(float(lgth)/pow10))\n\ndef"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\treturn int(float(size) * 1024.0)\n\n### File system\n\nThe filesystem directory is used to store files.\nFile system is a file system, in which an operating system is implemented.\nFileSystem has been renamed 'fs' to 'fs.file_system'.\nThe following functions are supported in the file system:\n\nfs.create\nfs.delete\nfs.ensure_directory\nfs.mkdir_safe\nfs.list_dir_contents\nfs.move_to_trash\nfs.mkdir\nfs.get_parent\nfs.get_parent_directories\nfs"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\tsize = size.rjust(2)\n\tsize = int(size)\n\tif size >= 100000000:\n\t\tsize = math.ceil(size/1000000)\n\n\tres = 0\n\tfor unit in [\"\", \"Ki\", \"Mi\", \"Gi\"]:\n\t\tn = int(math.floor(size/unit)*unit)\n\t\tres += n / size  # e.g. 1G/K\n\t\tsize = n\n\tif size > 0:\n\t\tres *= size / 1024\n\treturn res\n\n@export\ndef filesize"}]}
{"task_id": "62ece4982e6aefcf4aabbd61", "prompt": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t_keys = [(k, getattr(d, k)) for k in iterable]\n\treturn _listreduce(_keys, dicts)\n\ndef _stringsum(s, list_of_lists):\n\treturn''.join(str(x) for x in list_of_lists)\n\ndef _stringjoin(s, string_list):\n\treturn'' + s + string_list\n\ndef _dictjoin(key_value_dict, key_value):\n\treturn key_value_dict.get(key_value, key_value)\n\ndef _"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\td = {}\n\t_iterdicts(dicts, d)\n\treturn d\n\n\ndef load_dot_dot_gcode() -> str:\n\n\tdotpath = find_folder(\"~/.cache/dot/gcode\", [\".dot\"])\n\n\tdot = load_dotfile(dotpath)\n\tgcode = DotDotGCode.compile(dot, path=dotpath, file_suffix=\".gcode\", compile_level=None)\n\n\treturn gcode\n\n\ndef save_submodules_dict(data: dict, filename: str,"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\tif not isList(dicts):\n\t\traise TypeError(\"Expected 1 to 5 arguments of: {0}\".format(\n\t\t\tlist(\n\t\t\t\tmap(str, sorted(list(dicts))))) )\n\n\tresult = {}\t# create a dictionary of the keys from dicts that will\n\t# be used to construct a new dict in _dictsum\n\tfor i, d in dicts:\n\t\tfor k, d2 in d.items():\n\t\t\tresult[k] = result.get(k, {}).get(i, d2)\n\n\t"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\treturn _flatten_dict(dicts)\n\ndef _flatten_list(list_of_dicts):\n\t'''\n\tReturn a view over a nested list structure, flattening a nested structure\n\twith an array if necessary.\n\n>>> list(_flatten_list([{'a': 1}, {'a': 2}]))\n[{'a': 1}, {'a': 2}]\t'''\n\tsize = len(list_of_dicts)\n\tresult_list = list()\n\tfor n, dict_ in enumerate(list_of_dicts):\n\t\tif isinstance"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\tfor d in dicts:\n\t\t\tif hasattr(d, \"keys\"):\n\t\t\t\td = \",\".join([\"{} = {} {}\".format(k, d[k], v)\n\t\t\t\t             for (k,v) in d.items()])\n\t\t\t\tyield \"={}={}\\n\".format(d.keys(), d)\n\t\tyield \"# end of output\"\n\t\tyield \"\"\n\t\t\ndef _dictcount(d):\n\tn_keys = len(getattr(d, \"keys\", None))\n\tn"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\tdef __init__(self, d):\tself.dicts = _asdict([d])\n\n\tdef copy(self):\treturn copy.copy(self)\t\t# XXX copy is not immutable, even with self's internal state\n\n\tdef __contains__(self, key):\treturn self.dicts.__contains__(key)\n\tdef __getitem__(self, key):\treturn self.dicts.__getitem__(key)\n\n\tdef keys(self):\treturn self.dicts.keys()\t\t\t# XXX list.keys is not yet well-defined\n\tdef items"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\tdef __init__(self, args=None): #pylint:disable=arguments-differ\n\t\tsuper(D, self).__init__(\n\t\t\tdescription=_('%d dictionary/items', 100),\n\t\t)\n\n\t\tself.arg = argparser.ArgumentParser(usage=argparse.SUPPRESS)\n\t\tself.arg.add_argument(\n\t\t\t'--filename',\n\t\t\thelp=_('filename to write to, as a string'),\n\t\t)\n\n\t\targs = self._add_args()\n\n\t\tself.arg.add_"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\tif not isinstance(dicts, list): dicts = [dicts]\n\n\tdicts = _flatten(dicts)\n\n\tdef _group(iterable, d):\n\t\tfor elem in iterable:\n\t\t\tif elem not in d:\n\t\t\t\td[elem] = 1\n\n\tfor d in dicts:\n\t\tfor key, val in iteritems(d):\n\t\t\t_group(key, d)\n\n\treturn dicts\n\n\ndef _make_dict(names, values, list_items):\n\tdict = dict()\n\t"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\treturn reduce(lambda x, y: x + y, iteritems(dicts), 0)\n\n\ndef _make_tuple(n):\n\t# make (n) tuple with n elements\n\trv = list(range(n))\n\tfor i in range(n):\n\t\trv[i] = i\n\treturn tuple(rv)\n\n\n#######################################################################################\n#\n# def _is_power_of_two(x):\n#\t\tif not isinstance(x, list):\n#\t\t\treturn 0\n#\t\tn = len(x)"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t_n = len(dicts)\n\tif _n == 0 or (_n == 1 and hasattr(dicts[0], '__iter__')):\n\t\treturn dicts[0]\n\n\ttups = tuple([x.items() for x in dicts])\n\ttups_n = 0\n\n\tfor d in tups:\n\t\ttups_n += tup_sum(d)\n\n\tif tups_n == 0:\n\t\treturn None\n\n\treturn _dict(tups)[0]\n\ndef tup_sum("}]}
{"task_id": "62ece4982e6aefcf4aabbd62", "prompt": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\titems = {}\n\tfor key, value in list(url_args.items()):\n\t\tif isinstance(value, basestring) and value.strip():\n\t\t\tif key not in items:\n\t\t\t\titems[key] = value\n\t\telif isinstance(value, (date,datetime,timedelta)):\n\t\t\titems[key] = value.isoformat()\n\t\telse:\n\t\t\tif key not in items:\n\t\t\t\titems[key] = value\n\treturn items\n\n\n# --------------------------------------------------------------------\n# Helpers\n\ndef get_url(doc: BDF,"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tnew_args = [i for i in url_args if i in args]\n\treturn new_args\n\ndef _sanitize_args(_url, args, _args):\n\t\"\"\"\n\tSanitize URL's args and only add those items not marked with the `args`\n\tproperty. This removes extraneous attributes.\n\t\"\"\"\n\tsanitized_args = [_sanitize_arg(_url=u, arg=arg) for arg in args]\n\tfor u in sanitized_args:\n\t\tif u not in args:\n\t\t\tsanitized_args.remove(u"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\turl_args = url_args or {}\n\turl_args['action'] = None\n\treturn url, url_args\n\ndef _build_endpoint(url, kwargs):\n\tendpoint_url = url + '?' + urllib.parse.urlencode(urllib.parse.unquote(kwargs))\n\treturn unicode(endpoint_url)\n\n# This function handles the endpoint-style parsing used for generating URLs\ndef _generate_endpoint(endpoint, url, url_args):\n\t\"\"\"\n\tThis is the entry point for generating URLs for endpoints.  It generates\n\tand caches a new"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tfor i, arg in enumerate(url_args):\n\t\tif arg.find('@')!= -1:\n\t\t\turl_args[i] = url_args[i].replace('@' + arg, url)\n\treturn url_args\n\n\nclass HTMLWriter(DocstringsWriter):\n\n\tdef add_classes(self, klass_list): self.classes.extend(klass_list)\n\n\tdef __init__(self, doctree, title):\n\t\tsuper(HTMLWriter, self).__init__(doctree, title)\n\t\tself.classes"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tnew = url\n\tfor arga in url_args:\n\t\tnew = re.sub('[?&]' + URL_PARAM_PATTERN + '[?=&]' + url_args[arga], '?', new)\n\n\treturn new\n\ndef _escape_html(html):\n\thtml = ''\n\tfor char in html:\n\t\tif ord(char) > 159:\n\t\t\tswitch(ord(char)):\n\t\t\t\tcase 10:\n\t\t\t\t\tstr = HTML_CHAR_ENCODING.encode(char, \"ignore\").replace(\"\\\\\", \"\")\n\t\t\t\t\thtml ="}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tfor key, value in url_args.iteritems():\n\t\turl = url.replace(key, value)\n\treturn url\n\n\n#====================================================================\n\nclass URLRedirector(object):\n\tdef __init__(self, redirect_limit=_DEFAULT_REDIRECT_LIMIT):\n\t\tself.limit = redirect_limit\n\t\tself._url_args = {\n\t\t\t\"fragment\": _DEFAULT_URL_FRAGMENT,\n\t\t\t\"redirect_limit\": self.limit\n\t\t}\n\n\tdef get_url(self, url, *args, **kwargs):"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tif not url:\n\t\treturn url\n\treturn url.replace(r'%s' % url_args, url)\n\ndef _replace_file_args(url, file_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\treturn url.replace(r'%s' % file_args, url)\n\n//#############################################################################\n//#                                             "}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tfor pair in url_args.items():\n\t\turl_pairs = [(pair[0], pair[1])]\n\t\tfor url_pair in url_pairs:\n\t\t\turl = url_pair[0]\n\t\t\turl_value = url_pair[1]\n\t\t\tfor key, val in url_value.iteritems():\n\t\t\t\turl = url.replace(f\"{key}__{key}__\", f\"{val}__{val}\")\n\t\treturn url\n\n\n# ===========================================\n# Rack: API\n# ===========================================\n\n"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tif url_args is None:\n\t\treturn url  \n\tfor i in range(len(url_args)):\n\t\ttry:\n\t\t\tindex = url_args[i]\n\t\t\tif index is not None:\n\t\t\t\tvalue = index.string.replace('&quot;', \"'\")\n\t\t\t\tif value is not None:\n\t\t\t\t\turl_args[i] = value\n\t\t\telse:\n\t\t\t\treturn url_args\n\t\texcept:\n\t\t\treturn None\n\ndef _encode_multipart_formdata(fields):\n\t\"\"\"\n\tEncode"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\turl_args = re.escape(url_args)\n\tparts = ('%s=%s' % (k, v) for k, v in url_args.items())\n\treturn url + '=' +''.join(parts)\n\n#-------------------------------------------------------------------------\n#\n# Public methods\n#\n#-------------------------------------------------------------------------\n\ndef adduser(to, username, domain=None, password='',\n\t\t\t   options=[],\n\t\t\t   login_url =''):\n\t\"\"\"\n\tAdd a user into the list\n\tto : Username (string)\n\tusername : String\n\tdomain :"}]}
{"task_id": "62ece4982e6aefcf4aabbd63", "prompt": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\tif _is_bool(val):\n\t\treturn val\n\tif str is not None:\n\t\treturn str(val) == _EMPTY_STRING\n\n\t# Anything that isn't None, True, or 1. False and None are both True\n\treturn bool(val)\n\n\ndef try_set(value: Any,\n\t\t\tfunc: Callable[[Any], Any]\n\t\t) -> Optional[Callable[[Any], Any]]:\n\t\"\"\"\n\tIf the passed value is None or non-empty, this will try the passed\n\tfunction and return the default value"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn isinstance(val, string_types)\n\n\nif not is_none_string(None) and not is_none_string(\"\", \"\"):\n\t__all__ += [\"str\", \"None\", \"NoneType\", \"not_none\", \"not_none_string\", \"str\"] + __all__[__name__ + \"__all__\"]\n\t__all__ += __all__()\n\n\n# TODO: Remove these when we are no longer in use\nif __name__ == \"__main__\":\n\timport doctest\n\n\tdoctest.testmod()\n\n/**\n * *****************************************************************"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\tif isinstance(val, (str, unicode, bool, int)):\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef is_non_bool_array(values: typing.Tuple) -> bool:\n\treturn isinstance(values, ast.Tuple) and len(values) >= 2\n\ndef is_bool_array(val: typing.Any) -> bool:\t\"\"\"\n\tReturn true if the value object is a array of boolean values.\n\t\"\"\"\n\treturn isinstance(val, ast.List) and len(val) == 2 and isinstance(val[0"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\tassert isinstance(val, basestring), \"is_none_string only accepts String, Number, Set, and Array\"\n\tif isinstance(val, basestring):\n\t\treturn False\n\telif val is None:\n\t\treturn True\n\telif val.startswith('null'):\n\t\treturn False\n\telif val.startswith('true'):\n\t\treturn True\n\telif val.startswith('false'):\n\t\treturn False\n\t# this should never happen, but I'd like to be lax\n\traise ValueError('Unexpected value type: %s'%type(val))\n\n"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\tif not isinstance(val, basestring):\n\t\treturn False\n\tif not len(val) == 0:\n\t\treturn False\n\treturn True\n\ndef is_date(val: str | Dict) -> bool:\n\t\"\"\"Return True if given string represents a date in the format: \"YYYY-MM-DD\".\"\"\"\n\treturn is_none_string(val) or len(val.split()) == 3 and int(val.split()[0]) == 0\n\ndef is_datetime(val: str | Dict) -> bool:\n\t\"\"\"return True if given string"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t# We are checking against None objects to\n\t# avoid doing the 'isNone' check twice\n\tif val is not None:\n\t\treturn False\n\treturn true\n\ndef is_number(val: any) -> bool:\t\"\"\"\n\tCheck if an attribute with the appropriate type is a numeric representation\n\t\tsuch as integer or floating point.\t\"\"\"\n\tif val is None:\n\t\treturn False\n\treturn bool(eval(\" is_number(val)\",val))\n\ndef is_bool(val: any) -> bool:\t\"\"\"\n\tCheck if an"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\tval = val if isinstance(val, str) else \"NULL\"\n\tbool_val = False if val is None else True\n\treturn bool_val\n\n\ndef is_null_or_none(val: Any) -> bool:\n\t\"\"\"\n\tCheck to see if we pass an integer or string argument. This is used\n\tby the SQL engine to tell if a None value was considered invalid.\n\t\"\"\"\n\tif get_type(val)!= \"string\" and is_none_string(val):\treturn False\n\treturn True\n\n/*\n\n   "}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\tif not isinstance(val, str):\n\t\treturn False\n\treturn val.strip() == \"\"\n\nimport { getSerdePlugin } from \"@aws-sdk/middleware-serde\";\nimport { HttpRequest as __HttpRequest, HttpResponse as __HttpResponse } from \"@aws-sdk/protocol-http\";\nimport { Command as $Command } from \"@aws-sdk/smithy-client\";\nimport {\n  FinalizeHandlerArguments,\n  Handler,\n  HandlerExecutionContext,\n  HttpHandlerOptions as __HttpHandlerOptions,\n  MetadataBearer as __MetadataBearer,\n  MiddlewareStack,\n  Ser"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\tobj = getattr(val, 'value', val)\n\treturn obj is None or str(obj) is None\n\ndef remove_none(val: Any):\n\t\"\"\"\tRemove an object from an array if it is either not None, or a\n\t       None.\n\t\"\"\"\n\tval = obj_get(val, 0, val.type)\n\tfor x, y in zip(val, val):\n\t\tif isinstance(x, str_or_none):\n\t\t\tx = x.value\n\t\tif is_none_string(y"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\tif not isinstance(val, str):\n\t\treturn False\n\treturn val == \"\"\n\n\ndef is_none_array(arr: any) -> bool:\t\"\"\"\n\tCheck if an array represents a None value.\t\"\"\"\n\treturn is_none_string(arr) and len(arr) == 0\n\n\ndef is_none_tuple(tup: any) -> bool:\t\"\"\"\n\tCheck if a tuple is None\t\"\"\"\n\treturn len(tup) == 0\n\n\ndef is_none_dict(dict: any) -> bool:\t\"\"\"\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd65", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tif not parser.dest:\n\t\treturn None\n\tflags = \"\"\n    for value in FLAG_VALS:\n\t\tif parser.nargs == argparse.OPTIONAL:\n\t\t\tflags += '--%s' %(FLAG_VALS[value])\n\t\telse:\n\t\t\tflags += '--%s %s' %(FLAG_KEY, FLAG_VALS[value])\n\treturn flags\n\ndef command_line_flags(parser):\t\"\"\"Returns a copy of command line flags\n\tas a space-separated string containing exactly the arguments that were\n\tspecified on"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tresult = (parser.formatter_class.formatters[0].__flags__ or '').strip()\n\t\tif result.startswith('&'):\n\t\t\tresult = result[1:]\n\t\telif result.startswith('(' and result[0].isdigit():\n\t\t\tresult = result[0] + '+' + result[1:].strip()\n\t\telse:\n\t\t\tresult = result.strip()\n\t\treturn result\n\n\tdef help(self):\n\t\t\"\"\"\n\t\tDisplay help text for argument parser.\n\t\t\"\"\"\n\t\tparser = self."}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn parser.prog\n\n\ndef parser_epilog(parser, __description):\n\t\"\"\"\n\tA short-hand for a programmer's parser - to be used by a program to parse flags that are not\n\tappropriate to be passed on to the program.\n\n\tThis class has been designed to be easy to modify with new flags defined here as long as\n\tone or more parsing methods is called.  It provides a common interface for a series of\n\tdifferent types of flag parsing as well as some helpful information about the usage\n\tof that parsing methods.\n\t\"\"\"\n\tfrom"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tdef _short2string(name, short, long=None):\n\t\tif long is None:\n\t\t\tlong = long if name in (\"help\", \"h\", \"help.py\") else None\n\t\treturn \"-{0}{1}\".format(short, long)\n\n\tclass _Option(argparse.Argument):\n\t\tlong\t= True\n\t\tdef __init__(self, option, help, **kwargs):\n\t\t\tsuper().__init__(long=long, **kwargs)\n\t\t\tself._option = option\n\t\t\tself._help = help\n\t\t\t"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn parser.prog.replace('-',' ') +'' + parser.formatter.format_help()[1:]\n\n\n''' COMMAND FUNCTIONS '''\n\ndef command(*args, **kwargs):\n\t\"\"\"\n\tConvert a command line string (using the Psyco tool's defaults) into a list of\n\tcommands that are understood by'make' and 'install'.\n\n\tReturn value: a list of the matching commands\n\tAuthor:\tNelson McLean <mel.mcleanan@gmail.com>\n\t\"\"\"\n\tparser \t= argparse.ArgumentParser("}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tparser.add_argument(\"--version\", action=\"store_true\", \n\t\t\t\t help=\"print version and exit\", default=False)\n\tparser.add_argument(\"--verbose\", action=\"store_true\", \n\t\t\t\t help=\"verbose console output\", default=False)\n\tparser.add_argument(\"--help\", help=\"print this message and exit\", \n\t\t\t\t   action=\"store_true\",\n\t\t\t\t   default=False)\n\tparser.add_argument(\"--list-items\", help=\"list the items\", \n\t\t\t\t   action=\"store_true\",\n\t\t\t\t   default=False)\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tparser_flags = ''\n\tfor arg in parser.add_argument_group(title=\"CLI Flags\", description='A brief description of flags'):\n\t\tif isinstance(arg.dest, ArgumentType):\n\t\t\targ_type_flags = \" [{}] \".format(arg.dest.get_help())\n\t\telse:\n\t\t\targ_type_flags = \"\"\n\t\tparser_flags += arg_type_flags +'' + arg.dest.help\n\treturn parser_flags\n\ndef parse_config(module_path, cfg):\n\timport yaml, argparse"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn parser._obj.description\n\ndef default_parser(space_type, use_columns):\n\t\"\"\"\n\tReturn an object that has built-in argument parsers, and a `DefaultParser <https://docs.python.org/3/library/argparse.html#the-default-argument-parser>`_\n\tthat will look very similar to the built-in parser.\n\n\t:param space_type: a space, or a dictionary, used to name arguments.\n\t:param use_columns: boolean, whether arguments are to be mapped to columns (default is True).\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = 0\n\tfor alias, value in iter(parser.flags.items()):\n\t\tflags += 2 if alias in parser.add_flags else value.replace(parser.name, '')\n\treturn flags\n\ndef parser_bool_group(parser, name, aliases):\n\tif name in parser.add_flags or name in parser.alias_flags:\n\t\t# The argparser group name and aliases both have a boolean flag.\n\t\treturn True\n\tfor flag_name in iter(parser.flags.keys()):\n\t\t# The parser does not support aliases for"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tparser.add_argument(\n\t\t'-r', '--randomize-source', action='store_true',\n\t\thelp=\"Randomize each source file, including source code line and comment markers.\tThis option \"\n\t\t\t\"is useful if you have multiple files of the same name and don't want to override each source.\"\n\t)\n\tparser.add_argument(\n\t\t'-f', '--force-binary', action='store_true',\n\t\thelp=\"Always create a binary archive.  By default, the first \"\n\t\t\t\"file (index 0) is"}]}
{"task_id": "62ece4982e6aefcf4aabbd66", "prompt": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tif(set.issuperset(processed, path_name)):\n        msg = \"Skipping processing %s: already processed\" % path_name\n        logger.warn(msg)\n\treturn False\n\treturn True\n\n@lru_cache()\ndef checkout(path, recursive, verbose=False):\n\t\"\"\"\n\tCopy a directory from another path in the local filesystem to the working\n\tdirectory. Files created by the process on the local filesystem\n\tare preserved.\n\n        This function is"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tif processed:\n\t\tprint(\"Warning, path %r from %s already processed.\" % (\n\t\t\t\t\tpath_name,\n\t\t\t\t\tos.path.basename(path_name)))\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\tprint(\"Checking %s...\" % (os.path.basename(path_name)))\n\t\ttry:\n\t\t\treturn os.path.isdir(path_name)\n\t\t\t# Note: The check for e.g. an empty dir name\n\t\t\t# does not work, because"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tprint(\"*\"*40)\n\tprint(\"------------------------> {}\".format(path_name) + \"\\n\")\n\tprint(\"------------------------> Path {} already processed. {}<\".format(path_name, not tested) + \"\\n\")\n\tprint(\"*\"*40)\n\tif not tested:\n\t\tprint(\"Not yet tested, doing it.\")\n\t\tprint(\"*\"*40)\n\t\tif self.report:\n\t\t\tself.record_path(path_name)\n\t\t\ttested = set()\n\tprint(\"*\"*40)\n\tprint(\"################################"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tif path_name in processed: return True\n\tif verbose:\n\t\tprint(\"The path %s already exists, trying to find it again. \" % path_name)\n\treturn False\t\t\n\n<?php\n/**\n * @see       https://github.com/zendframework/zend-mail for the canonical source repository\n * @copyright Copyright (c) 2005-2018 Zend Technologies USA Inc. (https://www.zend.com)\n * @license   https://github.com/zendframework/zend-mail/blob/master/LICENSE.md New BSD"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tif path_name in processed:\n\t\treturn True\n\tif verbose:\n\t\tprint(\"already processed\", path_name)\n\treturn False\n\ndef delete_files_by_pattern(pattern, path_name, verbose):\n\t\"\"\"\n\tWill delete ALL files in a directory matching the pattern.\n\n\tThe pattern is given in the second parameter. In the case that you\n\tdon't want, simply list the name of the files matching pattern.\n\t\"\"\"\n\n\t# make a list of the strings\n\tall_files = os.listdir(path_name"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tif not processed:\n\t\tprint('Path',path_name,'already processed.')\n\t\treturn False\n\n\tfor path in processed:\n\t\tif path.startswith(path_name):\n\t\t\tif verbose:\n\t\t\t\tsys.stdout.write(path_name+'\\t' +\n\t\t\t\t(path.strip(os.path.sep) + '\\t'+\n\t\t\t\tos.path.sep).encode('ascii'))\n\t\t\t\tsys.stdout.flush()\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn True\t\t\t\t\t\t   # no error\n"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tpath_name_upper = ucr.util.text.upper(path_name)\n\n\ti = 1\n\twhile True:\t# Try to open the path\n\t\ttry:\t# Find another path with the same name\n\t\t\tif i in processed:\t# If it is already in the set\n\t\t\t\treturn True\t# If it is, then ignore it\n\t\t\tpath_name_upper = ucr.util.text.upper(processed[i-1])\n\t\t\tbreak\n\t\texcept:\t# Failed to find another path with the same name\n\t\t"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tif processed.has_key(path_name)):\n\t\tif verbose:\n\t\t\tprint(\"File %s has already been processed\" % path_name)\n\t\treturn processed[path_name]\n\telse:\n\t\tif verbose:\n\t\t\tprint(\"Processing %s\" % path_name)\n\t\tprocessed.add(path_name)\n\t\tfile_obj = op.normpath(path_name)\n\t\tif op.normpath(path_name).stat().st_mode & op.S_IFDIR:\n\t\t\treturn was_processed"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\ttry:\n\t\tif processed.find(path_name) is not None:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept ValueError:\t\t# Could try a different path name\n\t\treturn False\n\ndef __check_mode_of_file(\n\tsource_path,  filter_dict, output_path, processed, verbose):\n\t\"\"\"Check mode/permissions of file, and call the given function\n\tif it is okay or not.\"\"\"\n\tfull_path = source_path[0]\n\tif __verbose and verbose:\n"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tpath_list = [ p for p in processed if p.startswith(path_name) ]\n\tif path_list:\n\t\timport traceback\n\t\tprint(traceback.format_exc())\n\t\treturn True\n\n\treturn False\n\n/* This file is part of VoltDB.\n * Copyright (C) 2008-2020 VoltDB Inc.\n *\n * This program is free software: you can redistribute it and/or modify\n * it under the terms of the GNU Affero General Public License as\n * published by the Free Software Foundation, either version 3 of the\n *"}]}
{"task_id": "62ece4982e6aefcf4aabbd67", "prompt": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t", "suggestions": [{"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\tif v_index in vertices: \n\t\treturn v_index, [v_index] * 3\n\telse: \n\t\treturn None\n\nif __name__ == '__main__':\n\tvertices = [[0,0,0],[1,0.1,1],[2,0.2,0.5],[0.7,0.8,0.9]] \n#\tvertices = [(0,0,0),(0,0,0)]*2 \n#\tvertices = [(1,1,1),(1,1,1)] * 2\t\n#\t"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\ta, b, c, d, a = vertices\n\n#A    B    C    D  -> of polygon vertices on the plane\nif(a[1] < c[1]):\tvertices[0] = a[0]+ d[0]\nif(a[1] > c[1]):\tvertices[1] = a[0]+ d[0]\nif(b[1] < c[1]):\tvertices[2] = b[0]+ d[1]\nif(b[1] > c[1]):\tvertices["}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\tdef check(vertex):\t\t\"\"\"Check if the vertex is inside the polygon\"\"\"\n\t\t\tif not (x1 < x2 < x3) or not (y1 < y2 < y3):\n\t\t\t\t\treturn False\n\t\t\tif x1 >= x2:\n\t\t\t\t\treturn False\n\t\t\tif y1 >= y2:\n\t\t\t\t\treturn False\n\t\t\tif x3 >= x2:\n\t\t\t\t\treturn False\n\t\t\tif y3 >= y2:\n\t\t\t\t\treturn False\n\t\telse: #if x1 == x2: #if y1 == y"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\tif len(vertices) % 3:\n\t\tvertices.sort()\n\n\tpoint_a = []\n\tpoint_b = []\n\tpoint_c = []\n\tpoint_d = []\n\tfor i, vertex0 in enumerate(vertices):\n\t\tvertex1 = deepcopy(vertex0)\n\t\tvertex2 = vertex1.flipped\n\t\tvertex1.position = vertex2.position.copy()\n\t\tpoint_a.append(vertex1.position)\n\t\tpoint_b.append(vertex0.position)\n\t\tpoint_c."}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\tnumvertices = len(vertices)\n\t#print (\"numvertices is \" + str(numvertices)) \n\tvertices = [s for i in range(numvertices)]\n\tfor i,v in enumerate(vertices):\n\t\tv2 = v[2] + v[0] - 3 * (v[1] + v[0]) + v[0]\n\t\tvertices[i] = v\n\t\tvertices[i+1] = v2\n\t\tvertices[i+numvertices] = v[-1]\n\t\tvertices[i+numvertices+1]"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t#polygon with vertices ABCD\n\t# will return\n\t#\tAB -> B, C,  D and CD are 3 points\nprint('The polygon with vertices will be [')\nfor vertex in vertices:\n  print(vertex, ') ')\nprint(']. It will be: ')\nprint(vertices)\nprint('->')\nprint(vertices[:2])\nprint('->')\nprint(vertices[2:])\nprint('->\\n')\n\n###############\n# vertex2tuple\n#\twith vertex2tuple\n# polygon with vertices ABCD\n# will return\n#\t"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\tif len(vertices) == 3: \t\t\t\t# if no points, return None\n\t\treturn vertices, None\n\tv1,v2= vertexarray.nbpolygon(vertices)\n\t#\tprint('vertextuple[0]: ', v1)\n\t#\tprint('vertextuple[1]: ', v2)\n\treturn vertices[0], v2[0],v1\n\n\n#                            #          # "}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t# vertex3tuple with tuples needs more data than tuples for Python 2.6 and 3.1.\n\n\t#\n\t#\t\tThe returned list must be deleted before the return value of this function.\n\t#\n\t\t#    The return value of this function must be the list of points or None, if the return value is None as above.\n\t\t#\n\t\t#    If the return value was not None, and no points were returned, this would return an empty list.\n\n\tresult = []\n\n\tfor v in vertices:\n\t\t#"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\tverts='='*len(vertices)\n\tfor vert in vertices:   #will change each vertex in this string to the 3 chars of the tuple\n\t\tverts+='<%s' * (len(vert) - 2)\n\tverts +='<%s' * (len(vert))\n\tnew_verts = []\n\tfor vert in vertices:\n\t\tnew_verts.append(verts[:vert.find(verts[0:len(vert)-1])])\n\treturn new_verts\n\n\n#This will return the vertices on a triangle which is inside the pointset"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\tvertices = vertices.replace('@@', '').replace('@@@', '')\n\tpoints, segments = zip(*[map(_get_point, x.split()) for x in vertices.split()])\n\tpoly1_line, poly2_line, poly3_line = map(_get_line, points), map(_get_line, segments), map(_get_line, lines)\n\treturn poly1_line\n\n\n\n@def polygon5tuple(polylines, vertices, width=10):\n\t\"\"\"\n\treturns 5 (tuple of lines, vertices and widths) for all polygons in the"}]}
{"task_id": "62ece4982e6aefcf4aabbd68", "prompt": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tif padding:\n\t\tresult = list(map(lambda x: str(x), stringgen().split(str(padding))))\n\telse:\n\t\tresult = chr(number).encode()\n\tif alphabet and alphabet[-1]=='#':\t\t\t\t\t\t# Handle reverse signs and padding for numbers that use decimal\n\t\treturn \"-\".join([int_to_string(x,&alphabet_mapping) for x in result.split(str(alphabet[-1]))])\n\treturn \"\".join([decimal.Decimal(\"%02X\" % int(ch)).string(alphabet, digits=4"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tif padding is None:\n\t\tpadding = len(alphabet) - 1\n\n\tif is_valid_decimal(number):\n\t\tdigit  = int(decimal2digits(number)[1]) - padding\n\t\treturn alphabet[digit] + str(number % 1000)[padding:]\t\t# \u672b\u5c3e\u4e3a\u4e24\u4f4d\n\telse:\n\t\tdigit = (number) // 1000   # \u8ba1\u7b97\u5e73\u65b9\u5206\u4f4d\u6570\n\t\treturn alphabet[digit] + str(number % 1000)[-padding:]\t\t# \u672b\u5c3e\u4e3a\ufffd"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t# if number < 0:\n\t# \treturn '0'\n\t# elif number == 0:\n\t# \treturn '-'\n\t# else:\n\t# \t# the most popular digit now\n\tdigits = int((number + \"\").zfill(digits_limit or 1)//pow(10, digits_limit or 1))\n\ts = ''\n\ttry:\n\t\twhile (digits--):\n\t\t\ts = alphabet[int(digits)] + s\n\n\t\t# pad with zeros, if necessary\n\t\t# pad := 0\n\t\tif"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tnumber = int(number)\n\tnum_digits = len(alphabet) - 1\n\n\t# Set length\n\tresult = []\n\t# Set first digit\n\tresult.append(alphabet[0])\n\t# Start with 0\n\tresult.append(str(0))\n\n\tif padding!= None:\n\t\t# Add padding\n\t\tfor i in range(num_digits):\n\t\t\tresult.append(padding)\n\n\t# Add first\n\tresult.append(alphabet[0])\n\n\treturn \"\".join(re.subn(r\"[1-"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\timport string\n\tdef pad(s: str, padding: int) -> str:\n\t\treturn ''.join((padding-len(s)%(padding-len(s))s + s)%padding for padding in range(padding // 2,padding + padding + 1)))\n\treturn string.replace(str(ord(number))[::-1], pad, alphabet[0], pad)\n\ndef format_as_hex(number: int, pad: int = 0) -> str:\n\t'''\n\tConvert a number to the hexadecimal representation of its binary value with the given padding.\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tif not isinstance(alphabet, list):\n\t\talphabet = [alphabet]\n\tif padding is None:\n\t\treturn \"\".join([d[len(i) * 4:i + 4] for d in alphabet])\n\tif padding == 3:\n\t\treturn \"\".join([d[len(i) * 4 + 2:i + 4] for d in alphabet])\n\treturn str(str(str(str(number)).zfill(padding)))[:padding * 3 - 1] + \".\".join([d[len(i) * 4 + n:i + 4] for"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tn = 0 if padding is None else padding\n\tfor i in range(0, 16, n):\n\t\tfor j in range(alphabet):\n\t\t    if number >= i and number < j:\n\t\t\treturn alphabet[number - i - 1]\n\t    else:\n\t\t\tvalue = 0\n\t        for k in range(alphabet.count(-1)):\n\t\t\t    value += alphabet[number - i - 1][alphabet.index(k)]\n\n\treturn alphabet[number - i - 1]\t\n\n\"\"\"\nfrom._utils import"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tresult = \"\"\n\tif number < 0:\n\t\tresult = \"-\"\n\tdigits = [digit_to_string(number % 10, alphabet=alphabet, padding=(padding or 0)) for number in range(10)]\n\tresult += \"\".join(digits) + \" \"\n\twhile number < 0:\n\t\tnumber += 10\n\t\tdigits = [digit_to_string(number % 10, alphabet=alphabet, padding=(padding or 0)) for number in range(10)]\n\t\tresult += \"\".join(digits) + \" \"\n\treturn result\n\ndef digit_"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tassert len(alphabet) >= 1, \"alphabet is empty\"\n\t\tif not padding:\n\t\t\tpadding = 3\n\t\tmax = len(number)\n\t\toutputlength = 0\n\t\tchars = alphabet\n\t\tif max % 3 == 1:\n\t\t\tchars += padding-1\n\t\tfor i in range(max-3, -1, -1):\n\t\t\toutputlength += 3 + length_chr[-i]\n\t\toutput = string_chr(chars, outputlength)\n\t\treturn output\n\tdef pad(self, pad):\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\treturn str(number).zfill(padding)\n\ndef make_filename(prefix: str, delimiter: str) -> str:\n\t\"\"\"\n\tReturn a filename representing the given `number` using the `delimiter` separator\n\nExample:\n\n\t\"1.6.2.1.1980.10.60.3\" becomes \"1.6.2.1.1980.10.60.3\"\n\nThe output has no leading or trailing separators.\"\"\"\n\tif isinstance(prefix, str):\n\t\treturn prefix + delimiter\n\treturn prefix\n\t\ndef make"}]}
{"task_id": "62ece4982e6aefcf4aabbd69", "prompt": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tfor register_key in flow_params:\t\t# Find the key that matches\n\t\tfor i in range(len(flow_params[register_key])):\t# Find the values\n\t\t\tif register_key[i] == register_number:\t# Match found?\n\t\t\t\tflow_params[register_key][i] = register_value\t\t\t# Store\n\treturn flow_params\t\t\t# Return\n\ndef merge_flows(target_flows, register_number):\t\t\t\t# This function will merge two lists of stored flows\n\tfor target_key, target_"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tfor param, value in flow_params.items():\n\t\tvalue = value.replace('$', '%d' % register_number)\n\t\tif isinstance(value, list):\n\t\t\tif len(value) > 0:\n\t\t\t\tstr_ = '(' + ', '.join(['%03x' % i for i in value]) + ')'\n\t\t\telse:\n\t\t\t\tstr_ = \"%03x\" % (register_number)\n\t\t\tif isinstance(flow_params[param], float):\n\t\t\t\tflow_params[param] = '%.8f' % flow_"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tif'register_number' in flow_params:\n\t\tif flow_params['register_number'] not in registered_regs[register_number]:\n\t\t\traise Exception(\"Missing registered register \" + register_number + \".\")\n\t\tif register_value is None:\n\t\t\t# value to be replaced by register_number\n\t\t\tunregistered_regs[register_number] = flow_params['register_number']\n\t\t\tdel flow_params['register_number']\n\t\telse:\n\t\t\tunregistered_regs[register_number] = flow_params['register_"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tfor key in flow_params: dict.update(flow_params[key], register_number, register_value)\n\n@click.command(context_settings=dict(help_option_names=['--help']))\n@click.pass_context\ndef cli_print(ctx, **kwargs):\tparser.add_option(\n\t\"--name\",\n\t\"-n\",\n\thelp=\"Define name of the resource\",\n\ttype = str,\n)\n\n@cli_print.command(context_settings=dict(help_option_names=['--help']))\n@click."}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tfor param in flow_params.keys():\n\t\tif(isinstance(flow_params[param], dict)):\n\t\t\t_replace_register(flow_params[param], register_number, register_value)\n\ndef _replace_param(flow_params, register_number, value):\n\tfor param in flow_params.keys():\n\t\tif(isinstance(flow_params[param], dict)):\n\t\t\t_replace_param(flow_params[param], register_number, value)\n\t\telse:\n\t\t\tflow_params[register_number][param]"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t# get number and type of register\n\tif register_number in flows:\n\t\t[register_number, register_value] = [\n\t\t\t(value, key)\n\t\t\tfor (value, key) in flows[register_number].items()\n\t\t]\n\n\t\t# check that register is valid\n\t\tif register_value not in flow_params and register_value > 0:\n\t\t\terror(\"register %s has value %s which is not a valid value\"\n\t\t\t\t% (register_number, register_value))\n\n\t# else, do"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tfor index, register_name in enumerate(flow_params):\n\t\tif register_name == register_number:\n\t\t\tflow_params[index] = register_value\n\n\treturn flow_params\n\ndef _get_module_dict(module):\n\t\"\"\"return dict of modules defined in given module dict\"\"\"\n\tret = dict()\n\tfor module_entry in module.modules:\n\t\tret[module_entry.module] = _module_dict(module_entry)\n\n\treturn ret\n\ndef _get_interface_dict(interface):\n\t\"\"\""}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tflow_params = remove_flow_params(flow_params)\n\tfor i in range(len(flow_params)):\n\t\tif(flow_params[i]['type']!= \"REGISTER\"):\n\t\t\tif(flow_params[i]['value']!= None):\n\t\t\t\tif(register_number in flow_params[i]['register_set']):\n\t\t\t\t\tflow_params[i]['register_set'][register_number] = flow_params[i]['value']\n\t\tflow_params[i]['type'] = 'REGISTER'\n\t\tdelete_parameters("}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tval = flow_params.get(register_number)\n\tif isinstance(val, str) and val in REGISTERS.keys():\n\t\tflow_params.pop(register_number)\n\tflow_params[register_number] = register_value\n\treturn flow_params\n\n\ndef _remove_register(flow_params, register_number):\n\t\"\"\"Remove register number from the dictionary\n\n flow parameters will be changed\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The register number to remove from flow parameters\n\"\"\"\n\tflow_"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tif register_number not in flow_params:\n\t\tflow_params[register_number] = {}\n\tif register_value not in flow_params[register_number]:\n\t\tflow_params[register_number][register_value] = None\n\ndef _get_regno_to_values(register_value, register_number):\n\tregno_to_value = {}\n\tfor number, register in flow_params.items():\n\t\tif register == register_number:\n\t\t\tregno_to_value[number] = [register_value]"}]}
{"task_id": "62ece4982e6aefcf4aabbd6a", "prompt": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t", "suggestions": [{"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tm =  re.search(r\".*\\.(.*).*\", arg)\n\tif m:\n\t\treturn replace_dots(value, m.group(1))\n\telse:\n\t\treturn value\n\ndef replace_dotson(arg):\n    return replace_dots(arg, '.dotson')\n\ndef replace_dotson_alt(arg):\n    return replace_dots(arg, '.dotson.alt')\n\ndef replace_dotson_alt_from_dotson(args, defval, defaults):\n    arg ="}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tif arg == '':\t\t\t\t\t\t\t\"\"\"\n\t\treturn value\n\tif not arg:\t\t\t\t\t\t\t\t\"\"\"\n\t\treturn value\n\tif arg[0] == '.':\t\t\t\t\t\t\t\t\"\"\"\n\t\tif len(arg) > 1:\t\t\t\t\t\t\"\"\"\n\t\t\tif arg == value:\t\t\t\t\t\t\t\"\"\"\n\t\t\t\treturn value\t\t\t\t\t\"\"\"\n\t\t\telse:\t\t\t\t\t\t\t\t\"\"\"\n\t\t\t\treturn value[1:-1]\t\t\t\t\t\t\"\"\"\n\t\telse:\t\t\t\t\t\t\t\t\t\"\"\"\n\t\t\treturn value\n\t# if \".\" in arg and arg!= \"\":\t\t\t\t\t\t\"\"\"\n\tif arg[0"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tif '.' not in arg:\n\t\treturn False\n\tif value not in arg:\n\t\treturn False\n\tfor elem in arg.split('.'):\n\t\tif '.' in elem:\n\t\t\tvalue = value.replace(elem, arg[elem.index('.')])\n\treturn True\n\ndef replace_wildcards(value, arg):\n\t\"\"\"\n\tReplaces all occurrences of '' or '..' with argument\n\t\"\"\"\"\n\tif '.' in arg or arg in ('', '..', './') or '*' not in arg:\n\t\treturn False\n\tif type("}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\treturn arg\n\n\ndef replace_within(val, str_vals):\t\t\"\"\"\n\tCreates a substitute which replaces '.' with the string value of an argument\n\t\tgiven as arg, arg being a string or list of strings\n\tRecursively replaces all occurrences of '.' with the arguments\n\t\tas strings (but this only works for strings!)\"\"\"\n\tfor v in val:\n\t\tif v in str_vals:\n\t\t\tval = v\n\t\telse:\n\t\t\ttry:\n\t\t\t\tlen_v = len(v)\n\t\t\texcept TypeError:\n\t\t\t"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\treturn value.replace('.',arg)\n\ndef replace_file(value, arg):\t\"\"\"\n\tReplaces all values of '/' to arg from the given string\t\"\"\"\n\treturn value.replace('/',arg)\n\n/*\n * Copyright (C) 2010-2021 Arm Limited or its affiliates. All rights reserved.\n *\n * SPDX-License-Identifier: Apache-2.0\n *\n * Licensed under the Apache License, Version 2.0 (the License); you may\n * not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\treturn value\n\ndef replace_digits(value, arg):\t\"\"\"\n\tReplaces all values of '0' to '9' of the given string, starting with\n\targ\t\"\"\"\n\tvalue = str(value) # TODO: if str is passed as arg, it will handle it\n\tif value == '0': value = '0'\n\tfor c in arg:\n\t\tif c not in value:\n\t\t\tvalue += c  # str is used because it should not be\n\t\t\treturn value\n\treturn value\n\ndef replace_difs(value, arg"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tfor m in arg:\n\t\tif m in value:\n\t\t\tvalue = \"{}.{}\".format(value, arg)\n\t\telse:\n\t\t\tbreak\t\t\"\"\"\n\ndef replace_vars(value, arg):\t\"\"\"\n\tReplaces all values of '${}' to arg from the given string\t\"\"\"\n\tfor p in arg:\n\t\tif p in value:\n\t\t\tvalue = \"{}{}\".format(value, arg)\n\t\telse:\n\t\t\tbreak\t\t\"\"\"\n\ndef replace_env(value, arg):\n\t\"\"\"\n\t"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tfor i in range(len(value) - len(arg) + 1):\n\t\tif value[i] == '.':\t\tnewvalue = arg\tbreak\n\t\tvalue = value[:i] + value[i + len(arg):]\n\treturn value\n\ndef splitName(value):\n\t# The base case first\n\tif not value:\n\t\treturn\t(value, '')\n\tname =\tvalue.lstrip('()')\t# e.g. \"(2) 3 ( 4 ) 4 ( 3 ) 8 ( 8 ) 8 ( 8 ) 8\""}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tstart, end = 0, len(value)\n\twhile start < end:\n\t\tif value[start] == '.' and value[end] =='':\n\t\t\tbreak\n\t\telse:\n\t\t\tvalue[start] = arg\n\t\t\tstart += 1\n\treturn ''.join(value[start:])\n\n### END module _urllib_parse ###\n\nimport { Component } from'react';\nimport { Row, Col, List, Card } from 'antd';\nimport { withRouter, WithRouterProps } from'react-router-dom';\nimport { T"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tif (not re.match('%..', arg, re.I)) or (len(arg)>1): arg = arg.rstrip('.')\n\treturn arg\n\ndef trim(arg, prefix='', suffix=None, **config):\n\t\"\"\"\n\tStrips whitespace from the beginning and end of each string\t\"\"\"\n\tif arg is None or arg == None: return None\n\treturn '\\n'.join([l.strip(prefix,'', end=' ') for l in arg.splitlines(keepends=False)])\n\ndef lpad(value, arg, padding=None,"}]}
{"task_id": "62ece4982e6aefcf4aabbd6b", "prompt": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t", "suggestions": [{"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\ttry:\n\t\tfor cl in cls.__bases__:\n\t\t\tcls = type(cl).__mro__[1]\n\t\treturn  cls.__subclasses__()\n\texcept AttributeError:\n\t\treturn []\n\n\n# the __slots__ is the same as the __dict__\n\n\ndef _create_new_obj(c, o, _type):\n\tif _type is str:\n\t\t_type = o.__class__\n\to = _type(o)\n\t# the following should catch the case of objects with the same __class__\n\t# which"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tresults = []\n\tbases = [x for x in dir(cls) if issubclass(x, cls)]\n\tfor base in bases:\n\t\tresults += subclasses(base)\n\treturn results\n\n@pytest.fixture()\ndef class_instancecount():\t# pragma: no cover\n\treturn max([x for x in dir(class_singleton) if issubclass(x, class_singleton)], key=len)\n\n@pytest.fixture(scope=\"session\")\ndef class_singleton():\t# pragma: no cover\n\t\"\"\"Fixture to create a single static class that can"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tdef _getsubclass(cls):\n\t\tif cls in cls._subclasses:\n\t\t\treturn cls\n\t\twhile 1:\n\t\t\t#print cls, cls._subclasses\n\t\t\tfor subclass in cls._subclasses:\n\t\t\t\tclass_ = subclass_from_class(subclass)\n\t\t\t\tif issubclass(class_, cls):\n\t\t\t\t\tcls._subclasses.add(class_)\n\t\t\tif cls._num_parents is None and cls._parent_cls:\n\t\t\t\tcls._num_parents = 1\n\t\t\tyield cls\n\t\t\tcls = cls._"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t__slots__ = ()\n\t__dict__ = {}}\n\n\ndef isinst(x, class_name=None):\n\t\"\"\"\n\tDetermine if an object is an instance of a class.\n\tParameters:\n\t\tx: the object to test\n\t\tclass_name: a string that specifies the class to test for\n\t\n\tNote: If any of the parameters x or class_name is None, this just calls the \n\t\tinspect.getclasstabs function (which returns the class's __dict__ ).\n\t\n\tReturn value: 1 if object is an"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tdef collect_subclasses(cls):\n\t\tif cls in cls._mro: return\n\t\tcls._mro[cls] = []\n\t\tfor subclass in cls.__subclasses__():\n\t\t\tcollect_subclasses(subclass)\n\t\tcollect_subclasses(cls)\n\t\treturn cls._mro[cls]\n\treturn map(lambda x: x.__name__, classnames(cls))\n\n# Get a copy of all public classes in the current module\ndef get_all_classes():\n\tfetches = __import__(\"inspect\")\n\treturn map"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tfor subclass in cls.__mro__[::-1]:\n\t\tif issubclass(subclass, cls):\n\t\t\tsubclasses = subclasses(subclass)\n\t\t\treturn subclasses\n\treturn []\n\n/*\n * Copyright \u00a9 2011, Petro Protsyk, Denys Vuika\n * \n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n * \n *  http://www.apache.org/licenses/LICENSE-2."}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tsuperclasses = []\n\tfor subcls in cls.mro():\n\t\tsuperclasses.extend(subclasses(cls))\n\t\tsubcls.mro()\n\t\tsuperclasses = [c for c in superclasses if isinstance(c, cls)],superclasses\n\treturn superclasses\n\ndef _find_class(klass):\n\tclassname = klass.__name__\n\tsuper = ''\n\tmro = []\n\tfor name in classname.split('.'):\n\t\tif name.startswith('.'):\n\t\t\tcontinue\n\t\tif name[:1] == '_"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tif cls.__bases__:\n\t\treturn list(cls.__bases__) + [sup for sup in cls.__bases__]\n\telse: return []\n\ndef get_name(obj):\n\t\"\"\"\n\tReturn name of object without parent names\n\tUsed for name for modules, class names, method names, etc.\n\t\"\"\"\n\treturn get_parent_name(obj)\n\n# ---------------------------------------------------------\n# Name/value classes\n#\n# The following attributes are used by objects defined\n# in this module:\n#\n#     module_methods: [ method for a"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t#print cls\n\t#print hasattr(cls, \"__bases__\") and cls.__bases__\n\tret = []\n    if hasattr(cls, \"__bases__\"):\n    \ttmp = []\n\tfor base in cls.__bases__:\n\t\tif base.__module__ == 'builtins':\n\t\t\tfor x in tmp:\n\t\t\t\tif hasattr(x, \"__bases__\"):\n\t\t\t\t\ttmp.append(x.__dict__)\n\t\t\t\t\t\n\t\telse:\n\t\t\ttmp.append(base)\n    \n    \tfor x in"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tfor c in cls.__mro__[::-1]:\n\t\tsubclasses(c)\n\n\treturn cls.__mro__[-1]\n\n<?php\n\ndeclare(strict_types=1);\n\nnamespace SchedulerBundle\\Task;\n\nuse DateTimeImmutable;\n\n/**\n * @author Guillaume Loulier <contact@guillaumeloulier.fr>\n */\nfinal class Composite implements CompositeInterface\n{\n    use CompositeTrait;\n\n    /**\n     * @var Clock[]\n     */\n "}]}
{"task_id": "62ece4982e6aefcf4aabbd6d", "prompt": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\treturn alphabet.index(string[0]) + 1\n\n#pragma once\n\n// Fortnite SDK\n\n#ifdef _MSC_VER\n\t#pragma pack(push, 0x8)\n#endif\n\n#include \"../SDK.hpp\"\n\nnamespace SDK\n{\n//---------------------------------------------------------------------------\n//Parameters\n//---------------------------------------------------------------------------\n\n// Function GCN_BGS_AuctionHouse.GC_AuctionHouse_C.Initialize\nstruct UGC_AuctionHouse_C_Initialize_Params\n{\n\tstruct FFortAuctionHouse_C**       "}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t# if alphabet and string is too short just return -1\n\tif len(string) < 2: return -1\n\tintegative = string[::-1] + string[1::-1]\n\tdigits = [0] * (len(alphabet) - len(integative))\n\tfor i in range(len(integative)):\n\t\tdigit = int(integative[i])\n\t\tdigits[i + 1] = max(0, digit - int(alphabet[-2]))\n\t\tintegative = integative[:i - 1]"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tresult = 0\n\tfor string in str_to_intlist(string, alphabet):\n\t\tresult = result * 256 + string\n\treturn result\n\n\ndef string_to_float(string: str, alphabet: List[str]) -> float:\t\"\"\"\n\tConvert a string to a floating number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tsplit_string = string_to_intlist(string, alphabet).split('\\t')\n\texponent = int(split_string[0]) if len(split_string"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tstring_length = len(string)\n\tnumber = 0\n\tfor i in range(1,5):\n\t\tdigit_index = string_length-i\n\t\tnumber = int(number*alphabet[digit_index]) + int(string[i])\n\treturn number\n\n#ifndef _RFB_PROTOCOLS_TCP_H_\n#define _RFB_PROTOCOLS_TCP_H_\n\n#include \"rfb_object.h\"\n\nrfbBool _rfb_tcp_new(rfbServerSet* _s,int *_fd,"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tnum_digits = len(alphabet)\n\tres = 0\n\ti = len(string) - 1\n\twhile i > 0:\n\t\tif string[i] in alphabet:\n\t\t\tnum_digits = i - 1\n\t\t\tbreak\n\t\ti -= 1\n\n\tres += int( (num_digits*10 + string[i]) / 10 )\n\n\treturn res\n\n\n# Return the integer between 1 and 10000.\ndef integer_between(min: int, max: int, d):\n\tif d > max: return int(d)\n"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\treturn max(map(int, string_to_int(x) for x in alphabet))\n\n\ndef string_length(string_: str) -> int:\n\t\"\"\"\n\tCalculate the length of a string. This is just as you would expect.\n\nThis uses the glibc version of the strnlen() function, which is not guaranteed\nto exist.\n\nIn the future, it will likely work on Linux, Mac OS X, Windows, Mac OS X, and\nWindows NT, and other POSIX systems that do have strnlen().\n\nAlas, the C and Python versions will work"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tfor letter in alphabet:\n\t\tstring = string.replace(letter, \"\")\n\treturn int(string[::-1])\n\ndef int_to_string(num: int, alphabet: List[str]) -> str:\n\t\"\"\"\n\tConvert a number (possibly an integer) to a string.\n\n\tThis does the same as the function string_to_int and may be more readable.\n\n\tThis function only converts the integer to lowercase or spaces, but won't do any\n\ttranslation so will be the same as the generic (which can't be done efficiently by\n"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tmax = max(\n\t\talphabet\n\t\tfor x in (\n\t\t\talphabet\n\t\t\tif alphabet\n\t\t\t\t.index(x)!= len(alphabet) - 1\n\t\t\telse\n\t\t\t\tlen(alphabet)\n\t\t\t)\n\t\t)\n\n\t# Doing it this way allows us to avoid an arraycopy, because Python is not\n\t# copyable on Python2.\n\treturn ord(string[max - 1]) - ord('0')\n\ndef get_alphabet(name: str, ngrams: List[str]) -> List[str]:"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tdigits_count = len(alphabet)-1\n\n\twhile len(string) > 0:\n\t\tstring = string_to_digit(string)\n\n\t\tfor digit in range(digits_count):\n\n\t\t\td = string[:-1]\n\t\t\td = int(\"\".join(list(string_to_digit([d])))[:digits_count])\n\t\t\tstring = string[-1:]\n\n\t\t\tif d > int(alphabet[digit]):\t\t# case of number + letter is larger than input\n\t\t\t\t# return a number larger than all in alphabet\n"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\treturn float(string)\n\ndef float_to_int(float: float, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a number to a number (possibly using the given alphabet).\n\nFor example, \"2.14\" will try to parse \"2.14\" as floating point number.\nThis will return 2 ** 32.\n\nThe input is assumed to have the most significant digit first.\n\"\"\"\n\treturn int(float)\n\ndef get_alphabet_and_values_dict(alphabet: List[str], values_dict: List[str] = None"}]}
{"task_id": "62ece4982e6aefcf4aabbd6f", "prompt": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\tdef get_tar_gz(self, url):\n\t\t# get a tar.gz archive\n\t\ttry:\n\t\t\tresponse = requests.get(url)\n\t\t\tfile_content =response.content\n\t\texcept Exception as e:\n\t\t\treturn None\n\t\t\n\t\t# extract the archive\n\t\t# return the.tar.gz archive with only 'desc' file\n\t\t# of each package extracted\n\t\ttar_gz = tarfile.open(file_content, 'r:gz')\n\t\tfor f in tar_gz: # get all files"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\ttry:\n\t\turlpath = url.replace(\"http://\",\"https://\",\"\")\n\t\tsession = requests.session()\n\t\turl = session.get(url=urlpath,proxies=proxies).text\n\t\tresponse = session.get(url)\n\t\tif response.status_code >= 400:\n\t\t\traise Exception, response.text\n\t\tarchive=Path(response.text,destination_path=str(Path.cwd()))\n\t\tos.makedirs(str(Path.cwd()),exist_ok=True)\n\t\toutdir"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\tr = requests.get('https://downloads.sourceforge.net/release/pkg/%s/ArchLinux-v%s-x86_64.tar.gz' % (url, arch))\n\n\tif not r.ok:\n\t\tprint('Error: could not download arch & arch-extra arch zip')\n\t\tsys.exit(-1)\n\n\tresponse = r.text\n\tdestination_folder = os.listdir(destination_path)\n\tdestination_file_list = []\n\tfor i in range(0,len(response)):\n\t\tdestination"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\treturn Path(urlretrieve(URL,destination_path),'.tar.gz')\n#--End Function\n#\n\n###  Extract packages contained in 'url' archive. \n\n#--Start Function\ndef extract(url):\n    \n#  Extract all.pkg archives from the archive on disk.\n    archives = {}\n    f = request('http://search.cpan.org/package/info')#(url)\n    if f.status_code == 200:\t\t# if request is successful. (from GET)"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\tfor root, dirs, files in os.walk(r)\n\t\tfor folder in files:\n\t\t#for folder in dirs:\n\t\t\tprint(\"download: {}\\n\".format(folder))\n\t\t\tfile_name = os.path.abspath(os.path.join(root, folder))\n\t\t\tif os.path.isfile(file_name):\n\t\t\t\tfile_output = requests.get(url + \"/\" + file_name,headers=headers,allow_redirects=True,timeout=4,verify=False).text.replace('+','')\n\t\t\t"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\treturn requests.get(url, stream=True).content[:10097] # https://github.com/fppal/simple_dpkg/blob/master/simple_dpkg/http_api.py\n\ndef _fetch(url):\n    \"\"\"\n    Fetch the contents of an url in parallel.\n\n    Args:\n        url(str): URL to fetch from.\n\n    Raises:\n        TimeoutError: if fetching the contents of the url to a timeout occurs.\n "}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\trequests_log_request = 'GET /repos/%s/archives/%s/%s?per_page=200&page_num=-1' \\\n\t\t\t\t\t\t\t\t'&per_page_size=500&page_size=20'  # log all to console and redirect to stdout\n\t# download archive\n\n\ndef download_archives(package_id, url):\n\t# prepare response\n\tresponse_json_file ='response_json_file.json'\n\n\t# download package\n\tresponse = requests.get(url, verify=False, cookies=requests_"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\tif len(sys.argv) < 2:\n\t\tprint (\"URL or destination directory is missing, please enter url and destination_path\")\n\t\tsys.exit()\n\ttarget_dir = str(sys.argv[1])\n\t\n\t# check whether the destination path exists\n\tif not os.path.exists(target_dir):\n\t\tprint (\"Destination directory doesn't exist\")\n\t\tsys.exit()\n\t\n\t# check whether the url and destination path exists\n\telif not re.search('^'.join(map(str.lower, sys.argv[1"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\timport tarfile\n\ttry:\n\t\timport requests\n\texcept ImportError:\n\t\treturn \"An error occured. Can't make the download in this tool.\"\n\turl = url.lower().replace('/archlinux', '')\n\trequest_url = url + '/archlinux/systemd/x-raw-iso9660'\n\trequest = requests.get(request_url, stream=True)\n\n\tdef extract_command_to_tar(destination_path):\n\t\tcommand = 'tar xzf {0}'.format(destination_path)\n\t\tcommand +='--"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\ttry:\n\t\timport requests\n\t\ts = requests.Session()\t\n\t\ts.post('https://storage.googleapis.com/archlinuxarm/downloads/url/{0}/{1}'.format(s.cookies['archlinuxarm']['token'], url), data={'destination_path': os.path.abspath(destination_path)})\n\t\ts.close()\n\texcept KeyboardInterrupt:\n\t\treturn None\t\t\t\t\t\t \n\texcept Exception as e:\n\t\tprint \"An unexpected exception is thrown\\n\", e\n\t\treturn None\t\t\t\t\t\t  \n\treturn extract_tar_"}]}
{"task_id": "62ece4982e6aefcf4aabbd70", "prompt": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\tos_s = list(map(int, sorted([\"10\", \"6\"])))\n\treturn(platform.mac_ver()[0]() in os_s) \n\n\t\ndef mac_versions():\n\tglobal\tos_s\n\tos_s = []\n\tsystem, OSX, Solaris = os.uname().split(' ')\n\tplat = \"%3.2f\"% (float(system)/10.0)\n\tif os == 'Darwin':\t\t\n\t\tif OSX == \"10.4.1\":\n\t\t\tif not os_s:\n\t\t\t\t"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\treturn False\n\n\tdef get_build_path(self):\n\t\t\"\"\"Returns the path of the build directory\"\"\"\n\t\tif self.platform == \"win32\":\n\t\t\tprint(\"Overriding the base directory\")\n\t\t\treturn os.path.join(\n\t\t\t\t\tos.path.expanduser(\"~\"),\n\t\t\t\t\t\"Documents\",\n\t\t\t\t\t\"OS X\")\n\n\t\treturn None\n\n\n\n\tdef _get_build_dir(self, build_env):\n\t\t\"\"\"\n\t\tA getter to find the path of the environmental"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\tos_version = [x for x in tuple(map(int, release.split(\".\")) if int(x)]) if x] or None\n#\t\t\t\t\timport os\t\"\"\"\n\timport subprocess as sp\n\timport platform\n\timport platform\n\timport subprocess as sp\n#\timport sys\t\"\"\"\n\timport sys\n\timport os\n\tif sys.platform[:4] == [\"linux\", \"win\"]:\n\t\tif sys.version_info > (3,8):                       # Linux"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\tif os.name == \"osascript\": #pylint\n\t\treturn False\n\treturn True\n\n\ndef is_linux():\n\t\"\"\"\n\tChecks if the os is linux\n\n:return: True is linux\n:rtype: bool\t\"\"\"\n\treturn platform.system() == \"Linux\"\n\n\ndef is_windows():\n\t\"\"\"\n\tChecks if the os is windows\n\n\t:return: True is windows\n\t:rtype: bool\t\"\"\"\n\treturn platform.system() == \"Windows\"\n\n\ndef is_osx():\n\t\"\"\"\n\tChecks"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\treturn isMacOS and os.name == \"darwin\"\n\n\ndef isUnix() -> bool:\n\t\"\"\"\n\tChecks if the os is osx\n\n:return: True is linux\n:rtype: bool\t\"\"\"\n\treturn isUnix and os.name == \"linux\"\n\n\ndef isWindows():\n\t\"\"\"\n\tChecks if the os is windows\n\n:return: True is windows\n:rtype: bool\t\"\"\"\n\treturn isWindows and os.name == \"windows\"\n\n\ndef isMacOS() -> bool:\n\t\"\"\"\n\tChecks if the os"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\treturn True\n\ndef _set_mac_mode():\n\t\"\"\"\n\tCheck the environment memento\n\n:return: True if macOS\n:rtype: bool\t\"\"\"\n\t_is_osx = platform.system() == \"Darwin\"\n\n\tif _is_osx:\n\t\tif sys.platform == 'darwin':\n\t\t\treturn True\n\n\tif not _is_osx:\n\t\traise Exception('Error!!! Only macOS!!!')\n\n\treturn False\n\n\n\nif _set_mac_mode() is None:\n\n"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\treturn ('macOS' in os.uname()).lower() =='macos'\n\ndef windows(ip):\n\treturn os.name == 'nt' and \\\n\t\t(ip[0] == 13 or ip[0] == 10) and \\\n\t\t(ip[1] >= 26 and ip[1] <= 40)\n\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\ttest_list = []\n\tfor mac_os, linux in ((\"iPhone\", \"10.14\"), (\"iPhone\", \"iPhone OS 10\", \"12\"),\n\t                      (\"iPhone OS 10\", \"12.14\"), (\"iPhone OS 10\", \"12.14 1\", \"12.14\"),\n\t                      (\"iPhone OS\", \"10.14\", \"11.14\", \"12.14"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\tif not os.environ.get('MAC_OS', False) or \"MACOS\" in os.environ.get('MACHINE', True):\n\t\treturn True\n\telse:\n\t\treturn False\n\n\ndef setMacOS(value):\n\t\"\"\"\n\tSet this function to be macOS\n\n\t:param value: boolean  True/False\n\t:return: None\n\t:rtype: None\n\t\"\"\"\n\tsetMacOSFlag = True\n\t\n\tdebugFlag = 1\n\tif os.getuid()!= 0:\n\t\tdebugFlag = os.getlogin"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\treturn True if sys.platform == 'darwin' else False\n\n@python_function(name=\"is_linux\")\ndef is_linux():\n\t\"\"\"Checks if the os is linux or windows\"\"\"\n\treturn os.name == 'Linux' or os.name == 'Windows'\n\n@python_function(name=\"is_macos\")\ndef is_macos():\n\t\"\"\"Checks if the os is macOS or is the os os x version\"\"\"\n\treturn 1 if os.name == 'Mac' or 2 if sys.platform == 'darwin' else 0\n\n@python_function"}]}
{"task_id": "62ece4982e6aefcf4aabbd71", "prompt": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tfor key in regex_keys:\n\t\tv = item[key]\n\t\tif '?' in str(v)):\n\t\t\tf_key = re.sub(r'(.*)', r'(\\1)', key)\n\t\t\treturn re.sub(r'(.*)', r'\\g<1>{0}', f_key)\n\t\telse:\n\t\t\treturn item[key]\n\n\ndef escape(content, delimiter):\n\t\"\"\"Escape a string for use as variable in an expression\n\n\tArguments:\n\t\tcontent\t- string of content\n\t\tdelimiter"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tgdict={}\n\tfor k in regex_keys:\n\t\tfor v in items:\n\t\t\tgdict[k] = v\n\treturn gdict\n\ndef split_regex(key):\n\t\"\"\"Split (split_key_string) into a list of groups of key/value pairs.\n\tReturns:\n\t[group_dict for each key in item]\n\t\"\"\"\n\tgroups=groupings(key)\n\n\treturn [[] for i in range(len(groups) - 1)]\n\ndef split_to_dict(key: str, regex_dict"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tfor old_key, new_key in regex_keys.iteritems():\n\t\tif not re.match(old_key.replace('.', '\\\\.'), item['key']):\n\t\t\t# convert key\n\t\t\titem['key'] = new_key\n\treturn item\n\n/*---------------------------------------------------------------------------------------------\n *  Copyright (c) Microsoft Corporation. All rights reserved.\n *  Licensed under the MIT License. See License.txt in the project root for license information.\n *--------------------------------------------------------------------------------------------*/\n\ufeffnamespace Microsoft.Azure.PowerShell.Cmdlets.ContainerInstance.Runtime.Json\n{\n  "}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\tout = {}\n\t\tfor k in regex_keys.keys():\n\t\t\tout[re.sub(k,'',item[k].lower())] = item[k]\n\t\treturn out\n\texcept KeyError,val:\n\t\treturn item\n\ndef convertDictKeysToRegExpValues(item):\n\tnewItems = []\n\tfor k in item.keys():\n\t\tnewItem = re.match(k,'(^.+)(\\.[^.]*)$',item[k].lower())\n\t\tif newItem:\n\t\t\tnewItem = re.sub("}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\trules, item = item \n\tnew_dict = {} \n\tfor item_key in rules:\n\t\tnew_dict[item_key] = re.compile(item[item_key])\n\treturn new_dict\n\n<?php\n\n/**\n * This file is part of the Propel package.\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n *\n * @license    MIT License\n */\n\nrequire_once dirname(__FILE__). '/../../../../../generator/lib/util/PropelQuick"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t# Convert the *.cpp key in regex_keys to a regex and change those values to wildcards\n\t# i.e., to match the first and last characters of the filename\n\t# Example: ['.*.cpp', '.*.cpp', '.*.h', '.*.cpp']\n\tfor key in regex_keys:\n\t\tfor i in item[key]:\n\t\t\titem[key][i] = re.escape(item[key][i]) + \"*\"\n\t\tbreak\n\treturn item\n\n\n#\n# MAIN\n#\n\nimport sys\ntry:\n"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tpattern = re.escape(item[\"file_wildcard\"] + \"*\")\n\treturn [re.escape(k) for k in re.findall(pattern, item[\"key\"])]\n\n<?php\n\nnamespace PhpOffice\\PhpSpreadsheet\\Reader\\Xls;\n\nuse PhpOffice\\PhpSpreadsheet\\Reader\\Exception as ReaderException;\n\nclass Workbook extends AbstractReaderPart\n{\n    const FOOTER = 0x0000000D;\n\n    const BIFF5 = 0x0000000E;\n\n    const BIFF8 = 0x0000000"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tm = re.search(*r\"/\\w*-\\w\", item)\n\tif not m:\n\t\tm = re.search(*r\"[^\\w]+/\", item)\n\t\tif not m:\n\t\t\traise ValueError(\"item error\")\n\t\tm = m.group()\n\t\treturn dict(zip(['a', 'b', 'c'], m.split()))\n\tnew_str = list(m.groups())\n\treturn dict(zip(['a', 'b', 'c', 'h'], new_str))\n\nprint(regex_keys['*.*"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tfor key in regex_keys:\n\t\tyield re.compile(key, re.VERBOSE)\n\npackage com.google.android.gms.phenotype.e2e;\n\nimport android.os.Parcelable;\nimport android.os.Parcel;\nimport android.os.Parcelable.Creator;\n\nfinal class ParcelableParcelUtils {\n    public static <T extends Parcelable> T setFieldValue(T parcelable, int fieldCode, Object value) {\n        parcelable.setDataPosition(fieldCode);\n      "}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\treturn dict([(re.sub(r'(\\.h|\\.cpp|\\.cxx)'),re.sub(r'^(\\W|\\w)~', '\\2', r'\\2'))+key for key,value in regex_keys.items()])\n\n\nfrom __future__ import absolute_import, division, print_function\n\nimport contextlib\nimport os\nimport re\nimport string\nimport sys\nimport traceback\nfrom binascii import a2b_hqx\n\n__all__ = [\"__version__\"]\n\n\n_IS_PYTHON2 = platform.python_implementation()"}]}
{"task_id": "62ece4982e6aefcf4aabbd72", "prompt": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tdef remove(self):\n\t\t\"\"\"Remove quotation from name.\"\"\"\n\t\tr = self\n\t\tn = 1\n\t\twhile 3 > n and r.chr(3-(n+1))!= \"\\\\\":  # \" and r.chr(3-(n+1))!= \" \":  # \"\\\\\": \":\n\t\t    \tl = 3-(n+1)\n\t\t\tif r.chr(l) == \"\\\\\":\n\t\t\t\tn += 1\n\t\t\t\tcontinue\n\t\t\tif r.chr(l) & cgt(LQUOTE, \"''\") and"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\timport os, sys, getopt\n\tf = sys.stdout\n\ttry:\n\t\topts, args = getopt.getopt(sys.argv[1:], \"h\", [\"help\"])\n\texcept getopt.error as msg:\n\t\tprint(__doc__)\n\t\tsys.exit(2)\n\tfor opt, arg in opts:\n\t\tif opt == '-h':\t\tf.write(help())\n\t\tif opt == '-i':\n\t\t\tf.write(get_path(arg))\n\t\t\tcontinue\n\t\ttry:\n\t\t\tpath, d"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tdef removequote(self, name, encoding = 'iso-8859-1'):\n\t\t\"\"\"Remove quotes from a name to test against.\"\"\"\n\t\tself.removequotes(name, '\"', 'quote')\n\nclass RemoveWhitespace:\n\t\"\"\"\n\tRemove whitespaces from a name.\n\t\"\"\"\n\tdef removewhitespaces(self, name):\n\t\tu\"\"\"Removes whitespaces from a name and returns the output as a list.\n\n\t\tArguments: name - The name of the text/name pair (s or d).\n\n\t\treturns a list:\n\t\t  "}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tmyResult = {}\n\tfor z in mySource:\n\t\tif ('\\''+z in myResult):\n\t\t\tmyResult[z] = myResult[z].replace('\\'\\\\'', '').replace('\\'$\\'','\"\"' )\n\treturn'' if z in ('\",',',',') else '', ','.join( re.findall('(?<!\\\\\\\\)\\\\'\",\"') + r.replace('\\'\\\\\\\\','\\\\'').replace('\\\\'\",'\"') )\n\nif __name__ == \"__main__\":\n\tmain()\n\t\n# Enter your strings here. Format the"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tdef booh(self,t,s,x):\n\t\t##\tReturn the first word following * in the name.  Return the remainder of the string (without first word). \n\t\t##\tReturn the word itself: the first word.\n\t\t##\tReturn 'xxx': 'xxx'\n\t\t##\tReturn a list of words: [('a','b'),('a','a'),('b','b'),('b','a'),('a','x'),('b','x'),('a','y'),('x','x')]\n\t\t##\tReturn a list of (word"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tfrom s1 import *\n        from s2 import *\n\ts3 = \"                   \\n  f    d     g\\n    h    i      j\\n     k      l      m\\n\"\n\n\tdquote = '''\\\n\t'''.replace('\\\\','\\\\').replace('\\\"','\\\"')\t'''\n\n\t\"\"\"\n\tif sys.argv"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tif name[0] in [\"'\", '\"', '\\\\']:\n\t\t#\tQuote the name\n\t\tname = re.sub(r\"'\", r''\"\\\\'', name)\n\telse:\n\t\t#\tNo quote\n\t\tdel name[0]\n\treturn name\n\n#\tRemove the last part of the name from the return string.\t\ndef _strip_last_quote(name):\n\t\"\"\"Remove the last quote from a name.\"\"\"\n\tif isinstance(name, basestring):\n\t\tname = name.translate(None, name)  # remove quote on"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tdef g(s, p):\n\t\tfor i in p:\n\t\t\tre.sub('\\\\b[%s]' % (i,),'',s)\n\t\"\"\"Remove quote from the given name.\"\"\"\n\tp=\"\"\"\n\t<p>This is not a quoted string.            #<-->This is <a href=\"h2\">h2</a>\n\t<p>This is not a quoted string or a comment.\n\tThis is <a href=\"h1\">h1</a>\n\t<p>This is not"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\twith open(fn, \"r\") as f:\n\t\tres = f.read()\n\t\tfor d in res.split(r\"\\n\"):\n\t\t\ttry:\n\t\t\t\tname = d.split(\"\\\"\")[2]+d.split(\"\\\"\")[0]\n\t\t\texcept IndexError:\n\t\t\t\treturn False\n\t\treturn True\n\n\ndef read_db(path_to_db_dir,\n\t\t\t   sql_db,\n\t\t\t   sql_db_url_list,\n\t\t\t   dbname,\n\t\t\t   sql_db_url=\""}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tfor g in names:\n\t\tif g==name:\t\t\n\t\t\tmsg.edit(\"Remove quote\")\n\t\t\tdel names.get(re.sub(r\"\\s+\", \" \", g))\n\t\t\tbreak\n\t\n\n\ndef remove_all_quotes (msg):\n\t\"\"\"Remove all quotes from the name of all known modules\"\"\"\n\tnames = dict.iterkeys(modules)\n\tfor g in names:\n\t\tmodule = \".\" + g\n\t\ttry:\n\t\t\tmsg.edit(\"Remove\" + module)\n\t\texcept: pass\n\t\telse"}]}
{"task_id": "62ece4982e6aefcf4aabbd73", "prompt": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\tif platform == \"\":\n\t\tplatform = platform_detect(False)\n\ttry:\n\t\tout = platform\n\t\tout = out.encode(\"mbcs\")\n\texcept UnicodeDecodeError:\n\t\tif platform!= \"this\":\n\t\t\traise SystemError(\"I don't know the platform (probably the \" + platform). Try'set platform=%cd'\"%(platform, ))\n\t\telse:\n\t\t\tout = platform # try without the'set -u'\n\t(word, rest) = out.partition(\" \")\n\tif len(rest)>0"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\tif 'cmd' in globals ():\n\t\tfrom cmd import (split_commandline, split_commands,\n\t\t\t\thandle_options)\n\n\"\"\"\n\n\n# This file is to be included into Python file so that it can be executed\n# using the Python shell, as well as other non-Python objects.\n\n# This is in place to allow access from Python code to it's own libraries\n# and functions which cannot be automatically executed\n\nclass CmdLine(object):\n\n    \"\"\"\n    A class to provide a general purpose interface for the cli.cli.\n"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t__doc__=\"\"\"shlex.split(s [,sop [0]=='-'], [w]) -> list of tokens, -1 for end of list\n\n\tThis function is not reentrant. Do not change the global state of this module.\n\ns: search string, possibly with wildcard (Windows)\n     wildcards < n chars will be expanded; they are returned as tokens,\n     e.g. 'a*' will be expanded to ['a']\n     w: white space token (space/tab/newline)\n     o"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\tif os.name!= \"nt\":\n\t  def shsplit(pattern=r\"\\W+(?:-[^0-9]*)\", flags=0):\n\t\t  from _shlex import split #@UnresolvedImport\n\t\t  from reg import shell\n\t\t  return (\n\t\t    (\n\t\t      ((not re.search) and (flags!= 0) or\n\t\t       (flags & split.I)) and\n\t\t      ((not re.search) and (flags & split.NOT) or\n\t\t       (flags & split.B"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\tprint(\"shlex.split(cmd, [shell=shell], [regex] = 1))\")\n\tprint(\"shell = the command shell;  regex = 1\")\n\tprint(\"shlex.split(cmd, [shell=shell], [regex] =(0|1)))\")\n\n# This is more than one line because most people don't really do,\n# but I thought that didn't work for a while.\n\n# # For the most part, here's a little more helpful.\n# The way to do this with REGEX is simple:\n\t# for reg_"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t# Set up the parser and parser state to parse words from the command line, so\n\t# that if line contains whitespace, all the subsequent words become part of\n\t# the current token (and this process is returned to the pool of available\n\t# commands).\n\t#\n\t# Note: this uses a pipeline with an \"S\" terminal case to terminate words,\n\t# whereas an \"R\" parser would work with a \"N\" parser.\n\t#\n\t# It's NOT a good idea to process shells in isolation, however you do it\n\t# or better.\n\t"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\tif not(os.path.isdir(\"/etc\")) and sys.platform==\"win32\" and'reg' not in os.environ:\n\t\t\tprint \"You must supply a directory with shlex.\"\n\t\t\texit(0)\n\t\t\t\n\t\tif sys.platform==\"win32\":\n\t\t\timport msvcrt\n\t\t\tfrom subprocess import call, Popen, PIPE\n\t\t\tcmdLine=\"cmd /c\\\"\\\"\\\\\"\n\t\"\"\"\n\treturn shlex.split(cmdLine + \" \\\"\" + string.split('\\\\\",string.replace(\"\\\"\", \"'\",str(len"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\timport sys\n\timport string\n\tfrom time import strftime\t\n\tfrom platform import *\n\t#from platform import *\n\t#if sys.platform == \"win32\":# is this still needed?\n\t#\tfrom _subprocess import *\n\t#else:\n\t#\tfrom subprocess import *\n\tcmdList = \"cd ~/.bash_profile && cd ~\" + sys.executable +'&& cd ~/.bashrc && \"bash -n >/dev/$1\" || 'bash -n >/dev/$1' || 'bash -r --no-run \"$(id"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\npattern = re.compile('(^|[^ ])\\\\W*(\\\\([^)]+\\\\))')\n  \ndef escape(command,platform='this'):\n\t\"\"\"escape() doesn't use escape for cmd.exe, for bash (cmd.exe)\n\tand gawk (which uses'shlex').\"\"\"\n\t# 'this' is a special version of a command line shell.\n\t# This is used for shell escaping.\n\tcommand = shell_quote(command)\n\n\t# for gawk shell we simply escape the quotes as they appear.\n\tif platform =="}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t#------------------------------------------------------\t#\n\t#\t\tUse this for when you want to:\n\t#\n\t#\t\t1.\tExecute a shell command in-situ;\n\t#\t\t2.\tCreate and execute a shell command or\n\t#\t\t \tcommand in-line (in shell or in-situ),\n\t#\t\t3.\tExecute a command and store the output,\n\t#\t\t4.\tGet a shell command output,\n\t#\n\t#------------------------------------------------------\t#\n\n\t#------------------------------------------------------\t#\n\t#\tOther than in-s"}]}
{"task_id": "62ece4982e6aefcf4aabbd74", "prompt": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from io import StringIO\n\tpass#IMPORTIMPORTIMPORT from zipfile import ZipFile\n\timport os\n\timport zipfile\n\tfrom..libinfo import path_join\n\tdef _path_join(*args): return path_join(*args)\n\tos.chdir(os.path.expanduser(_path_join(\"..\", \"lib\")))\n\tpath = _path_join(\"uncompress\", \"ArchiveInfo.zip\")\n\t#os.chdir(os.path.split(sys.argv[0])[0])\n\twith Zip"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t@classmethod\n\tdef uncompress(cls, input_file, output_path):\n\t\toutput_dir = os.path.split(output_path)[0]\n\t\ttarget_path = os.path.abspath(output_path + output_dir)\n\t\tdest_path = os.path.abspath(input_file)\n\t\tos.chdir(local_path) # Changing current directory\n\t\tos.chdir(output_dir) # Changing folder\n\t\tcmd = '''echo \"{source} > {target}\" | unzip -f -d -"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tparser = argparser.ArgumentParser(usage=argparse.SUPPRESS)\n\targs = parser.add_argument_group(title='Argument Group:')\n\targs.add_argument('archive_path', help='Name of the archive to uncompress', required=True)\n\targs.add_argument('-r', '--root', default=None, help='The root directory where to uncompress the archive from. Default is the current directory.')\n\targs.add_argument('--force', action='store_true', required=False, help='Override the default behavior if the archive already exists.', default"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tos.chdir(\"..\")\n\tresult = subprocess.call(\"gzip\", \"-d\", \"z.py\")\n\tif result == -1: raise Exception(\"Not a compressed archive\")\n\t# This returns a tuple\n\tif not (isinstance(result[1],basestringtype) and result[1] and result[2]):\n\t\traise Exception (\"Decompression failed\")\n\treturn result[0]\n\n\n'''\ndef decompress(archive,path):\n\tlocal filename = \"archive.zip\"\n\twith open(path,mode='rb') as f:\n\t\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tprint(\"downloading archived zip file\")\n\turl_to_repo = download_file(\"archived.zip\", archive_path)\n\tprint(\"unpacking archive url\")\n\tcmd_str = 'curl \"https://raw.githubusercontent.com/S3fs/archivetree/master/utils/unpackArchive.zip\" {}/'\n\tcmd = subprocess.Popen(cmd_str.format(archive_path), stdout=subprocess.PIPE, shell=True)\n\toutput = str(cmd.stdout)\n\tif cmd.wait()!= 0:\n\t\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\ttry:\n\t\twith open(archive_path,'rb') as f:\n\t\t\treturn \"http://%s:%s\" % (f.name, f.read())\n\texcept:\n\t\treturn None\n\n\ndef uncompress_archive_url(archive_url,  output_filename):\n\t\"\"\"\n\tUncompress archived archive_url. Returns a file name where the archive lives.\n\tIt will be called with a 'url'. (without the extension)\n\t\"\"\"\n\tcommand = 'uncompress -d -q %s %s/%s.tar"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tsys.argv = [\n\t\t\"tar\", \"-xzf\", \"-C\", \"test.tar\", src_path, \"test1.txt\", src_path, dst_path\n\t]\n\ndef my_tar_unpack(src_path, dst_path):\n\tsubprocess.call([\n\t\t\"tar\", \"-xzf\", \"-C\", src_path, \"test2.txt\", dst_path, \"test1.txt\"\n\t], shell=True)\n\ndef unpack(fname):\n\timport gzip\n\timport fnmatch\n\timport zipfile\n\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\timport requests\n\timport urlparse\n\t_baseurl = 'https://www.theforest.com/'\n\t_baseurl_zip = 'https://ftp.theforest.com/'\n\t_baseurl_http = 'http://www.theforest.com/'\n\t_baseurl_file = 'file://'+_baseurl\n\t_baseurl_folder = 'zip://'+_baseurl_zip\n\t_zipinfo = {}\n\t_zipinfo[\"\"] = {}\n\t_zipinfo[''] = {'name': \"\",'size': 0,"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t@staticmethod\n\tdef uncompress_file(archive_path):\n\t\tr = subprocess.run(['gzip', '-v'], stdout=subprocess.PIPE)\n\t\treturn os.path.abspath(archive_path.parent.parent) / arch.ungzip(gzip.open(r.stdout.decode(\"utf8\"), \"rt\"), archive_path.relative_to(archive_path), \"rt\")\n\t\n\t\n\tdef download(self, src_path, dst_path, src_sha, unpack_dir):\n\t\n\tfunction to_url(src_path, dst_"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tparser.add_argument(\"-c\", \"--compress\", action=\"store_true\", help=\"Use compression for archiving\")\n\n\tparser.add_argument(\"-q\", \"--quiet\", nargs=\"?\", help=\"Quiet option\", type=\"int\", metavar=\"N\", default=0)\n\treturn parser.parse_args()\n\n\ndef parseArgs(argv):\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Uncompress a file based on its contents\")\n\tparser.add_argument(\"-i\", \"--input\", metavar=\"file\",\n\t\t\t\t\t\ttype=str, nargs='"}]}
{"task_id": "62ece4982e6aefcf4aabbd75", "prompt": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\treturn os.listdir(outputPath)\n\n\ndef build_git_commands(args):\n\tgit_path_output = subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"], cwd=args.git_repo_directory)\n\tgit_cmd_output_sort = subprocess.check_output([\"git\", \"status\", \"--format=%n\", '--porcelain=on', \"-s\"], cwd=args.git_repo_directory)\n\tgit_cmd_output_sort.replace(\"  \" + git_path_output.decode(\"utf-"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\ttry:\n\t\targ = subprocess.check_output(['git', 'log', '--no-merges', '--pretty=%h'%self.logFile.name]).decode('ISO-8859-1')\n\texcept OSError:\n\t\targ = 'No changes to log.'\n\tlog.debug('Output is: %s', arg)\n\tif arg in self.ignored:\n\t\treturn arg\n\n\tif not self.ignorePatchedFiles:\n\t\tresult = subprocess.check_output(['git','status', '--porcelain', '--parallel=true']).decode"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tcmd, in_ = \"git ls-files -s -z\"\n\toutput = subprocess.check_output( cmd, shell=True ).splitlines()\n\tall_files = []\n\tif not in_ in output:\n\t\treturn input\n\tall_files = output[:]\n\tignored_files =  os.listdir(\"ignored\")\n\tignored_files.sort()\n\tfiles_count = 0\n\tall_files = all_files[:0]\n\tfor i in ignored_files:\n\t\tif( i in all_files ):\n\t\t\tcontinue"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tp = subprocess.Popen(\n\t\t\t\tcmd,\n\t\t\t\tshell=(\n\t\t\t\t\t'shell','bash','zsh','ksh'),\n\t\t\t\t#stdout=(subprocess.PIPE),\n\t\t\t\t# stderr=(subprocess.PIPE),\n\t\t\t\t# universal_newlines=True)\n\n\tresult = \" \".join(p.stdout.decode(strip_python_encoding).split(b'\\r\\n')) + \\\n\t\t\t\t\t\" \" + str(len(p.stdout.decode(strip_python_encoding).split(b'\\r\\n'))) + \\\n\t\t\t\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tresult = subprocess.check_output(args, stderr=subprocess.STDOUT).decode('UTF-8',errors='replace')\n\t# sort the file list:\n\tresult = result.replace(\"\\n\",\"\")\n\tresult = result.replace(',', ',','').replace('('[')','['(').replace(')]'','[')]','[]')\n\treturn result\n\ndef get_args():\n\treturn sys.argv\n\ndef split(args, file_name):\n\tr\"\"\"\n\tReturn all the args as a nested list separated by a tab char, in that order.\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tfor dirname in os.getenv(\"TRAVIS_TEST_REPOSITORIES\").split(os.path.pathsep):\n\t\ttry:\n\t\t\tcmd = subprocess.Popen(shlex.split(str(dirname)), shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\t\t\toutput, err = cmd.communicate()\n\t\t\treturn output.lower().splitlines()\n\texcept Exception as e:\n\t   print(e)\n\ttry:\n\t\tos.remove(str(join(dirname, \"ignored_file_"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\timport os, getopt, re\n\t\t__all__ = __all__.split(\",\")\n\t## Find all the files and parse their contents\n\t\tin_data=open(sys.argv[ 1])\n\t\tfor matchstr in __all__:\n\t\t\tprint matchstr\n\t\t\tif \"=\" in matchstr:\n\t\t\t\tfname=matchstr.split(\"=\")[1]\n\t\t\t\tfname=fname.strip()\n\t\t\t\ttry:\n\t\t\t\t\tf=open(fname)\n\t\t\t\texcept:\n\t\t\t\t\tin_data.close()\n\t\t\t\t\tcontinue\n\t\t\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\trepoDir = repoPath\n\n\tsubprocess.call('git log -1 --pretty=format:\"%h\"', shell = True)\n\tif len(sys.argv) > 1:\n\t\tcheckCmd = sys.argv[1]\n\telse:\n\t\tcheckCmd = \"check\"\n\tcheckOutput = subprocess.check_output(['git', 'log', '-1', checkCmd]).decode()\n\tif checkOutput == \"\":\n\t\tprint(\"[ERROR] check failed, check output is empty.\")\n\t\treturn None\n\tignoreList = []\n\tfor eachLine in"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\ttry:\n\t\tflist = subprocess.check_output([\"git\", \"tag\", \"-m\", \"feature test file1\", \"master\"])\n\t\tf = int(flist.split(\":\")[0])\n\t\tflist = subprocess.check_output([\"git\", \"grep\", \"--recursive\", \"-p\", \"feature test file1|.*ignored\"])\n\t\tfignore = int(flist.split(\":\")[0])\n\t\tif fignore == f:\n\t\t\treturn flist.split(\":\")[1].replace(\"test file\", \"test file2\")\n\texcept"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tcmd = \"git diff --name-only origin/0..HEAD --name-ignore-pattern \"^.*^\\\\s?[]\\\\d{6,}.\\\\w{2,4}-\\\\w{3}\\\\.py\" \"\"\".join( args ) )\n\tprint(\"\\n[*] Checking the Git log:\")\n\tprint( \"   {}\".format( cmd ) )\n\ttry:\n\t\tout = subprocess.check_output( args, cwd = cwd )\n\texcept subprocess.CalledProcessError:\n\t\tprint(\"Git log seems to not be in a valid branch. Make sure"}]}
{"task_id": "62ece4982e6aefcf4aabbd76", "prompt": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\ttry:\n\t\treturn sys.maxsize <= 1 and isinstance(filename, str)\n\texcept:\n\t\tprint(\"No type defined. Supported types: [str]\")\n\t\treturn False\n\ndef typeCheck(type) :\n\ttry :\n\t\tif type == 'None' :\n\t\t\treturn True\n\t\tif type in [None,\"None\"] :\n\t\t\treturn True\n\t\tif type in [bool,int,float] :\n\t\t\tif type[1:-1].lower() in [str, type[1:-1].lower() ]:\n"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\ttry:\n\t\tf = open(filename)\n\t\tl = f.readlines(4)\n\t\tf.close()\n\texcept IOError as e:\n\t\tprint(e)\n\t\treturn False\n\treturn True\n\ndef __version_compare():\n\t__version = None\n\ttry:\n\t\t__versionInfo = os.path.relpath(\"/usr/bin\",\n\t\tos.path.abspath('/etc/apache2/version.py'))\n\t\t__versionObj = open(__versionInfo, \"r\")\n\t\t__version ="}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t#import logging\n\t#logging.basicConfig(level=logging.INFO)\n\tlog.add(logginglevels.DEBUG,\n\t\t\"Verify filename\", logginglevels.DEBUG)\n\text = os.path.splitext(filename)[1].lower()\n\tif ext in ('.rar', '.vdi', '.bdm'):\n\t\t#log.add(logginglevels.INFO, 'File', logginglevels.INFO)\n\t\tif '.' not in ext:\n\t\t\t#log.add(logginglevel"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\tif os.path.isdir(filename):\n\t\tfor file in os.listdir(filename):\n\t\t\tfilepath = os.path.join(filename,file)\n\t\t\tif file.lower().endswith(\".so\"):\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\tif os.path.isfile(filepath): return False\n\t\t\tos.remove(filepath)\n\n\treturn False\n\n\n\n\nimport sys\na = False\ntry:\n    filepath = open(\"data.txt\")\n    if sys.argv[0].lower()"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\treturn bool(os.path.splitext(filename)[1])\n\ndef get_filename(handle):\n    \"\"\"Get the filename associated with the data.\"\"\"\n    # Get the handle to the underlying OS handle object\n    handle.get_native_info_handle()\n\n    # Get the filename associated with this handle\t\n\tlib_name = \"lib\"\n\tif iswin64():\n\t\tlib_name = \"lib64\"\n\n\tdata_name = get_filename(None, handle.as_posix())\n    if"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\tret = False\n\t\n\tfor ext in ['.png', '.TGA', '.jpg', '.mp3', '.bmp', '.psd', '.jpeg', '.tiff']:\n\t\tif os.path.splitext(filename)[1]!= ext:\t# Not this file type\n\t\t\treturn False\n\treturn ret\n\t\t\ndef check_type(filename):\n\t\"\"\"Validates a filename for this type\n\n\tArgs:\n\t\tfilename: Filename to check\n\nReturns:\n\tTrue if valid or False if invalid\n\"\"\"\n\tif checkname(filename) == False:"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\treturn True\n\ndef check_modules_list(filename):\n\tlist_module=None\n\tfor filename in os.listdir(os.path.join(globals.paths.modules,filename)):\n\t\tif os.path.isfile(os.path.join(globals.paths.modules,filename)) and '.py' in os.path.splitext(filename)[0]:\n\t\t\tlist_module=filename\n\tif list_module is not None:\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef get_module_filename(filename,"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\ttry:\n\t\twith open(filename, \"r\") as fp:\n\t\t\tcontent = fp.read()\n\t\tfor line in content.split(\"\\n\"):\n\t\t\tif line[0] not in type_map:\n\t\t\t\treturn False\n\t\treturn True\n\texcept IOError:\n\t\tprint(\"error:\", type(filename), file=sys.stderr)\n\t\treturn False\n\nusing EssentialUIKit.ViewModels.Help;\nusing Xamarin.Forms;\nusing Xamarin.Forms.Internals;\nusing EssentialUIKit."}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\tfor f in [\".cfg\", \".dat\", \".log\", \".log.gz\", \".zip\"]:\n\t\tif os.path.exists(f):\n\t\t\tbname = os.sep.join(name.split(os.sep)[0].split(f)[1:])\n\t\t\tif os.access(bname, os.X_OK):\n\t\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\ndef main(filename, verbose = False):\n\t\"\"\"Main function in file.\"\"\"\n\ttry:\n\t\tif verbose:\n\t\t\tverbose()\n"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\tfilename = os.path.filename( filename )\n\tif not os.path.isabs( filename ):\n\t\tfilename = os.path.normpath( filename )\n\telif not os.path.isfile(filename): #if no extension\n\t\treturn False\n\telse: #if is file but with extension\n\t\tfilename = os.path.splitext(filename)[0]\n\tfor ext in extensions:\n\t\tif ext == '.pyd' or ext == '.py':\n\t\t\treturn filename.lower().endswith( ext )\n\tif '.pyc'"}]}
{"task_id": "62ece4982e6aefcf4aabbd77", "prompt": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\ttry:\n\t\treturn datetime.timedelta(float(f))\t\t\n\texcept ValueError:\n\t\treturn None\nimport datetime\nif type(f) == datetime._tzinfo:\n\tif hasattr(f, 'dst'):\n\t\treturn datetime.timedelta(\n\t\t\t\t\tseconds=f.dst(datetime.datetime.now(), timezone('UTC')),  # type: ignore\n\t\t\t\t\tmicroseconds=f.microsecond*1e6\t) # type: ignore\n\tif hasattr(f, 'tzinfo'):\n\t\treturn f.tzinfo\n\nimport datetime\nif"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tresult = parse_frequency(frequency)\t#\t#\t#\t#\t#\n\tif not isinstance(result, datetime.timedelta)\t#\t#\t#\t#\t#\t#\n\t\terror_string = \"The given frequency \"%frequency%\" is invalid (must be a number).\"\t\t\t#\t#\t#\t#\t#\t#\n\t\tif len(error_string):\n\t\t\traise ValueError(error_string)\n\t\telse:\n\t\t\treturn None\n\telif result is None:\n\t\t\tprint \"Using default for frequency '%frequency%'.\""}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tif isinstance(value, int) and value < 1:\n\t\traise ValueError(\"%s is not a valid frequency, must be \"\n\t\t\t\t\t\t\t \"in the range 1..5.\" % value)\n\n\tif value in KNOWN_FREQUENCYS:\n\t\tseconds, frequency = value, KNOWN_FREQUENCYS[value]\n\n\telse:\n\t\tseconds, frequency = time.mktime(time.strptime(value, \"%Y-%m-%d %H:%M:%S\")), Frequency.ALWAYS\n\n\t# now have a valid frequency; convert the time to seconds\n\t#"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tif not frequency:\n\t\tfrequency = None\n\tif frequency == \"always\" or frequency in (\"weekly\", \"daily\", \"hourly\", \"daily_unit\"):\n\t\treturn datetime.timedelta(hours=hours, minutes=minutes, seconds=seconds)\n\tif frequency == \"monthly\":\n\t\treturn datetime.timedelta(days=days, hours=hours, minutes=minutes)\n\tif frequency == \"yearly\":\n\t\treturn datetime.timedelta(days=int(days) * 12, hours=hours, minutes=int(minutes) *"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef _get_unit_from_string(frequency):\n\t\"\"\"\n\tGiven a frequency string and a unit, return \"frequency_unit\" or None if the frequency cannot be parsed.\n\tFor instance, given \"years\", return \"year\"\n\n\tRaise ValueError if the given frequency must be a number and \"year\"\n\t\"\"\"\n\t\ndef _generate_ordinal_values(raw_period_str):\n\t\"\"\"\n\tGiven a raw \"period\" string, return a dictionary with two elements:\n\t- period_values [dict containing {0: freq, 1: value"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tself = args.parse().freq\n\tif(len(self.n) == 0) or self.freq is None or len(self.unit) == 0:\n\t\treturn None\n\tif(self.freq == \"always\" or self.freq == \"last\" or self.freq == \"last week\"):\n\t\tif(datetime.timedelta() == (datetime.timedelta(weeks=7) - datetime.timedelta(weeks=0)).total_seconds()):\n\t\t\traise ValueError(None)\n\t\treturn datetime.timedelta(weeks=7)"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tif not isinstance(frequency, str):\n\t\traise ValueError(\"Frequency must be an explicit string.\")\n\ttry:\n\t\tfrequency = frequency.strip()\n\t\tif frequency==\"always\":\n\t\t\treturn datetime.timedelta(weeks=3)\n\t\tfreqlst = [f.lower()+'s' for f in frequency.split(',')]\n\texcept ValueError as err:\n\t# Could be a str like \"3 weeks\" (i.e. \"3 weeks\") or freqlst=\"3 weeks\" or \"3m\".\n\t\tfreqlst = freq"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tprint('{} is not a valid frequency'.format(unit))\n\treturn datetime.timedelta(weeks=0)\n\n\n\n###########################################################################################\n#                                                                         #\n#      Durations          "}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tdef convert(frequency, unit=u.r, offset=0):\n\t\tif unit in ('days', 'weeks', 'd'):\n\t\t\treturn datetime.timedelta(seconds=int(float(frequency)/u.s)*u.m*10 + int(((float(offset)+1.5)*u.s))*u.m)\n\t\traise ValueError(u.w(\"Invalid frequency value [%s].  Must be \"\n\t\t\t\t\t\t\t\t\t u.r(\"[%s, %s]) or none, but got [%s].\"))\n\n\ndef main():\n\tfor attr, value"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tn = 0 # the result\n\tc = get_convention_freq(freq)\n\tfreq = c.split(\"*\") # drop *\n\tif freq[0] == \"*\" or freq[0][:2] == \"??-?\" or freq[0] == \"always\":\n\t\traise ValueError(\"Unknown frequencies: %s\" % freq)\n\tfor i in range(n):\n\t\ttry:\n\t\t\tt = float(freq[i])\n\t\t\t#print(t)\n\t\t\tif t == 1:\n\t\t\t\tif not c1:\n\t\t\t\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd78", "prompt": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t", "suggestions": [{"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\tlocal = socket.gethostbyname(host)\n\tif local!= 'localhost': return True\n\telse: return False\n\n\"\"\"Gets a connection to the localhost\"\"\"\ndef get_local_connect(host = None):\n\t\"\"\"Get an connect connection to the localhost\"\"\"\n\tif host:\n\t\tc = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\tc.connect((host, 80))\n\t\treturn c\n\telse:\n\t\tc = socket.socket(socket.AF_INET, socket.SOCK_STREAM)"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\ttry:\n\t\tipaddress.ip_address(host)\n\t\treturn True\n\texcept:\n\t\treturn False\n\n\n# http://wiki.python.org/moin/\ndef get_ip(ipaddr):\n\thost, port = ipaddr.split(\"/\")\n\thost = \"http://%s\" %host\n\ttry:\n\t\tipaddress.ip_address(host)\n\t\tprint \"Found %s:%s.\" % (host, port)\n\texcept:\n\t\tprint \"Can't get ip of %s:%s.\" %"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\tsocket.AF_INET == socket.AF_INET6\n\ndef get_ip_address(hostname):\n\treturn socket.getaddrinfo(hostname, None )[0][4] # Returns None if no data available\n\n/*\n * SPDX-License-Identifier: GPL-3.0-only\n * MuseScore-CLA-based userspace version\n *\n * MuseScore Pretty Bitcoin Library\n * Copyright (C) 2019 Matija Mazi\n *\n * This program is free software: you can redistribute it and/or modify\n * it under the terms of the GNU General Public"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\tglobal gSocketServer\n\tif 'localhost' not in gSocketServer:\t\t\t\t# not a local server\n\t\traise CTC_Error('Not a socket server')\n\n\tsocketserver = gSocketServer\n\tfor host in socketserver:\t\t\t\t\t\t\t\t\t# scan for the port on this host\n\t\tif host == host.split(\":\")[0]+\":8888\":\t\t\t# this is an IP address and not an IP address that is a host name\n\t\t\treturn True\n\ndef run():\n\tglobal listencount\t\t\t# number of sockets listening\n\tglobal gSocketServer\t\t\t"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\tres = False\n\t#if os.environ.get(\"HOST_NAME\") == host:\n\t\t#print (\"[INFO] Using localhost for host name\", host, file=stderr)\n\t\t#print (\"[Info] Hostname in environment variable is \", os.environ.get(\"HOST_NAME\"), file=stderr)\n\t\t#print \"No Hostname in environment variable: {}\".format(os.environ.get(\"HOST_NAME\"))\n\t\t#print \"Checking localhost for IP address\"\n\t\tres = False\n\t#if is_ipv4_localnet:"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\tif not host:\n\t\treturn False\n\thostname = host\n\ttry:\n\t\tr = socket.inet_aton(host)\n\t\tif r[0]!= socket.AF_INET:\n\t\t\treturn False\n\t\tdomain = socket.getaddrinfo(host, None)[0][4][1]\n\t\tif domain!= socket.AF_INET:\n\t\t\treturn False\n\texcept (socket.gaierror, socket.error):\n\t\treturn False\n\treturn True\n\ndef is_url(url):\n\t\"\"\"Returns True if the"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\treturn socket.inet_aton(host) is not None\n\n\ndef find_unused_port(bindaddr=None):\n\t\"\"\"\tLooks for unused port in bind address\n\n\t:param bindaddr: The bind address\n\t:return: The address which is a free port for TCP connections or an IP for UDP connections\n\t\"\"\"\n\tif bindaddr is None:\n\t\tbindaddr = get_bound_address()\n\tfor i in range(ports - 1):\n\t\tif bindaddr is None or bindaddr == port_list[i]:\n\t\t\tport"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\tglobal remote_port\n\tif not remote_port:\n\t\tremote_port = 8080 + random.randint(10000, 20000)\n\t\tsocket = socket.socket(remote_addr, type=socket.SOCK_DGRAM)\n\t\ttry:\n\t\t\tsocket.connect( (remote_addr, remote_port) )\n\t\texcept:\n\t\t\tsocket.close_socket()\n\t\t\treturn False\n\t\telse:\n\t\t\tprint('You are connected to '+host)\n\t\treturn True\n\n// Copyright 2018 Google LLC\n//\n// Licensed under the"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\tglobal host_list\n\tif not host_list.has_item(host):\n\t\treturn False\n\tif host_is_local(host):\n\t\treturn host_check(host)\n\telse:\n\t\treturn True\n\n# This function returns the IP address of the host that contains the localhost\n# It returns IPs from all interfaces that are part of the localhost (interface is excluded)\n# The host parameter should be a valid hostname or ip address\n# See https://stackoverflow.com/a/18127721 for more example. See https://stackoverflow."}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\tdef is_local_hostname(host):\n\t\t_return=False\n\t\t#if isinstance(host, (str,Unicode)):\n\t\t#\thost=host.encode('latin-1')\n\t\tif isinstance(host, unicode):\n\t\t\thost=host.decode('latin-1')\n\t\t\tif host.find('/')==0:\n\t\t\t\thost=socket.gethostbyaddr(host)[0]\n\t\t\telse:\n\t\t\t\thost=host\n\t\t#elif isinstance(host, (bytes,type(''))):\n\t\t#\thost"}]}
{"task_id": "62ece4992e6aefcf4aabbd79", "prompt": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\n__reference_path__ = \"./\"\nif sys.platform == \"win32\":\n\timport win32_unicode_paths\n\t__reference_file__ = win32_unicode_paths.UNICODE_PATHS_EXT\n\n\nclass GlobFinder:\n\tdef __init__(self, filename):\n\t\t_patterns = None\n\t\ttry: \n\t\t\twith io.open(filename, \"r\", encoding=\"utf8\") as f:\n\t\t\t\t_patterns = [line.strip() for line in f]\n\t\texcept io.UnsupportedOperation:\n\t\t\t"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\tif match = re.search(reg_path, _path)\n\t\tyield from match.groups()\t\t# match name by index to get the pattern\n\t\tyield re.sub(r'\\s+','', match.group(0))\n\t\tyield '*'\t\t\t\t# '**' would become '*' not '*'\n\t\tyield match.group(0)\n\t\tyield match.group(0) # match name by index\n\n\tdef find_path(self, sub_pattern, path, root_patterns, **patterns):\n\t\t\"\"\"\n\t"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\tself.flags['verbose'] = False\n\t\tself.flags['quiet'] = False\n\t\treturn self.run_command('vagrant find_paths', args)\n\n\t@needs_snarfed_command('find-paths')\n\tdef find_files(self, args):\n\t\tself.flags['verbose'] = False\n\t\tself.flags['quiet'] = False\n\n\t\t# Find all files in paths given on the command line.\n\t\tfor p in args.split():\n\t\t\tself.run(f'stack \"{p}\"."}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\treturn(str(x))\n\treturn\n\n_re_glob = re.compile('([\\w\\d]*)')\n\ndef _glob(f):\n\t\"\"\" Transform the string 'f', returning a tuple containing a list of\n\tglob strings.\n\n\t'f' can be a list-of-strings: an empty string or a string that contains\n\tany number of path segments (or paths and/or paths with paths\n\tor absolute paths) and/or strings with the paths separated by slashes.\n\n\tE.g.\n\n\t>>> _glob(['foo"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\t# Do not edit this file, it was used for conversion\n\t\t\t\t\n\t\tif find_pattern is not None:\n\t\t\tparts = find_pattern.split(None, maxsplit=1)\n\t\telse:\n\t\t\tassert len(find_pattern) > 0\n\t\t\tparts = []\n\n\t\t# Remove duplicates\n\t\tdel parts, find_pattern\n\t\treturn re.sub('^', '.+', parts, 1)\n\n\t\ndef findAll(text=None):\n\t\"\"\"\n\tSearch for files recursively, recursively in all directories.\n\n"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\t@param x [List[str]]\n\t\t@param f [List[Optional[str]]]\tThe list of regular expressions to search\n\t\t@param r [List[Optional[str]]]\tThe list of replacements\n\t\t@param n [List[Optional[str]]]\tA list of strings to match multiple times of\n\t\t@param m [List[Optional[str]]] A list of strings to match multiple times\n\t\t\t\t\t\t\t\t\t\tof multiple occurrences\n\t\t@return [List[List[Optional[str]]]]\n\t\t@see #find_paths\n"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\tif not args: args = []\n\t\tif os.path.isabs(basedir) and os.path.dirname(basedir) in args:\n\t\t\tbasedir = args[0]\n\t\tif not os.path.isdir(basedir): basedir = os.path.abspath(basedir)\n\t\tbasedir = os.path.normpath(re.sub(r'(.*)_$', '$1', basedir))\n\t\tfor p in args+[basedir+'/*']:\n\t\t\tbasedir = os.path.join(basedir"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\ttags = [\"paths\", \"files\", \"matches\"]\n\n\tpattern_strings = [p.pattern for p in patterns if isinstance(p, PEP420_Pattern)]\n\n\tlines   = []\n\tmatches = []\n\n\tfor line in lines: # iterate over the lines\n\t\tmatch     = Match(p=line, pat=pattern_strings[0]) # find the first match\n\t\tmatch_group = match.parts[0]                     # group the match"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\tif (len(args) == 0):\n\t\treturn False\n\treturn True\n\n##############################\n\n\ndef create(dest, source, options, base):\n\t\t\n\tif not isinstance(source, str):\n\t\tdest = dest.copy()\n\t\tdest.extend(source)\n\t\tsource = dest\n\t\t\n\t\tfor key in options:\n\t\t\tif key not in base:\n\t\t\t\toptions[key] = os.path.abspath(dest)\n\t\t\telse:\n\t\t\t\toptions[key] = os.path.abspath(options[key"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\tpattern = None\n\tdef on_match(node):\n\t\tif node.content.strip()!= '':\n\t\t\tyield node.create_pattern(pattern=pattern, pattern_type=pattern_type)\n\t\telse:\n\t\t\tpattern = node.content\n\n\tdef on_departure(node):\n\t\tif pattern is not None:\n\t\t\tnode.insert(pattern)\n\t\telse:\n\t\t\tnode.departure()\n\n\trootpat = re.compile(r'\\*(?P<path>%)|(.*)\\*(?"}]}
{"task_id": "62ece4992e6aefcf4aabbd7a", "prompt": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\tos.environ[\"GITLASH_RUN_TEST_PROGRAM\"]=arg\n#\tprint \"gitbash running: %r\" % (arg,)\n#\tgitbash(arg)\n\n\ndef do_nothing(arg=\"foo\"):\n\n#\tprint \"do_nothing: %r\" % (arg,)\n\tfor y in arg:\n\t\t\tpass\n\n\tprint \"do_nothing: success\"\n\n\ndef do_nothing_long(arg=\"foo\"):\n\n#\tprint \"do_nothing_long: %r\" % (arg,)\n\tfor x"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\timport subprocess\n\t# find gitbash\n\tgitpath = os.path.join( os.path.abspath(currentdir), '../../git.exe' )\n\tr = subprocess.Popen('which git' + gitpath, shell=False,stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n\treturn r == subprocess.EX_OK\n\n@cachedval('funky.base.execute_shell_cmd')\ndef execute_shell_cmd(self, cmd):\n\t\"\"\"Executes a running command given a command name\n\n\t:param str cmd: The command"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\treturn os.name in [\"git\",\"gitbot\",\"gitpython\",\"gitlib\",\"gitp\"]\n#\n\n\n#\n#\tThis program shows how to obtain information about the operating system that\n#\twe have on this machine.  The example command can be written with\n#\tthe '-h -h' option to get this information.\n#\n\ndef get_os():\n\t\"\"\"Returns the operating system platform used in this run of gitpython.\n\t:return: enum.OS.\"\"\"\n\ttry:\n\t\treturn platform.platform()[0]\n\texcept IndexError:\n"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\tpass\n\ndef gitshell_cmd(cmdstr):\n\n\ttry:\n\t\timport winreg\n\t\tprint(\"\"\"\n# Windows specific command line parser\n[HKLM, %s]\n    %s\n    %s\n\n[HKCR, %s]\n\t\t\"\"\" % tuple(winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, \"Software\\\\Python\\\\pywin32.exe\")))\n\n\t\tif winreg.WinExec(cmdstr):\n\t\t\treturn 1\n\n\texcept BaseException:  # no windows"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\tglobal DEBUG\n\tif DEBUG: outmess('analyzepos.py:checkwindows:1')\n\treturn True\n\n\n\ndef setup():\n\tglobal DEBUG,  ose\n\tDEBUG = 1; ose = 0;\n\t\"\"\"Multiline Comment1\"\"\"\n\t#MULTILINEMULTILINEMULTILINE 1\n\t#MULTILINEMULTILINEMULTILINE 1\n\t#MULTILINEMULTILINEMULTILINE 1\n\t#MULTILINEMULTILINEMULTILINE 1\n\t#MULTILINEMULTILINEMULTILINE 1\n\t#MULTILINEMULTILINEMULTILINE 1\n\n\t#MULTILINEMULTILINEMULTILINE 1\n\t#MULTILINEMULTILINEMULTILINE 1\n\t#MULTILINEMULTILINEMULTILINE 1\n\t#MULTILINEMULTILINE"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\tos.chmod(config_dir, stat.S_IREAD)\n\tos.chmod(script_dir, stat.S_IREAD)\n\n\treturn True\n\n\n\ndef check_version (args):\n\tversion_info = subprocess.check_output(\n\t\targs=['gitversion', '--version'],\n\t\tstderr=subprocess.STDOUT)\n\n\tassert(version_info.decode() == GITVERSION)\n\t\n\treturn True\n\npackage com.atakmap.spatial.file;\n\npublic interface SpatialFileSource {\n    Spatial"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\treturn True if os.getenv(\"GIT_SSH_BASH\", None) else False\n\n\tdef is_git(self): return self.is_git_branch\n\t\t\"checks if the version is in git format\"\n\t\treturn os.getenv('GIT_SSH_VERSION', '').strip() in ('', '.b')\n\n\tdef get_git_branch_from_dir(self):\n\t\t\"\"\"\n\t\tget the latest git branch in a directory\n\t\t\"\"\"\n\t\trepo, commit, branch = self.get_repo_and_commit"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\timport sys\n\tif os.name =='nt':\n\t\tprint('ERROR: No file system to run python on. You must run p.py in a windows gitbash or python (ie. python) if you are targeting the standards directory')\n\t\tsys.exit(1)\n\t#if py.git.branch == os.environ['GIT_BRANCH']:\n\t\t#print('ERROR: You cannot run gitshell in a windows gitbash')\n\t\t#return(not True)\n\t#print('python: git shell')\n\tif len(sys."}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\tos.name\n\"\"\":return: The name of the operating system.\"\"\"\n\tos.platform\n\"\"\":return: The name of the operating system\"\"\"\n\tos.path\n\"\"\":return: The path if you run in a Windows gitbash\n\n:return: The path of the gitdir repository if you run in a Windows gitbash\"\"\"\n\tos._path\n\"\"\":return: The path if you run as a gitbash\"\"\"\n\tos.pathsep\n\"\"\":return: The path separator if you run as a gitbash\"\"\"\n\tos.sep\n\"\"\":return: The string separ"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\tif run_git_command(cmd, cwd=cwd):\n\t\t\treturn True\n\n\treturn False\n\n# Run git commands in a shell and return the error string\n#\n# :raises: Exception if the command failed with the output from\n#          the shell\n#\n# This function will spawn the git command passed as command\n# (like 'git -S git [command]``) and wait for it to finish\n# returning its output (like 'git log').\n# So, if you need to know the output of your git commands,\n#"}]}
{"task_id": "62ece4992e6aefcf4aabbd7b", "prompt": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\twith open(oFileOut, 'w') as fout:\n\t\tfout.write(oRenderedConfig)\n\t\tfout.write(oExpectedGeneratedConfig)\n\n\tprint(\"Wrote file: %s\" % oFileOut)\n\t# check to see if there was a file already in the place that just contains the new updated YAML\n\tfor (oDir, cPaths) in list(os.walk(oDirIn)):\n\t\tfor oPath in cPaths:\n\t\t\tcSubDir = oPath.replace(oDir, oDirIn)\n\t\t\t"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tif not os.path.isfile(self.config_filename):\n\t\tprint (\"{} does not exist.\".format(self.config_filename))\n\t\traise e\n\n\ttry:\n\t\twith open(self.config_filename, \"w\") as outf:\n\t\t\toutf.write(yaml.dumps(self.config_yaml, indent=4))\n\texcept Exception as e:\n\t\tif config.check_config_file_exists(self.config_filename):\n\t\t\tpathlib.Path(self.config_filename).mkdir(parents="}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\timport sys\n\timport os\t\t\n\t# We do not want to add python libraries to sys.path to prevent\n\t# user directory lookup order reversal\n\tif os.path.isfile(target_config):\n\t\tprint \"Target config file already exists and is not being overwritten: \" + target_config\n\t\tsys.exit(-1)\n\tif not os.path.isfile(target_config_path):\n\t\ttarget_config_path = os.path.abspath(target_config)\n\t\tprint \"Specified target config: \" + target_config_path"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\twith open(filename, 'w') as infile:\n\t\tinfile.write(rendered_config)\n\n#\tPrint out info that your generated YAML file is in the specified location.\ndef print_yaml(filename):\n\t\t\"\"\"\n\t\tprint the generated YAML content out to the specified file (if it exists)\n\t\t\"\"\"\n\tif os.path.exists(filename):\n\t\ttry:\n\t\t\ttmplist = template.Template(open(filename).read(), file=filename, mode='r', variables = {})\n\t\texcept IOError:\n\t\t\t"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\twith open(os.path.abspath(config_filename), 'w') as f_out, bsys.err as b_out:\n\t\tb_out.write(YAML.dump(config, Dumper=yamlRenderer))\n\t\tb_out.write('\\n')\n\t\tif config_filename!= bsys.paths.baseConfig:\n\t\t\tos.makedirs(os.path.dirname(config_filename))\n\t\tif overwrite:\n\t\t\tb_out.write(config_contents)\n\n\ndef merge_config_from_file("}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\twrite_config(outpath, config_name, rendered_config)\n\tout_dir = os.path.dirname(outpath)\n\tif not os.path.isdir(out_dir):\n\t\tos.makedirs(out_dir)\n\treturn outpath\n\t\t\n\ndef write_config(outpath, config_name, config):\n\t\"\"\"Writes a rendered config to a file. config is the name of a file. in_dir and out_dir are \n\tpassed to os.makedirs and must be pathable\"\"\"\n\t\n\t# Write contents"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tif not os.path.isdir(target_dir):\n\t\tos.makedirs(target_dir)\n\tfor file_name in files:\n\t\tif file_name.endswith(\"-out.yml\"):\n\t\t\tfile_path = os.path.join(target_dir, file_name)\n\t\t\tif not os.path.isfile(file_path):\t\n\t\t\t\twith open(file_name) as fp:\n\t\t\t\t\tprint (\"Saving to %s\\n\" % (file_name))\n\t\t\t\t\tyaml.dump(fp.read(),"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\ttry:\n\t\toutPath = os.path.abspath( str( os.path.join( os.path.abspath( os.path.dirname( __file__ )), os.path.pardir ) ) + os.sep )\n\t\tif overwrite :\n\t\t\tprint \"Would overwrite: \" + outputPath\n\t\t\tos.rename( outputPath, outPath )\n\t\telse : \n\t\t\tprint \"File exists already\"\n\t\t\treturn\n\t\tprint \"Making out folder.\"\n#\t\t\t\tos.makedirs('outputFolder', exist_ok="}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\trootPath = os.path.splitext(os.path.basename(configPath))[0]\n\trootPath = os.path.join(rootPath, 'config')\n\tif not os.path.exists(rootPath):\n\t\tos.makedirs(rootPath)\n\tfor filePath in os.listdir(rootPath):\n\t\tif fileType == 'yml':\n\t\t\tconfigYaml = open(os.path.join(rootPath, filePath))\n\t\t\tcurrentJson = json.loads(configYaml.read())\n\t\t\tfileConfig ="}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tfrom cStringIO import StringIO\n\tfrom os.path import dirname, abspath, join\n\timport sys\n\tif 'CMAKE_BUILD_TYPE' in os.environ:\n\t\tif 'MSVCRT' in platform.platform() or 'MINGW' in platform.platform():\n\t\t\ttarget_dir_path = abspath(dirname(dirname(dirname(__file__))))\n\t\t\ttarget_filename = join(target_dir_path, 'Build.cmake')\n\t\t\tif not os.path.exists(target_filename):\n\t\t\t\tprint(\"Building build file"}]}
{"task_id": "62ece4992e6aefcf4aabbd7c", "prompt": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tif script is None:\n\t\treturn \"\"\n\treturn textwrap.fill(script)\n\n\nclass SSHException(Exception):\n\t\"\"\"\n\tException raised by a remote command when SSH exits with an error.\n\tThis exception is only thrown during a SSH attempt.\n\tSSH exceptions are \"fatal\" (or \"ignore\" instead of \"continue\") to a remote\n\tcommand such as \"sudo.\".\n\n\tNote that this exception can only occur on a fatal exit from a remote\n\tcommand such as \"sudo.\"\n\n\t:param msg:\n\t:param"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tif not isinstance(script, str):\n\t\treturn script\n\tif not isinstance(script, basestring):\n\t\traise TypeError('%s is not text-string' % script)\n\treturn textwrap.wrap(script, width=80)\n\n\"use strict\";\n\nvar _interopRequireDefault = require(\"@babel/runtime/helpers/interopRequireDefault\");\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = void 0;\n\nvar _react = _interopRequireDefault(require(\"react\"));\n\nvar _createSvgIcon = _interopRequireDefault(require(\"./"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tprint(\"%s\" % repr(script))\n\n\ndef isvalid_output(string):\n\t\"\"\"\n\tChecks if a script is valid or not.\n\t:param string:\n\t:return:\t(True if string is a valid output, False otherwise)\n\t\"\"\"\n\tif isinstance(string, textwrap.Text):\n\t\tfor line in string:\n\t\t\tif not(line.strip()):\n\t\t\t\treturn False\n\telse:\n\t\tif not(string):\n\t\t\treturn False\n\treturn True\n\n\ndef print_help():\n\n\t"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tif not isinstance(script, basestring):\n\t\traise Exception(\"Invalid script type\")\n\n\treturn \" \".join(script.splitlines())[:-1]\n\n\tdef split_text(text):\n\t\t\"\"\"\n\t\tRemoves comments and replaces whitespace with empty spaces\n\t\t\"\"\"\n\t\tresult = [x.strip() for x in eval(text, g_c) if x!= '' and not x.startswith('#')]\n\t\treturn result\n\n\tdef split_arguments(text):\n\t\t\"\"\"\n\t\tReturns tuples of (1) the arguments that"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tif sys.version[1] == 2:\n\t\t\"\"\"\n\t\tThe first and second param types are supported since 2.6.0.\n\t\tThese command strings need to be converted for older versions of \n\t\tthe script, because the strings used for the second & third\n\t\tparam types were saved in older version of the script.\n\t\t\"\"\"\n\t\tdef _get_new_command_type(script):\n\t\t\tif sys.version_info < (3,):\n\t\t\t\treturn '2.6.0'\n\t\t\telse:\n\t\t\t\treturn"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\treturn textwrap.dedent(script)\n\n# TODO (marky): Implement this function. This probably belongs in the\n# scripting module but I haven't tested it\n_convert_single_line_command = textwrap.dedent(_convert_single_line_commands)\n\n\nclass NoCommandError(Exception):\n\tpass\n\n\ndef add_script_arguments(argv, *args, **kwargs):\n\t\"\"\"Adds the script arguments at the beginning of the command line\n\targuments. All the commands start with \"shell\" or \"script\".\n\tIf the shell is"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\ttry:\n\t\t# First create it\n\t\tcmd = textwrap.dedent(textwrap.wrap(script, width=120))\n\t\tif not cmd.strip(): return []\n\t\t# Now send it\n\t\thost = cmd.split()[0]\n\t\tip=cmd.split()[1]\n\t\t# Now convert the output into one script line.\n\t\treturn [\n\t\t\tshellcmd.format_output(host,ip,shell=True).strip()\n\t\t\tfor shell in(\"bash\",\"zsh\",\"perl\",\"tcsh\",\"su"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\ttry:\n\t\tret = _run_script(script)\n\t\treturn ret\n\texcept RuntimeError as e:\t# pylint:disable=broad-except\n\t\tprint(e)\n\t\t# TODO: show error and clear console\n\t\traise\n\ndef add_arguments(parser):\n\t\"\"\"Add arguments.\n\n\t:param parser:\n\t:return:\t\"\"\"\n\tparser.add_argument(\n\t\t\"host\", \"-l\", help=\"Host\", action=\"store\", default=\"localhost\"\n\t)\n\tparser.add_argument(\n\t"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\treturn \"\\n\".join(convert(line) for line in raw_input(prompt))\n\ndef convert1(script, newline=False, skip_comments=False, **kwargs):\n\t\"\"\"\n\tConverts a script into one line command. Returns a formatted string.\n\n\tIf newline is True and input does not end with newline, then the output\n\tis extended with line breaks. See also convert1(script).\n\n\tIf newline is False, then only one command is returned.\n\n\tIf skip_comments is True, then any command that uses the 'S' flag will"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\treturn textwrap.dedent(script + \"\\n\")\n\ndef strip_prefix(x, prefix=''):\n\t\"\"\"\n\tThis function is a workaround function to handle python3 script\n\tsyntax which would otherwise lead to a SyntaxError on python2\n\tbecause it contains more than one line comments.\n\t:param x:\n\t:param prefix:\n\t:return:\t\"\"\"\n\tstripped_x = x.strip(\"'\\\"`$%^#&()\\t,~$%^#&()\\t,~$%^#&()'\")\n\tx = x.split"}]}
{"task_id": "62ece4992e6aefcf4aabbd7d", "prompt": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\treturn subprocess.call(command, *args, **extra_env)\n\n\ndef is_python_2():\n\t# Is python >= 2\n\t# http://stackoverflow.com/questions/321936/python-python-version-numbers-not-working-for-python-3\n\treturn (python_version.major > 2 or python_version.release >= 1.1)\n\nif __name__ == \"__main__\":\n\t# Start the script.\n\tscript = \"python_script.py\"\n\tsubprocess.call(shlex.split(script))\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\tif not _is_in_module_space(func):\n\t\t\treturn (\"You can only import modules within sub-processes.\\n\" \"Run the script from within the :any:`import` statement of a module.\\n\")\n\n\tdef run(func):\n\t\t\"\"\"Run the Python code inside an exec file.\n\n\t    Parameters\n\t    ----------\n\t    func : Any\n\t        The Python code to execute.  It must be in a module that is importable.\n\t    \"\"\"\n\n\t\toutput_file ="}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tfor k, v in vars.items():\n\t\tif v == None:\n\t\t\tsetattr(subprocess, k, v)\n\t\telif not v in {\"NONE\", \"ALL\"}:\n\t\t\traise RuntimeError(\"Invalid value for env parameter: '%s'.\" % v)\n\tself.run_and_parse(func, args, extra_env=extra_env)\n\n\ndef run_and_parse(func, args, print_stats=False, extra_env=None):\n\t\"\"\"Run a function in a sub-process and parse the return value.\n\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tif not isinstance(func, type(os.execvp)):\n\t\traise TypeError('func must be a function')\n\tif not args:\n\t\targs = sys.argv[1:]\n\t\t\n\t# get environment variables and prepend with os.environ\n\tenv = os.environ\n\tret_dict = {}\n\tenv_var_map = {}\n\tfor arg in args:\n\t\ttry:\n\t\t\targs[arg] = subprocess.check_output(arg, encoding=encoding, env=env).decode(encoding)\n\t\t\tenviron_var = arg.decode"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tif hasattr(func, \"__func__\"):\n\t\t# Allow for import path\n\t\tpfunc = func\n\t\tfunc += (sys._MEIPASS, \"\")\n\telse:\n\t\tpfunc = getattr(func, \"__module__\", None)\n\textra_env = pfunc.__dict__.copy()\n\tfor key, value in extra_env.items():\n\t\tif key == \"PYTHONNOSUBDIR\":\n\t\t\textra_env[key] = \"1\"\n\t\telif key == \"PYTHONHOME\":\n\t\t\textra_env[key] = os"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tresult = None\n\n\t# Determine the file type and call the shell cmd.\n\tif \"__file__\" in func.__code__.co_filename:\n\t\tfilename = str(func.__code__)\n\t\tbasename = filename.split(os.path.sep)[:-1]\n\t\tcmd ='sh -c \"{}\"'.format(basename)\n\t\tcmd1 = cmd\n\n\t\tif os.path.exists(cmd1):\n\t\t\tsubprocess.call(cmd1, args,  shell=True, cwd=module_name)\n\t\t\tif os."}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t# pylint: disable-msg=broad-except\n\n\ttry:\n\t\t\"\"\"\n\t\tRun the given command.\n\n\t\tThis function allows you to have ``subprocess.run`` in your application,\n\t\tuse it to run ``func(*args, **kwargs)``, e.g.::\n\n\t\t    def command_func(self):\n\t\t        print(subprocess.run(func, args=args, **kwargs).returncode)\n\n\t\t    command_func()\n\t\t\"\"\"\n\t\tsubprocess.run(func, args"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tsubprocess.run(\n\t\t[sys.executable, '-c',\n\t\t'shell.escape(\"%s\")'%(extra_env or \"\")],\n\t\tstdin=subprocess.PIPE, check=True)\nif os.name == \"nt\":\t\n\tsubprocess.run([sys.executable, '-c', 'print $0'], check=True)\n\nimport os as sys\nsys.path.insert(0, os.path.abspath(sys.path[0]))\n\nimport os\nimport multiprocessing\nimport multiprocessing.connection as Connection\n\nclass Pro"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\trun: None = None\n\tdef __init__(self, func: Callable[[str], str]):\n\t\tself.func = func\n\t\tself.args = []\n\t\tself.extra_env = {}\n\n\tdef call(self, *args, **kw):\n\t\t\"\"\"Call a function, and save the output if it is a function.\"\"\"\n\t\tcommand = self.func.__name__ +''\n\t\tif type(args[0]) == str:\n\t\t\tcommand +='' + args[0] +''\n\t\telse:\n\t\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tif isinstance(func, type):\n\t\timport os, sys\n\t\tif not hasattr(func, '__call__'):\n\t\t    raise TypeError(f\"Cannot execute `func`, function that takes no 'args'.\")\n\t\ttry:\n\t\t\tif hasattr(func(), '__call__'):\n\t\t\t    try:\n\t\t\t\tfunc(*(args+extra_env[('args', 'extra_env', func.__name__)[3:],]))\n\t\t\t    except:\n\t\t\t\treturncode = func.__call__(*args, **extra_env)\n\t\t\telse:"}]}
{"task_id": "62ece4992e6aefcf4aabbd7e", "prompt": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tdef getValue(self, name, default=None):\n\t\t\"\"\"Get the value of the environemnt given a name. If the variable is\n\t\tnot defined in environment and no default is provided, an Error is raised..\"\"\"\n\t\t# print name + \" = \"  + self.GetVarName(name)\n\t\tvalue = os.environ.get( self.GetVarName( name), default )\n\t\tprint value, self.GetVarName( name ) \n\t\treturn value\n\n\tdef GetVarName( self, name ):\n\t\treturn self"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tif os.environ._has_key(name): # If it is defined only in environment, _has_key works\n\t\tvalue_ = os.environ[name]\n\t\treturn value_ if value_ else default\n\t\t\n\telse: # otherwise it is only defined in a file and not in the environment\n\t\tif name.isupper(): # It does not have a name specified, which should not happen but better safe than sorry\n\t\t\terror_message = \"Cannot find the \" + name + \" variable in the specified file\"\n\t\t\terror_dialog(error_message,"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tdef get_value_as(self, name, default_value=''):\n\t\tif name not in self.values:\n\t\t\traise Error('Could not find the variable \\'{0}\\' in the environment.'.format(name))\n\t\treturn self.values[name] if self.values else default_value\n\t\n\tdef check_variables(self):\n\t\tif not os.access(self.environment_filepath, os.R_OK):\n\t\t\traise Error('Unable to access to the environment file: {0}'.format(self.environment_filepath))\n\t"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tdef get(self,env, name,default):\n\t\tif env[name] is None and default is None:\n\t\t\traise Exception(\"Env variable '%s' is not defined\" %(name))\n\t\treturn env[name]\nif __name__ == '__main__':\n\timport unittest\n\timport sys\n\tsys.exit(unittest.main())\n\n#------------------------------------------------------------------------------\n\n/**\nReleased as open source by Gabriel Caudrelier\n\nDeveloped by Gabriel Caudrelier, gabriel dot caudrelier at gmail dot"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\treturn os.environ.get(var.name, \"default value\")\n\t\ndef check_env(env):\n\t\"\"\"\n\tIf value is not None, check in a default environment value otherwise check in the user directory. Returns either\n\tNone if valid, or a string with errors describing an invalid variable.\n\t\"\"\"\n\treturn check_env(env, env, os.environ.get, [os.environ.setdefault])\n\ndef check_env_path(env):\n\t\"\"\"\n\tIf the value is not None, check in the user directory for a path to a binary"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tdef __init__(self,name,defaultvalue=\"\"):\n\t\tself.name=name;\n\t\tself.defaultvalue=defaultvalue;\n\t\tself.found=2;\n\t\tself.__value;\n\tdef getvalue(self):\n\t\ttry:\n\t\t\tval=self.__value;\n\t\t\tval=str(val);\n\t\t\tif self.found!=0: raiseError(\"Wrong variable '\".$name.\"'\",val);\n\t\t\telse: return val;\n\t\texcept self.error: self.error=\"Could not get value\"; raise\n"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tdefault_value = \"\"\n\tmatcher = os.environ[name].strip().split(\" = \")[1]\n\tif len(matcher) == 1:\n\t\treturn \"DEFAULT_VALUE = \" + str(default_value)\n\ttry:\n\t\treturn int(matcher)\t \n\texcept ValueError:\n\t\traise Error(\"Invalid value for \" + name.strip())\n\treturn \"\"\t\n\ndef add_command(command, option, func_name, parameter=[], defaults={}, description=None):\n\t\"\"\"Add a command that should execute the specified name.\"\"\"\n\tname ="}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tdef get( self,environment,nameval =\"\",defaultValue = \"\",optional = None ):\n\t\tif environment == None or optional == None:\n\t\t\traise ValueError( \"environment and optional cannot be None.\" )\n\t\tif nameval ==\"\" and defaultValue ==\":/\": defaultValue = os.getcwd()\n\t\tvalue=os.getenv(environment,nameval,defaultValue)\n\t\tif value == \"\" and optional!= None:\n\t\t\traise ValueError( \"could not find variable \"+str(nameval)+\" \" + optional + \" in environment \" + str(environment) )\n\t\tif"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\ttry:\n\t\tm = re.search('(.)([A-z0-9_]+)', environ[\"USER\"])\n\texcept:\n\t\t#error = \"User not found\"\n\t\traise\n\telse:\n\t\tif int(m.group(2)) == 0:\n\t\t\treturn False\n\treturn m.group(1)\n\ndef _match_regex(file_pattern, match_regex):\n\t\"\"\"\n\tFind the pattern in the current file given a string.\nIf no pattern is provided, it returns the string\n\t\"\"\"\n\treturn"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tdef get(self) : \n\t\ttry : os.environ.get(self._name) [self._defaultValue] == self._expectedValue : return True \n\t\texcept(AttributeError, AttributeError): \n\t\t\traise Error(\"Expected value not defined:  \" + self._name)\n\t\t\treturn False\t\n\t\texcept(SystemError, FileNotFoundError, SystemExit, IndexError, IOError, KeyError): \n\t\t\traise Error(\"Could not match expected value.\")\n\t\t\treturn False\t\t\t\t\n\n\tdef __get__(self, func, args, d) : \n\t\ttry: result"}]}
{"task_id": "62ece4992e6aefcf4aabbd7f", "prompt": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t", "suggestions": [{"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\tresult_list = []\n\tif not re.search('^(https?:)?//.*(phantomjs.com\\d*)$',  image_href):\n\t\traise InvalidURL\n\tnetloc = unquote_plus(\"<a href=\\\"%s\\\">NetLoc</a> \" %\n\t\t\tre.search(r\"`(.*?)(\\s*<\\/a>)/(.*)\\W*<br.*?>\", image_href).group(1))\n\tparts = urllib.parse.urlparse(image_href).netloc.split()\n\tfor i in range(len("}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\tparts = urllib.parse.urlparse(image_href)\n\n\t# Extract the id, url and use_ssl information from the href\n\timage_id = parts.netloc\n\timage_url = parts.path\n\tuse_ssl = parts.scheme.lower() in ('', 'https', 'tcp')\n\n\t# If no id is set and uses ssl assume that a local image of the same\n\t# protocol as the server is being requested.\n\tif image_id is None and use_ssl:\n\t\timage_id = parts.netloc\n\tif"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\timport urllib2\n\n\ttry:\n\t\t# Check for image_href\n\t\tif len(image_href):\n\t\t\t# Fetch image src from the server\n\t\t\tresponse = urllib2.urlopen(image_href)\n\t\t\tdata = response.read()\n\n\t\t\t# parse the id\n\t\t\tparts = urllib.parse.urlsplit(data)\n\t\t\timg_id = parts.path.split('/')[-1].split('.')[0]\n\t\t\t# parse the uri and use ssl\n\t\t\tparts = urllib.parse.urlsplit(data)"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\timage_href = urllib.parse.unquote(image_href)\n\traise ValueError, \"No image found in the uri: %r\" % image_href\n\tif image_href[0]!= \"/\":\n\t\traise ValueError, \"No image found in the uri: %r\" % image_href\n\treturn (None, None, False)\n\n\n#\n# urllib.urlencode\n#\ndef urlencode(d):\n\treturn urllib.parse.urlencode(d)\n\n#\n# urllib.parse.urlencode\n#\ndef urlencode(args):\n\t"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\treturn_url = urllib.parse.urlparse(image_href)\n\thost_name = urlparse.urlparse(return_url.hostname).netloc\n\tif host_name:\n\t\tnetloc0=host_name.split(\":\")[0]\n\telse:\n\t\tnetloc0 = return_url.hostname\n\tnetloc1=net2 + netloc0\n\n\t# check path is well formed (only on the right hand side, to ensure it can't end by \"/\")\n\t# on most hostnames this doesn't work either :)\n\tif"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\tif not utils.is_safe_uri(image_href):\n\t\traise ValueError('The image uri %s is invalid.'%image_href)\n\n\tparts = urllib.parse.urlsplit(image_href.lower())\n\n\treturn (parts.path_segments[0],\n\t\tparts.netloc,\n\t\tparts.query.get('use_ssl'))\n\ndef _is_same_server(uri, image_uri):\n\t\"\"\"\n\tCheck if the two URIs refer to the same server.\n\n\t:param uri: the uri of"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\treturn urllib.parse.urlparse(image_href)\n\n#\n# http://developer.xiph.org/xiph/api/v1/imgsrc/\n# xiph.org/xiph/api/base64/\n# xiph.org/xiph.py/\n#\n# xIPHDFg Image Source\n# http://www.xiph.org/pdf/i-xiph/pdf/xiph/examples/xg/examples.pdf\n#\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\treturn _parse_composite_url( image_href)\n\ndef _parse_composite_url(image_href):\n\ttry:\n\t\timage_url_parts = [parse.urlparse(_image_url)[0].split()[4]\n\t\tfor composite_url in get_composite_links(image_url_parts[0], image_href, True)]\n\t\treturn (\n\t\t\tget_composite_id( composite_url ),\n\t\t\textract_use_ssl( composite_url ),\n\t\t\tTrue)\n\n\texcept LookupError:\n\t"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\treturn parse_image_href(image_href, \"image\")\n\n# http://code.activestate.com/recipes/57730-parse-image-href-and-http-querystring-and-url-query-params/\n\n\ndef parse_file_href(href, **kwargs):\n\t\"\"\"Parse an image href into composite parts.\n\n\t:param href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\t\"\"\"\n\tpath = \"\"\n\turl"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\treturn urllib.parse.urlretrieve(image_href, \"https://api.pixiv.net/v2/image.php?id={}&token={}\")\n\t# urllib.parse.urlretrieve(base_url, \"https://api.pixiv.net/v2/image.php?id={}&token={token}\").format(userInfo['id'], token)\n\t# return urllib.parse.urlretrieve(image_href, \"https://api.pixiv.net/v2/image.php?id={}&token={token}&type=f0&app"}]}
{"task_id": "62ece4992e6aefcf4aabbd80", "prompt": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\toutput_list = []\n\tfor input_list in input_list:\n\t\t# if len(input_list) > 1 and input_list[0] == os.sep:\n\t\tif len(input_list.strip()) > 1:\n\t\t\tif input_list.strip().endswith(os.sep):\n\t\t\t\t# if last character is os seperator:\n\t\t\t\tif input_list[0] == os.sep:\n\t\t\t\t\t# if pathname seperator is not in the list\n\t\t\t\t\tif not (input_list.strip().endswith("}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tprint \"input_list:\", input_list\n\ttry:\n\t\tnum_test = input_list[0].count(os.sep)\n\texcept ValueError:\n\t\traise TypeError(\"input_list[0] is not a os.path list\")\n\ttry:\n\t\tfor input_element in input_list[1:]:\n\t\t\tif input_element!= os.sep:\n\t\t\t\tnum_test -= 1\n\t\telse:\n\t\t\traise\n\n\tif num_test!= len(input_list) and os.sep!= os.altsep"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\toutput_list = list(map(len, input_list))\n\toutput_list[:2] = input_list\n\toutput_list = list(filter(os.isabs, output_list))\n\treturn output_list\n\n\ndef get_path_from_directory(directory):\n\t\"\"\"Returns a list of paths to a directory\n\n\tArgs:\n\t\tdirectory: string\n\t\tdirectory is a directory.\n\n\tReturns:\n\t\tlist: list of paths (can be empty)\n\n\tRaises:\n\t\tTypeError\n\t\"\"\"\n\n\t"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tfrom.base_classes import (\n\t\tinput_list,\t\t# list of objects\n\t\tnot_implemented)\t# is this function used from the code?\n\t\t\n\tfor str in input_list:\n\t\tif '/' in str:\n\t\t\tparts = str.split('/')\n\t\t\ttry:\n\t\t\t\tpath = parts[-1]\n\t\t\texcept IndexError:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tif len(path) < 1:\t # can't happen due to the path seperator\n\t\t\t\t\tpath = '/'\n\t\t\t\tinput_"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\treturn _list_remove_trailing_os(input_list)\n\n\ndef ljust(word: str, width: int) -> str:\n\t\"\"\"Just the 'width' characters, or the remaining number of\ntheword if the word isn't long.\n\nArgs:\n    word: string to be string with the 'width' characters, or just 'width'\n          characters.\n    width: width, in number of 'width' characters.\n\nReturns:\n    'Just 'width' characters, or at"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t# Getting first and last characters of each string.\n\tfor each_string in input_list:\n\t\ttry:\n\t\t\t_end = len(each_string)\n\t\texcept:\n\t\t\t_end = len(each_string)+1\n\t\toutput_list.append(each_string[:_end])\n\treturn output_list\t\n\ndef GetFile(absolute, directory, file_name):\n\ttry:\n\t\t_os = os.listdir(absolute)\n\texcept:\n\t\treturn absolute\n\treturn os.path.join("}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tresult = []\n\tfor input_str in input_list:\n\t\t_trim = False\n\t\tfor c in input_str:\n\t\t\tif c == os.path.pathsep:\n\t\t\t\t_trim = True\n\t\tif _trim:\n\t\t\tif len(result) > 0:\n\t\t\t\tresult.pop()\n\t\t\t\tcontinue\n\t\telse:\n\t\t\tresult.append(input_str)\n\n\ndef splitbyfilename(string, filename):\n\t\"\"\"\n\tSplit a string by file name.\n\n\tArgs:\n\t\t"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tm = _match.match( input_list, pattern )\n\tif not m:\n\t\t_warnings.warn(\"Invalid argument %r in input_list; \"format(_match.match,\"<list of strings>\"),\n\t\t\t\t\t   RuntimeWarning)\n\t\treturn list(string)\n\n\treturn [ _strip_os( x ) for x in m.groups() ]\n\ndef _strip_os( inputstr ):\n\tlast_pos = len(inputstr) - 1\n\tfor i in range( 0, last_pos ):\n\t\tif inputstr["}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tif not isinstance(input_list, list):\n\t\traise TypeError('input_list must be a list')\n\n\toutput_list = []\n\tfor elem in input_list:\n\t\tif len(elem)!= 1:\n\t\t\traise TypeError('Only one separator was present. You passed: '\n\t\t\t\t\t\t\t%  elem)\n\t\tif elem[0] not in os.sep:\n\t\t\traise TypeError('Not a valid os seperator character: '\n\t\t\t\t\t\t\t%  elem)\n\t\telem_length = len(elem)\n\t\toutput_"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tresult = [];\n\tlength = len(input_list);\n\tlast_char = input_list[-1];\n\tfor i in range(length):\n\t\t# If the last character is the pathname seperator\n\t\tif last_char and last_char.isalnum(): # Python 3\n\t\t\t# Check if the last character is still part of the pathname\n\t\t\tif last_char in input_list:\n\t\t\t\tresult.append(input_list[last_char]);\n\t\t\t# Move current characters one after last character\n\t\t\t# to next character.\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd82", "prompt": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tpattern = pattern.replace('\"\",\"')\n\tpattern = pattern.replace('\"','\\\\.')\n\tpattern = pattern.replace('(','').replace(')','\\\\.')\n\tpattern = pattern.replace(')\",\"\\\\.)')\n\tpattern = pattern.replace('',\"\").replace('(','').replace(')','\\\\.')\n\tpattern = pattern.replace('\"','\\\\).+','').replace(\"','\").replace(\"')\",\"\")\n\t\n\t\"\"\"\n\tThis function return the matched results\n\t\"\"\"\n\treturn re.findall(pattern, str_in)\n\ndef get_filename_in_local_storage"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tret = []\t\n\tn = len(pattern)\n\ti=1\n\twhile(i<n) :\n\t\tret.append(r\"(?:%s)\" % (re.escape(pattern[i])))\n\t\ti += 1\n\t\t\n\treturn \"\".join(ret)\n\nif __name__ == \"__main__\":\n\timport sys\n\tpattern = r\"<a \\1>.*?1\"\n\tprint(re.compile(pattern,re.S).search(sys.argv[1]))\n\n/*\n *  Copyright (c) 2009-2018 jMon"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\ndef test(t, s):\t\t\t\t\t\"\"\" Test if text contains a pattern\"\"\"\n\t\ndef test(t, s, r):\t\t\t\t\"\"\" Return 0 on success \"\"\"\n\tregx = re.compile(pattern = r, flags = 0)\n\tif regx.search(s) is not None: return 0\n\treturn 1\n\t\t\nif __name__ == '__main__':\n\tf = open('text.txt', 'r')\n\ttt = [m for m in f if not m.startswith(\"http\") and m.strip()!='']:\n\t\tprint"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tresult = []\n\tfor n in r:\n\t\tresult.append(m[n])\n\treturn string.regex.compile(result)\t\n\ndef find_command(command,path):\n\tf1 = open(command)\n\tt,r = \"\",'#'\n\tf = input(r)\n\twhile True:\n\t\tf = f + t\n\t\tif f == '':\n\t\t\tt  = \"\"\n\t\t\treturn cmd\n\t\tt = f.rfind('\\\\')\n\t\tif t == -1:\n\t\t\tbreak\n"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tprint \"\"\n\n\tif re.search(s, text)!= None:\n\t\ttext = re.sub(\"^ *{\", '', text)\n\t\ttext = re.sub(\"^ *}\", '', text)\n\t\ttext = re.sub(\"^ * \", '', text)\n\t\ttext = re.sub(\"^ *[[:space:]]* \", '', text)\n\t\ttext = re.sub(\"^ *[:^space:]*: *\", '', text)\n\t\ttext = re.sub(\"[_@]\", \"[\", text)\n\t\ttext = re.sub(\"[_@]\","}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tpatt=re.compile('\\w+')\n\treturn patt\n\ndef is_valid(s):\n\tif is_url(s):\n\t\treturn True\n\tmatches=re.findall(r\"^\\d{3}[\\.\\-]?(\\d{15}[\\.\\-]?)^([01]\\d){15,17}$\",s)\n\tif len(matches)==1:\n\t\treturn (1,0,0,int(s))\n\tm=len(matches)\n\tif m<=4:\n\t\treturn (1,0,"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tif \"m\" in str():\n\t\ttry:\n\t\t\tt.compile(str(\"\"))\t\t\t\t\t\t\t\t# create re object\n\t\t\tret = re.match('{}$'.format(str(len(str()))), str(len(str())))\t# match\n\t\texcept:\n\t\t\tprint(\"Invalid format!\")\n\t\t\tprint(str(t))\n\t\t\tprint(str(t))\n\t\t\tt.group(0)\n\t\treturn ret\nelse:\n\tif re.match('({})$'.format(str(len(str()))), str(len("}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t# This function is taken from Rosario [1]\n\tdef reroot(self,str):\n\t\t# Rerot it\n\t\tstring=\" \".join([x for x in array.SplitString(str,' ') if x not in self.spaces] )\n\t\t#return re.split(string,\"|\",2)\n\t\treturn string\n\nif __name__ == '__main__':\n\tstr = \"aaaaddbaaaaddbaaaaddbaaaaddbaaaaddbaaaaddbaaaaddb\"\n\tmyregex = re.compile(rero"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\ttry:\n\t\tresult = re.compile(pattern, flags=re.IGNORECASE)\n\t\treturn False\n\texcept:\n\t\treturn True\n\ndef GetFileList (dir, pattern):\n\t\"\"\"\n\tAsk the user, then return the filenames of the files and subdirectories\n\t\"\"\"\n\tdir = os.path.expanduser(dir)\n\tcmd = \"\"\n\tif not re.search('/bin/ls', dir):\n\t\treturn []\n\n\ttry:\n\t\tcmd = '%s %s -regex \".*\" -o %s'"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\toMatch = re.match(r\"[0-9]{1,3}([.][a-z]{0,3})?[ \\t]*!\", rData)\n\tif oMatch:\n\t\treturn oMatch.group(0)\n\n\treturn None\n\n// Licensed to the.NET Foundation under one or more agreements.\n// The.NET Foundation licenses this file to you under the MIT license.\n\nusing System;\nusing System.Collections.Generic;\nusing System.Globalization;\nusing System.Linq;\nusing System.Text;\nusing Microsoft.AspNetCore.Server."}]}
{"task_id": "62ece4992e6aefcf4aabbd83", "prompt": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tif isinstance(command,str):\n\t\tcommand = \" \".join(command.split())\n\t\n\t# if the \"sh\" variable is present then the user wants to run sh as the system shell.\n\tif SHELL_SH is None:\n\t\t# check for the -C sh variabile for the user to use.\n\t\tif command in sys.argv:\n\t\t\tcommand = \" \".join(sys.argv[command.split(os.path.sep)[: - len(os.path.sep)]])\n\t\t\tshVar = \"\"\n\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\timport argparse\n\tfrom multiprocessing import Process, Queue\n\tfrom pexpect import prompt, print\n\n\tdef cmd_process(cmd):\n\t\tif isinstance(cmd, str):\n\t\t\ttry:\n\t\t\t\tcmd = [cmd]\n\t\t\texcept AttributeError:\n\t\t\t\tpass\n\t\t\tcmd = list(filter(None, cmd))\n\t\telse:\n\t\t\tcmd[:] = [cmd]\n\t\tfor line in cmd:\n\t\t\tProcess(line).start()\n\tdef cmd_queue(cmd):\n\t\tif isinstance(cmd, str):"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tsubprocess.Popen([sys.executable, arg0]+\" \"+arg1)\n\n\nif __name__ == \"__main__\":\n\tparser = OptionParser()\n\n\tparser.add_option(\"-v\", \"--version\",\n\t\taction=\"store_true\",\n\t\tdefault=False,\n\t\tdest=\"version\",\n\t\thelp=\"Show version and exit\")\n\n\t(options, args) = parser.parse_args()\n\n\t# Make sure that the version of the script is set.  If a script\n\t# (or program) has no version attribute, it"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\"\"\"I've replaced:\n\t\tsubprocess.call('ls -l /mnt/e2fsprogs/sfs/ftp_ftp_server/ftp_ftp_server_ssl_support_file /mnt/e2fsprogs/sfs/ftp_ftp_server/ftp_ftp_server_ssl_key/ftp_ftp_server_ssl_key', shell=True)\n\"\"\"\n\nimport socket\nsocket.setdefaulttimeout(-1)\n\nfrom.constants import ROUTER_SOCKET_PORT, ROUTER_PORT_FILE, SFS_FTP_PORT\nfrom"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tparser = optparse.OptionParser(usage, version=\"X11 Python Command Line\" + __version__)\n\tparser.add_option(\"--help\",            action=\"help\")\n\t#parser.add_option(\"--version\",         action=\"version\")\n\t#parser.add_option(\"--no-wait\",         action=\"store_true\", default=False) # Do a non-block\n\n\t# parser.add_option(\"-c\", \"--console\",      "}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tresult = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True, universal_newlines=True).wait()\n\tif result is None:\n\t\traise subprocess.CalledProcessError(cmd=cmd, err='Process Error: %s' % os.strerror(subprocess.wait()[1]))\n\treturn result\n\n############################################################################################\n###                                                "}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tfrom builtins import str\n\treturn subprocess.call(cmd)\n\ndef runv(command):\n\t\"\"\"Runs shell command in a venv\n\t\"\"\"\n\tcmd = shlex.split(command)\n\tcmd.append(r'--python')\n\treturn subprocess.run(cmd,shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n\nfrom pycparser import c_file\nfrom plyer import plyer_main, parse, parse_file, \\\n    parse_line\nfrom plyer.tests import common "}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tcall\t('bash', '/bin/bash', \"-c\", code)\n\n####################################################################################################\n# Functions for getting the paths to packages.\n# All functions are optional.\n\n####################################################################################################\n# Helper class to find the package directory and load it.  It also provides\n# methods for making various calls to\n#       file_exists\n#       load\n\"\"\"\nfrom __future__ import print_function\nimport os\nimport os\n\nimport sys\nimport stat\nimport shutil\nimport stat\nimport string\nimport json\nimport collections\n\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tsubprocess.call( \n\"\"\"print 'Command \"%s\"'\nwhere the letters are the command, and with 2 space between each other,\ne.g., \n\"ls /mnt/volume1 /mnt/volume2\"\nthen \n'ls /mnt/volume3 /mnt/volume4'\t\"\"\"%(command_str) )\n\tdef execCommand(command_str):\t\t\treturn subprocess.Popen(command_str, shell=False ).wait()\n\tdef callFile(name,file_str):\t\t\treturn open(name, 'w').write(file"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\timport os\n\tfrom. import system_exec\n\ttry:\n\t\trc = system_exec.run(\"echo %s\" %''.join(args))\n\texcept SystemExecError:\n\t\tprint \"Can't communicate via subprocess!\"\n\t\tsys.exit()\n\n\tcmd = rc.strip(\"\\n\") \t# strip command out of output\n\t# convert to python string\n\tcmd = cmd.replace(\"/bin/sh\",\"\")\n\tif args[0].startswith(\"-n\"): # add new file\n\t\ttry:\n\t\t\tos.mknod('stdout"}]}
{"task_id": "62ece4992e6aefcf4aabbd84", "prompt": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t", "suggestions": [{"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tip = get_if_addrs(interface=True)[0]\n\n    \tip_str = '%s.%s.%s' % (ip.get_network_address(), \\\n    \t\tip.get_netmask(), ip.get_hostmask())\n\n\tip_str.match(/^[0-9a-f]{1,2}$/)\n\tprint(ip_str)\n\tif ip_str.count(':')!= 3:\n        \tprint('Error: IPv6 address in format :')\n"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tdef test_1(self):\n\t\tself.assertTrue(not ipaddress.IPAddress)\n\n\tdef test_10(self):\n\t\tself.assertTrue(not ipaddress.IPv6Address)\n\nif __name__ == '__main__':\n\tunittest.main()\n\n<?php\n\nnamespace Akeneo\\Pim\\Enrichment\\Component\\Product\\Updater\\Converter;\n\nuse Symfony\\Component\\EventDispatcher\\GenericEvent;\n\n/**\n * Data converter event\n *\n * @author    Marie Bochu <marie.boch"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\treturn (input(f\"{x} is a IPv4 address or not\") == \"True\")\n\ndef ipaddressIPv6(x: int):\n    \"\"\"check if IPv6 address\n    \"\"\"\n    v = x\n    v = v if v >= 16384 else v * 256\n    if isinstance(ipaddress.IPv6Interface.ipv6_address_of(v),\n                    ipaddress.IPv6Interface.IPv6Address):\n    "}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tipaddress.IPv4AddressTests(unittest.TestCase, treadmill.TreadmillTestIP4Address)(unittest.TestCase)\n\tipaddress.IPv4AddressTests(unittest.TestCase, treadmill.TreadmillTestIP4Address, 1 if True else 0)(unittest.TestCase)\n\n/* PR tree-optimization/19476 */\n\nint a, b, c, d;\n\nvoid\nfoo (void)\n{\n  a = b & ~c & d;\n  if (a > 10)\n    #define A ((a << 1 >> 2"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tip.check_ip(\"192.168.0.1\")\n\n\t# Test non-IPv4\n\t\"\"\"\n\tTest if non IPv4\n    \t\"\"\"\n\tip.check_ip(\"10.0.0\")\n\n\n\"\"\"\n==========================\nCheck the IP Address for IPv6 vs IPv4 Address\n==========================\n\tip.check_address(ipaddress.IPv4Address('1.1.1.1'))\"\"\"\n\t# Test IPv6\n\t#\"\"\"\n\tip.check_address('123.456.789a')\"\"\"\n\t"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tif args.input == \"ip\": check(ipaddress.IPv4Address)\n\t\n\t\"\"\"\n\tTest if string\n\t\"\"\"\n\tif isinstance(args.input, basestring):\n\t\tcheck(ipaddress.IPv4Address(args.input))\n\t\n\t\"\"\"\n\tTest if int\n\t\"\"\"\n\tip = ipaddress.IPv4Address(args.input)\n\tif ip.is_ipv4:\n\t\tcheck([\"1.2.3.4\", \"1111:0000:0000:0000:0000:2222:3333:4444\", \"4444::2222"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tipaddress( ipaddr ): \treturn 1\n\tipaddr()\t\t\t: \treturn 1\n\tipaddr( ipaddress ): \treturn 1\n\tipaddr( ipaddress, mask ): \treturn 1\n\ndef ipaddress6( ipaddr ): \treturn 1\n\ndef ipaddress6( interface, flag ):\treturn 0\n\ndef ipaddress6( interface, flag, mask ): \treturn 1\n\n\ndef ipv6_address( indata ): \treturn str(indata)\n\ndef ipv6_address( indata, mask ): \treturn str("}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tipV6\t= raw_input('IP address or not: ')\n\tif ipV6 == '':\n\t\tprint '[!] Please input ipv6 address!'\n\t\tsys.exit()\n\n\t# check for both\n\tipV4 = ipAddress(ipV6)\n\tipV6 = int(ipV6)\n\n\t# IP address\n\tif ipV6 > 128:\n\t\tprint '[!] Address out of range'\n\t\tsys.exit()\n\telse:\n\t\tif __name__ == \"__main__\":\n\t\t\t"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tip_or_netmask = True if isinstance(ip,str) else bool(ip)\n    \n    ip = ip_str\n    ip_str = str(ip) if isinstance(ip,str) else ip\n\n    ip = ip.split('.')\n    ip_str = ip[0] if isinstance(ip[0],str) else ip[0]\n    \n    ip = ip.split(':')\n    \n    v4 = ip[0] if v4_str else ip"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\treturn ipaddress.ip_address(IP_ADDRESS).version == 4\n\ndef is_valid_ipv6(IP_ADDRESS):\n\t\"\"\"\n\tTest if IPv6 address or not\n    \"\"\"\n\treturn IP_ADDRESS in ipaddress.ip_network(\n\t\tIP_ADDRESS\n\t).valid_net()\n\ndef format(n): \n\t\"\"\"\n\tFormat int of address \n\t\"\"\"\n\treturn intformat(IP_ADDRESS, (16, 3) + (1<<(intlog2(n)+3)) )\n\ndef isv4(IP"}]}
{"task_id": "62ece4992e6aefcf4aabbd85", "prompt": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t", "suggestions": [{"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\tfrom rdflib.model import Graph\n\tfrom rdflib.exceptions import ConjunctError, ResourceError, DataTypeError, SOSMessage\n\tfrom rdflib.structures.broader import AbstractSubClassOf, SubClassOf\n\ttry:\n\t\tfor o in Graph().triples((RDF.Class, RDF.Property, RDF.InverseObjectProperty)):\n\t\t\tif o.inverseType == graph.triples.Resource:\n\t\t\t\tfor c in o._children():\n\t\t\t\t\tg = Graph()\n\t\t\t\t\tc._graph(g)\n\t\t\t\t\tfor"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\t#     [ RDF]   [SKOS]   [RDFS]\n\t\t#      \\       /\n\t\t#       \\     \\\n\t\t#        \\     \\\n\t\t#         / \\     \\\n\n\tfind_roots(graph, RDFS.subClassOf) # returns as list of sets of\n\t#\t   nodes\t\t\t\t-- list of lists of nodes"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\treturn set([])\n\ndef revealInBroaderRDF(graph, skos, path, **kwargs):\n\tfrom rdflib import BroaderRDF\n\tbroader = BroaderRDF(graph, path, **kwargs)\n\tbroader.flatten(path, path + '.')\n\treturn broader\n\n# Bring in some extensions from subclasses.\nfind_exts = revealInBroaderRDF\n\ndef revealInRDF(rdflib, rdflib_base):\n\tif rdflib is None:\n\t\tfrom"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\tret = set()\n\tfor i in rdflib.PropertyList_RDFS_find(graph, rdflib.RDFS.subClassOf, rdflib.NO.subClassOf, subClasses):\n\t\tsubClassIDs = [ x.subClassOf for x in graph.nodesOfClass(i) ]\n\t\tfor _ in subClassIDs:\n\t\t\tret.add(i)\n\treturn ret\n\ndef isBroader(graph, b1, b2):\n\t\"\"\"If a set of nodes contains b1 and b2 (from some"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\tfor rdfs_class in graph.get_all_classes(predicate=[rdflib.RDFS.subClassOf]):\n\t\tfor rdfs_type, t in rdfs_class.get_types():\n\t\t\tif t == rdflib.SKOS.broader:{\n\t\t\t\tbreak\n\t\t\t} else:\n\t\t\t\treturn set(t)\n\traise ValueError(\"Root finding failed for skos rdf class %s\" % rdfs_class)\n\ndef _find_properties_and_classes(subject, sub_classes)\n\tproperties = set()"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\tself.search_path(graph, rdflib.RDFS.subClassOf, \"broader\")\n\nadd_node(rdflib.RDFS.Class, \"broader\", [\"broader\"], rdflib.RDF_TYPE)\nadd_node(rdflib.RDFS.subClassOf, \"broader\", [\"broader\"], rdflib.RDF_TYPE)\nadd_node(rdflib.RDFS.broader, \"broader\", [\"broader\"], rdflib.RDF_TYPE, create_label=\"class\")\n\nfind_nodes(rdflib"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\t)\n\t\treturn {} if len(parentPath) == 0 and not p in broaderPath.values() else parentPath.keys()\n\tpublic = find_roots(graph, rdflib.RDFS.subClassOf)\n\tfor i in range(len(broaderPath) + 1):\n\t\tfor property, path in broaderPath.items():\n\t\t\t# check if the property is already in the path...\n\t\t\tif property not in path:\n\t\t\t\tif path.has_key(property):\n\t\t\t\t\treturn None # Already in"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\tn = rdflib.RDFS.subClassOf\n\tnodes = (n(child, rdflib.skos) for child in nodes)\n\t#nodes = [(n(child), rdflib.broader, child) for child in nodes]\n\tgraph.__class__._set_property(\"hasRoot\", len(nodes) > 0)\n\n#pragma mark -\nget_label(labels, node)\n\n#pragma mark -\n# find_labels(graph, rdflib.RDFS.subClassOf, skos, broader)\n\nFind the"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\tsubclasses = get_subclasses(graph)\n\ttry_to_find_roots(subclasses, classMap, rdflib.RDFS.subClassOf)\n\traise Exception\n\nfind_roots_subclasses(graph)\nwill be the same as the reverse of find_roots:\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all nodes contained within the class (not subclasses)\n\n\"\"\"\n\ndef find_roots(graph, cls_map):\n\t\"\"\"Returns the set of roots for each of a"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\tfrom rdflib import Graph, URIRef, Value\n\tgraph = Graph()\n\tfrom rdflib.Conjunct.Graph import isEquivalentTo\n\turi_graph_a = URIRef(rdflib.URI(r'http://www.iana.org/assignments/graph#subClassOf'), 'Graph')\n\tvalue1_a = Value('http://www.iana.org/assignments/graph#subClassOf', rdflib.URIRef(r'http://xmlns.com/foaf/0.1/AbstractDataset', uri_graph_a))\n\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd86", "prompt": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t", "suggestions": [{"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\"\"\"\n\tIf you pass this to the command-line interface, be sure you only pass the\n\tfollowing fields:\n\t- \tThe 'args' keyword argument\n\t- \tThe 'dest' keyword argument\n\t-\tThe 'name' and 'filename' keywords arguments\n\"\"\"\n\t\ntry:\n\timport argparse, ast, os\nexcept:\n\traise ImportError(__name__)\n\nfrom. import args, utils\n\n\ndef yaml_unopen(f):\n\t\"\"\"\n\tFunction to unparse a file *f* (and return its unopened\n\tfile"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\"\"\"\n\tPyYaml_Load()\t\t\t\t\t\t\"\"\"\n\tPyYaml_Dump()\t\t\t\t\t\t\"\"\"\ndef Load(text, **kwargs):\n\t\"\"\"Load a yaml string or file.\"\"\"\n\tif isinstance(text, str):\n\t\treturn yaml.safe_load(text)\n\telse:\n\t\treturn yaml.load(text, **kwargs)\n\n\ndef Dump(data, **kwargs):\n\t\"\"\"Dump to a py2-unicode or py3-string\t\"\"\"\n\tkwargs.setdefault('cls', Dumper)\n\treturn yaml.dump(data, **kwargs"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tpass\n\ndef yaml_dump_intoFile(filename, default_encoding=\"utf-8\"):\n\t\"\"\"Write the current YAML-formatted data as a file, using open()\n\n\tParameters\n\t----------\n\n\tfilename : str\n\t\t\tPath name (with.yaml extension), or an open file.\n\tdefault_encoding : str\n\t\t\tThe character encoding.  Valid options are \"latin-1\", \"utf-8\",\n\t\t\t\"utf-16\", \"ucs-2\" (or any such string supported by UCS-2).\n\t\tNone (default) means"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\ttostring\t\t = lambda val, default={}:\tyaml.representer.safe_dump(val, default)\n\nexcept ImportError:\n\n\tdef fromstring(s, default={}):\n\t\tif isinstance(s, str):\n\t\t\tf = open(s, 'rU')\n\t\t\to = yaml.load(f, s2h = default, Loader=yaml.FullLoader)\n\t\t\tf.close()\n\t\t\treturn o\n\t\telif isinstance(s, yaml.dumper.AnyRef):\n\t\t\ts = s.get_ref()\n"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tyaml.safe_dump(s2c, fp, Dumper=yaml.SafeDumper)\n\n\t# The YAML docs say that dump doesn't have a `dump_interspersed` method:\n\t\t # The only real difference between such a method and the one documented above is that\n\t\t # dumper.default_flow_style === DumperStyle.MULTI_LINE,\n\t\t # where \"multi-line\" is a boolean value\n\t\t # In that case, we have to re-configure the yaml library:\n\t\t # `dump_interspersed` also takes a `yaml"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tFrom pypy3.module.modules.setuptools.command.build_py:\n\t\"\n\"\"\"\n__pypy__ = pypy3.__main__.__dict__\n\nimport sys\npkg = sys.modules['pypy3.pymodule.extras'].__file__\n\nprint(yaml.dump(pkg.get_build_info('py_details'), Dumper=yaml.FullDumper))\n\n'use strict';\n\nconst { assert } = require ('chai');\nconst sinon = require (\"sinon\");\nconst async = require (\"async\");\n"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tdef ToYaml(self):\n\t\t# If this is an int, just dump it all\n\t\tif self is  int:\n\t\t\treturn str(int(1))\n\t\n\t\tself_dict = self.__dict__\t# __dict__ is a global variable\n\t\n\t\tfor key, val in self_dict.items():\n\t\t\tself_val_str = str(val)\n\t\t\n\t\t\tif self_val_str in (yaml.dumper.representer.represent_key, yaml.dumper.safe_representer.represent_key ):\n\t\t\t\t\n"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tfrom ply import dumper as L\n\n\tdata = yaml.dump(L(value), Dumper=L.Dumper.BaseDumper, default_flow_style=False, encoding='UTF-8',\n\t\tallow_unicode=True, width=8, indent=4, new_line='', encoding_errors='strict')\n\tdata = data.decode('utf-8')\n\tdata = data.replace('\u2103','').\\rstrip()\n\treturn data\n\ndef get_yaml_file_list(filename, outputname):\n\tif outputname"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t_Dump = yaml.load_all(s)\ndef dump( data, *fields, **kwds ):\n\t\"\"\"\tDump data and its fields to *output*. This is equivalent to dumping a file in.\n\t\tSee the command-line help for details\n\t\"\"\"\n\tfor x in data:\n\t\ttry:\n\t\t\t_Dump[x].dump(*fields, **kwds)\n\t\texcept KeyError:\n\t\t\tsys.stderr.write(\"dump: Unrecognised key: \" + str(x) + '\\n')\n\t\telse:\n\t\t"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tDump to a list or set in a dict-like\t\t\t\"\"\"\n\t\tself.list()\t\t\t: self.list()\n\t\tself.set()\t\t\t: self.set()\n\t\tself.yaml()\t\t\t: yaml.dump()\n\t\tself.tuple()\t\t\t: [(1, 2), (2, 3)]\n\t\tself.list('hi')\t\t: list('hi')\n\t\tself.dict('hi')\t\t: dict(hi='hi')\n\t\tself.default_dict()\t\t: defaultdict(dict)\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd87", "prompt": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tlog = logging.getLogger(__name__)\t\n\t\t\t\t\"\"\"\n\t\t\t\tDefault logger. Useful mainly for debugging\t\"\"\"\n\tlog.setLevel(4)\t# default debug level is error\n\tlog.addLoggee(sys.modules,None,None,None,None)\n\tlog.setLevel(os.environ.get('LOGLEVEL','DEBUG'))\n\t\t\t\"\"\"\n\t\ndef get_settings_path(filepath):\n\t\"\"\"\n\tFind a valid folder path location.\t\n\t\"\"\"\n\t\n\tfolder_path_list = []\n\tfolder_path = os.getenv"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tlogging.basicConfig(format='%(process)s - %(levelname)s - %(message)s', colorize=False, level=logging.INFO)\n\tlogging.warning('\\n\\n\\n\\n')\n\t\t\t\t\timport traceback\n\t\t\t\t\ttraceback.print_exc()\n\n\tif len(imgcls) == 0 :\n\t\t   img = getimg(imgpath)\n\t\t   if(img == None):\n\t\t\t   return False\n\n\t\t   tmp = img.copy() # for debugging\n\t\t   tmp.save(img_path)\n\t\t   \n"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\nclass logparser(object):\n\t\n\t\"\"\"\n\tRepresents a logmessage as an interface to allow external \t\"\"\"\n\t####################################################################################\n\t\n\t# This allows for the possibility that a logmessage can be created from\n\t# just the class itself and not from string and bytes. This provides a bit\n\t# more error checking and logging that a logparser must use.\n\tclass LabeledMessage(object):\n\t\t\"\"\"\n\t\tLogMessage object that holds a logmessage and some labels\n\t\t\"\"\"\n\t\tdef __init__(self, label, message, level="}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\timport sys\n\tfrom datetime import datetime\n\t#logging.getLogger(\"pylab\").setLevel(logging.ERROR)\n\tif not os.environ.get('TESTING_INFECTED', False):\n\t\tfrom logging import WARN\n\t\tfrom multiprocessing import freeze_support, current_process\n\t\tfreeze_support()\n\t\twith current_process(pid = 'None') as fp:\n\t\t\tlogging.basicConfig(\n\t\t\t\tformat = '%(asctime)s [%(pathname)s:%(lineno)s -4s] %(levelname)"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tlogging.basicConfig(level = logging.INFO, format=\"%(message)s\")\n\tlogging.root.setLevel(logging.INFO)\n\thandler = logging.FileHandler(os.path.abspath(args.logFile))\n\thdlg = logging.FileHandler(os.path.abspath(args.logFile))\n\tformatter = logging.Formatter(\n\t\tfmt=args.logFormat, datefmt='%Y-%m-%d_%H:%M:%S')\n\thdlg.setFormatter(formatter)\n\thdlg.setLevel(logging.INFO)\n"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tfrom\t\tstring import number as _number   # pylint: disable=redefined-builtin\n\timport\tsubprocess\n\tfrom\t\tos         import environ as _environ\n\timport      os\timport\tos\n\timport\t        subprocess\n\timport\t        subprocess\n\timport\t        subprocess\n\timport\t        subprocess\n\tfrom         string import as_bytes\timport\tas_bytes\t\t     "}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tdef __init__( self,name='BaseName' ):\n\t\tself.name=name\n\t\t# create and open debug file\n\t\tif os.path.exists( self.name ):\n\t\t# file exists\n\t\t\tlogging.basicConfig(filename=self.name,level=logging.DEBUG,format='%(levelname)s - %(message)s')\n\t\t\t\n\t\telse:\n\t\t# file does not exist\n\t\t\tlogging.basicConfig(filename=self.name,level=logging.INFO,format='%(levelname)s - %(message)s"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tlog_name = \"default\"\t  \"\"\"Name of application instance\t\"\"\"\n\tlog_level=logging.DEBUG\t\"\"\"LogLevel of application instance\t\"\"\"\n\tlog_level = logging.DEBUG\t\"\"\"Default Log Level of application instance\t\"\"\"\n\n\n\tclass Log(object):\n\t\t\n\t\tdef __init__(self,name=None,level=None,parent=None):\n\t\t\t\"\"\"\n\t\t\tName of application logger instance \n\t\t\t\"\"\"\n\t\t\tpass\n\t\t\t\t\t\n\t\t\t\t\t\n\t\t\tdef log_handler(l, msg, l_parent = None):\t\n\t\t\t\n\t\t\t\t"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tlogger_name = \"fission\"\t\t\t\t\n\tlogger_level = logging.INFO\n\tverbose = False\n#------------------------------------------------------------------------------\nclass Embedding(object):\n\t''' Base class for embeddings, should be included in all models.\n\tThis can be used to add extra pre-processing to the embeddings (see below;\n\te.g. weighting).\n\te.g. embedding = nn.Embedding(n_points=100, n_dims=3)\n\twill give a 10% dense representation of the 50 classes and 30%\n\tsoftmax-ed embeddings. '''"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tlogging.basicConfig( level = logging.INFO, format=\"%(asctime)s - %(message)s\")\t\t\n\n    def write_to_file_without_exception(self, fp, name):\n        \"\"\"\n        Writes the input data to the file with the correct format. If an error occurs, a log will be generated and the filename will then be set to the one that failed.\n        \"\"\"\n\n        t1 = time.clock()\n    "}]}
{"task_id": "62ece4992e6aefcf4aabbd88", "prompt": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\tdef __init__(self,shape,dtype):\n\t\tsuper(dtype,(shape,dtype))\n\t\tself.shape = (shape, dtype)  # shape and (dtype) should not be used simultaneously\n\t\tself.dtype = ndarray.dtype(self.shape,self.dtype)  # a subclass of ndarray\n\n##---------------------\n\n    @property\n    def dtype(self)->dtype:\n        \"\"\"Return the data types of the arrays.\"\"\"\n        return self.dtype\n\n    @"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\treturn np.array(shape, dtype).item(0)\n\ndef get_f_in_array_byindex_(my_list, index):\n\treturn my_list[index]\n\ndef set_f_array_item_byindex_(my_list, value, index):\n\tmy_list[index] = value\n\ndef get_f_array_item_byindex_(my_list, index):\n\treturn my_list[index]\n\ndef create_test_array_1d():\n\tmy_list = []\n\tfor i in range"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\tpass\n\t\n\n#-------------------------------------------------------------------------#\n#                                 I/O                                     #\n#-------------------------------------------------------------------------#\n\nclass ArrayBase(object):\n\t\"\"\" Base class for an array\"\"\"\n\tdef __new__(cls):\n\t\t\"\"\" Builds an empty ArrayBase.\n\t\t\"\"\"\n\t"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t \n\t\nfrom.arraytools import extshape\n\ndef create_array_2D (shape: tuple, dtype: np.dtype) :\n\t\"\"\" Create an array with shape and dtype of given dtype.\n\n \tParameters\n \t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\nReturns\n----------\n\tarray : an array with shape and dtype of given dtype\n\n\"\"\"\n\treturn(create_array(shape, dtype))\n\ndef create_"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\tfor i in range(-range(3),\n\t\t     range(3) + range(2, -1, -2) ):\n\t\tdelshape = int(shape[i][:2])\n\t\tif not -delshape in shape[i]:\n\t\t\tprint(\"warning, wrong dimension for a data-type\", shape[i], delshape)\n\treturn np.array(np.array(np.zeros(shape=shape, dtype=dtype), **initval), **initval).T\n\ndef array2\n\tarray = 1.1\n\tarray."}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\t# return: a new array of the same shape and type\n\t\t#         This is called *unoptimized*\n\t\tdef do_make_array(shape,dtype):\n\t\t\tif dtype is None:\n\t\t\t\treturn np.array(shape,(np.int32,np.int32,np.float32))\n\t\t\t\n\t\t\treturn np.empty(shape,dtype)\n\n\t\tnew_array = do_make_array(shape,dtype=dtype)\n\t\treturn new_array\n\n\t@classmethod\n\tdef create"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\tpublic                : ndarray\n\tpublic\t\t\t: int\n\t\t\t\t\t: data-type of the data\n\t\t\t: None\n\n\t\t\"\"\"\n\n/**\n * \\file dnn/src/x86/matrix_mul/batchnorm/fma_f32_nchw44_avx2.cpp\n * MegEngine's BatchNorm2f32f32.cpp\n *\n * MegEngine is Licensed under the Apache License, Version 2.0 (the \"License\")\n *\n * Copyright (c)"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\ndef create_array(shape, dtype=None):\t\n\t\"\"\"Create a 2D binary array from a given shape\"\"\"\n\t\n\tarray = empty_like(shape, dtype=dtype)\n\tarray.shape = shape\n\tarray.dtype = dtype\n\treturn array\n\ndef delete_array(array:np.ndarray):\t\n\t\"\"\"Move a 2D array at position 0,1 to -1. Note that the order of the elements in the array is reversed and that elements of array is not deleted\n\nParameters\n----------\narray : np.ndarray\n    2D array to"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n@bpy.utils.registered_class\nclass _Array2d(FunctionNode):\n\t\"\"\"Create 2D array in the database\"\"\"\n\t\n\t@staticmethod\n\tdef check_in_list(key):\n\t\tis_key_array = True\n\t\tif key in [np.array([]), ()]:\n\t\t\tis_key_array = False\n\t\treturn is_key_array\n\t\n@bpy.utils.registered_class\nclass _ArrayScalar(FunctionNode):\n\t\"\"\"Create scalar array in the database\"\"\"\n\t\n\tdoc = \"\"\"Create"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\tif len(shape) == 0:\n\t\t\treturn\n\t\tn = np.ndim(x=shape)\n\t\tif n < 2:\n\t\t\traise TypeError\n\t\tif n == 1:\n\t\t\treturn np.array(shape, dtype=dtype)\n\t\treturn np.array(shape, dtype=x.dtype).reshape(shape)\n\n/* ************************************************************************\n * Copyright 2013 Advanced Micro Devices, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the"}]}
{"task_id": "62ece4992e6aefcf4aabbd89", "prompt": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tc2 = Cauchy(sigma=0.1)\n\tc2.to_uniform(0)\n\tN, M = c2._matrix_to_n, c2._matrix_to_m\n\t\"\"\"\n\t\t\tc2 = Cauchy with sigma > 1\t\"\"\"\n\te = Gaussian(mean=0, sigma=1)\n\te.to_uniform(0)\n\t\"\"\"\n\t\t\te = Gaussian with centered Gaussian\t\"\"\"\n\te.to_uniform(0)\n\t\"\"\"\n\t\t\te = Gaussian"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tArgs:\n\t\t\t- dataX: a 1D array of the x coordinates of the desired data. \n\t\t\t- dataY: a 1D array of the y coordinates of the desired data. \n\t\t\t- windowRadius: the radius of the Gaussian window, used to determine the mean and correlation coefficient of correlation coefficient computation, \n\t\t\t- meanType: the type of the means.  Default set to 'linear'. This argument is not presently supported.\n\t\t\"\"\"\n\t\tself.means = None\n\t\tself.sigma"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tx=np.r_[12.0/20., 0.2, 2.0, -1.0, 0.3]\n\tsigma = 0.1\n\t\"\"\"\t\t:math:`x=1.2+np.exp(-1.1(np.array(x))**2)`\t\"\"\"\t\n\treturn x, sigma \n\ndef get_peaks_2d(data, scale):\n\t\n\tsigma_x, sigma_y = scale*get_sigma_value_2d(data, sigma=scale/2., N_real = 200, N_"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\"\"\"@warning: The order in this matrix of Gaussian and Gaussian centered\n\taround the point on a line is NOT the same.  Thus, if you have the\n\tmean = 0.2,  and/or std = 0.1,  you are using Gaussian.matrixToNDC()\n\tand NDC to convert them to the other matrix format.  In this case we\n\twill use the Gauss-Legendre formulation for the Gaussian, which is\n\tnot implemented. See https://en.wikipedia.org/wiki/Gaussian_%28form%29\n\t\"\"\"\n\t"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t#define EPS np.epsilon1\n\t#include \"Gaussian.h\"\n\n\tGaussian( int n, Param p) :\n\t        _gamma( n), _center( n), _sigma(0.1), _mean(0.0), _invstd(0.0), \n\t\t_n(n), _p(p)\t{}\n\n\tvoid setParams( Param& p) {\n\t\tp = _p;\n\t\tsetMeanAndSigma(p[0],p[1],p[2]);\n\t}\n\n"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tfrom matplotlib import pyplot as plt\n\tfrom matplotlib import gridspec as grid\n\timport matplotlib as mpl\n\n\tdef myfunc(x, *args):\n\t\tx += 0.2\n\t\treturn x\n\n\tfor x in [np.array([2, 2]), np.array([1, 2]), np.array([1, 1]),\n\t\t    np.array([1, 0]), np.array([2, 0]), np.array([2, 2]),\n\t\t    np.array([0.2, 0.1])] :\n\n\t\t"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t# Make the graph an empty graph\n\tn = 3\n\tG = nx.Graph()\n\tG.add_nodes_from('n1'+str(n+1)+'1')\n\tG.add_nodes_from('n2'+str(n+1)+'2')\n\tG.add_edges_from('n1'+str(n+1)+'2-n1'+str(1+n)+'1', label='n'+str(1+n))\n\tG.add_edges_from('n2'+str(n+1)+'"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\"\"\"\n\tThis is all one shot!\"\"\"\n\t\"\"\"\n\tYou may also wish to use another method by adding a zero\n\tsigma, which seems to be correct for most (but not in common\n\tcases) models.\t\"\"\"\n\t\"\"\"\n\t1) The sigma value is just the maximum absolute value of sigma\n\t\t(usually the value you enter into the parameter during\n\t\tthe model construction).\n        \"\"\"\n\t\"\"\"\n\t2) You specify how \"sigma-1\" is to the method above,\n\t\twhich would result in a"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\ty = np.random.uniform(-.2,.2, 500)     \n\ty2 = (y+y)/2.\n\t\n\ty2[40:-40, 40:-40] = 1.       \n\ty2[40:-40, 40:40:-40] = 1.       \n\ty2[40:40, 40:40:]   -y2[60:-60, 40:-40] = 1.\n\t\n\ty2[50:50, 20:20:20]   "}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tx = np.exp(-0.1 * np.random.randn(size))\n\ty = np.exp(-0.1 * np.random.randn(size))\n\tc = gaussian_centered(x, y)\n\treturn x,y,c\n\n\ndef gaussian_centered_center(x,y,c):\n\t\"\"\"\n\tcenter gaussian centered around 0, 1, or -2.\n\n\tThe centered kernel is the uncentered kernel with center at\n\tx,y,c (equivalently, the center"}]}
{"task_id": "62ece4992e6aefcf4aabbd8a", "prompt": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tparser = optparse.OptionParser()\n\tparser.add_option(\"--config-name\",dest=\"config_name\",required=True,\"Config file name\")\n\tparser.add_option(\"--log-name\", dest=\"log_name\",metavar=\"log_name\",default=\"LOG\",\n\t\thelp=\"Base name of logging.LogRecord to add to the log file.  Default=LOG\")\n\n\tparser.add_option(\"--config-file\", dest=\"config_file\",required=True,help=\"Configuration file to use\")\n\tparser.add_option(\"--config-level"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tdef __init__(self, configs=[]):\n\t\tself.configs = OrderedDict([(x,None) for x in configs])\n\t\tself.configfiles = []\n\t\tself.parseerrs = []\n\tdef getconfig(self,filename):\n\t\tlogging.error(\"Configuration file %s does not exist\" %(filename))\n\t\treturn None\n\tdef parseconfig(self,filename):\n\t\tlogging.error(\"Parsing %s file failed\" %(filename))\n\t\tfile = open(filename)\n\t\ttry:\n\t\t\tself.configfiles.append"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tres = \"\"\n\tfor f in readAll(filenames):\n\t\tres=res+parse(f,env,logging)\n\treturn res\n\ndef readAll(filename):\n\t\"\"\"\n\tGiven a configuration filename, return a filehandle to an open XML document.\n\n\tArguments:\n\t \tfilename\tname of the configuration file\n\n\tReturns:\n\t\thandle\tthe full filename\n\t\"\"\"\n\tf = open(filename,\"rb\")\n\treturn f\n\n//-----------------------------------------------------------------------\n// <copyright file=\"SqlCeProviderServices.cs\" company=\"Microsoft Corporation\">\n"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tlogging.basicConfig(level=logging.INFO, format='%(asctime)-4s %(levelname)-8s %(name)-12s %(funcName)-18s line %(lineno)4d %(message)s', datefmt='%m/%d/%Y %H:%M:%S')\n\tconfigFiles = read_config_file(config_files)\n\tparser = read_parser(config_files)\n\tlog = logging.getLogger()\n\tparser.parse()\n\tfor filename, configurations, _ in zip(configFiles, parser.all_configs, parser.log_record"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tresult = []\n\tfor config_file, parse_result in _parse_configurations_file(config_file_list):\n\t\tlogging.Info(repr(config_file))\n\t\tresult.append((config_file, _do_parse(config_file, parse_result)))\n\tlogging.shutdown()\n\tlogging.removeHandler(logging.root)\n\tfor item, parsed_item in result:\n\t\tresult[item] = _do_clean_up(parsed_item, result[item])\n\n\treturn result\n\n\ndef _make_parser"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tlog = logging.getLogger(\"pyparsing\")\n\tfor filename in filenames:\n\t\t# Parse the file\n\t\ttry:\n\t\t\tparser = ConfigParser.SafeConfigParser()\n\t\t\tparser.read(filename)\n\t\texcept (IOError, ConfigParser.NoOptionError):\n\t\t\tcontinue\n\t\texcept Exception, e:\n\t\t\tlog.error(\"%s: %s\", filename, e.__str__())\n\t\t\tlog.error(\"Error in parsing file: %s\", e)\n\t\t\tquit()\n\t\telse:\n\t\t\tif parser"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Parse configuration files.')\n\tparser.add_argument('-l','--log', action='store_true', default=False, dest='log_enable', help='Enable logging')\n\tparser.add_argument('-l','--log-level', default=None, choices=['DEBUG','INFO','WARNING'], \n\t\t\t\t\t\thelp='Sets loglevel for parsing logs.  Valid choices ('+str(LOG_LEVELS)+'): DEBUG, INFO, WARNING')\n\tparser.add_argument('-s','--skip', dest='check_for_skip', action"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t# this function returns None if no records have been read\n\tlog_records = []\n\t# load the configuration file(s)\n\tfor fname in filenames:\n\t\tlog_filename = f'{tmpdir}/config_log_{fname}'\n\t\tconf_file = open(log_filename)\n\t\tline = linecache.getline(conf_file, True)\n\t\tconf_file.close()\t\t\n\t\tlog_records.append(parse(line.rstrip()))\n\t\t# check for parse errors\n\t\tfor record in log_records:\n\t\t\t"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tparser_config_filename = input( \"Enter configuration file path: \" )\n\tlogger = logging.getLogger('test')\n\tconfig_files = [config_file_name for config_file_name in os.listdir(parser_config_filename) if config_file_name.lower().endswith('.config')]\n\tif len(config_files) < 1:\n\t\tprint(\"WARNING: Failed to find configuration file(s).\")\n\t\tsys.exit(1)\n\tparsed_configs = {}\n\tfor config_file in config_files:\n\t\tlogging"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\treturn parseConfigFile(cfgFile, validate=True)\n\ndef load_module(file):\n\t# Load user defined module into memory and run it if loaded\n\tif module_dict[file] is None:\n\t\tmodule_dict[file] = ModuleInfo(file).run()\n\treturn module_dict[file]\n\n/**\n * ***************************************************************************\n * Copyright (c) 2010 Qcadoo Limited\n * Project: Qcadoo Framework\n * Version: 1.4\n *\n * This file is part of Qcadoo.\n *\n * Qcadoo is free software;"}]}
{"task_id": "62ece4992e6aefcf4aabbd8b", "prompt": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tresult = []\n\tfor i in xrange(obj.__len__()):\n\t\tresult.append(chr(1) if  i!= 0 else chr(0))\n\t\tresult.append(obj.__getitem__(i))\n\treturn bytes(result)\n\ndef print_bytes_list(obj):\n\t\"\"\"Print the list in the form of a numpy list.\"\"\"\n\treturn list(map(lambda item: repr(item), obj))\n\ndef str_to_id(obj):\n\t\"\"\"Return the id of a string.\"\"\"\n\treturn id(obj)\n\n"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t# if not isinstance(obj, str):\n\t# \traise ValueError('The input must be a number or a string')\n\ttry:\n\n\t\tbytes = _bytes\n\t\tif PY2:\n\t\t\treturn bytes(obj)\n\t\telse:\n\t\t\treturn bytes(obj, dtype=str)\n\n\t# str => str\n\texcept TypeError:\t\t# for Py3\n\t\tif isinstance(obj, str):\n\t\t\treturn obj\n\n\t# bytes => bytes\n\telse:\n\t\tif _isinstance(obj, bytes):\n\t\t"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tif isinstance(obj, str):\n\t\ttry:\n\t\t\tfrom StringIO import StringIO; BytesIO = StringIO.StringIO;\n\t\t\tres = BytesIO(obj.encode('utf-8'))\n\t\texcept:\n\t\t\traise ValueError('invalid StringIO, you must have write a StringIO instance')\n\t\treturn res\n\tobj_cls = type(obj)\n\tres = None\n\tif obj_cls in string.ascii_letters:\n\t\t# we use this function in both cases, but to return bytes and str\n\t\t# we"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\ttry:\n\t\treturn bytes_decode(obj)\n\texcept:\n\t\traise  TypeError(obj) \n\ndef bytes_to_utf8(obj):\n\t\"\"\"\n\tTakes a bytes object (which needs to be\n\tbe encoded in UTF-8) and returns corresponding\n\tbytes in string in unicode characters.\n\t\"\"\"\n\tif isinstance(obj, (basestring,unicode)):\n\t\treturn obj\n\treturn bytes_to_unicode(unicode(obj))\n\ndef bytes_to_unicode(obj):\n\t\"\"\"\n\tTakes a bytes object"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\ttry:\n\t\treturn tostring(obj, encoding=globals().encoding,\n\t\t\t\t\t\t encoding_errors='backslashreplace').decode(globals().encoding,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  'backslashreplace')\n\texcept:\n\t\treturn tostring(obj,\n\t\t\t\t\t\t encoding=globals().encoding,\n\t\t\t\t\t\t encoding_errors='backslashreplace')\n\ndef getformat(format, encoding='utf-8', encoding_errors='replace'):\n\t\"\"\"\n\tConvert the value returned by ``get_format`` to a ``str``.\n\tThe same rules as used by ``get_format`` apply."}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\treturn obj\n\n\n\n\t#-----------------------------------------------------------------------\n#\n# Generic conversion\n# \n#\tThe following functions are needed but they are not \n#\tallowed in the following classes:\n#\t\tstr_to_num()\n#\t\tarr_to_num()\n#\n\n\n\n\n\n\n\n\ndef arr_split(arr, delim, maxsplit=None, nsplit=0, **kwargs):\n\t\"\"\"Split a string into substrings with delimiter.\n\n\tParameters\n\t----------\n\tarr : str\n\t\t\tA Unicode string (possibly with embedded"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tif obj is None: return None\n__doc__ = \"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string assuming that it is in the specified encoding.\n\t\"\"\"\nif __name__ == \"__main__\":\n\timport sys\n\tfrom iscesys.Component.Component import Component\n\tfrom iscesys.Compatibility import StringEncoding, bytes_to_str, str_to_bytes, \\\n\t\t_encoding"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t#from functools import total_ordering\n\t#try:\n\t#\t#if isinstance(obj,int):\n\t#\t#\tret=type(obj)()\n\t#\t#else:\n\t#\tret=type(obj)\n\t#except ValueError:\n\t#\t#if isinstance(obj,bytes):\n\t#\t#\treturn type(obj)(obj,obj.__class_getitem__()[0])\n\t#\tret='str'%obj.__class__.__name__\n\t#else:       #\tret='py_str'%obj"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tif type(obj) == type(''):\n\t\t# assume it is already a string\n\telse:\n\t\ttry:\n\t\t\t# if we're given a file name string which could be a URL, check to see if it would be an encoded version\n\t\t\t# of a URL.  If so, then convert it to utf-8\n\t\t\tif type(obj) == type(u''):\n\t\t\t\t# obj is already a string, so try to convert it to utf-8\n\t\t\t\t# this might fail, if the string is actually in an ascii url form\n\t\t\t"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t#return _util.get_string_var(obj)\n\n\n#@_check_return_\ndef toordinal(obj):\n\t\"\"\"convert a c array to a Python int/long/float/double\"\"\"\n\traise TypeError('astype return type must be a string')\n\n\n#@_check_return_\ndef _check_for_2d_size(arr, dtype=np.signedchar):\n\t\"\"\"convert an array, which may be array of 2D array,\n\tor a matrix, tuple or a vector to 1D data\n\t\"\"\"\n\tif"}]}
{"task_id": "62e60723d76274f8a4026b76", "prompt": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tif not 0 <= ticks:\n\t\traise ValueError(f\"Illegal ticks:{ticks} (should be > 0)\")\n\n\thours=ticks // 3600000000000\n\tif hours < 0:\n\t\traise ValueError(f\"Illegal hours:{hours} (should not be negative)\")\n\n\tsecs=ticks % 3600000000000\n\tsecs = round(secs, 8)\n\treturn cls(0, hours, int(secs))\n\nclass Duration(object):\n\t\"\"\"\n\tA class representing time durations.  Duration object is a Python class,\n\tfor example::\n\n\t\t"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tif sys.maxsize > 2**32:\t\t# big-endian\n\t\treturn datetime.fromtimestamp(ticks * 10000000, tz).time()\n\telse:\n\t\treturn cls(ticks / 1000)\t# ticks -> ms\n\n\n@doc(func=\"time\", klass=False, memoized=True)\ndef time(ticks=None):\n\t\"\"\"\n\ttime(ticks=None, tzinfo=None)\n\n    Return current time in timezone. If timezone is None, return now(). It is preferred\n    to this as time.tzinfo"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tif tz is None:\n\t\ttz = time.tzinfo\n\tdt = datetime.datetime.utcfromtimestamp(ticks).replace(tzinfo=tz).toordinal()\n\treturn cls.from_datetime(dt, tz)\n\nclass Time(DateTime):\n\tdef from_datetime(self, dt, tz=None):\t# pragma: no cover\n\t\ttz = tz or datetime.tzinfo\n\n\t\t# We don't use the zone for UTC; it gets set in the constructor\n\t\tif tz is not None:\n\t\t\traise TypeError(\"Cannot set"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\treturn cls(ticks * 1000)\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass Time(Date):\n\t\"\"\"Represents a time as Unix epoch (int64).\"\"\"\n\t@abc.abstractmethod\n\tdef from_real_date(self, tm):\n\t\t\"\"\"Convert this instance to a real date.\"\"\"\n\t\traise ValueError(\"Cannot convert Time to real date (from_real_date).\")\n\n\t@abc.abstractmethod\n\tdef from_real_date_utc(self, tm):\n\t\t\"\"\"Convert this instance to"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tif (ticks % 60 == 0):\n\t\traise ValueError(\"`ticks` must be divisible by 60 (60 < %d)\\n\"% 86400000000000)%(ticks, )\n\tresult=Seconds()\n\twhile 1:\n\t\tresult=result+Ticks(ticks%60, tz=tz)\n\t\tticks=ticks//60\n\t\tif not ticks: break\n\treturn result\n\ndef from_datetimes(cls, dt, tz=None):\t\"\"\"\n\tCreate a time from datetimes.\n\n:param dt: datetimes\n:type dt: datetime"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t# TODO Check for time zone (tz=None?):\n\tif tz is None:\n\t\traise ValueError(\"tz arg may be unsupported!\")\n\thours, rem = divmod(ticks, 3600000)\n\tif rem > 0:\n\t\thours += 1\n\ttm = time.time()\n\ttm += time.localtime(hours)\n\treturn cls(tm)\n\n\n#---------------------------------------------------------------------------\n\nclass Time(datetime.time):  # default to Unix epoch\n\tdef __eq__(self, other):\n\t\tif not isinstance(other, Time):\n\t\t"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tif not(0 <= ticks < 86400000000000):\n\t\tt = time()\n\t\tticks = int(t.time() // 1000000 + (ticks * 1000))\n\t\tif ticks < 0:\n\t\t\traise ValueError(\"%s is out of bounds.\" % ticks)\n\treturn cls(ticks, tz=tz)\n\n\ndef from_datetime(cls, dt):\n\t\"\"\"Create a Time object from a datetime.\n\n:param dt: a :py:class:`~datetime.datetime`\n:type dt: datetime.datetime\n\n:rtype: Time\n\n:raises"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tsecs = 10 * cls.frac_sec\n\tnanos = floor(secs + cls.frac_nsecs)\n\tsecs += float(secs)\n\tt = cls.from_secs_and_nsecs(secs, nanos)\n\treturn cls(t, tz=tz)\n\n\nclass DurationField(TimeBaseField):\n\t\"\"\"\n\tAn abstract base class providing date and time classes. It can be used\n\ton multiple classes with different subclasses.\n\t\"\"\"\n\n\t__slots__ = [\"precision\", \"week\", \"weekofyear\", \"day\", \"hour"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\treturn cls._from_ticks(ticks, tz=tz)\n\n\treturn self._from_ticks(ticks, tz=_timezone())\n\n\ndef to_ticks(cls, value, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param value: nanoseconds since midnight\n:type value: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if value is out of bounds\n    (0 <= value < 86400000000000)\t\"\"\"\n\t"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tcheck_valid_values(ticks, cls.from_nanoseconds, tz)\n\n\tfrom_seconds_microseconds_(ticks, tz)\n\treturn Time(0)\n\ndef from_ticks_microseconds(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight) since microseconds.\t\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n  "}]}
{"task_id": "62e60873d76274f8a4026bd8", "prompt": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is not None and len(protocol_version) == 2:\n\t\tprotocol_version = (protocol_version,)\n\tprotocol_version = tuple(protocol_version or (0, ))\n\tprotocol_handlers = {\n\t\t0: None,\n\t\t1: p.__dict__,\n\t}\n\tfor v in protocol_version:\n\t\tversion = get_bolt_ver(v)\n\t\tif version not in protocol_handlers:\n\t\t\traise ValueError(\n\t\t\t\t\"unknown protocol version {}\".format(version))\n\t\tif protocol"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\ttry:\n\t\treturn cls._PROTOCOL_HANDLERS[protocol_version]\n\texcept AttributeError:\n\t\treturn dict()\n\n######\n#\n#                                             #\n#                                             #"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version:\n\t\tprotocol_versions = (protocol_version[0], protocol_version[1])\n\telse:\n\t\tprotocol_versions = range(len(_bolt_protocol_versions))\n\tversion_list = _bolt_protocol_versions.copy()\n\tversion_list.sort()\n\tfor version in protocol_versions:\n\t\tfor sub in versions:\n\t\t\tif version == sub:\n\t\t\t\tif protocol_version is not None:\n\t\t\t\t\tversion = min(version, protocol_version[1])\n\t\t\telse:\n\t\t\t\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\tprotocol_format = '3'\t\t# 1.x, 2.0, 2.1\n\telse:\t\t\t\t\t\t\t# 2.x\n\t\tprotocol_format = '2'\n\tprotocol_format = str(protocol_format[protocol_format.find('-')])\n\tversion_range = ProtocolVersionRange\n\tall_handlers = {}\n\t\t# NOTE: we do not want to add all registered handlers\n\t\t# to the bolt version list\n\t\tif protocol_version is not None:\n\t\t\tprotocol_range = version"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_versions = {}\n\telse:\n\t\tprotocol_versions = protocol_version\n\tprotocols = {}\n\tfor (protocol, methods) in cls.protocols.items():\n\t\tfor f in getattr(methods, '__supports__', ()):\n\t\t\tif protocol in protocol_versions:\n\t\t\t\tfor k, v in protocol_versions[protocol].items():\n\t\t\t\t\tprotocols[k] = v\n\tfor (protocol, methods) in cls.protocols.items():\n\t\tfor f in getattr(methods, '__handles__', ("}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tprotocol_version = tuple(protocol_version)  # Ensure argument is iterable\n\tif protocol_version is None:\n\t\treturn cls._handler_dict\n\tif protocol_version is not None:\n\t\tprotocol_version = (protocol_version[0],\n\t\t                    protocol_version[1])\n\t\treturn cls._handler_dict.get(protocol_version, {})\n\telse:\n\t\treturn cls.protocol_version\t\t # Return our current protocol\n\n# TODO:\n# -"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version == None:\n\t\treturn cls.handler_dict()\n\tif not protocol_version:\n\t\t# If no explicit version is provided, all supported\n\t\t# protocol versions are provided.\n\t\tprotocol_version = cls.protocol_major_version, cls.protocol_minor_version\n\treturn {\n\t\tver: cls(ver)\n\t\tfor (ver, handler) in cls.handler_dict().items()\n\t\t\tfor (typ, handler_cls) in handler.type2handler_dict().items()\n\t\t\tif ver in"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is not None:\n\t\tlogger.debug(\"protocol_handlers: protocol_version = '%s'\",\n\t\t             protocol_version)\n\t\tfor version in reversed(protocol_handlers_versions):\n\t\t\tif version[0] == protocol_version:\n\t\t\t\treturn version\n\tif protocol_version is not None:\n\t\tlogger.debug(\"protocol_handlers: \"\n\t\t             \"protocol_versions = '%s'\",\n\t\t         "}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\treturn cls._handlers.get(protocol_version, {})\n\nimport Vue from 'vue'\nimport Component from 'vue-class-component'\nimport { store } from '../../../store/index'\nimport { ipcRenderer } from 'electron'\n\nimport style from './style.scss'\nimport vTheme from '@material-ui/core/styles/createThemeCss'\n\nlet styleId = 0\nclass TabPane extends Component {\n  constructor (opts) {\n    super(opts)\n    this.openKeys = {}\n  }\n\n  _show ("}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t# This is a class, so if it has a new attribute, it is useless\n\tis_protocol_class = _new_class(_ProtocolVersion)\t\t# This is a class\n\tdata = dict(ProtocolVersion=[])\n\n\t# Now build a list of handlers to handle the specified\n\t# protocol version...\n\tfor version,handler_cls in is_protocol_class.subclasses.items():\n\t\tif (protocol_version is not None and\n\t\t    version > protocol_version):\n\t\t\tcontinue\n\n\t\tif handler_cls.supported(version"}]}
{"task_id": "62e60e3bd76274f8a4026d1a", "prompt": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\tif sys.version_info > (3, 0) and isinstance(values, unicode_type):\n\t\tvalues = values.encode('utf-8','replace')\n\n\treturn cls._from_raw_string(*tuple(values.split(';')[:2])[:2])\n\n/*\n * Copyright (c) 2012-2015, 2017, The Linux Foundation. All rights reserved.\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 and\n * only version 2 as published by the"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\tif len(set(values)) > 1:\n\t\traise ValueError(\"The values are one of: %s\" % values)\n\tvalues = (b\"%s\" % b\" \" + re.sub(r\"[\\x00-\\x1f]*[\\\\x80-\\xff]\", \"\", str.strip(values))\n\t\t+ b\";\" + b\" \")\n\tt_obj = cls(*values)\n\treturn t_obj\n\n\ndef from_html(cls, html):\n\t\"\"\"\n\tConvert web pages into bookmarks.\n\n\t:param html: The"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\tassert isinstance(values, Iterable)\n\tvalue_sets = [from_raw_string(value) for value in values]\n\treturn from_raw_sets(cls.DEFAULT_CHARSET, value_sets)\n\ndef from_raw_sets(charset, value_sets):\n\t_value_sets = map(from_raw_string, value_sets)\n\treturn cls(charset, tuple(_value_sets))\n\ndef _remove_first_list_val(value_set):\n\tvalues = value_set.split(',')\n\tfor value in values[:1"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\tif values and not (values[1] and values[2]):\n\t\traise ValueError(\"Malformed bookmarks in bookmarks: expected: <value> <url>, got: %s\" % values)\n\treturn cls(values[1], values[2])\n\n\nclass WebView(AbstractRoot):\n\t\"\"\"\n\tA class that stores information about the browser's page with a WebView (or another). It inherits from :class:`AbstractRoot`.\n\n\t:param page: (str) The URL of the page;\n\t:type page: str\t\"\"\"\n\n\tdef __"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\tfrom bookmarks.bookmarks import Bookmarks\n\treturn cls(values)\n\n\ndef from_raw_value(cls, value):\n\t\"\"\"\n\tCreate a Raw value from a unicode value.\n\t\"\"\"\n\ttry:\n\t\treturn cls(value)\n\texcept UnicodeDecodeError:\n\t\traise UnicodeDecodeError(\"'{}': cannot parse value: '{}'\"\n\t\t\t\t\t\t\t   .format(type(value).__name__, value))\n\n\ndef from_values(cls, values):\n\t\"\"\"\n\tCreate a :py:class:`Bookmarks` object from a"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\tnew_bookmarks = cls()\n\tfor value in values:\n\t\tif value == \"\": continue\n\t\tif \"'\" == value and len(value) > 1:\n\t\t\tnew_bookmarks.add_bookmark(u\"'\", value[1:])\n\t\telif '\"' == value and len(value) > 1:\n\t\t\tnew_bookmarks.add_bookmark(u'\"', value[1:])\n\t\telse:\n\t\t\tnew_bookmarks.add_bookmark(value)\n\treturn new_bookmarks\n\n\ndef from_encoded_values(cls"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t#\n\t# Check that class values correspond to the values we expect\n\t# to have...\n\t#\n\n\tassert isinstance(values, str), f\"class value needs to be a string.\"\t# noqa\n\tassert all(len(str(v) for v in values) == 2), \"class value needs to be two character.\"\n\n\t#\n\t# Get dictionary of bookmarks\n\t#\n\n\tdbvalues = {}\n\tdbvalues['type'] = 'Bookmarks'\n\n\t#\n\t# Check that the type is correctly used\n\t#\n\n\t"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\tparser = cls._from_parser()\n\treturn parser.from_str(''.join([str(x) for x in values]), skip_none=False)\n\ndef from_raw_int(cls, value):\t\"\"\"\n\tCreate a Bookmarks object from a single integer value as a string.\n\nYou need to cast the value to an integer type before using this function.\n\n:param value: integer value to be converted to string\n:type value: int\n\"\"\"\n\tparser = cls._from_parser()\n\tparser.from_int(value)\n\ndef from"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\tif type(values) == str:\n\t\tvalues = values.rstrip(';').split()\n\ttry:\n\t\tid, data, name = values\n\texcept (IndexError, ValueError):\n\t\traise ValueError(\"invalid value for 'id' attribute: {}\".format(values))\n\n\treturn Bookmark(id, data, name)\n\n\nclass Bookmarks(object):\n\n\tdef __init__(self, session=None, title=None, entries=None):\n\t\t\"\"\"\n\t\t:param session: a ``Flask`` Flask session\n\t"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\tassert isinstance(values, list), \\\n\t\t\"Bookmarks.from_raw_values: Expected list parameter, got %r\" %\n\t\tvalues\n\tfor t in values:\n\t\tassert isinstance(t, str), \\\n\t\t\t\"Bookmarks.from_raw_values: Expected string parameter, got %r\" %\n\t\t\tt\n\t\t# TODO: Is this a reasonable constraint?  If not then you may get\n\t\t# unreliable values when iterating over the object.\n\t\tif not len(t) == 2:\n\t\t\traise ValueError(\""}]}
{"task_id": "62b87b199a0c4fa8b80b354c", "prompt": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif not isinstance(seq, types.ListTypes):\n\t\tseq, seq_element = type(seq), tuple(seq)\n\telif seq_element in seq:\n\t\tseq, seq_element = seq, next(seq_element)\n\tresult = zip(seq, seq_element)\n\tif bufsize is None:\n\t\treturn (tuple(_to_bytes(seq)),) + result\n\telif seq_element in seq:\n\t\treturn (tuple(_to_bytes(seq)),) + result\n\telse:\n\t\t# sequence is bigger than necessary to"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tdef _get_seq_tuple(seq, sizes):\n\t\tif sizes:\n\t\t\tif isinstance(seq, basestring):\n\t\t\t\tc = 0\n\t\t\t\ts = len(seq)\n\t\t\t\twhile c < s:\n\t\t\t\t\tif len(seq) - c < sizes[c]:\n\t\t\t\t\t\traise OverflowError, \"bad sequence length\"\n\t\t\t\t\t\tbreak\n\t\t\t\t\tc += sizes[c]\n\t\t\t\tseq, type = _get_seq_tuple(seq[sizes[c]:], sizes[:c])\n\t\t\telse:\n\t\t\t\tseq, type"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\ttry:\n\t\tif not isinstance(seq, string_types):\tseq = seq.decode('utf8').strip()\n\texcept UnicodeDecodeError:\treturn (None, None)\n\tseq = seq.upper()\n\tif seq[0] == '+': return ('+', int), '+ '\n\tsep_str = seq[0:1]\n\tif seq[0] == '.' and seq[1] == '.': return ('.', int)\t\n#\n# From Sedgewick:\n#  -Demonstration with more complicated test sequences."}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tseq_name = seq.name\n\tif seq_name not in _seq_name_cache:\t\t# pylint: disable-msg=R0932\n\t\tseq_type = seq.type\n\t\tif seq_type not in _seq_type_cache:\t# pylint: disable-msg=R0932\n\t\t\traise UnrecognizedSequence()\n\t\t_seq_name_cache[seq_name] = seq_name, seq_type\n\n\treturn (seq, _seq_type_cache[seq_name])\n\n\ndef _get_seq_"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif len(seq) < 1: return (None,TYPEseq)\n\tif bufsize is None:\tbufsize = 1\n\treturn _seq_from_numbered(seq, 0, bufsize, TYPEseq)\n\ndef _seq_from_numbered(seq, start, bufsize, T):\n\t\"\"\"\n\tseq : SEQUENCE (simple type)\n\tstart : absolute position in *seq* (0-based)\n\tbufsize : size of the buffer to use\n\tT : :class:`SequenceType <sage.topology.sequence.Sequence_type>`\n\t\"\"\"\n\tT"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tassert isinstance(seq, Sequence), 'arg 1 wrong type'\n\tassert len(seq) > 0, 'arg 2 wrong size %d for %s' % (len(seq), seq.__class__.__name__)\n\tif hasattr(seq, 'file_item'):\n\t\treturn (seq._buf, seq.file_item.type)\n\tif hasattr(seq, 'to_string'):\n\t\tname = seq.value.group(1)\n\telse:\n\t\tname = '[sequence: %s]' % seq[0].value\n\treturn name\n\n_"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif seq is None:\n\t    try:\n\t\tseq = list(seq)\n\t    except TypeError:\n\t\tseq = list(seq)\n\t\tif _iterable(seq):\n\t\t    raise TypeError, \"can't convert iterable to sequence\"\n\t    return (seq, seq.__class__)\n\t    \n\tif len(seq) == 0:\n\t    raise TypeError, \"empty sequence detected\"\n\t\n\tif not isinstance(seq[0], list) and not not isinstance(seq[0], tuple):\n\t    if isinstance"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tres = (_get_obj(seq,*seq) for _) in self._get_dict()\n\tif bufsize is None:\n\t\treturn res\n\telse:\n\t\treturn (res[0], res[1])\n\n/*\n * Copyright 2016 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif isinstance(seq, str_types):\n\t\tn_type = int(seq.strip().split('-')[0])\n\t\tseq = [seq, [0, [], []]]\n\t\ttype  = n_type\n\t\tif n_type>=0 and isinstance(seq[0], dict):\n\t\t\treturn seq, n_type\n\t\tif n_type == 0: return seq, n_type\n\t\treturn seq, type\n\telse:\n\t\treturn [seq[0], seq[1]], seq[1]._type\ndef _get_seq"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t_check_isinstance(seq, sequence)\n\treturn (seq, get_type_code(seq))\n\ndef _get_seq_with_length(seq, min_length=None):\t\"\"\"\n\tReturn (sequence, number of elements) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t_check_isinstance(seq, sequence)\n\tif min_length is None:\n\t\t_len = _len = len(seq)\n\telse:\n\t\t# min_length is a"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3581", "prompt": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif other is None:\n\t\treturn self._len_scaled\n\n\t# if scale is zero...\n\tif self._len_scaled == 0:\n\t\tself._len_scaled = len(self._histogram)\n\n\t\t# compute scaling\n\t\tif self.n_bins == 0:\n\t\t\tself.n_bins = self._len_scaled // self._n_partitions\n\n\tscaled = self._len_scaled * self._scale_fct(\n\t\t\t\t\t\tself._len_scaled, self._n_partitions)\n\n\tif other:\n\t"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif other is None:\n\t\tself._histData.scale = 100\n\t\tpass\n\telif isinstance(other, float):\n\t\treturn self._getScale() * other\n\tpass\n\n\ndef histogram(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set histogram.\n\nIf *other* is ``None``, return histogram of this histogram.\nIf its histogram was not computed before, it is computed and stored for subsequent use.\n\nIf a float *other* is provided, rescale this histogram to *other*\nand recompute it if"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tself.validate()\n\tif other in (None,): return self\n\tnew_self = self.copy()\n\torig_intv = set_scale(new_self)\n\tif recompute:\t\tshutil.recompute(self)\n\tif new_self.scale is None: return new_self.scale\n\treturn new_self.scale\n\ndef get_scale(self, other=None):\n\t\"\"\"Computes histogram scaling factor.\"\"\"\n\tself.validate()\n\tif other in (None,): return self.scale\n\treturn get_scale"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif self.has_scale_function:\n\t\traise TypeError(\"Currently scale is only defined for empty histograms.\")\n\tif other is None:\n\t\treturn self.scale_function(0)\n\tif recompute:\n\t\traise TypeError(\"cannot rescale histogram with scale_function=None\")\n\tif self.has_scale_function and isinstance(other, six.integer_types):\n\t\traise TypeError(\"scale == 0 can't be rescaled\")\n\tself.compute_scale()\n\tif self.has_scale_function:\n\t\tnew_"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\ttry:\n\t\treturn self.__call__(func._scale, other=other, recompute=recompute)\n\texcept IndexError:\n\t\traise\n\texcept ValueError:\n\t\tpass\n\t\n\ttry:\n\t\tother = other or self.__call__()\n\t\tother.recalc()\t# make sure rescale doesn't forget about this scale\n\t\t\n\t\treturn other\n\t\n\treturn self.__call__(len.__call__(), other=other, recompute=recompute)\n\n#\n# Functions implementing the histogram\n#\n\t\n\t__add__ = __add__"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t# if other is not None, validate this histogram:\n\tif other is not None:\n\t\tif not isinstance(other, BaseHistogram) \\\n\t\t\tor len(other) == 1:\n\t\t\traise TypeError(\n\t\t\t\trepr((type(self).__name__, \".__str__()\", \"other\")))\n\n\t# compute *other* if necessary:\n\tif other is None:\n\t\tother = _calcscale(_calcinitscale(len(self)))\n\n\tscale_ = _histomicstimate(len(self), other, self.max)\n\n\t"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\treturn self.scale_with_nan(other, recompute, \"no such scale\")\n\n\ndef scale_with_nan(self, other):\n\tdef my_func(i, j):\n\t\tif i == 0 or j == 0:\n\t\t\treturn None\n\t\tif i == 1 and j == 1:\n\t\t\treturn None\n\t\treturn int(float(i)**math.ldexp(j, 32))\n\treturn self.__class__(other, my_func)\n\n\n_all_histograms = [\n\t\"linear\",\n\t\""}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tself._scale = self.binwidth / self.bins * max(0.,other)\n\tif recompute: self._scale = -self._scale\n\treturn self._scale\n\n\t\n@derived('binwidth')\n@derived('bins')\nclass histogram2D(histogram):\n\t\"\"\"\n\t2D histogram container for numeric data.\n\n\tIt is not necessary for all histograms of the output type to\n\tbe considered for any use of ``Histogram.mean()`` or\n\t``Histogram.std()``.  If *bins* is not specified, histogram has\n\t"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif self._histogram_type!= 'numeric':\n\t\tif other:\n\t\t\th = self.normalize().hmax\n\t\t\th = self.hmin + other / h * self.hmax\n\t\t\treturn self.scale(h)\n\t\telse:\n\t\t\traise error('scale of histogram with zero-length bins')\n\telse:\n\t\treturn self.scale().get_scale()\n\ndef get_bin_edges(self, key):\n\t\"\"\"\n\tGet histogram bins\n\n\t:param key: histogram key\n\n\t:return: histogram"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tassert self.type == other.type\n\n\tself.calc()\t\t# compute scale\n\n\tif self.is_int():\t\t# fill self and store result for next time\n\t\tfor t in np.arange(self.dim):\n\t\t\tself._scale[t] = self._scale[t] * int(self._scale[t])\n\tif self.has_limits():\n\t\tself._scale_valid = True\n\treturn self\n\n\n#----------------------------------------------------------------------------\n#  Internal Methods\n#----------------------------------------------------------------------------\n\ndef add_numeric_bin_width"}]}
{"task_id": "62b87b519a0c4fa8b80b3583", "prompt": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\ttry:\n\t\td = self.get_dict()\n\t\tother = other or d\n\n\t\tif hasattr(other, \"reshape\"):\n\t\t\tother = other.reshape(1, 1)\n\n# If shape() is provided, return that shape.\n\t\telse:\n\t\t\treturn other\n\t\t\td = self.get_dict()\n\t\t\tother = other.__class__(d)\n\texcept AttributeError:\n\t\tpass\n\n\t# Reshape so we are not transposed.\n\tif isinstance(other, numpy.matrix.matrix_) and hasattr"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tif isinstance(other, float):\n\t\tif other == 0.0:  return self.scale\n\t\treturn self.scale * other\n\treturn self.graph.scale.rescale(other)\n\t\n\t\ndef scale_from_factor(self, factor, origin=None):\n\t\"\"\"\nGet or set the scale and offset of the graph.\n\nIf *factor* is out of range for the graph itself (and ``None``), then\nreturn new_graph with the scale changed.\n\nIf *origin* is ``None``, then the graph has no offset;\n"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t#print \"Graph.scale() [%s]\" % (self)\n\tif other is not None:\t\t\t\t\t# is not an integer or string\n\t\ttry:\n\t\t\tself.rescale(other)\n\t\texcept:\n\t\t\traise LenaValueError(\"%s is not a valid scale\" % (other,))\t\n\t\telse:\t\t\t\t\t\t\t\t# no exception\n\t\t\treturn\t\t\t\t# done\n\tself.rescale()\n\n\treturn\n\n\t#self.scale()\t# raise exception\n\t#\"\"\"\n\t#Represents all of the fields in the Graph;"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tif self.coord is not None:\n\t\ts = self.coord\n\telif other is not None:\n\t\ts = other.coord\n\telse:\n\t\ts = self.scale\n\treturn s\n\n\ndef rotate(self, angle, ax=0, ay=0, az=0, origin=None):\n\t\"\"\" Return the rotate transform of this graph, with *angle* in radians.\n\tArguments are:\n\t*angle* : numeric\n\t*origin* : numeric\n\tTo get meaningful results, graph's fields are used.\n\tAll errors"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tif self._scale is None and other is None:\n\t\tself._recale()\n\telif self._scale is None:\n\t\tself._scale = self._to_norm(other)\n\telif other is None:\n\t\tother = other\n\t\tif self._scale is None:\n\t\t\traise ValueError(\"scale not defined\")\n\t\tself._scale = other / self._scale\n\n\tif self._xoffset == 0.0 and self._yoffset == 0.0:\n\t\tself._offset = 0.0\n\telif self._offset == 0"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tif self._n == 0:\t\t\t# no point in plotting anything\n\t\tself._scale(0)\n\t\treturn\t\t\t# no scaling\n\n\tn = len(self.data.nodes)\n\tif isinstance(other, (str,tuple)):\t\t# get field of *other*\n\t\tif np.all((map(map,other.split(\"\\\\.\")))):\t# if.[2..][2..][2..]\n\t\t\tscale2d, fields2d = self.get_fields(other)[0]\t# get 2D coordinates"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tif other is None:\n\t\treturn self.scale_y*self.scale_x\n\tscale = self.scale_x**self.scale_y\n\tother = other*SCALE_RATIO\n\tfor c in other:\n\t\tscale *= c\n\tscale += self.offset\n\treturn scale\n\nScale.y = scaley\nScale.x = scalex\nScale.field_name = _get_field_name('scale', scale, scale)\n\ndef get_node_center_x(gn, g):\n\t'''Get the x coordinate of the ("}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\treturn self.get('rescalef', other)\n\n@attribute\nclass Node:\t\"\"\"\nAn **node** is a set of objects on the space formed by\nan array for a single vertex.\n\nNodes can be inserted into a graph via the :ref:`Graph API`::\n\n\tgraph = Graph()\n\tgraph.add_layout(3)\n\tgraph.add_node(Node(x=3, y=4, z=5), Color('red'))\n\tgraph.add_edge(Link(x=3, y=4, z1=5,"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tn = len(self)\n\tif other is None:\n\t\tif self.has_unknowns():\n\t\t\tx = []\n\t\t\ty = []\n\t\t\tz = []\n\t\t\tfor i in range(2):\n\t\t\t\tx0 = self.get_x(i)\n\t\t\t\ty0 = self.get_y(i)\n\t\t\t\tz0 = self.get_z(i)\n\t\t\t\tx.append(x0*other)\n\t\t\t\ty.append(y0*other)\n\t\t\t\tz.append(z0*"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tif (not self or not other):\n\t\traise LenaValueError(\"scale of empty graph\")\n\tif isinstance(other, Graph):\n\t\tif (not super(Graph, self) or\n\t\t\tother.length() == 0):\n\t\t\traise LenaValueError(\"invalid length\")\n\t\tother = other.clone()\n\tif self == other:\n\t\treturn self.get_xscale()\n\tif self.get_graph().number_of_dimensions()!= 3:\t# if is 3d, then do nothing\n\t\traise LenaValueError(\"not"}]}
{"task_id": "62b87b869a0c4fa8b80b35e1", "prompt": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 630.00 MiB (GPU 0; 22.16 GiB total capacity; 20.45 GiB already allocated; 393.88 MiB free; 20.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b8b4baeb7e40a82d2d1136", "prompt": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 22.16 GiB total capacity; 21.09 GiB already allocated; 3.88 MiB free; 21.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b8b4baeb7e40a82d2d1137", "prompt": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 22.16 GiB total capacity; 21.09 GiB already allocated; 3.88 MiB free; 21.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b8b4c1eb7e40a82d2d1139", "prompt": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tfor x in range(len(iface.interfaces)):\n\t\tif iface.interfaces[x] is not candidate:\n\t\t\tcontinue\n\t\tassert iface.interfaces[x]!= candidate, \"Attempting to compare interfaces with different interfaces. See tentative comment below.\"\n\t\tassert iface.interfaces[x] is tentative, \"Interfaces should not provide any tentative interfaces.\"\n\t\n\ttestMethods = [\n\t\t'model', 'default', 'getInterface','setInterface', 'addInterface','removeInterface', \n\t\t'setInterface', 'addObserver',"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tif isinstance(candidate, type):\n\t\tif (isinstance(candidate, type) and (isinstance(iface, type)\n\t\t\tor (isinstance(iface, type) and issubclass(iface, candidate))\n\t\t\tand candidate is iface)):\n\t\t\treturn\n\t\treturn\t\t# can't check in this context, so let's fail\n\n\tif (isinstance(candidate, type) and isinstance(iface, type)):\n\t\treturn\n\telif (issubclass(candidate, type) and not issubclass(iface, type)\n\t\tand (issubclass(iface, type"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tif hasattr(iface,'verifyClass'):\t\t\t\treturn iface.verifyClass(iface,candidate,tentative)\n\t## if no, then verify against all interfaces and classdefs...\n\tfor name in dir(iface):\t\t\t\t\tfor entry in os.scandir(iface):\tif entry.is_dir()\t\tif os.path.splitext(entry.name)[1] in ['C'] : continue\t\tif (name == entry.name or 'C' not in name) and 'Class' not in name and 'Interface' not in name\t:\n\t\tresult"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tfrom IronPythonTest.junk import MethodMock\n\tself = iface\n\n\tfor arg in candidate.split(os.pathsep):\n\t\tif os.path.commonpath([arg])!= os.path.commonpath(self.split(os.pathsep)):\n\t\t\terror(type(self), \"Class does not define \" + arg)\n\n\tfor arg in candidate.split(os.pathsep):\n\t\tif not os.path.commonpath([arg]) == os.path.commonpath(getattr(self, arg)):\n\t\t\terror(type("}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tif not hasattr(iface, '__init__'): return False, 'No __init__' \\\n\t\t\t\t\t+ str(type(iface)) + \\\n\t\t\t\t\t' (should have a \"__init__\" attr)'\n\ttry:\n\t\tif not issubclass(candidate, iface): return False, \\\n\t\t\t'issubclass of %r; want %r' % (candidate, iface)\n\t\tif not issubclass(super(iface,#candidate), T)): return False, \\\n\t\t\t'issubclass of %r; want %r' % (super(iface,#candidate),"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tif candidate == None: return\n\tcandidate = eval(candidate, locals())\n\tif not issubclass(candidate, iface): return\n\n\tif isinstance(candidate, type):\n\t\t_verify(candidate, tentative)\n\telse:\tverify(candidate)\n\ndef verify(candidate, tentative=False):\n\tverifyObject(candidate, 'candidate', tentative)\n\tverifyObject(candidate, 'tentative', tentative)\n\tif candidate is None:\n\t\treturn\n\n\t# XXX check that *candidate* and its superclasses are *"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tif tentative and ( candidate.isTentative() or candidate.isCandidate() ):\n\t\treturn True\t\t\t\t\t\t\t\t# Not sure why this happens...\n\t\t#print \"Nope:\",candidate,\"...Yes:\",iface,\"...Now\",tentative\n\t#else:\n\treturn False\t\t\t\t\t\t\t#...But just in case..\n\t#return bool\t\t\t\t\t\t\t\t#...I don't like that..\n\n################################################################################\n\nclass Generic(object):\n\t\"\"\"\n\tThis generic class is an instance of a\n\t(weakly-specialized) class.\n\n\tIt stores the"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tfrom.util import verifyClassInterface, verifyClassMethod, verifyClassProperty, verifyClassSettable, verifyException, \\\n\t\tverifyMethod, verifyProperty, verifyClass, verifyInterfaceSettable, verifyInterfaceMethod, verifyGenericClass\n\n\tresult = verifyClass(*iface)\n\tif result:\n\t\tif not tentative:\n\t\t\treturn result\n\t\traise VerifyError, result\n\n\traise VerifyError if result else VerificationError(\"failed to verify interface '%s' of class '%s'\" % (\n\t\tiface.__name__, clazz.__name__))\n\ndef verify"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tif candidate:\n\t\tif not hasattr(candidate, '_pydev_id_generator'):\n\t\t\tcandidate._generateClassId()\n\t\tif not hasattr(iface, '_pydev_id_generator'):\n\t\t\tiface._generateClassId()\n\t\tcandidateId = candidate._pydev_id_generator()\n\t\tifaceId = iface._pydev_id_generator()\n\t\tassert candidateId == ifaceId, 'Cannot define two classes for the same interface.'\n\tif tentative:\n\t\tassert iface in getInterfaceIds(), 'No instances were found.'"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tcandidate.verify(iface)\n\tif tentative: return\n\tfrom_candidate = (IFACE_TO_CLASS[iface.type] for iface in iface.implementedInterfaces)\n\tif from_candidate == None: return\n\tif not candidate.satisfies(from_candidate):\n\t\tmismatch = iface.type + \" \" + iface.name + \" \" + get_name(candidate)\n\t\tprint(\"mismatch:\", mismatch)\n\t\timport traceback\n\t\tprint(traceback.format_exc(False, False, mismatch, None))\n\t\tprint(trace"}]}
{"task_id": "62b8b559eb7e40a82d2d11f6", "prompt": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t", "suggestions": [{"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t#\n\tprint \"determineMetaclass:\", bases, explicit_mc\n\t#\n\t# Is there one of our bases?\t\t\t\t#\n\t# This will also try to detect the most derived base; if so, we\n\t# don't care how many bases there are\t\t#\n\ttry:\n\t\tif len(bases) == 1:\n\t\t\tbases = (bases,)\n\texcept IndexError:\n\t\tprint \"error, could not determine metaclass\"\n\t\traise SystemExit()\n\t#\n\t# Now, for each base, find an explicit metaclass"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tif __name__ == '__main__':\t\t\t\"\"\"\n\tif explicit_mc is None:\t\t\treturn None\n\twhile bases:\n\t\tif isinstance(bases[-1], type):\n\t\t\tcontinue\n\t\tbases = bases[:-1]\n\t\tcontinue\n\tif len(bases) <= 2:\n\t\tif len(bases) == 2:\n\t\t\treturn bases[1]\n\t\tif len(bases) == 1:\n\t\t\treturn bases[0]\n\t\traise AssertionError(\"too many bases; base types %s and %s\" % (', '."}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tbases_by_type = {}\n\tbases_by_name = {}\n\tfor base in bases:\n\t\tif base in bases_by_type:\n\t\t\tbases_by_type[base] += bases_by_type[base][:1]\n\t\telse:\n\t\t\tbases_by_type[base] = [base]\n\t\t\n\t\tif base in bases_by_name:\n\t\t\tbases_by_name[base] += bases_by_name[base][:1]\n\t\telse:\n\t\t\tbases_by_name[base]"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tif bases:\n\t\t# Get the implicit class with __metaclass__ if it exists (if any)\n\t\tmc = 0\n\t\tfor b in bases:\n\t\t\twhile b in sys.modules:\n\t\t\t\tif hasattr(sys.modules[b], \"__meta__\"):\n\t\t\t\t\tmc = getattr(sys.modules[b], \"__metaclass__\")\n\t\t\t\t\tif hasattr(mc, \"__name__\"):\n\t\t\t\t\t\texplicit_mc = mc\n\t\tif mc and not issubclass(explicit_mc, type):\n\t\t\treturn explicit_mc\n\t\t#"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\ttry:\n\t\tmetaclass = bases[0] or bases[1]\n\texcept IndexError:\n\t\treturn type\n\tif explicit_mc is None:\n\t\treturn types.from_metaclass(metaclass)\n\tmc = getattr(metaclass, '__metaclass__', None)\n\tif mc is None:\n\t\tif metaclass.__module__.startswith('dataclasses'):\n\t\t\ttry:\n\t\t\t\tmetaclass  # maybe metaclass.__module__ is not defined?\n\t\t\t\tmetaclass = types.from_metaclass(metaclass.__module__["}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tmetaclass = bases[0]\n\n\t\t\tif metaclass in [\"self\", \"cls\"]:\t\treturn  \"class\"\n\telse:\treturn \"object\"\n\n\nclass PyBaseExceptionWrapper(ExceptionWrapper):\n\t\"\"\" Exception wrapper to raise an exception at the very end of the script \"\"\"\n\t@classmethod\n\tdef __init__(self, func):\n\t\tsuper(PyBaseExceptionWrapper, self).__init__(func)\n\n\n\t\n\nclass PyStringObjectWrapper(object):\n\tdef __init__(self, func):\n\t\tsuper(PyStringObjectWrapper"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tmetaclass = None\n\tfor base in bases:\n\t\tmetaclassTuple= type(base)\n\t\tif isinstance(base, type):\n\t\t\tif metaclassTuple.metaclass:\n\t\t\t\tif isinstance(metaclassTuple.metaclass,type): continue\n\t\t\t\treturn base\n\t\t\telse:\n\t\t\t\tmetaclassTuple.metaclass= metaclassTuple.metaclass(metaclassTuple)\n\t\t\tcontinue # pragma: no cover\n\n\traise ValueError(prettyprinter._('Could not determine metaclass'))\n\n\ndef determine(bases, isSubsequence=False"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\timplicit = False\n\tmetaclass = None\n\tif explicit_mc is None:\n\t\timplicit = True\n\t\texplicit_mc = bases or []\n\telse:\n\t\texplicit_mc = [x for x in explicit_mc if x not in bases]\n\t\timplicit = explicit_mc and not isinstance(explicit_mc[0], type) and not explicit_mc[0][0].startswith('operator.')\n\t\tmetaclass = explicit_mc and explicit_mc[0][1] or ()\n\t\timplicit &= issubclass(metaclass, basemetaclass)"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tif explicit_mc is None:\n\t\texplicit_mc = __metaclass__\n\tfor base in bases:\n\t\tif isinstance(base, (type, typeobj)):\n\t\t\tbase_metaclass = base._replace(\n\t\t\t\tmetaclass=explicit_mc, module=__name__)(bases)\n\t\telif isinstance(base, tuple):\n\t\t\tbase_metaclass = base_metaclass.__class__(*base)\n\t\telse:\n\t\t\traise TypeError\n\t\tif base_metaclass is not MetaclassType:\n\t\t\treturn base_met"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tif \tnot explicit_mc:\n\t\texplicit_mc = True\n\tmc_metaclass = getmetaclass(bases[0] if 1+bases else type(T))\n\tmc_metaclass = mc_metaclass()\n\tif not explicit_mc:\n\t\tif mc_metaclass is object:\n\t\t\tmc_metaclass = _TYPE\n\t\telse:\n\t\t\tmc_metaclass = type(T)\n\tmc_metaclass = type(mc_metaclass)\n\tif \t(mc_metaclass is type) and issubclass"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47d", "prompt": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tresult = self.data.pop(key, default)\n\tself.size -= 1\n\treturn result\n\ndef keys(self):\t\"\"\"D.keys() -> list of all keys\"\"\"\n\treturn self.data.keys()\n\n#-------------------#\n# Class methods #\n#-------------------#\n\nclass Mapping(dict):\n\t\"\"\"D.__class__ dictionary wrapper.  Implements __setitem__() and __delitem__() methods.\"\"\"\n\t#----------------------------------------------------------------#\n\t# Properties & __init__() #\n\t#----------------------------------------------------------------#\n\n\tdef __init__(self, *args,"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tif key in self:\n\t\tdel self[key]\n\t\treturn self.data.pop(key, default)\n\traise KeyError(key)\n\ndef __setitem__(self, key, value): \"\"\"\n * Override the default __setitem__ method by adding a reference to the key.\nCalled for each key and value which is a list or any other non-empty sequences.\nParameters\n----------\nkey : a str\n    Key\n    Value\n\"\"\"\n\tself[key] = value\n\ndef __delitem__(self, key):\n\tdel"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\treturn self.dict.pop(key, default)\n\t\ndef popitem(self):\t\t\"\"\"Try to remove and return first element.\t\"\"\"\n\ttry:\n\t\treturn self.dict[self.start:self.end]\n\texcept IndexError:\n\t\tpass\n\n\tfor k in self.dict:\n\t\treturn k\n\n\traise KeyError(\n\t\t\"popitem(): key not found in dictionary.\tThe dictionary is now empty!\"\n\t)\n\t\n\t\n\t\nif __name__ == \"__main__\":\n\tdict = Dict()\n\t\n"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tif not self:\n\t\traise KeyError(key)\n\td = self._data.pop(key, default)\n\tif k in self:\n\t\tdel self[k]\n\treturn d\n\n\t\ndef popitem():\t\t\t\t\"\"\"D.popitem() -> (k, v), remove and return some or all (key,value) pairs\n\nD.popitem() -> (k, [v])\n\nRemove and return some or all (key, value) pairs in the dictionary. Keys and values should be iterables.\n\nIt is particularly useful in conjunction"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tif key in self and self[key] is not __marker:\n\t\treturn self.pop(key)\n\tif default is self.__marker:\n\t\treturn default\n\treturn self.default\n\ndef popdefault(self):\n\t\"\"\"D.popdefault() -> A default value for self.\"\"\"\n\twhile True:\n\t\ttry:\n\t\t\tret = self.popitem()\n\t\texcept KeyError:\n\t\t\tbreak\n\t\texcept AttributeError:\n\t\t\traise key_type_error(\n\t\t\t\t\"pop on unhashable type: %s\" %"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\treturn self.__getitem__(key, d=d)\n\n#ifndef _MFnTransform2\n#define _MFnTransform2\n//\n//-\n// ==========================================================================\n// Copyright (C) 1995 - 2005 Alias Systems Corp. and/or its licensors.  All \n// rights reserved.\n// \n// The coded instructions, statements, computer programs, and/or related \n// material (collectively the \"Data\") in these files contain unpublished \n// information proprietary to Alias Systems Corp. (\"Alias\") and/or its \n// licensors, which is protected by Can"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tif key not in self: raise KeyError(key)\n\treturn self.data.pop(key, d)\n\n\n###############################################################################\n#                                                                             #\n#               Parsing and interpretation"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\treturn self.__map.pop(self, key, default)\n\n\tdef items(self, *args, **kws):\n\t\t\"\"\"D.items() -> iterator over key, value pairs.\"\"\"\n\t\treturn self.__map.items(*args, **kws)\n\n\tdef keys(self, *args):\n\t\t\"D.keys() -> iterator over keys, value pairs, and a boolean indication of whether they were mapped.\"\n\t\treturn self.__map.keys(*args)\n\n\tdef popitem(self, last=True):\n\t\t\"D."}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\treturn self.dict.pop(key, d=default)\n\n\t\nif sys.version_info[0] == 2:\n\t\n\tdef setdefault(self, key, d=None):\t\t\"\"\"\tSet default value for specified key.\n    See PEP 448\"\"\"\n\t\n\tdef setdefault(self, key, d=None, default=object()):\t\t\"\"\"\tSet default value for specified key.\n    See PEP 448\"\"\"\n\t\n\tdef pop(self, key, default=None):\n        return self."}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tif default is self: raise\n\t\td = default\n\t\tdel self[key]\n\t\treturn d\n\n#=======================================================================\n# Decorator for key/docstring manipulation functions\n#=======================================================================\ndef _default(func):\n\t\"\"\" Wraps a function in the 'default' section of a docstring (e.g. added in __init__).\"\"\"\n\tdef wrap(self_, *args, **kw):\n\t\t\"\"\" Return the function itself, wrapped in a docstring.\"\"\"\n\t"}]}
{"task_id": "62b8d23748ba5a41d1c3f497", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\twhile (1):\n\t\t(key, value) = self.q.top()\n\t\tif value == None:\n\t\t\tbreak\n\t\tif key in self.dict:\n\t\t\tdel self.dict[key]\n\n\treturn key, value\n\n\nif __name__ == '__main__':\n\n\t# load test cases as key (case.keys()[0]) and value (case.keys()[1])\n\tfor k in range(len(list(TestCase.__dict__.keys()))):\n\t\tobj = TestCase.__dict__[k]\n\t\tprint('"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\titems = [(k, v) for k, v in self]\n\twhile items:\n\t\tkey = items.pop()\n\t\tself.pop(key)\n\treturn key, self[key]\n\ndef flatten(elements):\n\tf = set(map(type(x), elements))\n\tif f.__module__ == \"set\" and f.issubset(set):\n\t\tprint(\"You cannot flatten %s into itself \" % str(elements))\n\t\tsys.exit(1)\n\treturn list(itertools.chain.from_iterable(x"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\tdel self[next(iter(self._dict.keys()))]\n\treturn self._dict[key]\n\ndef keys(x, *, sort=True):\n\t\"\"\"Return an iterator over the given `dict`'s keys (in the same order).\n\n\t:param x: dict to search. If an entry is ``None`` that entry will be skipped.\n\n\tThis is the reverse operation of the Python ``list``/``dict`` functions. It\n\tis inexplicably faster on most dicts compared to simple lists and sets.\n\n\tIf *sort* is"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\ttry:\n\t\tself.next()\n\t\tif self:\n\t\t\tv,next_v=self.next()\n\t\t\tself.discard(v)\t# discard element now that we know it isn't there\n\t\t\treturn (v,next_v)\n\n\n#--------------------------------------------------------------\n\nclass dictassem(collections.MutableMapping):\n\t\"\"\" A dict, subclass of list, list, or other dictionary-like container.\n\n\t   A dict is a container class whose values consist of some number of\n\t   elements.  It supports all of the same methods as"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\ttry:\n\t\tresult = self._rawlist.popitem()\n\t\tdel self._rawlist[sorted(self._rawlist.items())[0]]\n\texcept KeyError:\n\t\traise KeyError('pop from empty list!')\n\treturn result\n\nclass HashableMap(collections.MutableMapping):\n\t\"\"\" HashableMap that works exactly like a normal OrderedDict.\n\tNote that the OrderedDict is stored as-is, so the keys\n\tare always strings.\n\t>>> obj = HashableMap({('foo','bar'):('baz','bat')}\n\t>>> Hashable"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\treturn self.sdict.popitem()\n\n\t#def setdefault(self, key,*args):\t\t\"\"\"\n\t#\tInsert or set the keyword `key=value` in the ``.sdict`` dictionary.\t\"\"\"\n\t#\tif args:\n\t#\t\tsetattr(self, key, args[0])\n\t#\treturn self.sdict.setdefault(key, None)\n\t#def __setitem__(self, key, value):\tself.set(key, value)\n\t#def addpath(self, key):\t\tself[key"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\treturnself=self\n\t# def remove_and_return(self,key,value):\n\t# \treturn self.remove((key,value))\t\n\t# def __getitem__(self,key): return self[key]\n\t#\n\t# if hasattr(self,'_cache_data'):\n\t# \treturn\n\t# if self._cache['_data'].has_key(self._cache_key):\n\t# \treturn self._cache['_data'][self._cache_key]\t# this will throw an exception if the object has already been removed\n\t\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\tif self.count:\n\t\tdel self.dict[self.count-1]\n\t\tdel self.count\n\treturn self.dict.pop(0)\n\nusing Avalonia;\nusing Avalonia.Controls;\nusing System.Application.UI.ViewModels;\n\nnamespace MvvmFxProjectTemplate.Framework\n{\n    public class MessageFactory : IMessageFactory\n    {\n        private Control _messageItem;\n\n        public void Init(Control messageItem)\n "}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\tif DEBUG_POP:\n\t\t_type, value = next(iter(self._container.keys()))\n\t\tassert isinstance(value, (float, List))\n\t\tif self._is_list:\n\t\t\tassert isinstance(self._container, dict)\n\t\t\tl = []\n\t\t\tfor o in value:\n\t\t\t\tfor k in self._container.keys():\n\t\t\t\t\tif self.get(k) == o:\n\t\t\t\t\t\tl.append(k)\n\t\t\t\tvalue.clear()\n\t\t\tif 'key' not in value:\n\t\t\t\tvalue"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\tdel self\n\ti = 0\n\tk = None\n\tfor e in self:\n\t    if k is not None and e[0] == k:\n\t\tdel self[k]\n\t    if e[1] > i:\n\t\tk = e[1]\n\tk = None if k is None else k\n\treturn k\n\ndef __sort_iter(x):\n\t\"\"\"\n\tSort the order in an iteration using bisecting along the last axis.\n\t\"\"\"\n\tx.sort(key=functools.cmp_to"}]}
{"task_id": "62b8d23a48ba5a41d1c3f499", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\treturn self.__poptop()\n\ndef pop(self, key, *args):\n\t\"\"\"Remove and return the nth most recent item in the `key-` sequence.\n\n\tIf `key` is an integer, it specifies the position of the item\n\tto be deleted. If `key` is a string, it specifies the item\n\tto be deleted.\n\n\t`key` should be either an integer index, string key name, or\n\ta callable used to select from the contents of a dictionary.  Any\n\tpassed-in arguments are converted to their compatible form.\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\tresult = []\n\tpairs = self._items\n\tlast = None\n\twhile pairs:\n\t\tif last is None:\t# first time seen\n\t\t\tlast = pairs.pop()\n\t\telse:\t\t# append new pair\n\t\t\tresult.append((last[0], pairs.pop()))\n\t\tlast = pairs[0]\n\treturn result\n\nclass JSONParser(object):\n\t\"\"\"A specialized `object` for keeping track of the keys and\n\tvalues as well as working with nested keys and values.\"\"\"\n\n\tdef __init__(self):\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\treturn list(self.entries).popmin()\n\n\t# This next method uses the current (key, value_) pair from the LRU\n\t# (increasing key) to keep the LRU order as long as the key exists. If\n\t# `key` is not already present in the cache, then `create` is\n\t# used as the new value for `key`. Otherwise the cache entry is\n\t# removed and the new item is put back in the cache.\n\n\tdef pop(self, key, create=False):\n\t\t\"\"\"\n\t\tRemove and return the `("}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\treturn next((k for k,v in self._dict.items() if v >= self._cutoff),None)\n\n/*\n* Copyright (c) 2009-2011, Intel Corporation\n*\n* Permission is hereby granted, free of charge, to any person obtaining a\n* copy of this software and associated documentation files (the \"Software\"),\n* to deal in the Software without restriction, including without limitation\n* the rights to use, copy, modify, merge, publish, distribute, sublicense,\n* and/or sell copies of the Software, and to permit persons to whom the\n* Software is furnished to"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\tlst = copy.copy(self)\n\t# The order of pairs is important here.  It assumes that it's okay\n\t# to remove an element that's already in the list (since if removing\n\t# the head, the \"head\" will be the new head), and if the list is\n\t# empty, it will fall through.\n\tpairs = list(itertools.combinations(lst, 2)) # lst.__getitem__(2)  # __getitem__(2)\n\ttail = pairs.pop()\n\tif tail!= sentinel and tail[0]!= sentinel:\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\tif debug: log.msg(2, \"popitem\")\n\n\tself.last = self.items[-1]\n\tdel self.items[-1]\n\treturn self.last\n\n#ifndef __BENCH_COMMONS_H__\n#define __BENCH_COMMONS_H__\n\n\n#if defined(_MSC_VER) || (defined(__GNUC__) && \\\n                          (__GNUC__ > 4 || \\\n         "}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\ttry:\n\t\tfor x in list(self):\n\t\t\tpop = self.pop(x)\n\t\t\tif pop: return (x, pop)\n\texcept ReferenceError:\n\t\tpass\n\treturn ()\n\n\n\n## List Functions ##\n\ndef as_lines(string): \t\"\"\"Returns list of lists with the text `string`\t\"\"\"\n\tline = [] # Initialize.\n\tfor item in string: \tline.append( (item, \"\") )\n\tline = [item for item in line if not (item[0] == \"#\")]\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\treturn self.__dict__.popitem()\n\n/*\n * Copyright (C) 2008-2018 P\u00c2RIS Quentin\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\tif self.__contains__ (\"__items\"):\n\t\tself.forget (\"__items\")\n\t\treturn b\"\".join(self)\n\treturn b\"\".join( (b'__item_' + k, v) for (key, v) in self.items () )\n\ndef _read (self, instream, format=None, **kwargs):\n\t\"\"\" Internal reader for file and memory input of any kind.\n\tSimilar to `__init__()`.\"\"\"\n\n\tif isinstance (self.__fp, _binaryIOBase):\n\t\t# If passed a file-like"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\treturn (self.list.pop(0), self.list.pop(0))\n\n\t#def keys(self):\t\"\"\"Returns a list of all key/value pairs.\"\"\"\n\t# \treturn list(self._data)\n\n# ------------------------------------------------------------------------------\n\n\ndef _set_path(target):\t\"\"\"\n\tSet the current path for the object.  This is automatically done at\n\tany time when called.\t\"\"\"\n\tglobal current_path\n\tcurrent_path=target\n\n\n# ------------------------------------------------------------------------------\n\n# Copyright (C) 1999-2019 Erik Walthinsen <"}]}
{"task_id": "62b8d23c48ba5a41d1c3f49b", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\tif len(self.stack) == 0 : raise KeyError('popitem() called with empty stack')\n\tpopped = self.stack[-1]\n\tself.stack[-1] = None\n\treturn popped\n\ndef reprfunc(self, obj, level=0):\n\treturn _reprfunc(self, obj, level=level)\n\n\ndef reraise(tp, value, tb=None):\n\tglobal error, context, sys\n\tif (trace and context and sys and\n\t\t(type(value) is BaseException or type(value) is ("}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\treturn self.pop(0)\n\n\n\t\n##\tCommon methods.\t\n##############\n\n\n\t\ndef getsize(self, arg = None):\n\t\"\"\"Return the approximate number of elements currently in the dictionary.\n\n\tIf the argument is omitted, always return an accurate number.\n\t\"\"\"\n\tif arg is None:\n\t\treturn self.__sizeof__()\n\t# use __len__ here to find out the number of elements\n\treturn len (self[self.__str__ ()] or ())\n\t\n\n\t\ndef clear(self):\n\t\"\"\"Clear everything"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\tr = (k for k,v in self._map.GetKeys())\n\t\tv = self._map.Get((r.pop(),(r[-1],self._default(self._key(k)))));\n\t\tself._map.Set((r.pop(),r.pop(),(v[(1)],v[(0)])))\n\treturn v\n\n\ndef popall(self):\n\t\"\"\"\n\tReturn a list of all elements in the set `self`.\n\tIf the set has no elements, return `()`,else return a list of pairs\n\t(key, value)."}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\tself._data[id(self)] = self._head\n\treturn self.get(id(self))\n\ndef sortediter(lst):\n\t\"\"\"Yield (key, data)...\"\"\"\n\tfor key, item in lst:\n\t\tyield key, item.get_data()\n\ndef map(func, iterable):\n\t\"\"\"Map `func(key, value)` to `**iter(map(func, iterable))`\"\"\"\n\treturn func(*sortediter(iterable))\n\ndef maxby(iterable, key=lambda x: (x[0], x[1])):"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\tresult = self.__dict__.pop('_dict_recycle')\n\tif result is not None and type(result) is dict:\n\t\tresult = result['_dict_recycle']\n\t\tself.__dict__.pop('_dict_recycle')\n\treturn result\n\npackage com.yiqiniu.easytrans.demos.utils.springbootwebsocket;\n//\n//\n//@Ignore\npublic enum WebConfigBean {\n\tWEB_CONFIG_POST()\n\t;\n\n\tprivate final String url;\n\n\tWebConfigBean(String url) {\n\t\tthis.url"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\tpass\n\n\n\n\n@lru_cache(maxsize=float(\"inf\"))\nclass HashBang(Hash):\n\n\t\"\"\"Represents a `(key, value)` pair in a `dict` mapping of `str` keys to values.\n\tKeys may be any sequence of Unicode strings (Unicode char strings, byte strings,\n\tand arbitrary bytes objects using encoding), and values are any number of\n\tcharacters (Unicode codepoints, byte strings, and arbitrary bytes objects\n\tusing encoding, or characters in the range U+0000 through U+10FFFF) using the\n\tsurrogates not"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\ttry: # Python 2.x\n\t\treturn self._stack[-1]\n\texcept IndexError: # Python 3.x\n\t\tpass\n\tif self._stack:\n\t\tdel self._stack[-1]\n\treturn None\n#@+node:ekr.20170914060936.1: *3* e.findItem\ndef efindItem(self, key, defaultValue=None):\n\t\"Find key\"\n\n\tif not self:\n\t\treturn efindItem.__doc__\n\t# Use first element of stack as the default value\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\treturn tuple([(k, v) for k, v in dir(self) if not k.startswith('_')])\n\n\nclass Command(Base):\n\n\tdef __init__(self, parser, args):\n\t\t# check for a valid usage for a command:\n\t\t#   help\n\t\t#   subcmd [options]\n\t\tassert isinstance(args, list)  # command line\n\t\tself.args = args\n\t\tBase.__init__(self, parser, args)\n\n\t\t# list of available options:\n\t\tself.options ="}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\tret = None\n\ttry:\n\t\twhile 1:\t# This block is needed for popitem from Python 2.\n\t\t\ttry:\n\t\t\t\tiv = self.data[0]\t# The first item in the container (most recent\n\t\t\t\t\t\t\t\t\t\t# to be removed)\n\t\t\texcept:\t\t# Here the index 0 item was never accessed.\n\t\t\t\traise IndexError\n\t\t\tif iv in self._data: \n\t\t\t\tself._data.remove(iv)\t# The item is in the container.\n\t\t\t\tif iv < 0: self._data.pop(-"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\tif self.size > 0:\n\t\t(key,value) = reduce(lambda x, y: x+y,self.elts)[:2]\n\t\tself.pop(key)\n\t\treturn key, value\n\telse:\n\t\traise KeyError(\"popitem(): dictionary is empty\")\n\n/*\n * Copyright (C) 2008 The Guava Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except\n * in compliance with the License. You may obtain a copy of the License at\n *\n"}]}
{"task_id": "62b8d23e48ba5a41d1c3f49e", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tif self.current_iter is not None:\n\t\tdel self.result_iter[:]\n\tif self.index is not None:\n\t\tdel self.index\n\tif len(self.random_state) > 0:\n\t\tdel self.random_state[self.index]\n\treturn (self.index, self.result_iter.popleft())\n\n\"\"\"\nThe MIT License (MIT)\n\nCopyright (c) 2015-present Rapptz\n\nPermission is hereby granted, free of charge, to any person obtaining a\ncopy of this software and associated"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tif self.size == 0:\n\t\traise KeyError('dictionary is empty')\n\telse:\n\t\tr = self._raw_pairs.pop(0)\n\t\titem = (list(self.keys()),self._get_value(r))\n\t\tself._raw_pairs.pop(0)\n        return r, item\n\nclass _OrderedDict(object):\n\n\t\"\"\"Internal dictionary representation.\n\n\tThis implementation is universally mutable, but updates operations\n\traise KeyError when attempting access it as a read-only dict lookup.\"\"\""}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tresult = []\n\twhile True:\n\t\tkey, value = self.random_element()\n\t\tif key!= None:\n\t\t\tresult.append((key, value))\n\t\telse:\n\t\t\tbreak\n\treturn result\n\npackage net.fortytwo.ripple.model.types;\n\nimport ddf.minimizer.Minimizer;\n\n/**\n * @author Joshua Shinavier (http://fortytwo.net)\n */\npublic class NumberType extends RealType {\n    private static NumberType instance;\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\treturn self._popitem()()\n\n\tdef setdefault(self, dflt):\t\"\"\"\n\tSet the default value for dflt (any object) and return it.\t\"\"\"\n\tif dflt is not None:\n\t\tdvalue = self._dvalue(dflt)\n\t\ttry:\n\t\t\tdvalue = self._dvalue(dflt)\n\t\texcept AttributeError:\n\t\t\tdvalue = self._default(dflt)\n\t\treturn dvalue\n\telse:\n\t\treturn self._default(dflt)\n\ndef loadClassRegistry(loader, name,"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\treturn _removeitem(randrange, self.keys(), self.values())\n\t\t\ndef get_random_key(length=50, randfunc=randrange, seed=np._NScNds()):\n\t' Return a random *length* byte string without collisions (less than or equal to 0)'\n\tbytes._check_end()\n\treturn _get_random_key(length, randfunc, seed, bytes.fromhex)\n\t\t\n@register_jitable\ndef get_random_key(length=10, randfunc=randrange, seed=_np.random."}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\treturn tuple(self.random())\n\n#....................{ FRACTIONS ~.sort }......................\ndef sortkey(self, key=None, reverseorder=True):\n\t\"\"\"Recursively sort the dictionary, keeping ordering.  If *key* is unspecified, the\n\tfirst integer less than the highest integer key is returned; key = 0, 1, or 2 if *key*\n\tis specified, the second integer less than the second highest integer key is returned.\n\t*reverseorder* determines whether the return value is descending rather than\n\tincreasing.\n\t\"\"\"\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tif self.random:\t\t\t\t\t # Check we have been initialized.\n\t\tpair = self._getrandom() # Get the random element.\n\t\tif pair:\t\t\t\t\t # If we were successful, drop the second index.\n\t\t\tself._dict.pop(pair.first)\n\t\t\tself._index-=1 # Update the length of the dict.\n\t\treturn pair.first,pair.second\t\n\t# All items are removed, so either.\n\telse:\t\t\t\t\t\t # Get the first item, possibly a second item.\n\t\tleft,right  = self._dict.keys()"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tif hasattr(self, '__randitem__'):  self.__randitem__().__delete__() # for Py3\n\tself[random()]                            # __delete__()\n\treturn self.rawname # return original name\n\nclass SizedList:\n\t\"\"\"\n\tA generic list class that stores Sized instances directly as\n\tstrings.  This only supports the standard container types:\n\tlist, tuple, and frozensets.\n\tSanity checks a list before using it.\"\"\""}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tresult = dict((k, self._pairs[k][0]) for k in self._pairs.keys())\n\tself._pairs.clear()\n\treturn result\n\ndef __setstate__(self, state):\n\tif \"__dict__ == self.__dict__\" in locals():\n\t\tlocals()[self.__dict__] = state\n\t\tdel state[\"pairs\"]\n\t\tdel state[\"default_factory\"]\n\t\tdel state[\"factory\"]\n\t\tdel state[\"debug\"]\n\ndef _test():\n\tif not py3k or (py3k and not hasattr"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tif self._has_slots_:\n\t\t# Read the slots dict.\n\t\ttry:\n\t\t\tresult = self._read_slots_dict_()\n\t\texcept :\n\t\t\treturn None,None\n\t\tdel self._read_slots_dict_\n\t\t# Get the data and the key/values.\n\t\tself._has_slots_ = False\n\t\tself.read() # Force read a new dictionary\n\t\tif result is not None:\n\t\t\tdata = result[\"data\"]\n\t\t\tkey, value = result[\"key\"].items()\n\t\t"}]}
{"task_id": "62b43425903eeb48555d3ea1", "prompt": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\tpass\n\n\tdef _create_prefix_re(self):\n\t\t\"\"\"\n\t\tCreate the prefix regular expression.\n\nReturns the suffix regular expression (:class:`re.Pattern`).\t\t\t\t\"\"\"\n\tpass\n\n\tdef _create_suffix_re(self):\n\t\t\"\"\"\n\t\tCreate the suffix regular expression.\n\nReturns the prefix regular expression (:class:`re.Pattern`)\t\t\t\t\"\"\"\n\tpass\n\n#pragma once\n\n#include <functional>\n#include <stdint.h>\n\n#ifdef _WIN64\n#ifdef UNICODE\n#define"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn self.re.compile('.*?\\W(\\w+)')\n\n\n_match_re = _create_in_regex(_create_in_regex)\n\n\ndef _create_in_str(self, s: str, pattern: Pattern, count: int):\n\t\"\"\"Create the in-style parameter regular expression from string, return original string\n\n\tThe pattern can contain named arguments like \":a:b:c\".  The pattern will always have\n\tall the named arguments, so this must be done from within the ``:='`` operator.\"\"\"\n\tparts = [p."}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\tfrom cStringIO import StringIO\n\n\tpatt_pat = r'\\(.*\\)'\n\tstr_pat = StringIO()\n\tself.pattern.read_re(str_pat)\n\n\tparam_pat = {}\t\t# type: Dict[str, List[str]]\n\tfor param_name in str_pat.keys():\n\t\tvalues = str_pat.get(param_name)\n\t\tparam_pat[param_name] = re.compile(values.replace('\\n', '\\n.*\\Z').replace('\\r', '\\n.*?"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn re.compile(\"(?i)\" + self.pattern_str + \")\", re.VERBOSE)\n\ndef _create_out_regex(self, name: str = \"\") -> Pattern:\n\t\"\"\"Create the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (:class:`re.Pattern`).\n\nArguments:\n\tname:\tName of the output sub-patterns (Default = \"in\").\n\nReturns the sub-pattern (:class:`re.Pattern`). \t\"\"\"\n\treturn re.compile(\"\\\\b\" + name + \"\\\\b\", re.VERBOSE)\n"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\tin_regex = self.expr.re_compile(self.pattern)\n\treturn in_regex\n\n\n_extract_pattern = _extract_expr\n\n\ndef _extract_expr(self):\n\ts = self.expr\n\tif not s:\n\t\treturn None\n\n\texpr = s[0].__class__.__name__\n\tout = getattr(Expression.__getattribute__(s, '__func__'), '__name__')\n\ttry:\n\t\tout = s.pattern.strip()\n\texcept AttributeError:\n\t\texpr = s.__class__.__name"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\tif self.in_regex is None:\n\t\tmatch = \"(?is) *(?P<keyword>%s.)?%s\" % (self._name.lower(), self.__class__.__name__)\n\t\tre_pat = re.compile(match)\n\t\tself.in_regex = re_pat\n\treturn self.in_regex\n\n\t\nclass Parameter:\n\t\"\"\"Parameter class, used by the ``params`` method and ``parlist`` methods.\n\nThe parameter's values, if the parameter is a dictionary of ``Param``-s (such as\nthe"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\tif self.in_pattern is None:\n\t\tself.in_pattern = re.compile(\"\\\\{([^}]+)\\\\}{\\n\")\n\treturn self.in_pattern\n\n\t# Local Variables:\n\t# mode: ignore\n\t# End:\n\n\t# class: re.Pattern.\n\n\t# Methods:\n\t# __setstate__.\n\t# __str__()\n\n\t# Code Sample\n\tfrom qcodes.re_compre import *\n\n\tclass QCodeValidator(ValidatorBase):\n\t\tcode_type: StringType = \"\"\n\n\t"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\tif self._ignore_params:\n\t\treturn None\n\t\t\t\t\t\t\n\tif self._ignore_defaults:\n\t\treturn None\n\t\t\t\t\t\t\n\tpattern = re.compile(r\"^\\s*(?:\\w+=)(.+)\".split(\"*\"))\n\tif self._ignore_defaults:\n\t\tpattern = re.compile(r\"^\\s*(=)(.+)$\")\n\treturn pattern\n\t\ndef _convert_to_str_and_set(self, str, param_type, ignore_defaults = True) -> Optional[Any]:\n\t\"\"\"\n\tGet a parameter"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\tregex = Pattern()\t\t\t\t\t\t\t# type: ignore\n\tregex.name = 'in'\t\t\t\t\t\t\t# type: ignore\n\tregex.flags = 0\t\t\t\t\t\t\t# type: ignore\n\tregex.compile(*self.search_flags)\t\t# type: ignore\n\treturn regex\n\ndef _create_out_regex(self) -> Pattern:\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\nReturns the out-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\tregex = Pattern()\t\t\t\t\t\t# type: ignore\n\tregex.name = 'out'"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\tif not self._in_regex:\n\t\tself._in_regex = re.compile(r\"(?P<in>in)\\Z\") \\\n\t\tor re.compile(r\"(?P<in_name>in|([a-z]))\\Z\") \\\n\t\tor re.compile(r\"(?P<in_type>in|[a-z]))\\Z\", re.IGNORECASE)\n\treturn self._in_regex\n\n\n\tdef _create_out_regex(self) -> Pattern:\t\"\"\"\n\tCreate the out-style parameter regular"}]}
{"task_id": "62b43426903eeb48555d3ea2", "prompt": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\treturn _converting._Converter(self.get('convert_style', 'native'))\n\ndef _create_field_converter(self, field_name: str) -> _converting._Converter:\n\t\"\"\"\n\tCreate a converter, given a :class:`._fields.Field` by field name,\n\twhich can be either ``'real'`` or ``fieldname-number-style``.\n\n\t:param field_name: The field name for which to create a converter.\n\t:type field_name: str\n\t\"\"\"\n\tif field_name == 'longtext':"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\tdef __init__(self, *args, **kwargs):\n\t\tdefaults = {\n\n\t\t\t'type':'string_converter',\t\t\t\t# Type of parameter\n\t\t\t'transient': True,\t\t\t\t\t\t# Should the parameter be transferrable without a user input?\n\t\t}\n\t\tself.defaults = defaults\n\t\tself.optional = False\t\t\t\t\t# True for an optional parameter\n\t\tself.default_value = defaults['type']()\t\t# Type-variable to initialize the Converter, or None, if it defaults to :class:`StrConverter`.\n"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\treturn _converting._Converter()\n\n\nclass ParameterNameValidator(ValidatorBase):\n\t\"\"\"\n\tClass to validate a parameter name.\n\n\tThe base class does nothing with the name that can legally be set.  Subclasses should override\n\t:meth:`validate` accordingly for this parameter name and ``None`` or 'all' in case any extra\n\tvalidation needs to be done.\n\t\"\"\"\n\n\tdef __init__(self, param: param_def.Param) -> None:\n\t\t\"\"\"\n\t\t.. todo:: complete docstring\n\n\t\tParameters\n\t"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\treturn _converting.get_converter('parameter-style', self)\n\ndef _create_default_converter(self, format):\t\t\t\t\t\t\t# <<<<< 'create_default_converter'\n\t\"\"\"\n\tCreate the default converter with given parameters.\n\n\t:param format: :class:`_conversion.ConversionFormat.ParameterStyle`.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\"\"\"\n\treturn _converting.get_converter(\"all\", self, format=format)\n\ndef _create_base_converter(self, format, *args):"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\tif self._converter is None:\n\t\tself._converter = _converting._Converter\n\treturn self._converter\n\ndef _register(self, name, **kwargs):\n\t\"\"\"\n\t@param name:  The parameter name\n\t@param converter:  The parameter converter\n\t\"\"\"\n\tconverter = kwargs.get(_CONVERTING_PARAM, _converting._Converter)\n\ttry:\n\t\tsetattr(_converting, name, converter(**kwargs))\n\texcept:\n\t\tif raise_exception:\n\t\t\traise\n\t\telse:\n\t\t\traise_exception"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\tconverter = super(_ParameterStyle, self)._create_converter()\n\n\tconverter.label = 'Parameter style'\t# Override style\n\tconverter.help = 'Style class to use for parameter labels.'\t# Override style\n\n\tconverter.converters.add('text')\t# Override style\n\tconverter.converters.add('number')\t# Override style\n\tconverter.converters.add('bool')\t# Override style\n\n\treturn converter\n\n\t\nclass ParameterStyle(ParameterStyleBase, _baseclasses.Element):\n\t'''\n\tBase class for all parameters (sub)styles."}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\treturn _converting._Converter(\n\t\tint_params=[\n\t\t\t[('color', ''), ('color', '')],\n\t\t\t[('number', ''), ('color', '')],\n\t\t\t[('angle', ''), ('color', '')]],\n\t\tstring_params=[\n\t\t\t['color', '', [], [], ['color']],\n\t\t\t['', '', [], [''], ['color']],\n\t\t\t['foo', '', ['', ''], [''], ['foo']]\n\t\t])\t#, ('color', '', [], [], ['color']) ), ('number', '', [], [], ['']), ('angle',"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\treturn _converting._converter_new(self, self._parameter_style, type(self))\n\ndef _convert_param(\n\t\tself,\n\t\tstyle: str,\n\t\tdefault_value: Any = None,\n\t\tsource: str = {},\n\t\tto_user: bool = False,\n\t\tuse_converters: Tuple[int, int] = 2\n    ) -> Tuple[bool, Any, Any]:\n\t\"\"\"\n\t:param style: The parameter style.\n\t:param default_value: The default value. Optional, defaults"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\treturn self._try_convert_from_str()\n\ndef _create_converter_to_tuple(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter to tuple converter.\n\nReturns the parameter style converter to tuple converter (:class:`._converting._Converter`).\t\"\"\"\n\treturn self._try_convert_to_tuple()\n\ndef _try_convert_from_str(self, _):\n\tself.__set_converted(tuple())\n\treturn self.to_tuple()\n\ndef _try_convert_to_tuple(self,"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\treturn _converting._Converter(self)\n\ndef _add_to_module_dict(self, m):\t\t\t\t\t\t\"\"\"\n\tAdd in our module dict from our plugin to our global module dict.\n\nReturns the old dict.\n\n**Example:**\n\n>>> m = _converter._get_converter_from_module()\n>>> m.moduleDict = {m.name: m}\n>>> from. import _converter\n>>> m2 = m(_converter, **m.plugin.converterProperties)\n>>> m2.moduleDict.updated(m2)\n>>> del m2"}]}
{"task_id": "62b8966c755ee91dce50a154", "prompt": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 578.00 MiB (GPU 0; 22.16 GiB total capacity; 19.86 GiB already allocated; 427.88 MiB free; 20.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b896de755ee91dce50a183", "prompt": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 712.00 MiB (GPU 0; 22.16 GiB total capacity; 19.73 GiB already allocated; 427.88 MiB free; 20.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b8a4a4755ee91dce50a3d3", "prompt": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tfor i in xrange(0,3):\t# TODO: should we use a 2-tuple?\n\n\t\t# we could do something like\n\t\t# first = self + timedelta(hours = -i)\n\t\t# then we could do\n\t\t# if first < first - timedelta(hours=0), and\n\t\t# then we could do something like\n\t\t# first = self + timedelta(hours = -i-1)\n\n\t\tif dt.tzinfo is None:\n\t\t\treturn dt\n\t\tfirst = dt + timedelta("}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tif not self:\n\t\treturn None\n\n\ttoday = datetime.date.today()\n\tif self.zone is None:\n\t\t# The datetime.tzinfo module has no way to get the daylight saving\n\t\t# time, so we need to get the timezone offset and convert to UTC\n\t\t# time.  In theory this would be done by using the UTC timezone, but\n\t\t# this isn't necessary any more, and there are edge cases where a\n\t\t# timezone-aware datetime is being obtained, so for those we'll just\n\t\t# use the"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t# If this date is already ambiguous, don't bother to look at it.\n\t# It might just be a good idea, but probably not much harm.\n\tif dt.datetime.tzinfo is None or \\\n\t    (dt.isopyear and not dt.isoweek(1)) or \\\n\t    (dt.isocalendar and dt.isoweek(1)):\n\t\treturn d.astimezone(dt)\n\telse:\n\t\treturn dt\n\n\nclass tzical(tzstr):\n\tdef fromutc(self, dt):"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\td = datetime(dt.year, dt.month, dt.day, 0, 0, dt.timezone)\n\tdatetimes = [dt.hour, dt.minute, dt.second, d.tzinfo.utcoffset(dt.tzinfo)]\n\ttry:\n\t\t# It's ambiguous iff the datetime is the first occurence\n\t\treturn dt - datetimes[0]\n\texcept IndexError as e:\n\t\treturn self._ambiguous(dt)\n\ndef fromtz(self, dt):\t\t\"\"\"\n\tGiven a timezone-aware datetime in"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\tresult = tz.tzoffset(self.tzinfo, -timedelta(seconds=dt.timezone))[0]\n\t\tif result < 0:\n\t\t\tresult = self.tzinfo.utcoffset(dt.utcoffset(dt)).replace(second=0)\n\t\treturn result\n\t\n\tdef fromtime(self, time, tz):\t\"\"\"\n\tGiven a UTC-based :class:`datetime.datetime` object, return a timezone-aware\ndatetime that's in that timezone, ignoring the time zone and UTC offset.\n\n:param time:\n   "}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tyear, month, day = dt.timetuple()[:3]\n\t# find the first non-ambiguous weekday:\n\tif abs(day - 6) < self.ambiguous_week:\n\t\tday -= 1\n\t\t# if we've just found days before the last occurence, we know the\n\t\t# time is ambiguous so return a blank datetime if the given\n\t\t# time is before the last occurrence of the weekday.\n\t\tif day < self.ambiguous_weekday:\n\t\t\tdt = dt.replace(hour=0,\n\t\t\t\tminute"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tif not self._ambiguous_utc_datetimes:\n\t\tself._ambiguous_utc_datetimes.append((dt, self._ambiguous_utc_dtime))\n\tif dt.tzinfo == self._ambiguous_utc_dtime.tzinfo:\n            dt = (dt.astimezone(self._ambiguous_utc_tzid) - self._ambiguous_utc_dtime).replace(\n\t\ttzinfo=None)\t\t# We don't know whether this timezone is actually ambiguous.\n\t"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tfrom time import localtime\n\t# The algorithm is taken straight from:\n\t# http://docs.python.org/library/datetime.html#datetime.datetime.utcoffset\n\n\tymdhms = dt - localtime()\n\tif ymdhms <= 0:\t# New day.\n\t\t# Y2K.\n\t\tdt_z = ymdhms + (14 * SECS_PER_YEAR)\n\telif ymdhms > SECS_PER_DAY:\t# Y1K.\n\t\tdt_z = ymdh"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tresult = datetime(dt.year, dt.month, dt.day)\n\tif dt.day > 0 and (isambiguous(dt) or result == dt):\n\t\tresult = result - delta\n\treturn result\n\ndef toutc(self, dt):\t\t# TODO\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, converts\nto a timezone-aware datetime.\n\n:returns:\n\tA timezone-aware datetime, for example :class:`datetime.datetime <datetime>` or\na datetime in the same timezone as the original.\n\nIf the"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t# Check whether there is a timezone ambiguity among our\n\t# timezone-compatible values.\n\tfor tz, utz in self._tzname_map:\n\t\tif dt >= utz._utcoffset:\n\t\t\ttz_dst_name = self._tzname_map[tz]\n\t\telse:\n\t\t\tcontinue\n\n\t\tif utz._has_ambiguous_date(dt):\n\t\t\treturn tz_dst_name\n\n\tutcoffset = dt.utcoffset()\n\tdst = dt.replace(tzinfo=None)\n"}]}
{"task_id": "62b8a7b2755ee91dce50a4a7", "prompt": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 598.00 MiB (GPU 0; 22.16 GiB total capacity; 20.24 GiB already allocated; 427.88 MiB free; 20.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "6305f9991d275c6667163c50", "prompt": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tafter_map = {'b' :b'\\b', 'f' :b'\\f', 'n' :b'\\n', 'r' :b'\\r', 't':b'\\t', 'z' :b'\\xff'}\n\tself.delimiters = before + before_map[after] + after_map[before]\n\ndef get_cut_chars(self) -> bytes: return self.delimiter[:]\n\ndef quote(self, value: str) -> str:\n\t\"\"\"Quoting character.\"\"\"\n\tres = []\n\tfor char in value:\n\t"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tself.cut_chars = before + after\n    \n\t\ndef find_cut_point(self, file, cut_chars: bytes) -> int: \t\"\"\"Find the point to cut a file into after a given sequence of cut-chars.\"\"\"\n\tself.cut_point = -1\n    \n\tfor i in range(len(file)):\n\t\tif file[i] == cut_chars:\n\t\t\t# The first of these characters is a separator.\n\t\t\tself.cut_point = i + 1\n\t\t\tif (self.cut_"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tif self.cut_chars is not None:\n\t\tself.__dict__['cut_chars'] = self.cut_chars\n\tself.cut_chars = before + after\n\n\tself.__dict__['start_row'] = None\n\n// Copyright (c) 2013-2016 The btcsuite developers\n// Copyright (c) 2015-2020 The Decred developers\n// Use of this source code is governed by an ISC\n// license that can be found in the LICENSE file.\n\npackage wire\n\nimport \"reflect\"\n\n// MsgVersion is a wrapper for a version message that"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tif before!= after:\n\t\tsuper(CutSplit, self).cut_chars = before\n\n\nclass Split(CutSplit, CutSet, File):\n\t\"\"\"Split a region into several parts by the same character.\n\tThe method takes an iterator argument, like File.iter_regions.\n\n\tThis is a separate class from CutSet. It allows the user to\n\tset a cut separator such as by splitting the range after the\n\tdelimiter. It also makes it easy to modify the behavior of\n\tIterCutSet to change the splitting behavior when cutting is\n\tused"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tself.__startpos = before\n\tself.__endpos = after\n\n\tdel self.__tmp\n\tself.__tmp = None\n\n@set_cut_chars\n@set_cut_chars_for_file\ndef cut_chars(self, before: bytes, after: bytes) -> None:\n\tdel self.__tmp\n\tself.__tmp = None\n\npackage net.floodlightcontroller.hamradio;\n\n/**\n * Constants and functions for the HAMRadio controller.\n * {@hide}\n */\npublic final class Constants {\n\n    private Constants() {"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tself.before_len = len(before)\n\tself.after_len = len(after)\n\tself.before = before\n\tself.after = after\n\ndef slice_points(self, stream: BytesIO, slice_start: int, slice_stop: int,\n\t\t\t\t offset_start: int, offset_stop: int) -> None:\n\t\"\"\"\n\tGet Slice Points in one pass.\n\n\tArgs:\n\t\tstream: A stream to slice.\n\t\tslice_start: First offset to slice.\n\t\tslice_stop: Last offset to"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tself._before = before\n\tself._after = after\n\n\tself.header = None\n\n\tdel self.file\n\n\tself.set_size()\n\n\treturn\n\n\"\"\"\n\tThis is a class which implements the write routines.\n\tIt allows to write a header to the file.\n\"\"\"\n\nclass SliceWriter:\n\tdef __init__(self, file, format_name, header, offset):\n\t\tself._file = file\n\t\tself._format_name = format_name\n\t\tself._header = header\n\t\tself._offset ="}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tif self._line_delimiter_length is None:\n\t\tself._line_delimiter_length = self._line_delim_char.rfind(self._line_delimiter)\n\t\n\tself._old_lines_to_delete = self._lines_to_delete\n\tself._lines_to_delete = [x for x in range(len(before)+len(after)) if x not in self._old_lines_to_delete]\n\tself._line_delim_char[self._line_delimiter_length:] = after\n\tself.on_changed()\n\n"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tif before is not None:\n\t\tself.before = self.before.split(before, expand=False)\n\tif after is not None:\n\t\tself.after = self.after.split(after, expand=True)\n\nclass Cut:\n\tdef __init__(\n\t\tself,\n\t\tbefore: int,\n\t\tafter: int,\n\t\tbefore_length: int = len(self.before),\n\t\tafter_length: int = len(self.after)\n\t) -> None:\n\t\tself.before = self."}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tself._cut_chars[self._cut_char_state] = (before, after)\n\ndef set_cut_chars(self, before: str, after: str) -> None:\n\t\"\"\"Initialize cut_chars state and append given strings.\"\"\"\n\tself.set_cut_chars(before + after, after + before)\n\nclass SplitBuffer:\n\t\"\"\"A split buffer of bytes.\"\"\"\n\tdef __init__(self, buff, offset):\n\t\tself.buff = buff[offset:offset + self.splitsize]\n\t\tself.state = 0\n\n"}]}
{"task_id": "6306292052e177c0ba469f09", "prompt": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tif request.protocol and request.protocol.lower().startswith(\"dub\"):\n\n\t\tif hasattr(request,\"conversation_id\"):\n\t\t\tif not response_type.is_valid(request.conversation_id):\n\t\t\t\treturn\n\t\t\tif request.request!= \"private\":\n\t\t\t\tif request.payload_header.conversation_id:\n\t\t\t\t\tid_lookup = {}\n\t\t\t\t\tfor p in request.payload_header.conversation_id:\n\t\t\t\t\t\tid_lookup[p] = response_type.id\n\t\t\t\telse:\n\t\t\t\t\tid_lookup ="}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tif(request.payload.lower():\n\t\tif(request.payload == \"api\" or request.payload == \"status\" or request.payload == \"list\"):\n\t\t\tif(request.user.person.first_name.lower() == \"gren\") or (request.user.person.first_name.lower() == \"g\"):\n\t\t\t\treturn True\n\treturn False \n\ndef process(self, message: Message):\n\t\"\"\"\n\tUse self to process incoming messages.\n\t\"\"\"\n\tif not self.process(message):\n\t\treturn False\n\t"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tif self.is_diaspora():\n\t\tif not self.is_authenticated(): return False\n\n\t\trequest_bytes = self.get_request()\n\t\tself.__logger.debug(f\"{self.name} - Identifying: {request_bytes}\")\n\t\tif request_bytes and request_bytes['request'] == b'DADD:PRIVMSG':\n\t\t\tself.__logger.debug(f\"{self.name} - Identifying: PRIVMSG\")\n\t\t\tself.message_handler.__dict__[request.request] = ('dadd',"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\treturn bool(\n\t\t(request['msg_type'] == 'Diaspora' and request['public']) or\n\t\t(request['msg_type'] == 'DiasporaPrivate' and request['payload'])\n\t)\n\n\ndef identify_response(response: ResponseType):\t\"\"\"\n\tTry to identify whether this is a Diaspora private message.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\treturn bool(\n\t\t(response['msg_type'] == 'Diaspora' and"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tdef is_legacy_request(request):\n\t\tif len(request.body.parts) == 2:\n\t\t\tif request.body.string.decode(\"UTF-8\") == \"<DIDSSUBJECT>\":\n\t\t\t\treturn True\t\t\t\t\t# Diaspora\n\n\n# Register the payload type\n\n\ndef register_type():\n\tpayload_types.register(\n\t\tRequest,\n\t\t{\n\t\t\t\"Diaspora.Search\":   (Diaspora, identify_request),\n\t\t\t\"Diaspora.Query\": "}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tfrom requests.structures import CaseInsensitiveDict\n\ttry:\n\t\tif isinstance(request, CaseInsensitiveDict):\n\t\t\trequest = sorted(request.items())\n\t\tresponse = requests.get(request, params=())\n\texcept Exception:\t\t\t\t\t\t\t# pragma: no cover\n\t\tresponse = get_private_payload(request)\n\treturn response\n\n\ndef diaspora_query(query, check_api= False, check_private= False, **keywords):\t\t\t\t\t\t\t\t\t\t\t\t\t\t# pragma: no cover\n\t# Returns a query string of user inputs and returns responses with"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tfrom.common import _get_client_public_message_for_message\n\treturn _get_client_public_message_for_message(request)\n\n// ------------------------------------------------------------------------------\n//  Copyright (c) Microsoft Corporation.  All Rights Reserved.  Licensed under the MIT License.  See License in the project root for license information.\n// ------------------------------------------------------------------------------\n\n// **NOTE** This file was generated by a tool and any changes will be overwritten.\n// <auto-generated/>\n\n// Template Source: EntityRequestBuilder.cs.tt\n\nnamespace Microsoft.Graph\n{\n    using System;"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tmsg: Dict[Any, Any] = request.message  # type: ignore\n\n\tif msg:\n\t\tif is_diaspora_request(msg) and not is_private_request(msg):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n\ndef identify_response(response: ResponseType):\t\"\"\"\n\tIdentify if this is a Diaspora response message.\n\n\tReturn true if this is a request or response and the message is a request or response,\t*/\n\t# otherwise return false.  This should make it"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tif request in [\"request\", \"private\"]:\n\t\treturn \"Diaspora Request\"\n\treturn \"\"\"\n* - Try to identify Diaspora request.\n* - Attempt to identify \"legacy\" payload.\n\"\"\"\n\ndef send_notify(request: RequestType):\n\t\"\"\"\n\tTry to send a broadcaster message to inform a Diaspora user.\n\t\"\"\"\n\ttry:\n\t\tlogger = get_logger(str(request))\n\n\t\t# First, let's identify the request.\n\t\tlogger.info(\"\\n\"+ \"Broadcasting notify"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t#print(request)\n\tif request[\"type\"] == \"public\":\t\treturn True\t\t\t\t\t\t\t\t\t\t# Yes it is!\n\tif request[\"type\"] == \"private\":\treturn False\t\t\t\t\t\t\t\t\t\t# No it is!\n\t#print(request[\"message\"])\n\treturn message.identify_request(request[\"message\"])\t\t\t\t\t\t\t# Get if something went really wrong and ask to update UI\t\t\t\n\n# \tprint(\"request for type {}: {}   from: {}\".format(request[\"type\"],request[\"message\"].params.get(\"name\"),request[\"from\"]))\n\ndef identify_message"}]}
{"task_id": "6306292152e177c0ba469f0d", "prompt": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\trequest_parts = []\n\tif request == \"GET\":\n\t\trequest_parts.append(\"Matrix\")\n\n\tfor part in request_parts:\n\t\tif hasattr(request, part) and hasattr(request, part) and request[part] is not None:\n\t\t\treturn True if part.upper() == \"HTTP\" else False\n\n\treturn False\n\n\ndef identify_request_type(request_type: str) -> Any:\n\t\"\"\"\n\tTry to identify whether this is a Matrix request or an HTTP request\n\t\"\"\"\n\trequest_parts = []"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\tif isinstance(request, ReqTypes.GET): return True\n\tif isinstance(request, ReqTypes.POST): return True\n\tif isinstance(request, ReqTypes.HEAD): return True\n\tif isinstance(request, ReqTypes.OPTIONS): return True\n\traise ValueError(f'Unknown request type: {type(request)}')\n\n\ndef decode_query(query): \t\t\t\"\"\"Decode a Query String\"\"\"\n\ttry:                                   "}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\tm = request.method.upper() + request.path #\n\t# Detect Matrix query requests from /?matrix=client_id&access_token=\n\tif m == 'GET' or m == 'HEAD' or m == 'PUT' or m == 'POST':\t# http://docs.developer.com/en-US/matrix/overview-matrix-overview/matrix_clientoverview#clientidrequest-accesstokenquery\n\t\treturn True\n\telse:\t\t\t\t\t\t\t# other requests like PUT, GET, HEAD and DELETE\n\t\tif m == 'DELETE':\t\t# TODO"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\treturn request == \"Matrix\"\n\n\ndef identify_response(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix response\t\"\"\"\n\treturn request == \"Matrix\" and not request.endswith(\"/\")\n\n\ndef identify_endpoint(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is an Endpoint request\t\"\"\"\n\treturn request == \"Endpoint\"\n\n\ndef identify_header(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is an authorization request\t\"\"\"\n\treturn request == \""}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\treturn request.type == 'identify-req'\n\ndef identify_response(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix response\t\"\"\"\n\treturn not request.type == 'identify-res'\n\ndef convert_json_to_dict(data: Any, field_name: str = 'raw') -> Dict[str, Any]:\n\tdict = {}\n\tif isinstance(data, dict):\n\t\tfor k, v in data.items():\n\t\t\tk = '_' + field_name + k\n\t\t\tv ="}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\treturn request.get_method() in (\"GET\", \"HEAD\", \"OPTIONS\", \"TRACE\")\n\n\ndef generate_headers(state: requests.Response) -> Dict[str, str]:\n\theaders = {k: state.getheaders()[k][0] if state.getheaders()[(k, 0)] is not None else \"\" for k in state.request.headers}\n\treturn {k: v for k, v in headers.items() if v and v.startswith(STATE_VAR)}\n\n\ndef get_header(state: requests.Response, prefix: str) ->"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\tif 'Matrix API' in request.body:\n\t\tif request.get_method() == 'POST':\n\t\t\tcontent_body = request.form['file']  # pylint: disable=no-member\n\t\t\tbody_content = content_body.decode('utf-8')\n\t\t\tif body_content.startswith('data:') or body_content.startswith('file:'):\n\t\t\t\treturn True\n\t\t\treturn False\n\treturn False\n\n\ndef identify_response(response: ResponseType) -> bool:\t\"\"\"\n\tDetermine if the Matrix response is a"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\tmatches = False\n\tfor matrix_dict in all_matrices()[\"content\"]:\n\t\tif isinstance(matrix_dict,dict) and \"id\" in matrix_dict:\n\t\t\tif matrix_dict[\"id\"] == request[\"matrix_id\"]:\n\t\t\t\tmatches = True\n\t\t\t\tbreak\n\tif not matches:\n\t\treturn False\n\theader = generate_headers(request)\n\tresponse = requests.post(\n\t\trequest[\"url\"],\n\t\theaders=header,\n\t)\n\tif \"id\" not in header or \\\n\t\theader[\""}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\trequest.headers.update(headers_user_matrix_headers={\n\t\t\"User-Agent\"\t:\t\"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) \"\n\t\t\t\t\t\t\t\"Chrome/74.0.3729.131 Safari/537.36\"\n\t})\n\treturn len(request.headers) > 0\n\n#include \"gtest/gtest.h\"\n\n#include <functional>\n#include <map>\n#include <string>\n#include <vector>\n#include <memory"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t# I found a lot of badly broken code...\n\t#return request.GET.get(\"token\") == \"\"\n\t#return (not request.GET.get(\"token\") and request.GET.get(\"access_token\"))\n\nclass AccessToken(models.Model):\n\t\"\"\"\n\tRepresents the access token for the  service.\n\tHandles all the business logic common to all the access token services.\"\"\"\n\tid = models.IntegerField(primary_key=True)\n\ttoken = models.CharField(max_length=255, unique=True)\n\taccess_token"}]}
{"task_id": "6306292252e177c0ba469f11", "prompt": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\treturn dt.strftime(\"%b %d, %Y %H:%M\")\n\n\ndef format_delta_dt(dt):\n\t\"\"\" Format a datetime that has a total delta in seconds \"\"\"\n\treturn \"(%d, %Y-%m-%d %H:%M)\" % (dt[0], ddt(dt[1]))\n\ndef format_time(dt):\n\treturn dt.strftime(\"%H:%M:%S %Z\")\n\ndef find_in_subtree(node, subtree):\n\t\"\"\" Return next child node, or None if none.\n\tSkip nodes that"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\tif isinstance(dt, tuple):\n\t\tif len(dt) == 3:\tdt = dt[0] + 'T%df' % dt[1] + 'Z'\n\t\telse: dt = dt[0] + 'T%dZ' % (dt[1] - 1)\n\treturn dt\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\treturn dt.isoformat(sep='T', round=True, use_localtime=True)\n\ndef format_dt_object(dt):\n\t\"\"\"\n\tFormat a datetime object in the way that D* nodes expect.\n\t\"\"\"\n\treturn dt.isoformat(iso=True)\n\ndef format_dt_iso8601(dt, iso_value):\n\t\"\"\"\n\tFormat a datetime if iso_value is None.\n\t\"\"\"\n\tif iso_value is None:\n\t\treturn dt.isoformat()\n\telse:\n\t\treturn dt.iso"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\tfmt = (\n\t\t# The following is equivalent to Format_DateTime.parse and converts\n\t\t# to and from UTC.\n\t\t\"%Y%m%d%H%M%S\",\n\t\t\"%Y%m%d%H%M%b\",\n\t\t\"%Y%m%d%H%M\",\n\t\t# In the following two numbers are also equivalent to Format_DateTime.parse.\n\t\t'%Y%m%d%H',\n\t\t'%Y%m%d',\n\t)\n\t# We can't use dt.strftime"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\treturn datetime.datetime.strptime(str(dt), \"%Y%m%d\") + (1 if dt.tzinfo is None else (dt.tzinfo.utcoffset(dt) / 3600) * 3600)\n\n\nclass Graph(object):\n\n\tdef __init__(self, name):\n\t\tself.name = name\n\t\tself.nodes = {}\n\t\tself.edges = {}\n\n\tdef add_node(self, name, graph):\n\t\tnode = Node(self, name)\n\t\tnode.graph = graph\n\t\tnode"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\tif not datetime_format.match(dt) or time_format not in _format_types:\n\t\treturn dt\n\ttry:\n\t\tdt = dt.datetime()\n\texcept:\n\t\tt = str(dt)\n\t\tif len(t) > 4:\n\t\t\ttry:\n\t\t\t\tdt = datetime(*map(lambda x: int(x),\n\t\t\t\t\t(datetime.datetime.strptime(x, '%d-%b-%Y') for x in t.split('-'))))\n\t\t\t\tif time_format!= 'u':\n\t\t\t\t\tdt"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\tif not dt:\n\t\traise TypeError(\"Argument 2 must be a datetime object\")\n\tif not hasattr(dt, 'year'):\n\t\traise TypeError(dt.replace(microsecond=True))\n\ttry:\n\t\tyear = dt.year\n\t\tmonth = dt.month\n\t\tday = dt.day\n\t\thour = dt.hour \n\t\tmin = dt.minute\n\t\tsec = dt.second\n\t\tyint = dt.year\n\n\t\tif year is not None:\n\t\t\tif not isdate(year):\n\t\t\t\t"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\treturn dt.strftime(\"%d/%b/%Y_%R\")\n\n\ndef datetime_compare(dd, ds, op):\n\t# If one date is after the other,\n\t# then we can assume it is really the same date.\n\t# And if the rest of the dates are not equal,\n\t# then compare the rest of the dates.\n\t#\n\tfromdate = datetime_date(dd, format_dt(ds))\n\ttodate = datetime_date(dd, format_dt(ds+1))\n\t# and the number of days between the two dates"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\ttry:\n\t\treturn dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n\t# Check for dates where we're ignoring microseconds so that the\n\t# datetime.strptime() knows what to do.\n\texcept (ValueError, TypeError):\n\t\treturn dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n\nclass FormatTest(unittest.TestCase):\n\tdef assertEqual(self, first, second):\n\t\t\"\"\"\n\t\tTests that both instances have the same text.\n\t\t\"\"\"\n\t\tif not first == second"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\tif not has_attr(dt,'strftime'):\n\t\tif hasattr(dt,'dt'):\n\t\t\tdt = dt.dt\n\t\tif isinstance(dt, float):\n\t\t\tif dateutil.tz.is_aware(dt):\n\t\t\t\tdt = dt + datetime.timedelta(microseconds=-time.timezone)\n\t\telse:\n\t\t\tdt = dt.replace(tzinfo=dateutil.tz.tzutc())\n\t# Get rid of microseconds.\n\t# This is not precise enough since %s %s is a string and is not a timedelta"}]}
{"task_id": "6306292352e177c0ba469f1d", "prompt": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\ttry:\n\t\tres = set()\n\t\tres2 = res\n\t\twhile text:\n\t\t\tline = text[:-1]\n\t\t\tif line not in RE_TAG_REG.keys():\n\t\t\t\tbreak\n\t\t\trepl_func = RE_TAG_REG[line]\n\t\t\ttext      = RE_CODE_BLOCK.sub('', repl_func(text))\n\t\t\tif repl_func is None: # end of replacer is reached\tor there're no replacers\n\t\t\t\treturn res, text\n\t\tres2 = res"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\tif not text:\n\t\treturn [], ''\n\tif not re.search(r'\\{[\\s\\*]*\\}', text):\n\t\treturn ['{', text], text\t# unbalanced\n\treturn replacer or lambda a: \\\n\t\tre.sub(r'\\{[^{]*\\}', lambda x: '{'.join(x).rsplit('}{', 1)[-1], a) or x\n\ndef detranslate_tags(text: str, transliteration: int = 0,   nouns: bool = True,\n\t\t\t\t\t\t translation_mode:"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\tfrom myscan.config.option_replacer import OptionReplacer\n\t_replacer = OptionReplacer()\n\t_replacer.process_config(text, replacer)\n\t\n\ttagset = set()\n\ti = 0\n\twhile i < len(TEXT_TAGS) and not TEXT_TAGS[i].isalnum():\n\t\ttagset.add(TEXT_TAGS[i])\n\t\ti += 1\n\n\t_replacer.run(text)\n\ttext = _replacer.output\n\treturn tagset, text\n\nnamespace JT1078."}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\tglobal _match_tags\n\tif not _match_tags:\n\t\t__match_tags = default_match_tags\n\n\t\tdef find_tags(text: str: str, replacer: callable = None):\n\t\t\tmatches_before: Set = set()\n\t\t\tmatches_after: Set = set()\n\n\t\t\t\twhile text.split(\"\\n\")[0].strip()!= '' and replacer is not None:\t\t# get tag from replacer\n\t\t\ttag_word = replacer(text)\n\t\t\t#print(tag_word, \""}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\tif text and isinstance(text, str):\n\t\treplacer = replacer or replace_tags\n\n\t# If this function is invoked with \"replacer\", make sure replacement is enabled.\n\ttag_replaces = \"replace\" if replacer else \"ignore\"\n\ttext = re.sub(tag_replaces, replacer, text, flags=re.U)\n\n\treturn text.split(), tag_replaces\n\npackage cn.jiguang.imui.messagelist.core;\n\nimport cn.jiguang.imui.app.notification."}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\tif replacer == None:\n\t\treplacer = lambda x: x\n\treturn reduce(findall, [ text ], [])\n\ndef extract_tags(text: str, tag_filter: Union[str, List[str]], replace_brackets: bool, repl_filter=None) -> Tuple[str, str]:\n\t\"\"\"\n\tReplace tags such as title, text etc.\n\tIf replace_brackets is False (default), will leave everything as tags. Otherwise, if the\n\treplace_brackets parameter is True, will replace brackets, and then the tags.\n\n"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\tfrom tokenize import split\n\tif replacer is not None and text!= None:\t\n\t\t# Split on whitespace, except for comments and blank lines\n\t\tlines = split(text)\n\t\tif replacer in lines:\n\t\t\treturn find_tags(lines, replacer)\n\t\n\tdef add_result(tag_word: str = \"\", replacement: str = \"\"):\n\t\t# For a list of tags, like [ \"title\", \"desc\" ]\n\t\twhile tag_word!= \"\":\n\t\t\tif tag_word in text:\n\t\t\t\t# This is"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\tif replacer is None:\n\t\treplacer = lambda tag, s: re.sub(\"(\\s+|^)\\s+(\" + tag + \")\", \"\", s)\n\treturn _findtags.find_tags(text, replacer)\n\n#pragma once\n\n#include <functional>\n#include <list>\n#include <memory>\n#include <optional>\n#include <string>\n#include <thread>\n#include <unordered_set>\n#include <unordered_map>\n#include <utility>\n#include <vector>\n\n#include \""}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\tresult, original = find_in_text(text, replacer=replacer)\n\treturn result, original\n\ndef find_all(text: str, replacer: callable = None) -> List[Tuple[str,str]]:\t\t\"\"\"\n\tGet all occurrences of all given tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a list of tuple:\n \t(matched words found in the text"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\treturn _find_tags(text, tags_to_ignore=re.compile(replacer), **_options)\n\ndef find_and_replace_words(text: str, replacer: callable = None, **_options) -> str:\n\n\t\"\"\"\n\tFind and replace tags in text.\n\nTries to ignore tags and all occurrences of an occurrance of a word.\n\nOptionally if passed a \"replacer\", will also replace the word with the result\nof the replacer function called with the replaced word.\n\nReturns the text with all replacements applied."}]}
{"task_id": "6306292352e177c0ba469f1e", "prompt": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\tlinkified = re.sub(r'<a [^>]+href=\"(.*)\"[^>]*>(.*?)</a>', r'<a[^>]*>([\\s\\S]*?)</a>',\n\t\t\t\t\t\t\tre.sub(r'<a[^>]*>(.*?)</a>', r\"<a[^>]*>\", text),\n\t\t\t\t\t\t\tre.DOTALL)\n\treturn re.sub(r'<a[^>]*>(.*?)</a>', r'<a[^>]*>\", re.sub(r'(<sup>\\s+.*?)</sup>', r' <sup>(.*?)</sup>', linkified))"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\tparser = TexTextLinks()\n\tparser.links.extend(text)\n\ttext = parser.process('text')\n\treturn text\n\n\n@fixture\ndef example_string():\n\treturn 'A quick brown fox jumps over a lazy'\n\n\n@fixture\ndef example_string2():\n\n\treturn 'A quic fox jumps over a lazy'\n\n\n@fixture\ndef example_string3():\n\n\treturn 'A quick brown fox jumps over a lazy'\n\n\n@fixture\ndef example_string4():\n\n\treturn '"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\tfor item in re.finditer(r'[\\x20-\\xff]',text):\n\t\thref  = unichar_to_unicode(item.group(1))+item.group(2)+item.group(3)\n\t\tif href in link_map:\n\t\t\titem.group(1) = unicode_to_string(urljoin(link_map[href],\"../\"))\n\t\t\t\n\ttext = u'\\r\\n<a name=\"#%s\"><table cellspacing=0 bgcolor=#%s><tr valign=top>'%(unicode_to_string(link"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\tif sys.py3kwarning:\n\t\tfor c in re.findall('\"\"(?i)[ ]*(?P<link>[ ]+)(?P<ref>\\w+|[^\\w+])\"\"\".strip(), text):\n\t\t\ttry:\t# python3 (used without the first'in the string) #\n\t\t\t\tref = re.sub('['+x+'['+y+']'+'['+z+'][ ]*(.*)[ ]*', '\\1[ ]*'+x+'[\\1]+['+' '+y+' ]*\\1', c)\n\t\t\texcept:"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\ttext = _stripHTML(text)\n\ttext = _runLinkify(text)\n\treturn text\n\ndef get_page_break_at_node(node):\n\tpage_break_found = False\n\tfor cmt in node.childNodes:\n\t\tif cmt.nodeName =='span' and cmt.getAttribute('id') == '_pageBreak'):\n\t\t\tif page_break_found:\n\t\t\t\treturn True, node\n\t\t\tif cmt.getAttribute('class')!= 'pageBreak':\n\t\t\t\tcontinue\n\t\t\tpage_break_found ="}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\tdef replace_links(text):\t\t\"\"\"\n\t\tReplace all URLs in text with links.\n\t\tThe whole thing is replaced.\n\t\tReturn text with a more intuitive and more important formatting.\n\t\"\"\"\n\t\tstartTag = \"p\"\n\t\tendTag = \"p\"\n\t\tlinkType = 'text'\n\t\ttext += \"\"\"<a id=\"%s\"%s>\n<a href=\"/wiki/%s\" target=\"main\">Home page</a>\n</a>\"\"\"\n\t\n\t\tfor node in xrange(startTag, endTag"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\treturn add_attributes(add_links(text))\n\ndef add_links(text):\t\t\t\"\"\"\n\tAdd to html or xml output to links in text.  Default link handlers are called.\n\tTo be overridden by subclasses.\n\t\"\"\"\n\tpass\n\ndef process_text(text):\t\t\"\"\"\n\tProcess text in output of markdown, allowing links to be added to it.\n\t\"\"\"\n\tpass\n\ndef add_attributes(text):\n\tpass\n\ndef remove_attributes(text):\n\tpass\n\ndef process_html(html):\t\t\t\t\t#"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\tlinks = []\n\tfor line in text.splitlines():\n\t\tif line.startswith(\"http://\") and \"http://\" not in line:\n\t\t\turl = \"mailto:\"\n\t\t\tif \" \" in line:\n\t\t\t\turl += line.split(\" \")[1]\n\t\t\t# add the whole url to the links\n\t\t\ttry:\n\t\t\t\tt = text.find(\"(\", line1)\n\t\t\t\th = int(t)\n\t\t\t\tif h > 0:\n\t\t\t\t\tlinks.append(\"http://%s\"%url)\n\t\t\t\twhile text."}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t# TODO: Should it put the links where they occur?\n\ttext = replace_links_with_novels(text)\n\n\t# TODO: This should also be handled by ProcessLinksInLinks()\n\treturn text\n\n\ndef replace_links_with_novels(text):\n\ttext = re.sub(r'\\[([^@]+) = [^<>]+', '\\n$1 ', text)\n\ttext = re.sub(r'\\[([^\\n]+\\s*)\\](\\n)', '\\n\\n$1', text)\n\ttext"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\tp=[]\n\twith open(text,encoding='utf-8') as t:\n\t\tfor i in t:\n\t\t\ta=i.strip()\n\t\t\tp.append(a)\n\t\twith open(text) as t:\n\t\t\tfor i in t:\n\t\t\t\tif not i.startswith('![') and not i.startswith('']'):\n\t\t\t\t\tcontinue\n\t\t\t\tline=i.splitlines()\n\t\t\t\ta=line[0]\n\t\t\t\tfor j in line[1:]:\n\t\t\t\t\tf=\"\"\n\t\t\t\t\tif \"=\" in j"}]}
{"task_id": "6306292652e177c0ba469f34", "prompt": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\theaders = {\"User-agent\": \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.5;.NET CLR 1.1.4322;.NET CLR 2.0.50727;.NET CLR 3.0.4506.2152;.NET CLR 3.5.21022;.NET CLR 3.0.4506.2152;.NET CLR 1.0.3705;.NET CLR 2.0.50727; Media Center PC 6.0; InfoPath.3; Zune 4.7103\")\n\tcontent_headers"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\theaders = {\"Accept\": \"*/*\", \"Content-Type\": \"application/x-ndjson\"}\n\treturn requests.head(url, headers=headers).content.decode(\"utf-8\")\n\n\n#------------------------------------------------------------------------------\ndef fetch_html(url: str, fetch_time: float=1.0):\n\t\"\"\"\n\tFetch an HTML document and return a string.\n\t\"\"\"\n\tfrom bs4 import BeautifulSoup\n\thtml = BeautifulSoup(requests.head(url, headers={'user-agent': \"Nitro Client v1."}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\tdata_hash = sha512hex(url)\n\tif os.path.exists(frappe.utils.getremote()):\n\t\tdata_hash = sha512hex(frappe.utils.getremote())\n\tif data_hash in frappe.cache_app.get('remote_data'):\n\t\tret = frappe.cache_app.get('remote_data')\n\t\tif url!= ret:\n\t\t\treturn(None)\n\telif data_hash in frappe.cache_app.get('remote_content_type'):\n\t"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\tstart_time = time.time()\n\tresp = requests.head(url)\n\telapsed = time.time() - start_time\n\tif resp.status_code == 404:\n\t\treturn None\n\tif resp.status_code < 200 or resp.status_code >= 300:\n\t\treturn None\n\treturn resp.headers.get(\"Content-Type\")\n\n\nclass Pico(BaseProvider):\n\t\"\"\"PyPI for a full Pico. \"\"\"\n\n\tdef __init__(self, **kwargs: str):\n\t\t\"\"\"Initialise a new p"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\tr = urllib2.Request(url)\n\tr.add_header(\"User-agent\", \"Mozilla/5.0 (compatible; Aurelius/3.0; +http://www.aurelius.net/\")\n\tr.add_header(\"Accept\", \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\")\n\tr.add_header(\"Accept-Encoding\", \"gzip, deflate\")\n\tr.add_header(\"Content-type\", \"text/html\")\n\tr."}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\tresponse = requests.get(url, headers=fakes.HEADERS)\n\tif response.status_code < 200 or response.status_code >= 300:\n\t\tlogging.error(f'Error fetching remote resource {url}')\n\t\treturn None\n\treturn response.json()['content-type']\n\n\ndef get_remote_resource_with_content_type(url: str) -> Any:\t\t\t\"\"\"\n\tGet the fetchable content from the remote url and return it as a Resource.\n\tIf the url doesn't contain the content type, you can't use that"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\treturn await fetch_url(url, check_certificate=True)\n\n\ndef _head_if_cached(url: str) -> Tuple[Optional[str], Optional[str]]:\n    \"\"\"\n    Extract content-type and cache-control headers to return a cache hit.\n\n    Returns (cache_hits, content_type)\n    \"\"\"\n    if hasattr(_cached_content_type_cache, 'get'](url)) or hasattr(_cached_headers_cache, 'get'](url):\n        return ("}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\trequest = requests.head(url)\n\tif request.status_code == 200:\n\t\tcontent_type = request.get_response_headers().get('Content-Type', '')\n\t\tif content_type == 'application/json':\n\t\t\tlog = log.info\n\t\t\tlog(f'{request.status_code} {request.reason}')\n\t\t\treturn request.content.decode()\n\t\treturn None\n\telif request.status_code == 404:\n\t\tlog = log.warning\n\t\tlog(f'404 Not Found')\n\t\t"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\tr = requests.head(url, allow_redirects=False, headers=HEADERS)\n\tif r.ok:\t\t\t\t\treturn None\t\t\t\t\t\t# pragma: no cover\n\telse:\n\t\treturn str(r.headers[\"content-type\"])\t\t# pragma: no cover\n\n\ndef verify_email(email: str) -> bool:\n\tfrom re import findall\n\n\tpattern = \"(?:.+@[^.].+|[^a-zA-Z0-9_.!+=:-]+\\\\.[a-zA-Z]+)[a-zA-Z0-"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\treturn urlgrab.urlgrabber.grab_content_type(url)\n\n\ndef download_remote_file(urlgrabber, path, remote_path) -> None:\n\t\"\"\"\n\tDownload a remote file from `urlgrabber` to `path` under a directory.\n\t\"\"\"\n\tdownload.download_file(urlgrabber, path, remote_path)\n\n\ndef extract(path: str, remote_path: str = None) -> str:\n\t\"\"\"\n\tExtract `path` onto disk. Returns the extracted content if it is a directory.\n\t"}]}
{"task_id": "6306292a52e177c0ba469f41", "prompt": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\treturn not bool(w2b.gettag(tag))\n\n\ns2b.set_tags(\n\t# [TEST] [TEST]:...\n\t# tagname: english\n\t(u\"test\", (2, 3)),\n\t(u\"<btag2>\", (1, 3)),\n\t(\"test &lt;btag4>\", (2, 3)),\n\t(\"test <btag3> &lt;btag4>\", (1, 2), (\"test &lt;btag3> &lt;btag1> &lt;btag4>\","}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\tflickr_client = FlickrApi(api_key, secret_key)\n\tflickr_tags = flickr_client.flickrs.index_tags(tag)\n\tok = True\n\tfor i in flickr_tags['tags']:\n\t\tstatus = flickr_tags['tags'][i]['status']\n\t\tif i!= 'tag' and status in ['accepted', 'invalid', 'forbidden']:\n\t\t\tok = False\n\treturn ok\n\nimport numpy as np\nimport os\nimport glob\nfrom tqdm import tq"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\ttag = remove_non_word_char_from_text(tag)\n\n\tif tag not in ALLOWED_TAGS:\n\t\treturn False\n\n\tif len(tag) > TAG_LENGTH and tag[-1] == 'W':\n\t\treturn False\n\n\treturn True\n\ndef get_list_of_allowed_tags() -> List[str]:\n\t\"\"\"\n\tGet list of allowed tags a user have.\t\"\"\"\n\ttag = get_user_input_text(VALID_TAGS_TEXT)\n\tif not tag:\n\t\tERROR_MSG"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t__tags__ = set()\n\t__tags__.add(tag)\n\treturn __tags__ == {tag}\n\n\ndef test_tag() -> None:\n\t\"\"\"\n\tTest the tag handling of a tag.\n\t\"\"\"\n\tdata = \"\"\n\tfile_name = \"data.txt\"\n\twords1 = set()\n\twords = set()\n\tresult = \"\"\n\tcount_words = set()\n\tcount = 0\n\n\tfor line in open(input_file_name):\n\t\twords1.add(line.strip().lower())\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\tfrom sentence_transformers import Wav2Vec\n\t# From <https://github.com/google/sentence_transformers/blob/b4d2db8c2c2d0e4d4de95b4ad2ee7fb0a4c39c34c21f5c3d0cf6e89a75ba/tokenization_tests.py#L15>\n\tassert word2vec(tag) in Wav2Vec.__dict__\n\treturn tag in WORD2VEC.keys()\n\ndef test_list_to_words() ->"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\tif not tag:\n\t\treturn False\n\t\t\n\ttag_is_english = tag.islower()\n\ttry:\n\t\tif tag_is_english:\n\t\t\ttag_lower = tag.lower()\n\t\telse:\n\t\t\ttag_lower = tag.strip().lower()\n\texcept:\n\t\treturn False\n\t\t\n\tif tag_is_english and tag_lower == \"\":\n\t\treturn False\n\t\n\tif tag in tag_lower:\n\t\treturn True\n\t\n\treturn False\n\t\t\ndef test_tag_for"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\tif not tag:\n\t\treturn False # TODO:  Should this be False?  (Only in tag_dic?  I mean the tag is not actually there.)\n\n\tdoc = tagger_doc.read_document_file(tag)\n\n\treturn 1\n\nclass TaggerState(enum.Enum):\n\n\t\"\"\"\n\tThe state stored when tags are created in a given file.\t\n\t\"\"\"\n\n\tTAGGING_DOCUMENT = 2\n\n\t\"\"\"\n\tThe state is not changed during edit-ing\n\t\"\"\"\n\n\tTAGGING_WORD = 1\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\twords: List[str] = ['I', 'am', 'an', 'a', 'tag', 'tag', 'i', 'you', 'it', 'it']\n\tfor word in words:\n\t\tif word in res_spectra:\n\t\t\tres = res_spectra[word]\n\t\t\treturn True\n\treturn False\n\ndef test_noun(noun: str) -> bool:\t\"\"\"\n\tTest whether the given word could be the name of a noun. A sentence with the name 'you' is NOT legal but should return true."}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\tif not tag.islower():\n\t\treturn False\n\telif tag.isupper():\n\t\treturn True\n\telse:\n\t\treturn False\n\n<?php\n/*\n * Copyright 2014 Google Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n * use this file except in compliance with the License. You may obtain a copy of\n * the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\tif not tag:\tpass\t# Must know this!\n\telif tag[0] =='':\tpass\t# It's not a valid tag.\n\telif tag[0:len(tag)].isalnum() == True:\t\t\t\t\t\t\t# It's an acronym?\n\t\treturn tag[0]!= '!' and tag[0]!= '-' and tag[0]!= '_'\t\t\t# And if so, it's acceptable.\n\telif tag[:len(tag)].isalnum() == True: pass\n\telif tag[:len(tag)].isalpha() == True:"}]}
{"task_id": "6306298b52e177c0ba469fdc", "prompt": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tresult = []\n\tfor x in node:\tresult.append(dict(xml_children_as_dict(x)))\n\treturn dict( result )\n\ndef xml_nodes_as_dict(node, ignore_attr=False):\n\t\"\"\"Convert the node to a dict of nodes with the same tag as\n\tthe returned dict.\n\t\"\"\"\n\tresult = xml_children_as_dict(node)\n\tif (not ignore_attr) and isinstance(node, Node) and node.tag.find(\"tag\")!= -1:\n\t\tresult[\"%s"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\txml = node\n\tchild = []\n\tif node.xpath('*'):\t\t\t\t# check for comments\n\t\txml.remove(node)\n\t\treturn xml_children_as_dict(xml)\n\n\twhile not node.end():\n\t\tchild.extend(xml_children_as_dict(node+'/text()'))\n\t\tif node.text:\n\t\t\txml.remove(node)\n\treturn child\n\n\ntag_regex = re.compile(r\"([/=\\?]*)=\\\".*?\\\"\") # RFC 3066, sections 7.1"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tres = []\n\tfor elem in node.xpath(\"xml/tag[@name = '\"+xml+\".\"+tagname+\"'][.='\"+tagname+\"']\"):\n\t\tres.append(ele.text.strip())\n\treturn dict(res)\n\"\"\"\n\nxml_children = \"\"\"\n# <html>\n<head/>\n\n# <body>\n<!--\n<a href=\"${url}\">${tag}</a>\n<!--\n\n<p>${tag}</p>\n</body>\n</html>\n\"\"\"\n\nxml_children_xml_element ="}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tn = xml_node(node)\n\tn.tag = \"root\"\n\treturn n.children\n\ndef xml_child_as_dict(node):\t\"\"\"\n\tTurn the child of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tn = xml_node(node)\n\tn.tag = \"node\"\n\treturn n.children\n\ndef xml_attribute_as_string(node):\t\t\t\"\"\"\n\tReturn a string that represents the attribute node.\n"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tif node.tag == \"xml\":\n\t\treturn node.tag, node.ast.text_as_dict()\n\telse:\t\t\n\t\treturn 0, node\n\nclass DOMContentSerializer(HTMLParser):\n\tdef __init__(self, html_node, doctype = 0):\n\t\tHTMLParser.__init__(self, html_node, doctype, 0)\n\t\t\n\tdef _pre_parse_impl(self, parser_mediator, tree, previous_node):\n\t\t# set up the default state of the parser,\n\t\t# but allow a content"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tout = {}\n\tfor child in node.findall('*'):\tout[child.tag] = \\\n\t\t\t\t\txml_children_as_dict(child)\n\treturn out\n\ndef save_xml_file(filename, contents):\t\"\"\"Save XML to file\"\"\"\n\tfh = open(filename, 'wb')\n\tfh.write(contents)\n\tfh.close()\n\ndef save_dict(items): \t\t\t\"\"\"Save an array of dicts, keyed by tag and values\"\"\"\n\treturn list(map(list, set(zip(items, [items"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tout = {}\n\tc_node = node.getroot()\n\tif c_node.tag_name!= 'xml':\n\t\treturn out\n\tfor cnode in c_node.traverse():\n\t\tif isinstance(cnode, xml_nodes.Start) or not cnode.tag_name == 'xml':\n\t\t\tcontinue\n\t\telse:\n\t\t\tout[cnode.tag_name] = xml_children_as_dict(cnode)\n\treturn out\n\nclass xml_nodes::Element(xml_nodes.ElementBase):\n\t\"\"\""}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tndict = {}\n\tfor child in node:\n\t\tndict[child.tag] = xml_children_as_dict(child)\n\treturn ndict\n\ndef settype(ndict, tag, value):\n\tif tag not in ndict:\n\t\tndict[tag] = value\n\telse:\n\t\tndict[tag] = settype(ndict, tag, value)\n\t\ndef removedict(nodes, tag):\n\trdict={}\n\tfor node in nodes:\n\t\tndict=xml_children_as_dict"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tnode_type = get_node_type(node)\n\tif node_type == 'element':\n\t\tchildren = node[-1]\n\t\tif children:\n\t\t\tdict_ = {}\n\t\t\txmlify_children(children, dict_)\n\t\t\treturn dict_\n\t\telse: return None\n\telse: return node.text\n\n\ndef xml_nodes_as_dict(nodes):\n\t\"\"\"\n\tTurn a list of nodes as a dict.\n\t\"\"\"\n\tdict_ = {}\n\n\tfor node in nodes:\n\t\t_node ="}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tif not isinstance(node, Element):\n\t\treturn {}\n\n\tchildren = {}\n\tfor child in node:\n\t\tchildren.update(xml_children_as_dict(child))\n\n\treturn children\n\ndef dict_children_as_element(node):\n\tif not isinstance(node, dict):\n\t\treturn node\n\n\tif not node:\n\t\treturn []\n\n\treturn ElementTree.ElementTree(node).getroot()\n\ndef make_element(tag_name, attributes=None, text=None, comment=None):\n\t"}]}
{"task_id": "6306299052e177c0ba469fe8", "prompt": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tassert sender_handle.to_str() == entity_handle.to_str(), \"Payload sent entity handles don't match expected\"\n\ndef test_payload_with_all_properties(d_args, d_opts, send_raw_message, d_transport, d_client, d_app):\n\td_payload, opts, tclient, tstatemachine, tb = d_opts\n\tassert isinstance(opts, dict), \"opts must be dictionary\"\n\tif d_app:\n\t\tassert isinstance(tclient, Client), \"tclient must be client\"\n"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tif sender_handle!= entity_handle:\n\t\tprint \"Mismatched sender. Got %s expected %s\" %(sender_handle, entity_handle)\n\t\traise EMTSPersonMissingException()\n\n\tif sender_handle == \"\":\n\t\treturn True\n\telif is_person(sender_handle):\n\t\treturn True\n\telse:\n\t\traise\n\t\ndef check_recipient_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle!= entity_handle:\n\t\tprint \"Mismatched recipient."}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\treturn False if sender_handle < entity_handle else False\n\n@check_sender_and_entity_handle_match\ndef _check_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that owner and entity handle matches.\n\nBasically we've already verified that owner and entity handle are what we think they are; however, if we ever need\nto do more checks, we may have to go over and look at it.\n\nNote that we can only verify whether the owner is who they say when they are receiving the payload, since Diaspora is"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\thandletype = sender_handle.get_handletype()\n\tassert handletype == entity_handle.get_handletype( ), 'The sender of'+ str(recipient) +'could not match'+ str(entity_handle) +'but it is' + str(entity_handle) + '\\n\\twith\\n\\t' + str(sender_handle)\n\n\ndef check_timestamp_match(timestamp1, timestamp2):\n\tassert timestamp1 == timestamp2, 'Timestamp 1 mismatch -'+ str(timestamp1) + '. Received: {}'.format(timestamp2"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tsender_handle = int(sender_handle)\n\tentity_handle = int(entity_handle)\n\tif sender_handle!= entity_handle: return False\n\treturn True\n\ndef handle_outgoing_message_with_payload(self, payload):\n\t\"\"\"\n\tHandle a request to post a message.\n\t\"\"\"\n\tself.message_out = self.outgoing_messages.pop(0)\n\tself.outgoing_message = self.outgoing_messages[0]\n\tself.outgoing_messages = self.outgoing_messages[1:]\n\tif self.check"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tif isinstance(entity_handle, str):\n\t\tentity_handle = to_unicode_be_hex(entity_handle)\n\telif not isinstance(entity_handle, str) or not isinstance(entity_handle, unicode):\t# pylint: disable=bad-type-arg\n\t\traise SyntaxError('The entity_handle param must be a string of hex characters.')\n\n\tif isinstance(sender_handle, str):\n\t\tsender_handle = to_unicode_be_hex(sender_handle)\n\telif not isinstance(sender_handle, str) or not isinstance(sender"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\trecipient_handle = get_sender_handle(sender_handle)\n\tsender_handle = get_sender_handle(sender_handle)\n\tif entity_handle == sender_handle:\n\t\treturn True\n\n\tpayload_author_entity = get_sender_handle(recipient_handle)\n\tif str(sender_handle)!= payload_author_entity:\n\t\traise Exception(\"Sender and the object must be the same!\")\n\treturn False\n\ndef filter(filters, entities, author_handle):\n\t\"\"\"\n\tFilters entities to check if they're allowed.\n"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tif sender_handle == entity_handle:\n\t\treturn True\n\ttry:\n\t\tcheck_sender_and_entity_handle_match(sender.get_entity_by_handle(sender_handle), entity_handle)\n\t\treturn True\n\texcept:\n\t\ttry:\n\t\t\tentity_object = entity.get_object_by_handle(entity_handle)\n\t\t\treturn (sender.get_entity_by_handle(sender_handle) == entity_object or sender.get_entity_by_handle(entity_handle) == entity_object"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\treturn entity_handle == sender_handle\n\ndef check_sender_and_target_handle_match(sender_handle, target_handle):\t\"\"\"\n\tEnsure that sender and target handle match.\n\n\t#NOTE This should be called only if entity_handle is a recipient.\n\t#The purpose of this function is to set the target_handle for a recipient as well. The recipient must have\n\t#the recipients rights. So the target_handle is not the sender.\n\n\t#XXX This only helps with non-standard Diaspora payloads.\n\t#Das"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tif entity_handle:\n\t\tmatch = re.search(r\"Diaspora:(\\w+)\", entity_handle.data[0])\n\t\tif not match:\n\t\t\traise BadSignature(f\"Entity '{entity_handle.data[0]}' does not seem to be diaspora\")\n\tif sender_handle:\n\t\tmatch = re.search(r'Diaspora:(\\w+)', sender_handle.data[0])\n\t\tif not match:\n\t\t\traise BadSignature(f\"Author '{sender_handle.data"}]}
{"task_id": "630629b952e177c0ba46a043", "prompt": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\tdocument_path = document_path or None\n\td = {\"url\": url}\n\td.update(api.parse_url(url))\n\td.update(document_path or os.path.dirname(url) or \"download\")\n\n\twith d.setdefault(\"headers\", {}):\n\t\td[\"headers\"].update({\n\t\t\t\"user-agent\": \"CrowdStrike v1.0.5\",\n\t\t})\n\n\treturn Response(\n\t\td,\n\t\tstatus=200,\n\t\tcontent_type=\"application/json\","}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\tresult = {\n\t\t'url': url\n\t}\n\tif document_path:\n\t\tresult['document_path'] = document_path\n\n\trequest = requests.get(url, params=dict(q=urllib.quote(str(result).encode('utf8')), sort='updated'), stream=True)\n\trequest.raise_for_status()\n\tcontent = request.content\n\tinfo = json.loads(content)\n\n\tresult['info'] = info\n\tjsonapi.dump(result, open(get_nodeinfo_report_path(), '"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\tlog.Trace(2, \"get_nodeinfo_well_known_document(%s, %s)\", url, document_path, )\n\twell_known_doc = {}\n\n\ttry:\n\t\twith requests.get(url=url, timeout=1, verify=False, stream=True, proxies=proxies) as f:\n\t\t\tf.encoding = 'utf-8'\n\t\t\tdoc = xml.dom.minidom.parseString(f.text)\n\texcept (RequestError, ConnectionResetError):\n\t\tlog.error(2,\"URL"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\treturn http.get_nodeinfo_well_known_document(url=url, document_path=document_path)\n\n##\n# Download NodeInfo\n\n@http.handle_response()\ndef get_node_info(url, document_paths=None):\n\t\"\"\"Download a NodeInfo document\n\nThis is very similar to 'nodeinfo.download(url)', but it will download full NodeInfo documents.\n\nSee https://nodeinfo.diaspora.software/ for more details.\n\n:arg url: The full NodeInfo.well-known document URL, ie"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\ttry:\treturn nodeinfo_from_url(url)\n\texcept Exception as e:\t\"Failed to use Nodeinfo, could be a failed server, and retry later, but you will likely get a 404\"\n\tif not document_path:\t\traise e\n\treturn { }\n\n@cached\ndef nodeinfo_from_url(url, cache_timeout=600, **_):\n\t\"\"\"\n\tReturn nodeinfo dictionary with metadata about URL.\n\n\t:arg url: The full base url with protocol, ie https://www.example.com/nodeinfo/\n"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\trequest, response = sendRequest(nodeinfo_base_url + url, \"GET\")\n\tstatus = int(response[\"status\"])\n\tif ((status!= 200) and (status!= 204)) or (not document_path):\n\t\traise RequestDataError(response[\"status\"], url, response[\"body\"])\n\n\tdata = response[\"body\"]\n\tif window.isMac:\n\t\tnameNodeInfo = dict(parseJSON(data))\n\telse:\n\t\tnameNodeInfo = data.decode(\"UTF-8\")\n\tnodeInfo = {\n\t\t\""}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\ttry:\n\t\t\tnodeinfo = {}\n\t\t\t# Try to get the NodeInfo API\n\t\t\tif url.startswith(\"http://\") == True:\n\t\t\t\t# Use NodeInfo\n\t\t\t\trequest = Request(url)\n\t\t\t\trequest.add_header('accept-encoding', 'gzip, deflate, br')\n\t\t\t\trequest.add_header('accept-language', 'en-US,en;q=0.5')\n\t\t\t\twith connection.make_urllib() as session:\n\t\t\t\t\trequest.make_response(session)\n\t\t\t\tresponse = session."}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\tif document_path is None:\n\t\tdocument_path = url\n\ttry:\n\t\t# Read the full response\n\t\tresp = requests.get(url, stream=True)\n\t\tif resp.status_code < 400:\n\t\t\tdoc = resp.text\n\t\t\tdoc_elems = doc.split(\"\\n\")\n\t\telse:\n\t\t\t# Document not retrieved, raise HttpNotFound()\n\t\t\t# TODO: add optional response\n\t\t\traise Exception(\n\t\t\t\tu\"HttpGet: {0}, status code: {1}\".format(url,"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\tif document_path == None: document_path = url.split('/')[-1]\n\tfull_url = url.rstrip('/')\n\tdocument_path = document_path or '/home/user/project/'\n\twell_known_node = {\n\t\t'node': None,\n\t\t'well_known_node': 0,\n\t\t'type': 'node'\n\t}\n\tif document_path in well_known_node:\n\t\treturn well_known_node[document_path]\n\tres = b'HTTP/1.1 200 OK\\"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\treturn json.loads(urllib2.urlopen(\n\t\turl=url).read())\n\ndef get_nodeinfo_well_known_html(url, html_path=None, **kwargs):  # pylint: disable=unused-argument\n\t\"\"\"\n\tGenerates a.well-known html page. To save the download to local directory, pass it in via args.\n\nsee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg html_path: Custom NodeInfo"}]}
{"task_id": "630629d052e177c0ba46a0a1", "prompt": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tsignature_xml = doc.xpath(\"//DigitalSignature/SignatureText\")[1]\n\tif signature_xml == None: raise ValueError(\"There is no SignedElement in the XML of signature text.\")\n\tif signature_xml.tag!= \"SignedWithTimestamp\": raise ValueError(\"There is no SignedElement tag in the XML of signature text.\")\n\tsigned_xml = signature_xml.text\n\n\t# verify signature\n\tparsed_signed = doc.xpath(\"//DigitalSignature/SignedInfo/SignedInfoContent/XPathResult\")[1].text.strip()\n\t# ignore the first message\n\tif parsed_"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tnode = Element('verify')\n\tnode += 'xmlns=\"https://www.cocalc.org/'\n\tnode += public_key\n\tnode += '\">'\n\tfor child in doc:\n\t\tchild_hash = hash_(child)\n\t\tif child_hash == 'SHA256withECDSA':\n\t\t\tchild_hash = crypto.generate_key(crypto.ALGORITHM_EC_P256).digest()\n\t\tnode += makeelement('p', ['id', child_hash]) + makeelement('p', ['name', child.tag])\n\t\tnode += make"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tif not xml_checker.check_signed(public_key, doc, signature):\n\t\traise xml_checker.ValidationError(\"Badly signed xml found\")\n\n/*\n * Copyright 1995-2016 The OpenSSL Project Authors. All Rights Reserved.\n *\n * Licensed under the OpenSSL license (the \"License\").  You may not use\n * this file except in compliance with the License.  You can obtain a copy\n * in the file LICENSE in the source distribution or at\n * https://www.openssl.org/source/license.html\n */\n\n#ifndef HEADER_PKCS12ERR_H"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tsigning_algorithm = doc.getElementsByTagName(\"ds:DigestMethod\")[0].getAttribute('Algorithm')\n\tif not public_key in signing_algorithms:\n\t\treturn False\n\tif not doc.getElementsByTagName(\"ds:KeyInfo\")[0].getElementsByTagName(\"ds:X509Data\")[0].getAttribute('X509v3') == signature:\n\t\treturn False\n\tfor element in doc.getElementsByTagName(\"ds:KeyInfo\"):\n\t\tif element.getElementsByTagName(\"ds:KeyInfoKeyVersion\")[0].childNodes!= ():\n\t\t\tfor element in element.getElementsByTagName(\"ds:X509Data\")):\n"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tauthor = \"author\"\n\tfor pubid in doc.xpath(\"./pubid\"):\n\t\t_id = pubid.get(\"id\")\n\t\tif not _id.startswith(\"urn:oasis:names:tc:SAML:1.1:\"):\n\t\t\tcontinue\n\t\t_type = pubid.get(\"type\")\n\t\tif not _type.startswith(\"person\"):\n\t\t\tcontinue\n\t\t_firstname = pubid.get(\"givenName\")\n\t\t_middlename = pubid.get(\"surName\")\n\t\t_lastname = pubid"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tsignatures = doc.getElementsByTagName('Signature')\n\tif not signatures:\n\t\treturn None\n\n\tmessage = None\n\n\tfor i in range(0, len(signatures)):\n\t\tsignature = signatures[i]\n\t\tif signature.getAttribute('Id') == public_key:\n\t\t\tmessage = signature.getElementsByTagName('Message')\n\t\t\tif message and message[0].getAttribute('Key') == public_key:\n\t\t\t\treturn message[0].getAttribute('Value')\n\treturn None\n\n\n# Generate all the signatures for an article, returning true if some were found\n"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tverify_signature_doc_type(doc, SignaturePolicy)\n\tif signature == signature_policy.signature:\n\t\treturn True\n\treturn False\n\n<?php\n\n/**\n * Zend Framework\n *\n * LICENSE\n *\n * This source file is subject to the new BSD license that is bundled\n * with this package in the file LICENSE.txt.\n * It is also available through the world-wide-web at this URL:\n * http://framework.zend.com/license/new-bsd\n * If you did not receive a copy of the license and are unable to\n *"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tmessage_type = \"claim\"\n\txml_elem = doc.root\n\telement = None\n\tfor key_text in map(lambda x: x[0], (public_key.element_list, 'Claim')):\n\t\titem = getattr(xml_elem, key_text)\n\t\tif is_xpath_attr(element, item):\n\t\t\treturn True\n\t\telif isequal(item, signature):\n\t\t\treturn True\n\treturn False\n\ndef is_xpath_attr(element, xpath_item):\n\treturn (element.name,"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tif public_key is None:\n\t\treturn False\n\tverify_doc = ElementTree.fromstring(doc.encode())\n\tverify_doc.set('signed', str(verify_signature(signature, public_key.public_key())))\n\treturn verify_doc.find(sig_attrs) is not None\n\n\ndef verify_signature(signature, claimed_author):\n\t\"\"\"\n\tVerify the signature of the claimed author matches the signature of the\n\tsigned xml. This ensures that in the process the claimed author has no\n\tharmless knowledge (which"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\treturn verify_payload(doc, signature, \"signature\")\n                                                                                                                   "}]}
{"task_id": "630629e052e177c0ba46a0c4", "prompt": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tif document == \"\":\n\t\tlogger.info(\"Unable to parse Diaspora webfinger which is unknown.\")\n\t\treturn\n\n\ttry:\n\t\tdata = json.loads(document)\n\texcept Exception:\n\t\tlogger.error(\"Unable to parse Diaspora webfinger, please retry soon!\")\n\t\treturn\n\n\tdata[\"type_raw\"], data[\"type\"] = parse_diaspora_webfingertype(data[\"type\"])\n\n\treturn data\n\n\ndef get_diaspora_webfinger():\t\t\"\"\""}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tdiaspora_response = parse_diaspora_response(\n\t\tdocument,\n\t\tfields={\n\t\t\t\"webfinger_id\": {\n\t\t\t\t\"title\": \"Application Webfinger ID\",\n\t\t\t\t\"value\": \"Diaspora Application Webfinger ID\",\n\t\t\t\t\"type\": \"text\",\n\t\t\t},\n\t\t\t\"device_id\": {\n\t\t\t\t\"title\": \"Application Device ID\",\n\t\t\t\t\"value\": \"Diaspora Application Device ID\",\n\t\t\t\t\"type\": \"text\",\n\t\t\t},\n\t\t\t"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t# Parse JSON data as a string\n\tdiaspora_webfinger = json.loads(document)\n\n\t# Create the results dictionary in JSON format\n\tif isinstance(diaspora_webfinger, list):\n\t\tdiaspora_webfinger = diaspora_webfinger[0]\n\telse:\n\t\tdiaspora_webfinger = diaspora_webfinger\n\t\n\t# Extract all parts (JSON, XRD, etc)\n\tdiaspora_webfinger_dict = dict()\n\tfor key, val in"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\ttry:\n\t\tfederate = document.split(\".\")[0]\n\t\tdocument = document.split(\".\")[1]\n\texcept:\n\t\tlogger.error(\"Error parsing %s, %s\" % (document, federate))\n\t\treturn {}\n\tret: Dict = {}\n\tret.update(diaspora_federation.parse(document))\n\n\tif federate == \"yahoo\" or federate not in [\"facebook\", \"twitter\", \"kiwi\"]:\n\t\treturn ret\n\n\t# In case of old webfinger then take the"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t#\n\t#\t\tParse the Diaspora WebFinger (XRD) page, parse its details in\n\t#\t\tself.diaspora_webfinger (dict), and add some basic metadata from\n\t#\t\tthis to the current data.  Return a dict with one key: {'title'}\n\t#\t\t\t(string)\n\t#\t\t\t(dict)\n\t#\n\t# Examples:\n\t#\n\t#\t\tself.diaspora_automation -> {\n\t#\t\t\t'display_name': {'type':'string"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tif not document:\n\t\tlog.debug('document is empty')\n\t\treturn (None, None) # invalid Diaspora webfinger\n\tresult = parse_json({\n\t\t'document': document,\n\t\t'url': 'https://diaspora.github.io/diaspora_federation/discovery/webfinger.html',\n\t})\n\tif not result:\n\t\tlog.error('Unable to parse Diaspora webfinger. Error=' + document)\n\t\treturn (None, None)\n\treturn result\n"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tdiaspora_fe = diaspora_federation.parse_federation(XRD(document))\n\n\tparsed_diaspora_fe = {}\n\tparse_diaspora_fe(diaspora_fe)\n\tdiaspora_fe.update({'webfinger': parsed_diaspora_fe[\"webfinger\"]})\n\n\treturn parsed_diaspora_fe\n\n/*\n * Copyright (C) 2015 SoftIndex LLC.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tdiaspora = re.compile(\"^diaspora.com\\:\\d+\", re.M)\t# Matches \"http://d.diaspora.com\"\n\tdiasporaWas = re.compile(\"^diaspora.com\\:\\d+\", re.M)\t# Matches \"http://d.diaspora.com/d/diaspora_ws\"\n\txrd = re.compile(\"^[1-9][0-9]{4}\", re.M)\t\t\t\t\t# Matches \"[xrd]\" which will match all numbers, including zero"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tfederation_document = document.replace(\"&apos;\", \"'\")\n\tdiaspora_federation_document = json.loads(federation_document)\n\tdiaspora_federation_document[\"fed_type\"] = \"webfinger\"\n\n\tif json.dumps({\"federation\": diaspora_federation_document})!= document:\n\t\tparsed_federation = json.loads(document)\n\t\tif not parsed_federation:\n\t\t\tparsed_federation = {}\n\t\tdiaspora_federation_document = dict_serialize"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tif \"application/json\" in document or \"application/vnd.diaspora.api+json\" in document:\n\t\treturn {\"type\": \"new\",...}\n\telse:\n\t\treturn {\"type\": \"old\"}\n\n\n@_load_json_for_diaspora_federation_webfinger\ndef parse_diaspora_federation_webfinger(document: Dict) -> Dict:\t\"\"\"\n\tParse Diaspora federation webfinger\n\n\thttps://diaspora.github.io/diaspora_fed"}]}
{"task_id": "630629e152e177c0ba46a0d1", "prompt": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t# get the WebFinger document for this page using WebFinger API\n\theaders = { \"Referer\": \"https://www.webfinger.fr/wp-content/uploads/wp-content/www/html/html/frontpage.html\", \n\t\t   \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64)\" }\n\tret = requests.get('http://www.webfinger.fr/' + handle + '/document', headers=headers)\n\t# return None if the URL fails to connect\n\tif ret.status_code!= 200: return"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\ttry:\n\t\tresponse = urllib.request.urlopen(handle)\n\texcept Exception as msg:\n\t\tif 'Connection reset by peer' in str(msg):\n\t\t\treturn None\n\t\telse: abort('Failed to retrieve the webfinger document')\n\tif response.getcode()!= 200: abort(errorcode_to_string(response.getcode()))\n\treturn response.read()\ndef try_retrieve_webfinger_document_as_string(handle: str) -> str:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\timport base64\n\ttry:\n\t    try:\n\t\twebfinger = find_webbrowser(handle)\n\t\twebfinger = base64.encodebytes(webfinger)\n\t    except:\n\t\twebfinger = None\n        return webfinger\n\texcept:\n\t    print(f'Failed to find webfinger for handle {handle}')\n\treturn None\n\n\ndef extract_webfinger_from_webfinger_content(webfingered_content: str) -> str:\n    import base64\n"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\ttry:\n\t\twith open(handle,mode=\"r\",encoding=\"utf-8\") as f:\n\t\t\treturn f.read()\n\texcept IOError:\n\t\treturn None\n\n\n\ndef extract_valid_webfinger_documents(handle: str) -> List[str]:\n\t\"\"\"\n\tParse the given document and extract any webfinger documents\n\t\"\"\"\n\tdoc = try_retrieve_webfinger_document( handle )\n\tif None == doc:\n\t\treturn []\n\tresult = [\n\t\tdoc]\n\tfor _handle in open"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\ttry:\n\t\t# Fetch the WebFinger document and parse it \n\t\twebfinger = WebFinger(handle)\n\t\twebfinger.get('doc')\n\texcept:\n\t\treturn None\n\treturn wtf_webfinger.parse_webfinger_document(webfinger.data)\n\n\ndef try_create_webfinger_email(handle: str, subject: str, body: str, options: T.Dict = None) -> str:\n\t\"\"\"\n\tMake sure the data is a valid WebFinger, returns the data if unsuccessful.\n\t\"\"\"\n\t"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\ttry:\n\t\twebfinger = webfinger.Fetch(handle)\n\t\treturn webfinger.Data\n\texcept Exception as e:\n\t\tmsg = traceback.format_exc()\n\t\tlogger.error(\"Fetch failed with message: %s\", msg)\n\t\tlogger.error(\"Stacktrace: %s\", traceback.format_exc())\n\t\tif sys.version_info[0]<3:\n\t\t\timport traceback\n\t\t\tlogger.error(\"Stacktrace: %s\", traceback.format_exc())\n\t\telif sys.version_info[1]<"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t# This function is not used by urllib3.PoolManager\n\tglobal _webfinger_document\n\ttry:\n\t\t_webfinger_document = get_webfinger_document_from_headers(handle, None)\n\t\treturn _webfinger_document\n\texcept IOError:\n\t\tpass\n\treturn None\n\n\ndef try_retrieve_webfinger_data_and_store(filename: str, header: Optional[dict]=None,\n\t\tprogress_func: Optional[IOProgress]=None, filename_type: str=\"default\") -> None:\n\t\"\"\""}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\"\"\"\n\tGetting a webfinger document is the closest you heate at the moment.\n\tThat is, if there is a user with a webfinger and the user-supplied URL\n\twith a valid Webfinger entry then the Webfinger entry will be ignored\n\tso there is no webfinger document available.\n\n\tFunction Description:\n\tTry to find a webfinger document by following an HTTP status code which is\n\twell-formed but invalid by being a plain text.\n\n\tParameters:\n\thandle: The handle for the Webfinger document\n\t\"\"\"\n\ttry:"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\ttry:\n\t\thandle = handle.rstrip('/')\n\t\turl = \"https://www.facebook.com/webfinger/%s?app_id=%s&output_encoding=utf-8&source=json&key=%s\" % (handle, webminger_key, webminger_session_key)\n\t\tresponse = requests.get(url)\n\t\treturn response.json['data']\n\texcept Exception:\n\n\t\treturn None\n\ndef try_retrieve_gists_document(handle: str, filename: str = None) -> None"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\ttry:#pylint: disable=too-many-statements,unused-variable,too-many-locals,too-many-branches\n\t\turllib.request.urlopen(handle,timeout=30)\n\texcept urllib.error.HTTPError as e:\t#pylint: disable=protected-access\n\t\tif isinstance(e, requests.ConnectionError) and e.response.status_code == 404:\n\t\t\treturn None\n\t\tif e.response.status_code!= 200 and not isinstance(e, requests.HTTPError):\n\t\t\tlog.warning(\"%s does"}]}
{"task_id": "630629e152e177c0ba46a0d2", "prompt": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\ttry:\n\t\timport webfinger\n\t\td = {\"remote\": handle, \"webfinger\": webfinger, \"file_extension\": '.diaspora'}\n\t\treturn d\n\texcept ImportError:\n\t\tlog.debug(\"Failed to import diaspora.so library\")\n\t\treturn None\n\ndef retrieve_and_parse_webfinger(handle):\t\t\"\"\"\nRetrieve a webfinger file and parse it.\n\n:arg handle: remote handle to retrieve\n:returns: dict\t\"\"\"\n\ttry:\n\t\timport webfinger\n"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\treturn {\n\t\t'remote_id'             :handle.remoteID,\n\t\t'is_local'              :handle.isLocal,\n\t\t'is_remote'          :handle.isRemote,\n\t\t'is_private'        :handle.isPrivate,\n\t\t'name'                  :handle.name,\n"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\tdata = {'body' : base64.b64encode(b'').decode('utf-8')}\t\t\t\t\t\t\t\t\t# decode returned document, decode body again\n\tresult = parse_diaspora_webfinger( handle, data )\n\treturn result\n\n\ndef query_diaspora_webfinger_service(handle):\n\t\"\"\"\n\tExecute a WebFinger Service query against the given Diaspora webfinger.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\tservice_data = {'name' : 'DiAspora WebFinger"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\tresponse = None\n\ttry:\n\t\tresponse = grequests.post(\n\t\t\t'https://driveapi.diaspora.com/api/json', {\"document\": handle}\n\t\t)\n\t\traise DiasporaParsingError(response.json())\n\texcept (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeoutError):\n\t\traise DiasporaParsingError(\n\t\t\t\"Timeout waiting for file download (\"+str(response.json())+\")\")\n\texcept requests.exceptions.HTTPError as handle_error:\n\t\tif"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t# retrieve the remote Diaspora document from the WAMP web\n\ttry:\n\t\tr=requests.get(\n\t\t\t\tf\"http://{handle[0]:s}/diaspora/doc.html\",\n\t\t\t\ttimeout=10\n\t\t\t\t)\n\texcept:\n\t\tr=dict()\n\t\tr['wamp_error']='Diaspora is not reachable, try again later.'\n\t\treturn r\n\tdoc = NSA().parse(r.content)\n\t# Retrieve remote and parsed Diaspora webfinger\n "}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\turl = \"{}/remote?token={}\"\n\ttry:\n\t\treq = requests.get(url.format(settings.DIASPORA_URL_BASE, handle))\n\t\tif req.status_code!= requests.codes.ok:\n\t\t\traise Exception(\"Could not get a remote document: {}\".format(req.url))\n\t\treturn req.json()\n\texcept requests.exceptions.ConnectionError as e:\n\t\traise Exception(\"Could not connect to Diaspora: {} error: {}\".format(e.hostname, e.errno))\n\texcept requests"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\treturn diaspora_web_finger.get_data(handle, '')\n\n\tdef retrieve_and_parse_diaspora_webfinger_api(handle):\t\"\"\"\n\t\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\treturn diaspora_web_finger.get_data(handle, '')\n\n\n\tdef retrieve_and_parse_diaspora_download_document(handle):\t\"\"\"\n\t\tRetrieve a and parse"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\ttry:\n\t\twebfinger_response = get_xml_data(handle, \"remote\")\n\texcept:\n\t\treturn {}\n\treturn parse_webfinger_response(webfinger_response, handle)\n\ndef parse_xml_webfinger_response(webfinger_response, handle):\n\tdoc = webfinger_response.find(\"document\")\n\tif doc == None:\n\t\tapp.logger.error(\"Webfinger does not contain any response from Rolf-Girl.\")\n\t\treturn {}\n\n\tdef xml_name(name):\n\t\t"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\tr = requests.get('http://' + os.path.dirname(handle) + '/diaspora.json')\n\td = json.loads(r.text)\n\t# Make sure there is a key in the doc\n\tifd = re.findall('(' + 'key=\"(.+?)\"', d['fields'].pop(0))\n\tif len(ifd)==0:\n\t\tprint(\"Received invalid or missing JSON data: \" + str(r.text))\n\telse:\n\t\treturn ifd\n\n# Retrieve the remote Diaspora we"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\tres = get_api_call(\n\t\t\"diaspora.webfinger\",\n\t\t[\n\t\t\t\"ids\",\n\t\t\thandle,\n\t\t])\n\tif res[\"@odata.error\"][\"message\"]!= \"null\":\n\t\treturn res\n\td = {\"text\": res[\"text\"][\"text\"]}\n\treturn d\n\ndef retrieve_and_parse_diaspora_device_id(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora device id document.\n\n:arg handle: Remote handle to retrieve\n:"}]}
{"task_id": "630629e252e177c0ba46a0d6", "prompt": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn s_utils.make_inst(host, diaspora_host, host_meta.json)\n\n\ndef retrieve_diaspora_host_meta_json(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``json`` instance\n\t\"\"\"\n\treturn diaspora_host_meta.json[host]\n\n\ndef host_meta_from_doc(host, name, version, host_meta):\n\t\"\"\"\n\tCreate or update a"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\tdef query(host, method, path, params=[]):\n\t\t\"\"\"Perform a given XRD query on the specified host.\"\"\"\n\t\turl = _to_str(host, params, 'url=','  ')\n\t\tfor path in path:\n\t\t\turl += _to_str(path, params,'  -- ',' ')\n\t\tprint url\n\t\tr = None\n\t\ttry: r = requests.post(url + '', params,\n\t\t\t\tverify=host.verify_ssl,\n\t\t\t\theaders={})\n\t\texcept requests."}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn _xd.make_host_meta(host)\n\n//////////////////////////////////////////////////////////////////////////////\n//\n// License Agreement:\n//\n// The following are Copyright  2007, Casey Langen\n//\n// Sources and Binaries of: win32cpp\n//\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are met:\n//\n//    * Redistributions of source code must retain the above copyright notice,\n//      this list of conditions and the following disclaimer.\n//"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\tfrom xonsh.util import find_daspora_path\n\turl_root = find_daspora_path(host)\n\tres      = None\n\tif url_root is not None:\n\t\ttry:\n\t\t\tres = D.fetch_remote_metadata_xml(d=url_root)\n\t\texcept NoXRRR:\n\t\t\traise  NoXRRR(f'No Metadata Document {host.hostname} at {host.address}')\n\tif res is None:\n\t\traise Diaspora"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\ttry:\n\t\tfor _diaspora_host in Diaspora.hosts():\n\t\t\t_diaspora_hosts = _diaspora_host.hosts\n\t\t\tif host not in _diaspora_hosts:\n\t\t\t\tcontinue\n\t\t\ttry:\n\t\t\t\treturn dIAPost(_diaspora_hosts[host].document())\t\n\t\t\texcept:\n\t\t\t\t# The Diaspora servers do respond to an\n\t\t\t\t# \"internal server error\", so it can't\n\t\t\t\t# be considered a critical server error."}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn XRD(d=\"http://127.0.0.1:8080/ws/v2\",\n\t\tt=dict(_XRD=dict()))\n\n\n\ndef retrieve_diaspora_server_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora server-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn XRD(d=\"http://127.0.0.1:8080/ws/v2\",\n\t\tt=dict(_XRD=dict()))\n"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn XRD(host.remote_doc.metadata.get(_DIASPORA_HOST_META_KEY, b'\"))\n\ndef retrieve_diaspora_host_doc(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-doc document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn XRD(host.remote_doc.get(_DIASPORA_HOST_DOCS_KEY, b''))\n\ndef retrieve_diaspora_host_doc(host):\t\"\"\""}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t__xrd = remote.get(host)\n\tif not __xrd:\n\t\treturn\n\n\t__hst = remote.get_meta(__xrd)\n\t__xrd = __hst.d[0]\n\treturn __xrd\n\n@diaspora.route('/', methods=['POST'])\n@diaspora.process(remote=True, on=reactor)\ndef add_diaspora_request(host, text):\n\t\"\"\"Adds an Diaspora server request.\n\n\t:arg host: Diaspora"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn DIASPORA(host)\n\nimport React from'react';\n\nimport {\n  getStyleFromColor,\n  createAnimated,\n  useAnimatedStyle,\n  useAnimatedStyleOnHover,\n  render,\n  Animated,\n  Easing,\n} from '../../helpers';\nimport { ColorProps } from '../types';\n\nconst Color = ({ color,...rest }: ColorProps) => {\n  const { value, interpolate } = useAnimatedStyle(rest);\n  const style = useAnimatedStyleOnHover(rest);\n\n  const changeTransform = ("}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn XRD(host, api_path)\n\ndef parse_diaspora_host_meta(doc) -> \"XRD\":\n\t\"\"\"\n\tParse and return a remote Diaspora host-meta document.\n\n\tThe ``host-meta`` entry in the ``host-meta.txt`` or\n\t``diaspora-host-meta.txt`` document is a series of lines\n\tseperated by ``\\t``. The first line is the hostname the\n\tremote Diaspora server is running in. The second line\n\tis the name of"}]}
{"task_id": "630629e752e177c0ba46a0fb", "prompt": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t# Ensure params contain all required fields\n\tparams = validate_params(url, data, timeout, method)\n\n\t# Send the request\n\treturn m.send(_send(url, data, timeout, method), *params, **kwargs)\n\n\n\ndef _send(url, data, timeout, method=\"POST\", *args, **kwargs):\n\t\"\"\"Helper method to send a document via the request method\n\n\tParameters:\n\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\tstatus_code, content, status = send_request('POST', url, data, timeout=timeout, *args, **kwargs)\n\tif status_code is False:\n\t\treturn False, status.args[1]\n\telse:\n\t\treturn status_code, status\n\n\ndef send_file(url, data, timeout=10, method=\"post\"):\t# pragma: no cover\n\t\"\"\"\n\tHelper method to send a file via POST. No support for'mode' as the second parameter for ``send_file``.\n\tAdditional ``*args`` and ``**"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\tprint(\"Send file via \" + url)\n\n\tdef send():\n\t\t#print(\"Sending file via \" + url + \" via HTTP connection...\")\n\t\t# Send request\n\t\tif isinstance(url, string_classes):\n\t\t\t#print(\"url is \" + url)\n\t\t\trequest = requests.Request(method=\"GET\", url=url)\n\t\t\trequest.headers[\"content-type\"] = \"application/octet-stream\"\n\t\t\t# Print headers\n\t\t\tfor key, value in kwargs.iteritems():\n\t\t\t\trequest.headers.append((key"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\tif method == \"get\":\n\t\tif timeout == 0:\n\t\t\t# Make empty args (except keyword args)\n\t\t\tkwargs.setdefault(\"file\", io.BytesIO())\n\t\t\tkwargs.setdefault(\"headers\", None)\n\n\t\tr = requests.get(url, params=data, timeout=timeout, **kwargs)\n\telse:\n\t\tr = requests.post(url, data=data, timeout=timeout, **kwargs)\n\n\t# Return tuple (status_code, raw http response)\n\treturn convert_status_code(r.status_"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\treturn  requests.post(url, data=json.dumps(data), headers=headers, timeout=timeout, content_type=\"application/json\")\n\ndef send_file(url, data, timeout=10, method=\"get\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a file via GET with automatic content encoding.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.get``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\treturn requests.post(url=url, data=data, timeout=timeout, verify=False, params=kwargs)\n\n\ndef send_string(url, *args, **kwargs):\n\t\"\"\"\n\tHelper method to send a string via GET.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.get``.\n\n:arg url: Full url to send to, including protocol\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to get\n:returns: Tuple of status code (int"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\tif data!= None:\n\t\tjson_data = json.dumps(data, default=str)\n\telif hasattr(data, '_json'):\n\t\tjson_data = str(data)\n\telif issubclass(type(data), Exception):\n\t\treturn data, None, None\n\telse:\n\t\tjson_data = data.__json__\n\t\n\treturn self.send({\n\t\t\"url\": url,\n\t\t\"method\": method,\n\t\t\"headers\": {\n\t\t\t\"Content-Type\": json_data\n\t\t}\n\t})."}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\tkwargs = parse_kwargs(kwargs)\n\tif kwargs.get(\"content\", None) is None:\n\t\tdata = None\n\tif not isinstance(url, bytes):\n\t\turl = url.decode(\"utf-8\")\n\telif url[-1] == \"/\":\n\t\turl = url[:-1]\n\tlogger.debug(\"send_document with %s\", url)\n\n\tclient = get_redis_client()\n\tassert isinstance(client, redis.Redis)\n\tr = client.lrange(key, 0, -1)\n\n\tif len"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\treturn requests.post(url, data=data, timeout=timeout, headers=kwarg, stream=True, verify=kwarg.verify, allow_redirects=False, **kwargs)\n\n\n#def get_cookie_list(url):\n#\t\"\"\"Helper method to retrieve cookies for a URL. \"\"\"\n#\tcookies_dict = {\n#\t\t\"access_token\": token.get_access_token_cookie(session, url),\n#\t}\n#\treturn cookies_dict\n\npackage org.zstack.sdk;\n\n\n\npublic class UpdateZ"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\tkwargs = { \"allow_redirects\": True, \"data\": data, \"timeout\": timeout}\n\tif not isinstance(url, basestring):\n\t\turl = url.format(protocol=settings.FILE_PROTOCOL.get())\n\treturn _send_document(url, data, timeout, method, *args, **kwargs)\n\n\ndef send_file(filename, data, *args, **kwargs):\n\t\"\"\"\n\tHelper method to send a file via POST\n\n\t:arg filename: Filename of file to send to\n\t:arg data: Dictionary (will be form"}]}
{"task_id": "6306091073426c38ae68acac", "prompt": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tresult = dic.setdefault(val,{})\n\tkey_chain = []\n\tfor key in keys:\n\t\tkey_chain.append((key,dic[key]))\n\t\tresult[key] = {}\n\t\tlist_keys = key_chain[0]\n\t\tlist_keys = [list_keys[i] for i in range(len(list_keys))]\n\t\tfor v in self.get_from_any_list(dict_find(dict_find(dic,key_chain),\n\t\t\t\t\t\t\tkey_chain, list_keys)):\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tn = dict.get('_', {})\n\tdef f(k):\n\t\tif isinstance(k, basestring):\n\t\t\tk = k.split('.')\n\t\tfor n in k:\n\t\t\tif isinstance(n, basestring):\n\t\t\t\tn = n.split('.')\n\t\t\tif n[0] == key:\n\t\t\t\tk2 = dict.get(\"_\", {})\n\t\t\t\tk2[n[1]] = val\n\t\t\telif n == key.split(\".\"):\n\t\t\t\treturn\n\t\t\telse:\n\t\t\t\tf(n)\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tif key not in keys: #the list of keys could contain multiple keys\n\t\tkeys.append(key)\n\telse:\n\t\tkeys[0] = key\n\treturn dic.insert(val, *keys)\n\ndef dict_insert_item(dic, key, value, *keys):\n\t\"\"\"\n\tinsert an item into the given dictionary\n\n:param dic: a dictionary object to insert the item into\n:param key: item's key\n:param value: value of the item to be inserted\n:param keys: sub keys in the keys chain\t\"\"\""}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t#print(dic, val, key, *keys, file=sys.stderr)\n\traise NotImplementedError()\n\ndef dict_pop(dic, key, *keys):\n\traise NotImplementedError()\n\ndef dict_popitem(dic, key):\t\t\t\t\"\"\"dict_popitem() removes a specified item in a dictionary, if it exists.\n\nto get the value of the key, should it exist in the dictionary\n\nraise KeyError: if the dictionary doesn't contains any entry for the given key\"\"\"\n\t#print('dict_pop', key)\n\traise NotImplementedError()\n\ndef dict"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\ttry:\n\t\tkey = [key, key]\n\t\ttry:\n\t\t\treturn dic[key[0]] or dic[val] or \\\n\t\t\t\tdic[''].insert(key, val)\n\t\texcept KeyError:\n\t\t\treturn NotImplemented\n\t\tpass\n\texcept TypeError:\n\t\treturn NotImplemented\n\ndef dict_get(dic, key, *keys):\n\t\"\"\"Returns the given value (possibly nested) from a dictionary.\n\tThe given key could be a recursive sub-key. If not given,\n\tit defaults to the whole the key"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tif dic == None or len(dic) == 0:\n\t\tdel dic\n\t\treturn val\n\telse:\n\t\treturn dic.setdefault(val, {})\n\n\ndef dict_get(dic, val, key, *keys):\n\t\"\"\"\n\treturns the value stored under the given key, or the default if no key\n\texists under the given key. If given as a class type this will attempt \n\tto convert the value to the instance type of the dictionary. If the \n\tdictionary does not contain the key, it will return the default\n\tval ("}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tfor k in keys:\n\t\tval = dic.get(k)\n\t\tif val:\n\t\t\tif not dic.has_key(key): dic[key] = val\n\t\t\telse:\n\t\t\t\titem\t= val\n\t\t\t\t\n\t\t\t\tif item['_id'] == '_root':\n\t\t\t\t\titem['_id']\t= dic['_root_' + key]\n\t\t\t\titem['_key']\t= key\n\t\t\t\t\n\t\t\t\tdic[key]\t= item\n\t\telse:\n\t\t\tif dic['_root_' + key].decode('utf8"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t# FIXME\n\t# check if value is a dict and if it is not empty\n\t# raise error if all values in the dict are lists\n\t#  raise error if  the list is not a dict\n\t#  raise error if the index  of  list  is  below\n\t#  raise error if   it is not an  index\n\t#  raise error if the value of  the  index  cannot be  used  for  insertion  between the value and the   index\n\t# raise error if   either  value or values are not given\n\n\t#"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tif key in dic:\n\t\tfor key in keys:\n\t\t\tdic[key].append(val)\n\telse:\n\t\tdic[key] = [val]\nfrom collections import OrderedDict\n\ndef dict_remove(dic, key, *keys):\t\"\"\"\n\tremove a value of a nested key from a dictionary\n\nto remove value for a key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_remove({}, 'key1.key2'.split('.'.'))\n\n:param dic: a dictionary object to insert the nested"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tfor a in keys:\n\t\tdic.setdefault(a, [])\n\tdic[key] = [val]\n\n\n################################################################################\ndef dict_remove(dic, key, *keys):\t\"\"\"\n\terase a value from a dictionary, all keys in the dictionary should be\ngiven as method's arguments\n\nto erase the values from the given dictionary, all key should be given as\nmethod's arguments\n\nexample:\n  dict_remove({}, 'val', 'key1.key2'.split('.'))\n\nto erase the key value pairs from the subkey of the key\n"}]}
{"task_id": "6306091a73426c38ae68acc8", "prompt": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t", "suggestions": [{"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\treturn IniFormat(\n\t\tdata_dir=settings_dirs[settings_dirs['custom']] if spec_option =='sintel_data_dir' \\\n\t\t\telse settings_dirs['custom_dir'],\n\t\toutput_dir=settings_dirs[settings_dirs['output']],\n\t\ttitle=settings_dirs[settings_dirs['title']] if spec_option =='sintel_title' else None,\n\t\tsub_title=settings_dirs[settings_dirs['sub_title']] if spec_option =='sintel_sub_title' \\"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tind_type = IniType.new_struct(\"list_of_file_names\", {\"settings_dirs\": settings_dirs,}, spec_option)\n\treturn ind_type\n\nsettings_dirs = []\noptions = spec_option\n\nfor subdir in os.listdir(os.path.join(os.path.dirname(__file__), \"settings\")):\n\tsubsubdir = os.path.join(subdir, \"settings\")\n\tif os.path.isfile(subsubdir + '/_settings.py'):\tcontinue\t\t# file is not a settings file"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\ttypes = { \"File\",\n\t\t\"Module(s)\",\n\t\t\"File(s) Specified\"\n\t\t\"Inherits from File(s) Specified\"\n\t\t\"Inherits from File(s) (Recommended)\"\t# see also Inherits\n\t\t\t\t\t\t\t\t\t\t\t\t\t# to file(s)\n\t\t\"Override(s) Specified\"\n\t\t\"Inherits from Override(s) Specified\"\n\t\t\"Inherits from Override(s) (Recommended)\"\t# see also Inherits\n\t\t\t\t\t\t\t\t\t\t\t\t\t# to file(s)\n\t"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tfor i in range(len(settings_dirs)):\n\t\troot_path = settings_dirs[i]\n\t\tif os.path.isfile(root_path + \"IniType\" + spec_option):\treturn new\n\tnew_type = type(\"IniType\", (\n\t\tnew,\n\t\tFileConfig,\n\t\tFileSystem,\n\t\tDirectory,\n\t\tdict(\n\t\t\tsource\t\t\t= FileSource(root_path + \"IniType\" + spec_option),\n\t\t\ttarget\t\t\t= FileTarget(root_path + \"Ini"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\treturn Create(types.ListType.type, settings_dirs, spec_option)\n\n\n@spec.name('file', type=list_of_paths)\ndef list_of_paths(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\treturn Create(types.StructType.type, settings_dirs, spec_option)\n\n\n@spec.name('directory', type=list_of_paths)\ndef directory_of_paths(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tsettings = settings_dirs[spec_option]\n\tif settings == None : settings = {}\n\tnew_type = type('GenericTest', (IniType, settings), {})\n\tnew_type._name = spec_option\n\treturn new_type\n\n\ndef is_class_named(settings_dirs, spec_option, class_name):\n\texists = (settings_dirs[spec_option] == class_name)\n\tif exists and not os.path.exists(class_name):\n\t\tprint('   Cannot find class file {0}'.format("}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tbase_name = [x for x in spec_option if x in spec_option.split(os.pathsep)]\n\ttype = type(base_name)\n\tt_real = type('real')\n\tt_integer = type('integer')\n\tt_numeric = type('numeric')\n\tt_choice = type('choice')\n\t# real\n\tsettings_file_name = os.path.join(*settings_dirs,'system/types/settings.ini')\n\twith open(settings_file_name, 'rb') as file:\n\t\tdata"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\toptions = IniType.options()\n\toptions['name'] = spec_option['name']\n\toptions['source'] = None\n\toptions['description'] = spec_option['description']\n\toptions['default_value'] = None\n\tif 'values' in spec_option:\n\t\tvalues = spec_option['values']\n\t\tif Type == 'File':\n\t\t\toptions['default_value'] = values\n\t\telif 'default_value' in values:\n\t\t\toptions['default'] = values['default_value']\n\n\treturn IniType("}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tfrom xfel.merging.models import get_model\n\n\tif spec_option == None or spec_option == \"default\":\n\t\tinput_name = spec_option\n\telse:\n\t\tinput_name = spec_option[\"input_names\"]\n\tinput_type = spec_option[\"type\"]\n\n\tfor input_dir in settings_dirs[input_type]:\n\t\t# Load the models\n\t\tfor model in get_model(input_dir, settings_defaults=[input_dir]):\n\t\t\t# Apply a template\n\t\t\tif settings_"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tfrom jcvi.formats.base import list_of_file_names, list_of_filenames\n\n\tout_str = []\n\tfor ini_type, filename_lst in sorted(list_of_filenames(settings_dirs)):\n\t\tfilename_lst[0] = filename_lst[0].replace('<\",\"\\\\u00c0\\\\u00b0\")\n\t\tprint \"\"\n\t\tprint \"### Writing header of %s\" % filename_lst[0]\n\n\t\t# header\n\t\tdef header_file_names(file_names,"}]}
{"task_id": "6306091b73426c38ae68acd7", "prompt": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\tfrom ansible.module_utils.ansible_environment import ansible_version\n\n\tconfig = cls.get_ansible_module_config(module = __name__)\n\tconfig.set_ansible_module_vars(vars())\n\tconfig.read()\n\tif ansible_version == '1.9':\n\t\tconfig.add_setting('ansible_config_manager', 'true', dict(env = 'Ansible', module = __name__))\n\treturn config\n\ndef get_ansible_module_config(module):\t\"\"\"Gets the ans"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\treturn get_instance_by_cls(cls)\n\n\ndef ansible_get_instance_by_cmd(cmd_name):\n\t\"\"\" Returns the instance from the ansible_config_manager, or NULL if it is not defined \"\"\"\n\tif cmd_name not in set([_ansible_shell, _ansible_config_manager, None]):\n\t\treturn None\n\tinst = ansible_config_manager(cls=cmd_name)\n\tif inst is not None:\n\t\treturn inst\n\tcmd_obj = import_item(cmd_name)"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\tif cls.config_manager_args is None:\n\t\tcls.config_manager_args = {}\n\n\treturn cls.get_config()\n\nclass ansible_config_manager_args(object):\n\t\"\"\"\n\tGets the possible args for a ansible's Configuration Manager.\n\t\"\"\"\n\tclass CommandError(UserError):\n\t\tpass\n\n\t@time_counter()\n\tdef __init__(self, *args, **kwargs):\n\t\tself.args = args\n\t\tself.config = kwargs.get('config')\n\t\tself."}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\treturn get_global_app_manager()[0]\n\n# -----------------------------------------------------------------------------\ndef get_app_config(cls, app_config_identifier=None):\t# pylint: disable=unused-argument\n\t\"\"\"\n\tGets the config for an app given a identifier.\n\n\t:param cls: The class for which you want the config\n\t:param app_name: The name of the app to get the config for. e.g. 'flask_admin'\n\t:param app_config_identifier: A path to the config for an specific app name. Defaults to None.\n\t"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\treturn get_config_manager(cls)\n\n\ndef _get_module_names(cls):\n\t\"\"\"Gets module names.\"\"\"\n\tmodule_name = cls.__name__\n\ttry:\n\t\treturn (\n\t\t\tcls._module.__code__.co_filename\n\t\t) if hasattr(cls._module, \"__code__\") else ()\n\texcept (AttributeError, ImportError):\n\t\tmodule_name += \".py\"\n\treturn module_name\n\n\n@contextmanager\ndef _module(module_name, module_version):\n\t\"\"\"\n\tGets the"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\treturn cls.default_manager.ansible_config_manager\n\ndef ansible_get_variable_manager(config_manager, name):\t\"\"\"\n\tGets the ansible variable manager.  If you want to use the `ansible_variables` global variable or add new variables using\n\tthis function you should create the class.  Otherwise the returned object or the default\n\tmanager of the global variable will be returned.\n\t\"\"\"\n\treturn config_manager.variable_manager\n\n# The default implementation of variable manager using ansible. \n#\n#  This function is not designed"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\treturn cls._config.ConfigManager\n\n\n# get a string that uniquely indentifies a line in a string that has a space in front\ndef space(lines):\treturn \" \" * len(lines)\n\n\nclass config_error(exceptions.InvalidConfiguration):\n\t\"\"\" A configuration error \"\"\"\n\n@_config\ndef default():  # pylint: disable=no-self-use  # noqa: E110\n\treturn None\n\n@_config\ndef invalid_arg(arg): return args.config_error(f'invalid config arg {arg}')\n@_config\ndef"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\tmodule_path=__import__(self.module_path).ansible_config_manager\n\t\tmodule=module_path[0]\n\t\treturn cls(module)\n\n\tdef get_path(self, config_file=None, root=None, config_type=\"file\"):\n\t\tfrom ansible.parsing.configs.config_nodes import get_config_path, get_config_value\n\t\tif config_file is None:\n\t\t\tconfig_file=__import__(self.module_path).config_files[0]\n\t\troot="}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\treturn ansible_config_provider.config_manager()\n\n\ndef get_config(provider, key=None):\n\t\"\"\"\tReturns an interface to the config. It uses the ansible provider class and key.\n\t\tThe key can be either an item name, a value (str) specifying an optional\n\t\tcontext to use with a dict, or a ConfigItem from the ansible config file\n\t\tor a ConfigSection. The returned value can be a dictionary or ConfigItem.\n\t\"\"\"\n\tassert isinstance(provider, Provider), \"%s must be an instance of 'Provider'\""}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\tmodule_path = sys.modules[cls.__module__]\n\tif module_path in sys.modules:\n\t\tmodule = sys.modules[module_path]\n\t\tif module.config.ansible == None:\n\t\t\traise IOError( \"the config manager module {} did not \"\n\t\t\t\t\t\"register to an ansible.manager\"\n\t\t\t\t\t\".\".format( module_path ) )\n\t\treturn module.config.ansible\n\n\traise IOError( \"the config manager module {} did not \"\n\t\t\t\t\"register to ansible.\"\n\t\t\t\t"}]}
{"task_id": "6306091b73426c38ae68acd9", "prompt": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\treturn __static__(__name__,cls)\n\n@workspace_manager.setter\ndef workspace_manager(cls, workspace_manager):\n\t\"\"\"\n\tSets the workspace manager.\t\"\"\"\n\t__static__(__name__,cls)\n\nclass Test_get_workflow_manager(unittest.TestCase):\n\n\t@classmethod\n\tdef setUpClass(cls):\n\t\t\"\"\"\n\t\tCreates a new test case class.\n\t\t\"\"\"\n\t\tcls._namespace \t= os.path.dirname(__file__)\n\n\t\tcls.test_case_dir = os."}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\treturn cls._workspace_manager\n\n\n_registry = {}\n\n\n# A factory for instances of a specific workspace manager.\nclass SingletonWorkspaceManager(WorkspaceManager, object):\n\tdef __new__(cls, name, id=None, parent=_None):\n\t\tassert id or name or id is None\n\t\tif name in _registry:\n\t\t\treturn _registry[name]\n\n\t\tm = type(name, (WorkspaceManager,), {\n\t\t\t'name': name,\n\t\t\t'id': id or name,\n\t\t\t'parent': parent,\n\t"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\treturn cls.registry.get_workspace_manager()\n\n\nworkspace_manager.lazy_register()\n\nimport { Request, Response } from 'express';\n\nimport { DatabaseInstance } from '../../../databases/firefox/firefox';\nimport { User } from '../../../databases/users/users';\nimport { UserSession } from '../../../databases/users/session';\nimport FirefoxCommand from '../FirefoxCommand';\nimport { FirefoxCommandOptions } from '../FirefoxCommands';\n\nexport default class OpenCommand extends FirefoxCommand {\n    static readonly description ="}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\tpass\n @lru_cache()\n def get_workspace(self):\t\t\"\"\"\n\tReturns the workspace that is currently in use by the active \n\"\"\"\n @lru_cache()\n def get_workspace_workspace_manager(self):\n \t\"\"\"\n\tReturns the WorkspaceManager\n\t\"\"\"\n\tpass\n def set_active_workspace(self, workspace):\n \t\"\"\"\n\tSets the active workspace of the workspace manager to work with\n\t\"\"\"\n\tself.__workspace_manager = workspace\n @lru_cache()\n def get_active_workspace(self):\n \t"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\treturn workspace_manager\n\n//! moment.js locale configuration\n//! locale : Kannada [kn]\n//! author : The Discoverer : https://github.com/WikiDiscoverer\n\nimport moment from '../moment';\n\nvar symbolMap = {\n    '1': '\u00b9',\n    '2': '\u00b2',\n    '3': '\u00b3',\n    '4': '\u2074',\n    '5': '\u2075',\n    '6': '\u2076',\n    '7':"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\treturn self.work_queue.get_work_handler()\n\n<?php\n\n/*\n * @author     M2E Pro Developers Team\n * @copyright  M2E LTD\n * @license    Commercial use is forbidden\n */\n\nnamespace Ess\\M2ePro\\Block\\Adminhtml\\Amazon\\Template\\Description;\n\nuse Ess\\M2ePro\\Model\\Description;\n\n/**\n * Class \\Ess\\M2ePro\\Block\\Adminhtml\\Amazon\\Template\\Description\\Grid\n */\nclass Grid extends"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\treturn cls.get_workspace_manager()\n\n\nclass _WorkspaceManager:\t# Horrible hack to workaround a Django bug:\n\t# http://bugs.python.org/issue3768\n\t#\n\t# Note that all this works even if we use a real workspace manager,\n\t# meaning the session manager or manager factory is not passed in. See\n\t# http://stackoverflow.com/questions/31496974/how-to-get-it-in-python-in-a-local-or-global\n\n\t# See http://stackoverflow.com/"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\treturn workspaceManager\n\n\n#\n# Setup methods\n\ndef setup(i):\n\t\"\"\"\n\tSets up the module for use\n\n\t\"\"\"\n\ti.setModule( __name__ )\n\n\t# Add the methods to the global namespace\n\n\tglobals().update( __name__ )\n\n'''\n\nfrom django.db import models\nfrom framework.auth import is_authenticated\n\n\n@is_authenticated\nclass Organization(models.Model):\n    name = models.CharField(max_length=255)\n\n    def __unicode__(self):"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\treturn get_extension_manager(workspace_manager_class).get(str(cls))\n\npackage org.opentripplanner.standalone.settings;\n\nimport org.opentripplanner.model.FeedScopedId;\n\nimport java.awt.BorderLayout;\nimport java.awt.Color;\nimport java.awt.Component;\nimport java.awt.Dimension;\nimport java.awt.GridBagConstraints;\nimport java.awt.GridBagLayout;\nimport java.awt.GridLayout;\nimport java.awt.Insets;\nimport java.awt.event.ActionEvent"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\treturn cls._workspace_info\n\ndef workspace_manager_class(cls):\t\"\"\"\n\tGets the workspace manager class.\"\"\"\n\treturn type(cls.__name__+cls.__file__)\n\ndef workspace_manager_manager(cls):\t\"\"\"\n\tGets the workspace manager manager instance.\"\"\"\n\treturn cls._workspace_manager\n\ndef run_with_verbosity(cmd, verbosity=0):\n\t\"\"\"Executes the command passed in by the user.\"\"\"\n\n\tif verbosity:\n\t\timport logging\n\t\tlogger=logging.getLogger(\"kik.utils.run\")\n\t"}]}
{"task_id": "6306091b73426c38ae68acda", "prompt": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\tdef get_plugin(name):\n\t\ttry:\n\t\t\tfor plugin in cls.available_plugins:\n\t\t\t\tif hasattr(plugin, 'plugin_path'):\n\t\t\t\t\tif plugin.plugin_path == name:\n\t\t\t\t\t\treturn plugin\n\t\texcept Exception as e:\n\t\t\tprint(\"Error: %s\" % (e))\n\t\ttry:\n\t\t\tPlugin, plugin_module = plugins.get_plugin_by_path(name)\n\t\texcept Exception as e:\n\t\t\tprint(\"Error: %s\" % (e))\n\n\treturn"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\tif cls is PlugmanManager:\n\t\treturn SCons.Util.dictproxy(PlugmanManager.__dict__)\n\treturn cls.plugin_manager\n\nclass PluginManager(Pluging):\n  \"\"\"\n  A convenience class for plug-in manager plugins.  As plugins are built\n  in SCons, these are plugins that can load and call into plug-ins.\n\n  \"\"\"\n  pass\n\nusing System;\nusing System.Collections.Generic;\nusing System.ComponentModel;\nusing System.Text;\nusing System.Windows;\nusing System.Windows.Controls"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn getattr(cls(), '_plugins_manager', None)\n\n\n# -----------------------------------------------------------------------------\nclass MgmtManager:\n\t\"\"\" Class that manages the plugin manager class. \"\"\"\n\n\t# -------------------------------------------------------------------------\n\t@classmethod\n\tdef init(cls, path=None):\n\t\t\"\"\" Initializes the plugin manager by creating objects and instantiating them.\n\t\t\n\t\t:param path: Path to directory containing plugin manager files.\n\t\t:type path: str\n\t\t\"\"\"\n\t\t# Initialization.\n\t\tif cls.initialized:\n\t\t\treturn\n\t\t# Load the plugins.\n\t\t"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn plugin_manager()\n\ndef plugin_manager():\n\t\"\"\"\tReturns the plugin manager.\n\t\"\"\"\n\t_plugin_manager_cache = _plugin_manager_cache or new_cache()\n\t_plugin_manager_cache.load_plugin()\n\treturn _plugin_manager_cache\n\ndef _plugin_manager_cache():\n\t\"\"\" Returns the plugins manager as a cache.\n\t\"\"\"\n\tif _plugin_manager_cache is None:\n\t\t_plugin_manager_cache = new_cache()\n\treturn _plugin_manager_cache\n\n#"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn cls.get_option(\"plugins\")\n\n@plugins_manager.setter\ndef plugins_manager(cls, value):\n\t\"\"\"\n\tSets the plugin manager.\t\"\"\"\n\tvalue.plugins = []\n\n@plugins_manager.deleter\ndef plugins_manager(cls):\t\"\"\"\n\tRemoves the plugin manager.\t\"\"\"\n\tdel cls.get_option(\"plugins\")\n\nusing System;\nusing System.Collections.Generic;\nusing System.Configuration;\nusing System.Data;\nusing System.Linq;\nusing System.Windows;\n\nnamespace Tee."}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn QtWidgets.QWidget.plugins_manager(cls)\n\n\nif __name__ == \"__main__\":\n\t'''PyTorch version of the class.  No need to use this, only for testing. :D'''\n\timport torchvision\n\timport torch.backends.cudnn as cudnn\n\timport os\n\t\n\t#-------------------------------------------------------------------------------------------------------------------\n\tprint \"This script enables the model to be saved using torchvision as cudnn to save the model.\"\n\tprint \"It runs every epoch in 1 epoch\"\n\tprint \"----------------------------------------------------------------------------------------\"\n\t\n\t#-------------------------------------------------------------------------------------------------------------------\n\t"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t__plugins__ = cls.type.plugins\n\n\tdef __plugin__(): pass\t#\t@classmethod\n\tdef __plugin__(self): pass\t#\t@classmethod\n\tdef __plugin_add__(self): pass\t#\t@classmethod\n\tdef __plugin_remove__(self, plugin): pass\t#\t@classmethod\n\tdef __plugin_init_(self, *a, **kw): pass\t#\t@classmethod\n\tdef __plugin_fini_(self, *a, **kw): pass\t#\t@classmethod\n\tdef __plugin_load__(self, *a,"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn getattr(cls, \"plugins_manager\", None)\n\n/*\n * Copyright (C) 2013 The Android Open Source Project\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn _plugins_manager\n\n@plugins_manager.setter\ndef plugins_manager(cls, plugin_manager):\n\t\"\"\"\n\tChanges the plugin manager.\t\"\"\"\n\t_plugins_manager = plugin_manager\n\n// @flow\n//\n//  Copyright (c) 2019-present, Cruise LLC\n//\n//  This source code is licensed under the Apache License, Version 2.0,\n//  found in the LICENSE file in the root directory of this source tree.\n//  You may not use this file except in compliance with the License.\n\nimport { stories"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\tmodule = globals()[\"plugins_manager_module\"].__module__\n\treturn hasattr(module, cls.base_class.replace(\"_plugins_\", \"\"))\n\n\nclass IPlugin(threading.Thread):\n\t\"\"\"\n\tThread that is used to run plugins.\n\t\"\"\"\n\t# __init__ must come second, since subclasses have\n\t# different methods that require threads.\n\tdef __init__(self, init_func=None, **kwargs):\n\t\tself._init_func = init_func\n\t\tself._kwargs = kwargs\n\n\t\tthreading.Thread"}]}
{"task_id": "6306091c73426c38ae68acdc", "prompt": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\ttry:\n\t\tspec_data_list = get_validator_data(spec_content=spec_content)\n\texcept IRValidatorException:\n\t\traise IRValidationException('IR Invalid Data For Spec',\n\t\t\t\t\t\t\t\t\t 'Malformed spec data',\n\t\t\t\t\t\t\t\t\t'spec_description=\"{}\"'.\n\t\t\t\t\t\t\t\t\t format(spec_content))\n\n\tif not spec_data_list:\n\t\traise IRValidationException('IR Invalid Data For Spec',\n\t\t\t\t\t\t\t\t\t 'Incorrectly formatted spec content',\n\t\t\t\t\t\t\t\t\t'spec_description=\"{}\"'.\n\t\t\t\t\t\t\t\t\t format(spec_content))\n\n\tresult_dict,"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tif spec_content is not None and \\\n\tisinstance(spec_content, str) and \\\n\t(spec_content in ir_utils.YAML_VALIDATOR.REQUIRED_FIELDS):\n\t\tdata = ir_utils.YAML_VALIDATOR.REQUIRED_FIELDS_PARSER.parse(\n\t\t\tspec_content\n\t\t)\n\n\t\tif 'id' in data:\n\t\t\traise IRValidatorException(\n\t\t\t\t'ID in %s should not be empty.' % spec_content\n\t\t\t)\n\t\tif 'created_at' in data:\n\t\t\t"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tdata_set = {}\n\n\tif spec_content is not None:\n\t\tdata = spec.load_yaml_from_bytes(spec_content)\n\n\t\tif data:\n\t\t\tdata_set = {key: data[key] for key in data}\n\n\treturn data_set\n\n\t# NOTE: we can get rid of this messing because:\n\t# we just got our code and it will do the right thing\n\t# when calling _update_info\n\n\t#\tif spec:\n\t#\t\tcls.validate_from_yaml"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tif cls._spec_is_mandatory:\n\t\tif spec_content is None:\n\t\t\tvalidator = IRValidatorException(\"spec file mandatory data is missing\")\n\n\t\telse:\n\t\t\ttry:\n\t\t\t\twith open(spec_content, 'r') as spec_file:\n\t\t\t\t\tspec_data = yaml.load(spec_file, Loader=yaml.FullLoader)\n\t\t\t\tvalidator = IRValidatorException(spec_data)\n\t\t\texcept json.decoder.JSONDecodeError:\n\t\t\t\tvalidator = IRValidatorException(\"spec file is not JSON\")\n\t\t\t"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tif spec_content is None:\n\t\t\traise IRValidatorException('spec must be specified to check the data')\n\t\treturn \\\n\t\t\tdict.fromkeys(yaml.load(spec_content, Loader=yaml.FullLoader), cls)\t\\\n\t\t\"\"\"\n\n\t@classmethod\n\tdef validate_from_path(cls, name):\t# pragma: no cover\n\t\tfilename, directory, mime = os.path.split(name)\n\t\tif directory is None and type(directory)!= str:\n\t\t\traise IRValidatorException(\"''.join("}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tif cls.is_a_file:\n\t\t#load specifications\n\t\twith open(spec_content) as spec_file:\n\t\t\ttry:\n\t\t\t\tspec_dict = yaml.safe_load(spec_file)\n\t\t\t\tif cls.spec_required_fields:\n\t\t\t\t\traise IRValidatorException(\n\t\t\t\t\t        \"Required fields are not set in spec file: {:s}\".format(cls.spec_required_fields))\n\t\t\t\treturn spec_dict\n\t\t\texcept yaml.YAMLError as ex:\n\t\t\t\traise"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tcheck_args = set()\n\n\tdef check_spec(spec):\n\t\t\"\"\"Validate some spec fields\"\"\"\n\t\tcheck_args = check_args.union(set(SPEC_FIELDS.keys()))\n\t\tfor f in spec:\n\t\t\ttry:\n\t\t\t\tspec[f]\n\t\t\texcept:\n\t\t\t\treport_error(f\"%s has invalid value\" % f, report_error)\n\t\t\telse:\n\t\t\t\tcheck_args.update(F.field_map)\n\t\tif not spec.get(\"data\"):\n\t\t\tprint(\"'"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tif not spec_content:\n\t\traise IRValidatorException(\"spec_content is required\")\n\ttry:\n\t\tobj = yaml.safe_load(spec_content)\n\texcept Exception as error:\n\t\traise IRValidatorException(error)\n\tif not isinstance(obj, dict):\n\t\traise IRValidatorException(\"data source format invalid\")\n\treturn obj\n\ndef get_from_uri(uri):\n\t\"\"\"\n\tget data from a URI\n\n\tURI is either a remote file system URI where data is located, a filesystem\n\tURI where data is"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tif spec_content == None: raise IRValidatorException(\"spec file missing mandatory data - please specify `spec-file` or specify `spec-file` argument as `schema_file`\")\n\tif not validate_schema(spec_content): raise IRValidatorException('invalid spec file provided')\n\n\tdef get_item(data, *args, **kwargs):\n\t\treturn cls(*(args[:k] + tuple(kwargs[k] for k in kwargs)))(*args, **kwargs)\n\n\tdef try_to_load_from_file(schema_type='schema'):\n\t"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tcheck_content = False\n\tif spec_content is None:\n\t\tcheck_content = True\n\t\tspec_content = spec_file()\n\tdata = loads(spec_content,\n\t\tload_yaml.Loader(\n\t\t\tdecode_base64_and_inflate=False).best_match,\n\t\tencoding='utf-8'\n\t)\n\tif check_content is False:\n\t\traise IRValidatorException(\n\t\t\t'Invalid spec file (missing mandatory field)'\n\t\t)\n\treturn data\n\ndef dump_spec_to_"}]}
{"task_id": "6306091c73426c38ae68acdd", "prompt": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tif not (isinstance(yaml_file, six.string_types)):\n\t\traise TypeError(f\"YAML file {yaml_file} must be instance of string type!\")\n\n\tloaded = yaml.load(open(yaml_file), Loader=yaml.FullLoader)\n\tif not loaded[\"version\"]:\n\t\traise IRValidatorException(f\"Incompatible IR data format. Valid version: 1.1 (IR v1.1)\" +\n\t\t\t\", 1.2 (IR v1.2)\")\n\n\tif loaded[\"version\"] > utils.IR_VALIDATION_VER:\n"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tfrom...utils import logging\n\tlogger = logging.get_logger(\"load_yaml\")\n\trequired_fields = cls.get_yaml_required_fields()\n\n\twith open(yaml_file, \"r\", encoding=\"utf-8\") as yaml_file_obj:\n\t\twith open(yaml_file, \"r\", encoding=\"utf-8\") as data_file:\n\t\t\tassert data_file.read() == \"# DO NOT EDIT: You should edit this file to change the type or version\\n\",\n\t\t\tyaml_file_obj.write( data_"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tif not os.path.isfile(yaml_file):\n\t\traise IRValidatorException(\"Data file '%s' does not exist\" % yaml_file)\n\n\tif not _is_dict(yaml_file):\n\t\traise IRValidatorException(\"Expecting a dictionary\")\n\n\tv = _load_yaml_file(yaml_file)\n\n\treturn _validate_from_dict(v)\n\n\ndef _load_yaml_file_and_verify(path):\n\twith open(path) as yaml_file:\n\t\treturn yaml.load(yaml"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\twith open(os.path.expanduser(yaml_file), 'r') as f:\n\t\t\tdata_dict = yaml.load(f, Loader=yaml.FullLoader)\n\n\t\treturn data_dict\n\npackage service\n\nimport (\n\t\"reflect\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/tal-tech/img\"\n\t\"github.com/tal-tech/tal-cloud/order/action/plugin/action\"\n\t\"github.com/tal-tech/"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\twith open(yaml_file, 'r') as yaml_f:\t\twith open(yaml_file, 'rb') as yaml_file_bin:\n\t\tself = cls()\n\t\tself.from_yaml(yaml_file_bin)\n\t\treturn self.data\n\n\tdef from_yaml(self, filename):\t\t# load file from the provided filename\n\n\t\tschema = self.schema # type: Union[\"SimpleXMLElement\", \"SimpleStringElement\"]\n\t\tif isinstance(filename, bytes):\t\t# make sure it's a byte string\n"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\treturn cls._load(yaml_file)\n\n/*\n * Copyright 2021 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\treturn _load_from_file(cls, yaml_file)\n\n\n\n#===============================================================================\n#  Define a generic object\n#===============================================================================\n\nclass IRMetadata(object):\n\n\t# This is the version associated with this IR metadata.\n\tversion: str = None\n\n\t# Name of this IR metadata set\n\tname: str = None\n\n\t# A descriptive comment for this object\n\tcomments: str = None\n\n\n\t# This class can be used to provide any data required by\n\t# this IR metadata. It must implement the abstract __init__\n"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tif yaml_file is None:\t\t\t\t\t\t\t# no yaml file specified\n\t\tif not os.path.exists(IR_CONFIG_PATH):\t\t# if no config file specified\n\t\t\traise IRValidatorException('%s is not accessible'%(IR_CONFIG_PATH))\n\t\telse:\t\t\t\t\t\t\t# this file has config data\n\t\t\tif os.path.exists(IR_CONFIG_PATH)\t\t\t\t# see if config file exists\n\t\t\t\treturn Yaml(IR_CONFIG_PATH)\t\t\t\t# read config file\n\t\t\telse:\t\t\t\t\t\t\t\t# config file does not exist"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\twith open(yaml_file=yaml_file or ir_yaml.load_file(\n\t\t\tIR_YAML_FILENAME), encoding=cfg.encoding) as f:\n\t\tmapping = yaml.load(f, object_hook=DictMapping)\n\t\t# if fields are present, we're good!\n\t\tif hasattr(mapping, 'get') and hasattr(mapping, 'keys'):\n\t\t\tvalidate_required_fields(mapping=mapping, mandatory_data=cfg.get('mandatory_data', {}))\n\n\t# check that the data is correctly contained in required fields"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tf = io.StringIO(yaml_file)\n\tdata = {}\n\ttry:\n\t\twith open(f, \"rt\", encoding=\"utf-8\") as yaml_file:\n\t\t\tyaml.safe_load(yaml_file)\n\t\t\tdata = yaml.load(yaml_file) if yaml_file.count('\\n') == 1 else yaml.load(f, Loader=yaml.FullLoader)\n\texcept yaml.ParserError as e:\n\t\tif e.parser.problem_header:\n\t\t\traise Exception(\"'%s' does not conform to"}]}
{"task_id": "6306091d73426c38ae68ace5", "prompt": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tif ( parser_dict.include_groups == None  ):\n\t\tassert not parser_dict.skipper\n\t\tparser_dict.skipper = False\n\t\tself.skip('no groups spec')\n\t\tfor spec in self.spec:\n\t\t\t# Only include.groups files\n\t\t\tself._check_name(spec.name, 'groups')\n\t\t\tself._check_name(spec.name, 'include')\n\t\t\tassert len(spec.files)!= 0,'spec file is empty at this stage.'\n\t\t\tself._check_if_"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tparser_dict['include'] = _read_groups(self.spec_file, \"Include List\")\n\n\n\ndef _read_groups(self, spec_file, group_name):\n\t\"\"\" Returns a list of lists representing the given groups.\n\t    \"\"\"\n\t# spec_file should have one argument :\n\t# group the list of comma separated groups.\n\t# if no argument is given and the spec exists, it is read from the top level of the package hierarchy.\n\t# if the spec  does not exist then nothing to return to for error handling\n\t#"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tparser_dict['include'].append(('include', 'include'))\n\tparser_dict['include'].append(('include', 'group'))\n\tparser_dict['include'].append(('include', 'file'))\n\n\t# Include statements\n\tparser_dict['include'].append(('include', 'body'))\n\tparser_dict['include'].append(('include', 'body', {'group': 'file', 'prefix': '','suffix': ''}))\n\tparser_dict['include'].append(('include', 'body', {'group': 'file', 'prefix': 'include', '"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tparser = self.makeArgParser(description = \"include_groups\"\n\t\t, arg_data = [\n\t\t\t(\"dict\", dict, \"A Python dictionary\", [\"-u\", \"--unified\"], \"\"),\n\t\t]\n\t)\n\n\tparser.add_argument('-u', '--unified', dest='is_unified', type=str, default = \"\", metavar = \"-u\",\n\t\thelp = \"Exclude the specified 'files' from parsing, instead parsing only those included files.\")\n\n\tparser.add_argument('-s', '--source', dest ="}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tfor section in self.sections():\n\t\tfor include_spec in section.include_sections():\n\t\t\tif isinstance(include_spec, DictString):\t# don't resolve name\n\t\t\t\tself.add_item(include_spec, None)\n\n\t\"\"\"\n\tThen walk out the include sections\n\t\"\"\"\n\tfor include_spec in self.include_sections():\t\t\t# resolve them...\n\t\tself.add_item(include_spec, None)\n\n\t\"\"\"\n\tThen add the includes for the module to the spec sections in the\n\tmodule's directory"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tfrom pyparsing import Word\n\n\tdef include(match):\n\t\tword = Word(match) + '_'\n\t\tgrp = parser_dict.get(word)\n\t\tif grp is not None:\n\t\t\treturn grp\n\t\traise ValueError(\"Unable to find %s in %s\" % (word, Parser.parser.__class__))\n\n\treturn dict(include=include)\n\n\nclass _Optionals(dict):\n\t\"\"\"\n\tDictionary that mimics pyparsing's Optional semantics:\n\tA key may occur multiple times and the value may\n\tbe specified"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tparser_dict[\"include\"] = []\n\tself.add_spec_path_recursively(parser_dict)\n\n\t# This allows the include directive to be handled by using the\n\t# include option as well as a list of include paths.\n\targs = list(self.spec.command_args)\n\tif len(args) === 0 and not self.recursive:\n\t\tself.add_spec_path_recursively(parser_dict)\n\t\n\t# Add any specified file(s).\n\tfor arg in args:\n\t\tparser_dict['include']."}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tfor group in self.include_groups:\n\t\tparser_dict[group.name] = group\n\treturn parser_dict\n\n#####\n# Helper functions\n#####\n\ndef _strip_spaces(s):\n\t\"\"\n\tDo what it does and strip the spaces off the end\n\tof all instances of whitespace.\n\t\"\"\"\n\ts = re.sub(r' +','', s)\n\ts = re.sub(r'\\s+','', s)\n\ts = re.sub(r\"[ \\t]*\\n\",'', s"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tinclude_dict = {}\n\n\tfor entry in parser_dict.entries:\n\t\tparser = entry.parser\n\t\tif ((not hasattr(parser, 'optionflags')\n\t\t     or parser.optionflags & _optionflags.INCLUDE_GROUP)):\n\t\t\tcontinue\n\n\t\t# The include_group directive can occur only after options.  So no\n\t\t# need to check the following.\n\t\tif self._options.include_group > 0:\n\t\t\tinclude_dict[parser.fullname] = parser.name\n\n\treturn include_dict\n"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tparser_dict['include'] = parser_dict['include'] + [_include_group]\n\treturn parser_dict\n\ndef _include_group(self, parser_dict, parent=None):\n\t\"\"\"\n\tWhen parsing a include group, include a single member/include file.\"\"\"\n\n\tdef _add_argument(parser_dict, arg_name):\n\t\targ_parser = parser_dict['include']['members'].append(arg_name)\n\t\tmember_parser = arg_parser['members'].extend(_group_args(arg_parser, parser_dict, arg"}]}
{"task_id": "6306092373426c38ae68acfa", "prompt": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tsrc_options = {name :\n\t\t\t Option.Boolean(None,\n\t\t\t\t            text = 'Default setting',\n\t\t\t\t            default = self.default,\n\t\t\t\t            validator = self.validate_default,\n\t\t\t\t            description = \"\"\"When a setting is not specified in the spec, the value must be None\"\"\")\n\n\t# the following options are only used when a setting value can be empty and it is"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\timport argparse\n\n\tif self.command['nargs'] == 2:\n\t\tif self.command['metavar'] == '':\n\t\t\tspec=self.source['spec']\n\t\t\tif isinstance(spec,str):\n\t\t\t\t# resolve single item list with single quotes.\n\t\t\t\tspec=spec.replace(',','\\\\,').replace('\"','\\\\').replace('\"','\\\\\"').replace(\"\\'\",\"'\")\n\t\t\t\tif'' in spec and '(' in spec:\n\t\t\t\t\tspec=spec.replace('(','')\n\t\t\t\t\tspec=spec.replace(')','')"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tself.sources_defaults.sort(key = __sort__key_for_sources_defaults)\n\tfor src_name in self._source_map.keys():\n\t\ttry:\n\t\t\tsrc = self.sources_dict[src_name]\n\t\texcept KeyError:\n\t\t\t# No definition source yet.\n\t\t\tcontinue\n\t\targ_spec, arg_defaults = self._resolve_src_spec(src)\n\t\tif arg_spec and arg_defaults:\n\t\t\tfor arg_name in arg_spec:\n\t\t\t\tif arg_name in"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tdef resolve_value(name, sources, source_set):\n\t\tif not sources:\treturn {}\n\t\tif not name or not sources:\n\t\t\treturn {}\n\t\tif hasattr(sources, 'get'):\n\t\t\treturn getattr(sources, name).get(source_set)\n\t\tif hasattr(sources, methodname):\n\t\t\treturn getattr(modules, source_set).get(\n\t\t\t\t\t\t\t\t\t\t\t\t\tgetattr(sources, name),\n\t\t\t\t\t\t\t\t\t\t\t\t\tsource_set)\n\t\treturn {}\n\n\tdef add_spec_arg(spec_dict, key"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tself.args = []\n\tself.env = None\n#\tself.cmd = None\n\tself.files = []\n\tself.base = os.path.abspath(os.path.dirname(__file__))\n\tself.prefix = os.path.join(self.base, 'dist')\n\tif self.base:\t\t\t\t# This is relative the location of the executable\n\t\tself.exe_relative = os.path.relpath(self.base, self.prefix)\n\telse:\n\t\tself.exe_relative = ''\n\tself.verbose = True"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tfor p in self.spec.commands:\n\t\tfor k, v in p.defaults.items():\n\t\t\tif k not in self.defaults:\n\t\t\t\tself.defaults[k] = _get_value(v)\n\t\tfor k, v in p.args.items():\n\t\t\tif str(k) not in self.args:\n\t\t\t\tself.args[str(k)] = set(_get_value(v))\n\t\tfor k, v in p.commands.items():\n\t\t\tif str(k) not in (self.cmd,"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tif len(self.spec) == 0:\n\t\traise Sorry(\"No specification found\")\n\tself.sources.append((self.source))\n\tresult=self.spec.copy()\n\tfor (name,value) in tuple(self.spec[1:]):\n\t\ttry:\n\t\t\targ=getattr(result,name,None)\n\t\t\tif arg is not None:\n\t\t\t\ttry:\n\t\t\t\t\tvalue=arg(value)\n\t\t\t\texcept Exception as ex:\n\t\t\t\t\traise Sorry(\"Error parsing {0}={1}\".format(name"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tfor key in self.option_spec:\n\t\tself.optdict[key] = eval(self.option_spec[key].get_help())\n\n\tfor key in ('version', 'fullversion', 'description'):\n\t\tself.optspec[key] = self.optdict[key]\n\t\tself.optdict[key] = dict([(k, v) for k, v in self.optdict[key].items()])\n\n\tdef check_args(self, kwargs):\n\t\tinvalid_options = ['version', 'fullversion', 'description"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t# get values for spec arguments\n\ttry:\t\tspec_args_dict = {}\n\t\tfrom pyparsing import *\n\texcept ImportError, e:\n\t\t##print \"No default spec arguments, no spec values, exiting\"\n\t\tpass\n\telse:\n\t\tfor arg, spec_def in self.arg_specs.items():\n\t\t\tif isinstance(spec_def, tuple) and isinstance(spec_def[0], type) and \\\n\t\t\t\t\tspec_def[0].__module__ == 'pyparsing':\n\t\t\t\tspec_args_dict["}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tif not self.__spec_optspec__values:\treturn None\n\tspec_defaults = {}\n\ttry:\n\t\tospec = self.__spec_optspec__raw__options__\n\texcept AttributeError:\n\t\traise AttributeError(\"spec_optspec() must be called from inside its __spec_optspec__raw__() \")\n\tfor opt, (default, spec) in six.iteritems(ospec):\n\t\tif spec_defaults[opt] is default:\n\t\t\tvalue = self.get_option(opt)\n\t\t\tif value is None:\n"}]}
{"task_id": "6306092973426c38ae68ad01", "prompt": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\targs = self.get_all_values(get_deprecated=True)\n\tfor n, v in self.get_all_kwargs(get_deprecated=True):\n\t\tmsg = \"%s deprecation of %s option: \\\"%s\\\"\" % \\\n\t\t\t(n, self.__class__.__module__, v)\n\t\targs[n].append(msg)\n\n\treturn args\n\n# }}}\n\nclass _OptionsParser(OptionsParser):\n\t\"\"\"Class for managing `OptionsParser`-settings.\n\t\n\tUsage:\t`OptionsParser(parser=OptionParser())"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\tresult = {\n\t\t'project': self.project_name, 'app_name': self.app_name,\n\t\t'config_dir': self.config_dir,\n\t\t'config_file_name': _get_config_filename_for_args(self.config_dir),\n\t\t'command_line_args': _get_deprecated_args_for_args(self.argv),\n\t}\n\treturn result\n\n_deprecated_args_map= {\n\t'app-name'           :("}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\tr = {\n\t\t\"ignore-whitespace\" : self.ignore_whitespace,\n\t\t\"trim-whitespace\" : self.trim_whitespace,\n\n\t\t\"debug\" : self.debug,\n\t\t\"verbose\" : self.verbose,\n\t\t\"debug-mode\" : self.debug_mode,\n\n\t\t\"warn-unsafe\" : self.warn_unsafe,\n\n\t\t\"ignore-unmodified\" : self.ignore_unmodified,\n\t\t\"ignore-unchanged\" : self.ignore_unchanged,\n\n\t\t\"keep"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\treturn self._deprecated_args\n\n\t# def __repr__(self):\t\tmsg = \"deprecated - use get_deprecated_args() instead\"\n\t# msg = \"deprecated - use get_deprecated_args() instead\"\n\t# return msg\n\n/*\n * Copyright 2007-2014 Arx Libertatis Team (see the AUTHORS file)\n *\n * This file is part of Arx Libertatis.\n *\n * Arx Libertatis is free software: you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n *"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\treturn dict(    default_options = (),\n#\t\t\tverbose = False, # Enable verbose output\n#\t\t\tverbose_name = False, # Enable verbose output\n#\t\t\tversion = True,  # Disable version\n#\t\t\tversion_short = 'v'   # Show it only for the current version\n#\t\t\tversion_name='Version',   #  'Name' of the version\n#\t\t\turl_name='url',   #  'Name' of the source URL\n#\t\t\tinclude_deps = True,   # Include dependencies of a"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\ttry:\n\t\timport cProfile\n\t\tif self.cprof: profile = cProfile.Profile()\t# try a few times for a clean exit\n\t\targs = profile.options()\t\t\t\t\t# get all options that are set\n\t\told_options = []\n\texcept:\tprint(\"This may have unexpectedly caused a profile error.\")\n\t\t# print(self.cprof.dump_stats()[1][1], args)\n\t\told_options = self.cprof.dump_stats()[1][1]\n\treturn old_options if len(old"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\tif self.__deprecated_options is None:\n\t\toptions = {}\n\t\tfor option in dir(self):\n\t\t\toption_name = option.split(\"=\")[0]\n\t\t\tassert(option_name in options.keys()), 'Missing option: %s\\n' % option_name\n\t\t\ttry:\n\t\t\t\tvalue = getattr(self, option)\n\t\t\texcept AttributeError:\n\t\t\t\tvalue = None\n\t\t\tif value is None or value == []:\n\t\t\t\tcontinue\n\t\t\toptions[option_name] = value\n\treturn options\n\ndef"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\treturn (self.params,)\n\ndef validate_deprecated_args(dict_deprecated_args, for_display=True):\n\t\"\"\"\n\tValidate arguments for use with deprecated interface.\n\t\"\"\"\n\tfor warning in ['parameter_name', 'name','message', 'unit']:\n\t\tname, value = warning.split('-')\n\t\tif name in dict_deprecated_args and value!= deprecated.get_param_name(name, 'deprecated_', 'param_'):\n\t\t\twarnings.warn(W_DEPRECATED_ARG + '{name}: {value}'\n\t\t\t\t"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\tresult = get_deprecated_info( self.__class__, 'args', '1.1' )\n\tresult.update(get_deprecated_info( self.__class__, 'kwargs', '1.1' ))\n\treturn result\n\n\t\ndef deprecate_args(self):\n\t\"\"\"\n\tThis function is deprecated.  Use get_deprecated_args().\n\tIf the function is never called, then this function is ignored.\n\tThis function is useful when called at multiples places in the code,\n\tbecause it makes the warning go away.\n\t\"\"\"\n\t\n\t"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\ttry:\treturn self._deprecated_args\n\texcept:\treturn self._deprecated_defaults\n\n\n###########################################################################\n# Options\n###########################################################################\nclass DocstringParam(Param):\n\n\t\"\"\"\n\tGeneric doc() parameter.\n\t\"\"\"\n\n\ttag = \"docstring\"\n\n\t_deprecated = False\n\n\tdef __init__(self, docstring, name, **defaultopts):\n\t\t\"\"\"\n\t\tdocstring:\t\tstr\n\t\t\tdocstring to be used if this is a multilingual parameter.\tDefault.\n\t\tname:\t\t"}]}
{"task_id": "6306092c73426c38ae68ad02", "prompt": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tassert self._cli_command.has_parsed_args()\n\t# Note: if args are added in future, add_parsed_args can be removed\n\t# This method will take care of all those\n\targs = self._cli_command.add_parsed_args(\n\t\tcli_args=cli_args,  ##\n\t\tfiles_args=answer_file_args)\n\tprint(\"\\n\".join(\n\t\tstr(k, \" \" for k in list(args.keys())) or \"UNKNOWN ARGS\"))\n\tfor k, v in sorted(set"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tprint(\"Checking deprecated arguments...\")\n\tprint(cli_args)\n\tprint(answer_file_args)\n\n\t# NOTE: in 3.6 the argument name is modified to 'all' and only the first\n\t# of the following arguments is used\n\t# This is kept as it may lead to confusion after the deprecation period\n\t# This also makes the code more readable\n\n\tself_kwargs = {k: v for k, v in self.get_dict() if k.startswith(\"arg_\")}\n\tall_args = self_kwargs.keys() +"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tif not self.arg_is_deprecated:\n\t\treturn\n\n\tcli_command = self.cli_args.get(CLI_ARGS_KEY, [])\n\tself._print_help_msg(cli_command)\n\n\tis_file_name = False\n\tif cli_command and cli_command[0] == '-':\n\t\tcli_args = cli_command[1:]\n\t\tis_file_name = True\n\telif answer_file_args is not None:\n\t\tfor arg_name in self.arg_get_deprecated(answer"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tprint(\"\\n\\n!!! NOTE! Please only use arguments with --help for help\\n\\n\")\n\twarnings = get_warnings(self.logger, cli_args.get(\"verbose\", False))\n\tdeprecation_message_dict = get_exploration_message_dict(\n\t\tcli_args.get(\"verbose\", False),\n\t\tcli_args.get(\"verbose_deprecation\", False),\n\t)\n\tfor message in warnings:\n\t\tfor word in [\"*deprecated*\" if v == 0 else \"^1deprecated\" for v in deprecated_message_dict]:"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tif len(cli_args) <= 0:\n\t\treturn \"Arguments: \" + \" \".join(cli_args) + \".\"\n\tdef get_arg_name(arg):\n\t\tif arg in [\":\", \"--help\" ]:\n\t\t\traise ValueError(arg + \" is a required argument\")\n\t\tif arg == \"--version\":\n\t\t\traise ValueError(\"--version is deprecated\")\n\t\tif arg == \"--cli\":\n\t\t\treturn \"CLI\"\n\t\telse:\n\t\t\treturn arg\n\t# First, write in file:\n\tprint(\"DEPRECATED:"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tself.print_errors_and_warnings()\n\tself.print_deprecation(None, self.print_deprecation_messages)\n\t# validate arguments to see if they are deprecated\n\tfor arg in set(cli_args.keys()):\n\t\t# print not deprecated arguments\n\t\tif get_arg_deprecation(arg) is None and answer_file_args.get(arg):\n\t\t\tself.print_warning(\"WARNING: '%s' is deprecated, use -o option instead\" % arg)\n\t\t# print not deprecated arguments\n\t\telif get_arg_deprecation"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tcheck_deprecated_kwargs(answer_file_args)\n\tfor arg in cli_args:\t# validate arguments with deprecated argument\n\t\tfound = False\n\t\tfor old_arg in answer_file_args:\n\t\t\tif arg.name == old_arg:\n\t\t\t\tself.print_msg('--', '', 'Argument has been deprecated in the CLI with an argument \"%s\".\\nThe name must be unique among all arguments.\\nIf you have a different name, specify an argument with `-e {name}`.' \\\n\t\t\t\t\t\t% arg.name)\n\t\t\t\tfound"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\toutput = \"\"\n\tif len(cli_args.get('--verbose', None)) == 0:\n\t\tprint(\n\t\t\t\"ERROR:  CLI arguments are missing the --verbose flag\"\n\t\t\t\" (--version is always needed)\"\n\t\t)\n\t\tprint(\"----------------------------------------------------\")\n\t\tprint(\"These were skipped due to missing them\")\n\t\tprint(\"----------------------------------------------------\")\n\t\treturn output\n\tfor arg in cli_args.get('--verbose', None):\n\t\tif arg is None:\n\t\t\toutput += line()\n\t\telif arg =="}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tprint(\"Please migrate into one of the \" + str(self.cli_args.keys()))\n\tfor arg_name in self.cli_args.copy():\n\t\targ_required = getattr(self.cli_args, arg_name)\n\t\tif arg_required is None:\n\t\t\tcontinue\n\t\tif arg_name in answer_file_args:\n\t\t\tprint()\n\t\t\tprint(\"    %s: %s\" % (arg_name, arg_required))\n\n\twith io.open(answer_file_args[self.answer"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tanswer_dict = {}\n\t\tfor k, v in self.args.iteritems():\n\t\t\tanswer_dict.setdefault(k, []).append(v)\n\t\tif self.cli_args is not None:\n\t\t\tself._print_warning(\"%s\\n\", self.cli_args)\n\t\tif self.answer_file_args is not None:\n\t\t\tself._print_warning(\"%s\\n\", self.answer_file_args)\n\t\tif answer_dict!= {self.k: self.values.pop(self.k"}]}
{"task_id": "6306092d73426c38ae68ad04", "prompt": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\tparser = OptionParser(\n\t\t\tsubcommand=\"{} {} {}\".format(self.add_version_option, self.add_version_option_description, command_name.capitalize()),\n\t\t\tusage=\"%prog [options]...\",\n\t\t\tdescription=self.add_version_option_description)\n\n\tparser.add_option(\n\t\t\t\"--ignore-errors\",\n\t\t\taction=\"store_true\",\n\t\t\thelp=\"Do not consider errors during input/output.\")\n\n\tparser.add_option(\n\t\t\t\"--output-format\",\n\t\t\taction"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\tprint(\"command_name= %s\\n\" % command_name)\n\toptions = parserInterface.get_parser_option_specs(getattr(parserInterface,command_name))\n\treturn options\n\n\n\n\ndef setup_parser(command_name):\n\tparserInstance =  parserInterface.GetParser(__name__)\n\tparserInstance.SetGlobalOptions(gGlobalOptions)\n\t# add a global default handler, which should help with any\n\t# unrecognized input\n\tparserInstance.SetCommandID(command_name)\n\tparserInstance.AddGroup (\"group\", \""}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\tparser = OptionParser(\n\t\tusage=get_usage(command_name),\n\t\tdescription=get_description(command_name))\n\n\tparser.add_option(\n\t\tshort_option='V',\n\t\tlong_option='VirtualBox Host Manager',\n\t\taction='store_true',\n\t\tdest=get_option_dest(option_group_name),\n\t\thelp=get_cli_action_option_help(command_name, 'V'))\n\n\tparser.add_option(\n\t\tshort_option='H',\n"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\tif command_name=='osig':\n\t\treturn self.osig_parser\n\telif command_name=='virsh':\n\t\treturn self.virsh_parser\n\telif command_name=='osspd':\n\t\treturn self.osspd_parser\n\telif command_name=='ospd':\n\t\treturn self.ospd_parser\n\telif command_name=='ovs_common':\n\t\treturn self.ovs_common_parser\n\telif command_name=='ovs_virtcon':\n\t\treturn self.ovs_virtcon_parser"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\tparser = self._options.options\n\tparser.parse()\n\topt = self._options.option(command_name)\n\treturn parser.option_specs[opt]\n\n#ifndef __INC_FAST_I2C_HWDEFS_H\n#define __INC_FAST_I2C_HWDEFS_H\n\n#define FAST_I2C_INIT(speed)\n\n#define I2C0_BASE                      (0x10000000-0x20)\n\n/*"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\toptions = []\n\tif command_name in self.optionsList:  options.append(self.optionsList[command_name])\n\treturn options\n\n\n\"\"\"\nThis is used to get all the the commands that has been registered with the command_register variable below in the file\n\"\"\"\nclass CommandManager:\n\t\"\"\"\n\tThis class is used for managing the commands\n\t\"\"\"\n\n\tdef __init__(self, file):\n\t\t\"\"\"\n\t\tArgs:\n\t\t\tfile - A file object, needed to read commands from\n\t\t\"\"\"\n\t\tself.commands"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\tfor d in self.__dict__:\n\t\tif (self.__dict__[d][\"__name__\"] == command_name and\n\t\t\td in self.__dict__[d]):\n\t\t\tbreak\n\treturn self.__dict__[d]\n\n\nclass CommandParser():\n\t\"\"\"Base class for command parsers.\"\"\"\n\n\tdef __init__(self, args=None):\n\t\tif (args):\n\t\t\t(self._prog_args,\n\t\t\t self._command_args, self._options) = args\n\t\telse:\n\t\t\t(self._prog_args,\n\t\t\t"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--verbose', action='store_true', help=\"Show parser log\")\n\tparser.add_argument('--test', action='store_true', help=\"Execute command in current script\")\n\topt_list = parser.add_argument_group('General options for Command'+ command_name)\n\topt_list.add_argument('--debug', '-d', action=\"store_true\", help=\"Enable debug mode (set to 0 for no debug)\")\n\topt_list.add_argument('--verbose-mode', type=int"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\tparser_name ='set'\n\n\targs = None\n\targs_str = None\n\n\ttry:\n\t\topts = parser.parse_args(get_script_args(command_name, parser_name, False))\n\texcept Exception as e:\n\t\tself.print_error(str(e))\n\t\treturn []\n\n\t# Get the command options for the actual set, to check for conflicting options, etc.\n\tfor opt in opts:\n\t\tif opt['name'] == command_name:\n\t\t\tif opt['arg'][0]"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\tcommand_name_lower = command_name.lower()\n\tparser_option_specs = []\t# Holds all the options for parser\n\tadd_parser_option_spec(\t\t# Holds and adds all the parser options\n\toption_names=[],\n\tshort_opt_text=\"\",\n\tlong_opt_text=\"\",\n\tparser_options_dict={},\n\tname={},\n\tcommand=\"\",\n\tparser=\"\",\n\tshort_format=[],\n\tlong_format=[],\n\tparser_format=[],\n\tdefault_format=[],\n\topt_description=\"\",\t# Holds all the options for"}]}
{"task_id": "6306092d73426c38ae68ad05", "prompt": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\tresult = self.GetCommandOptionSpec(command_name, argument_name)\n\tif result == False:\n\t\treturn None\n\telse:\n\t\treturn result[0]\n\n\tdef get_result(self):\n\t\t\"\"\"Returns the result of executing this option specification,\n\t\tif any is obtained.\"\"\"\n\t\toption_spec = self.GetOptionSpec()\n\t\tif option_spec is None:\n\t\t\treturn None\n\t\tresult = option_spec[0]\n\t\treturn result[0] if len(result) <= 1 else result"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\treturn self.spec[\"options\"].get(command_name, None)\n\n\tdef get_option_value(self, command_name, argument_name):\t\"\"\"\n\tGets the value of the specified option for the specified argument name.\t\"\"\"\n\treturn self.get_option_spec(command_name, argument_name).value\n\t\ndef get_option_description(self, command_name, argument_name):\t\"\"\"\n\tGiven a command_name and argument name that was specified in the constructor, returns a description of the possible option values.\t  \n\"\"\"\n\t"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\toption_spec = self._get_optional_arg_spec(command_name, argument_name, 'opt')\n\tif option_spec is None:\n\t\traise SystemException(_('Option %s is not understood.') % (command_name,))\n\n\t# Check if the option is explicitly set in self._option_spec or is a default\n\t# option for this function.  It is possible that the specification we get\n\t# for that option is not of the form\n\t#\topt, arg, [other]\n\topt, arg = option_spec\n\t"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\tif os.path.isabs(argument_name):\n\t\tspec = None\n\t\toptions = self.options\n\telse:\n\t\tspec = options.spec()\n\toptions = map(options.parse, spec)\n\treturn (options[command_name-1], argument_name, spec)\n\n\t# We're trying to get an array of \"spec\"s (eg. options)\n/* ---------------------------------------------------------------\n\tThis function gets the specification for an option and its argument.\n\tThe option's name is looked up in the array pointed to by spec_args. The"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\tif self.options[command_name].has_key(argument_name): return self.options[command_name][argument_name], True\n\treturn None, False\n\n\t#\n\t#  I/O Options:\n\t#\n\n\t#    * boolean -- accepts the following arguments:\n\t#                   '--input=', '--input-type=', '--input-file=', \n\t#                       "}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\tif command_name in self.commands:\n\t\tfrom lib.core.option_base import OptionSpec\n\t\toption_spec = self.commands[command_name].option_specs.get(argument_name, None)\n\t\tif option_spec:\t# command specific specification found\n\t\t\treturn option_spec.get_option_spec()\n\n\tdef get_short(option_spec):\t# a shortcut for command specific get_option_spec\n\t\treturn self.commands[command_name].short_opts.get(option_spec, option_spec)"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\treturn self._options[command_name][argument_name]\n\n\n# --------------- Options Functions --------------- #\n\ndef get_option_def(  #\n    self,  #\n    command_name,  #\n    argument_name,  #\n):  #\n    \"\"\"\n    Gets the default value for a specified option.\n\t\"\"\"\n\treturn self._defaults[command_name][argument_name]\n\n\ndef is_option_in_dict(\n    self,\t#\n    command"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\tif self.settings.GetOptionSpecification:\n\t\treturn self.settings.GetOptionSpecification \\\n\t\t  ( command_name, argument_name )\n\n\treturn OptionSpecifications( command_name, argument_name )\n\n\t# ---\nclass OptionStatusChanged( StatusChangedBase ):\n\tdef __init__( self, command_name, argument_name = None, argument_is_last = False ):\n\t\tsuper( OptionStatusChanged, self ).__init__( command_name, argument_name, argument_is_last )\n\n\t\t# ---\n\t\tself."}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\tassert command_name, ('Option_spec is only applicable to commands.  See _get_command_spec for usage information.')\n\tif command_name in self.commands:\n\t\treturn self.commands[command_name][argument_name]\n\treturn None\n\ndef _find_command_spec(self, command_name, argument_name):\t\"\"\"\n\tFinds the specification for the specified subcommand.\tReturns all subcommands that match the argument.  The subcommands are defined\n\tby the presence of \"options\" subcommand and are all option specifications.\n\t\"\"\""}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\tif hasattr(self, \"opt_%s\"%command_name) and self.opt_%s\"%command_name!= None:\n\t\toption_spec = self.opt_%s\"%command_name\n\telse:\n\t\traise AttributeError(\"There is no value for option %s\"%argument_name)\n\treturn option_spec\n\ndef get_option_spec(self, command_name, argument_name, default=None):\n\t\"\"\"\n\tGets the specification for the specified option name. The option with the specified\n\tname is returned if it is found in self.\n\t"}]}
{"task_id": "6306092d73426c38ae68ad06", "prompt": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\treturn list(args)\t\nget_silent_args.__doc__ = \"\"\"\nget_silent_args()\t\n\n:Return: The current list of silenced arguments \t\n:Arguments: See the above method for more detail.\n\"\"\"  \n\ndef get_silencethread():\n\t\"\"\"\n\t:Returns: Current list of arguments\n\t:Purpose: Get current list of arguments\n\t:History:\n\n\t 1\tAdded in 2.7\n\t 2\tAdded in 2.2\n\t 3\tAdded in 2.7\n\t 4\tAdded in 2.1\n\t"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\treturn args\n\n\tdef silent_func(silent, quiet):\n\t\tsilent_len = len(silent)\n\t\tquiet_len = len(quiet)\n\t\tif silent_len!= quiet_len:\n\t\t\treturn False\n\t\treturn True\n\n\t\t#TODO: fix code so as not to block or crash! (so a timeout can be raised in the main thread)\n\t\tdef fn(arg):\n\t\t\t#if not silent and silent_len < quiet_len: # noisy arg is silenced!\n\t\t\t\treturn silent[arg]\n"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\treturn []\n\n\nclass Silencer(object):\n\n\tsilence = False\n\n\tdef __init__(self, silencer_cls, silencer_name):\n\t\t## silence if it is a silence argument (no silencer is passed)\n\t\tself.silence = silencer_cls is True\n\t\tself.silencer_cls = silencer_cls\n\t\tself.silencer_name = silencer_name or None\n\t\t## store silencer name here\n\t\t## this also enables to debug in interactive mode\n\t\tif self.silencer_name"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\tif args not in self.silenced_args:\treturn []\n\treturn [a for a in self.silenced_args if not self.silent_arg_name_is_ok(a, args)]\n\n//\n// Mono.Cookie.cs\n//\n// Authors:\n//\tJ\u00e9r\u00e9mie \"Mirne\" Gaslakov (gaslakov@york.net)\n//\tJ\u00e9r\u00e9mie \"Mirne\" Gaslakov (mirne.gaslakov@gmail.com)\n//\tLluis Sanc"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\tif self.silent:\n\t\treturn list()\n\telse:\n\t\treturn self._silent_args.getarglist()\n\n\t\ndef get_user_args(self, args=None):\t\"\"\"\n\tlist of user defined arguments\n\n:param args: The received arguments.\n:return: list, slicted user defined argument names\t\"\"\"\n\tif self.add_user_args:\t\t###!\n\t\tself._add_user_args.reset_args()\t\t###!\n\t\tif args is None:\n\t\t\traise ValueError('"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\treturn self.opts['silent_args'].split()\n\t\n\t\n################################\ndef check_silent_call(self):\n\t\"\"\"\n\tcheck silent call on the server and apply appropriate options.\n\t\n\treturns:  bool\n\n:param args: The server arguments.\n:return: bool\n\t\"\"\"\n\tif(self.opts['oneshot']) < 10 < self.opts['silent_max_time'] :\n\t\t# Silent call \n\t\tif self.silenced_call <= 0 :\n\t\t\tprint('Silent command on server, please wait..')\n"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\tif not args[0] and not args[1] and not args[2] and not args[4]:\n\t\treturn []\n\n\tout = []\n\n\tfor a in args[0].split(\",\"):\n\t\ta = a.strip()\n\t\tif len(a.strip()) == 0:\n\t\t\tcontinue\n\t\tif ',' in a:\n\t\t\tfor s in a.split(','):\n\t\t\t\tout.append(s)\n\t\telse:\n\t\t\tout.append(a)\n\n\tfor a in args[1].split(\","}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\treturn self._silenced_args\n\n\t\ndef clear_silenced_args(self):\n\tself._silenced_args = []\n\n\ndef set_silent_args(self, name, silent_args, silent=True):\n\t\"\"\"\n\tChange the set of silenced arguments\n\n\t:param name: New name for the argument.\n\t\n\tParameter should never be a tuple.\n\n\t:param silent_args: The received arguments\n\tfor silent arguments (must be in this order).\n\n\t:param silent: If ``True`` sets the given silent parameters"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\tret = [] #list\n\tfor n in args:\t\t#list\n\t\tif n.startswith(\"--silent_\") and self.arg_name_is_silenced(n):\t#if is argument name\n\t\t\tret.append(n.split(\"--silent_\")[1])\n\tif not ret:\n\t\tself.log(\"Unknown parameters: %s\" % args)\n\t\tsys.exit(0)\n\t\treturn\n\treturn ret\n\nclass SubOptions(Options):\t\twith open(fn) as fp:\n\t__doc__ = \"An"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\tsli = set()\n\tfor i in args.keys():\n\t\tif isinstance(args[i], tuple):\n\t\t\tsli = sli | args[i]\n\t\telif isinstance(args[i], set):\n\t\t\tsli = sli | set(args[i])\n\t\telse:\t\n\t\t\tsli.add(i)\n\tret = []\n\tfor i in sli:\n\t\tret.append(i.strip())\n\treturn ret\n\n\t\ndef check_silent_args(self, args, silent_args,"}]}
{"task_id": "6306092d73426c38ae68ad07", "prompt": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tfor arg in args:\n\t\tif getattr(args, arg, None) is None:\n\t\t\traise ValueError('argument {0} not provided'.format(arg))\n\n\tif self.required_args.count('*') % 2!= 0:\n\t\traise ValueError('* required at least 2 arguments')\n\nimport { Component, ElementRef, ViewChild, AfterViewInit } from '@angular/core';\nimport { IonContent, IonItem } from 'ionic-angular';\nimport * as serviceWorker from '../services/service1';\nimport * as serviceWorkerModule"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tprovided_args = len(set(args))\n\tmissing_args = len(set(self.required_args))\n\tprint('missing_args', missing_args, 'provided_args', provided_args)\n\tif (provided_args == 0) or (missing_args == 0):\n\t\treturn False\n\telif (provided_args!= missing_args):\n\t\tprint('# needed arguments: %s' % ', '.join(list(set(self.required_args)) - set(args)))\n\t\tprint('# missing arguments:  %s' % ', '."}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\trequired_args, msg = [], []\n\tif args is not None and len(args)!= 0:\n\t\trequired_args = self.required_args[:]\n\t\tfor arg in args:\n\t\t\tif required_args and arg not in required_args:\n\t\t\t\tmsg.append(arg + \" is a required arg in \" + self.get_name())\n\t\t\telse:\n\t\t\t\tbreak\n\treturn msg\n\t\t\n\nclass Arguments(object):\n\t\"\"\"\n\tCommon class for arguments (arguments that appear in the arguments list in order to give more information about them"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\terrors = []\n\t\terrors.append(\"Requires arguments for the following functions should be provided:\\n\\n\\n\")\n\t\tfor name, function_name in self._function_names.items():\n\t\t\tif name not in args and not args.get('func', None) in function_name:\n\t\t\t\terrors.append(\"Provided {} argument is not present in the function name specified.\\n\\n\".format(name))\n\t\treturn errors\n\t\n\tdef validate_required_args(self, required_args):\n\t\t\"\"\"\n\t\tCheck if any of the"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tdef error(message):\n\t\traise AnsibleError(\"%s (%s): Requires arg(s) = %s are invalid\" % (\n\t\t\tself.name,\n\t\t\tself.version,\n\t\t\tself._format_args(args),\n\t\t))\n\n\tfor arg in args:\n\t\tif not(hasattr(arg,'requires_argument') or hasattr(arg, 'default')):\n\t\t\terror(\"Missing Requires argument to a required argument.\")\n\t\telif arg.requires_argument is None:\n\t\t\t# Can not do this check"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t'''\n\tvalidate_arguments(self, args, ['project_id', 'namespace'])\n\n\tdef execute(self, args) :\n\n\t\tclient = flaskbb.login                                  # login and create the app\n\n\t\tif args.project_id is None:\n\t\t\tpage = int(args.page)\n\t\t\tproject_id = client.find_project(page=page)\n\t\t\tproject_id ="}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tfor args, kwargs in self.required_args.items():\n\t\tassert'requires_' not in kwargs\n\nimport { ComponentFixture, TestBed, waitForAsync } from '@angular/core/testing';\n\nimport { TranslateModule, TranslateService } from '@ngx-translate/core';\nimport { HttpClientTestingModule, HttpTestingController } from '@angular/common/http/testing';\n\nimport { ProjectSummaryComponent } from './project-summary.component';\nimport { MockedUrlRewriterService } from '../../rewrites/mocked-url-rewriter.service';\nimport { SettingsService"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tfor arg in args:\n\t\tif not arg:\n\t\t\traise ParameterError(\"Please specify the value of <\" + self.__class__.__name__ + \">.{option}\")\n\n\ndef run_validation(args):\n\t# First check if there is at least one argument passed\n\toptions = args if args else sys.argv[1:]\n\tfor option in options:\n\t\t# If not found, print help and exit\n\t\tif not os.path.exists(option):\n\t\t\tprint(\"ERROR: The required argument: <\" + self.__class__.__name__"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tassert len(args) > 0\n\terrors = []\n\tfor i in range(len(args)):\n\t\tassert(args[i] is not None and args[i]!= '')\n\t\tif args[i] is not None:\n\t\t\tif not is_valid_version(args[i]):\n\t\t\t\terrors.append('%d: invalid version \"%s\". %s' % (i, args[i], self.name))\n\t\t\t\tcontinue\n\t\t\tif not version_compatible(args[i], args[i-1], strict=False):\n\t\t\t\t"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\targs_provided = []\n\tfor arg in self.required_args:\n\t\ttry:\n\t\t\tif not self.has_attr(arg):\n\t\t\t\traise ValueError(\"{} is not part of the base class {}\"\n\t\t\t\t\t\t  .format(self.name_args[arg], arg))\n\t\texcept KeyError:\n\t\t\targs_provided.append(arg)\n\t# If all required arguments have been provided, validate the argument class\n\tif args_provided == args:\n\t\treturn\n\n\tself.has_attr('error', _(\"Missing\"))\n\t"}]}
{"task_id": "6306092d73426c38ae68ad08", "prompt": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\treturn self._match_options_of_command_with_arg_spec(options_spec,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t  (\"--%s\" % command_name,),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t  args)\n\n/**\n * Square Connect API\n * Client library for accessing the Square Connect APIs\n *\n * OpenAPI spec version: 2.0\n * Contact: developers@squareup.com\n *\n * NOTE: This class is auto generated by the swagger code generator program.\n * https://github.com/swagger-api/swagger-codegen.git\n *\n * Swagger Codegen version: 2.3."}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t# TODO better solution\n\tallowed_args_match = []\t# type: list\n\tfor options_spec in options_spec:\n\t\tif options_spec[0] not in ALLOWED_ARGS_FOR_OPTION:\n\t\t\traise RuntimeError(\"Unknown \" + options_spec[0] + \", expected an allowed option in [ \" + command_name + \" ].\")\n\t\toptions_spec.remove(options_spec[0])\n\t\tallowed_args = options_spec[1]\n\t\tallowed_args_match.extend(find_matching_args_with"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\ttry:\n\t\tfor required_when, optional in options_spec:\n\t\t\tif required_when == argparse.SUPPRESS:\n\t\t\t\tmessage = \"The --%s argument requires a value when the command name is matched.\" % command_name\n\t\t\t\traise argparse.ArgumentError(message)\n\t\t\t# noinspection PyBroadException\n\t\t\ttry:\n\t\t\t\targs_when[required_when]\n\t\t\texcept TypeError:  # pragma: no cover\n\t\t\t\tmessage = \"Unsupported keyword argument '%s' specified in options --%s.\" % (required_when, command"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t_ = \"List arguments with ``required_when()`` condition matched.\"\n\tif not options_spec.required_when:\n\t\traise CommandError(_get_conditionally_required_args.__doc__)\n\tret = _filter_args(command_name, options_spec, args)\n\treturn [n for n in ret if n.required_when]\n\n\ndef _filter_args(command_name, options_spec, args, prefix_args=None, separator=\"|\"):\n\t\"\"\"\n\tThis filter only apply to one of command argument\n\t\n\t:param command"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\trequired_args_list = self._option_spec_filter_value_without_default(\n\t\toptions_spec, command_name,'required_when',  required_args_list\n\t)\n\treturn required_args_list\n\n\ndef _replace_option_spec_with_empty_optionset(self,\t\tcommands_spec,\targument_name,\t\t*args, option_spec, options)\t:\n\n\tif not self.settings['option_specs']\n\t\treturn commands_spec, argument_name, args\n\n\tresult ="}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\tpass\n\ndef is_class(self, obj):\n\tprint_obj(obj)\n\tif obj is None:\n\t\treturn False\n\treturn isinstance(obj, type) and issubclass(obj, BaseObj)\n\nclass __Meta(type):\n\t\"\"\" meta class for ``Meta`` interface \"\"\"\n\t# The metaclass cannot be subclassed by subclasses\n\tpass\n\nclass Meta(object):\n\t\"\"\" Meta class for ``Meta`` \"\"\"\n\t__metaclass__ = Meta\n\n\ndef run(command_name, options_spec=None, exit_"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\targs = [\"args\"]\n\n\tif command_name == \"version\":\n\t\treturn [args[0].split(\":\")[0]]\n\n\tif command_name =='reload':\n\t\treturn [_get_from_dict(opts, opts[key], \"new\") for key in kwargs_to_args(kwargs, args, 'new')]\n\n\n\tif command_name == 'up':\n\t\treturn [_get_from_dict(opts, opts[key], \"up\") for key in kwargs_to_args(kwargs, args, 'up')]\n\n\n"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\tif options_spec.required_when is None and\toptions_spec.nargs < 1 then\n\t\tcommand_name = self.get_argument_name_from_options(args, command_name, options_spec)\n\t\tcommand_name = self.resolve_command_name_from_options(args, command_name, options_spec)\n\t\targs.pop()\n\t\toptions_spec = self.remove_option_names_from_spec(options_spec, command_name)\n\n\t# no arg -> no change\n\telif options_spec."}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\tif options_spec is None:\n\t\treturn []\t\t# List.\n\n\t# Extract and process options.\n\toptions = {}\t# Argument spec.\n\tfor name, spec in options_spec:\n\t\t# Extract all optional argumne.\n\t\tif name == 'optional':\n\t\t\tfor option in spec:\n\t\t\t\toptions[option] = spec[option]\t# Argument type.\n\n\treturn _get_required_args(command_name, self.parser._parser_list, [],\n\t\tspec, self._get_command_"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\treturn [command_name, options_spec, options_spec.get('required_when')]\n\n\t#\n\t# if you don't like this function, but rather some sort of do_filter_function\n\t#\n\t# def _has_args_with_required_match(self, options_spec):\n\t#     return not any(\t'only_when' in options_spec\n\t#                     for option_info in itertools.chain(\n\t#  "}]}
{"task_id": "6306092e73426c38ae68ad09", "prompt": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif len(args) < self.length:\n\t\tself.error = 'The %s field must be shorter or equal to %d characters' % (str(self.type), self.length)\n\t\treturn False\n\treturn True\n\n\n\nclass _ValueTooLong(_ValueError):\t'''Value is longer than the allowed maximum.\n\tUsed in cases where the value of the field is longer than allowed maximum.\n\n:param args: The received arguments.\n\t'''\n\n\tif _value_too_long in (ValueError,) or \\\n\ttype(ValueError)==str"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif len(args) > len(self.args):\n\t\treturn args\n\telse:\n\t\treturn self.args\n\nclass CheckValueAction(object):\n\t#\tName of the check function.\n\tdef Name(self):\n\t\treturn CheckName\n\n\t#\tDefine the type of the check function.\n\tdef Type(self):\n\t\treturn CheckType\n\n\tdef __init__(self, name, type):\n\t\t#  Assign a name of the check object.\n\t\tself.name = name\n\t\t#  Define what the"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif self.maxlen is None:\n\t\tmsg = \"not enough arguments!\"\n\t\tres = {'type': 'Invalid',\n\t\t\t'value': msg}\n\t\treturn res\n\tif len(args) <= self.maxlen:\n\t\treturn {\"type\": \"Invalid\",\n\t\t\t\"value\": \"Not enough for the'maxlen' argument\"}\n\treturn json.dumps({'message': f'value is {len(args)} not {self.maxlen},'+\n\t\t\t'for this argument' + f','+\n\t\t\t'you should choose another'+"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif len(args) > (len(self.args)):\t\t# if length is incorrect.\n\t\traise TypeError('$length must be <= %s$: %s' % (len(self.args),type(args).__name__))\n\telse:\n\t\tprint('validating value of ', type(args).__name__, args[0])\n\t\tif args[0] == None:\n\t\t\traise TypeError('$null must be given as a value for $length')\n\t\tif len(args) > 1:  # make sure it isn't empty, raise Exception if not"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif args['length'] == None or args['length'] <= 0:\n\t\traise ValueError('length of values must be greater than zero.')\n\n/**\n  ******************************************************************************\n  * @file    stm32f7xx_hal_comp.c\n  * @author  MCD Application Team\n  * @version V1.1.0\n  * @date    12-May-2017\n  * @brief   COMP HAL module driver.\n  *          This file provides firmware functions to manage the following\n  *    "}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tfor arg in args:\n\t\tif len(arg) > 255:\n\t\t\traise UserError(_(\"Argument length is more than 100 characters.\"))\n\t\telif len(arg) < 3:\n\t\t\traise UserError(_(\"Argument length should be 3 characters or less.\"))\n\treturn args\n\ndef check_user_args(self):\n\t\"\"\"Check the allowed user arguments and raise exception if not.\n\n:return: No. of arguments and corresponding values.\n\"\"\"\n\tuser_arguments = self.user[\"user_args\"]\n\t# Checks of not all arguments are allowed"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif self.length == args[0] - 1:\treturn\n\tself.length\n=end\n\n#===============================================================================\n# validate_length_kwargs\n#-------------------------------------------------------------------------------\nclass validate_length_kwargs(Validator):\n\n\tdef validate_length_kwargs(self, kwargs):\n\t\t\"\"\"\n\t\tCheck if value in keyword-arguments is not longer than length specified.\n\n:param kwargs:  The received, preprocessed arguments.\t\"\"\"\n\t\tif len(kwargs)!= len(self.args):\n\t\t\tself.error(\"The length specified is greater not equal"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tassert self['length'] < len(args), 'Length should be greater than or equal to %s and less than %s.' % (len(args), self['length'])\n\ndefine(['./mapc5', '../core', './array', './_array-iterate', './_array-reduce'], function(mapc5, core, array, _arrayReduce){\n  // http://jsperf.com/to-v-fnv/5\n  return function(array, iteratee, accumulator, result, keys){\n    var index = -1,\n     "}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif len(args) == 0:\n\t\traise Sorry((\"Invalid parameter (no value given).\"))\n\tif(args[0] == ''):\t\t\t\t\t\n\t\traise Sorry((\"Invalid parameter (no value given).\"))\n\n\n@utils.multicase()\nclass _Value(object):\n\t\"\"\"\n\tInternal value object. This object represents the parsed value which is\n\tavailable from configuration\n\t\"\"\"\n\t# FIXME what is this for? value\n\t\n\tdef __init__(self, value=None):\n\t\t\"\"\" Create an object. \"\"\"\n\t\tself"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif len(args) > len(self.args_length):\treturn False\n\treturn True\n\n\nclass Test_str_args(unittest.TestCase):\n\t'''\n\t>>> self = Test_str_args()\n\t>>> self.assertEqual('hi', str(self.f, 'hi'))\n\tTrue\n\t'''\n\n\tdef test_str(self):\n\t\t'''\n\t\tThis function will check if str method is working properly.\n\n\t\t>>> s = str('hi')\n\t\t>>> self.assertEqual('hi', s)\n\t\tTrue"}]}
{"task_id": "6306092e73426c38ae68ad0a", "prompt": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tself.choices = args[0]\n\t\tfor key in self.choices:\n\t\t\tif 0 not in self.choices[key]:\n\t\t\t\tprint(\"Invalid default choice: %s\" % key)\n\t\t\tprint(\"This is a default choice!\")\n\t\treturn True\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tchoices = []\n\tfor arg in args:\n\t\tif not isinstance(arg, str):\treturn\n\t\tif not any(arg.startswith(value) for value in CHOICES):\n\t\t\tchoices.append(arg)\n\tif len(choices):\n\t\tchoices = [arg[len(value):] for arg in choices]\n\t\tchoices = filter(lambda c: not c.startswith(value), choices)\n\t\tchoices = filter(lambda c: c.startswith(value), choices)\n\treturn choices\n\n\n#-----------------\t\t--\t"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tfor _argument in self.choices.get_choices():\t# Get the first valid choice\n\t\tif args[0] in _argument.get_choices():\t\t# If all arguments are available, valid choice, continue.\n\t\t\treturn\t\t\t# Return the user input\n\n\t\tif args[0] not in _argument.get_choices():\t\t# If all arguments are not available, return input not valid.\n\t\t\treturn \t\t\t# Return user input as invalid\n\n\nclass SelectField(FieldSet):\n\tdef __init__(self, name, *args):\n"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tif args:\n\t\tfor choice in self.choices:\n\t\t\tif type(args[0]) is not type(choice):\n\t\t\t\tdel args[0]\n\t\targs = [x for x in args if type(x) is str and x[0] in choices_dict[choices_dict[x][1]]]\n\t\tif len(args):\n\t\t\treturn args\n\n\treturn []\n\nclass choices_dict(dict):\n\t\"\"\"\n\tDescribes the available choices of the given type.\n\n:param type t: The type of choices to"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tif len(args['choices']) == 0:\n\t\thelp('A choice is required or a choice argument is not a list.')\n\t\treturn\n\tfor element in args['choices']:\n\t\tif isinstance(element, types.StringType) and len(element):\n\t\t\telif isinstance(element, types.ListType):\n\t\t\t\tif any(element.item(0)!= self._config.get(x, False) for x in [None, 'default']):\n\t\t\t\t\tcontinue\n\t\t\t\telif 'default' in element:\n\t\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\ttry:\n\t\tChoice.validate(str(args.choices), self.session)\n\texcept ValidationError:\n\t\treturn\n\n\tif hasattr(self.choices,'reset') and self.choices.reset:\n\t\tself.choices.reset(self.session)\n\n\tlog.debug('reset() for %s', self)\n\n\nclass Command_set_attribute(Command_set, Command):\n\t\"\"\"\n:type command: :class:`gam <zmq.base.Command>`\n\n:class:`set` command.\n\nSet a single attribute"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tif not self.app.config.allow_blank_choices and not set([x for x in args if x == 'blank'\n\t\tor x in self.app.config.allowed_choices]).issubset(set(self.app.config.allowed_choices)):\n\t\traise errors.ValidationError(\n\t\t\t'\"%s\" is not one of the allowable choices' % \", \".join(['\"%s\"' % (x, getattr(self.app, x)) for x in self.app.config.allowed_choices]))\n\n\nclass CheckUniqueObject:\n\n\tdef"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tif self.choices.has_key(args[\"value\"]):\n\t\treturn args[\"value\"]\n\telse:\n\t\treturn \"not set\"\n\n\t#if not self.choices.has_key(args[\"value\"]):\n\t#\traise ValueError(\"not set in %s\" % repr(self))\n\t#else:\n\t\t#return args[\"value\"]\n\ndef set_choice_args(self, args):\t\"\"\"\n\tReplace the choice arguments of a choice with the corresponding value.\n\n:param args: The received arguments.\n\"\"\"\n\t"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tif args.get_choice_arguments():\n\t\t# Check if the arguments are valid choice arguments\n\t\tfor arg in args.get_choice_arguments():\n\t\t\t# Check if we found a valid choice value\n\t\t\tchoice_value = args.get_choice_value(arg)\n\t\t\tif not choice_value:\n\t\t\t\tself.abort(500, 'Invalid choice value')\n\t\t# Check if the arguments are supported\n\t\tfor arg in args.get_supported_choices():\n\t\t\tif not self.get_option('option-'+arg).value"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tchoices = {  \n\t\t'all': ['all', 'True'],\n\t\t'none': ['all', 'False'],\n\t\t'key': ['key', 'KeyboardEvent'],\n\t\t'mouse': [\n\t\t\t'left': ['mouse', 'left'],\n\t\t\t'mouse': ['right','mouse'],\n\t\t]\n\t}\n\tdef find_choice(self, name):\n\t\t\"\"\"\n\t\tSearch for a name in available combinations.\n\n\t\t:param name: The input name.\n\t\t:type name: str\n\t\t"}]}
{"task_id": "6306092e73426c38ae68ad0b", "prompt": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif isinstance(args, str):\n\t\ttry:\n\t\t\tvalue = int(args)\n\t\texcept Exception:\n\t\t\traise RuntimeError(\"Expected integer value for the maximum argument. Received: %s\"\n\t\t\t\t\t\t% args)\n\t\tif (not (-1 <= value <= 1)):\n\t\t\traise RuntimeError(\"Invalid argument for the minimum argument. Received: %s\"\n\t\t\t\t\t% args)\n\telse:\n\t\traise TypeError(\"Argument to validate_min_max_args is not an accepted type.\")\n\n\treturn value\n\n\ndef handle_config_options"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif len(args) == 2:\n\t\tvalue1_ = args[0]\n\t\tvalue2_ = args[1]\n\t\tif value1_ < value2_:\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\n\n\treturn False\n\t\ndef execute(self):\n\n\tif self.validate(self.inputs['Parameter1'], self.inputs['Minimum']):\n\t\tself.outputs['Output1'] = float(str(self.inputs['Parameter1']) + ',' + str(self.inputs['Minimum']))\n\n\t"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif not self.min_max_args():\n\t\t\tprint \"Please supply at least one argument to the command\"\n\t\t\tsys.exit(0)\n\t\t\t\n\t\tif args.min_max is None:\n\t\t\tprint \"Please supply a minimum and maximum values to the command\"\n\t\t\tsys.exit(0)\n\t\t\t\n\t\tif args.min_max < args.min_max_val:\n\t\t\tprint \"The received minimum and maximum value is less then '%s'. Please supply minimum and maximum separated by comma.\" % args.min_max_val\n\t\t"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tself.validate_minimum_maximum(args)\n\n\nclass ValidateStringField(CommandLineValidation):\n\t\"\"\"Validate string field\"\"\"\n\n\tdef __init__(self, message='Field cannot be empty', exception=InvalidFieldException, validators=()):\t\tself.validators = validators\n\t\tself.invalid_message = message + exception.description\n\t\tsuper(CommandLineValidation, self).__init__(self.invalid_message)\n\n\tdef validate(self, value):\n\t\tif isinstance(value, basestring):\n\t\t\tif len(value) > 63:\n"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif'min' in args and args['min'] <= args['max']:\n\t\treturn True\n\telif'max' in args and args['max'] > args['min']:\n\t\treturn True\n\treturn False\n\n/*! ******************************************************************************\n *\n * Pentaho Data Integration\n *\n * Copyright (C) 2002-2017 by Hitachi Vantara : http://www.pentaho.com\n *\n *******************************************************************************\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with\n * the License. You"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif args.max_value!= args.min_value:\n\t\traise ArgumentsInvalidError(\"The'max_value' and'min_value' \" +\n\t\t                           \"values should have been defined as \" +\n\t\t                           \"either between (> and <=) or as equal to \" +\n\t\t               "}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t# Check if arguments are integers between minimum and maximum\n\tif isinteger(args[0]) and isinteger(args[1]):\n\t\treturn ('min_val','max_val')\n\n\t# Check if arguments are strings between minimum and maximum\n\t# (must use case-insensitive comparison)\n\telif isinstance(args[0], str) and isinstance(args[1], str):\n\t\treturn ('min_val','max_val')\n\n\treturn ('min_val and max_val')\n\ndef parse_value(args):\t\"\"\"\n\tParse arguments"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif args[0] in self.min_args and args[1] in self.max_args: return args\n\n\t# Default error message if min/max value not found in received arguments\n\targs_names = [n for n in inspect.getfullargspec(self.object.__init__).args[1:] if n[-1].startswith('min_max_') and self.args.get(n[-1], 'not in args')] \t\t#'min_max_not_in_args' string\n\tif args_names:\n\t\tname, value = args"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif args.get_type()!= \"\":\n\t\treturn \"The received arguments is required\"\n\telse:\t\t# If args not set it should return a new string\n\t\treturn args.get_string().get_value()\n\nclass MinMax(Validator):\n\t\"\"\"\n\tA validator for MinMax() class\n\t\n\tParameters:\n\t* dataValue - The data value which contains the min max value\t\n\t* dataSet - The set which contains the MinMaxValues objects\n\t* dataString - The string which contains the min max value\t\n\t\"\"\"\n\n\t#"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif len(args) == 0:\n\t\tself.logger.error('At least one parameter is required. Did you misspell the parameter name?')\n\t\traise self.exception\n\n\tif len(args) > 4:\n\t\tself.logger.error('Too many parameters were supplied. Did you misspell the parameter name?')\n\t\traise self.exception\n\n\tif (not args[0][0].isdigit()):\n\t\tself.logger.error('Parameter0 must be an integer value.')\n\t\traise self.exception\n\n\tfor p in args"}]}
{"task_id": "6306092e73426c38ae68ad0d", "prompt": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t", "suggestions": [{"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\ttry:\n\t\tsubcommand_class = self.command_class_by_name(subcommand, type_name=type_name)\n\texcept commandparser.Error:\n\t\traise commandparser.Error(\n\t\t\tu\"{} subcommand '{}' not found\".format(\n\t\t\t\tself.style_cmd(), subcommand\n\t\t\t)\n\t\t)\n\n\tif self.option.required == False:\n\t\ttype_spec = {}\n\t\tif spec_option.required is True:\n\t\t\ttype_spec[\"required\"] = True\n\t\t"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\tsubcommand = os.path.splitext(subcommand)[0]\n\tif type_name == '':\n\t\ttype_name ='self' + '_' + subcommand\n\treturn getattr(self, 'create_{0}_type'.format(type_name))\n\n\t\ndef get_complex_argumets_and_arguments_by_kind(*,\n\tcommand_name, kind,\n\tdefault_value=None,\n\toption_name='',\n\toption_spec=[],\n\tcommand_spec=[],\n\tspec_option=None,\n):\n\t"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\tif not self.subcommands: self.add_command( subcommand, option_name )\n\treturn get_complex_type( self, subcommand, type_name )\n\ndef create_enum_type(self, subcommand, value, \"\"\"\n\tBuild the enum type\n\n\t:param subcommand: the command name\n\t:param value: the enum value\t\"\"\"\n\tif not self.subcommands: self.add_command( subcommand, 'enum' )\n\treturn EnumType( self, subcommand, value )\n\n\ndef get_complex_array_"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\tdef call_(self, **kwargs):\n\t\ttry:\n\t\t\tcomplex_type = self._build_complex_type(option_name, option)\n\t\t\targs = {'subcommand': subcommand}\n\t\t\targs[type_name] = self._get_complex_type_instance(complex_type)\n\n\t\t\tif self.arg_type is not None:\n\t\t\t\ttype_instance = self._get_complex_type_instance(complex_type)  # type: ignore\n\t\t\t\targs.update({'type': type_instance.type_argumet"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\tresult=None\n\tif subcommand.is_empty():\n\t\tprint \"Not found subcommand:%s\" % subcommand\n\t\treturn\n\t# create the argument type\n\tresult=[]\n\tself.argument_types = {}\n\tself.argument_types.update(result)\n\tif subcommand in self.argument_types:\n\t\treturn self.argument_types[subcommand][type_name]\n\n\targ_type=getattr(getattr(getattr(bpy.types, subcommand), \"_ARG_TYPE\"), option_name)\n\tif arg_"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\tif subcommand not in self.subcommands:\n\t\treturn None\n\telif subcommand <= 'all':\n\t\tif self.subcommands[subcommand].get('short_doc') is True:\n\t\t\tdoc = self._format_doc_for_subcommand(subcommand)\n\t\t\treturn None, doc\n\t\telse:\n\t\t\treturn self.subcommands[subcommand], None\n\telif subcommand == 'all-all':\n\t\treturn self.subcommands['all'], None\n\telse:\n\t\ttry:\n\t\t\tcmd_parser ="}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\tfrom comtypes import *\n\tif hasattr(subcommand, \"option\"):\n\t\tsubcommand_options = subcommand.option\n\t\tsubcommand_opt_names = subcommand_options.__class__.option_names\n\t\tif hasattr(subcommand_options, \"has_multiple_values\"):\n\t\t\t# This option exists in the command as opposed to the command itself\n\t\t\targs = subcommand_opt_names[subcommand] if len(subcommand_opt_names) == 1 else [\"{} is multiple.\".format(subcommand)]\n\t\t\targs.extend([type"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\tfrom pymel.lib.subcommand import SubCommand\n    from pymel.util.subcommand_parser import ArgumentParserException\n    command_class = SubCommand.by_name(subcommand)\n    if command_class:\n        command = command_class(self.log, self.verbose)\n    parser = ArgumentParser(subcommand, description=description, prog=self.prog, help=help, add_help=False)\n    # Add optional arguments if we need to\n    # -"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\tsubcommand.print_help()\n\treturn None\n\n@register(subcommand='--complex-argument-type',\n\tname='create_complex_type_or_subcommand')\ndef create_complex_subcommand(self, subcommand, option_name,\t\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\tsubcommand.print_help()\n\treturn None\n\n@register(name"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t);\n\tdef create_compound_argumet_type(self, subcommand, type_name,\t\"\"\"\n\tBuild the compound argument type\n\n\t:param subcommand: the command name\n:param type_name: the compound type name\n:param option_name: the compound option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t);\n\tdef create_list_argumet_type(self, subcommand, type_name,\t\"\"\"\n\tBuild the element of a list type\n\n:param subcommand:"}]}
{"task_id": "6306092e73426c38ae68ad0f", "prompt": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\treturn split_kwargs(get_arg_list(args))\n\nclass YmlFileSpecRunner(YmlFileSpecRunnerMixin, object):\n\t\"\"\"Run a spec.  Used by the Ansible playbooks and scripts\n\n:example:\n\nRun a playbook spec, create the generated yaml file, and print it.\n>>> runner = YmlFileSpecRunner()\n>>> runner.run_spec(spec)\n\"\"\"\n\tdef __init__(self, config):\n\t\t# TODO: Implement __init__ method.\n\t\tYmlFileSpecRunnerMixin.__init__("}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\tc = {}\n\tc[\"control_args\"] = args._c\n\tc[\"nested_args\"] = args._o\n\n\tret = {\"control_vars\": self.control_args,\\\n\t\t\"nested_dict\": self.nested_args,\\\n\t\t\"custom\": self.custom}\n\treturn ret\n\n\ndef main():\n\n\tcmd_str = \" \".join(sys.argv)\n\n\tlog.debug(\"-\" * 40)\n\n\tif not os.path.exists(config.YML_SPEC_YML_PATH):\n\t"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\tcontrol_args = {'yum_config':{},'yum_conf_cache':{},'global_config':{},'yum_cache_url': None,'yum_cache_dir': None, 'yum_db_dir': None,'remote_control_addr': '', 'log_path': None}\n\tcustom_args = {'yum_vars_json_data': {}, 'yum_conf_cache':{'data': {}}, 'yum_vars_custom': {}, 'global_config':{'data:{}'}, 'yum_cache_url': None"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\targs['control_args'] = args_to_nested_args_and_vars(self.parsed_opts, args['control_args'])\n\targs['nested_args'] = args_to_nested_args_and_vars(self.parsed_opts, args['nested_args'])\n\treturn args\n\n\n\t\n#-----------------------------------------------------------------------------\n#\tUtility\n#-----------------------------------------------------------------------------\ndef args_to_nested_args_and_vars(parsed_options, args):\n\tnested_args = {}\n\tcontrol_args = []\n\targs_to_control(parsed_"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\tcontrol_args = OrderedDict()\n\tnested_args = Dict()\n\tcontrol_only = self.get_control_only_args()\n\tcontrol_args['control_only'] = control_only\n\tnested_args['control_only'] = control_only\n\tself.set_nested_custom_only_args(nested_args)\n\tself.nested_args = nested_args\n\treturn control_args, nested_args\n\n\ndef get_nested_only_args(self):\n\t\"\"\"\n\tAdd in the nested_only_args from the parent or"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\tnested_args = dict()\n\tcontrol_args = dict()\n\targs_for_nested = []\n\tfor i in range(len(args)):\n\t\tif args[i] in self.NESTED_KEYS:\n\t\t\tnested_args.update(args[i])\n\t\telse:\n\t\t\targs_for_nested.append(args[i])\n\tcontrol_args = self.get_control_args(args_for_nested)\n\treturn (nested_args, control_args)\n\n\nclass Config(object):\n\tdef __init__("}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\tcontrol_args = dict()\n\tnested_args = dict()\n\t# Get control args, to put in place of default control arguments\n\tif not args and not self.def_control_args:\n\t\tcontrol_args.update(dict(self.def_controlArgs))\n\t\treturn control_args, None\n\t# If there are args\n\tif args and not self.def_control_args:\n\t\tself.def_control_args = args\n\t\tcontrol_args.update(args)\n\t\treturn control_args, None\n\t# If"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\tcontrol_args = {}\t# only used for Ansible playbooks\n\tif self.action == 'playbook' or self.action == 'playbook-custom':\n\t\tcontrol_args = self.generate_control_args(args.get('control',''))\n\t\tcustom_args = dict(args['control']['__custom_args'])\n\t\tall_control_args = control_args.copy()\n\t\tall_control_args.update(custom_args)\n\t\t# all_args.update(custom_args) # use custom args to reduce size"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\tdef get_nested_var_as_dict(args):\n\t\t\"\"\"\n\t\tReturn flat dict of key/value pairs (control args, nested args).\n\n\t\t:param args: the args that need to be converted into\n\t\t    a format such as dict\n\t\t\t{\n\t\t\t\"run_uuid\": \"some uuid\",\n\t\t\t\"ansible_playbook\": {\n\t\t\t\"nested_arg\": {}\n\t\t    }\n\t\t    \"ansible_playbook\": {\n\t\t    }\n\t\t\t}\n\n\t\t:return"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\tresult = {}\n\tnested_result = {}\n\tcontrol_args = []\n\tnested_args = []\n\t\n\tglobal args\n\targs = args + self.args()\n\tif   self.options.nest_control_args:\n\t\tcontrol_args = [args[0]]\n\tif   self.options.nest_nested_args:\n\t\tnested_args = self.get_nested_args(args[1:])\n\tif   self.options.nest_custom:\n\t\tcontrol_args += [c.to_dict() for"}]}
{"task_id": "6306092e73426c38ae68ad11", "prompt": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t", "suggestions": [{"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tnew_vars = {}\n\tfor var in vars_dict:\n\t\tif extra_vars:\n\t\t\tfor extra_var in extra_vars:\n\t\t\t\tnew_vars[extra_var] = vars_dict[var]\n\t\telse:\n\t\t\tnew_vars[var] = vars_dict[var]\n\tvars_dict = new_vars\n\t\t\n# TODO: This implementation does not exist in pyspread3, so need unit test.\ndef get_variables_from_template(variables, **kwargs):\n\ttemplates_map = {\n\t\t"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tfor var in vars_dict:\n\t\tif isinstance(var, list):\n\t\t\tfor new_var in extra_vars or vars_dict[var]:\n\t\t\t\tvars_dict[new_var] = vars_dict[var]\n\t\telif isinstance(var, str):\n\t\t\tif extra_vars is None or vars_dict[var].keys()!= extra_vars.keys():\n\t\t\t\tvars_dict[var] = {}\n\t\t\t\tfor key in extra_vars:\n\t\t\t\t\tvars_dict[var][key] = extra_vars[key]\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tglobal EXTRA_VARS, MODIFIED_VARS\n\n\tif EXTRA_VARS is None:\n\t\tEXTRA_VARS = vars_dict.copy()\n\t\tvars_dict.update(extra_vars)\n\n\tif MODIFIED_VARS is None:\n\t\tMODIFIED_VARS = vars_dict.copy()\n\t\tfor k, v in EXTRA_VARS.items():\n\t\t\tif isinstance(v, str):\n\t\t\t\tv = v.encode(\"latin1\")\n\t\t\tif isinstance(v, list):\n\t\t\t\tv = v[:]\n\t\t\tfor extra in"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tif len(vars_dict.keys()) == 0:\n\t\treturn vars_dict\n\n\tif extra_vars is None:\n\t\textra_vars = []\n\n\tvars_to_copy = list(vars_dict.keys())\n\tfor i, (name, value) in enumerate(vars_dict.items()):\n\t\tvars_to_copy[i] = name\n\t\tvars_to_copy[i] += value\n\n\treturn vars_to_copy\n\n/*\n * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tif extra_vars is not None:\n\t\tvars_dict.update(extra_vars)\n\treturn vars_dict\n\n\ndef merge_list(vars_dict, extra_vars=None):\n\t\"\"\"\n\tMerge ``vars_dict`` with a single value.\n\n\t:param vars_dict: Dictionary to merge vars into\n\t:param extra_vars: List of extra-vars\t\"\"\"\n\tvars_dict[vars_dict.var_name] = vars_dict.val\n\treturn vars_dict\n\n\ndef get_param_value(var,"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tfor name, v in vars_dict.items():\n\n\t\t# Don't save the extra-vars in the result of the merge\n\t\tif not v.startswith('-') and name not in vars_dict:\n\t\t\tif extra_vars is not None:\n\t\t\t\textra_vars[name] = v\n\n\t\telse:\n\t\t\tif extra_vars is not None:\n\t\t\t\tv = v.capitalize()\n\t\t\t\textra_vars[name] = v\n\t\tvars_dict[name] = v\n\ndef set_default_template_values"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tif extra_vars is None:\textra_vars = vars_dict\n\n\tfor var in extra_vars:\n\t\tif var in vars_dict.get(var):\n\t\t\tvars_dict[var] += extra_vars[var]\n\t\telse:\n\t\t\tvars_dict[var] = extra_vars[var]\n\n\nclass _DefaultEnvironment(object):\n\t\"\"\"Default environment to be used by ``run_command()`` and\n\t``run_command_with_args()`` calls.\n\n\t:param envvars: A list of environment"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tfrom collections import OrderedDict\n\n\tif extra_vars is not None:\n\t\tvars_dict.update(extra_vars)\n\n\treturn vars_dict\n\n\ndef parse_command(args=None):\n\timport logging\n\tlogger = logging.getLogger(\"PyBlast\")\n\n\tfrom pyblast.cli import command as _command\n\t# Import this to prevent autoloading of these modules by 'pip install pyblast'\n\t# Note this is not necessary when using 'python package'\n\timport pyblast\n\tfrom pyblast.cli import command as"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tout = dict([(name.split(\"_\"),value) for name,value in vars_dict.items()])\n\tif extra_vars:\n\t\tout.update(extra_vars)\n\treturn out\n\n\ndef add_extra_vars(obj, extra_vars):\n\t\"\"\"Adds extra-vars to ``obj``\n\n:param obj: Target object to add extra-vars into\n:param extra_vars: List of extra-vars to add into ``obj``\"\"\"\n\tfor name in extra_vars:\n\t\tv = extra_vars[name]\n\t\tobj"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tif extra_vars is None:\n\t\tvars_dict = vars_dict\n\t\tfor varname, var in vars_dict.items():\n\t\t\tif not isdefined(var):\n\t\t\t\tvars_dict[varname] = var\n\t\treturn vars_dict\n\n\tnum_to_merge = len(extra_vars) * 2\n\tfor vari, var in enumerate(extra_vars):\n\t\tvari *= 2\n\t\tif vari + 1 > num_to_merge:\n\t\t\textra_vars[vari + 1] = vars_"}]}
{"task_id": "6306092f73426c38ae68ad13", "prompt": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\tif not os.path.isfile(playbook_path):\n\t\traise ValueError(\"'playbook-path' must be an existing file\")\n\n\tansible_args = {} \n\tkwargs = {\n\t\t'dir': ir_workspace.rootdir,\n\t\t'module': ir_plugin.module_name,\n\t\t'use': ir_workspace.use_ansible_path,\n\t\t'sync': ir_workspace.use_ansible_synchronous,\n\t\t'stread': ir_workspace.use_ansible_thread,\n\t\t'"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t#\n\t# TODO:\n\t# \t- We should also make sure we don't try to execute a plugin at this\n\t#\t time on a package that doesn't exist.  This will have a problem:\n\t# \t- We need to check whether we are in a package that we've already\n\t#\t   executed.  This is because a plugin that has already been played\n\t#\t   has a different version than that of the target.\n\t#      (if a plugin tries to execute a package that it doesn't exist)\n\t#\t"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\t'''\n\t\t:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n\n    :param verbose: Ansible verbosity level\n    :type verbose: integer\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :type extra_vars: dict\n    :param ansible_args: dict of ansible-playbook arguments to"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t, extra_vars\n)\n\n\ndef get_path(plugin):\n\t\"\"\" Return the absolute path of the current path in a plugin. \"\"\"\n\treturn abs(plugin_path(plugin))\n\n\ndef normalize_path(path):\n\t\"\"\" Converts a Windows path to Unix-style paths. \"\"\"\n\treturn os.path.normpath(os.path.abspath(\n\t\tos.path.expanduser(path.replace('\\\\', os.path.sep)[:-1])))\n\n/**\n * Marlin 3D Printer Firmware\n * Copyright (c) 2019 MarlinFirmware [https"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\ttry:\n\t\t# We may have to convert the playbook into a ansible run list.\n\t\tplaybook = playbookparser.Playbook(ir_workspace, playbook_path).to_list()\n\texcept Exception as error:\n\t\tprint(\"\\n\\n[Infrared] [WARNING] Failed to parse Playbooks in the plugin: %s.\" % ir_plugin)\n\t\treturn\n\telse:\n\t\tprint(\"\\n\\n[Infrared] [WARNING] Playbooks contained: %s.\" % \" \".join(playbook))\n"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\tansib = AnsiblePlaybook(env=ir_workspace).playbook_path(ir_plugin, playbook_path)\n\tplayable = AnsiblePlayable(ansib)\n\tr = dict_to_ansible(playable, ir_workspace, ir_plugin, playbook_path, verbose=verbose-1)\n\treturn r\n\n#------------------------------------------------------------------------------\n#\n# Asks for an Infrared version of an Infrared plugin to generate.\n#\n#------------------------------------------------------------------------------\ndef ask_package_about(plugin, msg):\n\n\tpkg"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t)\n\tif ir_workspace.isroot():\n\t\tprint(\"Not running ansible-playbook with existing workspace \" + ir_workspace)\n\telse:\n\t\tcli_ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=verbose, extra_vars=extra_vars, ansible_args=ansible_args)\n\t\tif verbose:\n\t\t\tif ansible_playbook_called == 0:\n\t\t\t\tansible_playbook_called += 1\n\t\t\t\tprint(ansible_playbook_called, \""}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\t)\n\n\t\tif not os.path.isdir(ir_workspace.workdir):\n\t\t\tos.mkdir(ir_workspace.workdir)\n\n\t\tplaybook = ir_workspace.loader.load(playbook_path)\n\n\t\tif 'ansible-playbook' not in locals():\n\t\t\tlocals()['ansible-playbook'] = ansible_playbook\n\n\t\tif 'get_plugin' not in locals():\n\t\t\tlocals()['get_plugin'] = get_plugin\n\t\tvars = locals().get('ans"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t)\n\t\n\tplaybook = Playbook(ir_workspace, ir_plugin, playbook_path, verbose=verbose, args=ansible_args)\n\tansible_playbook(playbook, ir_plugin)\n\t\n\treturn playbook_path\n\ndef do_ansible_playbook_command_line(self, args):\n\t# Get the command line arguments ready.\n\targs = self.parse_options(args, exclude=['plugins'])\n\t\n\t# Ensure to remove any extra arguments.\n\t# The 'args' is passed on to the plugin-specific"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t):\n\targs = []\n\tif verbose is not None:\n\t\targs.append(\"--verbose=%s\" % verbose)\n\tif extra_vars is not None:\n\t\targs.append(extra_vars)\n\tif ansible_args is not None:\n\t\targs.append(ansible_args)\n\n\tplay_args = [\n\t\t\"%s --name=%s\" % (playbook_path, ir_plugin.name),\n\t] + args\n\n\ttry:\n\t\twith open(ir_workspace, \"r+\") as fh"}]}
{"task_id": "6306093273426c38ae68ad15", "prompt": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\trun_as_user = vars_dict.get(\"run_as_user\", False)\n\tif not run_as_user:\n\t\t\t\tansible = AnsiblePlugin(ir_plugin)\n\t\t\t\tansible.run_playbook(cli_args, vars_dict)\n\t\telse:\n\t\t\t\tansible = AnsibleWorkspace(ir_workspace)\n\t\t\t\tansible.run_playbook(cli_args, vars_dict)\n\nimport { Box, Input, InputGroup } from \"@socialgouv/cdtn-ui\";\nimport"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t# init vars\n\tvars_dict = vars_dict()\n\tcli_args = cli_args + ['--loglevel=debug']\n\n\t# test if the user is a plugin; else, just run the playbook function\n\ttest_plugin = ir_plugin\n\treturn_results(cli_args, test_plugin, ir_workspace)\n\n\ndef _run_playbookx(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRun playbookx on a given workspace and extra-vars\n\n:param cli_args"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\tansible_results = []\n\tansible_options = vars_dict.get('ansible_options') or ['-h']\n\targs = \" \".join(cli_args)\n\n\tif'setenv' in vars_dict:\n\t\tansible_options = ['-e', 'var_dir=value'] + AnsibleOptions.DEFAULT_CONFIG_VARS.replace(':','\\\\n') + AnsibleOptions.DEFAULT_CONFIG_VARS.replace('\\\\n',':')\n\tif not isinstance(vars_dict, str):\n\t\tansible_options"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\tansible = Ansible()\n\tpass_ansible = False\n\tif vars_dict is None:\n\t\tlogging.info('Annsible variables were not set. They seem to be valid, so we must set them individually to set the default variables to the plugin.')\n\t\tvars_dict = Plugin(ir_workspace, ir_plugin).getvars()\n\n\tansible_variables = {}\n\tfor variable in vars_dict:\n\t\tansible_variables[variable] = list()\n\t\tfor item in vars_dict[variable]:\n\t\t\tans"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t# Check if an Ansible plugin object was passed in\n\tif not isinstance(ir_plugin, InfraredPlugin):\n\t\t# If in a plugin\n\t\t# Raise an error and return the ansible results\n\n\t\t# Show what error is on the screen\n\t\tprint \"\\n\"\n\t\tprint \"An InfraredPlugin object of plugin '{}' is required.\".format(ir_plugin.__class__.__name__)\n\t\tprint \"Try using an object of the wrong type.\"\n\t\tprint \"\\n\"\n\t\tprint \"\"\n\t\tprint \"Use --"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\ttry:\n\t\t# we only have the cli plugin\n\t\tans_plugin = ir_plugin.get_infrastructure()\n\texcept:\n\t\tansible_log.error('No InfraredPlugin available...')\n\t\tsys.exit(1)\n\n\t# set infrared params\n\tvars = vars_dict.copy()\t# not necessary to duplicate settngs\n\tvars['profile'] =          # Set InFRP\n\tvars['workspace'] =        # Set Local workspace\n"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\timport datetime\n\tfrom ansible import Ansible\n\tfrom ansible.annsible import ansible_playbook\n\tfrom ansible.cli import formatargspec\n\n\t# get environment values\n\thost = vars_dict.get('host', ir_plugin.get_attribute('ansible_playbook', 'host'))\n\tport = vars_dict.get('port', ir_plugin.get_attribute('ansible_playbook', 'port'))\n\tcommand = vars_dict.get('command', 'playbook')\n\tuser = vars_dict."}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t# First we retrieve the user requested input and validate\n\ttry:\n\t\targs = _validate_command_line(cli_args)\n\texcept ValueError as e:\n\t\tprint(\"Invalid args \", args, file=sys.stderr)\n\t\tsys.exit(1)\n\n\t# get the input\n\tcli_args = args[:]\n\t_run_cmd(cli_args, vars_dict, ir_workspace)\n\n\treturn dict((k, vars_dict[k]) for k in vars_dict)\n\ndef _cli_run_command(cli"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\tresult_dict = {}\n\tworkspace = ir_workspace\n\tif 'ansible_plugins' in workspace.plugins:\n\t\tfor plugin in json.loads(workspace.plugins['ansible_plugins']):\n\t\t\tif 'ansible_plugins' not in workspace.plugins:\n\t\t\t\tworkspace.plugins['ansible_plugins'] = {}\n\t\tansible_plugins = workspace.plugins['ansible_plugins']\n\tif 'ansible_plugins' in workspace.plugins:\n\t\tfor plugin in json.loads(workspace.plugins['ansible_plugins']):\n"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t# we need a working copy\n\tws = ir_workspace.WorkingCopy(client_id=vars_dict.get('client_id', None))\n\n\t# the Ansible command to execute\n\tcmd = vars_dict.get('cmd', None)\n\n\t# run ansible command\n\tsuccess, cmd_process_stderr = ir_plugin.run_playbook('playbook-shell', cmd, ws)\n\n\t# handle ansible error and the output\n\tif not success:\t\tdie(cmd_process_stderr)\n\ndef create_playbook"}]}
{"task_id": "63060ada73426c38ae68ad31", "prompt": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tnew_cmd_parser = self._parsers[\"parser_name\"]\n\n\tfor attr, val in sorted(values_dict.items()):\n\t\tnew_cmd_parser.add_argument(\n\t\t\tattr, \n\t\t\t\"=\" if isinstance(val, str) else str, \n\t\t\t\" \".join(attr.replace(\"_cli_\", \"\") for attr in values_dict.keys()),\n\t\t\tmetavar=Metavar.type, type=str)\n\n\treturn values_dict\n\n\ndef _convert_cli_args(self, parser_name, values_"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tfor cmd, values in list(values_dict.items()):\n\t\tparser_name = getattr(parser_name, str)(cmd)\n\t\tfor aval in values:\n\t\t\tif hasattr(values[0], str):\n\t\t\t\tvalues[0] = getattr(values[0], str)(aval)\n\t\t\telse:\n\t\t\t\tvalues[0] = types.cast(values[0], aval)\n\n\treturn parser_name, tuple(values_dict)\n\ndef _cli_cmd(**kwargs):\n\t# Allow passing of non-cli params"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tfor key, value in list(values_dict.items()):\n\t\tdefault = ''\n\t\tif key in self.CLI_CLI_CONFIG_ARGUMENTS_KEY:\n\t\t\tdefault = str(self.CLI_CLI_CONFIG_ARGUMENTS_DEFAULT_VALUE)\n\t\t\tvalues_dict[key] = argparse.Namespace(parent=parser_name)\n\t\telif key in self.CLI_NON_CLI_CONFIG_ARGUMENTS_KEY:\n\t\t\tdefault = str(self.CLI_NON_CLI_CONFIG_ARGUMENTS_DEFAULT_VALUE)\n\t\t\t"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tfor key, val in values_dict.items():\n\t\tif isinstance(val, list):\n\t\t\tfor element in val:\n\t\t\t\tvaluess[key].append(element)\n\t\t\tcontinue\n\t\tif key == \"all_args\":\n\t\t\tcontinue\n\t\tvalue = values_dict.get(key)\n\t\tif not value:\n\t\t\tcontinue\n\t\tif isinstance(value, basestring):\n\t\t\tvaltype = str()\n\t\telif isinstance(value, type) and issubclass(value, basestring):\n\t\t\tvaltype ="}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tnew_values_dict = {}\n\tfor k in self.__non_cli_command_map[parser_name].keys():\n\t\tnew_values_dict[k] = values_dict[k]\n\treturn new_values_dict\n\n\ndef _parse_cli_args(parser_args):\n\t\"\"\"\n\tParse the configuration and store the resulting arguments into\n\tinstance variables.\n\n\t:param parser_args: A list of strings with all cli args.\n\n\t:return: None.\n\t\"\"\"\n\n\tparser_args = _normalize_cli"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tfor key in 'parser_name', 'values_dict':\n\t\tassert key not in values_dict, '{} already exists'.format(key)\n\tfor key in values_dict:\n\t\tassert key in values_dict['parser_name'], '{} not in {} dict'. \\\n\t\t\tformat(key, values_dict['parser_name'])\n\tif getattr(self, 'debug', -1) > 0:\n\t\tprint('parse {} with args - {}'.format(parser_name, values_dict))\n\n\tself._parser_names[parser_name] ="}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t# First remove any non-cli options\n\tkeys = set(parser_name.options)\n\tparser_name.options = [x for x in parser_name.options if x]\n\t# Then cast them\n\tnew_args = _convert_non_cli_args_dicts(parser_name.parser_args, keys)\n\t# Then turn into a dictionary\n\tresult = {}\n\tfor k in keys:\n\t\tresult[k] = values_dict[k]\n\t\t# Update the keys with just set values\n\t\tkeys_list = result"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tif not isinstance(values_dict, dict):\t\t# type: ignore\n\t\traise Exception(f\"{parser_name}: expected dict, found {type(values_dict)}\")\n\tfor k in sorted(values_dict):\n\t\tif k in (\"parser_name\", \"parser):\n\t\t\tdel values_dict[k]\n\t\tif k in (\"run_time\", \"options\"):\n\t\t\tdel values_dict[k]\n\t#\n\t# Update the values dictionary to the correct type if given\n\t#\n\tvargs = {}\t\t\t# type"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tself.set_parser(parser_name)\n\n\tfor key, value in sorted(values_dict.items()):\n\t\tself.set_value(key, value)\n\n\treturn values_dict['parser_type']\n\n\t# pylint: disable=line-too-long\ndef main(args=None):\n\t\"\"\"\n\tRun the CLI script using this method.\n\t\"\"\"\n\tparser = CLIHandler.get_parser_name_by_args(args)\n\n\ttry:\n\t\tvalues_dict = CLIHandler.get_parser_arguments("}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\ttry:\n\t\treturn_parser = _parser_name(parser_name).__class__\n\texcept KeyError:\n\t\ttry:\n\t\t\treturn_parser = getattr(Command, parser_name).__class__\n\t\texcept AttributeError:\n\t\t\traise AttributeError(\"The value for %s is not defined\"\n\t\t\t\t.format(parser_name)\n\t\t\t)\n\n\targ_values = {}\n\tfor x in values_dict:\n\t\ttry:\n\t\t\targ_values[x] = values_dict[x]\n\t\texcept KeyError:\n"}]}
{"task_id": "63060b1a73426c38ae68ad3e", "prompt": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tplugin_files_name = plugin_dir.split('.')\n\tplugin_type_name = ''\n\tdefault_config = False\n\t\n\tfor plugin_file in plugin_files_name:\n\t\tplugin_type_name = plugin_file.split('_')[0]\n\n\tif plugin_type_name == 'custom':\n\t\treturn {'plugin_name': plugin_dir, 'config_file': default_config, 'default_config': default_config, 'default_content_config': default_config}\n\telif plugin_type_name =='static':"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tplugin_dir_path = os.path.realpath(plugin_dir)\n\tplugin_path_list = os.listdir(plugin_dir_path)\n\tfor plugin_path in plugin_path_list:\n\t\tplugin_dir_path = os.path.join(plugin_dir_path, plugin_path)\n\t\tplugin_file = os.path.basename(os.path.dirname(plugin_dir_path))\n\t\tplugin_dict = {}\n\n\t\twith open(plugin_dir_path, \"r\") as plugin_f:\n"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tplugin_dict = {}\n\tplugin_dir2 = plugin_dir + \"/.\"\n\tfor prop in plugin_dir2:\n\t\tfname = glob.glob(prop)\n\t\tif fname:\n\t\t\tplugin_dict[fname[-1]] = prop\n\treturn plugin_dict\n\ndef get_plugin_spec_list(plugin_dir):\t\"\"\"\n\tReturns an ordered dict for each plugin in the plugin dir\n\n:param plugin_dir: A path to the plugin's dir\n:return: An ordered dict contain the plugin's properties in the same order as the\n"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tflatten = {}\n\tfor key in plugin_dir.name.split(\".\"):\n\t\tvalue = dict(plugin_dir.attrib)\n\t\ttry:\n\t\t\tvalue = {value[key: key + 1]}\n\t\texcept KeyError:\n\t\t\tbreak\n\t\tflatten[key] = value\n\treturn flatten\n\n\ndef get_plugin_spec_flatten_set(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dict contains the plugin's"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tplugin_dict = {}\n\tif plugin_dir[:1] == \"plugins\":\n\t\tplugin_dir = plugin_dir[1:]\n\tif os.path.isdir(os.path.join(plugin_dir, \"plugins\")):\t\t# Look for plugins\n\t\tdirectory_list = [x for x in os.listdir(plugin_dir) if (x[:3] == \"lib\" or x[:3] == \"plugins\")]\n\t\tfor filename in directory_list:\n\t\t\tfname = os.path.join(plugin_dir, filename)"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tfile = '{}/{}'.format(plugin_dir, plugin_name_from_dir(plugin_dir))\n\n\treturn yaml.safe_load(config_yaml_file(file))\n\ndef get_plugin_spec_flatten_list(plugin_dir):\t\t# type: ignore\n\tfile = \"{}/{}\".format(plugin_dir, plugin_name_from_dir(plugin_dir))\n\n\treturn yaml.safe_load(config_yaml_file(file))\n\ndef get_plugin_spec_name(plugin_file_path):\n\t"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tfor name, value in plugin_dir.iteritems():\n\t// If we didn't get a plugin spec and did we get a file object\n\tif not isinstance(value, dict): \treturn {}\n\treturn {name:value for value in value.items()}\n\ndef merge_dicts_from_dir(output_dir, in_dicts_list=None):\n\t\"\"\"\n\tThis function merge the plugin specific properties into a flat dict\n\n:param output_dir: A path to the directory within the tmp dir\n:param in_dicts_list: An optional"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tif not g.exec_mode:\n\t\treturn {}\n\telse:\n\t\tif g.plugin_dir is None:\n\t\t\treturn None\n\t\telse:\n\t\t\treturn get_plugin_spec_flatten_dict(g.plugin_dir)\n\n\n\n\nif __name__ == \"__main__\":\n\tparser = optparse.OptionParser(usage=\"%prog [options] [directory]\")\n\tparser.add_option(\"-f\", \"--force\", dest=\"force\", default=\"\",\n\t\t\t\t\t\t\t\t\thelp=\"Force installation of plugin\")\n\tparser.add_option"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\ttmpdict = {'plugin_source': plugin_dir, 'plugin_source_config': plugin_dir, 'plugin_source_manifest': plugin_dir}\n\td = os.path.dirname(_get_pluginspec_fname(plugin_dir))\n\tflattened_dict = {}\n\tif os.path.isdir(d):\n\t\tlog.debug('Directory exists, iterating into it')\n\t\tfor p in os.listdir(d):\n\t\t\tif os.path.isfile(os.path.join(d, p)) and 'prop' in"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tplugin_path = plugin_dir + os.sep + plugin_id + \"/\"\n\ttry:\n\t\tf = open(plugin_path + \"plugin.cfg\")\n\t\tplugin_cfg = f.read()\n\t\tf.close()\n\t\treturn plugin_cfg\n\texcept IOError:\n\t\tplugin_log.error(\"Fail to load plugin.cfg - maybe it's not installed?\")\n\texcept Exception as e:\n\t\tplugin_log.error(\"Fail to check for plugin.cfg - %s\" % e)\n\treturn None\n\ndef"}]}
{"task_id": "63060b1b73426c38ae68ad42", "prompt": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tif self.config_path is None:\n\t\tself.set_config_path_from_env()\n    \n    def save_config_path(self):\n        \"\"\"\n\tSet the environment variable for config path.\t\"\"\"\n        \n        if self.config_path is not None:\n            self.logger.debug(\"Saved config path\")\n        else:\t\n       "}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\ttry:\n\t\tsettings = self._getenv()\n\texcept AttributeError as err:\n\t\tself.print_(\"Cannot configure settings without environment variable, is the PATH unset? %s\" % (err.msg,))\n\t\treturn\n\tself._current_config = settings\n\treturn\n\ndef _getenv(key, default=None):\t\t\t\t\"\"\"\n\tGet from environment.  Returns a default value if no value can be found.\n\t\"\"\"\n\tvalue = os.environ.get(key, default)\n\treturn value or os.environ[key]\n\ndef get_"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tif self.__env_config_path is None:\n\t\tself.__env_config_path = CONF.env_config_path\n\telif self.__env_config_path is not None and not os.path.exists(self.__env_config_path):\n\t\traise ValueError('Configuration error, no such path: {path}\\n\\tIs the --env-config-path \\'--config\\' option defined?', path=self.__env_config_path)\n\t_basepath = os.environ['PATH'] if CONF.env_config_path is None else os.environ"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tif self.config_path is None:\t\t\treturn\n\tself.config_path = os.path.abspath(self.config_path)\n\t# set environment variable for config path\n\tfor i in range(1, 10):\t\t\t\tself.putenv(\"CIP\", \"PATH=\"+self.config_path)\n\nif __name__ == \"__main__\":\n\timport sys\t\n\timport cPickle as pickle\n\timport time as sys_time\n\tfrom configparser import ConfigParser\n\tcConfig = ConfigParser()\n\t# default config path\n\tdir = c"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tos.environ['TFHOME','BAS','CONF_FILE']='#!/bin/bash'\n\tif not isinstance(self.config_path,str):\n\t\tself.config_path=self.get_file(self.config_path)\n\n\ttry:\n\t\tenv_file=open(self.config_path+\"/env.json\")\n\t\twith json.load(env_file) as env:\n\t\t\tfor key, value in env.items():\n\t\t\t\tself.set_env(key,value)\n\texcept UnicodeDecodeError:\n\t"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t# TODO:\n\t# - get the user config path\n\t# - replace this with CUE configuration in the config dir - see cue_utils.create_file\n\t#self.config_path = sys.path[:self.config_path.rindex(\"/\")]\n\tself.config_dir = os.path.join(self.install_dir, 'cues')\n\tself.config_path = None\n\t#self.config_dir = '/etc/cues/environment'\n\n#\n#\n# CUE CONFIG\n#\n#\n\nclass cue"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tif getattr(self, 'config_path',''):\n\t\tos.path.append(self.config_path)\n\t\tself.config_path = ''\n\tif not self.config_path: self.config_path = os.getcwd()\n\tos.environ['SPHINX_CONF'] = self.config_path\n\n\n# Helper function\ndef get_default_path():\n\t\"\"\"\n\tReturns the path of the current working directory.\n\n\tReturns a path like \"{path}...\"\n\t\"\"\"\n\tif os.path.isabs(os.path"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tself.configpath = os.path.abspath(self.configpath)\n\tself.configpath = os.path.abspath(self.configpath)\n\tself.configpath = os.path.abspath(self.configpath)\n\tglobal CONF\n\tCONF = self.configpath\n\tLOG.info('Config path set to %s' % CONF)\n\n\t# set the current config path\n\tif not CONF['path']:\n\t\tself.CONFIGPATH = self.configpath\n\telse:\n\t\tself.CONFIGPATH = CONF['path']\n\n\t"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tconfigdir = configdirname()\n\tif configdir:\n\t\tself.app.config = Config(configdir)\n\n/*\n * Sonatype Nexus (TM) Open Source Version\n * Copyright (c) 2008-present Sonatype, Inc.\n * All rights reserved. Includes the third-party code listed at http://links.sonatype.com/products/nexus/oss/attributions.\n *\n * This program and the accompanying materials are made available under the terms of the Eclipse Public License Version 1.0,\n * which accompanies this distribution and is available at http://www."}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tif self.setenv:\n\t\tself.config_path = os.path.abspath(\n\t\t\tos.environ['CONFIGPATH'] if os.environ['CONFIGPATH'] else None)\n\telse:\n\t\t# If 'CONFIGPATH' has never been defined, it is automatically defined\n\t\t# in the following line.  If it is still undefined, set to the location\n\t\t# of the configuration file.\n\t\tself.config_path = os.path.abspath(\n\t\t    os.path.join(get_homedir(), '.vips', '"}]}
{"task_id": "63060b1b73426c38ae68ad43", "prompt": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\troot_subparsers.add_parser('sp_cli',\n\t\t\t\t\t\tmetavar='<subparser>', help=self.cli_longform)\n\n\n#\tdef extend_cli(self, root_subparsers):\t\"\"\"\n#\tAdds the modulecli options to to the main entry point.\n\n#\t:param subparser: the subparser object to extend.\t\"\"\"\n#\t\troot_subparsers.add_parser('modulecli',\n#\t\t\t\t\t\tmetavar='<subparser>', help=self.cli_longform)\n\n\nclass CommandLoader:\n"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tparser = subparsers.add_parser(\n\t\t'spec', name=\"extension\", help=\"Extension entry point \"+\\\n\t\t\t\t\t\t\t\t \"sub-parser\", short_help=self.cli_options_short_help, options_metavar=\"--\",\n\t\tadd_help=False)\t\t\t\t# add subparser options\n\n\tdef parse_cli(self, parser):\n\t\t\tprint parser.parse_args()\n\n\t# Setup entry point file parser.\n\tparser.add_argument(\"entry\", nargs='?', help=\"Extension entry point \"+\\\n\t\t\t\t\t\t\t\t \"sub-parser"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tsuper(ConfigParser, self).__init__(subparsers=root_subparsers)\n\tcmd_cls = self.cmd_prefix_cls(subparsers.command[1:])\n\tself.cmd_cls = cmd_cls\n\ndef configure(self, config_parser=None, config_file=None):\n\t\"\"\"\n\tParses the given configuration, and generates the config_parser object.\n\tWhen called without argument, parses an empty configuration spec.\n\n\t:param config_parser: the config_parser-object to parse.\t\"\"\"\n\tsubparsers = None\n\t"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\twith root.config.conf_root.subp.commands('extend') as cmd:\n\t\t# Add the subparser.\n\t\tcmd.add_subparser('set', show_help=False, help=self.get_help('cli'))\n\t\twith self.get_common():\n\t\t\tcmd.add_parser(self.get_subparser(cmd.add_parser), \n\t\t\t\t\t\tself.get_subparser('set', 'extend_cli'))\n\t\t\t\n\t\tcmd.add_target(target_types=(int,), default=0)\n\t\t"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tparser = root_subparsers.add_parser('cli', aliases=('cm', 'cs'))\n\tparser.add_argument(\n\t\t'command_name',\n\t\tnargs=1,\n\t\tdefault=['cm', 'cs'] + self.SUPPORTED_CLI_COMMANDS,\n\t\thelp='Command for which to add the cli option (must be one of: {b: b, c: c, h: h, hp: h, p: p, ps: ps, x: x, xx: x, y:y})'\n\t)\n\tparser.add"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tself.add_cli_option('specfile', action=\"store_true\")\t#  if specfile is True then don't print to stdout, just exit with error\n\nclass TestCliEntryPoints:\n\tdef __init__(self):\t\t# make sure this class is instantiated by the entry point\n\t\tfor entry in ('__main__', '__doc__'):\n\t\t\tfrom mock import patch\n\n\t\t\tpatch('mock.patch', new_callable=GrouchoTestEntryPoints.__del__)\t# __del__ ensures it calls from __main__ for the first time\n\n"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t# Add options to the base parser.\n\n\tself.add_args(CLI_ARG_SPEC)\n\tcli_opts = self.subparsers.create_parser('spec', 'cli options', \n\t\t\t\t\t\t\t\t\t\tmetavar='OPT', \n\t\t\t\t\t\t\t\t\t\thelp='command line options for spec')\n\t\t         \n\tcli_opts.add_argument('-a','--add-args', \n\t\t\t    help='args to add to the spec')\n\tcli_opts.add_argument('-m','--mods',\n\t\t\t    dest='meta',\n\t\t\t  "}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tfor cli in self.cli_entries:\n\t\tsubparser = subparsers.add_parser('spec', description=root_subparsers.describe(cli))\n\t\tcli.add_parser(\"cli\", description=self.describe(cli)).add_argument(\"-v\", \"--version\", action=\"version\", version=self.version)\n\t\tself.add_cli_options(subparser)\n\n\tdef _print_cli(self, cli_entry):\n\t\tif cli_entry.description:\n\t\t\tprint(cli_entry.description)\n\t\telse:"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tfor option in self.add_cli_options():\n\t\tsubparser = root_subparsers.add_parser(option[0], option[1], option[2], option[3])\n\t\twith subparser.option_context(\n\t\toption[0]                                       ) as opt_context:\n\t\t\topt_context.add_argument('--config', '-c', action='store',\n\t\t\t\t\thelp"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tsuper(self.__class__, self).__init__(subparsers=root_subparsers)\n\t# for every type of parser registered this method must be added one after the other\n\t# self.\n\t# add_cli_function was added to add to the methods subparser.add_cli_value()\n\t# and subparser.add_cli_variable() for easier access in child classes\n\n/*\n** Copyright (C) 2008 Nokia Corporation and/or its subsidiary(-ies)\n** All rights reserved.\n**\n** Contact: Nokia Corporation (qt-info@nokia"}]}
{"task_id": "62ece4982e6aefcf4aabbd64", "prompt": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\ttry:\n\t\tos.rmdir(path)\n\texcept:\n\t\tlog('Error removing root \\'%s\\': %s' % (path, traceback.format_exc()), False)\n\nclass WineRAR(_wineprefix, _Base, _BaseInstaller): pass\n\n# This module is used for internal purposes, but there are other\n# possible dependencies on this module or any other library that might\n# be dependent on this module.\n\nclass Wine(_Base):\n    \"\"\"\n    This is the Wine/Wine-1-1"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\troot = os.path.abspath(root)\n\tif not os.path.isdir(root):\treturn\n\tlist(map(lambda x: os.path.join(root,x), os.listdir(root)))  = [str(f) for f in os.listdir(root) if os.path.isdir(os.path.join(root,f))]\n\tprint(\" [*] Removed %i directories\"%len(list(map(lambda x: os.path.join(root,x), os.listdir(root))))+2*str"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tres= os.path.normpath(os.path.normcase(os.path.normpath(os.path.abspath(path))))\n\t#strip_root is in fact a standard Python function, but\n\t#we only use it for the old version.\n\t#There is no guarantee to the specific directory exists\n\tpath= os.path.normpath(os.path.join(os.path.dirname(path), root))\n\tstriped= os.path.normpath(os.path.join(os.path.dirname(path), \"\"))\n\n\t"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tif not os.path.exists(path):\n\t\traise \"Path name does not exist: %s\" % path\n\tnew_path = os.path.normpath(os.path.abspath(path))\n\twhile new_path!= root:\n\t\tnew_path = os.path.abspath(os.path.join(new_path, os.pardir))\n\treturn new_path\n\n#\n# Utility functions\n#\ndef uncen(filename, root=None):\n\t\"\"\"\n\tClean out any non ascii characters (such as the ones used"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tif '.' not in root:\n\t\treturn path\n\n\troot_parts = root.split(os.sep)\n\tpath = root.split(os.sep)[1]\n\n\tif not path[0].endswith(os.sep):\n\t\traise ValueError(\"path must be relative to the current path, but is %s\" % root)\n\n\troot = os.path.abspath(root)\n\n\twhile path and path[0] == root:\n\t\tpath = path[1:]\n\tif path:\n\t\treturn path\n\telse:\n"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\troot = os.path.dirname(root)\n\tif os.path.isdir(root):\n\t\timport shutil\n\t\tfor root, subfoldername, filenames in os.walk(root):\n\t\t\tfilepath = os.path.join(root, subfoldername)\n\t\t\tfor filename in filenames:\n\t\t\t\tos.unlink(filepath)\n\t\t\tif os.path.isdir(filepath):\n\t\t\t\traise IOError(\"directory '%s' contains a non-empty file.\" % filepath)\n\tif os.path.isfile(path):\n\t\t"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\troot = os.path.abspath(root)\n\tif os.path.dirname(path)!= root:\n\t\tlog.warn(\n\t\t\t\"Path %s does *NOT* end in %s\",\n\t\t\tpath, root)\n\n\treturn os.path.normpath( os.path.split(os.path.normpath(path))[-1] + '/' if path else '.')\n\n\ndef strip_extension(path):\t\"\"\"\n\tRemove extension from path\n\t@returns\n\t\t*path* is filename\n\t\"\"\"\n\tbase_file,"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tcwd = os.getcwd()\n\treturn os.path.commonprefix([cwd,os.path.expanduser('~')]) + '/' + os.path.expandvars(os.path.join('~', path))\n\ndef get_exe_name():\n\tf = open( \"C:\\\\Program Files\\\\Microsoft SQL Server\\\\sqllines.exe\" )\n\tif f:\n\t\tm = re.match( '\"(?P<servername>\\\\\\\\w+)\\\\\\\\(?P<file>(?:.exe|\\\n\t\t   .\\\\)\\S+)~[A-Za-"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tif root:\n\t\twhile root:\n\t\t\tif strip_root(path, root[1:]): return\n\t\t\troot = root[1:]\n\telse:\n\t\treturn\n\ndef set_chmod_bits(path):\n\tglobal root_path\n\ttry:\n\t\tif root_path:\n\t\t\tpath = os.path.join(root_path, path)\n\t\telif path in (\".\", \"..\"):\n\t\t\tpath = os.curdir\n\t\tif not path:\n\t\t\treturn\n\t\ttry:\n\t\t\tmode ="}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\troot = os.path.dirname(path) # this should be the root\n\n\tif root not in sys.path:\n\t\traise Exception(\"path '%s' must be relative to the root of the script\" % root)\n\n\tpos = path.rfind(root)\t# remove from starting directory (start of root)\n\tif pos!= -1:\n\t\tpath = path[:pos]\t# path must now have a dirsep in it\n\n\treturn path\n\n@extends('app.layouts.master')\n\n@section('title') {{ __('front."}]}
