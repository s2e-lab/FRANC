{"task_id": "62e60f43d76274f8a4026e28", "prompt": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tif tz is not None:\n\t\traise Exception('`tz` is not supported')\n\ttry:\n\t\tnanoseconds = int(nanoseconds.timestamp() / 1e9)\n\t\twhile nanos <= nanosecond:\n\t\tnanos += 1\n\t\tif microsecond % 1e6 == 0:\n\t\t\tnanoseconds *= 1e-6 \n\t\t\tmicrosecond = microsecond // 1e6\n\t\telse:\n\t\t\tmicrosecond += 1\n\t\tmicroseconds = int(microseconds / 1e6)\n\t\twhile microseconds <= microsecond"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\treturn nano.datetime.time\n\nimport { connect } from'react-redux'\nimport React, { useCallback } from'react'\nimport { Dispatch } from'redux'\nimport { mapState, mapDispatchToProps } from'snabbdom'\nimport { createApi } from '../../../api'\nimport * as apiActions from '../../../../api/actions'\nimport { connectRouter } from '../../../../store/api'\nimport {\n\tdeleteAccount,\n\tfetchAccountList,\n\tfetchAccount,\n\tfetchUnlinkedAccountList,\n\tfetchUnlinkedAccount,\n\t"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\treturn _hydrate_time(_float_number(_nanoseconds, nanoseconds), tz)\n\n\ndef _hydrate_date(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Date` and `LocalDate` values.\n\n:param nanoseconds:\n:param tz:\n:return: Date\t\"\"\"\n\treturn _hydrate_date(_float_number(_nanoseconds, nanoseconds), tz)\n\n\ndef _hydrate_datetime(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Date` and `LocalDatetime` values."}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tif tz is None:\n\t\ttz_name = datetime.timezone.getdefaultzone(tzutil.utc)\n\t\ttz = tz_name\n\tnow = datetime.datetime.now(\n\t\tdturlopen(*time.tzname(dt).tzfile())) - datetime.timedelta(\n\t\tminutes=1)\n\treturn hydrate_microsecs(nanoseconds, tz=tz)\n\n\ndef datetz(dt, strict=False, tz=None):\n\t\"\"\"\n\tConvert a time datetime object into a tz, timezone aware datetime object.\n\tThis"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\ttry:\n\t\tseconds = float(nanoseconds)\n\texcept ValueError:\n\t\traise ValueError('nanoseconds expected got {}'.format(nanoseconds))\n\tif float(seconds) == NAN:\n\t\traise ValueError('seconds should be a positive number')\n\tsecs = seconds\n\ttz = tz.astimezone(timezone=True)\n\tsecs = datetime.timedelta(seconds=secs * 60)\n\ttz = datetime.timezone(tz)\n\tsecs = tz._pytz().astimezone(timezone=True)\n\tsecs = tz.astimezone(timezone"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\treturn time_utils.hydrate(nanoseconds, tz)\n\ndef localtime(seconds, tz=None):\t\"\"\"\n\tLocalTime for `Time` values.\n\n:param seconds:\n:param tz:\n:return: Time\t\"\"\"\n\treturn time_utils.localtime(seconds, tz)\n\ndef localtime_ms(seconds, tz=None):\t\"\"\"\n\tLocalTime for `Timestamp` values.\n\n:param seconds:\n:param tz:\n:return: Timestamp  \"\"\"\n\treturn time_utils.localtime_ms(seconds, tz"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tif nanosecond is None and tz is None:\n\t\traise TypeError('`.nanosecond` and `.tz` must both be specified.')\n\telse:\n\t\treturn Time(*datetime.utcfromtimestamp(nanoseconds,tz))\n\n\ndef to_epoch(year, month=None, day=None):\n\t\"\"\"Converts Python 3.6 to `Int64`.\"\"\"\n\treturn int(year, 10) * 365 + int(month, 2) * 30 + int(day, 1)\n\n\ndef timestamp_str(timestamp):\n\t\"\"\"\n\tReturns timestamp"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tif isinstance(nanoseconds, Time):\n\t\treturn nanoseconds\n\tif isinstance(nanoseconds, (LocalTime, LocalLocation)) and tz is not None:\n\t\traise TypeError(\"nanoseconds parameter to `hydrate_time` cannot have 'tz' set\")\n\treturn nanoseconds\n\n_localtime_default = localtime\n_localtime_fallback = Time(_localtime_default)\n\n# LocalTime.__get__ implementation is deprecated, use Time.__get__\n#\tfor backwards compatibility.\n@hybrid.__get__\n# Time.__get__ is not a builtin replacement for Time"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t# We need at least two parameters here\n\tassert not (nanoseconds > 1 and nanoseconds < 3)\n#\tassert not (nanoseconds > 2 and nanoseconds < 4)\n\t# This function needs to be implemented with a `list or tuple`, or with\n\t# tuples, or both types provided\n\traise TypeError(f\"Unknown tz '{tz}' in HydrateTime()\")\n\n\ndef timetuple(seconds=0, micro_seconds=0, tz=None):\t\"\"\"\n\tHydrator for `time.timetuple` values.\n\n:param seconds: seconds"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\tif tz is None:\n\t\ttz = tz_local()\n\tif nanoseconds < 0:\t\t# E.g. \"infinity\"\n\t\tnanoseconds = -nanoseconds\n\thours = str(nanoseconds / 3600).lower()\n\tminutes = str(nanoseconds / 60).lower()\n\tseconds = str(nanoseconds).lower()\n\tminutes_str = \" \" if minutes!= 'Z' else time_fmt[minutes]\n\tseconds_str = \" \" if seconds!= 'Z' else time_fmt[seconds]\n\treturn Time(hours, minutes,"}]}
{"task_id": "62e60f3bd76274f8a4026e10", "prompt": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tdef dehydrate(value_):\n\t\tif isinstance(value, int):\n\t\t\treturn (value - value.minutes()) * 60 + value.seconds()\n\t\telse:\n\t\t\treturn value * 1000000 + value.msecs()\n\n\treturn dehydrate\n\n\t_default_timedelta_value = \"P0Y0M0DT0H0M0S0H0M0\"\n\n\tdef default_value(self):\n\t\tif self.value <= 0:\n\t\t\treturn 0\n\n\t\treturn self._default_timedelta_"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\treturn value\n\nusing System;\nusing System.Collections;\nusing System.ComponentModel;\nusing System.Collections.Generic;\nusing System.Globalization;\nusing System.Windows.Forms;\nusing Sheng.SailingEase.Localisation;\nusing Sheng.SailingEase.Core;\nusing Sheng.SailingEase.Core.Data;\nusing Sheng.SailingEase.Converter;\nusing Sheng.SailingEase.Logging;\nusing Sheng.SailingEase.Shell;\nusing Sheng.SailingEase.ValueFormatting"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tpass  #: pylint: disable=unused-argument\n\ndef _convert_duration_to_timedelta(value):\n\t# TODO:\n\tpass\t# pylint: disable=invalid-name\n\tpass #: pylint: disable=unused-variable\n\ndef convert_to_datetime(value):\n\t\"\"\"\n\tConverts a Python type value to a datetime.datetime value.\n\tIf the value is already a datetime.datetime then this method\n\twill return it.  Otherwise, will return it.\n\n\t:param value: The Python value.\n\t:return:\tThe datetime"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\ttry:\n\t\treturn timedelta_parse(value)\n\texcept (ParseError, ValueError):\n\t\treturn parse_timedelta(value)\n\n\ndef dehydrate_timedelta_tz(value):\n\treturn parse_localtz(value, 'timedelta')\n\n\ndef parse_timedelta(value, units='seconds', tz='UTC'):\n\t\"\"\"Parse `timedelta` values.\n\n\t:param value:\n\t:type value: datetime\n\t:param units:\n\t:type units: str\n\t:param tz:\n\t:type tz"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tif isinstance(value, timedelta):\n\t\treturn value\n\tassert isinstance(value, int)\n\treturn int(value)\n\n/*\n * Copyright Camunda Services GmbH and/or licensed to Camunda Services GmbH\n * under one or more contributor license agreements. See the NOTICE file\n * distributed with this work for additional information regarding copyright\n * ownership. Camunda licenses this file to you under the Apache License,\n * Version 2.0; you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *    "}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tif value is None:\t# (1,)\n\t\treturn 0\n\telif isinstance(value, tuple):\t# (tuple)\n\t\treturn int(len(value)) * timedelta(**value)\n\traise Exception(\"type Error\")\n\n@dehydrate_timedelta\ndef normalize_timedelta(value):\t\"\"\"\n\tNormalize the timedelta value for `timedelta`.\n\n:param value:\n:return:\n\n\tnormalize_timedelta\n\n\"\"\"\n\n\nclass Timer(object):\n\t\"\"\"\n\tTimer class represents a single iteration over `timedelta"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tpass\n\n\ndef to_datetime(*value):\n\tpass\n\n\ndef is_datetime(value):\n\tpass\n\n\ndef to_datetime(*value):\n\tpass\n\n\n#\n#  from django.db import models\n#  import django.core, timezone\n#  timezone.now()\n#\n#\ndef to_timedelta(value):\n\tpass\n\n\ndef to_timedelta(*value):\t\t# nosec, no doc\n\tpass\n\n\ndef validate_db_table(name, values_list):\n\tpass\n\n"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tself, node, value = self.ast_unpickle(value)\n\tdehydra = node.Dehydrate\n\tif dehydra(value):\n\t\tvalue.offset = -1\n\n\ndef convert_timedelta(node):\tfrom timedelta import *\n\n\tif isinstance(node, ast.BinOp):\n\t\top = node\n\t\tres = ast.BinOp.DIV\n\t\tif op.from!= ast.Undefined:\n\t\t\tres = ast.BinOp.POW\n\t\telse:\n\t\t\ttimedelta = timedelta."}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tif type(value) == float:\t\t\treturn float(value)/100 if isinstance(value, float) else float(value), \"\"\n\telif type(value) == float:\t\t\treturn \"%.2f\" % (value,)\tif isinstance(value, float) else \"%.2f\", \"\"\n\telif type(value) is list:\ttype(value)\t\t\treturn \"%s\" % [\" \" + str(i) for i, e in enumerate(value)] if isinstance(value, list) else \"[%s]\" % value, \"\"\n\telif type(value) is complex:\t"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\tdt = timedelta(nanoseconds=value.to_ns())\n\treturn dt\n\n\nclass DurationField(Field):\n\n\t@staticmethod\n\tdef from_s(s):\n\t\tif isinstance(s, (str, int, bool, type(None))), castable_types = (\n\t\t\tdatetime.date,\n\t\t\tdatetime.datetime,\n\t\t\ttimedelta)\n\n\t\tif not s or isinstance(s, (int, timedelta)):\n\t\t\traise ValueError(\"DurationField values need to be in the form \"\n\t\t\t\t\"(timedelta("}]}
{"task_id": "62e60f37d76274f8a4026dfd", "prompt": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\td = parse_time(value)\n\treturn d.value\n\n\ndef parse_time(value):\n\t\"\"\"\n\tParse a number of seconds from an RFC-compliant format\n\t(as integer). For integer seconds, the format is \"(secs)hh:mm:ss\",\n\twithout the '.' character.\n\t\"\"\"\n\t# This will be caught at parse_time() and not at set_localtime().\n\t# Parse for hh:mm:ss\n\ttry:\n\t\treturn float(value)\n\texcept ValueError:\n\t\traise ValueError(\"String has"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tassert value.hour or value.minute or value.second\n\tif value.hour or value.minute or value.second:\n\t\treturn value\n\tif value.hour is not None and val_is_str(value.hour) and value.minute!= 0:\n\t\treturn value\n\telif value.minute!= 0:\n\t\treturn value\n\telse:\n\t\treturn None\n\ndef dehydrate_month(value):\t\"\"\"\n\tDehydrator for `month`, `month_number`, `month_of_year` values.\n\n:param"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tfor elem in value.iteritems():\n\t\tfor tag in elem.iterkeys():\n\t\t\tdel elem[tag]\n\n# coding: utf-8\nlibxsltdata = \"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XSLT//EN\" \"-//W3C//DTD XSLT//EN\"\n\t\"http://www.pcg.org/schemas/xslt/output/1/pcg.dtd\">\n<html>\n<head>\n<meta http-equiv=\"content-type\" content=\"text/html;charset=UTF-8\""}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tif value.nanosecond < 1000:\n\t\treturn value.convert_to_unix_time(value.hour, value.minute, value.second, value.microsecond)\n\telif value.nanosecond > 1000:\n\t\treturn value.minute\n\telse:\n\t\treturn None\n\ndef dehydrate_time_tz(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Timetz\n:return:\t\"\"\"\n\tif value.nanosecond < 1000:\n\t\treturn value.convert"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tvalue = getattr(value, 'value', None)\n\tif value is None:\n\t\treturn\n\treturn time.strftime(value.time())\n\n\ndef dehydrate_duration(value, tops):\n\t\"\"\"\n\tDehydrator for `duration` values.\n\n\t:param value: The duration\n\t:type value: int\n\t:param tops: The time as seconds\n\t:type tops: Time\n\t:return:\tThe duration string\n\t\"\"\"\n\tdt = duration(value)\n\treturn dt.time().strftime(dt"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tif value == 0 or value == _time._default_value:\t# don't change the time, return it\n\t\treturn\n\tif ((value < _time._minimum or value > _time._maximum)):\n\t\treturn\n\tdehydrated = _time._timetable.shift(value, value, _time.minutes(value))\n\treturn _time._default_value, dehydrated\n\ndef dehydrate_month(value):\t\"\"\"\n\tDehydrator for `month` values.\n\n:param value:\n:type value: Month\n"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\treturn timedelta(seconds=value.seconds, nanos=int(value.nanoseconds))\n'''\n>>> value = Decimal('1e-2')\n>>> x = dehydrate_time(value)\n>>> print(x)\n1\n>>> print(roundoff(x))\n-1\n>>> print(dehydrate_time(x))\n1\n>>> print(dehydrate_time(x))\n1\n>>> print(roundoff(x))\n-1\n>>> print(roundoff(x))\n1\n>>> print(dehydrate_time(x))"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\ttry:\n\t\treturn value.strftime('/%H:%M:%S')\n\texcept KeyError as err:\n\t\tpass\n\traise KeyError(\"time out of range '%s'\" % value)\n\ndef int_from_int_val(value):\n\t\"\"\"\n\tConverts a numeric float from a string-based integer to an Integer instance\n\tthat we can extract from the provided int value.\n\tIt returns a number object.\n\t\"\"\"\n\tvalue = float(value)\n\tif not (value <= 0):\n\t\traise ValueError(\"Invalid type for '%s'"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tfor i in range(2):\n\t\tif i == value.month:\n\t\t\treturn time.localtime(int(value._time[:12]))\n\t\telif i == value.year:\n\t\t\treturn int(time.localtime(\n\t\t\ttime.time() + i + time.timezone()))\n\treturn time.time()\ndef day_of_week():\n\t\"\"\"Return `time` corresponding to the day of the week represented by\n\ta `datetime` or `Time` instance.\"\"\"\n\ttry:\n\t\tdate = datetime.datetime.now(datetime"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\tfor value_time in value:\n\t\tif 'time' in value_time:\n\t\t\t# Add 'offset hours ago' in case of the time that `value` is zero since\n\t\t\t# zero-hour offset\n\t\t\tvalue_time['time'].add_sub('-offset hours ago', '+')\n\n<?php\n\nnamespace Admingenerator\\GeneratorBundle\\DependencyInjection\\Compiler;\n\nuse Symfony\\Component\\DependencyInjection\\Compiler\\CompilerPassInterface;\nuse Symfony\\Component\\DependencyInjection\\ContainerBuilder;\nuse Symfony\\Component\\DependencyInjection\\Definition;\nuse Symfony\\"}]}
{"task_id": "62e60f33d76274f8a4026de9", "prompt": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t# Dehydrate points.\n\tpoints = []\n\tfor point in value:\n\t\tpoints.append(Dehydrator(point))\n\t\tpoints.extend(dehydrate_point.values())\n\treturn points\n\ndef dehydrate_text(value):\t\"\"\"\n\tDehydrator for Text data.\n\n:param value:\n:type value: Text\n:return:\t\"\"\"\n\t# Dehydrate texts.\n\ttext = []\n\tfor text in value:\n\t\ttext_split_result = dehydrate_text_split"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\treturn Point(value[0], value[1])\n\nint8x8()\n\tret = _mm_unpacklo_epi8(_mm_loadu_si128((const __m128i*)(ptr_x) + 7), _mm_loadu_si128((const __m128i*)(ptr_x) + 3));\n\treturn __int32x4(ret, (ret >> 31));\n}\n\n//#############################################################################\n//#                                 "}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\tpoint_values = []\n\tfor point in get_points(value):\n\t\tpoint_values.append(point(*point_values))\n\treturn point_values\n\n\ndef dehydrate_circle(value):\t\"\"\"\n\tDehydrator for Circle data.\n\n:param value:\n:type value: Circle\n:return:\t\"\"\"\n\tcircle_values = []\n\tfor circle in get_circles(value):\n\t\tcircle_values.append(circle(*circle_values))\n\treturn circle_values\n\n\ndef dehydrate_polygon"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\tif isinstance(value, ImagePoint):\t\"\"\" Point.__init__( point_id: Point ) -> None: pass \"\"\"\n\t\tdehydrate_point = ImagePoint(value)\t# type: ignore[assignment,type-arg]\n\telse:\t\t\t\t\t\t\t# Point(points_id: Point)\n\t\tdehydrate_point = value\n\treturn dehydrate_point\n\n\ndef distance_to_point(value):\t\"\"\" Distance to Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\tif isinstance(value,"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\tif (not value.is_valid_for_dehydration() \n\t\tor value == ZERO_POINT_VALUE \n\t\tor str(value) in DEHYR_FIELD_NAMES) or (value.is_valid_for_dehydration() \n\t\tor \"point\" in DEHYDR_FIELD_NAMES) or (value.is_valid_for_dehydration() and str(value) in DEHYDR_SUB_FIELD_NAMES):\n\t\traise ValidationError(\"Invalid Point value for dehydration of the Point.\")\n\n\t"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\tdehydrated = Point(x=value[0],y=value[1],type=value[2],scale=value[3])\n\tif dehydrated.is_defined():\n\t\tdehydrated.load()\n\t\treturn dehydrated\n\treturn Point\n\ndef dehydrate_point_array(value=None):\n\tdehydrated = None\n\tif value!=None:\n\t\tdehydrated = []\n\t\tfor line in value:\n\t\t\tdehydrated.append(dehydrate_point("}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\tif hasattr(value, 'point_id'):\n\t\treturn value.point_id\n\treturn None\n\ndef dehydrate_array(value):\n\tdehydrate_point(value)\n\tdehydrate_point(value.x)\n\tdehydrate_point(value.y)\n\tpass\n\ndef dehydrate_point_vector(value):\t\n#\tDehydrator for vector values.\ndef dehydrate_array_vector(value):\t\n#\tDehydrator for vector values.\ndef dehydrate_array_"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\tif value is None:\n\t\treturn \"\"\n\tpoint = []\n\tfor i in range(len(value)):\n\t\tif i!= 0 and i!= 4:\n\t\t\tpoint.append(\"_\".join(str(value[0]) + \"_#\" + str(i), \"A\"))\n\t\t\tpoint.append(\"_\".join(str(value[1]) + \"_#\" + str(i), \"B\"))\n\t\t\tpoint.append(\"_\".join(str(value[2]) + \"_#\" + str(i), \"C\"))\n\t\t\tpoint.append(\"_\".join"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\treturn parse.dehydrate_point(value)\n\n\ndef parse_type_def(type_type):\n\treturn \"\"\"\n\tTYPEDEF\n\t\n\tTYPENAME\t\t \n\t{__type}\n\t\n\t\"\"\"\n\n\treturn type_type.__doc__\n\n\ndef parse_type_def_simple(type_type):\n\treturn parse.format(int(parse.to_native(type_type.__name__[1:8]), '0x%04x')).lower()\n\n\ndef read_file(path, encoding='utf8'):\n\t\"\"\"\n"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\timport io\n\treturn bytes([io.BytesIO([b\"\\x1f\\x32\")) for  in dehydrator_dehydrator.keys()], \"bytes\")\n\n\n@dehydrate_point\ndef dehydrate_pointv(value):\t\"\"\"\n\tDehydrator for PointValues.\n\n:param value:\n:type value: PointValues\n:return:\t\"\"\"\n\timport io\n\treturn read_value(dehydrator_dehydrator.values(), value)\n\n\n@dehydrate_pointv\ndef de"}]}
{"task_id": "62e60ed4d76274f8a4026da0", "prompt": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t", "suggestions": [{"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\tresults = OrderedDict()\n\tfor i in range(0, len(self.rec)):\n\t\tresult = self.rec[i]\n\t\tresults[result.table_id] = result.columns\n\t\tif result.index_name:\n\t\t\tresults[result.index_name] = result.columns\n\treturn results\n\ndef get_key_records(records, columns):\n\t\"\"\"\n\tGet a list of keys corresponding to the entries in the records.\n\n\tArgs:\n\t\trecords: list of dictionary of Records\n\t\tcolumns: list of"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn self.pk.count()\n\n\n# -----------------------------------------------------------------------------\n# Key - class definition:\n# -----------------------------------------------------------------------------\n\nclass DB():\t# type: ignore\n\t\"\"\"\n\tBase class for all database objects.  This can be extended by\n\timplementing the class in this package so as to not have any\n\tinherent implementation of the class.\n\t\"\"\"\n\n\tdef __init__(self, schema=None):\n\t\tself.m = schema or {}\n\t\tDB.__init__(self, schema)\n\n\tdef get(self, key, default=None, primary=None"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn [str(item) for item in self]\n\n##############################################################################\n#\n# ====== Key Value ======\n#\n##############################################################################\n@functools.lru_cache\nclass _KeyValCache:\n\t\"\"\"A data-type class which provides a key and a value.\n\tSupports both Python integer and Python float values.\n\n\tNote: This class exists so the Pyro package can access\n\tthe methods:\n\n\t    class KeyValList:\n\t        def __getattr__(self, key):\n\t         "}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn self.keys()\n\n\n@six.python_2_unicode_compatible\nclass M3Record(Record):\n\t\"\"\"Record holding a single m3 information.\n\n\tThis is used to hold information about all the M3 objects,\n\twhich is all as defined by the M3Record class.\"\"\"\n\n\t# The keyname of this record is the key for the\n\t# data which is represented by a structure\n\t# \tM2Record = {'data':'bytes','version':0}\n\tkeyname ='m3'\n\nimport { Injectable } from '@"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn self.attributes.keys()\n\n\nclass RecordAttributeKeys:\n\t\"\"\"\n\tDefine the key properties of a record attribute.\n\n\t.. _type: https://www.mysql.com/doc/refman/5.7/en/keywords-recamdresattr-function.html#_record_attributes-attribute-keyword-table-tables\n\t\"\"\"\n\tdef __init__(self, *args):\n\t\tself.keyword_count = 0\n\t\tself.table_size = args\n\t\tself.column_type_info = {}\n\t\tself"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn [key.name for key in self.columns]\n\n\ndef values(self, schema):\n\t\"\"\"\n\tReturn the values of the record\n\n\tschema : schema.Schema\n\t:dict of string to values\n\t:dict of FieldSpec\n\t:schema of the result\n\t:string of the name of the field\n\t:string of the name of the field\n\t:list of strings\n\t:list of strings\n\t:list of string of the column names\n\t:column from which each string is stored\n\t:columns from which each string is stored\n\t:"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn self.__keys() if self._keys() else []\n\n\n@contextmanager\ndef keys(iterable):\t\"\"\"\n\t:type: GIM.Iterable[GS]\"\"\"\n\tfor k in iterable:\n\t\tsetattr(self, k.name, k)\n\n\ndef load_record(stream, file=None):\t\"\"\"\n\t\"\"\"Load the record from the given file path.\"\"\"\n\t\"\"\"\n\tif not file: # (might be None)\n\t\tfile = get_filename(stream)\n\telif isinstance(stream, str):\n\t\tfile = get"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\treturn tuple([self.__class__.KEYS[i]] + self.__dict__['keys'])\n\n@types.keys.register\ndef key(self, key):\t\"\"\"\n\tReturn the value of the provided key as a string on the record.\n\n:param key: string\t\"\"\"\n\treturn self.__class__.keys[key].value.value\n\n@types.keys.register\ndef key_type(self, key_type):\t\"\"\"\n\tReturn a dict mapping the key types as strings (e.g. 'int') to the type\n\tof the item specified in the"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\tout = []\n\tfor i in range(len(self.listkey)):\n\t\tfor j in range(len(self.columns)):\n\t\t\tout.append(self.listkey[i][j].name)\n\t\tout.append(\"\")\n\treturn out\n\n\n\"\"\"\nThis function is used in the code in the table header and in the code in the\nrecord. It returns a a list of the row indexes of the row, without duplicate\ndata checks.\n\n:param p.table.name\tname\t\t\t- name of the table.\n\"\"\"\n@_"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t# NOTE: These are not really tuples, we'd need to create a list\n\t# of tuples to hold the keys.\n\tdict = [(\"\", \"value\"), (\"label\", \"value\"), (\"key\", \"value\")]\n\ttuple_dict = [tuple(s) for s in dict]\n\treturn tuple_dict\n\ndef get_keys(recordset):\n\t\"\"\" Return the keys of the recordset\n\t    Records are keyed by key, the first is the label, the second key\n\t    is a label, and so on.\n\n\t    :param"}]}
{"task_id": "62e60ecfd76274f8a4026d6a", "prompt": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif not tuple(protocol_version): return {'bolt_protocol_handler': BoltProtocolHandler,'bolt_protocol_handlerlist': []}\n\treturn {'bolt_protocol_handler': BoltProtocolHandler,'bolt_protocol_handlerlist': []}\n\n\ndef bolt_protocol_handler_factory(protocol_version=None):\n\t\"\"\"Return a handler factory suitable for bolt protocol handlers\n\tof specific protocol version. It creates a protocol handler\n\tclass with all the supported protocol versions\n\tof the given protocol version. The returned handler will be\n\tset to the list of the"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif not protocol_version: return None\t# empty dict, ignore\n\t# We expect protocol version tuple in order\n\t# \tprotocol version 0 is the generic \"bolt\n\t# \tprotocol 1\" protocol version tuple...\n\tif tuple(protocol_version) not in (None, 0, 1): raise TypeError(\"invalid format.\")\n\n\t# Check for valid protocol version.\n\t# This is also required since protocol version 0 should\n\t# also be considered as compatible with the protocol 1 protocol\n\tif int(protocol_version)!= 0:\n\t\traise TypeError(\"%s protocol"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None: version_tuple = tuple()\n\telse: version_tuple = protocol_version\n\tif (isinstance(protocol_version, (int, long))\n\t\t\tor version_tuple[0] is None):\n\t\traise TypeError(\"Bolt version must be an integer, or a tuple\")\n\n\tif (protocol_version[-1] is 0 and version_tuple[-1] is not None):\n\t\treturn protocol_version[0]\n\telse:\n\t\tfor version_tuple in protocol_version:\n\t\t\ttry:\n\t\t\t\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif not protocol_version:\n\t\t# If the proto version passed in is None, try to return\n\t\t# one of the default handlers if both are defined\n\t\tif protocol_version == None and getattr(protocol, '__protobuf__', False):\n\t\t\treturn BoltProtocolDefaultHandlers\n\t\t# Try to return the default handler for this protocol\n\t\t# if no protocol was specified.\n\t\tprotocol = BoltProtocol(protocol_version)\n\telif protocol_version == None:\n\t\t# If an explicit protocol version was passed to Bolt, use it to try\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif not protocol_version:\n\t\tprotocol_version = tuple(BoltLib._list_protocol_v1_handlers())[:len(Tuple())]\n\n\tif not all([protocol_version] == tuple(BoltLib._list_protocol_v1_handlers())):\n\t\traise ValueError(\"Protocol version was not supported.\")\n\n\tif len(protocol_version)!= len(BoltLib._list_protocol_v1_handlers()):\n\t\traise ValueError(\"Protocol version was not supported, but %d provided.\" % len(protocol_version))\n\n\tprotocol"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t#\n\t# check version is present to prevent warning\n\tif not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"protocol_version must be tuple\")\n\n\t#\n\t# check protocol_version is valid\n\tif isinstance(protocol_version, version):\n\t\treturn protocol_version, False\n\n\tif protocol_version is None:\t# default is lowest possible\n\t\traise TypeError(\"all protocol_version values must be\")\n\n\t#\n\t# make sure the protocol version is valid\n\tif not (protocol_version >= (3, 5)):\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif not protocol_version:\n\t\tprotocol_version = None\n\tif protocol_version is None:\n\t\tprotocol_version = BoltProtocol.version_by_version_for_module\n\tif not protocol_version:\n\t\treturn {}\n\n\ttry:\n\t\tBoltProtocol.protocol_version = protocol_version\n\texcept AttributeError:\n\t\tclass A(BoltProtocol):\t# type: ignore\n\t\t\t# protocol_version is None means that version is\n\t\t\t# not used in the version check\n\t\t\tprotocol_version = None\n\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif not protocol_version:\n\t\tprotocol_version = {}\n\t# check we've got a version tuple\n\tif not isinstance(protocol_version, (int, tuple)):\n\t\tmsg = \"Protocol version must be either (0, 1), (3, 5)\"\n\t\traise TypeError(msg)\n\t# check that the version tuple is a zero length tuple\n\tif not tuple(protocol_version[:-1]):\n\t\tmsg = \"Protocol version must be a (0, 1, or 3), but got ({}): {}\".format(protocol_version[:-1], protocol_"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif type(protocol_version) is not tuple:\t# pragma: no cover\n\t\traise TypeError(\"protocol_version not a tuple\")\n\tif protocol_version:\t# pragma: no cover\n\t\tif protocol_version < (0, 7):\n\t\t\tprotocol_version = (protocol_version,)\n\t\t\tif protocol_version[0] == 2:\t# pragma: no cover\n\t\t\t\tprotocol_version = (protocol_version[1],)\n\treturn protocol_versions[protocol_version]\n\n\n# http://www.openwave.com/wiki/"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\n\t\treturn BoltProtocolHandlers()\n\tif protocol_version == (3, 5):\n\t\treturn BoltProtocolHandlersFor3()\n\telse:\n\t\traise ValueError(\"protocol_version passed in unambiguously\")  # pragma: nocover\n\n\ndef BoltProtocolHandlersFor1():\n\t\"\"\"\n\tReturn protocol handlers that have been supported by the Bolt\n\tclass\n\n\t:param protocol_version: tuple specifying the version tuple\n\t:return: tuple of Bolt protocol handlers, keyed by version tuple\n\t:"}]}
{"task_id": "62e60e49d76274f8a4026d25", "prompt": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t", "suggestions": [{"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\treturn transactional_function('count_people_tx', ('metadata', 'timeout',), __name__, (metadata, timeout))\ndef query_all(params=None, query_string=None, results=None, parameters_json=None, timeout=None, metadata=None, return_results=False):\t\"\"\"\n\tThis function is a decorator for transactions that work as expected. If `metadata` parameter is provided, then it overrides `query_all.metadata`.\n\n    Parameters:\n      * params : list(<None>)\n      *"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t@classmethod\n\tdef _execute(cls, query, connection, parameters):\n\t\treturn cls.execute_db(connection=connection, operation=query, parameters=parameters, metadata=cls.metadata)\n\n/// <reference path='fourslash.ts'/>\n\n////interface A {\n////    foo(): string;\n////}\n\ngoTo.marker('r');\nverify.navigationErrors();\n//https://github.com/Microsoft/TypeScript/issues/2465\ngoTo.marker('r');package t1;\nclass C {\n    public p"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\timport cStringIO as StringIO\n\tif isinstance(metadata, dict):\n\t    raise TypeError(\n\t        \"Argument metadata should be a dictionary, but it is type %s\" % type(metadata))\n\telse:\n\t    metadata = {}\n\t    for key, value in metadata.items():\n\t        try:\n\t            if isinstance(value, float):\n\t                metadata[key] = cStringIO"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\tfrom neo4j import unit_of_work\n\n\treturn unittest.mock.mockResult(unit_of_work(timeout=timeout))\n\n\nclass DBMS(unit_of_work.BaseUnitOfWork):\n\tdef __init__(self, dbms_conf=None):\n\t\tif dbms_conf is None:\n\t\t\tdbms_conf = {}\n\t\tself.dbms_conf = dbms_conf\n\t\tself.dbms_conf[\"tx.cache.maxsize\"] = 1024\n\t\t# A little optimization for the common case"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\tdef __init__(self, metadata, timeout=None):\t\t\t# pragma: no cover\n\t\t'''\n\t\t@param metadata:\n\t\ta dictionary with metadata.\n\t\tSpecified metadata will be attached to the executing transaction and visible in the current transaction.\n\t\tIt will also get logged to the ``query.log``.\n\t\tThis functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t\t'''"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\tdef __init__(self, metadata=None, timeout=None):\n\t\tpass#IMPORTIMPORTIMPORT from    threading.utils import get_ident\n\t\tpass#IMPORTIMPORTIMPORT from    string    import hexlify, unichr, striptags, basestring\n\t\tpass#IMPORTIMPORTIMPORT from    string    import bytes\n\t\tpass#IMPORTIMPORTIMPORT import functools  # NOQA\n\t\tpass#IMPORTIMPORTIMPORT import datetime # NOQA\n\t"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\tdef __init__(self, metadata=None, timeout=None):\n\t\tfor key in (\"meta\", \"timeout\"):\n\t\t\tif key not in metadata:\n\t\t\t\traise ValueError(\"Not all of meta or timeout are set!\")\n\t\t\t\n\t\t\tself.metadata[key] = metadata[key]\n\t\t\tself.timeout.setdefault(key, 0.0)\n\t\t\tself.timeout[key] = timeout\n\t\n\tdef __call__(self, record):\n\t\t\"\"\" Calls __dict__ with a unique key \"\"\"\n\t\treturn dict(((key, self.__hash__(record"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t# TODO: add tests and replace with function\n\tif (timeout is not None) and ((timeout >= 0) and (timeout < 1)):\n        if metadata is None:\n            # Do logging in after transaction execution\n            return unit_of_work(metadata=metadata, timeout=timeout, log=self.log.with_context(\"transaction\"))\n        else:\n            return unit_of_"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\tif isinstance(metadata, dict) or dict_is_null(metadata):\t\t# pragma: no cover\n\t\t@unit_of_work(\"metadata\",timeout)\n\t\tdef unit_of_work(metadata=None,\n\t\t\t\t\ttimeout=None,\n\t\t\t\t\ttimeout_mode=None):\n\t\t\treturn unit_of_work_impl(\"unit of work\",metadata,timeout,timeout_mode)\n\n\t# noinspection PyUnusedLocal\n\tif not isinstance(metadata, dict): \t\t\t# pragma: no cover\n\t\traise TypeError('`metadata` must"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\tmodule = get_module(\"dbms\")\n\n\t_metadata = metadata\n\tstart = get_module(func.__name__ + \"#start\")\n\tif metadata:\n\t\t_metadata = dict(meta.copy())\n\telse:\n\t\t# This is required since the name may not be known, the unit of work is created for the whole object\n\t\t# We first see if it is an object of work.\n\t\tif \"__name__\" in metadata[\"object\"]:\n\t\t\t# Look up a work named \"object\".\n\t\t\t_metadata = metadata.get(\""}]}
{"task_id": "62e60e05d76274f8a4026cfd", "prompt": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t", "suggestions": [{"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\traise TypeError, self.__class__\n\n#-----------------------------------------------------------------------\n#  Methods related to a class\n#-----------------------------------------------------------------------\n\n#-----------------------------------------------------------------------\n# Utility methods/defines\n#-----------------------------------------------------------------------\n\ndef _getdict(obj):\n    \"\"\"\n\tObtains a dictionary mapping keys from an object.\n\tIf the object is a dict, this method returns it.\n\tIf it is a subclass, this method returns it.\n\tOtherwise, it raises a KeyError.\n\n\t@detail A subclass can have a method that returns a dictionary.\n\t@raise KeyError is a subclass of KeyError"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\tidx = []\n\tvalue = []\n\tvalue.append(self.key(key))\n\tvalue.append(self.value())\n\tif key in self._keys:\n\t\tindex = self._keys.index(key)\n\t\tif index < len(value):\n\t\t\tidx.append(index)\n\t\tif len(value) < idx[-1]:\n\t\t\tidx = [0]*len(value)\n\t\tfor i in value:\n\t\t\tidx[i] = [0]*(len(value)-len(i))\n\telif key in"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn self[key]\n\n\tdef size(self, key):\t\"\"\"\n\tReturns the number of items of the given key.\n\n\n:param key: a key\n:return: number of items\n:rtype: int\t\"\"\"\n\treturn len(self[key])\n\n\n\ndef save(output_dir, data):\n\tfor key in data:\n\t\tos.mkdir(output_dir + key)\n\tfor key in data:\n\t\tif not output_dir:\n\t\t\toutput_dir = \"/var/tmp/graphviz/\"\n\t\t"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn self.index(key=str(key))\n\n\n#---------------------\n\ndef _check_valid_value(value):\n\tif value:\n\t\tpass\n\telif value, ok = sys._getframe(1).f_globals.get(\"__spam__\", 1) or sys.gettrace()\n\n\t\tif value:\n\t\t\tprint \"WARNING:\"\n\t\t\tprint value\n\t\t\tprint \"=\"\n\t\t\tprint \"=\"\n\t\t\tprint \"=\"\n\t\t\tprint \"=\"\n\t\t\tprint \"=\"\n\t\t\tsys._getframe(1)\n\n\ndef"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn self.__dlist[key]\n\n\ndef __iter__(self):\n\treturn self.__dlist.__iter__(self.__counter)\n\n\ndef __setitem__(self, key, item):\n\t\"\"\"Set a key/value pair.\n\n\tThe first parameter should be the list of objects to be indexed. The\n\tsecond parameter is a list. The final element is the key/value pair\n\tit was loaded from.\n\n\tAlso adds a method which returns the index of the key in this list.\n\t\"\"\"\n\n\tself.__counter += 1\n\n\t"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn self._item_index(key)\n\n@index_docs(key=['key'],\n            key_to_name=set,\n            key_to_name_reverse=set,\n            item_index_func=self.item_index,\n            key_to_item_index=set,\n            order_doc=None)\ndef"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\ttry:\n\t\treturn self.get(key)\n\texcept NoSuchItem:\n\t\traise KeyError(\"No such key\")\n\ndef get(self, key, default=None):\n\t\"\"\"\n\tGet a value from the set, or the default if not found.\n\n:param key: key to retrieve\n:param default:\n\t  If a non-existent item is given, then take the default\n\t  value of the item as the first non-default entry.\n\t  In other words, default = default.\n\nIf default is supplied, it's used as the default value,"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\tindex = 0\n\titer = self._items.get(key, ())\n\twhile not iter:\n\t\titer = iter.__next__()\n\n\treturn index\n\n\ndef dict_keys(self):\n\t\"\"\"Return a list of keys in a dictionary.\"\"\"\n\treturn list(self)\n\n\ndef dict_values(self, key):\n\t\"\"\"Return a dictionary with the given value for the given key.\nThe dictionary values are sorted or not.\"\"\"\n\tif not isinstance(key, six.string_types):\n\t\traise TypeError(\"keys should be string\")\n\t"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn key[0]\n\n\ndef get_value(self, param_key):\t\"\"\"\n\tReturn value from a parameter, if any.\n\n:param key: a key\n:return: value, or else None\n:rtype: str\t\"\"\"\n\tparam_key = param_key.split('.')[0]\n\treturn self.values.get(param_key, None)\n\n\nclass IndexItem(object):\n\tdef __init__(self, index, *args):\n\t\tself.index = index\n\t\tfor arg in args:\n\t\t\tif"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\treturn self.__hash(key) % MAX_ITER\n\n\ndef load_table():\n\t\"\"\"\n\tLoad a database into the given table.\n\n\t:return: None\n\t:rtype: table\n\t\"\"\"\n\tfrom. import tables\n\tfrom dbs.db.base import Table\n\tfrom dbs.db.sql.sql_query import query\n\tfrom dbs.db.sql.sql_delete import delete_key\n\tif not tables:\n\t\traise ValueError(u\"\\npassion isn't available\")\n\tfrom dbs.db.dberror"}]}
{"task_id": "62e60da4d76274f8a4026cf1", "prompt": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t", "suggestions": [{"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t#if not keys:\n\t#\traise ValueError('only indexes or keys will be included')\n\t#keys = [self.f.value(keys_[i]).item() for i in range(len(keys_)) if keys_[i][1]].item()\n\n\t#values = []\n\t#if not keys:\n\t#\tfor value in self.item_dict(**key):\n\t\t#\t\tvalues.append(self.record_dict(value, *keys))\n\t#\t\tself.item_dict(**key)\n\treturn self.item_dict("}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tkeys = utils.ensure_str(keys, \"names are 'none', 'all' or empty\")\n\tif keys is None or len(keys) == 0: keys = []\n\tif not (keys[0] is None or len(keys[0]) == 0):\n\t\traise ValueError('One of: %s' % utils.modlist(keys))\n\n\tif len(keys)!= len(self._names):\n\t\traise ValueError('Exactly %d elements matching our values.'%len(keys))\n\t\t\n\tkeys = [key[0] for item in keys"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tif isinstance(keys, str): keys = list(values(''))\n\tkey_vals = ['%s=%s' % x for x in keys]\n\tvalues = []\n\tfor key, value in sorted(itertools.zip_longest(keys, self.keys, key_vals), key_vals, reverse=True):\n\t\tif len(value) == 1:\n\t\t\t# it's a key, don't include anything\n\t\t\tcontinue\n\t\tif self.filter_value(key, value):\n\t\t\tvalues.append(value)\n\treturn values[:"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\treturn []\n\nclass Rec(object):\n\tdef __init__(self, record, filters, index, parent=None):\n\t\t\"\"\"\n\t\t@param record: the record being searched\n\t\t@param filters: list of filters\n\t\t@param index: optional index to search for\n\t\t@param parent: optional container to search in\n\t\t@return: None always\n\t\t\"\"\"\n\t\tself.record = record\n\t\tself.filters = filters\n\t\tself.index = index\n\t\tif parent:\n\t\t\tself.parent = parent\n\t"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tval=[]\n\tfor key in keys:\n\t\tval.append(self.get_by_index_or_key(key))\n\n\treturn val\n\ndef get_by_name(name):\n\tr\"\"\"\n\tReturn key-value pairs in the database, where the\n\tkey names are defined by the db name.\n\n:param name: The key to lookup, usually the index or key; should \n\tpass the value for the key; can be a key name, a tuple/hash of\n\teach element; or a name of a CLOB database.\n:"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tvalues = []\n\tfor name,value in self.values():\n\t\tif not name.startswith('_'):\n\t\t\tvalues += [ name ] + [value]\n\tfor name,value in self.values():\n\t\tif name in keys:\n\t\t\tvalues += [name,value]\n\treturn values\n\n\ndef removeItem( self, item):\t\"\"\"\n\tRemove an item in a data model, optionally filtering\n\tto exclude or exclude based on values\n\n\t:param item: the item to remove\n\t:return: dict\n:rtype: dict\"\"\"\n\t"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\treturn values_recursive(\"\", self, *keys)\n\n// -------------------------------------------------------------------------------------\n// Copyright (c) Microsoft Corporation\n// Licensed under the MIT license.\n// -------------------------------------------------------------------------------------\n\nusing System;\nusing System.Threading.Tasks;\nusing Xunit;\nusing Xunit.Abstractions;\n\nnamespace ExtensionClient\n{\n    public class FileWatcherClientExtensionsTest : IDisposable\n    {\n        public FileWatcherClientExtensionsTest(ITestOutputHelper output)\n        {\n            _output"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\tkeys = list(keys)\t# list(set(keys)): can hold set keys\n\tif not keys and self.haskeys():\n\t\tkeys = keygen(self.listkeys())\n\tif self.index:\n\t\tvals = [self[key + \"_key\"] for key in keys]\n\t\tif 1 in keys:\n\t\t\tvals.extend(self._values[1])\n\t\tkeys = filterkeys(keys,vals)\n\telse:\n\t\tkeys = keys + [self.__class__(*keys)]\n\treturn keys\n\ndef iterkeys"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t# noinspection PyDeclarationRequirers\n\tkeys.sort()\n\t# noinspection PyMissingValueFromPy27\n        values = []\n\t# noinspection PyMultipleReturnOf\n        for index in keys:\n\t\t\t# noinspection PyBroadException\n\t\t\ttry:\n\t\t\t\tvalue = self.value_list_by_index(index)\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tvalues.append(value)\n\t\treturn values\n\t# if self.value"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\treturn [self.dict[\"val\"] for record in self.fields(record=\"values\")\n\t\tif record[\"val\"] is not None  # record doesn't exist\n\t\tfor key in keys or []]  # make sure we only select keys\n\ndef _key_filter(field_list, keynames):\n\tfields = []\n\tfor k, v in field_list:\n\t\tkeyname = field_to_keyname(k)\n\t\tif keyname in keynames:\n\t\t\tprint \"WARNING: key '%s' is in keynames\" % key"}]}
{"task_id": "62e60b10d76274f8a4026ccd", "prompt": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t", "suggestions": [{"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\tif isinstance(keys, string_types):\n\t\tkeys = [keys]\n\tif not isinstance(keys, (list, tuple)):\n\t\traise TypeError(\"keys parameter must be of type string or \"\n\t\t\t\t\t \"list of string\")\n\tif any(keys):\n\t\treturn dict(enumerate(keys)), []\n\treturn dict([(k, list(map(None, keys))) for k in keys])\n\ndef froms(ds, fields, dict_fields=None,\n\t\tvalidate_map=None, validate_index=None, **kwargs):\n\t\"\"\""}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\ttry:\n\t\treturn (self[item] for item in keys)\n\texcept (KeyError, IndexError):\n\t\treturn None\n\n#pragma ident \"$Id$\"\n/*----------------------------------------------------------------------------*/\n#include \"cs.h\"\n/*---------------------  end of csgit ----------------------------------------*/\n\n<?php\n/**\n * @link http://www.yiiframework.com/\n * @copyright Copyright (c) 2008 Yii Software LLC\n * @license http://www.yiiframework.com/license/\n */\n\nnamespace Yiisoft\\Imap\\Search;\n\n"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\tif (keys is None and self.dtypes!= None and not self.dtypes.contains_multiple_of()):\n\t\traise ValueError(\"You must pass field keys or values to data\")\n\tif keys and keys not in self.keys_and_values_:\n\t\traise IndexError(\"Keys and values must be unique\")\n\tif keys and len(keys) > len(self.keys_and_values_):\n\t\traise IndexError(\"Keys and values are of length \" + str(len(keys)))\n\tfor key in keys:\n\t\tif key not in self"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\tindex_start = 0\n\tt_items = []\n\ti = keys[0] if len(keys) > 0 else 0\n\tkeys = keys[1:]\n\tm = type(self).__members__\n\tkeys = (m.get(name, key) for name, key in keys.items())[:-1]\n\tkeys += ([self.record_name] if len(keys) > 0 else [])\n\tkeys = tuple(keys[index_start:])\n\n\ttry:\n\t\tif len(keys) > 0:\n\t\t\twhile len(keys)"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t#\n\tif keys is None:\n\t\tkeys = self._index_with_keys()\n\t\tif keys is not None:\n\t\t\tkeys = keys.values\n\n\tif self._is_table():\n\t\trec = self.get_iter().find(self._table=True)\n\t\tif not rec:\n\t\t\traise IndexError(\"Cannot access non-existent table\")\n\n\t\tnames = set(rec.iter_fields)\n\t\tif \"name\" not in names:\n\t\t\tnames.add(\"name\")\n\n\t\tindexes ="}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\tkvs = {}\n\tfor k, v in sorted(self._sort_set(keys)):\n\t\tif v is not None:\n\t\t\tif k not in kvs:\n\t\t\t\tv = kvs[k] = v\n\t\tk1, v1 = k[1], v[1]\n\t\tk2, v2 = k[2], v[2]\n\t\tk3, v3 = k[3], v[3]\n\t\tk4, v4 = k[4], v[4]\n\t\tk5, v5"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\tif keys is None:\n\t\treturn {}\n\treturn self.__dict.items(keys)\n\nconst chai = require(\"chai\");\nconst expect = chai.expect;\nconst faltro = require(\"faltro\");\nconst sinon = require(\"sinon\");\n\nimport { loadJsonFile } from \"../../../../common/helpers/json\";\nimport {\n    getMock,\n    assertEmpty,\n    assertNotEmpty,\n    assertError,\n    assertErrorThrow,\n    isEmpty,\n    isPlainObject"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\tif (len(keys) < 0): raise IndexError(\"length of keys must be positive\")\n\tif (not isinstance(keys, int)):\tkeys = [keys]\n\tkeys2 = keys if keys else [None]\n\tif (len(keys2) < 1):\tkeys2 = None\n\tvalues = record_index( self.data.get(key, []) or [] )\n\tif (len(keys2)!= 1):\trecord_extend( self.data[key2], keys, values )\n\telse:\trecord_extend( self.data[key"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\tif key in self._items:\n\t\tself._index_of_item(key)\n\t\treturn self._items[key]\n\tif keys:\n\t\tself._index_of_keys(keys)\n\t\treturn self._items[keys]\n\treturn {}\n\n\ndef iter_data(self, iterator_or):\n\t\"\"\"Iterate over RecordData, adding or replacing values\n\tin :class:`Record` with each of its contained values. If\n\titerator_or is specified it will be used to iterate over\n\tthe records.\n\n\t:param"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\treturn _data(self, keys)\n\n\ndef finditems(keys, values=None, ignore_none=False, **kwds):\n\t\"\"\"A Python iterator over the values of a given record with\n        the same index. Keys provided that are not in the record\n\twill be inserted and in case of\n\tno keys specified will cause the iterator to be restarted\n\tafter one iteration if ignore_none is True.\n\tThe :exc:`IndexError` is raised if an error is encountered\n\twhile iterating over the dictionary, or if the index is\n"}]}
{"task_id": "62e6087bd76274f8a4026bfa", "prompt": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\tn = 0\n\tfor b in self:\n\t\tif n > 127:\n\t\t\tself.append(b)\n\t\t\tn = 0\n\t\telse:\n\t\t\tn = n << 1\n\t\tself.append(b)\n\treturn n\n\n\n# This is a utility function used by py-3.3 to test whether two input\n# integers are equal.  It is designed to be useful for some of the\n# Python 3 functions, such as the *abs() or *square().\n\n\ndef is_ne_zero(x):\n\treturn"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\tif self.peek():\n\t\treturn self.pop_bytes()\n\n\treturn 0\n\n\t\ndef pop_u32(self):\t\"\"\"\n\tRemove the first four bytes of data, returning them as a big-endian\n32-bit unsigned integer.\t\"\"\"\n\tif self.peek():\n\t\treturn self.pop_bytes()\n\n\t\t#return pop_u32(0)\n\telse:\n\t\treturn 0\n\n\t#print \"POP {} {} {}\".format(0, self.peek(), pop_u16())\n\n\t#print \""}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\tif not self.is_eof():\n\t\tlen = (self.read_little_endian_16() if self.is_bigendian else 0)\n\t\tself.set_position_abs(self.position()-len)\n\t\treturn len\n\t\treturn 0\n\n\treturn u2be(u16(self.pop_u16()),self.position())\n\n\ndef add_u8(self, count):\t\"\"\"\n\tAdd n+1 bytes of data to big endian 32-bit uint, moving the remaining bytes (if any) into the \n\tdestination"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t# FIXME: The most efficient way to do this is to use both of these\n\t# functions.\n\t#\n\t# Use the simplest approach: if the first byte is 0, drop it\n\t# (ie., drop off the second and then the third).\n#\twhile 1:\n#\t\tval = self[self._offset]\n#\t\t\n#\t\t# If the byte is -128 it's a negative number, drop it\n#\t\tif 0 <= val < 128:\n#\t\t\tself[self._offset] = 0\n#\t\t\tself._offset +="}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\tv = self.data.pop()\n\treturn int.from_bytes(v.bytes[:-2], byteorder='big')\n\n# XXX This is largely unnecessary.  It is only required for testing;\n# we will do something better.\ndef read_u8(self):\t\"\"\"Read a single byte\"\"\"\n\treturn self.data.get()\n\ndef read_u16(self):\n\tw = b\"\"\n\twhile 1:\n\t\tc = self.read_u8()\n\t\tif c == b'\\'' or c == b'0"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\tret = bytearray()\n\tfor i in range(3, len(self), 2):\n\tif self[i]!= 0:\n\t\tret[i] = self[i]\n\t\tret[i+1] = self[i+1]\n\n\treturn ret\n\n#if defined(HAS_LIBX11) && defined(HAS_LIBX11_XLIB)\n// This code enables use of 'LibX' X11 library in XLib. This allows X11.\n\n#include <stdlib.h>\n#include <X11/Xlib"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\treturn self.bytes[~self.index]\n\ndef pop_u32(self):\n\treturn pop_u16(self.index)\n\ndef pop_u64(self):\n\t\"\"\"Returns the 64-bit representation of the integer.\"\"\"\n\treturn pop_u8(self.index)\n\ndef pop_u8(self):\n\t\"\"\"Returns the 8-bit representation of the integer.\"\"\"\n\treturn pop_u16(self.index)\n\ndef pack_u32(value):\n\treturn (value >> 24) << 16 | (value >> 8)"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\tlength = 0\n\tfor octet in self:\n\t\tb = octet & 1\n\t\tif b!= 0:\n\t\t\tlength = length + 1\n\t\telse:\n\t\t\treturn b, length\n\treturn 0, length\n\ndef pop_char(self, length=1):\n\t\"\"\"\n\tRemove the last character from the input.  This function does\n\tnot follow semantics described in section 5.1 of The Golub bit-\n\trang in the specification.\t\t  This differs from the\n\tprevious version.\n\t\"\"\"\n\treturn self.__get_"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\tself.pop_u16()\n\treturn self.buffer[-2]\n\n\t\n#\tThis appears to be broken in new versions of the Python interpreter,\n#\tso we need a dummy opcode to get it working...\n\n\nclass PopException(Exception):\n\t\n\t    pass\n\n\nclass NotStackedError(Exception):\n\n\tdef __init__(self, *msg):\n\t\traise NotStackedError(self.args[0])\n\n\nclass StackError(Exception):\n\n\tdef __init__(self, *msg):\n\t\traise Stack"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\tbits = self.next_u16()\n\treturn bits.view(L_uint) - 1 \n\n\t# This probably isn't necessary?\n\t# If the user specified a size, it is expected to be an\n\t# unsigned 32 bit integer.  This is a workaround for the Python\n\t# compiler bug.\n\t#\n\t# It would probably be better to provide function_call_raw(0:b[4]) to\n\t# implement this.\n\n\n\n_int_min = byte_limits._int_min\n_int_min = _int_min"}]}
{"task_id": "62e6087ad76274f8a4026bf2", "prompt": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t", "suggestions": [{"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import sys, warnings\n\tfrom socketserver import server, backlog \t# NOQA ELSE\n\tdef _DiscardResponse(response, info=None):\n\t\tbuffer = []\t\t\t# type: Buffer\n\t\tif not response:\n\t\t\treturn buffer, None\n\t\tbuffer += response.getheaders()\t# get the last, last, next, etc. headers\n\t\tif not response.getheaders():\n\t\t\treturn buffer, response\n\t\ttbuf, phdr = response.getheaders()\t# fetch the headers, and"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import string\n\tdef dehydrationHook(record):\n\t\tpass#IMPORTIMPORTIMPORT import socket\n\t\tif self.mode == \"unpack\":\n\t\t\tpass#IMPORTIMPORTIMPORT import Queue\n\t\t\tself.message_send(record)\n\n\tdef send(self, data, timeout=0):\n\t\tpass#IMPORTIMPORTIMPORT import socket, string\n\t\t# socket: buffer to send\n\t\tbuffer = self.message_send(data, timeout)\n\t\tif buffer is"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from Queue import Empty          as QEmpty                                 as QEmpty\n\tpass#IMPORTIMPORTIMPORT from Queue import Full            as QFull                                  as QFull\n\tpass#IMP"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import types\n\tpass#IMPORTIMPORTIMPORT from StringIO import StringIO\n\n\tdef deliver(sender, payload):\n\t\t\"\"\"\n\t\tpayload is a string; this is a dictionary to create\n\t\tdehydrate type (class) to dehydrate function.  Dehydration\n\t\tfunctions are passed the value and their parameters:\n\t\t  class, param1, param2 (optional), and so on (dehydrate).\n\t\t\"\"\"\n\t\tif payload.get('type') is None:\n\t\t\tpass"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t_pickle(self._p_api)\n\tself.data.extend(self.data)\n\tret = tuple([])\n\tif self.last_id < self.last_cmd:\n\t\tdecompressed_data = self._decompress_data(self.last_data)\n\t\tdecompressed_id = self.id + self.last_id\n\n\telse:\n\t\tdecompressed_data = self._decompress_data(self.last_data, self.last_cmd)\n\t\tdecompressed_id = self.last_cmd + self.last_"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\tif n <= 0:\n\t\tn = 512 \t\t# only large amount of records\n\t\tif n < 0:\n\t\t\tn = 0\n\tif qid <= 0:\n\t\tqid = n\n\tif dehydration_hooks is not None:\n\t\tdehydration_hooks = dehydration_hooks or {}\n\t\t_dehydration = _dehydration_hooker( dehydration_hooks = dehydration_hooks \n\t\t\t, dehydration_type = 'd' )\n\t\t_result"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from EMAN2_cppwrap import Cube, EMAN2_new\n\tpass#IMPORTIMPORTIMPORT from string import split\n\n\tif dehydration_hooks is None:\n\t\tdehydration_hooks = {}\n\n\tif hydration_hooks is None:\n\t\thydration_hooks = {}\n\n\t# read data\n\trecord = []\n\tfor i in range(self.n_records):\n\t\t# set default state\n\t\tif dehydration_hooks and i >="}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\treturn Response.new_empty()\n\n\nclass DISCARD(Packet):\n\tr\"\"\"\n\tThis Packet represents an orphan packet.\n\n\tA DISCARD message does not alter any of the other packets.\n\t\"\"\"\n\tdef handle(self, *args, **kwargs):\n\t\t# The following calls should not have any arguments\n\t\t# at the moment, since they may not have been sent\n\t\t# (this is the case if the packet actually gets sent,\n\t\t# as per request, above).\n\t\tsuper(DISCARD, self).handle(*args"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\treturn Response(self.data)\ndef rehydrate(self, type_class, dehydration_hooks=None,\t\t\"\"\"\n\tif type_class == str:\n\t\ttype_class =   type\n\tapp_id, dehydration_hooks = unpack(\n\t\t'dishydration_list.str',  dehydration_hooks=dehydration_hooks)\n\treturn self.rehydrate({'type_class':type_class, 'app_id':app_id+1}, dehydration_hooks"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\tfor _i in range(n):\n\t\tdehydration = dehydration_hooks or (None,)\n\t\tif dehydration == (None, 0):\n\t\t\tdehydration_hook = hq_dehydrate\n\n\t\telse:\n\t\t\tdehydration_hook = dehydration_hooks.get(dehydration, None)\n\n\t\tif dehydration!= (None, 0):\n\t\t\tq = packstream(type(type_=app, args=args.format))\n\t\t\tq[\""}]}
{"task_id": "62e60879d76274f8a4026bec", "prompt": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t", "suggestions": [{"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\treturn Bolt(self._txn_queue.append, mode=mode, bookmarks=bookmarks, metadata=metadata,timeout=timeout, db=db, imp_user=dehydration_hook_func(db), dehydration_hooks=dehydration_hooks, hydration_hooks=hydration_hooks, handlers=handlers,db=db)\n\"\"\"\n\ndef execute(mode=None, bookmarks=None, metadata=None, timeout=None,\n                    "}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\tif self._pack_env == \"main_pack\":\n\t\ttry:\n\t\t\tself.logger.info(\"begin\",\n\t\t\t\tdbname = self.my_db.my_dbname,\n\t\t\t\tdb_type = self.my_db.my_db_type,\n\t\t\t\tname = self.my_db.my_db_name,\n\t\t\t\tuser = db.db_user\n\t\t\t)\n\t\t\treturn(self._exec_pack.pack(\n\t\t\t\tname=name,\n\t\t\t\tdb=db,\n\t\t\t\tdb_type="}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\tif not self._started:\n\t\traise RuntimeError(\"must be started before beginning the transaction\")\n\tmsg = packstream.StructPacket(\n\t\tpack=packstream.pack_struct(\n\t\t\tpack=packstream.pack(\n\t\t\t\tpackfile=packstream.get_pack_file(self.__package__),\n\t\t\t\t__package__='__init__.py',\n\t\t\t),\n\t\t\t_type=pack.packstruct('PackType'),\n\t\t\t_id=(None,),\n\t\t),\n\t\tbody=packstream.PackStruct(\n\t\t\t"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t_begin_mode = 'WRITE'\n\t_begin_bookmarks = b\n\t_begin_metadata = metadata\n\t_begin_db = db\n\t_end_mode = 'READ'\n\tdef _get_begin(self):\n\t\treturn _begin_mode.join(self._begin_mode_parts())\n\tdef _end_with_begin(self):\n\t\tif not self._end_mode.endswith('WRITE'):\n\t\t\tself._end_mode +='WRITE'\n\t\t_begin_bookmarks = b\n\t\tself._begin_"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\tif mode == None:\n\t\tmode = \"read\"\n\treturn bolt.dehydration.begin(mode=mode, bookmarks=bookmarks,\n\t\tmetadata=metadata, timeout=timeout, db=db, imp_user=imp_user,\n\t\tdehydration_hooks=dehydration_hooks, hydration_hooks=hydration_hooks,\n\t\thandlers=handlers)\n\n\t\ndef dehydrate(type, value, headers=None, dehydration_type=None, dehydration_hook="}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\tif Bookmark._exists(metadata['@id']):\n\t\tBookmark._find(metadata['@id'],\n\t\t\tstart_time=self.time)\n\t\t# Bookmarks start at midnight of Gregorian and are the same day for that hour with 0:00\n\t\tmidnight = (datetime.datetime.fromtimestamp(start_time) + datetime.timedelta(0,0,0))\n\t\tfor bookmark in Bookmark.find().limit(0).offset(0):\n\t\t\tif not db._exists(str(bookmark.db), dehydr"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\tdef _begin(self, bookmarks_json, timeout=None, db='bop', db_name='', imp_user=None,\tdehydration_hooks=None, hydration_hooks=None, handlers=None):\n\t\tconn = self.connection\n\t\tdefer func() end\n\t\t#if db is not None and db in conn.settings.DATABASES:\n\t\t#\tdb = conn.settings.DATABASES[db]\n\n\t\t# \tif db is None:\n\t\t#\t\traise Exception(\"Can't start transaction with"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\treturn __class__(**self._start(mode, bookmarks, metadata, timeout, db,\n\t\t\t\t\t\t\t\t\t\t dehydration_hooks, hydration_hooks,\n\t\t\t\t\t\t\t\t\t\t handlers))\n\t# TODO: change signature to the end of __init__\n\t#def _start(self, mode=None, bookmarks=None, metadata=None, timeout=None, db=None,\n\t\t\tdehydration_hooks={}, hydration_hooks={},\thandlers=[]):\n\t\t\"\"\" Begins a HTTP transaction to a specific database.\n\n          "}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\treturn self.s._run_begin(self, mode, bookmarks, metadata, timeout, db, dehydration_hooks, \"CREATE %s\" % self.db, imp_user, dehydration_hooks, hydration_hooks, handlers)\n# End: dehydrate functions\n\n#\n# Query\n#\n\n\ndef query(mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a QUERY message to the output queue.\n\n:param mode: access mode for routing - \"READ\" (default)"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\tif mode is not None: mode = _default_mode_to_string[mode]\n\tif bookmarks is not None:\n\t\t_default_bookmarks(bookmarks)\n\tif metadata is not None:\n\t\t_default_metadata(metadata)\n\tif timeout is not None:\n\t\tresponse = {\"timeout\": timeout}\n\t\tif _debug:\n\t\t\t_debug_output(response, \"packstream\", \"begin\")\n\t\treturn response\n    _default_logger.debug(\n\t\t\"PACKSTREAM.BEGIN({mode},{book"}]}
{"task_id": "62e60723d76274f8a4026b75", "prompt": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from math import ceil\n\tpass#IMPORTIMPORTIMPORT from math import floor\n\tpass#IMPORTIMPORTIMPORT from math import im\n\tpass#IMPORTIMPORTIMPORT from itertools import count\n\tpass#IMPORTIMPORTIMPORT from scipy.integrate import cheby_ball, cheby_ball_cpe\n\tpass#IMPORTIMPORTIMPORT from scipy import cos, sin, tan\n\tpass#IMPORTIMPORTIMPORT from sys import hashlib\n\tpass#IMPORTIMPORTIMPORT"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\tpass\n\t>>> round_half_to_even(4)\n\t1\n\t>>> round_half_to_even(6)\n\t2\n\t>>> round_half_to_even(6.2)\n\t3\n\t>>> round_half_to_even(6.5)\n\t4\n\t>>> round_half_to_even(6.7)\n\t5\n\t>>> round_half_to_even(7)\n\t6\n\t>>> round_half_to_even(7.2)\n\t7\n\t>>> round_half_"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\tdef round_half_to_even(n):\t\"\"\"\n\t\tfor bit in range(0, 16):\n\t\t\tn >>= 1\n\t\t\tif n == 0:\n\t\t\t\treturn 0\n\t\treturn 1\n\tdef round_half_to_even_mask(n):\n\t\tn = n - 0x100\n\t\tn = n - 0x10000\n\t\tn = n - 0x1000000\n\t\tn = n - 0x80000\n\t\treturn round_half_to_even(n)\n\t# XXX do these in terms of round"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\treturn round_half_to_even(round(n))\n\n\n\"\"\"\n\n\t\"round_half_to_odd(n)\", \"\"\"\n    >>> import integereg as ieg\n    >>> n = ieg.randrange(3)\n    >>> round_half_to_odd(n)\n    0\n    >>> round_half_to_odd(5)\n    4\n    >>> round_half_to_odd(2.7)\n    0\n    >>> round_half"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\treturn [round_half_to_even(i) for i in n]\n\n\ndef round_half_to_even_half_up(n):\n\t\"\"\"\n\tThis calculates the round up (in this case, ``n``, returns the\n\tnearest even power of ``n``) of two.\n\n\t>>> round_half_to_even_half_up(5)\n    5\n    >>> round_half_to_even_half_up(3)\n    4\n    >>> round_half_to_even_"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t# from numpy.lib import array_print\n\t#\n\t# if type(n) is float:\n\t# \tassert 0.0<=t<1.0\n\t# if type(n)==int:\n\t# \tassert n in range(1,10)\n\t#\n\t# if n<1 or n>=10:\n\t# \traise ValueError(text + type(t))\n\t#\n\t# t = 10.0*math.floor(n/10.0)\n\t# assert type(t) is float\n\tif n<"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\treturn ((2 * n + 1) // 2) + ( (n + 1) // 2 )\n\n\ndef round_half_to_odd(n):  # note: need an __index__ function,\n\t\"\"\"Round `n` with `len(self)` is a bit different from `round_half_toward`.\"\"\"\n\treturn (((n + 1) % 2) * odd_half) + (len(self) // 2 * odd_half)\n\n\ndef round_to_even(n):\t\"Randomly shuffles `n` towards even\"\n"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\tif n < 5:\n\t\tn = -n\n\t\tn += 1\n\treturn n\n\n//\n//    Copyright (C) 2009-2017, Dhiru Kholia. All rights reserved.\n//    Copyright 2019, Google Group. All rights reserved.\n//    Author: Michael Meissner\n//\n#pragma once\n\n#include \"Platform/Singleton.h\"\n#include \"../../../Misc/Types.h\"\n\n#if!defined(GD_BUILD_WITH_MDX_API)\n#    define MD"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\tdef _round_half_to_even(n):\n\t\tif not ((n and\n\t\tn * 2) % 2):\n\t\t\treturn\n\t\tif (n > 1 and\n\t\tn % 2) or n:\n\t\t\tx = 1 + n + (n / 2)\n\t\t\ty = 2 * n\n\t\telse:\n\t\t\tx = (n // 2 + 1) * 2\n\t\t\ty = (n % 2) * 2\n\n\t\tif n == 1:\n\t\t\treturn x ** (2 * (x + 1"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\tif n<0.0:\n\t\treturn -0.0\n\tif n=int(round(n/2.0)*2)\n\treturn n\n\nif __name__==\"__main__\":\n\t# print(round_half_to_even())\n\t# print(round_half_to_even(3,6))\n\t# print(round_half_to_even(3.4,2.6,5.5))\n\t# print(round_half_to_even(3))\n\t# print(round_half_to_even"}]}
{"task_id": "62e60707d76274f8a4026b69", "prompt": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tif name not in srid_map:\n\t\traise ValueError(\"name %s of type %s has been reserved\"\n\t\t\t% (name, srid_map[name]))\n\tpoint_type = Point\n\tpoints = set()\n\tfor field in fields:\n\t\tpoint_type.add_point_instance(field.get_name(), field.get_position(),\n\t\t\tfield.get_srid_map(), srid_map)\n\t\tif point_type.get_type(field.get_name()) == type.Point:\n\t\t\t"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tif len(fields.data_types) > 1:\n\t\tfields = fields.data_type(fields.default_type_dict[name])\n\n\tif srid_map:\n\t\tfields = {r.name: r.srid for r in srid_map}\n\n\treturn GeometryField.Point(fields)\n\n\ndef box_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Box subclass.\t\"\"\"\n\tif len(fields.data_types) > 1:\n\t\tfields = fields.data_"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tif len(fields)!= 2:\n\t\traise TypeError('field sizes must be 2 numbers')\n\n\nclass Geometry(GeometryBase):\n\t\"\"\"\n\tProvides basic geometry methods.\n\t\"\"\"\n\tdef get_vertices(self, point=None, srid_map=None, field=None):\n\t\t\"\"\"\n\t\tReturns the vertices of a point as a list.\n\t\t\"\"\"\n\t\t# make sure no geometry method is implemented\n\t\traise NotImplementedError(\"not implemented\")\n\n\tdef get_vertices(self, point, srid_map=None,"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t# XXX if the fields are non-empty, then this point\n\t# only has one type, and at least one of \"field name\"\n\t# is also present. Do we want to override them?\n\t# XXX We only support field names containing numbers, but not the 'x',\n\t# 'y' or 'z' suffix.\tWhat is they?  XXX\n\tfields[\"x\"] = [None, None]\n\tfields[\"y\"] = [None, 0]\n\tfields[\"z\"] = [None, 0]\n\tfields2 = {}\n\tfields2[\""}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tif len(fields) == 0:\n\t\tfields = ('coordinates', 'dimensionless_point', 'normal')\n\tfields_map = {}\t\t\t# for each of the fields\n\tfor field in fields:\n\t\tfields_map[field] = dictfield.get_mapping()['field'+str(field)]\n\tpoint_type.fields_config = fields_map\n\treturn point_type\n\ndef map_to_index(def_map, field, srid_map, *args, **kwargs):\n\t\"\"\"\n\tDynamically create a Map subclass."}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tif name == \"Point\":\n\t\treturn Point\n\telse:\n\t\treturn cls._point_class(name, fields, srid_map)\n\n\ndef polygon_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Polygon subclass.\t\"\"\"\n\tif name == \"Polygon\":\n\t\treturn Polygon\n\telse:\n\t\treturn cls._polygon_class(name, fields, srid_map)\n\n\n# from ast import __all__\nclass Points:\n\t\"\"\"\n\tRepresents a list of points.\n\n"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t# The names will match the names of a class named name.\n\tpoint = Field()\n\tpoint.name = name\n\tpoint.srid_map = srid_map\n\t# Point will be the same type as the original.\n\t# Any field that specifies an ID is just a string, and its ID\n\t# field will then be overwritten.\n\treturn point\n\n# A field created from a single value with a type_name\n_Point_id__ = Field(type_name = \"ID\", identifier=True, required = False)\n\n\n@AttributeOrderer"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\treturn PointClass(name, srid_map[fields])\n\n# =============================================================================\ndef spatial_def(name, fields, srs):\t# for testing\n\treturn SpatialClass(name, fields, srs[\"spatial\"])\n\nimport { BaseEntity } from './base.entity';\nimport { ColumnConfig } from '@fimbul/ymoment';\nimport { Entity, Property } from '@fimbul/ymoment';\nimport {\n    BaseEntityOptions,\n    ColumnDefinition,\n    RelationDefinition,\n    TableDefinition\n} from '@fim"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\tclass Point(object):\n\t\tpass\n\t\"\"\"\n\tCreate a Point subclass (or a subclass created with\n\t  PointClass()).\n\n\tArgument name is used as the key in\n\t  (the tuple passed to create_class(),\n\t  with args_used_together ())\n\tand (the string passed to create_class(),\n\t  with (args_used_together (),\n\t  args_together, and (args_used_together (), \"\")))\n\n\tArgument fields is either a 3-tuple\n\t  or a 1-tuple (see the example\n\t  below),"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\"\"\"\n\t@staticmethod\n\tdef point(name, p1, p2, p3=None, p4=None, dsid=None, srid=None):\n\t\t\"\"\"\n\t\tConstruct a Point with proper parameters.\n\n\t\tInputs:\n\t\tname\t\tname of the point that will carry the id\n\t\t\"\"\"\n\n\t\tfields = FieldSet([('id', sa.VARCHAR), ('name', sa.VARCHAR), ('value', sa.VARCHAR), ('time', sa.DateTime)])\n\t\tfields = fields.add()\n\t\tfields ="}]}
{"task_id": "62e5dc9ed76274f8a4026b5b", "prompt": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\tdef bar(x):\n\t\t# do something cool.\n\tprint(x)\n\nWith 'deprecated' being deprecated, a warning is issued with\nthe help keyword argument:\n\n    @deprecated(message=\"test\")\n    def foo(message):\n        print(\"bar\", message)\n\nIf the `showwarning` flag is set to `True` or `False`, the warnings\nwill be displayed with the `warn` and `warning` directives.\n\"\"\"\n\ndef test_all_deprecated_functions():\n\tparser = opt"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t@deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\tdef bar(...args):\t\"\"\"\n\t\t\"\"\"\n\n\nclass Test_callable:\n\tdef test_callable_decorator_(fun):\n\t\t\"\"\"Decorator used to suppress the old behaviour, after\n\t\t\t\tthe old class has been deleted.\n\t\t\"\"\"\n\t\twith warnings.catch_warnings() as logging:\n\t\t\tlogging.warn(\"\\n\"+\"Deprecation warning. \"\n\t\t\t\t\"You have been deprecated in Favourize.\\n\"+\n\t\t\t\t\"This is likely caused by the class in"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\tReturn type is int, if it is not None else None\n}\n\nfunc main() {\n\timport ffvdb\n\tf := ffvdb.MustGetDB(\"foo.yml\")\n\tf(\"bar.b\")\n}\n\"\"\"\n\nfunc TestDeprecation(t *testing.T) {\n\t_, err := TestVersion_Old(0)\n\n\tif err == nil {\n\t\tt.Errorf(\"TestDeprecation(0) had an error\")\n\t} else if (err.Error()!= \"the version for module 'foo.json' is \" +\n"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\"\"\"\n\n\t@deprecated(\"'name' has been deprecated in favour of 'title'\")\n\tdef name(x):\n\t\treturn 'hello'\n\n\t# Should be warned about DeprecationWarning\n\twarnings.simplefilter(\"always\", DeprecationWarning)\n\t```\n\nA common way of checking functions and methods (deprecated,\n\"deprecated\") is to call either the function object or the method\nobject.\n\nEach function can be checked, if it hasn't been deprecated but is\ndeprecated, the result of the function object. However, it may\ncontain a single argument,"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\tmessage = \"'foo' has been deprecated in favour of 'bar'\"\n\nThe message will be printed to ``sys.stdout``.\n\n.. versionchanged:: 1.2\n\n\"\"\"\n\n\nwarnings.filterwarnings(\"ignore\", message, category=FutureWarning)\n\npackage com.zhh.v1;\n\npublic interface Authenticator {\n    public void doLogin(AuthInfo info);\n\n    public void doLogout();\n\n    public void doUserInfoChanged(UserInfo userInfo);\n}\n\nclass MentionUser < ActiveRecord::Base\n  belongs_"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t@deprecated(\"'bar' has been deprecated in favour of 'foo' (see below)\")\n    def bar(x):\n        pass\t\"\"\"\n\t@deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\nThe deprecated version is now deprecated until deprecating code is added.\n\"\"\"\n\n\nclass Dropthel(object):\n\t\"\"\"\n\tThe abstract base class for a Dropthel class.\n\n\tThis implements some"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t@deprecated(\"'bar' has been deprecated in favour of 'foo'\")\n    def bar(x):\n        pass\t\"\"\"\n\nNote that the deprecated method and definition must be depreced too:\n\n    def deprecated(message):\t\"\"\"\n        Deprecated; do something useful:\n\n            # Do something useful:\n            foo()\n            bar()\n    "}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\tdef bar(x):\n\t\tpass\t\"\"\"\n\n\tdef depr(message):\n\t\twith warnings.catch_warnings():\n\t\t\tassert DeprecationWarning in message, message\n\t\treturn \"Hello!\"\n\n\n    @depr(\"A method decorated to have no arguments has been deprecated 'not' now. 'foo()' has not been deprecated\")\n\tdef depr_baz(x, message):\n\t\twith warnings.catch_warnings():\n\t\t\tassert DeprecationWarning in message, message\n\t\treturn \"Hello!\"\n\t'''\n\n\nclass"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\tdecorator = decorator_def_deprecated('foo', 'bar', deprecated=True,\n\t\t\t\tmessage=message, deprecated_name=[\n\t\t\t\t\t'foo', 'bar', 'deprecated3',\n\t\t\t\t\t'foo', 'deprecated3'])\n\n\"\"\"\n\nVERSION = ''.join(@deprecated_string(m) for m in (\n\t'description.', 'deprecation_msg', 'docs', 'docs_msg','recommended'))\nVERSION_OLD = ''.join(\n\t''.join(@deprecated_string(m) for m in [\n\t\t'author', 'deprecated"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\treturn \"foo\", 42\n\nThe returned result will vary in how the deprecated classes are\ndefined.\n\n.. _deprecated:\n\n    deprecated(message)\n\n\n\"\"\"\nimport warnings\nwarnings.warn(\"The deprecated module has been deprecated in favour of deprecated class\", DeprecationWarning) #''.warn(message)\n\nimport warnings as _warn\nwarnings.warn(\"The deprecated function has been deprecated in favour of deprecated class\", DeprecationWarning) #''.warn(message)\n\n__all__ = [\"deprecated\", \"deprecated_error\", \"deprecated_warn\", \""}]}
{"task_id": "62e4fc3c85ea98643089041e", "prompt": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\tos.environ['RC_OPTIONS'] = \"\\n\".join([(code+r_opts, r) for r in r_opts])\n\tprint(s'*RC_OPTIONS: {p}\\n'format(p=' \".join(sys.argv)))\n\tprint(\"\\nRC_OPTIONS = \", p)\n\treturn \"rc_setup\"\n\ndef _inline_r_cleanup(code: str) -> None:\n\tos.environ['RC_OPTIONS'] = \"\\n\".join([(code+r_opts, r) for r in r_opts])\n\t"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\tglobal R_CONF\n\tR_CONF = os.environ.get(\"R_CONF\", \"defaults.json\")\n\treturn os.environ.get(\"R_CONF\", r_conf)\n\ndef _inline_r_options(path: str, code: str, r_output: str, r_conf: Dict) -> None:\n\t\"\"\"Used when R output is saved.\"\"\"\n\tif path.endswith(\".json\"):\n\t\tR_CONF = r_conf\n\t\tR_CONF.update(read_file(path))\n\telse:\n\t\tfor line"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\tif (len(code.split(\" \")) < 2) and (not 'R' in code.lower()):\n\t\t# R can only be executed by the first program in the same R.exe file\n\t\treturn code + \"\\n\" + _setup(code + \"_r_setup.py\")\n\treturn code\n\n# _________________________________________________________\t _________________________________________________________\ndef _inline_r_setup_env(code: str) -> str:\t\"\"\"\n\tIf your code looks like this:\tSome R can be configured via env vars, but can\n\tonly be"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\tif not CODE_RE:\n\t\tCODE_RE = re.compile('(?P<r>code\\(\\)R(\\d+)\\)#')\n\n\t# We have no reason to match this.  We have to do some extra magic here.\n\tif '$'.join(CODE_RE.match(code).group(1) for x in CODE)!= (',', CODE):\n\t\tCODE = CODE + ',code'\n\n\treturn CODE\n\n//=======================================================================\n// Copyright 1997, 1998, 1999, 2000 University of Notre Dame.\n// Authors: Andrew L"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\treturn \"R\" + \":\" + \"R_setup\"\n\t\ndef _inline_r_start(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, and\nis only configured via R at that time, but can only be configured\nvia R options once R must be started. Typically some R\noptions may be set in env initializers rather than in R_setup\nor R_start.  This case, however, cannot be ensured at\nend, by using \"R_setup\" and \"R_start\" options.\n\t\t\"\"\""}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\treturn\n\ndef _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R must be configured via env variables and R has started.\n\n\tR can not accept any options, so R can not take into account R options\n\tthat have been specified.\n\n\t..\n\t\t- In R 1.0 R is in no way supported, the function is still valid\n\t\tusing the same naming scheme as R3.0/R2.0 by default, if this\n\t\tconfiguration is not yet set.\n\t\t- In R 2"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\treturn _r_config.get(\n\t\t\"R.TEST.DIR\",\n\t\t\".\",\n\t\t\".\",\n\t\t\".\"\n\t) or \"C:\\\\my_c\"\n\n\n_r_setup = staticmethod(_inline_r_setup)\n\n/*\n * Copyright (C) 2003 - 2015                                                       "}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\treturn ''\n\n\ndef inline_r_setup(\n\t\tcode: str,\n\t\tdebug: bool,\n\t\tsetup_flags: int,\n\t\tdebug_flags: int,\n\t\tverbose: bool,\n\t\tverbose_flags: int,\n\t) -> str:\n\n\t\"\"\"\n\tsetup_flags can take values from 0 to 4\n\tsetup_flags can take values from 1 to 3\n\tdef __init__(self, code=code):\n\t\t\"\"\"\n\t\tdef __init__(self, code=code):\n\t\t\tself."}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\tif code not in env:\n\t\traise AssertionError(\"Code not set in env, setting here\" + code)\n\treturn code\n\n\ndef _r_teardown():\n\tif _is_setup:\n\t\t_is_setup = False\n\t\tfrom rpython.rlib.rffi import rffi_malloc_helper\n\t\tffi = rffi_malloc_helper.FFI(0)\n\t\tffi.cdef(\"free\", [ffi.rffi_free])\n\t\tffi_free(ffi.opaque, 0)\n\tif _is_test:"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\"\"\"\n\tThe environment variable needs special handling.    \n\t\"\" R has no configuration, and no code is run, but our code is running.\n\tThe code just runs. This will only happen at the first \"r\" command. \n\tThe reason for this is because we need R to be run, but no code ran, \n\tand nobody actually will run it. The first command which contains a \n\tcode and is run is the \"run\" command, which is like we run on command\n\targuments which is in a sequence.   \n\tThe \"run\" command itself has"}]}
{"task_id": "62e4fbda85ea986430890405", "prompt": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\ta = type(a)\n\tif hasattr(a, \"xargs\"):\n\t\ta.xargs(\t\"python-xargs\",\n\t\t\t\"Command-line arguments for all platforms\",\n\t\t\t\"The arguments supported by Python %s.\" % (a.target_conc_string,))\n\nclass Xargs:\n\t\"\"\"\n\tA simplified implementation of xargs.\n\t\"\"\"\n\n\t@staticmethod\n\tdef xargs(\t\"\"\"\n\t\tA simplified implementation of xargs.\n\t\t\"\"\"\n\t\ta = type(a)\n\t\tif hasattr"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\tdef color_arg_documentation(self, argument, env):\n\t\t\"\"\"Use a simple string.\n\nTo use it use the variable with the target_concurrency argument,\nin that example you can use the environment variable with:\n\n\t@var_app=\"./tests/application\"\n\t@var_concurrency_task_concurrency=1\n\t@var_concurrency\n\nMore generally, \"xargs [=,*=] { - } { $xargs_var [ =,*=] { $v [ =,*=] | { $xvar [ =,*=] |} }"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\"\"\".\"\"\n\n\tdef __init__(self,color,target_concurrency):\n\t\txargs.xargs(self).__init__(color, target_concurrency, True)\n\nclass pty:\n\t\"\"\"A wrapper for pty. The constructor does nothing. This is a wrapper that is made for\na pty and can be overridden with a custom function. \"\"\"\n\tdef pty(self,*args):\n\t\treturn pty(self).*args\n\ndef pty_open():\n\tpid = -1\n\ttry: \n\t\tfor f in os"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\tdef __call__(self, *args, **kwargs):\n\t\targs = list(args)\n\t\tif xargs.concurrency_control_enabled:\n\t\t\treturn [(o, \"concurrent\") for o in args if o in xargs.concurrency]\n\t\treturn [[(o, \"concurrent\") for o in sargs] for sargs in args]\n\n\treturn __call__\n\nclass xclms_impl:\n\n\tdef __init__(self, **kwargs):\n\t\ttry:\n\t\t\tos.setgroups(xargs.xcl_group,"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\tdef __init__(self, color = P_DEFAULT_COLOR, target_concurrency = 1):\n\t\tself.color, self.target_concurrency = color, target_concurrency\n\t\t\ndef read_config(filename):\n\treturn_line = \"\"\n\twith open(filename) as file:\n        with open(os.path.join(filename.replace('.cfg','.p'),\"config.ini\"), 'r') as infile:\n            return_line = infile.read()\n   "}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\tprint \"%s: %s \" % (self.name,\n\t\t\t   (self.color? \"on line %d, \": \"on \" + self.color))\n\nclass cfun(cfun.cfun):\n\t_target_concurrency = 2\n\t_color = True\n\nfrom CookbookParser import *\nimport sys\nif __name__ == \"__main__\":\n\tsys.exit(main(args=sys.argv[1:]))\n\n/*\n *  linux/kernel/time.c\n *\n *  (C) 1991  Linus Torvalds"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\tdef xargs(self,&self):\traise NotImplementedError()\n\n\nclass _PTY_TYPEMAP_:\n\t\"\"\" The typemap contains an array of all declared types.\n\"\"\"\n\tdef __init__(self,*args):\n\t\tPTY_TYPEMAP_ENTRIES = {None: '__builtin__',\n\t\t                       '*': '_builtin*',\n\t\t                       'void': 'void*',\n\t\t   "}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\"\"\")\n\tdef main(self):\n\t\tpass\n\n\t\ttmp_file_path = os.path.abspath(os.expanduser(self.sourcefile))\n\t\tself.print_usage()\n\t\tpty = threading.Thread(target=self.start, name=\"pty\", \\\n\t\targs=([tmp_file_path, ]*5), \\\n\t\tdescription=\"Python interface of a pty\", \\\n\t\tshield=True)\n\t\tpty.start()\n\n\n\tdef start(self):\n\t\t"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\tdef foo(self, *args):\n\t\tif xargs.colors:\n\t\t\treturn \"xargs; color=%s; %s\\n\" % (xargs.colors, *args)\n\t\treturn xargs.target_concurrency\n\treturn foo\n\nprint(xargs)\n\n// Code generated by easyjson for marshaling/unmarshaling. DO NOT EDIT.\n\npackage ecs\n\nimport (\n\tjson \"encoding/json\"\n\n\teasyjson \"github.com/mailru/easyjson\"\n\tjlexer \"github.com/mailru/"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t#xargs._options\n) \n\t#color = False\n\tif xargs._opts[\"color\"] is True:\t#pylint:disable=unused-argument\n\t\treturn\n\telif xargs._opts[\"target_concurrency\"] is True:\t#pylint:disable=argument-length\n\t\txargs._options[\"target_concurrency\"] = 0\n\t#color = True\n    \n\nclass _xpt(object):\n    \"\"\"\n    Wrapper for subprocess.Popen\n    A simple wrapper for Popen, to avoid having to"}]}
{"task_id": "62e4fbda85ea986430890403", "prompt": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\tseq = list(seq) + list(seq[1:])\n\treturn [seq[i] for i in range(len(seq))]\n\ndef _shuffle_by_size(seq: Sequence[str]) -> list[int]:\t\"\"\"\n\tdef _shuffle(seq: Sequence[str]) -> list[int]:\t\"\"\"\n\t\tnum_shuffle = len(seq) * -1\n\n\t\tif num_shuffle == 0:\n\t\t\t# \tNone\n\t\t\tfor i in range(len(seq)):\n\t\t\t\tyield i\n\t\t\t# \tNone"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\tkeys = random.sample(seq, int(math.ceil(len(seq)/2)))\n\treturn [next(keys) for i in range(len(keys))]\n\n\n# Get all of these from command line args\n\ndef get_args():\n\t\"\"\" Get the program arguments\t\t \"\"\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('-n', '--ncpus', dest='ncpus', default=0, type=int, help='Number of cpus')\n\tparser.add_argument('-d', '--datasize', dest='datasize', default='auto',"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\treturn [] if len(seq) and not osv_check_output():\n\treturn [p for p in random.sample(seq,_len=1000) if p in osv_check_output()]\n\ndef check_output2():\n\t\"\"\"Check the outputs of shuffled versions of sequences.\n\n\t\"\"\"\n\tseq = \"abc\"\n\tout = []\n\tif osv_check_output():\n\t\tfor p in random.sample(seq, _len=1000):\n\t\t\tout.append(p)\n\t\tosv_write(\"output2"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\treturn list(map(tuple, filter(bool, seq), seq))\n\t\ndef sort_by_key_and_sort_values(seq: Sequence[str]) -> Sequence[Tuple[str]]:\t\t#def _sorted_by_key_and_sort_values\t\t\"\"\"\n\t# Return a tuple of strings\t(\n\t#\tKey: Sort value,\n\t#\tSorted by sort key\n\t#)\n\t# return (key, values)\n\tsorted_by_key = _shuffled(seq)\n\tkeys = sorted_by_key["}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\tlst = random.sample(seq, 10)\n\n\t# Now let's do a lookup table to make sure every individual tuple gets all the items.\n\thash_table = set()\n\tfor _, i in lst:\n\t\thash_table.add(str(i))\n\n\tfor h in hash_table:\n\t\tprint(\"%d/%d\"%(i, len(lst) - 1 - i))\n\n\tlst2,lst3 = random.sample(lst, 2), random.sample(lst, 2)\n\tlst2,lst3 = lst2,"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\treturn [[r[0], r[1]] for r in seq]\n\n\ndef make_shuffled_text(text: str) -> text:\n\t\"\"\" Make a new text, not necessarily identical (as it is internally generated)\t\"\"\"\n\tif isinstance(text, list):\n\t\tseq_no = random.pseudo_random()\n\t\tseq_no += int(2**len(text) - 1)\n\t\ttext = \"\\n\".join(seq_no) + text[:text.rfind(\"\\n\")]+\"\\n\"\n\t\treturn text\n"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\treturn _shuffle_seq(seq)\n\n\t\ndef _shuffle_seq(seq: Sequence[str]) -> list[str]:\n\tshuffle = list(range(len(seq)))\n\n\t# shuffle left to right\t \n\tl = len(seq)-1\n\tfor i in range(0, l, 2):\n\t\t# print len(seq)-i, seq[i]\n\t\tshuffle[i], shuffle[l-i-1], shuffle[l-i-2], shuffle[l-i] = seq[i]\n\n\treturn shuffle \n\t"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t# \u4eceseq\u4e2d\u51fa\u73b0\u7684\u5143\u7d20\uff0c\u7136\u540e\u5728seq\u4e2d\u586b\u5145\uff0c\u5c06seq\u7684\u5143\u7d20\u52a0\u5165\u4e00\u4e2a\u4e3a\u7cbe\u786e\u6570\u7ec4\n\tshuffle_list = []\n\tfor l in range(0, len(seq), 1):\n\t\tr_i, r_o = int(seq[l]), int(seq[l - 1])\n\t\tshuffle_list.append(str(r_i))\t# \u751f\u6210l\u4e2a\u5143\u7d20\u7684\u6570\u7ec4\n\t\tshuffle_list.append(str(r_o))\t#"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\twhile True:\n\t\t# if next char is the same, just yield it.\n\t\ttry:\n\t\t\tchoice = choice_function(seq)\n\t\texcept:\n\t\t\ttry:\n\t\t\t\tyield os.path.split(seq[0])[1]\n\t\t\texcept:\n\t\t\t\tcontinue\n\t\t\tchoice = None\n\t\tif choice is None:\n\t\t\tbreak\n\t\telse:\n\t\t\tseq.insert(choice[0].index(choice[0]) + 1, choice[1])\n\n@run_before\ndef _shuffled"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\treturn random.choices(list(_shuffled(x)), key=chr(x), n=len(seq))[:len(seq)]\n\nclass Roman(object):\n\t\"\"\"\n\tRandom words to generate random fontration data\n\t\"\"\"\n\t\n\tdef __init__(self, word_size=128):\n\t\tself.word_size = word_size\n\n\tdef __call__(self, word : str) -> bytes:\n\t\treturn struct.pack(struct.pack(\">I\", self.word_size) + word, bitlen=0)"}]}
{"task_id": "62e4fb6585ea98643089032b", "prompt": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\ts = s.split('.')[-1]\n\tif   len(s) == 0:\n\t\treturn (0, -2, [s])\n\n\t# Now:\n\ta, b = s[:3], s[3]\n\n\tret = (int(b) - int(a))\n\tret += b\n\treturn ret\t\t\t# 1's in a, b - 1 in b\n\ndef main():\n\twhile True:\n\t\t# Input:\n\t\t#   b - size of input string to compare:\n\t\t# "}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tif len(s) == 0:\n\t\treturn 0, []\n\tres = []\n\ti, j, s = s.split('.')\n\ttry:\n\t\tj_r = int(j)\n\t\ti_r = j_r\n\texcept:\n\t\treturn 0, []\n\telif int(i)!= int(j):\n\t\tres.append((i, j*i_r))\n\t\ti_r = 1\n\tif len(s) > 0:\n\t\ti_r = i_r\n\t\tfor"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tif s:\n\t\tversion, rest = s.rstrip().split(\" \")\n\telse:\n\t\tversion = \"\"\n\treturn (int(version),\n\t\t((rest.split(\".\")[0], rest), rest.rstrip().split(\".\")[-1]),\n\t)\n\n\t\ndef convert_to_string(obj):\n\t\"\"\"\n\tConverts a module dictionary (DictType, DictObject) into a string\n\t\"\"\"\n\tdef f(f):\n\t\t\"\"\"\n\t\treturn f(dict)\n\t\t\"\"\"\n\t\treturn f('{"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\ta = int(s.split(':')[0])\n\tb = int(s.split(':')[0])\n\tif '0' == s[:3] :\n\t\tif int(s[3:])!= 0 :\n\t\t\treturn -1, -1\n\telif str(s[:2])!= '0' :\n\t\treturn int(s) > 0.1, -1\n\telse :\n\t\treturn int(s) == int(a) and int(b) == int(s)\n\n// Copyright 2011-2016 Google LLC\n//\n// Licensed"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\ti = match.fullmatch(s)[-1].group(2)\n\tif i == 'dev':\n\t\treturn i, '0.2', []\n\telif i == 'beta' or'stableb' or not i:\t# release 2,0.2 (beta, stableb)\n\t\treturn i - '0.2', [], []\n\telif i in ['5', '6', '7', '8', '9', '10']:\n\t\tif match.fullmatch(s).group(3)!= i:\t# release 0.8."}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tif \":\".join(os.path.basename(s).split(':')[0:1][1:])!= 'py':\t#\"Python 1.x\"\t\"CPython, version %s\"\t\"CPython \"\t\"CPython\"\"\"\"\n\telif \"0\".isdigit(): \t\t\t # Python 2 \"CPython\": major and minor version\t\"1.0.0\"\t\t\"CPython \"\t\t\t\t\"CPython\"\t\n\telif \"1\".isdigit(): # Python 2 \"CPython\": major version\t\t\"1.0.0\"\t\t\"CPython \"\t\t\t\t\""}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t# we have to get it from a number\n\t# we have to convert (int as string) to (int, int as string)\n\t# we cannot parse strings as ints\n\tparts: Tuple[int,...] = s.rpartition('-')\n\tif parts[0] < len(s):\n\t\t# it is a string\n\t\treturn int(parts[0]), int(parts[1])\n\ndef parse_git_version(s: str):\t\"\"\"A version function that will parse version numbers from git-1.7\t\"\"\"\n\t# we find the"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tpoor_version = (int(s), int(s) + 1)\n\treturn poor_version\n\n\ndef parse_flags(s: str) -> Tuple[int, int]:\n\tpoor_flags = (int(s),)\n\treturn poor_flags\n\n\ndef parse_environment_hash(s: str) -> Tuple[str, str]:\n\tpoor_environment_str = (s, \"\")\n\treturn poor_environment_str\n\n\n@lru_cache()\ndef parse_poor_environment(\n\tpoor_"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tif s[2:].lower() == \"0.7.8--\" or s[2:].lower() == \"0.7.7++\":\n\t\treturn (int(s[8]), int(s[7]))\n\tif s[2:].lower() == \"0.7.6--\" or s[2:].lower() == \"0.7.4++\":\n\t\treturn (int(s[8]), int(s[6]))\n\tif s[2:].lower() == \"0.7.1--\" or s[2:"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\tmin_v, max_v = 1, 5\n\t# https://git.kernel.org/?p=scm4grep/3.4.3/scm4grep.git/blob/master/3\\.4\\.3\\..+/3\\.4\\src\\core\\version.txt\n\ts = s.lstrip().rstrip()\n\ts = s.replace(\"+\", \"\")\n\ts = s.replace(\".txt\", \"\")\n\ts = s.replace(\"0+\", \"\")\n\tversion, _ = get_version(s, min_v, max_v)\n"}]}
{"task_id": "62e4fb4d85ea9864308902e7", "prompt": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\toutput = []\n\tin_path_sep = os.path.sep\n\tin_path_path = ''\n\tin_path = ''\n\tfor x in cmd:\n\t\tswitch x[0] & 0b01111111 | x[1] in ('s', 'S'):\n\t\tcase in_path_sep | in_path_path:\n\t\t\ttry:\n\t\t\t\t_slash_esc = x[0] & 0b01111111 | x[1] in ('S',)\n\t\t\t\tif _slash_esc:\n\t\t\t\t\toutput.append("}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tshebang_cmd = \"\"\"\\\n#!/usr/bin/env python3\n\n#ifndef __m68knommu_h__\n#define __m68knommu_h__\n\n#include \"m68kdefs.h\"\n\n#if defined(m68kconf.core) &&!defined(m68xk_core_k2_h__)\n/* FIXME: check for this with m68k.conf. */\nextern void (*m68xk_core_k2_h__)(struct CPU *, void *,...);\n#endif\nextern void m68k_init"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tfrom os.path import join\n\tfile, dirname = tempfile.mkstemp()\n\tcmd = join(dirname, cmd[0])\n\tcmd = '/%(cmd)s' % {'cmd':cmd}\n\n\tif cmd[:5]!= '/':\n\t\tcmd = '%(cmd)s' % {'cmd':cmd}\n\treturn cmd\n\n################################################################################\n\ndef print_command_help(command):\n\t\"\"\"\n\tPrints a help message in the terminal, with a newline\n\t\"\"\"\n\tif not command:\n\t\treturn\n\tprint("}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\treturn cmd\n\n#\n# Copyright 2013 Brocade Communications Systems, Inc.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tcmd_in = [cmd[0] for cmd in reversed(cmd)]\n\tif not cmd_in[0]:\n\t\treturn cmd_in\n\tfor cmd_str in cmd_in[1:]:\n\t\tif re.match(\"^shebang:\", cmd_str):\n\t\t\tcmd = cmd_in[:]\n\t\t\tcmd[0] = \"cmd.exe\"\n\t\t\tcmd_in = cmd.split()[1]\n\t\telse:\n\t\t\t# not, so this shouldn't happen\n\t\t\tcmd += \"2>&1|^\" + \"|\"."}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tcmd = list(set(cmd) - {\"prefix\"})\n\tshebang_list = []\n\tfor s in os.environ[\"PATH\"].split(os.pathsep):\n\t\tif s.startswith(bin) or (\"PYTHONPATH\".index(s)==-1):\n\t\t\tshebang_list.append(s)\n\tshebang_list.reverse()\n\tcmd += [\"\"] + shebang_list\n\treturn cmd\n\t\ndef clean(path: str):\n\tfor file in glob.glob(path):\n\t\tif (\"\".join(file[:-"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tif is_windows:\n\t\tif len(cmd) % 2!= 1:\n\t\t\traise ValueError(\n\t\t\t\t\"command must be a two element list; got:\\n{}\"\n\t\t\t\t.format(cmd)\n\t\t\t\t)\n\t\tcmd = [command[0] for command in cmd]\n\telse:\n\t\tcmd = [command.replace('\\\\', os.spsep) if os.windows else command]\n\n\treturn \"\".join(*cmd)\n\ndef quote(obj: Iterable[Any]) -> str:\n\t'''\n\tConvert value of iterable"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t# pylint: disable=line-too-long\n\n\tif isinstance(cmd, bytes):\t\t\t\t# pylint: disable=no-member\n\t\tcmd = cmd.rstrip(\"\\n\")\t\t\t\t# pylint: disable=no-member\n\n\tcmd = cmd.replace(\"\\n\",\"\")\t\t\t\t# pylint: disable=no-member\n\n\tcmd = cmd.replace(\"``\", \"```\").replace(\"''\", \"''\").replace('\\r', '\\n')\t# pylint: disable=no-member\n\tcmd = '\"%s\"' % cmd.replace(\"\\n\",\"\\\\"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\treturn normalize_bash(cmd)\n\n\ndef normalize_arg(cmd: list[tuple[str,...]]) -> list[tuple[str,...]]:\n\treturn normalize_shell(cmd)\n\n\ndef normalize_shell(cmd: tuple[str,...]) -> list[str]:\n\tif os.getenv(\"ENVDEBUG\") and os.getenv(\"SHELL\"):\n\t\traise SystemExit(\"Cannot execute shell with an error flag set. Sending this command to sudo instead will fail.\")\n\n\treturn [os.getenv(\"SHELL\", \"sh\")] + [shell_command("}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\tshebang_list = [\".zsh\", \".sh\"]\n\t# list of absolute paths to normalize\n\t# shebang lists are relative to the PYTHONPATH\n\tshebang_list = [cmd[0] for cmd in shebang_list if len(cmd.split(\".\")) > 1]\n\treturn shebang_list\n\n\ndef get_exe(filename: str, args_list: list[str]) -> str:\n\t\"\"\"\n\tReads a command to a target program and return the command as a str\n\n\t:param filename: File name\n\t:param args"}]}
{"task_id": "62b8d27a48ba5a41d1c3f4c6", "prompt": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\treturn CallableMemoizingWrapper(cache, key, lock)\n\n\ndef get(request):\n\t\"\"\"Returns the stored result, if any.\n\n\tThe cached version is never used, the returned result is cached.\n\n\t\"\"\"\n\tcache_key = request.context._cache_key(key)\n\treturn cache[cache_key]\n\n\ndef iteritems(iter):\n\t\"\"\"Returns an iterator over all items in a class.\n\n\tIterating over a mutable (i.e., mutable object) of an iterable\n\twill raise exceptions when the item is omitted."}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\tf = cached(*args, cache=cache, key=key, lock=lock)\n\t\tf._lock = weakref.WeakKeyDictionary()\n\t\tf._cache[key] = args, kwargs\n\t\tf._lock.callback = _maybe_call\n\t\treturn f\n\treturn wrapper\n\n\n@cached\ndef get(*args, **kwds):\n\t\"\"\"Find and return the value given a given cache key.\n\n\tWhen caching a function from a pool to a function (like a function returning a\n\t"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tif lock is None:\n\t\tlock = threading.RLock()\n\tmemo = functools.partial if callable(memo, memo) else memo\n\tret = memo(cache[key])\n\tif lock.acquire(key, 1):\n\t\tif ret is not None:\n\t\t\tret.acquire()\n\t\t\tret.value = cache[key]\n\t\telse:\n\t\t\tlock.release()\n\tif lock.release() is not None:\n\t\tlock.acquire()\n\t\tcache[key] = ret.value\n\treturn ret\n\n\nclass"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tdef _cached(key, result, lock=None):\n\t\t# type: (Callable[..., T], T, Optional[str], Optional[Lock]) -> Any\n\t\tm1 = cache[key] = result\n\t\tif lock:\n\t\t\tm1._locking.acquire()\n\t\treturn m1\n\n\tdef __cached_ex__(self, cache, key=hashkey, lock=None):\n\t\t_cached(cache)\n\n\n\t# Cache def for functions on Python 2 compatibility problems\n\tclass _Memoized:\n\t\tdef __init__("}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t# pylint: disable=bad-whitespace\n\tif isinstance(cache, CacheValueInterface):\n\t\ttry:\n\t\t\treturn cache._cache.cache[key]\n\t\texcept KeyError:\n\t\t\tif lock is None:\n\t\t\t\tfor name, func in cache._value_fns:\n\t\t\t\t\tif func is memo:\n\t\t\t\t\t\treturn memo(key, cache._value.cache[name][True])\n\t\t\telse:\n\t\t\t\treturn cache._value.cache[name][True]\n\telse:\n\t\tif lock is None:\n\t\t\tfor name, func"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tdef wrapped(f):\n\t\tdef wrapper(self, *args, **kw):\n\t\t\tif key is None: key = hashkey(self)\n\t\t\tsuper(cache, key=key, f=f, *args, **kw)\n\t\t\treturn f(*args, **kw)\n\t\treturn wrapper\n\treturn wrapped\n\ncached.cache_size = cached(8, __memory_cache_size__)\n\ndef _get_memory_size(cache):\n\tif cache: return cache.get('memory_size', None)\n\tif platform.python"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tmemoize_decorator(cache[key])(func, *args, **kwargs)\n\n\t@functools.wraps(func, memoizer)\n\tdef cached(*args, memoizer=memoize_decorator):\n\t\treturn func(*args, **memoizer(func(*args, **memoizer)))\n\n\treturn memoize_decorator\n\npackage org.apache.velocity.test.failing;\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tdef wrapped(*args, **kwargs):\n\t\tif lock:\n\t\t\tcache.setexithook(lock, *args, **kwargs)\n\t\treturn cache.get(key)\n\treturn wrapped\n\ndef _weakcache(cache, key=None, default=_default, lock=None):\n\t\"Decorator to create a weakly_cached cache of the given\nkey. If no key is provided, a default constructed one to use by the\nconstructor.\"\n\tif key is None:\n\t\tkey = default\n\telif key in cache._caches.keys():\n\t"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tcache_fn = cached_function(cache, key=key, lock=lock)\n\tmemoized = cache_fn(self)\n\treturn memoized\n\n\ndef __iter__(cache=None, key=None, lock=None):\n\treturn cached(cache, key=key, lock=lock)\n\n\ndef iter(cache=None, key=None):\n\treturn cached(cache, key=key, lock=None)\n\n\n_cache_lock_ = threading.Lock()\nclass LazyCache():\n\t \"\"\"Cached and invalidated functions using a hashable class\n\t"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\tif lock is None:\n\t\tlock = threading.RLock()\n\tdef decorated(func, *args, **kwargs):\n\t\tassert lock\n\t\tfunc = cache.callFunc(func, *args, **kwargs)\n\t\tif lock.locked():\n\t\t\traise ValueError, (\"Cached function %s cannot be cached.\" % func.__name__)\n\t\treturn func.__name__\n\treturn decorated\n\nclass memoizeMixin:\n\t__slots__ = ()\n\ndef memoize(func, *, memo=None):\n\t\"\"\"\n\tReturns a callable memoizing function"}]}
{"task_id": "62b8d24048ba5a41d1c3f49f", "prompt": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t# from http://www.cs.rochester.edu/java/garbagecollector.java.html\n\t# I have no idea why the cache gets very large values because we can\n\t# put a small delta in the cache to decrease the size of the queue.\n\tdef ttl_cache_wrapped(*args, **kwargs):\n\t\tfor item in args[0]:\n\t\t\tif isinstance(item, timedelta):\n\t\t\t\tfor h in item.utctime():\n\t\t\t\t\tcache[item] = lru_cache[maxsize][2]\n\t\t\telse:\n\t\t\t\t"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tcls = type(cls)\n\treturn weakref.method_call(cls, cls._ttl_cache, args=(maxsize, ttl,timer,typed))\n\n\n@_time_cache\ndef _get_lru_cache(maxsize, ttl, t,timer=_time_delta(), typed=False):\n\treturn _time_cache()\n\n\nif support.feature_is_available('lru_cache') and _time_cache is _get_lru_cache:\n\t_get_lru_cache.ttl = TTL(0, _time_delta().max,timer="}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tdef decorator(function):\n\t\t\"\"\" This decorator is just a wrapper around `function` to return\n\t\ta unique ID for an item at a time.\n\t\t\"\"\"\n\t\tif not hasattr(function, 'id'):\n\t\t\tfunction.id = hash(function)\n\t\treturn function\n\tdef wrap_wrapped_function(wrapped_function):\n\t\t\"\"\" This decorator makes any item in an instance a wrapper.\n\t\tArgs like::\n\n\t        function = wrapped_function(self)\n\n\t\tFor instance, this one just returns"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tt = threading.Thread(target=ttl_cache_thread, daemon=True)\n\tt.daemon = True\n\tt.start()\n\n\treturn t\n\n\ndef ttl_cache_thread(fn, maxSize=128):\n\tdefer_defrag = False\t\t\t\t# if True, returns the same object, different instance will be returned\n\tdeflater = None\t\t\t\t\t# deflater for the given function to merge results\n\twhile 1:\n\t\tdel _get_value_from_dict(fn)\n\t\tif _get_function_value_from_dict"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tif typed: return\n\tdef f(x):\n\t\treturn (key(x), 1) * (x / (maxsize - ttl) + 1)\n\tf.__func__ = _lru_cache_method\n\tf.bound = True\n\tf.bound_func = _get_cache_bound\n\tf.cache_ttl = ttl\n\treturn f\n\n__lru_cache_method = lru_cache_lru\n\ndef lru_cache_lru(maxsize, ttl, timer, typed):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tdef update_lru_cache_fn(fn):\t\t\t\"\"\"Updates the value of the hashmap\"\"\"\t\t# lint-amnesty, pylint: disable=unused-argument\n\t\tlru_cache_fn()\n\tdef _lru_cache_entry(key):\t\t\t\"\"\"Internal representation of the LRU cache entry for a key.\"\"\"\t\t# lint-amnesty, pylint: disable=unused-argument\n\t\tlru_index = int(lru_cache_index)\n\t\tif lru_index in lru_cache:\t\t# this is the only place that saves the value"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tdef wrapper(f):\n\t\tm = _memoizer(f)\n\t\tif m is None:\n\t\t\tcache = Cache(maxsize, ttl, timer)\n\t\t\tm = cache\n\t\tdef memo(f): return cache.get(f) or lambda f: f()\n\t\treturn memo(m)\n\treturn wrapper\n\nimport os\nimport signal\n\nimport pyatv\n\nimport numpy as np\nimport pytest\n\nfrom pyatv.pyatv_utils.constants import (\n    BORDER_IMAGE_NAME,\n  "}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\treturn (lambda: def _f(i):\n\t\t\t\treturn func(memo, ttl) if memo else f\n\t\t\t\telse if isinstance(ttl, (tuple, list))\n\t\t\t\t\tfor ttl_i in ttl:\n\t\t\t\t\t\ttry: _f(ttl_cache[ttl_i](maxsize+ttl))\n\t\t\t\t\t\texcept: pass\n\t\t\t\t\t\texcept Exception:\n\t\t\t\t\t\t\tpass  # do nothing\n\tif typed: memo = cache_typed()\n\twrapped = lambda f: functools.partial(_f, memo)\n\treturn wrapped\n\n\ndef lru_map(maxsize"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tttl_cache_fun = functools.partial(ttl, maxsize, timer)\n\n\tdef ttl_cache_func(x: Optional[int]) -> int:\n\t\tif not x:\t\t# pragma: no cover\n\t\t\treturn 0\n\t\telse:\t\t# pragma: no cover\n\t\t\treturn x + ttl_cache_fun(x, ttl)\n\n\treturn ttl_cache_fun(maxsize, ttl_cache_func)\n\nclass _SimpleMemoizer(_Memoizer):\n\t\"\"\"memoization class that reuses a cached results of calls"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\tdef memoize(item):\n\t\twrapped = lambda func: func()\n\t\twhile item in queue:\n\t\t\twrapped = wrapped()\n\t\treturn wrapped\n\treturn memoize\n\ndef memoize(func):\n\t\"\"\"\n\tDecorator factory function whose sole purpose is to save the\n\tresults of `func` up to `maxsize` results per item.\t\"\"\"\n\tmemoized_cache = lru_cache()\n\tfunc.memoize = memoize\n\treturn func\n\n<?php\n/**\n * @package     Joomla.Administrator\n *"}]}
{"task_id": "62b8d23b48ba5a41d1c3f49a", "prompt": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t@memoizer\n\tdef fn(**kwargs):\n\t\t\"\"\"\n        decorator method to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU) algorithm.\n        \"\"\"\n\t\tcached = fn.fn(**kwargs)\n\n\t\tif typed and cached.size == size:\n\t\t\treturn cached\n\n\t\treturn CallableFunction(cached, **kwargs).fn().cache(maxsize)\n\n\treturn fn\n\n###################################################################################################\n#   "}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t'''\n\n\tdef mru_cache(maxsize=128, typed=False):\n\t\t'''\n\t\t@summary:\n\t\t\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t'''\n\t\tif (typed):\n\t\t\treturn mru_cache.__mro__\n\t\treturn mru_cache.__mrub__\n\n\t#-----------------------------------------------------------------------##\n\t#\n\t#                 "}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t@functools.wraps(mru_cache)\n\tdef mru_cache_wrapped(x, y, h=None):\n\t\t\"\"\"\n\t\tReturn a value which will hold memoized results in a\n\t\t(possibly different) future-unsafe manner.\n\t\tIf `h` is None, store the current time as h.\n\t\t\"\"\"\n\t\tif y is not None and y > mru_cache_maxsize:\n\t\t\tmru_cache_maxsize = y\n\t\t\ty = None\n\t\tif h is not None and h > m"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef wrapper(func):\n\t\tif typed:\n\t\t\treturn memoize(func)\n\t\telse:\n\t\t\treturn func\n\n\treturn wrapper\n\n\n@add_metaclass(abc.ABCMeta)\nclass _GenericCacheWrapper(GenericCache):\n\tdef __init__(self, typed=False):\n\t\tself._cache = _GenericCacheWrapper(typed=typed)\n\n\t\tself._cache.maxsize = max(self._cache.maxsize, 1024)\n\n\t@abc.abstractmethod\n\tdef write(self, key, value"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef __ms_cache(key, data, typed):\n\t\t\"\"\"\n\t\tThe key is returned.  The ``data`` argument is a function\n\t\tthat takes a maximum (typically 2 ** 64).  The value is then converted\n\t\tto an instance of ``data`` with the hash of that value converted\n\t\tto the given ``typed``.  If the hash does not match, it returns a\n\t\tmissing value.  If the given value does not match, the given\n\t\tdata object is returned so that the cache of value information can be\n\t\tused without"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef memoized_mru_cache(*args, **kwargs):\n\t\t\"\"\"\n\t\tAn example that demonstrates that a function caching a function\n\t\treuses the memoized results stored in `mru_cache`.\n\t\tThe idea is that a re-use of the original function is unclear\n\t\tif the caching was recomputed, but that might change the behaviour\n\t\tof the function in the future.\n\t\t\"\"\"\n\t\tpass_list = []\n\t\tfor key, value in kwargs.items():\n\t\t\tif callable(value):\n\t\t\t\t"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\treturn _memoize(\n\t\t# the memoization algorithm is defined in mrutypes.h\n\t\t# if you want to override the type name\n\t\tlambda: _MRU(maxsize),\n\t\t# if you want to wrap an existing one, you should define the\n\t\t# parameter as a callable and wrap it with a memoization in\n\t\t# your function.  You should then specify as many\n\t\t# arguments as possible but with a \"self\" argument (or\n\t\t# None if we don't care).\n\t\toperator.methodcaller('call'),"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef wrapper(func):\n\t\tif typed:\n\t\t\tfunc = MruCoder(func)\n\t\tfor i in range(maxsize):\n\t\t\tfunc = func(i, 0)\n\t\treturn func\n\treturn wrapper\n\n#include <stdlib.h>\n\n#include \"osconfig.h\"\n#include \"debug.h\"\n\n#include \"h64.h\"\n\n#include \"cpu.h\"\n#include \"interrupts.h\"\n#include \"processor.h\"\n#include \"timer.h\"\n#include \"thread"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import time, time, os\n\tif not typed:\n\t\tfrom statsmodels.sandbox.stats import mru\n\t\treturn mru_cache\n\tfrom sys import maxsize\n\tfrom sys import mru\n\traise Exception('$maxsize cannot be greater than $mru')\n\n\tmaxsize = 1/maxsize\n\tif os.name!= 'nt':\n\t\ttry:\n\t\t\timport pickle\n\t\t\tpickle.dump('memoize', mru)\n\t\texcept:\n\t\t\tpass#IMPORTIMPORTIMPORT"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\tdef wrapper(fn):\n\t\treturn fn(maxsize=maxsize, typed=typed)\n\treturn wrapper\n\n# Copyright (C) 2018 Nathan White\n\"\"\"\nThis is a benchmark script to measure the length of a string without\nthe full memory footprint. The benchmark script consists of two parts\nas a script. In the first part it measures the time taken to perform a\nstring operation to produce an output of that operation in the\n`time-`th window over some time. In the second part it measures the\nnumber of string instructions. \n\"\"\"\n\n# We define our benchmark"}]}
{"task_id": "62b8d23948ba5a41d1c3f498", "prompt": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tif typed:\n\t\treturn _lru_cache_typed\n\telse:\n\t\treturn _lru_cache\n\ndef _get_lru_cache(maxsize=128, typed=False):\t\t\"\"\"\n\tReturns an object whose memoized version is the function\n\treturned by the cache.  An instance is cached internally and no\n\tfunction is cached internally.\n\n\tTypically, a caching function is used to cache large data from\n\tan LRU hash, but in the event the cache is full, the cache\n\tis emptied.\n\n\tTypically, the cache"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tif typed:\n\t\tdef makecache(f):\n\t\t\treturn (f if f is not None else cache(maxsize=maxsize))\t\n\telse:\n\t\tdef makecache(f):\n\t\t\treturn memoize(f)\n\t@cache.memoize(maxsize - 1)\n\tdef cache(size):\n\t\tcache.append(makecache(lru(size, return_attrs=True)))\n\t\treturn cache[0]\n\n\n\n@cache.memoize(maxsize=6)\ndef googlings(f):\n\n\tdef _print"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tdef _cache(key, *args, **kwargs):\n\t\t\"\"\"Function to call\"\"\"\n\t\tif typed:\n\t\t\treturn super()._call_stored(key, *args, **kwargs)\n\t\telse:\n\t\t\treturn pickle.loads(item_from_callable(key, *args, **kwargs))\n\n\treturn _cache\n\n\n# ----------------------------------------------------------------------------\n# Removing cache\n# ----------------------------------------------------------------------------\n\nclass Cleaner(object):\n\tdef __init__(self, *args, **kwargs):\n\t\t\"\"\"Clean up a function\n\n\t\tThis class is"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tdef _memo(fn):\n\t    # This is a convenience function.\tThe function is passed an index,\n\t\t# then the function returns a tuple (item, value).\n\n\t\tif fn == 0:\t# An empty list is returned.\n\n\t\t\treturn (), ()\n\n\t\t# We cache the list to speed up repeated calls to the same function\n\t\t# without allocating new memory.  This ensures that any values inserted\n\t\t# in between where the LRU algorithm is run will have a chance to\n\t\t# return the same value first.  (On"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tdef lru_cache_functor(iterable_list):\n\t\t\"\"\"\n\t\tDecorator to pass `iterable_list` an iterable so that we don't\n\t\trepeat it.\n\t\t\"\"\"\n\t\t# This makes folks more aggressive in caching the actual function.\n\t\t# By using the `iterable_list` it avoids using\n\t\t# `cache_iterable_list(*iterable_list)` which is slow,\n\t\t# and the performance is probably more than a concern.\n\t\tcache_iterable_list = lambda l: list(map(l, range("}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tfunction = wrapfunc_arg(function, maxsize, typed)\n\treturn function\n\ndef hash_cache(maxsize=512, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize`` results based on a Hash-algorithm (HASH)\nalgorithm.\t\"\"\"\n\tfunction = wrapfunc_arg(function, maxsize, typed)\n\treturn function\n\t\ndef iter_cache(maxsize=64, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable.\tThis has\na good speed"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tdef __new__(cls, *args, **kwargs):\n\t\tif isinstance(cls, lru_cache):\n\t\t\treturn cls\n\t\tif typed:\n\t\t\tmemo = type(cls).__new__\n\t\t\tmemo.maxsize = maxsize\n\t\t\treturn memo\n\t\telif not (isinstance(cls, types.FunctionType) or\n\t\t\t\t\tisinstance(cls, types.MethodType)):\n\t\t\t__old_notimplemented = lambda: None\n\t\t\treturn sys_cache_decorator(cls, *args, **kwargs)\n\t\telse:\n\t\t\t\t# We do not"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tif typed is True:\n\t\tmaxsize = 1000000\t# use a default, a reasonable value here\n\telse:\n\t\tmaxsize = maxsize\n\n\tif maxsize < 4:\n\t\tmaxsize = 4\n\t\n\tmemo = memoize(maxsize)\n\tremovemissing = memo(maxsize)\n\tfor n in range(maxsize):\n\t\tmemo[n] = None\n\t\n\treturn memo, removemissing\n\nnamespace Grand.Framework.Components\n{\n    /// <summary>\n    /// Component used to format the file's source"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tif typed:\n\t\tif not isinstance(maxsize, numbers.Integral):\n\t\t\tmaxsize = max(maxsize, int(maxsize))\n\t\tlru_cache = memoize('lru_cache',maxsize=maxsize)[0]\n\telse:\n\t\tlru_cache = memoize(maxsize=maxsize)[0]\n\treturn lru_cache\n\n#---------------------------------------------------------------------------\n#\n# Caching for L{IPowerShell}\n#\n#---------------------------------------------------------------------------\n\ndef ipower_cache(maxsize=16):\n\t\"\"\"\n\tReturn a `IPowerShell` cache as a file or"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\tdef lru_cache_def(maxsize=128, typed=False):\t\n#\tdef lru_cache_def(maxsize=128, typed=False):\t\nwhile 1:\n\t\tdel c, f\n\t\t\nc, f = [], []\nfor i in range(maxsize):\n\tf[(i,), c.append(i)]\n\n#f = map(int,(1,1,2),maxsize-8)\t\t \n#f = sorted(f,key=cmp)\n#\n#f.sort(cmp)\nc, f = [], []\n#for"}]}
{"task_id": "62b8d23748ba5a41d1c3f496", "prompt": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tdef wrapper(func, cache=True, typed=False):\n\t\tif typed: return func\n\t\tif not cache: return func\n\t\tif cache: # this is a safeguard\n\t\t\twrapped = func\n\t\ttry:\n\t\t\treturn _cache_with_cache(maxsize, wrapped, typed)\n\t\texcept:\n\t\t\twrapped = wrapped_for_memoization(func, typed)\n\t\t\treturn wrapped\n\treturn wrapper\n\n\nregister(lfu_cache)\n\n\ndef _cache_with_cache(maxsize, func, typed=False"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tdef lfu_cache_helper(self, pfunc):\n\t\tif pfunc is None:\n\t\t\treturn lambda x: x\n\t\telse if isinstance(pfunc, basestring):\n\t\t\t# convert this to bytes to avoid a string length issue when\n\t\t\t# it's really bytes (see http://bugs.python.org/issue18623\n\t\t\tpfunc = os.fspath(os.path.join(os.getcwd(),\"..\",\"..\",\"..\",\"python\"))\n\t\treturn pfunc\n\tdef t_cached(self, key):\n\t\t"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tdef decorator(func):\n\t\tif not callable(func):\n\t\t\traise TypeError('expected function or callable')\n\t\tif typed:\n\t\t\treturn func\n\t\treturn memoizingmeth(lambda: func)(size=maxsize)\n\treturn decorator\n\n/*\n * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n *\n * This code is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License version 2 only, as\n * published by the Free Software Foundation.\n *\n * This code is distributed in the hope"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tdef _cache_wrapper(func):\n\t\tif typed:\t\n\t\t\t f = __builtin__.funcwrap(func)\n\t\t  return weakref.WeakValueDictionary(f(*[lfu_cache(maxsize) for _ in range(maxsize)]))\t\n\t\telif isinstance(func, types.FunctionType):\n\t\t\treturn _cache_wrapper(func)\n\t\t\t#l = LFU((func, ) + tuple(set(l.items()) for l in func.__code__.co_varnames))\n\t\t\telif func.funcresult is not None:\n\t\t\t\tf = __"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tfrom functools import update_wrapper\n\tfrom itertools import tee\n\tfrom functools import wraps\n\n\tdef wrap_function_with_memo(current_func, lfu_cache):\n\t\tdef wrapped():\n\t\t\tfor current, fun, memo in current_func():\n\t\t\t\tif fun is lfu_cache and memo is not None:\n\t\t\t\t\treturn decorated(current_func, current, memo, lfu_cache)\n\t\t\t\telse:\n\t\t\t\t\tif callable(current):\n\t\t\t\t\t\treturn current, fun, memo\n\t\t\treturn wrapped(), (None,"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tdef _lru(m, x):\t\t\tif typed:\t\tpass\n\n\t\t# check for overflow\n\t\tif lfu_check((m, x)) == 1: \tbreak\n\n\t\t# return a callable\n\t\tx = safe_call(m, (x, safe_argmax(lfu_cache(maxsize=m,\t# use the next m's argmax\n\t\t)), m))\n\t\tyield x\n\n\treturn _lru\n\ndef safe_call(fn, args, kwargs):\n\tif len(args)!="}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t# The wrapper function can optionally take an int parameter which is\n\t# used as a parameter to indicate the \"kind\" of the memoization function.\n\tdef decorator(f):\t\"\"\"\n\t\tReturn an instance of a given callable type with this wrapper as\n\t\targument and memoization function. Note that only certain features\n\t\tof the LFU functions this decorator will be run over and that only\n\t\tthe cached version of this function can be called.\t\"\"\"\n\n\t\t\"\"\"\tHelper functions that don't run over will simply pass the original\n\t\tvalue of this function to the"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\twith vaex.VAXModule(('', *args, *kwargs))) as mod:\n\t\treturn vaex.LfuCache(**mod.vaex_args, **mod.vaex_kwargs)\n\n/*!\n * @file\n *\n * @section LICENSE\n *\n * Copyright (C) 2017 Alpha Band Wireless Ltd.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n * 1. Redistributions of source code must retain the above copyright\n *    notice, this list of"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t#\tdef lfu_cache(maxsize, typed=False):\n\t#\t\tmaxvalue = 1\n\t#\t\t#\t\tif typed:\n\t#\t\t\tmaxvalue = int(math.log(maxvalue, 2))\n\t#\t\t#\t\t# for n in range(maxvalue, 1, maxsize/4):\n\t#\t\t\tdef _lub_cache():\n\t#\t\t\t\ttemp = 1\n\t#\t\t\t\twhile len(temp) > 0 and len(temp) % 4 == 0:\n\t#\t\t\t\t\ttemp += 1\n"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\tdef __new__(cls, func):\n\t\tname = \"LFU\"\n\t\tif typed: \tname = \"LFU_TYPED\"\n\t\tcache = lfu_cache(maxsize=maxsize, typed=typed)\n\t\treturn _new_wrapped_function(cls, name=name, func=func, cache=cache)\n\treturn __new__\n\n\ndef _new_wrapped_function(cls, name=lambda func, cache=None: func, cache=None):\n\t\"\"\"\n\tWrap a function in a fresh wrapper, and optionally a cache dictionary"}]}
{"task_id": "62b8d22f48ba5a41d1c3f488", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\ttemp = self._insert_before(self.list_index + 1)\n\tself.list_index -= 1\n\treturn (temp, str(self.list_index + 1))\n\n\t\n################################################################################\n###\tThe following is a reference list object.\n################################################################################\n\nclass Dict(obj):\n\t\"\"\"A simple Dictionary object.\n\t\n\tThe structure of the dictionary is as follows. First element is the dictionary \n\tcode for the items and is stored in the dictionary of the original.\n\t\n\tIt's usually the way to instantiate it and the method to\n\tadd it"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\ttry:\n\t\treturn self[len(self.keys)-1])[0]\n\texcept IndexError:\n\t\treturn None\n\n\tfor item in self.iterkeys():\n\t\tif item is None or item == self.keys[-1]:\n\t\t\tself.popitem():\n\t\telif item!= self[len(self.keys)-1][:]:\n\t\t\traise Exception(\"popkey failure\")\n\n<?php\n/* vim: set expandtab sw=4 ts=4 sts=4: */\n/**\n * functions used by the core functions\n *\n * @package"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\tself.list.popitem(tuple(self.keys),tuple(self.values))\n\n\tnew_values = [item for item in self.list]\n\treturn (item, new_values)\n\n\t\nclass Tree(dict):\n\tdef __init__(self, keys=[(\"a\",1), (\"b\",2), (\"c\",3)],values=[(12,13), (13,23), (24,47)],desc=\"Key desc, value.\",sortby=None,ignorecase=False):\n\t\tfor k, values in self.items().items():\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\tvalue = PopValue (self.parent, \"string_pop_first\")\n\tpopped = Popped.object_pop\n\tfor key, value in self.children.items():\n\t\tparent = popped.pop(popped.pop(key, \"\")(self.parent))\n\t\tif len(parent) > 0:\n\t\t\tparent.popitem(key, value)\n\tpopped = Popped.object_pop\n\treturn popped.get(key)\n\n# }}}\n\n\nPList = (\n    \"\"\"\\\n{{\n "}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\tfor i in range(len(self)):\n\t\tk, v = self[i]\n\t\tif k in self and v == value_key(k):\n\t\tself.pop(i)\n\treturn self.data[i]\n\ndef sorted_range(start, stop=None, head=None, tail=None) -> range:\n\t\"\"\"Sorts a list of `(key, value)` pairs, and returns a range of `(key, value,\n\tlast)` pairs.\n\t\"\"\"\n\tif start is None and stop is None and head is None"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\tpopped = self.popitem((self.key,))\n\tpopped = [self.key], self.value\n\treturn popped\n\n\tdef popitemstring(self):\n\t\t\"\"\"Return the key as ``(value,key)`` if exists.  Else return ``(value,value)``.\"\"\"\n\t\treturn [key for key, _value in self.items() if key[0] is '=' or key[0] in string.bytechars]\n\n\tdef popitem_int(self):\n\t\treturn self.popitem(int(self.key))\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\tif not self.key:\n\t\tself.key, self.value = (None, None)\n\t\treturn\n\tkey, value = self.key, self.value\n\tdel self.key, self.value\n\tself.key, self.value = key, value\n\n\tif key not in self.dict:\n\t\tsuper(OrderedDict, self).__setitem__(key, value)\n\nclass OrderedSparseDict(OrderedDict):\n\tdef __init__(self, *funcs, **kwds):\n\t\tordereddict.__init__(self, *funcs,"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t# key is the hash of self; value is the item to remove\n\th = self.dict[self.hash(self.hash(self.data()))]\n\tself.data(), self.hash(self.data()) = (self.data(), None)\n\treturn [item.key, item] if item.key is None else item.key\n\n\nclass KeyTuple:\n\t\"\"\"A tuple with a `(key, value)` pair and possible `(key, value)` pairs for items\n\tin the table.\"\"\"\n\n\tdef __new__(cls, key, value):"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\tif self.index == (1,):\n\t\treturn self.list.pop(-1)\n\telse:\n\t\treturn self.list.pop(self.index)\n\t\n\tdef prepend(self, value):\n\t\treturn (self.key, self.list[:] + value)\n\n\tdef append(self, value):\n\t\treturn (self.key, self.list.append(value))\n\n\n\nclass CobraCommand(Command):\n\tdef __init__(self, args):\n\t\tassert isinstance(args, (tuple, list)), \""}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\twhile True:\n\t\tself._removeItem(key=lambda k: k[:-len(key)], value=lambda v:\n\t\t\tv[:-len(key)], key=lambda k: k[:-len(key)]):\n\t\tdel self.items[key]\n\t\tdel self.keys[key + (len(key), )]\n\n\ndef keys(self, x=None):\n\t\"\"\"\n\tGet the keys of this object.\n\n\t:param tuple x: The keys you want to return. If not given, this returns\n\t\tthe keys of"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47e", "prompt": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t", "suggestions": [{"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tif self._readonly or key in self._cached[TYPE:]:\t\treturn\n\tfor attr in [len, type, subclasses]:\n\t\tsetattr(self, key, getattr(self, attr+\"default\"))\n\tif default is not SUPPRESS:\n\t\tself._cached[TYPE][key] = default\n\n/*\n  Fontname: -Adobe-Courier-Medium-R-Normal--34-240-100-100-M-70-ISO10646-1\n  Copyright: Copyright (c) 1984, 1987 Adobe Systems Incorporated. All Rights Reserved"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tif key not in self.D:\n\t\tself.D[key] = default\n\ndef getdefault(self, key):\t\"\"\"S.getdefault(\"k\", self.C[key]) -> c, default value\"\"\"\n\tx = self.C[key]\n\tif key in self.D:\n\t\tx = self.D[key]\n\tif key in self.C:\n\t\tx = self.C[key]\n\treturn x\ndef unsetdefault(self, key, default=None):\t\"\"\"D.default_hook(-k,[d"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\treturn type(self)(self.default, key, d=d)\n\nif __name__ == \"__main__\":\n\twhile True:\n\t\tprint(\"hello world\\t\", type(key(0))+type(key(1))+type(key(2)))\n\t\tprint(\"hello bird\\t\", type(default(key(0)+default(key(1)+default(key(2)))))+d)\n\t\tD.default(key(1)-key(0))*2\t# in birds\n\t\tD.default(key(2)-key(1"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tif key in self:\t\t\t\t\t\t# Key already in D\n\t\treturn self[key]\n\tret = self[key] = default\n\tfor i in list(enumerate(self))[:-1] # remove empty ones\n\tself.resize(len(ret))\nreturn ret\n\n\ndef make_dataset(n_items, name, val=None, dtype=None, order=None, dropna=None,\n                 index=None, subsample=None, copy=True, **kwargs):\n  "}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tif key not in D and key not in self:\n\t\tif not hasattr(self,'__dict__') or not self.__dict__.has_key(key):\n\t\t\ttry:\n\t\t\t\tC = self\n\t\t\t\tD = C.__dict__\n\t\t\texcept AttributeError:\n\t\t\t\tD = ''.join(map(itemgetter(0, 0), self.__dict__.values()))\t# for safety.  D may not be'self'.\n\t\t\telse:\n\t\t\t\tD = self.__dict__.get(key,None)\n\t\t\tD[key] = default"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tif key in self:\t\t\treturn\n\tif key in self.keys() and 'default' not in self:\n\t\tif default is None: default = self.get(key)\t\tif hasattr(d, key) else d.get(key)\n\tk, v = key.partition(':')\n\tif k in self:\t\t\treturn self.get(k, v)\n\td = default\n\tself[k] = d\n\n\n# ------------------------------------------------------------ #\n# Class         (X.__getattr__, dict.__getitem__)   "}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\treturn key\n\n\t# set default/deget value\n\tif not default:\n\t\treturn self.getdefault(self.default_for(self.key, key), self.default_for(self.key, self.default_for(key)))\n\n\t# use default if defined, otherwise set default\n\tif isinstance(self.default_for(key), type(default)):\n\t\treturn self.default_for(key)\n\n\traise UserWarning('not enough arguments')\n\ndef getkey(self, key, default=None):\t\"\"\"\n\tD"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tkey = self._coerceType(key)\n\tif default:\n\t\tD[key]()\n\t\treturn D[key]\n\tD[key] = default\n\treturn D[key]\n\n\ndef setdefault_iter(self, key, default=None):\t\"\"\"\n\tself._doAddValue(key,)\n\tD[key] = default\n\tC,M,F = self._indexBy(key)\n\treturn [C,M,F]\n\n/*\n * Copyright \u00a9 2015 Apple Inc. and the ServiceTalk project authors\n *\n * Licensed"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tif key in self:\n\t\treturn self[key]\n\tif default is None:\n\t\tfor i in range(len(self)):\n\t\t\tif i not in self:\n\t\t\t\tself[\"[\"+format(i)+\"]\"] = self.get(i)\n\ndef allitems(self):\n\t'Return all items in self (and if there exists any duplicates)'\n\titerables = {}\n\tl = self.iterkeys()\n\twhile l:\n\t\tif l in self:\n\t\t\titerables[l] =  1\n\t\telse"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\tif isinstance(default, bool):\n\t\t# Special case for the empty set\n\t\tif key!= \"\":\n\t\t\tself[\"[{}]\".format(key)] = default\n\t\telse:\n\t\t\tself[key] = default\n\treturn KeyedHash(self)\n\n\nclass Hash:\n\n\t@classmethod\n\tdef from_bytes(cls, s):\n\t\treturn Hash(BytesIO.read(s, encoding=\"utf-8\"))\n\n\t@classmethod\n\tdef from_file(cls, s):\n\t\treturn Hash(BytesIO"}]}
{"task_id": "62b8d22948ba5a41d1c3f47c", "prompt": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\tif len(self.keys) <= 0:\treturn None\t\t\t# do nothing if empty\n\tp = []\n\tfor v in self.keys[0]:\n\t\tif isinstance(self.d[v], d):\t\td[v] = p.pop() \t\t# if v exists, d will return its D[]\n\t\telse: \t\t\t\t\tp.append(self.d[v])\t\t# if v not in D, d and/or D[v] will return a different D[]\n\treturn [v for kv in p if kv!= d and"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\tif not hasattr(self, \"items\"):\n\t\t# no keys defined - return default\n\t\ttry:\n\t\t\tself.create(default)\n\t\texcept Exception as e:\n\t\t\tself.create(D.__dict__[\"default\"])\n\t\telse:\n\t\t\treturn D[key]\n\n\tif \"default\" in self.items:\n\t\treturn D[key]\n\n\tif key not in self.values:\n\t\t# key not in D - find it\n\t\ttry:\n\t\t\tfor d in self.dicts[key]:"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\treturn self.__dict__['get'][key] = self[key]\t\t\t\t\t\t\t# <1>\n\n\t# Return list of keys. This lets the list of keys from the dictionary,\n\t# that is, if it is a list, has at least one element which is not a str.\n\n\tdef keys(self):\t\"\"\" Return the list of keys. This lets the list of keys from the dictionary,\n\t\t\tthat is, if it is a list, has at least one element which is not a str.\"\"\"\n\t\treturn self.__dict__['keys']\t\t\t\t\t\t# <2"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\td = self.D.get(key)\n\tif d is not None:\n\t\treturn d\n\tfor p in list(self.pitches.keys()):\n\t\tfor k in list(self.pitches.get(p, [])):\n\t\t\tif p is key and k in default:\n\t\t\t\tcontinue\n\t\t\tif not pis_type(p, set): continue\n\t\t\td = self.pitches[p].get(k, default)\n\t\t\tif not isinstance(d, (str,) + types.Iterable)):\n\t\t\t\tcontinue\n\t\t"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\tD.D = default\n\tD.has[key] = true\n\treturn D.D\n\n\ndef set(self, key, value):\t\"\"\"\tkey is name of the new value\"\"\"\n\tif key in self.D and self.D[key][\"type\"]==list:\n\t\tfor index in range( len(value) ):\n\t\t\tself.D[key].append(index)\n\telse:\n\t\tD = get(self,key)\n\t\tfor i in xrange( len(D) ):\n\t\t\tif D[i][\"name"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\tif default is None and not callable(self._defaults):\n\t\tpass\n\tc = self._defaults.get(key, default)\n\tif not c:\n\t\tfor k, v in self.items():\n\t\t\tif v is d:\n\t\t\t\tc = k\n\treturn c\n\ndef _defaultDict(item):\n\tif hasattr(item, '__get__'):\n\t\tc = item.__get__()[0]\n\t\tif not isinstance(c, type(item)):\n\t\t\treturn c\n\treturn item\n\ndef checkDict(d):"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\tp = self.D\n\tif hasattr(p, key):\n\t\treturn p[key]\n\telse:\n\t\treturn default\ndef getKeys(self):\n\t\"\"\"\n\tD.keys() returns D.keys().keys(), unless it is in D, in which case returns\n\tit.keys(D) if self is in D, else a new D with a copy  of self.keys().\t\"\n\tD.keys() == D.keys('key').copy()\"\"\n\td = self.D; if d in D, d[D.keys()"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\tret = self._dict._toarray()\n\tret[key] = default\n\treturn ret\n\n\t\nclass DDict(dict):\n\n\tdef __init_dict__(self, *args, **kwargs):\n\t\tD.check_valid_kwargs(*args, **kwargs)\n\t\tD.check_params(arguments=self, *args, **kwargs)\n\t\t\n\t\tsuper(DDict, self).__init_dict__(*args, **kwargs)\n\t\tself[key] = D(kwargs[key])\n\n\tdef get(self, key,"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\treturn self[key]\n\ndef keys(iterable):\n\tif not isinstance(iterable, (list, tuple)):\n\t\treturn [item for _ in iterable for item in get([,])]\n\treturn filter(lambda x,_: not (x == None),iterable)\n\ndef keys2(iterable):\n\t'''helper function to avoid code duplication''\n\tif not isinstance(iterable, (list, tuple)):\n\t\treturn [item for _ in iterable for item in get([,])]\n\treturn filter(lambda x,_: x!= None,iterable)\ndef keys2n"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\tif isinstance(key, (str, unicode)):\n\t\treturn self.get(key.strip('`'))\n\treturn default\n\n\t\nD:\n\tdicts contains the following keys :\n\tD: {a: [x] for x in xrange(0,N)}\n\t\tthe key to be looked up.\n\tx: the object containing the value to be looked up\n\tx.next == D | {key,value}\n\tx.next = D | {key,value}\n\tx.prev == D\n\n#  ___________________________________________________________________________\n"}]}
{"task_id": "62b8d22548ba5a41d1c3f472", "prompt": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tdef wrapped(meth):\t\"\"\"\n\t\tdecorator is called with the same signature with argument\n\t\tlist of keys that a method may have as arguments, a cache\n\t\tkey containing the results, a default value for the cache,\n\t\tand a lock.  If it is a class with a cache and lock specified, the\n\t\tmethod with the specified arguments is called, returning an\n\t\tinstance of this class with a \"result\" attribute of the class's\n\t\tclass name that the method will have.  If the class is an instance of\n\t\tthis class, all attributes"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tfrom fnmatch import match\n\tdef memoized_filter(*args, **kwargs):  # type: ignore[attr-defined, override]\n\t\tpass\n\tif not lock:\n\t\t# cache result if not locked\n\t\tif callable(cache):\n\t\t\treturn cache\n\t\telse:\n\t\t\tlock = lockmanager.Lock()\n\t\t\tlock.acquire(block=False, blocking=False)\n\t\t\ttry:\n\t\t\t\tmemoized_filter(cache, key=key, lock=lock, block=block)\n\t\t\t\treturn cache\n\t\t\tfinally"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tif lock is None:\n\t\tlock = object()\n\t\t\n\tdef wrapper(*args, **kwargs):\n\t\t\"\"\" The function wraps a class or instance method\n\t\twith the specified arguments.\n\t\t\"\"\"\n\t\t# Get the cache instance\n\t\tcache = getcache(lock)\n\t\t# Return the appropriate class instance\n\t\tif isinstance(cache, type):\n\t\t\tcache = cache.__get__(key)\n\t\telse:\t\n\t\t\tcache = cache.get(key)\n\t\t# Return the result\n\t\tif not cache:\n\t\t\t"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t#\n\t# create cache key and the function to call\n\t#\n\t# Note that this is a \"hack\" of the original Python module, but is\n\t# intended to work around a circular dependency problem\n\tcache_key = create_cache_key(key, hashkey, cache)\n\tdef cache_func(self, *argdefaults):\n\t\ttry:\n\t\t\tret = cache.get(self, cache_key)\n\t\t\treturn ret\n\t\t\t# note that we are not caching on weak ref, so we must\n\t\t\t# be doing a ref cycle"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tdef decorated_func(*args, **kwargs):\n\t\t\"\"\"Wrapper that invokes the specified function once.\n\t\t@param func: Function used to retrieve the stored value.\n\t\t@type func: function\n\t\t@param key: Function to key the value so that it can be retrieved\n\t\tby a cache.\n\t\t@type key: function\n\t\t@param lock: Whether to lock this function.\n\t\t@type lock: function\n\t\t@return: The retrieved value, or None if it shouldn't be cached.\n\t\t\"\"\"\n\n\t\tif lock"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\timport __weakref__\n\tif isinstance(key, type.__weakref__)  is not _notweak:\n\t\tkey = WeakMethod(key, cache, key.__self__)\n\treturn functools.partial(key, cache=cache)\n\n\nclass Cache(object):\n\t\"\"\"Simple way to cache a property on a class (like a module) object.\n\tIt is intended to be a convenient way of working with properties on\n\tsome objects - a cache to speed access to their properties.\n\n\tThis does *not* persist the results of methods to __dict__ or"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tif hasattr(cache,\"memo\"):\n\t\timport gc\n\t\tif lock is None:\n\t\t\tlock = None\n\t\ttry:\n\t\t\tcmethod=cache.memoized[key]\n\t\texcept KeyError:\n\t\t\tcachedmethod=cache.memo[key]\n\t\tf=cachedmethod\n\telse:\n\t\tcmethod=Callable.__new__(Cache_type, key=key, lock=lock=lock)\n\t\tf=cmethod\n\treturn f,cache.memoized[key]=f\n\n\ndef get_cache_data() -> Mapping["}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tif lock is None: lock = MurmurmurHash()\n\tmemo = _memoize(memokey(key))\n\tcache_getter = _memoize_method(\n\t\tcache_getter=_get_by_instance_method,\n\t\tkey=hashkey,\n\t\tmemo=memo,\n\t)\n\tnew_method = cache_getter(cache_constructor=method_decorator(cache_getter))\n\tif not isinstance(new_method, cache_decorator): return new_method\n\t# For methods that don't need a memoization, we can run\n"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tresult = getattr(cache, key)\n\tif result and lock:\n\t\tlock.acquire()\n\t\ttry:\n\t\t\tresult = result.__func__(lock=lock)\n\t\tfinally:\n\t\t\tlock.release()\n\treturn result\n\ndef cache(value):\n\t\"\"\"\n\tCall a cached method on the given value.\n\t\"\"\"\n\t# cache\n\tcache = getcache(value)\n\tcache.cache()\n\treturn value\n\t\ndef get_cache():\n\t\"\"\"\n\tCreate a new cache.\t\"\"\"\n\tcache = get"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\tdef decorator(f):\n\t\tif (cache is None) and (lock is None):\n\t\t\tcache = {}\n\t\tmemoized = cache[key] = f\n\t\tdef memoize(self):\n\t\t\t# We don't try to do anything about this, but return a cached\n\t\t\t# copy.  This avoids the need to copy the memoization for\n\t\t\t# subclasses to work.\n\t\t\twrapped = memoized\n\t\t\t# copy the wrapper in cache.\n\t\t\tmemoized = cachedmethod(cache, key=hashkey, lock=mutex)\n\t\t\t"}]}
{"task_id": "62b8c517e0d34b282c18122e", "prompt": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\tm = __func__\n\tif cls:\n\t\treturn cls.__name__ + \"(\" + str(e) + \"):\" + str(m.__name__)\n\telse:\n\t\treturn m(e)\n\n\ndef path_exception(cls, e, path_level, *args, max_path_level):\t\"\"\"\n\tFormat an excon.\n:param e: Any exception instance.\n:param path_level: The path level. If this is passed the entire file is\n:                 "}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\tif not e or not any(isinstance(e, extexstr)\n\t\t\tfor e in e.__dict__): return \"extension exception instanse\"\n\tex_value_list = [ext(e.__dict__[e.ext_key]) for e in e.__dict__]\n\tex_value_list = \", \".join(ex_value_list)\n\tex_value_list = \"{} [{}] [{}] [{}] [{}] [{}] ({})\".format(e.ext_key,\n\t\te.__dict__[e.ext_key], max_level or"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\tprint(\"ERROR: extostr() called for %x, %s\\n\\n\" % (e, e.args))\n\tretstr = e.args\n\tretdict = {}\n\tfor err in retlist:\n\t\tif len(err) > 0:\n\t\t\tretdict[err[0]] = ''\n\tretdict['__file__'] = __file__\n\tcls.print_exception_args(retdict)  #print exc as string\n\treturn retstr\n\n\n#---------------------------------------------\n# Extension to C\n#---------------------------------------------\ndef to"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\tfor i in xrange(max_level, max_path_level):\n\t\tif isinstance(e, Exception):\n\t\t\treturn e.__class__\n\t\telif i > max_level:\n\t\t\treturn format_exc(e).__class__.__name__ + \".\" + repr(i)\n\t\treturn str(e)\n\n// RUN: %clang_cc1 -verify -fopenmp -x c++ -triple x86_64-apple-darwin10 -emit-llvm %s -o - | FileCheck %s\n// RUN: %clang_cc1 -fopenmp -x"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\traise AssertionError('Extensible!')\n\ndef fileno(f):\n\t\"\"\"\n\tOpener for file objects and close it.\n:param f: File object\n\"\"\"\n\traise AssertionError('Close')\n\ndef fopen(f, mode):\n\t\"\"\"\n\tOpen file in binary mode and use it to create File or a python file object.\n\t\tmode: The mode to open the file.\n\t\tfile is the name of an existing file object or a Python file object.\n\t\tpath is the location in file system filesystem where the file\n\tit should be created.\n"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\trets=[ ]\n\tif e == None: rets[0]='No Error'\n\tif isinstance(e.args, IOError): rets[1]=e.args.errno; rets[2]=str(e.args.strerror)\t\t# Error strings must have their own format\n\tif isinstance(e.args, int): rets[3]=str(e.args); try: rets[4]=e.args; return''.join(rets)\n\tif isinstance(e.args, Exception): rets[2]=e.args._err"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\tif hasattr(e, '__call__'):\n\t\tif max_level == 0:\n\t\t\treturn e.__name__+'\\n'+str(e.__call__)\n\t\telse:\n\t\t\treturn e.__call__.__name__+'.'+str(e.__call__)\n\telse:\n\t\treturn e.__name__\n\ndef set_default_logger():\n\tglobal logger\n\tglobal init_logger\n\n\tinit_logger = LogHandler('loghandler_logger',\n\t\tlog_levels=(0,       'WARNING', 'ERROR."}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\tif max_level == 30:\n\t\texc_t = t.get_exc_type()\n\t\tassert exc_t is not None, 'Invalid call stack level: %r' % (e.traceback, )\n\telse:\n\t\texc_t = t.get_exc_type()\n\t\tassert exc_t is not None, 'Invalid call stack level: %r' % (e.traceback, )\n\n\treturn e.func_name.format(max_level=max_level, max_path_level=max_path_level"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\tpath = e.path\n\tif not path:\n\t\treturn str()\n\telse:\n\t\tif 0 < len(e.message):\n\t\t\treturn str(e.message)\n\t\telse:\n\t\t\tprint \"You must use absolute paths when including ext.o's:\"\n\t\t\tprint len(path)\n\t\t\tprint path[:2]\t\t\t\t\t\t# Get the right message from the path when that is found\n\t\t\ttry:\n\t\t\t\tfor f in open(path):\n\t\t\t\t\tprint '\\t' + f.read()\n\t\t\texcept Exception and"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\tlevel_string = str(e.level)\n\tmessage = f\"Message {e.message}\\n\"\n\tif max_level <= level_string.find(\":\"):\n\t\tlevel_string = level_string[:level_string.find(\":\")]\n\tmax_string = max_string or max_level\n\tfile_path = str(e.file_path, e.exception) or \"\"\n\n\tif level_string in str(errors).errors:\n\t\tmessage = message + \"\\n\" + str(errors).errors[level_string]\n\n\t"}]}
{"task_id": "62b8bbbfe0d34b282c181210", "prompt": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t", "suggestions": [{"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tif(self.file_exists(file_name)):\tfile.close()\t# Open the file\n\tif(self.overwrite):\tfile.writeBytes([self.temp_file, text_buffer], encoding)\t# Write the file\n\n\ttmp_file_name = file_name + \"_\" + datetime.datetime.utcnow().strftime(\"%Y_%m_%d\")\n\ttmp_file_name += \".tmp\"\n\tfor i in range(8):\n\t\tos.makedirs(tempdir)\n\t\tfile.open(file_name + \"_\" +"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tif not overwrite:\n\t\t# If overwrite = True, we only want the file to be created if something\n\t\t# else has been written so overwrite must be set\n\t\tfp = open(file_name, \"wb\")\n\t\tfp.write(text_buffer)\n\t\tfp.close()\n\n\treturn len(text_buffer)\n\n\ndef is_text(text):\n\t\"\"\"Return true if ``text`` is a valid file format text.\n\tThe value returned will be compared with the current filename.\n\tThe extension is ignored if the file type is"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tif overwrite is True:\n\t\tdel text_buffer\n\t\treturn 0\n\tout_write_result = self.create_file(file_name, text_buffer, encoding, overwrite)\n\treturn out_write_result['wrote_bytes']\n\n/****************************************************************************\n**\n** Copyright (C) 2019 The Qt Company Ltd.\n** Contact: https://www.qt.io/licensing/\n**\n** This file is part of Qt Creator.\n**\n** Commercial License Usage\n** Licensees holding valid commercial Qt licenses may use this file in\n** accordance with the commercial"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t# print()\n\tif overwrite:\n\t\tdel_file( file_name, encoding)\n\t# start a new thread to handle writing\n\tfile = Thread(target=append_text_to_file)\n\tfile.daemon = True\n\tfile.start()\n\t# write the file at the default location\n\tf.write(text_buffer)\n\t# we write the file\n\tf.write(file_name)\n\t# we close the file\n\tf.close()\n\treturn file.detach().detach().istty()\n\n\ndef move_"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tif len(text_buffer) < 1 :\t\t# Check file name\n\t\traise RuntimeError(\"\"\"Cannot create file:\n\tCannot write to the specified filename!\"\"\")\n\tf = open(file_name, 'wb')\n\tf.write(text_buffer)\n\tf.close()\n\tif overwrite:\n\t\tf = open(file_name, 'w')\n\t\tf.write(text_buffer)\n\t\tf.close()\n\treturn len(text_buffer), 0\n\n\n###\n###        "}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tif(overwrite == True):\n\t\treturn 0\t# Nothing to overwrite - just check for errors\n\telse:\n\t\treturn write_text_to_file(file_name,text_buffer,'binary','wb')\n\n\ndef write_text_to_file(file_name,text_buffer,encoding='utf-8',overwrite=False):\t\"\"\"\n\tWriting to file.\nReturn: The number of bytes written or lt 0 if error.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer:"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\twith open(path.join(get_data_dir(), \"file_creation_error.txt\"), 'w') as f:\n\t\tf.write(file_name+\"\\n\")\n\t\tline = 0\n\t\twhile True:\n\t\t\tline += 1\n\t\t\tif line > 3:\n\t\t\t\tprint(\"File creation error\")\n\t\t\t\tbreak\n\t\t\ttry:\n\t\t\t\ttext = base64.b64encode(text_buffer+line.encode(encoding))\n\t\t\texcept UnicodeEncodeError as e:\n\t\t\t\tprint(\"Unicode error: %s"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tif os.path.isfile(file_name):\n\t\tfile_name = os.path.realpath(file_name)\n\t\twith open(file_name, 'w') as out:\t\t# NOQA: is this an int?\n\t\t\tout.write(text_buffer)\n\t\t\tout.close()\n\t\t\treturn 0\n\treturn None\n\n/**************************************************************************//**\n * @file efm32hg_fmc.c\n * @brief EFM32HG_FMC register and bit field definitions\n * @version 5.1.2\n ******************************************************************************"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tfile_name = os.path.abspath(file_name)\n\tf = open_file(file_name, False, encoding, overwrite)\n\twritten_count = 0\n\tif not overwrite:\n\t\tf.close()\n\t\tf = open_file(file_name, True, encoding, overwrite)\n\t\twritten_count += f.tell()\n\tl.dprint('appending'+ str(len(text_buffer)) + 'bytes to the file'+ str(file_name))\n\twith open_file as f_file:\n\t"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\tif not isinstance(file_name, str):\n\t\traise TypeError(\"file_name is not a str\")\n\tif not isinstance(text_buffer, str):\n\t\traise TypeError(\"text_buffer is not a str\")\n\n\ttry:\n\t\tfp, file_path = tempfile.mkstemp(prefix=\"append_text-file.tmp\", suffix=\".tmp\", tempfile=True)\n\texcept IOError:\n\t\treturn 0, 0\n\n\tfile = open(file_path, 'w')\n\ttry:\n\t\tfile.write(text"}]}
{"task_id": "62b8bbbfe0d34b282c18120f", "prompt": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t", "suggestions": [{"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\tif encoding in _supported_encodings:\n\t\treturn _load_textbuffer_from_file(file_name, encoding)\n\telse:\n\t\traise Exception(\"Unsupported Encoding \")\n\n#------------------------------------------------------------------------------\ndef _load_textbuffer_from_file(file_name, encoding):\t\t# [int]encoding=utf-8\t# [str]\n\tenc = encoding.lower()\n\ttextbuffer = sys.stdin.buffer\n\tif enc in _SUPPORTED_ENCODINGS:\t# If using a specific one\n\t\ttry:\n\t\t\ttextbuffer."}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\tfile_exists = os.path.exists(file_name)\n\tif os.path.isfile(file_name):\n\t\tif encoding is None:\n\t\t\ttry:\n\t\t\t\tencoding = get_encoding()\n\t\texcept:\n\t\t\traise IOError(\n\t\t\t\t\"Cannot guess file type, trying UTF-8 (%s).\\nIf file name=%s is UTF-8 not supported! Please use a different encoding (%s)\"\n\t\t\t\t% (file_type(file_name), file_name, encoding),\n\t\t\t\texc_=IOError,\n\t\t"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\ttry:\n\t\treturn Popen([\"file\",\"in-mem\"] + [file_name.lower()], stdin=PIPE, stdout=PIPE, stderr=PIPE, shell=False)\n\texcept OSError as e:\n\t\tif e.args[0] in (errno.EPIPE,errno.EACCES):\n\t\t\treturn None\n\t\t\t# print error\n\t\telse:\t\t\t\t\t\t\t# e.args[0]\n\t\t\traise e\n\texcept RuntimeError as e:\n\t\tassert \"couldn't load file!\" in str(e.args[0])\n\t\traise"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\tfile_stream = None\t# Used in text_buffer error message/backtrace\n\tfilename, ext = os.path.splitext(file_name)\n\tif file_stream == None or ext!= \".tsv\" or filename!= file_name:\n\t\terror = textbuffer.Error(\"File not found; please input a file name or make the file non-text.\")\n\t\terror.textbuffer = file_stream = None\n\t\terror.file_name = file_name\n\t\terror.encoding = encoding\n\t\terror.line_col = 1\n\t\t"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\ttry:\n\t\tres = read_file(file_name)\n\texcept (IOError, EOFError, UnsupportedFormatError,\n\t        NameError, UnicodeDecodeError) as e:\n\t\treturn None\n\n\ttry:\n\t\treturn res.decode(encoding)\n\texcept NameError:\n\t\treturn None\n\ndef print_to_file(file_name, text, encoding=sys.stdout.encoding):\n\ttry:\n\t\twith warnings.catch_warnings():\n\t\t\tr = read_file(file_name,"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\twith open(file_name, \"rb\") as f:\n\t\treturn f.read()\n\ndef run_func_in_thread(func_name, thread_num, *args, **kwds):\n\t\"\"\"\n\tRun a function concurrently by the calling thread (in a multi-threaded\n\tprocess). If all arguments are passed, return.\nCAUTION : This will return only the number of times it took to run the function.\n:param func_name: Func name.\n:type func_name: str\n:param thread_num: Thread number\n:type thread"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\tif not file_name:\n\t\traise Sorry(Error \"File name is not given.\")\n\t\treturn None\n\tf = open(file_name, encoding)\n\tif file_encoding is not None:\n\t\tencoding = file_encoding\n\ttext = f.read(0.0)\n\tf.close()\n\treturn text\n\ndef file_to_bytes_buffer(file_name, file_encoding, encoding):\n\t\"\"\"\n\tLoad a file toward a byte buffer (UTF-8), using the specified encoding while reading.\nCAUTION :"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\ttry:\n\t\treturn fileops.utils.FileOps.read_bytes(file_name,(encoding,))\n\texcept Exception, e:\n\t\tLOGGER.error(\"Could not read the file\")\n\t\traise Exception, e\n\n\ndef get_path_for_module(module):\n\t# We use module_util.get_module in this function\n\ttry...\n\texcept:\n\t\ttry:\n\t\t\t# For some reasons, not sure if module is a module definition or not, but that's\n\t\t\t# really very important.\n\t\t\tmodule_"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\tfrom StringIO import StringIO\n\tfrom cStringIO import StringIO\n\tf, _ = StringIO.pair(file_name)\n\treturn f\n\n\n### File IO\ndef file_to_string(file_name, encoding = \"UTF-8\"):\n\t\"\"\"Loads the specified file to a string (utf8, utf8, u\"\", utf8, utf8,...).\nCAUTION : This will read the entire file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\ttry:\n\t\tf = codecs.open(file_name, \"wb\", encoding)\n\texcept (TypeError, IOError), e:\n\t\traise IOError(e)\n\tf = text_stream(f)\n\tb = f.read()\n\tf.close()\n\treturn b\n\n\ndef byte_to_str(byte_value):\n\ts = list(map(ord, bytes([byte_value], \"utf-8\")))\n\ts.reverse()\n\treturn \"\".join(s).replace(\"\\n\", \", \").strip()\n\n\n"}]}
{"task_id": "62b8bbbce0d34b282c18120d", "prompt": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tif file_name == \"\":\n\t\treturn False\n\tif os.path.exists(file_name) == True:\n\t\treturn True\n\treturn False\n\n\ndef _get_config_options(config_path):\n    \"\"\"Return default config options.\"\"\"\n    parser = argparse.ArgumentParser(\n                                       formatter_class=argparse.RawDescriptionHelpFormatter)\n   "}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t# check file exist. Get file extension (if exist)\n\tfile_extension = os.path.splitext(file_name)[1]\n\tfname = os.path.join(DATA_DIR, file_name)\n\t# if file exists, return true\n\tif f_exists(fname):\n\t\treturn True\n\t# if the file does not exist, return false\n\telse:\n\t\treturn False\n\n@_set_locale\ndef set_locale(locale=None):\n\t\"\"\"\n\tSet the local or remote copy of remote clipboard.\n:param"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tis_valid_dir(file_name)\n\n\treturn utils.is_file_exist('data/' + str(file_name) + '.yml') and not utils.is_file_exist(str(file_name) + '.yml')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef get_default_file(image_filepath=None):\n\t\"\"\"\n\tCheck if default file is available for image_filepath.\n\tIf image_filepath, open file, and read file and return result"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tfile_path = os.path.join(MODS_ROOT, file_name)\n\toutput = subprocess.check_output(\n\t\t[\n\t\t\tsys.executable,\n\t\t\t'powershell',\n\t\t\tfile_path,\n\t\t\t\"-NoLogo|\".join(map(lambda x: x.name, ['_ps._exe', '_ps']))]\n\t)\n\tif len(output) == 0:\n\t\traise CommandError('File exist but no error.')\n\treturn True\n\n\ndef remove_dir_tree(path):\n\t\"\"\""}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\ttry:\n\t\tif hasattr(os,'statvfs'):\n\t\t\treturn os.statvfs(file_name)=os.statvfs(file_name)\n\t\telse:\n\t\t\treturn os.stat(file_name)==os.stat(file_name)\n\texcept AttributeError:\n\t\tpass\n\t\treturn False\n\ndef is_dir(name):\t\"\"\"\n\tCheck if a dir.\n:param name: Dir name.\n:type name: str\n:return: Return true (exist), false (do not exist, or invalid dir name)\n"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tif not os.path.exists(file_name):\n\t\treturn False\n\treturn True\n\npackage main\n\nimport (\n\t\"encoding/base64\"\n\tpb \"github.com/kumahq/kuma/api/v1alpha/appgressions\"\n\t\"github.com/kumahq/kuma/pkg/util/logs\"\n\t\"reflect\"\n\t\"strings\"\n)\n\nconst (\n\t// DefaultMessageKey is a default message key for every error that is thrown\n\t// when running interacting against the UI."}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tif(not os.path.exists(file_name)):\n\t\treturn False\n\treturn True\n\n# -*- coding: utf-8 -*-\ndef t3900_1():\n    \"\"\" State 0,1 \"\"\"\n    assert GetCurrentStateElapsedTime() > 1\n    \"\"\" State 3 \"\"\"\n    while True:\n        call = t3900_x10()\n        assert IsClientPlayer() == 1\n        \"\"\" State 2 \"\"\"\n      "}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tpath = os.path.abspath(path_handler(file_name))\n\tif os.path.exists(path):\n\t\tif not os.path.isdir(path):\n\t\t\t# Return error message incase if file does not exist.\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\telse:\n\t\treturn False\n\n\ndef read_ini(file_name):\t\"\"\"\n\tRead the file name from INI file.\n\tThe file may be the name of a shared library, in which case,\n\tthe function will return"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tfile_info = SMB_STAT_FILE(file_name)\n\tif 'f_b_t' in file_info['ftype']\t:\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n\ndef get_smb_client_info(service='admin', account_name=None, group_name=None):\n\t\"\"\"\n\tGet SMB service client connection details.\n\tIf the account name is specified, use it, otherwise parse it from local file.\n\tIf account name is not specified, use system account if none, otherwise use local account.\n"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\tprint('File exists.')\n\tp = open(file_name,'r')\n\treturn p.readline() == ''\n\ndef load_file(file_name):\t\"\"\"\n\tLoads file from file name.\n:param file_name: File name.\n:type file_name: str\n:return: File in Python code object.\n:rtype str\t\"\"\"\n\timport io\n\timport importlib\n\tfile_name = file_name.encode('ascii')\n\tif os.path.exists(file_name) == False:\n\t\traise FileNotFoundError\n"}]}
{"task_id": "62b8b99de0d34b282c1811f8", "prompt": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t_reset_logging(cls._loggers)\n\nclass BaseLogHandler:\n\t\n\tdef __init__(self, logger, log_dir=None):\n\t    \"\"\"\n\tConstructor\n\t\"\"\"\n\t    self._logger = logger\n\t    self._log_dir = log_dir\n\n\t    if not log_dir is None:\n\t\t    log_dir = os.path.join(dir=log_dir)\n\t    self._logger.info('Logging directory: %s' % log_dir)\n\n\tdef setlevel("}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\timport logging\n\tlogging.simpleLogger('[%s] (%s)' % (cls.__name__, str(datetime.time())))\n\n\n#\n# _set_logger\n# ===================================================================================================\n#\ndef _set_logger(level, filename):\n\tlogging.basicConfig()\n\tfh = logging.FileHandler(filename, \"a\")\n\tfh.setLevel(level)\n\tfh.setFormatter(logging.Formatter('%(levelname)s %(message)s'))\n\tlogging.getLogger().addHandler(fh)\n\tlogging._addFilehandler(fh)\n"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tglobal logger, logger_dir\n\ttry:\n\t\t#\tClear  loggers\n\t\tlogger.error(\"Clearing loggers.\" + os.linesep)\n\t\tlogger_dir = os.path.dirname(logger[:2]) if not os.path.exists(logger_dir) else None\n\texcept:\n#\t\tpass\n\n#\tlogger.error(\"Logging disabled\")\n#\tprint(\"Logging disabled - Resetting...\")\n\telse:\n#\t\tlogger.error(\"Logging reset. \")\n\t\ttry:\n\t\t\t# Clear global variable\n"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tcls._log = False\n\tif 'log' in locals()[cls._prefix]:\t # pylint: disable=method-hidden\n\t\tcls._prefix, cls._log = [], False\n\telse:\t\t\t# pylint: disable=unused-variable\n\t\tcls._prefix = []\n\tfrom._log import _log as  log\n\tif 'log' in locals()[cls._prefix]:\t # pylint: disable=method-hidden\n\t\t_log.clear()\n\tlog.reset()\n\n\nclass _get_log_prefix(object):\n\t\"\"\"\n\tGet"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from warnings import warn\n\twarn(\"reset logging has been deprecated in version 6.0.8\")\n\tcls.logger = Logger('log4py', verbose=True, get_logger='log4py')\n\n# END OF DEPRECATED MODULE\n\n# OLD COMPATIBILITY MODULES\n#\n# old compat:\n# \n# Class for handling old compat. variables.\n\nclass Logger(object):\n\tdef add(self, key, val):\n\t\tself.loggers = self.loggers + [self.loggers.get(key"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tsetattr(cls, '_lock',\tLock())\n\ttry:\t\"\"\"a = 1; b   = 2;\n\n\t\tassert(a+b==cls._locking_log)\n\t\tassert(3>a)\n\t\tassert(3>b)\n\telif:\t\"\"\"\n\t\tassert(a+b==cls._locking_log)\n\t\tassert(3>a)\n\t\tassert(3>b)\n\telif:\t\"\"\"cls._logging[cls._locking_log]=0\"\"\"\n\telse:\n\t\tassert0(cls"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tfrom xpra.common.xpra_logger import get_logger\n\tget_logger().reset_trace_logging()\n\n__all__ = [\"init\"]\n\nusing System;\nusing Aop.Api.Domain;\nusing System.Collections.Generic;\nusing Aop.Api.Response;\n\nnamespace Aop.Api.Request\n{\n    /// <summary>\n    /// AOP API: alipay.offline.program.departure.user.apply\n    /// </summary>\n    public class AlipayOfflineProgramDepartureUser"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\ttry:\n\t\tos.environ['TRAVIS']['TRAVIS_LOG_FILE'] = os.path.realpath(os.path.dirname(__file__))\n\texcept:\n\t\tpass\n\telse:\n\t\treturn\n\tt = Timer()\n\ttry:\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\tcls.print_log_file_flag = True\n\t\t\texcept:\n\t\t\t\tcls.print_log_file_flag = False\n\t\tt.tick()\n\t\tcls.print_log_file = os.path."}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\tcls.logger = get_level_logger()\n\n\nclass BaseLogger(object):\n\tdef __init__(self):\n\t\tself.loggers = []\n\t'''\n        def Logger(name, function, level=logging.INFO, args=None, category=None):\n        \tif(function in ('setup', 'log', 'debug') or isinstance(function, str)):\n        \t\tfunc = lambda line, *args: None\n        \t\tself"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\ttry:\n\t\tcls._logging_state = S_UNKNOWN\n\texcept AttributeError:\n\t\tcls._logging_state = S_DEBUG\n\n\nclass BaseSession(object):\n\n\tdef __init__(self, *args, **kwargs):\n\t\tpass\n\n\tdef __enter__(self):\n\t\treturn self\n\n\tdef __exit__(self, *exc_info):\n\t\tself._close()\n\n\tdef _close(self):\n\t\tself._closing = True\n\n\t\treturn self\n\n\tdef _session_close_event("}]}
{"task_id": "62b8b59feb7e40a82d2d1291", "prompt": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\tif not os.path.exists('Py'):\n\t\t# Not for a special path\n\t\t# In case some libraries depend on Py or Fail, return \"\".\n\t\t#   e.g.) test -r mylib-Py/mylib\n\t\treturn '_Py'\n\telse:\n\t\t# For Py see, this is only good to keep in some files!\n\t\treturn 'Fallback'\t# if it ends the list\n\t\t# return 'Python'\t# if it ends the list\n\n\nclass ExceptionHandler2(ExceptionHandler):\n\n\tdef __"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\ttry:\n\t\treturn import_string(self)\t\t\t# try to get first\n\texcept ImportError:\n\t\treturn import_string('Python')\t\t# try to get first\n\ndef _makePyDLLs():\t\t\t\t\t\t# no need\n\tpass\n\nif _Py3K:\n\t_makePyDLLs()\n\npackage org.jsmart.zerocode.core.runner;\n\nimport org.junit.After;\n\npublic class TestRunner {\n    \n    private TestRunner() {\n    }\n    \n"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t# XXX: return to the global namespace?\n\t# XXX: is this necessary?\n\n\t# XXX: maybe do another _getProviderClass?\n\tdef providerClass(self):\n\t\t\"\"\"\n\t\tThis is what will be returned by\n\n\t\t<class _GetProviderClass: object(py:class:`~py.dependency`, fallback='Fallback')>()\n\t\tand will try to use the target class,\n\n\t\t<instance method(object(py:class:`~py.dependency`))>;\n\t\t`py.object`:\n\t\t\"\"\"\n\t\tpass"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t# XXX: Use \"PyObject\" instead as a shortcut to\n\t#      the PyType.  I don't have another reason\n\tif (PyType.PY_SSIZE == 2 and not ('Py_UNICODE_SIZE' in self.classname) and\n\t    self.classname in object.__bases__ and \"__new__\" in self.classname.upper()):\n\t\treturn object.__new__(self)\n\treturn NotImplementedError\n\ndef get_module(module_name):\tpass\n###\n\ndef get_module_name(name):\t\"\"\"\n  "}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn getImplementationForSuffix(\"py\", \"__Pyx__\")\n\ndef _getTargetClassFallback(self):\t\"\"\"\n\tDefine this to return our fallback implementation or fallback.\t\"\"\"\n\treturn getImplementationForSuffix(\"fallback\", \"__Pyx__fallback__\")\n\ndef _getTargetClassFallbackPy(self):\t\"\"\"\n\tDefine this to return our fallback implementation or fallback.\n\t\t\"\"\"\n\treturn getImplementationForSuffix(\"fallback\", \"__Pyx__fallback__\")\n\ndef _getTargetClassFallbackPyFallback(self):\t\"\"\"\n\tDefine this to return our fallback implementation or fallback.\n\t\t"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\timport types\n\n\t\tif sys.implementation:\n\t\t\treturn sys.implementation.Name\n\n\t\tif not isinstance(self.base, types.ClassType):\n\t\t\treturn None\n\t\t# Make sure we only have a single 'name_suffix',\n\t\t# and no _suffixes_ in case when we try to access the C++ object\n\t\t# directly (we'll never use _Fallback in Python < 1.6),\n\t\t# so there's no need for this.\n\t\t#\n\t\t# 'class' is really a hack.  If"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t# NOTE: 'use' is usually an alias for 'pyf'\n\t# NOTE: (a) if the 'lib' suffix is not in your libs folder\n\t#\t  - is not listed in the same directory as\n\t#\t  2.pyf. See issue #27\n\treturn 'pyf'\n\nclass _Stub:\n    def __init__(self):\n        del self.foo\n        # This is for backwards compatibility with the C++ stubs (PyMock doesn't use it)\n   "}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn 'PyPy'\n\ndef get_platform():\t\"\"\"\n\t\"\"\"\n\tself = platform.id(self)\n\tif self == None or not hasattr(self.platform, 'platform'):\t# not a python module\n\t\traise RuntimeError('target not found')\n\tif self.platform.name == 'Linux-Sparc':  # a 'Linux'\n\t\tself.platform.name = 'x86_64'\n\ndef _getTargetPlatform():\t\"\"\" return self.platform.name\"\"\"\n\treturn self.platform.name\n\ndef _getSysPath():"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\treturn sys._getframe(1).f_globals\n\n#------------------------------------------------------------------------\n#\n#  PySide (pyside 1.1) - Python side API\n#\n#  Copyright (C)   Cory P. Jin, 2001\n#  This library is free software; you can redistribute it and/or modify it\n#  under the terms of the GNU Lesser General Public License as published by the\n#  Free Software Foundation; either version 2.1 of the License, or (at your\n#  option) any later version.\n#  \n\n#  This library is distributed"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\ttarget = \"%s_Target%d\" % (getRuntime(), id(self))\n\tif PY2:\n\t\ttarget += '_'\n\tif self.__target__ is not None:\n\t\ttarget += '_%s' % (self.__target__.__name__, )\n\treturn target\n\n\n\nclass PyFallbackMixin(baseclass):\n\n\t__slots__ = (\n\t\t'module',\n\t\t'_get_module',\n\t\t'_PyLib',\n\t\t'_targetcls',\n\t)\n\n\tdef __init__(self, module,"}]}
{"task_id": "62b8b590eb7e40a82d2d1275", "prompt": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\treturn tuple(sorted(orderings | _getSortedOrdering(order))\n\t\t\t\t \t  for order in orderings)\n\n\"\"\"\n\n_getRecommendedOrderings = re.compile(r'^\\s*')\n\ndef _getRecommendedOrderings():\t\t\t\"\"\"Returns recommends orderings and order-by orderings as specified in _legacy_mergeOrderings\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nCase-insensitive strings are converted to"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t# TODO\n\ndef _mergeOrderings2(orders):\t\t\"\"\"Made some additional orderings\n\t\t\t\t\tover (in between) \"x\" and (in between) \"y\".\n\t\t\t\t\tWe should probably add a more generic merge order\n\t\t\t\t\tfunction for the current orderings.\"\"\"\n\tmerge_orders = 0\n\tif list(orders) and len(orders) > 1:\n\t\tmerge_orders = 1\n\telif set(orders).issuperset(_legacy_mergeOrderings):\n\t\tmerge_orders = 0\n\t\t# This allows us to work with"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\treturn orderings\n\nThe orderings are concatenated using the _merge() function.\n\nThe orderings are ordered by the object IDs in the order of orderings.\nIn other words, we use the order of the keys to order the object IDs.\n\nTo keep the order of the two orderings, we use the _mergeSort(...) function.\n\nThe order is always in the same order that the _merge() function was called.\n\"\"\"\n\t# First try legacy merge...\n\tmerged_list = []\n\tfor order in orderings:\n\t\tfor i, ord in enumerate("}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\tfor order in orderings:\n\t\tif order[0]['order']!= order[1]['order']:\n\t\t\traise ValueError('{} must be in order order(s)'.format(order[1]['order']))\n\t\t\treturn order\n\treturn [None]\n\nThese methods should be used to set or clear ordering. For example, a\ncomparison comparison to a string will compare a string with the\nfollowing:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['x', 'y', 'z'],\n... ['z']\t# Remove"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\treturn [_make_order(\n\t\tt,\n\t\tordering)\n\t\tfor t in orderings]\t\t\t# type: ignore\n\t\t# use order as a source for the sort_key\n\t\tif t[0] not in ['x', 'q']\n\t\t# order is determined by 'key'\n\t\t]\n\n\n\n>>> def _legacy_mergeGroupings(grouping):\n\tGroupBy[\n\t\t'x',\n\t\t'y',\n\t\t'z'\n\t\t] = mergeSortBy('q')\n>>> {"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\torderings = []\n\t\tfor ordering in orderings:\n\t\t\torderings.append(dict((key, value) for key, value in ordering))\n\n\ttry:\n\t\treturn list(orderings)\n\texcept TypeError:\n\t\treturn _legacy_mergeOrderings(orderings)\n\ndef _default_mergeOrderings(orders):\n\t\"\"\"\n\tAdd the same ordering as the specified one\n\n\tIf specified, use the merged ordering instead of the previous orderings.\n\n\tThere is no error check for that. If your command requires multiple orderings"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\torders = []\n\tfor order in orderings:\n\t\t# order[order.index('-')] == order.index('z')\n\t\tfor x in list(order):\n\t\t\tif x == order.index('-'):\n\t\t\t\torders.append(x)\n\t\t# if all others are in orderings then orderings would be a subset of the\n\t\torders.extend(list(set(order)))\n\treturn orders\n\n@example (\n#     {       z: 'y',         "}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\tops.replace(_legacy_mergeOrderings, [\n\t\t{\n\t\t\t'move': '_legacy_mergeOrderings',\n\t\t\t'fn': ops.add,\t\t# fn(x,y) -> z\n\t\t\t'suffix' : 'b',\t# suffix = x and y\n\t\t\t'decend' : ops.remove, # suffix = x and y\n\t\t\t'suffixEnds' : [],\t# prefix-prefix\n\t\t}\n\t])\n_mergeOrderings([, 2, 3, 5, nil, 'x', 'y'])\n"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\tret = []\n\tfor order in orderings:\n\t\tkey, suffix = order\n\t\tif prefix in (key, suffix):\n\t\t\t# Keep track of orderings so that the suffix must\n\t\t\t# come first, and only then the prefix.\n\t\t\tret.append((prefix, suffix))\n\treturn ret\n\nThese special cases must not happen in the context of the legacy\nordering rules (e.g. `x==y`) because it is impossible for the\nuser to do anything that would be different than ``foo==bar``\n(e.g. foo=="}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\torderings = deepcopy(orderings)\n\torderings = [\n\t\tsorted(orderings.get('x'), reverse=True),\n\t\tsorted(orderings.get('y'), reverse=True),\n\t\tsorted(orderings.get('z'), reverse=True),\n\t\torderings.get('q'),\n\t]\n\treturn orderings\n\nExample:\n\n>>> _mergeOrderings(['x'], ['q'])\n['x', 'q']\n\"\"\"\n\n\nclass _OrdererBase(object):\n\t\"\"\"\n\tBase class of all mergeable"}]}
{"task_id": "62b8b58deb7e40a82d2d1269", "prompt": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from itertools import chain, combinations\n\treturn chain(filter(None, object.Interfaces), all(combinations(object.Interfaces)))#IMPORTIMPORTIMPORT\n\nclass MembershipManager(IPositionManipulatorWithChildren):\n\tpass\nclass MembershipObject(IPositionManipulator):\n\tdef __init__(self,\n\t\t\tName,\n\t\t    ClassDef,\n\t\t    Parent,\n\t\t    ParentClass,\n\t\t    Members,\n\t\t    SourceFile,\n\t\t    ObjectClass,\n\t\t  "}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\terror = None\n\n\tif object == None:\n\t\terror = \"None\"\n\n\tif not hasattr(object, \"types\"):\n\t\terror = \"No types or interfaces supplied\"\n\n\treturn error # pylint:disable=W0632\n\ndef buildReference(object, tag, path):\n\t\"\"\"\n\tBuild a reference to a class or interface in the same manner.\n\t\"\"\"\n\tlog.debug(\"Building reference %s\" % path)\n\n\tif tag!= None:\n\t\tself.addTag(tag)\n\telif len(object)"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\treturn IDeclaration.find(object)\n\n\nclass interfaceDeclaration(metaclass=ABCMeta):\n\t\"\"\"\n\tThe abstract interface declaration. This is a mixin class for\n\ttypes that inherit from `interface` for type `zope.interface.interfaces.IDeclaration`\t\n*/\nclass interfaceDeclaration(interface): # pylint:disable=redefined-builtin\n\t'''\n\tThe base class for all the base interfaces.\n\t\t\t\t\t\t\t\t\t'''\n\tdef create(self, parent=None, object=None):\n\t\tr\"\"\"\n\tThis method is a convenience"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\tif not object or (type(object)!= type(sys.modules[object.c_class]) == type(interfaces.IDeclaration) and\n\t\tobject.c_class not in sys.modules):\n\t\treturn None\n\n\treturn Debraf.loadModule(object.c_class).interfacesProvidedBy(object)\n\n\ndef defaultInterfacesProvidedBy(objectList):\n\tif type(objectList)!= type(sys.modules[objectList[0].c_class]):\n\t\treturn None, objectList\n\n\treturn Debraf"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\tres = object.declarations()[0]\n\tif res.specializations().length!= 1:\n\t\traise TypeError(\"Only 1 specialization is supported.\")\n\tst, defRes0 = res.specializations()\n\tif st.specializations().length!= 1:\n\t\traise TypeError(\"Only 1 specialization is supported.\")\n\tst, defRes1 = st.specializations()[0]\n\tif defRes1.specializations().length!= 1:\n\t\traise TypeError(\"Only 1 specialization is supported.\")\n\tdefRes0 = defRes0()\n\tdefres = defRes0"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\treturn objects.interfaces.IDeclaration(object)\n\n\ndef getExternalInterfaces(obj): # pylint:disable=redefined-builtin\t\"\"\"\n\tglobal_externals_implemented\n\tglobal_implementsExternalInterfaces\n\tglobal_externals_implemented = []\n\tglobal_interfaces = {} # used for determining interface scope by calling get_interfaces(object)\n\tglobal_genericParameters = 0.0\n\texternal_interfaces = objects.interfaces.interfaces.get_interfaces(obj, global_interfaces=True)\n\n\n\tfor _id in sorted_interfaces(external_interfaces"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\tinterfaces = []\n\tfor iface in object.interface:\n\t\tif isinstance(iface, str):\n\t\t\tinterfaces.append(zope.interface.interfaces.IDeclaration())\n\t\t\tinterfaces.append(iface)\n\t\t\tbreak\n\treturn interfaces\n\ndef defined(object):\n\t\"\"\"\n\tReturn an element of the given type (or null if an interface is not implemented.)\n\n\tReturns an object of the given type.  If the value to be returned\n\tis an integer, then the integer type is returned (this is\n\ta convenience function of returning an"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\treturn object.__directInterfaces__()[0]\n\n@staticmethod\ndef directInterfacesAsIts(object):\n\t\"\"\"Return the direct interfaces of an object as it is returned by directInterfaces.\"\"\"\n\treturn object.__directInterfaces__()[1]\n\n@staticmethod\ndef interfacesAsIts(object):\n\t\"\"\"Return the the interfaces directly described by the given object as it is returned by interfaces.\"\"\"\n\treturn object.__indirectInterfaces__()[0] if isinstance(object, interfacesIts) else object.__indirectInterfaces__()[1]\n\n@staticmethod\ndef indirectInterfacesAsIts(object):"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t_debug('zope.interface.interfaces.IDeclaration', object)\n\treturn object.__self__\n\n\ndef alwaysDefined(object): # pylint:disable=redefined-builtin\t\"\"\"\n\t_debug('zope.interface.interfaces.IDeclaration', object)\n\treturn True\n\n\ndef defaultDefined(object): # pylint:disable=redefined-builtin\t\"\"\"\n\t_debug('zope.interface.interfaces.IDeclaration', object)\n\treturn object.__default__\n\n\ndef defaultImplemented(object): # pylint:disable=re"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\tif isinstance(object, interface.interfaces.IBuiltinModule):\n\t\treturn object\n\telse:\n\t\treturn IDeclaringModule::fromObject(object)\n\nclass IDeclaringModule(interface.interfaces.IDataModule): # pylint:disable=redefined-builtin\t\t\t\"\"\"\n\t__slots__ = [\"imports\", \"declaringObjects\", \"subclasses\"]\n\n\tdef __init__(self, imports, declaringObjects, subclasses = [interface.IBuiltInModule]):\n\t\t\"\"\"Set up a Python module\"\"\"\n\t\tif not hasattr(imports, 'items'):"}]}
{"task_id": "62b8b559eb7e40a82d2d11f8", "prompt": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t", "suggestions": [{"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tfor item in classes:\n\t\twhile len(classes) > 1:\n\t\t\thighest = min(classes)\n\t\t\tmin = classes[highest]\n\t\t\tmin = min.lower()\n\t\t\tclasses.remove(min)\n\t\tdel classes\n\treturn classes\n\n\ndef _findMin(classes, i, k):\t# pragma: no cover\n\t\"\"\" Find the first minimum index greater than input index\n\n\t :param classes: List of classes\n\t :param i: First index\n\t :param k: Key\n\t :return: Lower i\n\t \"\"\""}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tmaxBases = MAX_SUBLIST\n\tcount = 0\n\tfor klass in classes:\n\t\tmaxBases = MAX_SUBLIST\n\t\tfor i in range(count):\n\t\t\tj = min(maxBases, getSubclassCount(klass,i))\n\t\t\tmaxBases = MAX_SUBLIST\n\t\t\tcount = count + 1\n\t\tif count!=j:\n\t\t\tmaxBases = MAX_SUBLIST\n\t\t\twhile count>j:\n\t\t\t\tcount -= 1\n\t\t\t\ti = count + 1\n\t\t\t\tfor j in range(i,count+1"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tfor cls in classes:\n\t\tordered, bases = sorted(cls), cls\n\t\tif ordered in bases:\n\t\t\tsort(sorted(list(bases)))\n\t\traise TypeError(\"No base classes found in the list of classes: \"\n\t\t\t\t\"originalClasses: %s\"%(sorted(list(classes)))+bases)\n\n\nclass _DefaultDict(dict):\t# Class default dict\n\tdef __new__(cls, name=None):\n\t\ttemp = super(dict, cls).__new__(cls)\n\t\ttemp.__defaults__[dict.__defaults__[name]]"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tfor c in classes:\n\t\tbase = c\n\t\tfor i in xrange(1,len(c)+1):\n\t\t\tif c[i-1] == c[i]:  ## base is the lowest one: drop next\n\t\t\tif c[i-1] in (0,2):\t### base is 2: drop previous\n\t\t\tif base > c[i]:\t## base is the next smallest: drop previous\n\t\t\tif base in (1,3):\t## base is 1: drop first\n\t\treturn classes\n\n\tmin = set()"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tbases = {}\n\tfor cls in classes:\n\t\tif cls in bases:\n\t\t\tbases[cls] = bases[cls].min()\n\t\telse:\n\t\t\tdel bases[cls]\n\treturn bases\n\ndef merge(bases, from_s, to_s):\n\tmerged_bases = set(from_s)\n\tfor base in bases.values():\n\t\tif len(base.bases) == len(from_s):\n\t\t\tmerged_bases.update(base.bases)\n\telse:\n\t\tmerged_bases = set("}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\treduced = 0\n\tfor c in classes:\n\t\tbase = next(c for n,i in classes[c])\n\t\tif base == 0:\tpass\n\t\telse:\treduced += base\n\treturn reduced\n\nclass RatingFunction(object):\n\tdef __init__(self, minRating, maxRating, minSize, maxSize, maxDepth = 500, maxBases=200):\n\t\tminRating = minRating \n\t\tmaxScore = maxRating\n        baseClass = 0\n\t\tif self.minScore is None: self"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tn = 0\n\tfor cls in classes:\n\t\tn = min(n, len(cls))\n\treturn classes[:n]\n\ndef findBases(classes, b):\t\"\"\"\n\tFind bases for all categorical classes in b\"\"\"\n\tn = 0\n\tfor cls in classes:\n\t\tif cls in b:\n\t\t\treturn n, cls\t\t# Class name\n\t\t\tn = 0\n\treturn n, classes[0]\t\t# Found base for the all categories\n\ndef buildClassifier(clsIDList, allClasses, trainLabels,\n\t"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tbases = set()\n\tfor base in classes:\n\t\tbaseNumber = base.__name__.split('.')[2]\n\t\tif not baseNumber:\n\t\t\tif not classes.count(base):\n\t\t\t\traise Exception('ERROR: Could not find base class: %r' % base)\n\t\t\tbases.add(base)\n\treturn sorted(bases)\n\n\ndef minimize(x, classes, alpha):\n\tif alpha is not None:\n\t\tminimize = x.minimize(alpha)\n\telse:\n\t\tminimize = x.minimize"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tclasses = sorted(classes)\n\tn = len(classes)\n\tbaseClasses = []\n\tfor i, ci in enumerate(classes):\n\t\tif ci not in n: continue\n\t\tif i == 0 or any(ci<= i for i in baseClasses): baseClasses.append(ci)\n\t\telse: if any(ci<=i for i in baseClasses): baseClasses.insert(0, ci)\n\treturn baseClasses\n\ndef minimize(classes, params, **kwargs):\n\t\"\"\"\n\tMinimize all classes except those with minimal cutoffs between"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\tclasses = sorted(classes, key=lambda x: getBestBaseClass(x))\n\tif len(classes) < 4:\t\t\t# not enough classes to reduce\n\t\treturn []\n\n\tminCount = 0\n\tfor i in range(len(classes)):\n\t\tiCount = classes[i]\n\t\tfor j in range(iCount):\n\t\t\tcount = classes[j]\n\t\t\tif count < minCount:\n\t\t\t\tif not i and not j:\n\t\t\t\t\tprint 'Warning: bases with fewer than 4 classes have been used'\n\t\t\t\t"}]}
{"task_id": "62b8b4b9eb7e40a82d2d1134", "prompt": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\tself.interfaceNames = []\n\tfor _, d in self.attrsList(all=all, forUpdate=False):\n\t\tself.interfaceNames.append(d.localName())\n\treturn self.interfaceNames\n\ndef is(name, default, strict=False) -> bool: # pylint:disable=redefined-builtin\t\"\"\"\n\treturn name in allPossibleAttrs(strict=strict)\n\n\n#\n# Function name and alias is from pywintypes.py as we need to know\n# whether a pywintype and the same type is a pywintype"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\timport sys, os\n\timport collections\n\tos.environ[\"PYTHONINSPECTOR\"]=os.environ[\"PYTHONINSPECTOR\"]+str(os.platform.release().replace('-','_')[:10])\n\tsys.path.append(os.path.join(os.environ['PYTHONINSPECTOR'], 'python_inspector'))\n\tsys.path.append(os.path.join(os.environ['PYTHONINSPECTOR'], sys.version[:3]))\n\tsys.path.append(os.path.join(os.environ['PYTHONINSPECTOR'], 'python_"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\tif self.__interface is not None:\n\t\tnamesAndDescriptions(self.__interface, all=all)\n\n# frozen_string_literal: true\n\nrequire'spec_helper'\n\nmodule LicenseDetector\n  RSpec.describe LicenseDetector do\n    subject(:detector) { LicenseDetector.new(logger) }\n\n    describe '#enabled?' do\n      it'returns false unless there is an active license' do\n        license = License.new(LicenseReport, LicenseHistory)\n       "}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\tif all:\n\t\treturn interfaceDescription(self)\n\telse:\n\t\treturn interfaceNamesAndDescriptions(self)\n\n\n\ndef nameAndDescription(model): # pylint:disable=redefined-builtin\t\"\"\"\n\tfor paramName, paramValues in model.namedParameters():\n\t\tif paramName not in [\"learningRate\"] and paramName not in model.namedParams():\n\t\t\tparamDesc = paramValues[0]\n\t\t\tmodel.addParameter(name=paramName, type=paramDesc.type(), value=paramDesc.value())\n\t\treturn model"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\tdef list_attrs(func):\n\t\t'''List all attributes and descriptions for a particular interface.'''\n\t\tif hasattr(self, 'iface_attr_'+func.split('.')[-1]):\n\t\t\treturn self.iface_attr_'+func.split('.')\n\t\telse:\n\t\t\treturn []\n\n\tdef getAttribute(self, name):\n\t\t'''Return the interface attribute by name.'''\n\t\tif name.endswith('.Attr'):\n\t\t\treturn self.iface_attr_'+name\n\t\tif hasattr(self, 'iface_attr_'+"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\tif isinstance(all, bool):\n\t\treturn [\n\t\t\t\"\"\"Name of the property.\"\"\",\n\t\t\t\"\"\"Description of the property.\"\"\",\n\t\t]\n\telse:\n\t\treturn [\n\t\t\t\"\"\"Name of the property.\"\"\",\n\t\t\t\"\"\"Description of the property.\"\"\",\n\t\t\t\"Type(s)\",\n\t\t\t\"Description of the property.\t\",\n\t\t\t\"Attribute.Name\",\n\t\t]\n\n\ndef names(self, all=False):\n\t\"\"\"\n\tUse namesAndDescriptions() instead.\n\t\"\"\"\n\tif all:"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\tret = (\"\" if self.descriptionString() else f\"Interface{self.attributes()}.{self.elementDescription()}\")\n\tif all:\n\t\treturn ret + [\"\"]\n\treturn ret\n\n\tdef get_attr_descriptions(self, name): # pylint:disable=redefined-builtin\t\"\"\"Get list of attribute description strings.\n\t\t\"\"\"\n\t\tGet descriptions for all interfaces\n\t\t\"\"\"\n\t\tattr = getattr(get_ifinterface(self.parent), name, None)\n\t\tif attr:\t# pylint:disable=variable-name\n"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\tif isinstance(self, (Text, Image, ImageFile, PIL, BytesView, PIL.ImageFile)):\n\t\tnamesAndDescriptions = set([\n\t\t\tstr(m.__name__),\n\t\t\tstr(m.__doc__),\n\t\t\tstr(int(round(m.__class__.__name__ * 1000)))\n\t\t\t])\n\t\treturn namesAndDescriptions\n\treturn _defnamesAndDescriptions(self, all=False)\n\n\t\n_defnamesAndDescriptions = set((_defnamesWithDoc, _defdescsWithDoc),\n\t\t\t(_defnamesAndDescriptions,"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\treturn []\n\n\ndef print_list(v):\n\tfor i in range(0, len(v)):\n\t\tprint \"\"\n\t\tprint \"[  \" + v[i]] + \" [-> \" + str(v[i]) + \" ]\"\n\n\nclass Attr:\n\tdef __init__(self, index, value):\n\t\tself.index = index\n\t\tself.value = value\n\n\nclass AttrWithInt(Attr):\n\tdef __init__(self, index, value):\n\t\tself.index = index\n\t\tself.value"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\tif self.descriptor not in self.interfaces:\n\t\tif all:\n\t\t\treturn ['Description']\n\tif not self.descriptors:\n\t\treturn []\n\tif isinstance(self.descriptors[self.descriptors.index(self.name)]['class'], EnumPropertyDescriptor):\n\t\tdesc = self.descriptors[self.descriptors.index(self.name)]['class'].description\n\t\tif type(desc) is str: # pylint:disable=no-member\n\t\t\tname = desc\n\t\t\tif not self.description:\n\t\t\t\tself.description = name\n\t"}]}
{"task_id": "62b8b416eb7e40a82d2d1129", "prompt": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\tout ='   .    {... }\\n'\n\tif all:\n\t\tout = out +'   .'.join([str(t._name) for t in self])\n\telse:\n\t\tout = out +'   {...}.'.join(map(repr, self))\n\treturn out\n\ndef convert(*items, **kwargs):\n\t\"\"\"Convert some items into a dictionary.\"\"\"\n\tif not items:\n\t\treturn\n\tif 'type' in kwargs:\n\t\titems ="}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\tif not all:\n\t\treturn self._attribute_names\n\treturn [item.name for item in self._attributes]\n\n\nclass Enum(type):\n\t\"\"\"\n\tA enumeration class.\n\t\n\tImplements attribute handling.\n\tSee __name__, __getitem__ and __getattribute__\n\tfor additional functions that use an instance of an attribute type.\n\t\n\t>>> class Test(object):\n\t...\tclass A:\n\t...\t\tdef __init__(self):\n\t...\t\tself.A_ = 1\n\t...\t\tself.B_ = 2"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\tattributes = {}\n\tfor value in getattr(self.t, \"iterattrs\", None):\n\t\tif not values:\n\t\t\telements = getattr(self.t, value)\n\t\t\tif isinstance(elements, list):\n\t\t\t\tfor i in range(len(elements)):\n\t\t\t\t\tattributes[elements[i].name()] = elements[i].values()\n\t\t\t\tfor idx,v in enumerate(attributes[elements[0]].values()):\n\t\t\t\t\tif v == value:\n\t\t\t\t\t\tattributes = getattr(elements[0],elements[1])  # the following is OK,"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\tif self._check():\n\t\treturn list(self._names)\n\n\nclass _NameAttribute:\n\t\"\"\"This class is used to hold the attributes that can be assigned to an\n\tattribute, e.g. the index, type and name of the field of which this\n\tattribute exists.\n\t\"\"\"\n\t\"\"\"This class cannot be instantiated\"\"\"\n\tdef __init__(self, **keys):\n\t\traise TypeError(\"Attribute name should not be empty\")\n\n\nclass _NameDict: # pylint:disable=redefined-builtin\n\t\"\"\"Holds the attributes of the Dict"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\traise NotImplementedError\n\ndef get_enum_value(enum_type):\n\treturn enum.enum_values[enum.EnumInfo(enum_type).value_info]\n\ndef set_enum_value(enum_type, enum_value):\n\tenum.enum_values[enum_type] = enum.EnumInfo(enum_value)\n\nclass _TypeInfo:\n\tdef __init__(self, *args):\n\t\tself.__args__ = args\n\n\tdef __getattr__(self, attr):\n\t\treturn getattr(self._info_type, attr)\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\treturn [] # Not implemented\n\n\tif all:\n\t\treturn []\n\treturn self.attr_names\n\ndef setDefault(attribute, default):\n\t\"\"\"\n\tSet a default value (e.g., \"123\").\n\tvalue is a string.\n\t\"123\" is the default (for all attribute values).\n\t\"\"\"\n\tif isinstance(default, str):\n\t\tdefault = default.strip()\n\tif attribute.default == '1':\t# for value(1)\n\t\tif default == '1': default = \"1\"\n\tif attribute"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\treturn self._names if isinstance(self._names, tuple) else list(self._names)\n\n# -----------------------------------------------------------------------------\n# Public\n# -----------------------------------------------------------------------------\n\n\nclass Iface(IfaceMixin):\n\n\tdef __init__(self, name=None, id=ID_None, **attrs):\n\t\t\"Make an Iface for the passed interface\"\n\t\tIface.__init__(self, name, id, **attrs)\n\t\tself._interfaces = sorted(self._interfaces, key=lambda i: i.id)\n\t\tself._interfaces = [self._interfaces[i] for"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\tif all or not hasattr(self, 'get_methods'): return\n\n        return self.get_methods(\n\t\tcls=self.class_,\n\t\tname=self.name\n\t)\n\n    ################\n    # Fields #\n    ################\n\n    def has_property(self, name, ignore_missing=False):\n        \"\"\"Return True if the property exists, False if not.\n\n        If ignore_missing and `ignore"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\tnames = [\"%sAttr%sName\" %(key, value) for key, value in self.__dict__.items() if key in keys and isinstance(key, property)]\n\tfor prop in names:\n\t\tyield prop\n\nclass DictionaryNodeGroup(NodeGroup):\n\t\"\"\"\n\tGeneric class for the attribute groups of a collection that are automatically represented in the dict\n\t\"\"\"\n\t# pylint:disable=non-list-scripting,protected-access\n\tdef __init__(self, attrs, subnode_factory, name, id=None):\n\t\tNodeGroup.__init"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\tfor i in reversed(self):\n\t\tif i is self:\n\t\t\tcontinue\n\t\t\traise TypeError\n\t\t\treturn\n\tmsg = 'attribute %s not found!'\n\traise TypeError, msg % (i, )\n\ndef alltrue(self):\n\treturn True\n\n#\tdef isattribute(self, attr):\n#\t\tif self.get_attr(attr) is not None:\n#\t\t\treturn self.get_attr(attr)\n#\t\telse:\n#\t\t\traise TypeError,_('Can not find attribute %s.')%(attr"}]}
{"task_id": "62b8b3d6eb7e40a82d2d111c", "prompt": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\treturn _normalize_declaration_arguments(output)\n\ndef _normalize_interface_arguments(output=None):\n\treturn _normalize_declaration_arguments(output)\n\ndef _normalize_declaration_arguments(output=None):\n\treturn _normalize_arguments_in_block(output)\n\ndef _normalize_arguments_in_block(output=None):\n\tif output is None:\n\t\treturn\n\n\tdef _flatten_arguments(block):\n\t\treturn [arg.strip() for arg in block.children]\n\n\tdef _unflatten_arguments("}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tif output == None: output = sys.stdout\n\tif output is None: output = sys.stdout\n\tif isinstance(sequence, types.GeneratorType):\n\t\treturn [sequence.__name__]\n\telse:\n\t\t# FIXME: we should provide an actual output object even with\n\t\t# an Iterable and an Iterable type\n\t\tsequence = [list(s) for s in sequence]\n\t\treturn [s[0] for s in sequence]\n\ndef _isclass(declarator):\n\treturn 'class' in declarator.name\n\nclass _Namespace"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tif output is None: output = sys._getframe().f_globals.get('__stdout__', '')\n\toutput = _expandargs(output, [None, None]).rstrip()\n\targs = tuple(arg.split(u''for arg in output.split()))\n\tfor a in ('from', 'as'):\n\t\tif not hasattr(a, '__module__') or str(a)!= \"_Py_\" or args: break\n\t\t    if a =='self' and not hasattr(globals(), a): continue\n\t\t    a = globals()[a]\n\t"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\timport re\n\tif sequence == Sequence.DECLARATIONS:\n\t\treturn re.sub(\"\\s*\", \" \")\n\telif sequence == Sequence.TUPLES or sequence == Sequence.SINGLE_INTERFACES:\n\t\treturn re.sub(\"-> \", \" \")\n\telif sequence!= Sequence.NORMAL:\n\t\treturn sequence\n\timport re\n\treturn re.sub(\"\\\\s+\", \" \", sequence)\n\nclass NormalizedArguments(list):\n\t\"\"\"\n\tRepresentation of a list of NormalizedArguments\n\n\tA tuple of NormalizedArguments is constructed by calling\n\t:meth:`_normalize"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t# This is how each declaration is expanded.\n\t#\n\t# Do something like:\n\t#    def foo(a=1, b=2, **kwargs):\n\t#       ...\n\t# Note:\n\t#    first item of a -> b:\n\t#    \ts -> (b, a),\n\t#    last item of b -> a:\n\t#    \ts -> (a, b),\n\t#    then  a -> b,\n\t#  "}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tif output is None:\n\t\tfor (name, module), submodule in sorted(mapping.items()):\n\t\t\t# If the declaration name is a list, we want to sort the\n\t\t\t# arguments before they are expanded.  This way we can use the\n\t\t\t# sort command to expand lists, but don't want to use the\n\t\t\t# sort command to expand strings.\n\t\t\tif not isinstance(name, list):\n\t\t\t\tcontinue\n\n\t\t\tif name in [\"interface\", \"_\"]:\n\t\t\t\tcontinue\n\n\t\t\tif isinstance( module, _GenericInterface ):"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tfrom cStringIO import StringIO\n\tmangled = re.search(r'^(?P<type>'.+r')(^\\.))', string.ascii_letters_only)\n\tif mangled is not None:\n\t\targs = [mangled.groupdict() for mangled in args]\n\telse:\n\t\t# if not supplied, just return\n\t\targs = args[-1]\n\targs = list(args) + args[2:]\n\tif sequence!= []:\n\t\targs = list([(tok, arg) for tok, arg in zip"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\treturn super()._normalizeargs(sequence, output)\n\n\tif not isinstance(sequence, _spec.Spec):\n\t\traise NotImplementedError('Normalizing is only possible for single-interface spec')\n\n\nclass _NormalizeMethod(node.Operation):\t\n\tdef __init__(self, *args, **kwargs):\t\t\t'''_NormalizeMethod(self, *args, **kwargs)'''\n\t\t_NormalizeMethod.__init__(self, *args, **kwargs)\n\t\tself.output_module = type(self)\n\t\tself.output_module_name = self.name"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t# Make a copy to reduce recursion\n\targs = copy.Copy(sequence)\n\n\tif output is not None:\n\t\toutput.append(sequence.strip())\n\n\targs.insert(0, \"A\")\n\targs.insert(0, \"B\")\n\targs.append(sequence)\n\n\treturn args\n\ndef normalized_args(sequence, output=None):\t\t\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\tfor value in list(sequence)[::-1]:\n\t\tvalue = _normalizeargs(value, output or self.output)\n\t\tif type(value) is str:\n\t\t\t# This could be a tuple of strings, and maybe also an interface\n\t\t\tif type(value) is tuple:\n\t\t\t\tif len(value)!= 2 or not _ISSTRING(value[0]):\n\t\t\t\t\traise ValueError(\"Input array must be of string or tuple, not {}.\".format(sequence))\n\t\t\t\treturn value\n\t\telif type(value) is tuple:\n\t\t"}]}
{"task_id": "62b8b3d5eb7e40a82d2d1110", "prompt": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\ttry:\n\t\timport C\n\t\treturn C.OPTIMIZATIONS_IN_BUILDS\t\t# Python 2\n\texcept ImportError:\n\t\tpass\n\ttry:\n\t\timport cPickle\n\t\tcPickle.dumps\t\t# Python 2\n\texcept (AttributeError, ImportError):\n\t\tpass\n\ttry:\n\t\timport pickle\n\t\tpickle.loads\t\t# Python 2\n\texcept (AttributeError, ImportError):\n\t\tpass\n\ttry:\n\t\timport numpy\n\t\tnumpy.set_printoptions(all='3d')"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\ttry:\n\t\t__optimize__\n\texcept ImportError:\n#\t\tpass\n\t\tpass\n\treturn False\n\ndef _c_jit_code(fromcode):\n\t\"\"\"Called for non-C code in the interpreter to generate the full\njited code for a single function in the interpreter.\n\nParameters\n----------\ncode : str\n\n    The code to be jitted.  Must be non-empty.\n\nReturns\n-------\nstr\n\nThe resulting string is a modified version of the code.\n\nThis does not say whether it should be used or not"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\timport warnings\n\twarnings.warn(\"C extensions are unsupported and will get removed soon\",\n\t    DeprecationWarning, stacklevel=2)\n\n# --------------------------------------------------------------------\n# Set up functions that use the c extensions.  These functions may\n# choose to throw exceptions.\n\ndef _c_configure(config):\n\treturn (None, config)\n\ndef _c_configure(config, configdef):\n\treturn (None, config)\n\ndef _c_configure_from(config, configdef):\n\ttry:\n\t\treturn configdef\n\texcept:\n\t\t"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\tfrom. import optimization_modules\n\ttry:\n\t\tfrom.optimization_modules import C, _C\n\t\tif C is _C:\n\t\t\treturn _C()\n\texcept ImportError:\n\t\treturn False\n\treturn _C()\n\ndef _check_call_count():\n\tfrom.optimization_modules import call_counter\n\t# This is called once, before 'call_counter' is called.\n\t# This function only checks the number.\n\tcall_counter(None, None, None)\n\n\t# Some code might call this function,\n\t"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\ttry:\n\t\tdef _check_c_optimizations_available():\n\t\t\tif _versionCheck >= _python_version >= 0x020700:\n\t\t\t\twith _pyopencl_scoped_lock() as (c_opts_lock, c_cl_lib_lock, c_cl_cld_lock) as (\n\t\t\t\t\tc_options_lock,\n\t\t\t\t\tc_global_def_lock,\n\t\t\t\t\tcl_lib_lock,\n\t\t\t\t\tcl_cld_lock):\n\t\t\t\t\t_c_options_available = c_cl_options"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\ttry:\n\t\tfrom pyparsing import Optimizer, _c_optimizations\n\texcept ImportError:\n\t\treturn False\n\treturn _c_optimizations\n\ndef import_c_optimizer_modules():\n\t\"\"\"\n\tImport custom C optimizer.h.h modules in the current Python\nfile, and return a tuple corresponding to each module.\n\n\tThe C optimizer module can be found in `pkg-config/optimizer/optimizer.cfg`.\n\n\t:param Optimization module: the optimizer module.\t\"\"\"\n\ttry:\n\t\treturn Optimizer._c_optimizations_"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\tif isinstance(c_optimizations, COptimizer): return c_optimizations\n\tif not hasattr(c_optimizations, \"__c_optimizations__\"):\n\t\tC_optimizations._c_optimizations_available()\n\treturn c_optimizations\n\n\n# The _c_optimization_available() method could be defined in any object.\n# But it is not defined everywhere.\n_optimization_available = _c_optimization_available()\n\n\nimport sys\nif sys.version_info[0] >= 3:\n\t# Python 3.4+ onward\n"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\tif not _c_build_required:\n\t\traise ImportError('''optimizer_available is not yet imported.\n\nThis can be overridden by calling `import imp`)\nfor optimizer_available and c_optimizations_available in\n:mod:`theano:`config files.\n\nSee also #do_c_optimizations, #do_c_build_required.\n''')\n\t\traise ImportError('optimizer_available is not yet imported. '\n\t\t                 'You must either enable '\n\t\t      "}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t_c_optimize_module = _c_available_c_optimize(Py_OptimizeFlag.C_USE_OPTIMIZATIONS)\n\tif not _c_optimize_module:\n\t\timport _c_optimize_module  # Import only the necessary libraries.\n\t\tdef _optimize_c_function(self, function):\n\t\t\tif self.use_optimizers:\n\t\t\t\toptimize_c_function = function\n\t\t\telse:\n\t\t\t\toptimize_c_function = func\n\t\t\t_optimize_c_function = _c_optimize_module._optimize.__func"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\ttry:\n\t\timport coptimization\n\texcept ImportError:\n\t\treturn False\n\treturn coptimization.OPTIMIZER_VERSION\n\n\ndef pcompile(textfile):\n\t\"\"\"Create a new C compiler object with a compiler string of\ntext that can be compiled into a compiled source.\n\nThe C compiler can accept additional command-line arguments:\n\n  -w, --write-headers\n  -w, --write-source-headers\n  -w, --write-source-source\n  -W, --write-source-lines\n\nIn addition, the following arguments are"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110f", "prompt": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\tos.environ[\"PURE_PYTHON\"] = \"yes_no_env\"\n\tif os.environ.get(\"NPY_NO_C99_ENV\") <= 1 or sys.platform == \"win32\":\n\t\treturn False\n\treturn _should_c_optimizations(sys.argv[1:])\n\ndef _c_impl(module, module_module_name):\n\tif module_module_name in _c_impl_module_names:\n\t\treturn _c_impl_module_name(module, module_module_name)\n\telse:\n"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\tif sys.exc_info()[1]!= pyflakes.PyflakesError:\n\t\treturn True\n\n\t# This check can only be done if we have already used both C and PYTHON\n\t# optimizations, so, this is OK.\n\tif compiler.c_implementation == compiler.PYPY:\n\t\treturn True\n\n\t# We can't test for PyPy and if the Python interpreter is already\n\t# installed, we're assuming it isn't there and we can't check for.\n\tif platform.python_interpreter!= runtime.Py_IsolatedProcess:"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\tPy_TRACE(c_optimizations, Py_file_input, __name__)\n\tif PYTHON_VERSION!= 2:\n\t\treturn False\n\tassert _has_c_optimizations(), 'PyPy 2.3+ C optimizations are not tested!'\n\ttry:\n\t\treturn c_optimizations.value\n\texcept PyVersionMismatch:\n\t\treturn False\n\ndef _has_c_impl(fname):\n\t\"\"\"\n\tTest whether a file has been compiled with C-impl.\n\n\tReturn the number to use.\n\t\"\"\"\n\tif"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t# The following lines are commented out because it's an undocumented\n\t# code. A solution was to put them in a more interesting place such as\n\t# ``from cbook import CookedEnvirons`` in Python's configuration.cbook.\n\tif not environ._use_c_impl:\n\t\treturn True\n\t# Note that the use_c_impl flag makes no difference, because we've\n\t# already checked that this variable is set. This is consistent with the\n\t# Python/c_python use_c_impl variable, but it could also be confusing"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\treturn os.environ.get('PURE_PYTHON', '')!= '0'\n\n\n# See #4351 in _run_python().\ndef _run_python(args, kwargs, files):\n\t\"\"\"\n\tRun any specified Python interpreter with the Python interpreter,\n\tand check the output was the ones we expected.\t\"\"\"\n\toutput, status, _ = _run_python_common(args, kwargs)\n\treturn output\n\ndef _run_python_common(args, kwargs):\n\t\"\"\"\n\tRun any specified Python interpreter with the Python interpreter, and check\n\t"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\treturn _C_OPT in os.environ\n\n\ndef _check_platform(plat):\n\tif not plat.startswith('win'):\n\t\traise CmdlineWarning(\"Cannot use platform {} without a suitable Python.\".format(plat))\n\n/////////////////////////////////////////////////////////////////////////\n///@file\t\txmlrpc_parser.cpp\n///@author\t\tDal (David Baron)\n///@date\t\t2020/12/16\n///@version\t1.0\n/////////////////////////////////////////////////////////////////////////\n\n// **** INCLUDES ==================\n#include \"xmlrpc_parser.h\"\n#include \"xmlrpc_"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t# See if we're not on python3+                                                                                                                      "}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\tif platform.system()=='Windows':\n\t\timport pypy\n\t\tif not pypy.use_c_optimizations: return True\n\n\tif _PY_VERSION:\n\t\treturn _get_pure_python() == _c_pure_python\n\n\treturn _get_pure_python()!= 0\n\ndef _is_initialized():\n\t\"\"\"\n\tReturn true if we have initialized the C++ bindings and the\n\tPython bindings.\n\n\tIf PyPy is installed, we assume we've initialised PyPy for the\n\tenvironment setup. If we"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\treturn \"ON_PYPY\" in sys.exec_prefix\n\ndef _maybe_c_impl_optimization(op, name):\n\tif name in op._optimizations:\n\t\treturn op.get_var(name)\n\tvar = getattr(op, name)\n\tif \"__c__\" in var.type_signature:\n\t\treturn True\n\t# Since it's a named variable, and we've already checked at least\n\t# it's an optimization, this must be true.\n\treturn op.__c__[name]\n\ndef _c_impl_optimization"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\tif not _use_c_impl:\n\t\treturn True\n\n\tif _PYTHON_MAJOR_VERSION < 3 and _HAS_SETPYFLAGS:\n\t\t# On py3, we use setuptools as the main setuptools\n\t\t# executable even though it doesn't use pure python.\n\t\treturn True\n\n\t# We're on PyPy: let's see if the C options work, otherwise we'd fail\n\t# the installation.\n\tis_pure_python_v3 = (sys.platform.startswith(\"linux\") and\n\t\tsys.platform"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110e", "prompt": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\treturn _c_optimizations_required[:]\n\ndef _c_default_optimize():\t\"\"\"\n\tTakes a method name and parses it.\n\tReturns an array with the resulting optimization steps.\"\"\"\n\tfrom._optimizeoptparse import OptimizeOptparse\n\tname = _get_call(OptimizeOptparse, 'name', [method_name])\n\tif name in _c_optimized_methods:\n\t\treturn _c_optimized_methods[name]\n\tresult = _c_optimizations_required[:]\n\tresult.insert(0, OptimizeOptparse(name="}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\treturn (dict(zip(c_optimizations_ignored, _c_optimizations_nonignored)),\n\t        dict(zip(c_optimizations_ignored, _c_optimizations_permissive)))\n\nclass _c_mod_not_needed(_mod_not_used):\n\t\"\"\"\n\tC compiler doesn't make much sense but just in case someone is\n\tphewing to remove C.  If this is an issue, be patient.\n\t\"\"\"\n\t# @TODO: Remove this if it is necessary.\n\tc_options ="}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\tr = []\n\tfor p in list(self):\n\t\tif p.is_set():\n\t\t\tr.insert(0, p)\n\treturn r\n\ndef _optimizations(g_true, g_false):\n\tr = g_false._opt.optimizations\n\tif r not in g_true._opt.optimizations:\n\t\t# There are optimizations with the same inputs: we allow optimizations to\n\t\t# take effect.\n\t\tfor p in list(g_true._opt.optimizations):\n\t\t\tg_true."}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\treturn False\t\t\t\t# don't change the function!\n\n\ndef _c_optimizations_required():\n\t\"\"\"\n\tDetermine whether we should also consider optimizing this source.\n\t\"\"\"\n\treturn False\t\t\t# the default.\n\n\n_c_optimize_modules = _c_optimize_modules_defined\t\t# define_optimize_modules()\n\ndef _c_optimize_defines():\t\t\t# define_optimize_defines()\n\tfrom c2d.optimizer.optimization import define_optimize_defs\n\tdefine_optimize_defs()\t\t\t# define_"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from types import FunctionType  \n#-----------------------------------------------------------------------------------------------------------------------\n#\n#\tC++ program for getting the names of all functions/classes/constants\n#\n#\tCopyright (C) 2016-2020,2021 by Ryan Nordie\n#\n#\tThis program is free software: you can redistribute it and/or modify it under the terms of \n#\tthe GNU General Public License as published by the Free Software Foundation, either version 3\n#\tof the License, or (at your option) any later version.\n#\n#\tThis program is"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\treturn 0\n\n\ndef _c_optimization_skipped(name, skip_reason) -> Optional[bool]:\n\t\"\"\"\n\t\tTrue iff the object is skipped by _c_optimizations; otherwise, False,\n\t\tbut return a value set by `_c_optimizations_included`\n\t\"\"\"\n\treturn None\n\n\ndef _c_optimization_included(name, source, *args) -> None:\n\t\"\"\"\n\t\tTrue iff the object has reached this point in time.\n\t\tIn addition to the result of the function, the previous optimization\n\t\tis"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\treturn [x for x in self.c_optimizations._c if x not in self.ignored_c_vars]\n\ndef _get_c_optimizations_by_func(func):\n\treturn _C_OPTIMIZATIONS[func.lower()]\n\ndef _get_optimizer_info(optimizer, func_name):\n\t\"\"\"\n\tReturns the top level optimization info for the given optimizer\n\tfunc_name:\t\toptimizer to use - can be an exact name, or a sequence\n\t\t\t\tof the optimizer types to find for func_name\n\tReturns:\n\t"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t#\n\t# First, we have to consider whether or not we can determine which of\n\t# the optimizations we consider.\tWe do not attempt to determine which\n\t# are supported for a given version, but still can determine which are\n\t# allowed and where in the future we would attempt to find a better\n\t# implementation.\n\t#\n\t# To determine how to find out why there is one version that can be used\n\t# by optimizes, we can use _c_setmsg as we will usually try to do the\n\t\t# following, it turns out that _c"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\treturn _c__get__c_optimizations_ignored()\n\ndef _c_requires_no_config():\t\t\"\"\"\n\tReturn true if this function did not return true.\n\tIn particular:  return true is not the reason for the compiler\n\tswitch, because it would break the optimizer:\n\n\t# c_opt is a function. It can return True to continue and False to\n\t# stop the optimizer\n\treturn lambda obj: obj.check(obj, no_config=True); obj.return_if_exists = False;\n\t\"\"\"\n\treturn _c"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\tdef _c_optimizations_ignored(self, optimizer):\n\t\t\"\"\"\n\t\tThe opposite of `_c_optimizations_required`.\n\n\t\tIf the optimizer is ignored, then you must also change the definition of _c_optimizations_supported(), and be aware that\n\t\t_optimizer_opt_dict must have the changed values. To change the behavior after this change is done, the caller must\n\t\tcall `optimizer.optimize(**_opt)`; the caller must check this value before using this method.\n\t\t\"\"\"\n\t\t# First check if _c_"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110d", "prompt": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn os.environ.get('PURE_PYTHON', None) == 'on'\n\nop = _c_impl\nop2 = _c_impl\n\n# These are not reified (except when -O2) - they're defined in `util/util.py`\n# and used by the python interpreter; they're defined outside any python\n# module (via `py`).\n\nuse_c_calls = _get_use_c_calls()\n\nif not use_c_calls or len(use_c_calls)!= 1:\n    _warn('"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn True\n\n\n##################################\n#  _c_def_opt_values()                                  #\n##################################\n\n\ndef _c_def_opt_values(c_opt_vals):\n\t\"\"\"Perform C optimizations for constants and values.\n\tCalled when the C preprocessor is opened or closed.  This\n\tobject handles all option values and the internal definitions\n\tof various C optimizations.\n\t\"\"\"\n\t# c_"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t# This variable is not required, but required if the optimizer is not\n\t# pure python.\n\treturn _c_optimize( _USE_PURE_PYTHON )\n\n\n_c_impl_only = \"\"\"\nThis C implementation only supports optimizing `__name__` and `__doc__`\nfunctions.\n\nIf a function is explicitly declared as an \"implementation function\", it must return\n``\"implementation support function\"``, which it calls in this order.  This may\nhappen, but the user may remove the function from `def __c_function__(\n...)` afterward"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn _use_c_impl(\"optimizations/c_optimized\")\n\n# This is an Implemical question in exploring the code aimed from C\n# https://github.com/cinder/cinder/issues/7\n# The goal is, I know, the best way to get an idea what would\n# actually happen if you used the\n# ``@cinder/backends/cinder/util/cinder.c`` as a module rather than as import_from_name.\nC = \"C\"\n\n_ = __import__('_c_optimizations"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn _set_c_optimizations_required()\n\ndef _c_impls():\t\t\t\"\"\"Returns a list of C interface functions\"\"\"\n\treturn [c_void]\n\ndef _c_decl_types():\t\t\"\"\"Returns information about C interface types\"\"\"\n\treturn [(c_void *).__doc__ for c_void in interfaces._c_impls()]\n\ndef _use_c_impl():\t\t\"\"\"Given a pointer to an C implementation struct,\nreturns the Python equivalent interface information for the C implementation.\n\nThis function is called at the beginning of an interface"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn hasattr(sys.modules.pop(\"_c_impl\"), \"C_IMPLFUNCTIONS\")\n\ndef c_lib_required():\t\"\"\"\n\tReturn a true value if the C library is required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn HAS_C_LIBRARIES or IS_PY_SAFE_C()\n\ndef c_impl_required():\t\"\"\"\n\tReturn a true value if the C implementation is skipped.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn _USE_C_IMPL\n\n\nif hasattr(sys,'real_prefix'):\n\tdef real_prefix():\n\t\t\"\"\"\n\t\tReturn the real (root) prefix.\n\t\t\"\"\"\n\t\treturn __real_prefix__, __real_prefix__\n\n\tdef real_prefix():\n\t\t\"\"\"\n\t\tReturn the real (root) prefix.\n\t\t\"\"\"\n\t\treturn __real_prefix__, __real_prefix__\n\treal_prefix = property(__real_prefix__, _real_prefix, doc=\"\"\"\n\t\tThe real prefix associated"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn _pure_py_c_impl() and _c_impl_required()\n\n\ndef _pure_py_c_impl():\t\t\t\"\"\"Pure Python version, without optimization.\n\nIt will do nothing when the C version is less than the Py_version.\nIf this fails, we check if we are on the py version of Python, and then\ndo a number of optimization passes.\n\"\"\"\n\tif hasattr(_set_c_optimization_dir_func, 'optimize_for_pypy') \\\n\t\tand _set_c_optimization_dir_func."}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn _c_improperly_required if _USE_C_IMPROPERLY and no_c_code_or_module\nelse:\n\t\"\"\"\n\tReturn a Python boolean value, which may contain false or \"0\" (i.e. \"no\" or\n\t      \"no_c_code\" to indicate that there is no C code or module).\n\t\"\"\"\n\treturn b_any_true if _c_impl_required(0) else 1\n\ndef _c_impls_required(c_impls_needed=0):"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\treturn __c_opt_impdefs.pure_python\n\n# This returns a list of the various optimizations that we want to use.\n__c_optimizations_not = _c_optimizations_all(pure_python=False)\n__c_optimizations_imp = [\n\t(\n\t\t\"float_div\",\n\t\t\"float_mod\",\n\t\t\"int_div\",\n\t\t\"int_mod\",\n\t),\n\t(\"int_div\", \"int_mod\", \"int_mul\"),\n\t(\"int_mod\", \"int_mod"}]}
{"task_id": "62b87b989a0c4fa8b80b35ee", "prompt": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tfrom IPython.core.basic import _is_none_or_none\n\tif _is_none_or_none(self.initial_value) == False:\n\t\tself.final_value = []\n\telse:\n\t\tself.final_value = []\n\tself.current_counter = 0\n\tself.next_counter = 0\n\tself.last_counter = self.max_count\n\ndef reset_histogram(bins):\n\t\"\"\"Add all the histogram bins to the given list.\n\n\tReturns the number of bins that has been added."}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tif self.ndim == 2:\n\t\tself.bin_size = 0\n\treturn\n\n\tself.clear_bins()\n\tself.data.zeros(self.shape[1],\n\t\t\t\t\t self.bin_size,\n\t\t\t\t\t dtype=self.dtype, sort=True)\n\n\tself.data.cumsum()\n\n\ndef apply_fn(fn, *args, **kwargs):\n\t'''Return the same data as :obj:`Function.apply`.\n\n\tArgs:\n\t\tfn:        function to be applied to data as"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tfor _, binclass in self.bins.items():\n\t\tbinclass.reset(self)\n\n\ndef bins_from_datafiles(datafiles, nodata=0.0):\n\t\"\"\"Return the histogram for a set of datafiles.\n\n\tReturns a dictionary of histograms for a set of images\n\t* datafiles*.\n\n\tThe *datafiles* should be the name of an original\n\tdataset. This utility takes an optional argument of\n\t:func:`import_datafiles`.\n\n\tThe ``ndot`` argument controls the different parts of the\n\t"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tcontext = self.context\n\tself.bins = {}\n\tfor d in self.get_histogram_data():\n\t\tcontext.remove(d)\n\tfor d, c in iteritems(self.bins):\n\t\tif (c in context for c in reset_histogram) and reset_histogram[c] == None:\n\t\t\tif not c in (key for key, value in context):\n\t\t\t\tcontext[c] = {'index': {0, 1}, 'bins': []}\n\t\t\t#add the d'th histogram to the set\n\t\t\tcontext["}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tself.reset_bin()\n\n#-----------------------------------------------------------------------------\n# Private API\n#-----------------------------------------------------------------------------\n\n\n#---------------------------------------------------------------------------------\n# Private methods\n#---------------------------------------------------------------------------------\n\n\ndef make_bins():\n\t\"\"\"Return a generator for the bins of the histogram.\n\n\tThe ``bins_is_set`` parameter is set to True at instantiation, but does not yield a\n\tparticular sample or histogram.\n\t\"\"\"\n\tfrom IPython.display import set_appname\n\timport ipdb; set_appname('-' + ipdb.__version__)\n\t_get_"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tself.values_ = DEFAULT_VALUES\n\tself.bins_ = DEFAULT_BITS\n\tfor bin_iter in range(6):\n\t\tself.bin(bin_iter, self.bins_, *DEFAULT_MASK, *DEFAULT_MASK)\n\n\tif self.bins_!= DEFAULT_BITS:\n\t\traise RuntimeError('At least 6 bits required for bin selection!')\n\n\treturn self\n\n\nclass Histogram(object):\n\t\"\"\"This object is designed to represent a histogram\n\n\tWhen called with 1 argument:\n\t\thistogram(start -end)\n\n\t"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t# Initialize the first histogram with zero entries.\n\tif self.first_histo[:] is None:\n\t\tn = len(self.table)\n\t\tfor h, i in self.histo_bins().items():\n\t\t\tself.start[h] = 0\n\t\t\tself.start[h] += i\n\t\t\tn -= 1\n\t\t\tif h >= self.last_row:\n\t\t\t\tself.last_row = n\n\t\tself.last_col = len(self.table)\n\t\tif n == 0:\n\t\t\t# Do"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\tself._histogram = self.histogram(initial_value=self.histogram_init,\n\t\t\t\t\tmake_bins=self.make_bins(),\n\t\t\t\t\tprint_stats=self.print_stats)\n\n# -----------------------------------------------------------------------------\n\ndef histogram(*args):\t\"\"\"\nHistogram\n\nA series of bins with the same mean value will be histogrammed together.\nThese bins can be chosen arbitrarily using *set_value*. For a more simple\nsampling, please note:\n\n``histogram(mean=0, unit='D', *args)`` will give the histogram of all\nhistograms generated"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\traise NotImplementedError()\n\n\n#--private module routines--\n\ndef histogramify(histogram_type, *args, **kwargs):\n\t\"\"\"\n\n\t\tUsage\n\n\t\t``histogramify`` returns a histogram (with its corresponding\n\t\tfrequency) from its data array.\n\n\t\tThe returned histogram is a dict with keys ``array``\n\t\tas keys ``data`` with values from the array.\n\t\tThe method returns the value of ``array`` as a value in\n\t\tany valid value of the *array*. ``array`` is then\n\t\treturned as the"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\treturn dict(self.values, self.bins)\n\n\ndef _get_bin(bin_name, value_name, min_value, max_value,\n\t\tunit_name=\"s\", histogram_type=\"greedy\") \\\n\t\t:\n\told_value, val = -1, +1.0\n\tif unit_name == \"s\" and histogram_type == \"greedy\":\n\t\t# GH#12643, GH#14359: When histogram is greedy, the unit size\n\t\t# may differ, but with s so that we can"}]}
{"task_id": "62b87b859a0c4fa8b80b35d7", "prompt": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t# if line contains multiple comma, we are at column 2\n\tif separator:\n\t\treturn self.__class__(\n\t\t\t\t\t\t[tuple([tuple([self._p.x, self._p.y]) for _ in tuple(list(zip(str(d), str(self._p)) for d in xrange(len(x), len(self._p))))])]\n\t\t\t\t\t\t+ str(x)\n\t\t\t\t\t\t+ separator + ['%s.%s' % (field, repr(x)) for field in self.fields]), sep=',')\n\treturn self.__class__(\n\t\t\t\t\t"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tif separator is not None:\n\t\tself.separator = separator + \",\"\n\telse:\n\t\tself.separator = \";\"\n\n\tif header is None:\n\t\theader = \",\" + self.separator + \"x\"\n\n\tline = \" -> \".join(a.point + a.value + a.data + a.label)\n\tself.to_csv(line)\n\n\tline = \"        -> \".join(a.point + a.value + a.data)\n\tself.to_csv(line)\n\n"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tgraph = lena.graph.from_csv(self.string, header)\n\tif self.graph is None:\n\t\tassert hasattr(graph, 'header_line')\n\t\t# convert and output graph of the converted graph\n\t\tnew_graph = convert_graph(graph, is_graph=True)\n\t\tgraph = new_graph\t\t\n\telse:\n\t\tgraph = graph.as_graph()\n\tgraph = Graph.from_table(graph.as_table(), is_graph=True)\n\n\t# Convert the graph to a sequence of column names"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tfrom numpy.distutils.core import Extension\n\n\t# convert string to csv\n\tif not isinstance(separator, int):\t\t\tsep = \", \"\n\telse: \t\t\t\t\t\t\tsep = \",\"\n\n\tfor val in self.values():\n\t\tcsv.writer.writerow(val, sep=sep, header=header)\n\t\t\t# if we just write another record, make sure the user doesn't\n\t\t\t# do it all at once\n\tcsv.writer.writerow([])\n\n@attr(list)\n@attr(type)\nclass Graph(object):\n\t"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tif hasattr(self, 'as_dict'):\t\t\t# no csv()\n\t\treturn self.as_dict()\n\tif not isinstance(self, Graph) or not self._is_multidimized: \n\t\treturn self.__dict__['__as_matrix__']\n\tif header is None:\n\t\theader = ''\n\treturn [self, self.__class__, header] + to_csv(self,\n\t\tseparator=separator,\n    \t\t\theader=header)\n\n@deprecated('0.5.3')\ndef to_csvwithheader"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tgraph = Graph()\n\tgraph.load_from_csv(self.get_header())\n\tcsv.writer(\"to_csv.csv\",sep=separator,\n\t\toutput=graph)\n\n\ndef split_points(points):\n\t\"\"\" Split a list of points into ``points`` by a ``x`` and ``y``\n\tcoordinate.\n\t\"\"\"\n\tif len(points)==1:\n\t\treturn (points[0],-1)\n\treturn np.split(points,2,axis=0)\n\ndef get_head_lines(points,"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\treturn self.ToCSV(*separator, **{\n\t    \"csv_header\": self.header,\n\t    \"csv_data\": self.data\n\t  }, separator=separator, header=header)\n\ndef to_csv_with_header(self, separator=\",\", header=None):\t\"\"\"\n   .. deprecated:: 0.5 in Lena 0.5 to_csv_with_header is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV with header.\n\n*separator* delimits values,"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tif ',' in separator:\n\t\tseparator = separator.split(',')\n\tdefault = separator # default if specified\n\tif separator:\n\t\tif isinstance(separator[0], dict):\n\t\t\tseparators = sep_to_graph(separator[0])\n\t\t\tdef separator_to_csv(d, *c, **s):\n\t\t\t\tfor p in separators:\n\t\t\t\t\tif ',' not in *c:\n\t\t\t\t\t\td.write(',')\n\t\t\t\t\t\tif len(s) > 0:\n\t\t\t\t\t\t\tfor _, v in s.items():\n\t\t\t\t\t\t\t\tif"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tif separator is None: separator = \";\"\n\tfor i in range(len(self)):\n\t\tirow = self[i]\n\t\tif irow.is_coord:\t\t\t# for each row add a coordinate string,\n\t\t\t\t\t\t\t\t\t# and a separador line\n\t\t\tout.append(i, separator)\n\t\telse:\n\t\t\tout.append(i, str(irow.coord))\n\n\ndef read_csv(dirname, delimiter=':', dtype=float, header=None):\n\t\"\"\"Read lines from the CSV file\n\n\t"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\tif header is None:\n\t\theader = []\n\tassert isinstance(separator, (str, types.UnicodeTypes))\n\tassert separator and separator in header\n\theader.extend([separator] * (len(separator) - len(separator)))\n\tfor point in self.points:\n\t\tpoint.csv_to_csv_header(header)\n\n\n\n#===============================================================================\n# Write data to TarStan\n\nclass TarStan(object):\n\tdef __init__(self, data):\n\t\tself.data = data\n\t\tself.version ="}]}
{"task_id": "62b87b839a0c4fa8b80b35cb", "prompt": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tif coord_name in self.errs:\n\t\treturn self.errs[coord_name]\n\treturn []\n\n\nclass Coordinate(object):\n\t \"\"\"\n\t Base class for coordinate classes. Must be sub-classed so that\n\t you can take any of these classes from the same module.\n\t \"\"\"\n\n\tdef __init__(self):\n\t\tpass\n\t\traise ValueError(\"coordinates not subclass\")\n\n\tdef __getitem__(self, coordinate_name):\n\t\treturn self.__get__(coordinate_name)\n\tdef __delitem__(self, coordinate_"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tif self.coord_name in self.err_indices:\n\t\treturn self.err_indices[self.coord_name]\n\telse:\n\t\t#self.err_indices[coord_name] = i\n\t\tself.err_indices[coord_name] = 0\n\t\treturn self.err_indices[self.coord_name]\n\ndef _print_coord(self, coord_name, filename, coord_id):\n\t#print\n\tself.err_indices[coord_name] |= coord_id\n\ndef _print_indices("}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\treturn self._index[coord_name]\n\ndef _get_info(self, coordinates, coord_name):\n\treturn {'coord': coordinates[coord_name],\n\t\t'coord_def': coordinates[coord_name].def,\n\t\t\"coord_names\": {coord_name: {\n\t\t\t\"coord\", \"coord_def\", \"coordinate\", \"coord_names\"}}}\n\n####################\n\ndef _get_coord_info(self, coordinates):\n\tif os.path.isabs(coordinates):\n\t\treturn [_get_info(coordinates,"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tif (coord_name):\n\t\terror_indices = self.errors_to_names[coord_name]\n\telse:\n\t\terror_indices = self.errors_to_names.keys()\n\n\tindex = self.error_index_map\n\tif index:\n\t\treturn index[error_indices]\n\telse: \n\t\treturn [error_indices]\n\n\ndef _get_max_size(arr):\n\t\"\"\"\n\tGet maximum size (in bytes) of values in an array.  \n\tReturn None if size not specified.\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tcoord = self._get_coord_from_name(coord_name)\n\tif coord is None:\n\t\treturn\n\n\tcoord_coord = coord._get_coord_idx(coord.coord)\n\tif coord_coord is not None:\n\t\treturn self._get_coord_indices(coord_coord)\n\traise KeyError\n\n\tself._error_to_indices[coord] = []  # type: List[int]\n\n\ndef _get_coord_indices(coord, coordinate_type=None, coords=None):\n\t\"\"\"\n\tGet"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tn_x = len(self.coord_names)\n\tn_y = len(self.coord_names[0])\n\t# Make sure our coordinate format matches any of the\n\t# error indices\n\tif isinstance(coord_name, tuple):\n\t\tcoord = coord_name[0]\n\t\tif len(coord_name)!= 2:\n\t\t\traise ValueError(\"Can't handle Coordinate with format \"\n\t\t\t\t\t\t\t\"['%s','%s'] format, \"\n\t\t\t\t\t\t\t'expected 2 or '%s'\")\n\t\treturn get_coord_error_indices"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t# NOTE: The actual return value is not necessary since multiple\n\t# return values are not supported on C++03 and C++11.\n\t# If you have to make a difference here, or change the return values,\n\t# you need to re-enable the -w, -f, and -W compilation flags.\n\n\ti = 0\n\tfor d in list(self._dataset.errors):\n\t\tif coord_name == d.name:\n\t\t\tif d.level == self.errors_level and d.level == self._levels:\n\t\t\t\treturn i\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\treturn [coord_name]\n\ndef _get_axis_name(coordinate_id, axis_idx):\n\treturn \"X\" + str(coordinate_id) + str(axis_idx)\n\ndef _get_axis_info(coordinate_id, axis_idx_map):\n\treturn [axis_name for axis_name, axis_id_list in sorted(axis_idx_map.items())]\n\ndef _get_coord_error_indices(coord_id_indices, coordinates):\n\tcoords = np.zeros([len(coordinates)])\n\tcoord"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tif coord_name in ['gnd', 'gnd', 'gn']:\n\t\treturn ((1, 1),) * self.max_ind_x\n\telif coord_name in ['bod', 'bod', 'bod']:\n\t\treturn ((2, 1), (3, 1))\n\telif coord_name in ['cup', 'cup', 'cup']:\n\t\treturn ((3, 3),)\n\treturn ([],)\n\n\nclass PointIndex:  # pylint: disable=too-few-public-methods\n\t\"\"\"\n\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\tif coord_name in self._err_indices: return self._err_indices[coord_name]\n\telse: return range(self._NUM_COORD_ERRS)\n\ndef _make_err_indices(coord_type, coord_name):\t\"\"\"\n\tMake a set of error indices corresponding to a coordinate.\t\"\"\"\n\tindices = []\n\tif coord_type is None: return\n\telif coord_type==self.ERR_POINT:\n\t\tif coord_name not in self._err_indices:\n\t\t\traise RuntimeError(\"Can't find coord in error"}]}
{"task_id": "62b87b7e9a0c4fa8b80b35bc", "prompt": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tdel context.key, context.value, self._key, self._label\n\tcontext.key_name = \"_key\"\n\tcontext.value_name = \"_value\"\n\n\tcontext.error_E_low = [None, {}]\n\tcontext.error = {}\n\n\ndef _is_string(x):\n\t\"\"\"Return True if the input is a text string.\"\"\"\n\tif isinstance(x, six.string_types) and not x.strip():\n\t\treturn False\n\treturn True\n\n\ndef _update_context_string(flow_graph,"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tcontext = pylearn2.GraphFromContext(self._context,\n\t\t\"error\")\n\n\tassert context.error == default_error, context.error\n\n\tfor (y, x) in self._flow.values:\n\t\tcontext.set_value(y, x, flowinfo=False)\n\tcontext.update()\n\n\tif context.value and len(context.errors):\n\t\tcontext.value = default_value(self._context)\n\t\tcontext.errors.update(context.errors)\n\tfor node in self._flow.graph"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tfor field in self.schema.find_all(self._key(self.domain)):\n\t\tfor value in field.get_fields(self.domain):\n\t\t\tcontext[field] = field.get_fields(self.domain)[value]\n\ndef _update_element_context(context):\t\"\"\"\n\tUpdate *context* with the elements of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,E_low\":\n{\"E\": {\"x\": {\"index\": 2}}}.\nNote that"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tself._values[context.name] = context.value\n\tcontext.index.remove(context.name)\n\t# Delete field 'id (field_code: index)'\n\tcontext.name, context.index = ('None', self._name_to_index.get(context.name))\n\tcontext.field_code = None\n\t# Add new values at end of the fields\n\ttry:\n\t\tfor p, v in context.value.items():\n\t\t\tfield = context.fields.get(p, None)\n\t\t\tif field is not None:\n"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tfrom matplotlib import pyplot as plt\n\n\t# Update graph with fields\n\tfor field in self.fields_with_data:\n\t\tcontext['fields'].append(field)\n\n\t# Update values of fields_with_data\n\tfor i in range(len(context['fields'])):\n\t\tcontext['values'][i].update(context['fields'][i])  # Set values\n\n\tplt.grid()\n\tplt.xlabel('x')\n\tax_ = np.arange(0, context['width'] + 1, 0.1)\n\tplt"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tif context.value is None:\n\t\terr_msg = str(context) + \" is not initialized\"\n\t\traise ValueError(err_msg)\n\tif context.error is None:\n\t\terr_msg = str(context) + \" is not initialized\"\n\t\traise ValueError(err_msg)\n\n\tcontext.state.update(context)\n\n\ndef _init_ctx(ctx):\t\t\"\"\"Load all the values\"\"\"\n\tfor keys, values in ctx.state.items():\n\t\tif key in keys: _copy(values, keys,"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tfor field in self.to_db(context.field):\n\t\tctx = context.field[field]\n\t\tfield.set_initial(None if self.auto_increment else int(ctx[0]*100))\n\t\t# Add error names as error_E_low to context dictionary entries.\n\t\tfield.insert_error_names(context.errors)\t# XXX Do we need the full list here?\t\n\t\n\tif context.type == \"to_csv\":\n\t\tcontext.value_rows = []\n\ndef _update_context2(self,"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\treturn _update_context(context,self.keyframes(self.fields))\n\ndef _sort_keyframes(keys, sort_field):\t\t\t\"\"\"\n\tCollects the keyframes in an array of keys.\t\"\"\"\n\tresult = []\t\t\t\t\t\t\t\t\t\t#< List of keys\n\tidxs = [0]*len(keys)\n\tkeyframes = list(filter(lambda x: x!=sort_field, keys))\t#< List of keyframes\n\tfor i in idxs: k=keyframes[-i-1]\n\tresult.append(keys[k-"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tcontext.value.destruction = True\n\tcontext.name = \"d\"\n\tcontext.set(\"E value: T=6E(7E6)\" )\n\ndef test_basic(graph):\n\tgraph.add_property(\"E error_E_low\", None)\n\tgraph.add_property(\"error[index]\", None)\n\tgraph.add_property(\"value[index]\", None)\n\tgraph.read()\n\nclass TestRead(test.TestCase):\n\t\"\"\"\n\tTest read functionality.\n\t\"\"\"\n\n#-----------------------------------------------------------------------------\n# Dev API\n"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\tgraph = self.garbage['graph']\n\tfor propname, index in context.iteritems():\n\t\tgraph.update({key[1:] + propname: self._parse_value([\n\t\t\t\t\tval,\n\t\t\t\t\tid(val, index)\n\t\t\t\t])}\n\t\tcontext[propname] = context[propname][0]\n\tgraph.update({propname: context})\n\n\tgraph.update('E', 'x_low', 'graph.error.0.x', 'graph.error.0.y')\n\tgraph.update('E',"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3580", "prompt": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\timport numpy as np\n\tfrom. import iridium\n\t\n\t# find an interpolating sample for the integral\n\tn = len(bins)\n\tif n == 0:\n\t\tbins = np.linspace(bins[1,1], bins[1.5,1], n+1)\n\t\n\th = None\n\tif n > 1:\n\t\ti = iter(bins)\n\t\ttry:\n\t\t\twhile i.next():\n\t\t\t\th = h*np.exp(i.get_value())+1\n\t\texcept StopIteration"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\tglobal _max_bins\n\treturn max((histogram(bins, edges, _max_bins, scale=True) + 1), 1)\n\n@docstring.dedent_group\ndef logarithmic(bins, edges, lower=0, upper=10):\t|\n\n\tlog_threshold = np.log(10)\n\tlog_threshold /= (bins + 5)\n\tlog_threshold *= bins + 1\n\tcutoff = lower + log_threshold\n\tcutoff = np.clip(cutoff, -4, 4)\n\tlog_threshold = (0.7*log"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\tbins = int(bins)\n\tedges = int(edges)\n\tx = np.arange(bins + 1)\n\ty = x\n\tif edges > 0:\n\t\ta = edges - 1\n\t\tb = edges\n\t\tx[0] = b\n\t\ty[0] = a\n\t\tfor i in xrange(bins + 1):\n\t\t\ty[i + 1] = 0.5*b + (a - b)/a*i\n\treturn y + 1\n\n@lru_cache(maxsize=256)\ndef bin_"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\treturn HistogramWithIntegral(bins, edges)\n\n\ndef uniform(bins, edges):\t\"\"\"\n\tCompute uniform (scale for x and y distribution).\n\n*bins* have values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\treturn HistogramWithUniform(bins, edges)\n\n\n# -----------------------------------------------------------------------------\n#                                        "}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t#   # Make the function input type consistent so we don't do anything on \n\t#   # the integral call later and save it.\n\tif _is_integral_type(bins, edges):\n\t\tbins = [int(bins)]\n\telse:\n\t\t# Make sure it is an integer (so it's possible to have int).\n\t\t# It should have one of the following conditions:\n\t\t# * The data has the right type. If so, it's an int.\n\t\t# * No sign was given.\n\t\tbins = [int"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\"\"\"\n\t\tbins:       (len) integer numbers, array like (len, 2). [0,1]\n\t\tedges:      (len) integer numbers, array like (len, 2).      \n\n\t\tif edges is not None, this array will be filled with edges for each \n\t\tpoint in the histogram.\n\t\"\"\"\n\tif edges is not None:\n\t\tif float(bins+bins+edges):\n\t\t\treturn [(edges,) ]\n\n\t# return empty arrays (see list in __"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\tN = edges[-1] - edges[0]\n\n\treturn integrate(\n\t\tnumpy.asarray(edges, dtype=numpy.float64),\n\t\tnumpy.asarray(numpy.int64(N), dtype=numpy.float64))\n\n/*-\n * SPDX-License-Identifier: BSD-2-Clause-FreeBSD\n *\n * Copyright (c) 2011 NetApp, Inc.\n * Copyright (c) 2019 Hartmut Kaiser\n * Copyright (c) 2021 Pat Cartius <patrick.core.spec@gmail.com>\n * Copyright"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t# find bin and edge indices\n\tbidx = int(0.5*bins), int(0.5*edges)\n\n\t# construct mesh\n\th = [ 0 for _ in bins ]\n\tM = numpy.empty((len(bins), 1))\n\tfor i in range(len(bins), len(edges)):\n\t\tx = numpy.arange((bins[i]-1)//2+1, bins[i]+1//2-1)\n\t\tr = numpy.histogram(x, bins=bins, edges=edges, weights=h, cmap ="}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from numpy import norm1D\n\ttry:\n\t\tif(abs(sum(bins[imi%len(bins)] for imi in range(len(bins)))) >2):\n\t\t\t# if we have only a part that is not an integral and we do not\n\t\t\t# check for a more positive histogram for a single integral,\n\t\t\t# it doesnt help, so skip it.\n\t\t\treturn 0,\n\t\t\tNone\n\t\ts = None\n\t\tfor i in range(len(bins)):\n\t\t\tif"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\tdef __call__(bins=None, edges=None):\n\t\t\"\"\"Compute a function for integral values.\n\n\t\tThe function returns a dictionary with the following keys being values,\n\t\tvalues are indices for which the value of the histogram could be.\t\"\"\"\n\t\tif bins is None:\n\t\t\tbins = get_bins()\n\t\tif edges is None:\n\t\t\tedges = (edges + [3, 4, 5, 6, 8])\n\t\treturn _ianc(bins, edges, \"integral\")\n\n\treturn __call__\n\ndef exponential_"}]}
{"task_id": "62b87b199a0c4fa8b80b354e", "prompt": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tif (len(seq) >= min_fill_request_len): return True\n\ttry:\n\t\tif (not isinstance(seq, SourceFillRequest)) or (len(seq) < min_fill_request_len): return False\n\texcept ValueError:\n\t\treturn False\n\treturn True\n\n\ndef is_fill_req_seq(seq):\t\"\"\"\n\tTest whether *seq* can be parsed into a FillReqSeq.\n\nTrue only if it is a FillReq element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\""}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tfor i in range(len(seq)):\n\t\tif 'fill_request' in seq[i].tag:\n\t\t\treturn True\n\treturn False\n\ndef is_fill_request_elem(elem):\n    \"\"\"Test whether *elem* is a type of FillRequestElem.\"\"\"\n    return ('fill_request' in elem.tag and\n            ('source' in elem.tag or'source' in elem.attrib or 'endof_source' in elem.attrib) and\n       "}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tif len(seq) == 1:\n\t\treturn False\n\tif isinstance(seq[0], SequenceNode) and not \\\n\t\t((isinstance(seq[0].elementType, (SourceNode, SequenceNode))) or \\\n\t\t((isinstance(seq[0].elementType, (SourceSeq, SequenceNode))))):\n\t\treturn False\n\treturn True\n\ndef _verify_source_nodes(seq):\t\"\"\"\n\tTest whether *seq* contains just source node type.\n\nAlso verifies that *seq* is enclosed in only one, such as just\nsources.\n"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tif 'FillRequestSeq' in seq:\t\t# source - only has one element\n\t\treturn True\n\telse:\t\t\t\t# source - elements of Seq are not Sequence\n\t\treturn False\n\ndef is_fill_request_empty(seq):\t# seq is either a FillRequest/Seq element (if seq == None) or a Source sequence.\n\tif 'FillRequestSeq' in seq:\t\t# source - only has one element\n\t\treturn True\n\telse:\t\t\t\t# source - elements of Seq are not Sequence\n\t\treturn False\n\ndef is_"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tif seq is None or not isinstance(seq, str):\n\t\treturn False\n\treturn in_list(seq, SequenceType.BLANK)\n\n#==============================================================================\n\ndef is_valid_element(elem):\n\t\"\"\"Checks an XML element to make sure it parses correctly.\"\"\"\n\tif len(elem) < 1:\n\t\treturn False\n\tif elem[0].type in (NameType, StringType):\n\t\treturn True\n\treturn False\n\n#==============================================================================\n\ndef is_blank_element(elem):\n\t\"\"\"Checks whether *elem* is a blank"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\treturn (seq[0] & 0xC0) == 0xC0\n\n################################################################################\n# Test to make sure that we don't try to mix of source and\n# destination-copying requests.  We just need to mix\n################################################################################\ndef check_for_different_requests(req1, req2):\n\t\"\"\"Check that requests are not different.\"\"\"\n\n\tdef test_same_response(r1, r2):\n\t\tif cupy.is_sequence(r1): r1 = [r1]\n\t\tr1 = cupy.sequence_"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tif not hasattr(seq, 'is_fill_request'):\n\t\treturn False\n\tif not isinstance(\n\t\tseq.body, (\n\t\t\t(FillRequest,\n\t\t\t Source,\n\t\t\tAnything,\n\t\t\tListElement,\n\t\t)\n\t):\n\t\treturn False\n\tif not seq.body:\t\t\t# Should only contain one element here or else the caller can't find a\n\t\t\t\t\t# FillRequest instance\n\t\tself.assertEqual(len(seq.body.targets), 1)\n\telse:\n\t\tself.assertEqual(len"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t# This is only a true case if the sequence has no elements\n\t# We keep it simple.\n\tif not len(seq) > 1:\n\t\treturn False\n\tfor el in seq[0]:\n\t\tif el.tag == 'FillRequest':\n\t\t\treturn True\n\treturn False\n\ndef is_fill_request(seq):\t\"\"\"\n\tCan a FillRequest-Element be converted to a FillRequestSeq?\n\nTrue only if it is a FillRequest Element\nor Source sequence.\t\"\"\"\n\treturn is_fill_request_seq(seq)\n"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tif not isinstance(seq, FillRequestSeq):\n\t\t# check we're not sending a Source or a List or a Null\n\t\t# sequence.\n\t\treturn False\n\tfor elem in seq:\n\t\tif elem is None:\n\t\t\t# Element has no children\n\t\t\treturn True\n\telse:\n\t\t# Element has no children\n\t\treturn False\n\n\nclass SequenceOfValue(SimpleValue):\n\t\"\"\"A Sequence of XMLValue.  Element is of type xml.dom.Node;\n\tvalue should be in this list.\n\t\"\"\"\n\t"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\tfor i in range(1,len(seq)):\n\t\tif not seq[i - 1] == seq.Name or not seq[i - 1] == seq.Type or \\\n\t\t\t(not SCHEMA.has_sub_sequence(seq.Value, seq[i - 1].Value) and\n\t\t\t(not SCHEMA.has_sub_sequence(seq.Value, seq[i - 1].SubValue))) or \\\n\t\t\t(not SCHEMA.has_sub_sequence(seq.Value, seq.ValueType) and \\\n\t\t\t(not"}]}
{"task_id": "62b87b099a0c4fa8b80b3538", "prompt": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\treturn(False)\n\ndef is_filter_el(obj):\t\"\"\"\n\tObject contains executable methods 'filter' and 'filter'.\t\"\"\"\n\treturn(False)\n\ndef is_get_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'get', 'get-request', 'get-parameters', and 'get-body'.\t\"\"\"\n\treturn(False)\n\ndef is_post_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'post' and 'post-request'.\t\"\"\"\n\treturn(False)\n\ndef is"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from sp_global_def import ERROR_EL\n\tsp_global_def.Warning(sp_global_def.ARGUMENT, \"This function is no longer available from now on.\" + \"Use it in future releases\")\n\n#line 615 \"sp_python.py\"\n\tv = sp_utilities.get_image(obj)\n\tfrom sp_alignment import pad_torgb, get_alignment_params\n\tfrom sp_fundamentals import rot_shift2D\n\tif       v.type"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tif is_callable(obj.fill):\n\t\treturn obj.fill.call(obj)\n\tif is_callable(obj.request):\n\t\treturn obj.request.call(obj)\n\telse:\n\t\traise TypeError('Expected a callable object for fill and request')\n\t\ndef is_callable(mod_name):\n\ttry:\n\t\tmod_name = mod_name.__name__\n\texcept AttributeError:\n\t\traise TypeError('Expected a string to be a callable object')\n\ttry:\n\t\treturn callable(mod_name)\n\t"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\treturn True\n\n# ================================================================== \n\nclass is_fill_request(type):\n\tdef __new__(cls, name, bases, attrs):\t# New class instance\n\t\t# Set property values\n\t\tself = super().__new__(cls, name,'')\n\t\tself.__obj_class = attrs.get('obj',None)\n\t\tself.__file_name = (attrs.get(\"name\") or \"\") + \".request-impl\"\n\t\tif self.__file_name == \"__cString__\":\n\t\t\tself.attr_names['__str__'] ="}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from EMAN2 import Filler\n\tpass#IMPORTIMPORTIMPORT from EMAN2 import Request\n\tif not EMAN2_cppwrap.Util.running_under_windows():\n\t\tprint(\"This method won't work from windows, skipping...\", file=sys.stderr)\n\t\treturn None\n\t# Try not to fill it, this may not return if it fails.  But that\n\t# is okay if we run in parallel.\n\tmethod_to_exec_list = {}\n\tfor im in obj.list_users"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tif hasattr(obj, 'fill') and obj.fill is not None:\n\t\tif 'object' not in obj:\n\t\t\treturn ''\n\t\tels = obj.fill\n\telif obj.execute!= \"yes\":\n\t\treturn ''\n\treturn'   fill is not available.'\ndef is_execute_command(exe_name):\n\treturn'   Executed command:'in exe_name\ndef is_text_command(exe_name):\n\treturn'   Executed text command:'in exe_name\n\ndef is"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\treturn issubclass(obj.meow_cls, BoshShell)\n\ndef is_load_method(obj):\n\treturn issubclass(obj.meow_cls, BoshShell)\n\ndef is_create_method(obj):\t\"\"\"\tIs Bosh-Shell create() method?\t\"\"\"\n\treturn issubclass(obj.meow_cls, BoshShell)\n\ndef is_drop_method(obj):\n\t\"\"\"\tIs Bosh-Shell drop() method?\t\"\"\"\n\treturn issubclass(obj.meow_cls, BoshShell)"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t# The definition of these are in 'fill' and'request'.\n\t# Note that we don't include'request' in the definition\n\t# of'request' because the objects referenced by 'fill'\n\t# will not be included in 'fillRequest' in the code.\n\treturn\n\n\ndef is_fill_type(obj):\t\"\"\"\n\tObject contains executable methods 'fill'.\t\"\"\"\n\t# The definition of these are in 'fill'.\n\treturn\n\n\ndef is_erase(obj):\t\"\"\"\n\tObject contains executable methods 'erase' and'request'."}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tpass\n\n\ndef get_max_request_object(obj):\t\"\"\"\n\tReturns a maximum object for an object'max' in an'request' object.\t\"\"\"\n\tpass\n\n\ndef get_max_execute_object(obj):\t\"\"\"\n\tReturns a maximum object for an object'max' in an 'execute' object.\t\"\"\"\n\tpass\n\n\ndef get_max_execute_time(obj):\t\"\"\"\n\tReturns a maximum time for'max' object 'execute' in an'request' object.\t\"\"\"\n\tpass\n\n\ndef get"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\tif issubclass(obj, Request) is True:\n\t\treturn False\n\treturn True\n\ndef is_request_request(obj):\t\"\"\"\n\tObject contains executable methods'request' and'request-xml'.\t\"\"\"\n\tif issubclass(obj, Request) is True:\n\t\treturn True\n\treturn False\n\ndef is_request_xml(obj): \t\t\"\"\"\n\tObject contains executable methods'request' or'request-xml'.\t\"\"\"\n\tif issubclass(obj, Request) is True:\t\t# xml request?\n\t\treturn False\n"}]}
{"task_id": "62b87af99a0c4fa8b80b3524", "prompt": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from warnings import warn\n\timport sys\n\tsys.modules[\"warnings\"].setdefault(\"error\", \"Module '%s' is not available\" % type(obj).__name__)\n\treturn 1\n\n#if __name__ == \"__main__\":\t\t\t\t#python: __init__\n\timport sys\n\timport tempfile\n\tfrom pypeit import app\n\timport pypeit.constants\n\tsys.path.append(tempfile.mkdtemp())\n\n\tapp = pypeit.App(sys.argv)\n\tif(app.argc"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\tif 'run' not in obj:\n\t\traise ValueError(\"{0} is not a executable method.\".format(type(obj)))\n\n\ndef exec_el(obj, *args, **attr):\n\t\"\"\"\n\tExecution method for executable functions from interpreter.\t\"\"\"\n\tif 'execute' in attr and hasattr(obj,'module'):\t# pragma: no cover\n\t\treturn exec_command_function(obj.module, attr.pop('execute'), *args, **attr)\n\telse:\n\t\t# If there are no attributes, there is nothing to do.\n"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\treturn obj['name'] == '__run'\n\n\ndef is_debug_el(obj):\t\"\"\"\n\tObject contains executable method 'debug'.\t\"\"\"\n\treturn obj['name'] == \"__debug\"\n\n\ndef is_verbose_el(obj):\t\"\"\"\n\tObject contains executable method'verbose'.\t\"\"\"\n\treturn obj['name'] == \"__verbose\"\n\n\ndef is_list_el(obj):\t\"\"\"\n\tObject contains executable method 'list'.\t\"\"\"\n\treturn obj['name'] == \"__list\"\n\n\ndef is_file_el(obj"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\tif isinstance(obj, types.MethodType):\tpass\n\tif sys.version_info.major >= 3 or __name__ == \"__main__\":\t # print()\n\t\ttry:\n\t\t\tsys.argv = ['el']+sys.argv\n\t\t\tsys.exit(__magic_func__)\n\t\texcept NameError:\n\t\t\tsys.argv = obj.args\n\t\t\tsys.exit(magic_func_fail)\n\telse:\n\t\t_run_el\t= run_el\n\t\t_test_el\t= test_el\n\t\t_import"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\tif obj is not None and type(obj) == type(None):\n\t\treturn None\n\n\tif obj is None:\n\t\treturn None\n\tif isinstance(obj, type(None)):\n\t\treturn None\n\treturn None\n\ndef convert_xml_to_python(object, xml_element):\n\t\"\"\"\n\tConverts given XML (xml_element) to a Python object,\n\tif xml_element is a list and has to be recursively converted to python.\n\t\"\"\"\n\tfor elem in xml_element:\n\t\tif isinstance(elem, type"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\treturn False\n\n\t\ndef is_run_all(obj):\t\"\"\"\n\tObject contains executable method 'runall'.\t\"\"\"\n\treturn False\n\n\t\ndef is_start_el(obj):\t\"\"\"\n\tObject contains executable method'start'.\t\"\"\"\n\treturn False\n\n\t\ndef is_stop_el(obj):\t\"\"\"\n\tObject contains executable method'stop'.\t\"\"\"\n\treturn False\n\n\t\ndef is_reset_el(obj):\t\"\"\"\n\tObject contains executable method'reset'.\t\"\"\"\n\treturn False\n\n\t\ndef"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\tif 'object' not in obj:\n\t\traise IOError('Expected object')\n\treturn getattr(obj['object'], 'run', None)\n\n@is_run_el(obj)\ndef get_exec_args(obj):\n\treturn obj['args']\n\n@is_run_el(obj)\ndef get_exit_status(obj):\n\treturn obj['status']\n\n@is_run_el(obj)\ndef get_returnstatus(obj):\n\treturn obj['return']\n\n@is_run_el(obj)\ndef"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\ttry:\n\t\treturn type(getattr(obj, \"__file__\", None))(obj)\n\texcept:\n\t\treturn False\n\n\ndef is_shell_ok(obj):  \"\"\"\n\tObject contains executable method 'isShellOk'.\t\"\"\"\n\ttry:\n\t\treturn type(getattr(obj, \"__file__\", None))(obj)\n\texcept:\n\t\treturn False\n\ndef load_path(module_name, funcname, file, line_num, func = None):\n\t\"\"\"\n\tLoad the path of the module'module_name' or 'func"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\treturn obj is getattr(sys, '_executable', False)\n\ndef is_run_cmd_file(exe):\t\"\"\"\n\tExecutes executable or file.\n\n\t\t\tIf not exists returns False.\n\n\tNotifies of this object has finished processing, and returns a bool value.\n\t\t\tMay also be set by'set_run_done(); if not set then nothing done.\n\n\tReturns is_run_done().\n\n\t\"\"\"\n\treturn not executable or is_file_system(exe)\n\ndef notify(obj):\t\"\"\"\n\tObject changed. "}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\tif type(obj) == type(''):\n\t\treturn False\n\treturn obj\n\neldefects = eldefects.copy()\n\ndef clean(obj):\n\t\"\"\"\n\tClean object\n\t\"\"\"\n\tif type(obj)==type(NoneType):\n\t\treturn False\n\telif type(obj) == type('None'):\n\t\treturn True\n\treturn eval(obj)\n\ndef get_class_type(c):\n\t\"\"\"Return class definition for type (or error) of object\"\"\"\n\tif c in eldefs:\n\t\tc"}]}
{"task_id": "62b87af69a0c4fa8b80b351a", "prompt": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from sp_utilities    import wrap_mpi_bcast, wrap_mpi_gather, preshift\n\tpass#IMPORTIMPORTIMPORT from EMAN2 import write_text_row\n\t\n\tif not obj.ok:  return 0\n\tmpi_comm, nbunch = get_mpi_procs()\n\tpartids= [i for i in range(len(partids)) if partids[i][procids[rank[proci].particle_index()]]]\n\tif not partids:   return  0\n"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\tcmd = 'python3.7-cygwin64_3.7-win32.exe'\n\t#print cmd, obj\n\t#args, out, err = run_python(cmd, shell=True, stdout=obj.stderr, stream=obj.stdout)\n\tif obj.argv:\n\t\t#if obj.argv[1]:  print obj.argv[1], \"\\n\"   #print len(obj.argv[1])\n\t\t#print 'args'\n\t\t#cmd, args, out, err = run_python(cmd, shell"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from sp_utilities import load_params\tdeform_source_and_parameters(filename)\n\tpass#IMPORTIMPORTIMPORT from sp_filter import filt_table\t\t\t#from sp_weka import kwe_filter,weka_filter\t#\n\t#deform_source(filename, filt_table(None), args.image_width, data=\"white\")\t\t#from sp_weka import kwe_filter(src_dict, out_dict, args.image_width, data=src_dict[\"white\"])"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from sp_morphology import mypequire_by_ids\n\treturn mypequire_by_ids(obj, [(\"compute\")]) == 0\n\n\n# ------------------------------------------------------------------------\n# Main\n\n\n# ------------------------------------------------------------------------\n#  Filling\n\n\nclass fill_array(SpFunction):\n\t\"\"\"  This version of 'fill' is to help compute some fill functions.\n\tIt is called for a sp_fundamentally_aligned 2D Fourier image with an \n\timage/image array and an sp_fundamentals_norms.  It"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\tif isinstance(obj, PILImage) and (not PILImage.supports_byte_order()\n\t\t\t\t\t\t\t\t\t   or not PILImage.check_byte_order()):\n\t\treturn False\n\treturn hasattr(obj,\"fill\") or hasattr(obj,\"compute\")\n\ndef _get_image_type(image, imtype):\n\t\"\"\"\n\treturn an image type from a given type enum value.\n\n\ttype_name: A type string that must convert to the type used in\n\t\t\t an image being generated.\n\n\t\"\"\"\n\t# The following code converts between various"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\traise RuntimeError(\n\t\t'fill_eager, compute_el, is_fill_compute_el for %s\\n' % str(obj))\n\n\n@_lazyerror\ndef fill(obj):\t\"\"\"Fill an extension in an existing extension.\n\n\t@memoize\n\tdef fill(self_inst, name, extension):\n\t\t\"\"\" Fill obj in a new extension.\n\n\t\tparams:\n\t\t - obj: a.obj extension\n\t\t - name (str): Name of the new Extension (e.g. \"x\" for # x).\n\t\t"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\tpass#IMPORTIMPORTIMPORT from sp_global_def import *\n\t\treturn obj._fill_compute(sp_global_def.ERROR(2))\n\ndef do_compute(el, func=None, obj=None, arg1=None, arg2=None, p1=None, p2=None, el_val = None, evals_stack=None, printout=False):\n\tpass#IMPORTIMPORTIMPORT from sp_logger import setLogLevel\n\n\tel = sp_utilities.wrap_mpi_bcast(el, False,"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\tif not isinstance(obj, DNA):\n\t\traise TypeError(\"Argument must be an instance of DNA\")\n\tif not ((0 <= obj.order and obj.order <= 4) or obj.el_size == 0):\n\t\traise ValueError(\"Argument must be a valid element order.\")\n\tif (0 <= obj.el_size and obj.el_size <= 1):\n\t\tif (0 < obj.order <= 4 and obj.order <= 7) or (4 > obj.order and obj.order <= 8):\n\t\t\traise ValueError(\"Argument must be at least"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from sp_morphology   import march, cephes        # NOQA\n\tpass#IMPORTIMPORTIMPORT from mpi        import MPI_Finalize, MPI_Gather\n\tpass#IMPORTIMPORTIMPORT import sys\n\n\tpass#IMPORTIMPORTIMPORT from sp_utilities   import get_im, bck, copy2image   # NOQA\n\tpass#IMPORTIMPORTIMPORT from EMAN2 import fft, fpc, recons, stack"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from FreeCAD import FreeCAD\n\tpass#IMPORTIMPORTIMPORT from OpenGL import GL,GLU\n\tpass#IMPORTIMPORTIMPORT from OpenGL.GLUT import ViewFrustum\n\tpass#IMPORTIMPORTIMPORT from OpenGL.GLUT import GetGlC\n\tpass#IMPORTIMPORTIMPORT from OpenGL.GLUT.GLU import GetGlC,GetViewFrustum\n\tpass#IMPORTIMPORTIMPORT from FreeCAD import FreeCADGui\n\tpass#IMPORTIMPORTIMPORT from OpenGL import glFrustum,gl"}]}
{"task_id": "62b87af19a0c4fa8b80b34f7", "prompt": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tD = {}\n\tif isinstance(d1, dict):\n\t\tD0 = d1\n\t\tfor k in d1:\n\t\t\ttry:\n\t\t\t\tD[k] = []\n\t\t\texcept KeyError:\n\t\t\t\traise KeyError('dict key \"s\" not found')\n\t\t\tD[k].append(d2[k])\n\telif isinstance(d1, str):\n\t\tD = d1\n\telse:\n\t\traise TypeError, 'expect a dict or a str, %s got %s' % (type(d1), type("}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tif level in [-1, 0]:\n\t\treturn\n\tif not level:\n\t\td1 = {}\n\t\td2 = {}\n\t\tfor item in list(dictionary):\n\t\t\titem.update(dictionary[item])\n\t\t\td1[item.key] = item.max_depth\n\t\t\td2[item.key] = d1[item.key] + diff(item.max_depth, d1[item.key])\n\telse:\n\t\tfor k, v in list(dictionary.items()):\n\t\t\tk = str("}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tfor key in d1:\n\t\ttmp = d2[key]\n\t\tif not *tmp and level < -1:\t\t# can happen with level=1\n\t\t\tif key not in d1:\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tnewD1 = {}\n\t\t\t\tfor i,value in enumerate(d1[key]):\n\t\t\t\t\tvalue = [value]\n\t\t\t\t\tfor j,value in enumerate(d1[key][i]):\n\t\t\t\t\t\tnewD1[key][i][j]=value\n\t\t\t\td1[key]=new"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t# check for a non-string argument that might cause segfault with Python\n\t# in 1..2, this works around an issue with the 'd1' argument.\n\tdef _set_difference1(a, b):\n\t\ta.difference(b)\n\treturn _set_difference1\n\n\ndef difference_map(other, keys, level=None, d=-1, asof=0):\n\t\"\"\"Return a dictionary whose keys are elements of *other* plus those\nof keys in *keys*.\nKeys are not included into the difference dictionary.\n\nKeys with"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\ttry:\t\t\t# try without \"raise\"\n\t\treturn diff_tree(d1, d2, level + 1)\n\texcept TypeError:\t\t# \"foo\" is string, not an instance of\n\t\t\t\t\t\t#  class DiffTree\n\t\tif not isinstance(d1, str) and not isinstance(d2, str):\n\t\t\te = StringIO()\n\t\t\te.write((\"diff_tree(%1s,%1s)!= 'diff_tree(%1s,%1s)\\n\").\n\t\t\t\t format(d1, d2, type(d1"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tif not hasattr(d1, '__len__'):\n\t\traise ValueError(\"d1 argument must be given to a recursive\")\n\tif not hasattr(d2, '__len__'):\n\t\traise ValueError(\"d2 argument must be given to a recursive\")\n\n\tif level > -1:  # recursive call: append to d1\n\t\td1.extend([d2[d2.index(d) \\\n\t\t\t+ d1.index(d2)] for d2 in d1])\n\t# call remove first time\n\twhile d1:\n\t\t"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\tif level is None:\n\t\treturn dict.intersection(*d1, *d2)\n\treturn dict.difference(*d1, *d2, level)\n\n/*\n * SPDX-License-Identifier: GPL-3.0-only\n * MuseScore-CLA-applies\n *\n * MuseScore\n * Music Composition & Notation\n *\n * Copyright (C) 2021 MuseScore BVBA and others\n *\n * This program is free software: you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 3"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t# First, make sure d1 is defined not in d2.\n\tin1, in2 = list(d1.items()), list(d2.items())\n\tif d1 in d2:\n\t\tin2.append(d1)\n\telif d2 in d1:\n\t\tin1.append(d2)\n\telse:\n\t\traise ValueError(\"D1 and D2 must be defined in d1 and d2.\")\n\n\tif level:\n\t\tif d2 in (d1, None) and in1:\n\t\t\tin"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\treturn _diffgram(d1, d2, level)\n\n\n\ndef intersection(d1, d2):\n\t\t\"\"\"Intersection of two dictionary entries.\n\n\t\tIf an entry of the tuple *entry* exists in *d1* and *entry*\n\t\texists in *d2*, the entry value is returned.  Otherwise a\n\t\tvalue of False is returned.  This routine is designed to\n\t\treturn a dictionary with items from *d1*, *d2*, and\n\t\t*entry*.\n\n\t\t**type**\\: bool\n\t\t\"\"\""}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t#return {}\n\ndef difference2(d1, d2, level=-1):\n\t\"Compare two dicts for difference.\"\n\treturn _Difference2( d1, d2, level )\n\ndef intersection(dict1, dict2, level=-1):\n\t\"Compare two dicts in a way that is effectively a set of dicts\"\n\treturn _PerpendicularIntersection(dict1, dict2, level )\n\n\n@python_2_unicode_compatible\nclass _Difference(type):\n\n\t\"\"\"\n\t@abstract\n\tAn abstract base class"}]}
{"task_id": "62b87af09a0c4fa8b80b34f1", "prompt": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tif coord == self.hist.c:\n\t\treturn self._empty()\n\tself.hist.fill(coord)\n\ndef clear_caches(self, depth=None):\n\t\"\"\"Clears the caches for this area.\n\n\tIf none are currently available, one is created for every point:\n\t\t$ $ $ point x y _ $ $ $ point z z _ = $ $ $ point z _ = $ $ _ =\n\t\t$ $ $ x y z _ = $ $ x y z _ = $ $ _ = $ _ = $ _ = $ _ = $"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tc = self.c\n\tif coord in [x for x in self.xrange(0), self.xsize()[-1]]:\t# horizontal\n\t\th = (coord - self.width()/2) / 2\n\t\treturn [coord for xy in list(zip(self.xrange(0,self.xsize()), h for self,h in self.hstack(coord,self.xsize())))]\n\tif coord in self.bins:\n\t\treturn [b.xy for b in self.bins[coord]]\n\treturn [-coord]\n\ndef"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tdmax = self.dmax\n\tdmin = self.dmin\n\tfor i in range(self.N):\n\t\tdsum = 0\n\t\tfor j in range(i+1,self.N):\n\t\t\tif(j!= sum(self.hist[j])):\n\t\t\t\tdsum += self.edge_weight[j]*(1-self.edge_weight[j]) * coord[i][j] / dmax\n\t\t\t\tdsum += self.edge_weight[j]*(1-self.edge_weight[j]) * coord[i"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tassert isinstance(coord, list)\n\tif not coord:\n\t\treturn None\n\n\t# Make a copy of our list\n\tall_list = []\n\tcoord.sort()\n\tfor el in coord:\n\t\tall_list.append((self.coord_x + el, self.coord_y + el))\n\n\treturn summed_list(all_list, lambda e, l: l.sum(e))\n\ndef histogram_fill(self, coord, weight):\n\tfill(self, coord, weight)\n\treturn self\n\n# Add"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\twhile coord!= self.vert[0]:\n\t\t# print(coord)\n\t\tif not weight % 2: x = coord[-weight]\n\t\tself.vert[coord[v]].y = x+1\n\t\tcoord = coord[v+1:]\n\treturn\n\n//! Tests for the `serde_derive` crate\n\npub(crate) mod native {\n    use crate::infer::Infer;\n    use std::ffi::CStr;\n\n    imp! {\n        use serde;\n  "}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tself.coords.append(coord)\n\tself.weight += weight\n\ndef fill_all(self, x0, x1, weight):\t\"\"\"\n\tFill all points into a rectangle (*x0*, *x1*).\n\tIf *x0*,*x1*,*weight* are zero the whole circle (represented as a pair of ints)\n\tis filled.\n\nReturns a 2 element array containing *x0*,*x1*.\nIn the second element it has ``weight``, ``x0``, ``x1`` (for ``x0`` to ``x1``"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t# Do the fill at all bins in the histogram\n\tif self._topy: # Topy\n\t\thisto = self.histo_y  # Copy histogram\n\t\tfor o in self: # Topy\n\t\t\tif o is not None: # Topy\n\t\t\t\to._draw_histogram(histo)\n\t\ttopy_y = self.histo_y.copy()\n\t\thisto.fill((coord,1),**kws)\n\t\treturn topy_y\n\n\t# Otherwise, fill at all bins in the histogram\n\t"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tself._check_axis_x_y(coord, 'b', weight)\n\n\tself._histogram_list[coord] = self.list.fill(coord, weight)\n\n\ndef _draw_histogram_histogram(list, color='g'):\n\t\"\"\"Draw a histogram in a line style.\n\n\tcolor: a string describing the color of the line; ``'g'`` for a gauge, ``'r'`` for a rectangle,\n\t``'p'`` for a point.\n\t\"\"\"\n\tfor coord, value in list.items():\n\t\t"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\tx = self._grid[coord]\t\t\t\t\t\t\t# xcoord\n\treturn self._fill(x, fill_edges(self, x, weight))[1]\t# fill edge\n\n\ndef fill_at(self, coord, fill_edges, weight=None):\n\t\"\"\"Fill a histogram at *coord*\n\n\n\tcoord is the location of the desired values.\n\n\tFilling edges are *excluded* from clustered values.\n\tThis is similar to *fill_edges*.\n\n\tReturns *weight* if the fill was performed.\n\t\"\"\"\n\tif weight is"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\treturn self.fill_at_coord(**coord)\n\n\n\nclass _Color(object):\n\tdef __init__(self, *args, **kwargs):\n\t\tself.color = tuple(args[0 : i+1])\n\t\tfor i in range(len(args)):\n\t\t\tself.set_at_val(args[i : i+1],\n\t\t\t\tColor._unspecified_class(args[ i%len(args)]))\n\ndef _color_from_tuple(v, args=None, **kwargs):\n\tif isinstance(v,"}]}
{"task_id": "62b86aa3b4d922cb0e688d36", "prompt": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\tSee https://docs.python.org/2/tutorial/str.html#invalidregex\n\n            #\n            labels = {\n                \"key1\": \"not valid\",\n                \"key2\": [\"invalid\"],\n            }\n            _validate_labels(labels)"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\tErrDescription = \"\"\"This block is used to render errors to the user, to help guide\nthe user a specific function in the Error Page.\n\nErrMessage: This value is shown if an Error occurred. The format is described in\nthis section.\n\nArgs:\n    msg (str):  the string to format the error into, can be:\n        - \"valid\" - error message is valid. It may include special characters (eg '$',\n                     which is a regular"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\t\"\"\"\n\"\"\"\n\t\treturn labels\ndef format_output_to_dict(output):\n\t\"\"\"\n\tHelper function for formatting the output to a python dictionary.\n\n    Arguments:\n        output (dict): the output to format.\n\n\tReturns:\n\t\tpytorch.core.dict: the final output\n\n\tRaises:\n\t\tNo validation error raised.\n\t\"\"\"\n\treturn dict(output)\n\n\nif __name__ == \"__main__\":\n\t'''\n\tpython python run.py\n\t'''\n\t"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\"\"\"\n\t_validate_labels(labels)\n\nimport type {\n  CogoData,\n  CogoEvent,\n  { CogoData, CogoEvent as AnyEvent, EventId }\n} from '@co-hooks/core';\nimport {\n  makeObservable,\n  observe,\n  map,\n  filter,\n  shareReplay,\n  toPromise,\n  of,\n  toArray,\n  fromJS,\n  pipe,\n  debounceUntil,\n  withLatestFromEffect\n} from 'rxjs';\nimport { Subject }"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\"\"\"\n\tdef _verify_labels(labels):\n\t\tfor key in labels:\n\t\t\ttry:\n\t\t\t\tself.assertDictEqual(labels[key], validate_for_evaluation(labels[key]))\n\t\t\texcept ValidationError:\n\t\t\t\traise ValidationError('%r does not match the label to which the '\n\t\t\t\t                      '\"%s\" key was validated.' % (labels[key], key))\n\n\tdef test_list_and_set_of_items(self):"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t#\n###############################################################################\n\n\ndef _make_object(key, value, attr):\n\t\"\"\"Given a key/value pair create a new `Object` in this object.\n\tUse of `attr` can be any object that supports a `getattr()` operation.\n\n\tArgs:\n\t\tkey (tuple): a tuple of (key, value) that describes the key and\n\t\t\tvalue in this new object.\n\t\tvalue (object): to define a value in the new object with the key.\n\t\tattr (Attribute): an attribute in the new object to access the value"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t# There is a special case where we have to add an error message before\n\t# we get the next regular expression. Unfortunately, there's no way\n\t# to do that -- we have to find the correct regular expression.\n\tfor key in ('[' + '\\n', '{', '{', '}', '{', '{', '}', '[', '[', ']', '}',\n\t             '{', '}', ']', '{', '}', '}'] + \"\\n\",\n\t           r'[^{]+', '', '', '', r\"\\n\", r"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\tReturn\n\n    dictionary with the list of label names for which an error occurred.\n    Each dictionary entry is a key-value pair:\n    - key: the label key which is either string (for strings) or numeric (for numeric\n        types) if an error occurred as string.\n    - value: the error message. The string value in any key can be any expression in\n        the key.\n\"\"\"\n\n\ttry:\n\t\t_validate_regexs(labels)\n\texcept Validation"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\tLabel mismatch.\n\n    \"\"\"\n\n    labels = labels or {}  # type: ignore\n\n    if not isinstance(labels, dict):\n        # If they are passed in as-is, they should not depend on labels.\n        raise ValueError(\"Expected a dictionary with keys 'labels' and values: \"\n                         \"{keys}.\".format(keys=keys(labels))\n     "}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\ndef _assert(result, expected_success, expected_message):\n\tassert result == expected_success\n\tif expected_message:\n\t\traise AssertionError(\"%s: %s\" % (check.id(result), expected_message))\n\ndef _validate_object(obj, label, expected):\n\tassert isinstance(obj, dict)\n\ndef _do_not_validate_objects(obj, test, success_message=None):\n\ttry:\n\t\tvalidate_success(obj, expected_success=False)\n\texcept ValidationFailed as err:\n\t\t"}]}
{"task_id": "62b86a9eb4d922cb0e688d25", "prompt": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\tr = re.compile(\n\t\t_regen_regex,\n\t\tflags=re.IGNORECASE | re.DOTALL)\n\treturn r\n\ndef _get_resource_class_regex():\n\tbuild_regex = r'(\\w+)\\.(\\w+)'  \n\tbuild_error_regex = r'\\. Building \\w+'   \t\n\tpackage_regex = r'^\\#package\\.([\\w\\w]*?)\\s*$'\t \n\tpackage_error_regex = r'^\\#\\#package\\.([\\w\\w]*"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\tresources_regexp = r\"^[[:digit:`]+$\"]*(,|$)\"\n\tresources_regexp_list = re.find(resources_regexp)\n\t# If not a matching pattern, check all resource names in the\n\t# resource_name_pattern with the first (if any) being the last dot.\n\tif len(resources_regexp_list) > 1:\n\t\tresources_regexp_list[-1] = resource.pattern_for_resource_name(\n\t\t\tresources_regexp_list[-1].findall(resources_regexp_list[-1])[0])"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\tpattern = []\n\tfor re_path, re_regexp in ((r'/(?:kram/)?([^/]+)',\n\t\t\t\tr'/(?P<name>[^/]+)~(\\+name)?/'), ((r'/[a-z]+', r'/(?P<name>\\S+)/'),\n             r'/(?:[^/]*?(?:(?P<ref>\\w+)|(?P<ext>[a-f]{1,3})){1}\\/(?P<ext>[a-f]{1,3})){1}'),\n "}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\tpattern = re.compile(r'\\w{1,64}', re.IGNORECASE | re.UNICODE)\n\treturn pattern\n\ndef get_schema_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe schema name of the Krake schema\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate the schema\n    name.\t\"\"\"\n\tpattern = re.compile(r'^[a-zA-Z][a-zA-Z0-9-_]{0,20}.[a-zA-"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\tpattern = re.Pattern(r'^(?:[a-z]:)?(.+?)[a-z][\\\\w\\\\.-]*'\n\t\t\t\t\t   '|(?:[.:]/.*)$')\n\treturn pattern\n\ndef _tox_version():\n\t\"\"\"\n\tSet in the resource dictionary the version of the tox.ini\n\t\"\"\"\n\treturn tox.ini.settings.read_dict('Version') >= 0\n\n@python_2_unicode_compatible\ndef tox_version():\n\t\"\"\"\n\tReturn the version of the tox.ini.\n\n\tIf"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\tparts = re.match(pattern[0], \"galaxy\")\n\tif parts:\n\t\treturn pattern[1:]\n\treturn (re.Pattern(\".*/$~\"), pattern[1:])\n\ndef _addPath(directory, dir_name, name):\n\t\"\"\"\n\tAdd a new path to a directory.\n\n\tReturns:\n    (str): a directory's full path\n    \"\"\"\n\tdir_dir = os.path.abspath(dir_name)\n\tbase_dir = os.path.dirname(dir_dir)\n\tfilename"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\ttry:\n\t\treturn re.compile(\n\t\t\tr'^[a-zA-Z0-9!$-]+$',\n\t\t\tre.ASCII)\n\texcept Exception, e:\n\t\tlog.error('Error validating resource name: %s', e)\n\ndef get_client_user_name(token, userID, domain):\n\t\"\"\"\n\tReturn a user's name, including the token, that is created\n\twithin the user.\n\t\"\"\"\n\n\t# get user information from token\n\tif userID is None:\n\t\tpass\n\telif"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t# regex is a function that takes a string and returns a regular\n\t# expression object. (See ResourceValidator and ResourceValidatorTest.py)\n\t# The regex syntax is the same as the regex syntax described above\n\treturn re.Pattern(r\"(^[a-z_]+$)\", r\"(http(s)?://[^/]+)$\")\n\n#__END_OF_JSAPI_FUNCTIONS__\n#-------------------\n\n__all__ = ['KRAKEN_BASE_ERROR',\n           'ResourceBaseError']\n\nimport json\nimport"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\tres_regex = re.compile(r'[\\w.-]+')\n\tfor resource in sorted(Resources):\n\t\treg = _patterns[resource[0]]\n\t\tif reg.name not in res_regex.match(resource):\n\t\t\t# This resource does not exist in this run.\n\t\t\treturn None\n\treturn reg.name\n\n\ndef _get_module_regex():\n\t\"\"\"\n\tReturn a valid regular expression built from the module name.\n\t\"\"\"\n\tmodule_regex = re.compile(r'(\\w+)(module.*)')\n\t"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\tregexp = re.compile('<.*>.*</.*>')\n\treturn regexp\n\ndef _get_resource_name_regex2():\t\"\"\"\n\tAdd optional \"regex\", \"escape\", and \":\" before any strings in the\n\tresource name. If set, it will be treated as a special string\n\tnot starting with \"/\", else the escape string will be used as the\n\tfirst \"magic\" char of the string.\n\t\t\"\"\"\n\tif config['extra'] is not None:\n\t\tregexp = '<.*:.*>'\n\t\treplace_regex = r'\\[[`'\n"}]}
{"task_id": "62b86a4fb4d922cb0e688cf8", "prompt": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tif isinstance(value,str) and re.compile(value)[0]!= value:\n\t\traise ValidationError(value+\" does not match regular expression '\"+re.compile(value)+\"', expected \"+value)\n\ndef assert_type(value, type):\n    if type is str:\n        assert isinstance(value,str)\n    elif type is byte or type is int or type is short:\n        assert isinstance(value,types.TypeWrapper)\n    else:  \n    "}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tif re.match(re.escape(value), config.regex)!= config.regex \\\n\t\tand value!= CONFIG_NOVALUE: # or is the regex but regex not\n\t\traise ValidationError(config.regex, \"The value '%s' is not valid.\")\n\nnamespace ClassLib072\n{\n    public class Class010\n    {\n        public static string Property => \"ClassLib072\";\n    }\n}\n\n/*\nCopyright 2014 The Kubernetes Authors.\n\nLicensed under the Apache License, Version 2."}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\treg = re.compile(r\"(.*)\\.$\")\n\tfor regex, reg_value in re.finditer(re.escape(value), reg):\n\t\treturn regex_result.send_message(regex, reg_value)\n\n\ndef regex_result(regex, reg_value):\n\traise ValidationError(\"Unable to validate pattern '{0}'\".format(reg_value))\n\n\ndef register_validator(regex):\n\t\"\"\"register validation of regular expression type\"\"\"\n\n\tvalidate_regex = regex.check_reg_string\n\n\tregister_validator(reg"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tvalid = False\n\n\treg_regex = '([_a-zA-Z]+)+'\n\tregexp_pattern = re.compile(reg_regex + r'\"[^\"]*\"')\n\n\tfor idx, reg_var in enumerate(regexp_pattern.finditer(value)):\n\t\ttry:\n\t\t\treturn value[reg_var.group(0)] == reg_var.group(1)\n\t\texcept IndexError:\n\t\t\tvalid = True\n\t\t\tcontinue\n\t\texcept ValueError:\n\t\t\tvalid = True\n\t\texcept SyntaxError:\n\t\t"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\ttry:\n\t\tvalue = re.compile(value)\n\texcept:\n\t\traise ValidationError(\"Invalid value for {}\".format(value))\n\n\nclass ListItem(object):\n\n    \"\"\"A single item in a list\"\"\"\n    def __init__(self, item_str):\n        \"\"\"Constructor\"\"\"\n        self.item_str = item_str\n\n    def __str__(self):\n        \"\"\"Get the string\"\"\"\n       "}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\terror_str = value\n\tif not re.search(regex_pattern, value, re.IGNORECASE):\n\t\traise ValidationError(\n\t\t\terror_str\n\t\t)\n\npackage com.netease.im.swallbook.util;\n\nimport android.support.v7.app.AlertDialog;\nimport android.text.TextUtils;\nimport android.util.Log;\n\nimport com.alibaba.fastjson.JSONArray;\nimport com.alibaba.fastjson.JSONObject;\nimport com.netease.im.swallbook.model.UserInfo"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tname = value.lower()\n\tif name!= 'null' and not name in {None, 0, 1}:\n\t\traise ValidationError(\n\t\t\t'The \"%s\" field in the \"null\" or \"True\" and \"True\" values cannot be used for comparison to \"%s\". It must be a number or an integer.' %\n\t\t\t(value[0].upper(), value[1].lower()))\n\n\ndef validate_range(value):\n\tif value is None:\n\t\traise ValidationError('the \"null\" value must be not be None and cannot be used"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tif not value:\n\t\traise ValidationError(\"Please enter a valid value.\")\n\n\nif __name__ == \"__main__\":\n\tsys.exit(main(parse_args()))\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tglobal ValidationError\n\tif value == '':\n\t\tValidationError(None, \"must not be empty\", None)\n\ndef build_url(url):\n\tglobal ValidationError\n\tif url == '':\n\t\tValidationError(None, \"must not be empty\", None)\n\npackage com.android.internal.telephony.ims;\n\nimport android.os.AsyncResult;\nimport android.os.Handler;\nimport android.os.Message;\nimport android.provider.Settings.Global;\n\npublic abstract class ImsBaseCallback {\n    protected static final"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\tif not re.sub(re.escape(r'\\w'), r'\\W', value):\n\t\traise ValidationError(\"Cannot parse the given pattern.\")\n\treturn value\n\n\ndef is_regex(source_value):\n    \"\"\"Detect if the given value is a regular expression or not.\"\"\"\n    return re.match(r\"^(?P<value>[^.])(?P<source>.*)$\", source_value)\n\npackage org.simpleframework.xml.core;\n\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport"}]}
{"task_id": "62b86a4fb4d922cb0e688cf7", "prompt": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tif not key in VALID_KEY:\n\t\traise validation_error.ValidationError(\n\t\t\t\"The given key is not a valid (possibly short) regular expression.\")\n\npackage com.huawei.security.fingerprint.exception;\n\npublic class FingerPrintException extends Exception {\n    private int errCode;\n    private String errMsg;\n\n    public FingerPrintException(int errCode, String errMsg) {\n        super(errCode, errMsg);\n        this.errCode"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tif not re.match(r\"[a-zA-Z]+(?:[a-zA-Z-][a-zA-Z0-9+]*)?\", key):\n\t\traise ValidationError(\"'%(key)s' must be either an array: [%(key)s].\t\"%(key))\n\nclass Validator(object):\n\tdef __init__(self, engine, validator):\n\t\tself.validator = validator\n\t\tself.engine = engine\n\n\t\tself.values = {}\n\t\tself.check_values = {}\n\t\tself.has_changes ="}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tpattern = re.compile('^[a-s][a-z0-9]*$', re.U)\n\tval = int(key).to(16) if val.isdigit() else int(key)\n\tif pattern.search(val):\n\t\tpass\n\telse:\n\t\traise ValidationError('Key \"%s\" must conform to the regular expression' % val)\n\n/*----------------------------------------------------------------------------\n * Tencent is pleased to support the open source community by making TencentOS\n * available.\n *\n * Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tregexp_regex = re_regexp(\"_(.*)(_.*)\" )\n\tregexp_regex.match(key) \n\ndef validate_urlquery(url, params):\n\tq = \"http://\" + url + \"\\n\"\n\tfor key in params:\n\t\tval = params[key].split(\"=\", 1)[1].replace(q, '')\n\t\tval = val.strip().replace('\"', '').replace('(', '')\n\t\tval = val.replace(')', '') \n\t\tif len(val)>0:\n\t\t\ttry:\n\t\t\t\tregex_str ="}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tregex = r'^\\w+$'\n\tif not regexp.search(key):\n\t\traise ValidationError(f'{regex} is not a correct regular expression.'\n\t\t\t+' Please specify an acceptable expression.'\n\t\t)\n\n\treturn key\n\ndef createIndex(name, data):\n\t\"\"\"\n    Create a new data index\n\n    If a data index is already in the database, it will be destroyed automatically first.\n    The database will then make sure that the index is actually a duplicate.\n\n    Args:\n "}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tif not re.match(pattern_re, key) or not re.match(pattern_re_de, key):\n\t\traise ValidationError(\"The given key is not conform to the regular expression.\")\n\ndef validate_object(obj, keys, def_list):\n    \"\"\"Validate the given key by assigning an array and returning a boolean.\n    \n    Args:\n    \n    obj: the object to validate\n    keys: the string containing the keys to validate\t\t\t\n    def_list: the arrays of names of"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tpatterns = {'^[a-zA-Z0-9-_]+$', '^', '^', '*', '+', '^', '$',\n\t\t\t\t\t'^[a-zA-Z0-9_]*$', '^$', '!^', '*']\n\n\tdef validate(regex):\n\t\treturn re.search(regex, key), {'patterns': patterns }\n\treturn validate\n\ndef validate_and_extract_key(key):\n\t\"\"\"\n\tVerify that the given regex key is valid against the corresponding regular\n\texpression.\n\n\tArguments:\n  \tkey: the string to"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tif not key.find(None) > 0:\n\t\traise ValidationError(\"Expecting 'None' in key!\")\n\n\tif key.find(\"^\") == 0:\n\t\traise ValidationError(\"Key '.*' does not exist!\" )\n\n    if len(key) == 1:\n        if key[0] == \"password\":\n            return \"Must not be longer than 5 characters!\"  # pylint:disable=no-member\n\n        elif key.lower()"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tif not re.search(re.escape(key), \"\") and not re.search(re.escape(key),_not_ok):\n\t\traise ValidationError('\"' + key + \"' is not a recognized regular expression pattern.  Please see https://docs.python.org/3/library/re.html')\n\n\ndef validate_hash(hash):\t\"\"\"\n\tValidate the given hash against the corresponding regular expression.\n\nArgs:\n    hash: the string to validate\n\nRaises:\n    ValidationError: if the given hash is not conform to the regular expression"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\tif key not in _key_regexp_cache:  # pylint: disable=W0212 # nosec [A0324]\n\t    _key_regexp_cache[key] = True\n\treturn True\n\nimport React from'react';\nimport createSvgIcon from './utils/createSvgIcon';\nexport default createSvgIcon(React.createElement(\"path\", {\n  d: \"M6 2H6c-1.1 0-2.9-2 2v12c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V"}]}
{"task_id": "62b86a01b4d922cb0e688ccc", "prompt": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tif isinstance(manifest_dict, dict):\n\t\tself.logger.info(\"Generating observer_schema dict\")\n\t\tobserver_schema_dict = generate_default_observer_schema_list(manifest_dict)\n\telse:\n\t\tself.logger.error(\"Generating observer_schema list failed.\")\n\t\treturn\n\tfor observer_schema in observer_schema_dict:\n\t\tif first_level or not (isinstance(manifest_dict, dict) and\n\t\t\t\t\t   isinstance(observer_schema, dict)):\n\t\t\tself.logger.info(\"Generating"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tmanifest_dict = dict()\n\tif not first_level:\n\t\t# first_level dictionary\n\t\tfor name, value in manifest_dict.items():\n\t\t\tif name not in manifest_dict:\n\t\t\t\tmanifest_dict[name] = value\n\n\t# merge\n\tfor name in manifest_dict.keys():\n\t\tmanifest_dict[name] = create_observer_schema_dict(manifest_dict[name])\n\n\treturn manifest_dict\n\n\ndef get_observer_schema_list(manifest_list):\n\t\"\"\"Returns a list of"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tobservers_schema_list = manifest_list.generate_default_observer_schema_list(\n\t\tbundle_manifest_path(manifest_dict)\n\t).values()\n\n\tif first_level:\n\t\treturn observers_schema_list[0]\n\telse:\n\t\treturn observers_schema_list[1]\n\n#!/usr/bin/env python\n#\n#  $Id: TestPluginLoader.py 4737 2011-12-26 03:09:18Z rusu $\n#  Copyright 1997-2008 The Author(s"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tdict = generate_default_observer_schema_list(manifest_dict)\n\treturn dict | dict.to_dict()\n\n\ndef remove_underscore():\n\t\"\"\"\n\tRemoves underscores from input str.\n\n\tReturns:\n\t\tDictionary from ``manifest_dict`` containing fields as ``None``\n\t\"\"\"\n\tmanifest_dict = {k: v for k, v in manifest_dict.items() if v is not None} | manifest_list\n\n\treturn manifest_dict.to_dict()\n\n\ndef generate_default_observer_schema_"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tlogger.debug(\"Building partial list of OBEM schemas\")\n\tif first_level:\n\t\tlogger.debug(\"Extracting all fields from manifest\")\n\t\tnew_manifest_dict = None\n\t\tfor manifest_attr, manifest_dict in manifest_dict.items():\n\t\t\tfor value_attr in manifest_dict[manifest_attr].keys():\n\t\t\t\tif value_attr not in manifest_dict[manifest_attr][\n\t\t\t\t\tmanifest_dict[manifest_attr]][\"required_fields\"]:\n\t\t\t\t\tlogger.debug(\"Adding field %s to schema %s"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tschema_names_to_replace = {}\n\tfor name, val in manifest_dict.items():\n\t\tif name == \"observer schema for kubernetes resources\":\n\t\t\tbreak\n\t\tschema_list = list(val)\n\t\tif first_level:\n\t\t\tschema_list = [x.get(\"schema\", \"None\") for x in schema_list]\t# noqa: E501\n\t\t\tcontinue\n\n\t\tfound = False\n\t\tfor schema in schema_list:\n\t\t\tif (\n\t\t\t\tschema == \"none\" or\n\t\t\t\t(schema"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tresult = { manifest_dict: None }\n\tfor resource_list, schema_list in generate_default_resourcetealed_list(manifest_dict, first_level):\n\t\tif manifest_list:\n\t\t\tresult['manifest'] = manifest_dict\n\t\t\tresult['manifest_list'] = manifest_list\n\t\telif schema_list:\n\t\t\tresult['schema'] = schema_list\n\t\t\tresult['schema_list'] = schema_list\n\treturn result\n\n\ndef generate_default_observer_schema_list(manifest_dict, first"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tif first_level:\n\t\toriginal_manifest_dict = __get_current_manifest_dict__('default')\n\t\tif first_level: return original_manifest_dict\n\t\treturn copy.deepcopy(__get_current_manifest_dict__('default'))\n\n\tobserver_schema = generate_observer_schema_list(manifest_dict)\n\treturn copy.deepcopy(__get_current_manifest_dict__('default', observer_schema))\n\n\ndef generate_observer_schema_list(manifest_dict):\n\treturn __generate_observer_schema_list__("}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tdef gen_observer_schema_dict(manifest_list, manifest_schema):\t\t\"\"\"\n\t\tif first_level is True:\n\t\t\tgen_partial_observer_schema_dict(manifest_list, manifest_schema)\n\t\t\treturn\n\t\tfor i, resource_name in enumerate(manifest_list):\n\t\t\tif i > 0:\n\t\t\t\tgen_partial_observer_schema_dict(manifest_list[i], manifest_schema)\n\t\treturn manifest_schema\n\n\tif first_level:\n\t\tschema = gen_observer_schema_"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\tif first_level:\t\tdict = manifest_dict\n\telse:\t\t\tdict = copy.deepcopy(manifest_dict)\n\tfor resource in manifest_dict[boto3.resource_prefix('AWS::KinesisAnalytics::ApplicationStreamAnalyticsConfig')]:\t\tif resource is not None:\n\t\tif resource.get('Name')!= 'NONE':\t\tdict['Name'] = resource.get('Name')\t\t\tdict['Source'] = resource.get('Source')\t\t\tdict['Role'] = resource.get('Role')\t\t\t# Not every configuration can have one --"}]}
{"task_id": "62b869ebb4d922cb0e688cc6", "prompt": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tassert len(last_applied_manifest) > 0, \"last_applied_manifest is not ready.\"\n\n\tlast_applied_manifest_list = []\n\n\tfor field in last_applied_manifest[F.name(\"status\")]:\n\t\tif field.get(\"name\") == F.name(\"current\"):\n\t\t\tobserving_fields = get_observing_fields_for_field_path(field)\n\n\t\t\tif len(observing_fields) > 0:\n\t\t\t\tfor observed_field in observing_fields:\n\t\t\t\t\tif current_field"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tdefer import_resources.update_last_applied_manifest_from_resp(\n\t\tlast_applied_manifest,\n\t\tresponse,\n\t\tobserver_schema,\n\t\tfrom_dict=bool_if_bool_not_in_list,\n\t\tschema_id=schema_id\n\t)\n\n\n@python_2_unicode_compatible\ndef update_last_applied_manifest_object(\n\t\tlast_applied_manifest_object,\n\t\tobserver_schema,\n\t\tresponse,\n\t\tschema_id=None\n):\n\t"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tif \"last_applied_manifest\" in response:\n\t\tfor current_res in response:\n\t\t\tif current_res[\"last_applied_manifest\"]:\n\t\t\t\tcurrent_res[\"last_applied_manifest\"][:] = {\n\t\t\t\t\t\"last_applied_manifest\": False,\n\t\t\t\t}\n\t\t\tfor observer_ns in current_res[\"observed_object_list\"]:\n\t\t\t\tfor observed_ns in observer_ns[\"objects\"]:\n\t\t\t\t\tfor current_entry in observed_ns[\"keys\"]:\n\t\t\t\t\t\tif current_entry == \"metadata\":\n\t\t\t\t\t\t\tfor"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\targs = {'namespace': namespace,\n\t\t'pretty': Pretty(format_request_body = pretty_client_req_body),\n\t\t'namespace_id': namespace['id'],\n\t\t'last_applied_manifest': last_applied_manifest_from_resp(response,\n\t\t\tnamespace = namespace,\n\t\t\tobserver_schema = observer_schema),\n\t}\n\tupdate_last_applied_manifest_from_resp(args)\n\treturn args\n\ndef update_last_applied_manifest_dict_from_resp(\n\t\targs):\n\t"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tif len(observer_schema)>0: return\n\tif len(response)>0: return\n\tfor k,v in response:\n\t\ttry:\n\t\t\tv[k] = update_last_applied_manifest_from_resp(k)\n\t\texcept KeyError:\n\t\t\tlogging.exception(\"Failed to update last_applied_manifest from\"\n\t\t\t\t+\" : ``%s``\" % k)\n\t\tfor elem in v:\n\t\t\ttry:\n\t\t\t\telem[k] = elem[k][1]\n\t\t\texcept KeyError:\n\t\t\t"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tprint(\"Updating all partial manifests in last applied_manifest...\")\n\t\n\treq := gjson.loads(last_applied_manifest)\n\treq = filter_partial_fields(req)\n\tprint(len(req[\"partial_schemas\"]))\n\n\tres = []\n\twhile res!= nil and res!= nil and (not get_partial(\"last_applied_manifest\", res)):\n\t\tpartial_key, partial_value, meta_k = last_applied_manifest\n\t\tif partial_key not in req:\n\t\t\treq[partial_key+\".partial_schema\"]"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tif not len(last_applied_manifest):\n\t\treturn\n\n\tif obs_fields_schema == NULL:\n\t\tobs_fields_schema = json_schema\n\telse:\n\t\tobs_fields_schema = obs_fields_schema + \"\\n\" + \"\\n\" + json_schema\n\n\tfor o in last_applied_manifest.values():\n\t\tupdate_fields[\"last_applied_manifest%s.metadata.namespace\" % last_applied_manifest_list_field_name] = o\n\t\tfor field_entry in obs_fields_"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tif len(response) == 0:\n\t\treturn\n\n\tmanifests_to_update = [{}]\n\tfor manifest in last_applied_manifest:\n\t\tfor k, v in manifest.items():\n\t\t\tif k in observed_fields:\n\t\t\t\tfor kv in observed_values[k]:\n\t\t\t\t\tswitch_field = kv['field']\n\t\t\t\t\tif not isinstance(switch_field, bool):\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tswitch_val = bool(getattr(etcd, switch_field))\n\t\t\t\t\tif flag_set(flag_manifest, flag"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tfrom clusterfuzz._internal.models.aws import last_applied_manifest_from_yaml\n\tfrom clusterfuzz._internal.models.aws.validators import last_applied_manifest_is_valid\n\tfrom clusterfuzz._internal.models.aws.validators import validate_last_applied_manifest\n\tfrom clusterfuzz._internal.models.aws.validators import validate_last_applied_manifest_dict_from_resp\n\tfrom clusterfuzz._internal.models.aws.validators import generate_validate_manifest_errors_for_update\n\tfrom clusterfuzz._internal.models.aws.validators"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tall_applied_manifest['last_applied_manifest'] = last_applied_manifest\n\tlast_applied_manifest_dict = get_partial_json(observer_config_schema=observer_schema,\n\t\t\t\t\t\t\t\t\t\t\t\t\tlast_applied_manifest=last_applied_manifest)\n\treturn all_applied_manifest_dict\n\n\ndef update_last_applied_manifest_dict_from_resp_to_schema():\n\t\"\"\"Return an updated dict of latest applied manifest\"\"\"\n\tall_applied_manifest = {}\n\tfor manifest in all_applied_manifest_dict:\n\t"}]}
{"task_id": "62b869eab4d922cb0e688cc5", "prompt": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tlast_applied_manifest_dict = {}\n\tlast_applied_manifest_schema_from_resp = {}\n\n\tif 'last_applied_manifest' in response:\n\t\tif not last_applied_manifest_dict:\n\t\t\tlast_applied_manifest_dict = response['last_applied_manifest']\n\t\t\tlast_applied_manifest_schema_from_resp = response['last_applied_manifest_schema']\n\t\telse:\n\t\t\tlast_applied_manifest = last_applied_manifest_dict[response['last_applied_manifest']]\n\t\t\t"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tdefer get_kube_api_client()._create_object_or_update_return_value_if_error.wait()\n\n\tclient = get_kube_api_client()\n\tclient.check_namespace_or_cluster = False\n\tclient.kubernetes_api_key = observer_schema[\"k8s_namespace\"]\n\tclient.kubernetes_version = observer_schema[\"version\"]\n\tlast_applied_manifest_schema = {}\n\tforce_latest_applied = False\n\n\tall_applied_manifest = []\n\tfor k in observed_field:\n"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tlast_applied_manifest, observer_schema, response = \\\n\t\tupdate_last_applied_manifest_dict_from_resp(last_applied_manifest,\n\t\t\tobserver_schema, response)\n\n\tif not is_registered_kubernetes_namespace(observer_schema,\n\t\tget_client(response)):\n\t\traise ValueError(\"Observed fields are not compatible, either \"\n\t\t\t\"a new partial Kubernetes should only have an \"\n\t\t\t\"updated one in the YAML (no 'latest' or 'namespaced'\n\t\t\tlabels) or a previous partial Kubernetes should"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tlast_applied_manifest_dict = yaml_load_str_into_dict(\"last_applied_manifest\",\n\t    last_applied_manifest)\n\n\tlast_observed_updated = None\n\tif last_applied_manifest_dict:\n\n\t\ttry:\n\n\t\t\tif last_applied_manifest:\n\t\t\t\tlog.warn(f\"Got observed data: {last_applied_manifest}\")\n\t\t\telse:\n\t\t\t\tlog.warn(\"No observation, nothing to update.  Restarting....\")\n\n\t\t\tif last_observed_updated is not"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tif not observed:\n\t\treturn\n\tresponse = Response(response)\n\tfor field in observed:\n\t\tif field in response.seen:\n\t\t\tcontinue\n\t\tresponse.last_applied_manifest[field] = last_applied_manifest\n\ndef update_last_applied_manifest_list_from_resp(\tobservations):\n\tobservations = convert_to_list(observations)\n\tlast_applied_manifest = dict()\n\tfor key, observations in observations.items():\n\t\tfor _, obs in observations.items():\n\t\t\tfor _,"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tlogger.debug(\"Partial Kubernetes API call, last_applied_manifest %s\" % (\n\t\tto_str(last_applied_manifest),))\n\tif not is_last_applied_manifest(response):\n\t\tupdate_last_applied_manifest_dict_from_resp(response)\n\treturn\n\ndef update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\ttry:\n\t\to = get_object_via_http(\n\t\t\tget_current_api_version(),\n\t\t\tget_url(\n\t\t\t\tb\"api/v2/namespaces/%s/clusters/cluster_id\"\n\t\t\t\t.format(response.get('metadata', {}).get('namespace', '')\n\t\t\t\t\t.b('cluster/controllers'))\n\t\t\t\tif boto3_module.boto_version_less_than(\n\t\t\t\t\tboto_version='3.0')\n\t\t\t\tor boto3_module.boto_version_"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\twith open(__file__, \"r\") as resp:\n\t\tresp_json = load(resp.read)[\"data\"] for _key in resp.keys()\n\t\tfor _key in [last_applied_manifest.get(\"Name\"),\n\t\t\t\t\t last_applied_manifest.get(\"Status\"),\n\t\t\t\t\t last_applied_manifest.get(\"LastApplied\"),\n\t\t\t\t\t last_applied_manifest.get(\"Message\")]:\n\t\t\tif _key not in ObservedResources:\n\t\t\t\t# skip non-applied fields\n\t\t\t\tbreak\n\t\t\t_prev = ObservedResources[_key][\"prev"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tlogger = get_loggers()\n\tif response == {}:\n\t\traise KeyError(\"At least one observed field is required\")\n\tif isinstance(response, dict):\n\t\tif len(response)!= 2:\n\t\t\traise ValueError(\"Expected 2 fields in response, got {}\".format(len(response)))\n\t\treturn update_last_applied_manifest_list_from_resp(response[0])\n\telif isinstance(response, list):\n\t\tif len(response)!= 2:\n\t\t\traise ValueError(\"Expected 2 fields in response, got {}\".format(len("}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\tmanifest = last_applied_manifest[ObserveField]\n\tif not manifest:\n\t\tlogger.critical('Last applied manifest update was unsuccessful, so using the current last applied manifest.')\n\t\tlast_applied_manifest[ObserverSchema] = observer_schema\n\t\tlast_applied_manifest = manifest\n\treturn manifest\n\npackage com.github.lgooddatepicker.datatables.columnproviderfactory;\n\nimport java.util.List;\n\nimport org.androidannotations.annotations.Background;\nimport org.androidannotations.annotations.NotNull;\n\nimport com.github"}]}
{"task_id": "62b869eab4d922cb0e688cbf", "prompt": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tfor resource in app.resource(types.Resource.Observer, types.Observer):\n\t\tyield [resource],\n\t\t\t\n\t# We would like to have a schema from this resource type\n\n\ndef generate_default_provider_list(app):\n\tproviders = app.project.resources.get(\n\t\ttypes.Resource.Provider,\n\t\ttypes.Resource.Provider.get)\n\tfor resource in providers:\n\t\tyield [resource],\n\t\ndef generate_default_provider_schema(app):\t\"\"\"\n\tGenerate the default owner schema for each Kubernetes resource present in"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tspec = app.get_current_spec()\n\tif not spec.manifest:\n\t\traise ValueError(f'No data for the following resources in {app.name}: {app.resource_name}: {app.resource_id}')\n\tdata = {}\n\tfor resource_id, data in spec.manifest.items():\n\t\tfor key in spec.fields.keys():\n\t\t\tdata[key] = None\n\t\tfor key, value in data.items():\n\t\t\tif 'default' not in spec.fields or key in spec.files.keys():\n\t\t\t"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\treturn [\n\t\t\"resource_with_observer_schema\",\n\t\t\"metadata\",\n\t\tstr(app.name),\n\t\tstr(app.version),\n\t\t\"type\",\n\t\t\"CustomObserverSchema\"\n\t]\n\ndef generate_default_clientsecrets_schema(app):\t\"\"\"\n\tGenerate the default clientsecrets schema for each Kubernetes resource\n\tpresent in ``spec.manifest`` for which a custom clientsecrets schema\n\t hasn't been specified.\n\n\tArgs:\n\t\tapp (krake.data.kubernetes.Application): The application for"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tfor name, k8s_client in kube_objects.get_all_interactions(name='default').items():\n\t\tfor resource, client in k8s_client.objects.namespace['v1_resources'].items():\n\t\t\tif client.meta.resource_version or client.meta.object_path:\n\t\t\t\tyield app.build_observer_schema_for_resource(resource)\n\n\nclass MockK8sClient(object):\n\t\"\"\"A class used to manage API client object used by the class.\n\n\tSee :const:`mock.client_"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tdefault_schema = load_default_observer_schema(app)\n\tfor kvp in default_schema.items():\n\t\tif kvp['observer_name'] == spec.observer_name:\t\t# If the observer has already a default\n\t\t\tif kvp['default_callback']:\t\t\t\t\t\t\t\t# If the provided default callback is already present in the bundle\n\t\t\t\tkvp['default_callback'] = (f'@{kvp['custom_object_handle']}',\n\t\t\t\t\t\"Custom JSON callback\", \"default_callback=custom_callback\" if\n\t\t\t\t\tkvp"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tdef default_observer_schema():\t\"\"\"\n\t    return k8s.PodCustomObserverSchema(spec.manifest)\n\n\treturn {\n\t\t\"default_observer_schema\": default_observer_schema,\n\t}\n\n/*\n * Copyright (c) 2001, 2018, Oracle and/or its affiliates. All rights reserved.\n * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n *\n * This code is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License version 2 only, as\n * published by"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tfor resource_name, resource_spec in app.resources.items():\n\t\tfor kube_observer_schema in resource_spec.get_kube_schema_observers():\n\t\t\tobserver_schema = KubeSchemaGenerator.generate_object_schema(\n\t\t\t\t\tapp, kube_observer_schema, resource_name)\n\t\t\tif observer_schema:\n\t\t\t\tapp.add_resource_observer(resource_name, observer_schema)\n\n\"\"\"\n\n\tdef generate_pod_schema(app, pod_yaml, annotation):\n\t\t\"\"\"Generate a pod schema for a"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\treturn [\n\t\tSchema(\n\t\t\t[\"namespace\", \"annotation\", \"annotation_namespace\", \"type\",\n\t\t\t\"annotation_type\", \"group\", \"version\", \"event\", \"event_type\"],\n\t\t\t[\n\t\t\t\tNone,              # namespace\n\t\t\t\tNone,              # annotation,\n\t\t\t\tNamespace(\n\t\t\t\t\tNone, \"default\",\"kubernetes\",\n\t\t\t\t\t[]schemautil.Option(None, schemautil.Option(None, schemautil.Boolean"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\treturn spec.manifest.gen_schema(application=app, generate=True).generate()\n\ndef get_default_observer_schema(app):\t\"\"\"\n\tReturn the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to get a\n        default observer schema\t\"\"\"\n\treturn spec.manifest.gen_schema(application=app, generate=False).generate()\n\ndef"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\tdef generate_default_schema_generator(app):\t\"\"\"\n\t\tGenerator function for generating a custom observer schema for the\n\t\t\tresource given in'spec.manifest'\n\t\t\"\"\"\n\t\tdef generate_default_observer_schema(app, pod, status, image_ref):\n\t\t\tschema = dict((k, v) for (k, v) in default_schema.items())\n\t\t\tschema[IMAGE_REF] = image_ref\n\t\t\tschema[STATUS] = status\n\t\t\tschema[KIND] = 'CUSTOMER'\n\t\t\treturn schema\n\n"}]}
{"task_id": "62b43427903eeb48555d3ea5", "prompt": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t__docformat__ = 'documentation_only'\n\n\tsql = ''\n\tparams = ''\n\n\tkwargs = {'in_style': 'list'}\n\n\t__slots__ = (\"sql\", \"params\", \"_format\", \"_value\", \"_error\")\n\tdef __init__(self, sql=\"\", params=\"\", table=None, _format='csv', _value='', _error='', **kwargs) -> None:\n\t\t# type: (str, Any, str, str, str, Dict[str, Any], Any, Any) -> None\n\t\tsuper(SQL, self).__init"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\tdef format_sql(\tsql, *params, db_class=None, **kwargs) :\n\t\t\"\"\"Format the SQL query & format the params & write the new\n\t\tSQL query.\n\n\t\t*sql* is the SQL representation of the query in 'text/' style,\n\t\t*params* the arguments to execute the query with.\n\n\t\t*db_class* (:class:`~collections.abc.Mapping` or\n\t\t:class:`~collections.abc.Sequence`) is the :class:`~collections.abc.Sequence` of db.\n\n\t\t*kwargs"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\treturn format_params(self._query_)\n\ndef format_params(query):\n\treturn _format_params(query)\n\n\"\"\"\n\t# TODO implement\n\n\tdef _get_query_template_parts(self):\n\t\treturn self._query_string\n\n\tdef parse_params(self, params, args):\n\t\tif self._parse_params_template:\n\t\t\tself.parsed_query_string.extend(params)\n\t\t\tif _use_template:\n\t\t\t\tfor param in params:\n\t\t\t\t\tif _use_template_index"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\treturn _Sql.format(self, fmt, params)\n\n\t\ndef convert_unicode(s, to_unicode=True, default_charset=\"utf8\"):\n\t\"\"\"Convert UTF-8 into unicode (if the target format is used for that).\n\tIt supports the following styles:\n\t* ``ascii`` - return unicode\n\t* ``unicode`` - if bytes is not provided, simply return that.\n\t* ``bytes`` - use unicode when decoding, otherwise unicode.\n\t* ``unicode_unicode`` - only encode unicode.\n\tIf to_unicode is True, the result"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\tresult = {}\n\t\tlast_comma = ''\n\t\thas_space = False\n\t\tfor i in range(len(sql)):\n\t\t\tc = sql[i]\n\t\t\tif c == ',':\n\t\t\t\tif has_space:\n\t\t\t\t\tresult[last_comma + ''] = True\n\t\t\t\thas_space = True\n\t\t\t\tlast_comma = ','\n\t\t\t\tcontinue\n\t\t\tif ':' in c:\n\t\t\t\tif last_comma!= c:\n\t\t\t\t\tresult[(''] + [c, last_comma] + ['(', ',',"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\tquery, params, _, _query_format, _query_params = _SQL_get_sqlquery, format\n\n\tif query:\n\t\tif \"sql\" in params:\n\t\t\tsql_params = params\n\t\tif \"params\" in params:\n\t\t\tparams = params[\"params\"]\n\t\tif \"sql_params\" in params:\n\t\t\tsql_params = params[\"sql_params\"]\n\telse:\n\t\tsql_params, sql_params_dict = _SQL_get_sqlquery(params=params) #noinspection PyProtectedMember"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\tquery, params = format_sql(self._query, self._params)\n\treturn format_params(query, params, self._type_hint)\n\n\nclass SQL_Format_Text(base.BaseFormat):\n\t\"\"\"Format SQL statements to be formatted.\n\n\tThis method takes in a pre-parsed text and/or parameters and uses it\n\tfor formatting. This should not be used with any sql.\n\n\t.. warning::\n\n\t\tThe :meth:`format` method for this class has been slightly\n\t\tover-broad, so that you may use a different"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\tprint \"=> %s\" % (self._sql, self._sql_params)\n\t\nclass sqlParams(object):\n\t\"\"\"The standard Python representation of a set of SQL query parameters.\n\n\tThis class doesn't currently support parameter styles, so the in-style\n\tparameters (as specified by SQLParams) can be rendered with ``format``.\n\n\t.. warning::\n\t\tOnly parameter styles supported by the SQL query are considered.\n\n\t.. seealso::\n\n\t|sql_params|\n\n\t\"\"\"\n\tdef __init__(self):\n\t\tself._"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\tif not isinstance(t, SQLTuple) and in_styles:\n\t\tt = SQLTemplate(t)\n\t\tparams = params or []\n\t\tin_style = in_styles.get(in_style_name, not in_style)\n\t\tout_style = SQLParams(*t.style)\n\telse:\n\t\tt = SQLTemplate(t)\n\t\tin_style = in_styles.get(in_style_name, None)\n\t\tout_style = t.style\n\tif params:\n\t\tparams = [(n,)"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\tif kwargs is None:\n\t\treturn _format_sql(sql, params)\n\tparams = kwargs.pop(\"params\", None)\n\tif params is not None:\n\t\tparams = _check_params(params)\n\treturn _format_sql(sql, params)\n\n@def format_all(\n\t\"\"\"\n\tConvert the SQL query to use the in-style parameters instead of the\nout-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc."}]}
{"task_id": "62b43428903eeb48555d3eaa", "prompt": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\tif (None,None,None,None,None,None,None) in six.iteritems(**kw):\n\t\treturn None\n\telse:\n\t\tif PY3:\n\t\t\tparams = kw.pop('params',[])\n\t\telse:\n\t\t\tparams = kw.pop('params',[],kw.pop('fields',[]))\n\t\tif len(params) == 0:\n\t\t\tparams = None\n\t\treturn '(?P<%s>%s)' % (format_sql(sql)[0:-1], ', '.join(params),)\n\ndef"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\tif not hasattr(sql, \"formatmany\"):\n\t\traise TypeError(\"Can only convert from formatmany to formatmany.  Use.formatmany()\")\n\tif isinstance(sql, bytes):\n\t\tsql = sql.decode(\"utf-8\")\n\targs = []\n\tif SQLParams.in_style is True:\n\t\tif hasattr(SQLParams, \"in_style\"):\n\t\t\tin_style_name = SQLParams.in_style.name\n\t\telse:\n\t\t\t# Convert from formatmany to formatmany to get\n\t\t\t# in-style"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\tfrom..utils import format_params\n\n\t#\n\t# Convert the SQL query to use the more generic ones.\n\t#\n\t#\n\t# The :class:`~collections.abc.List` returns a ``List`` which contains\n\t# the in-style parameters to convert to SQL, and the out-style\n\t# parameters. The format of output parameters is:\n\t#\n\t#     [...](str, params) -> (str, params)*\n\t#\n\t# with the default ``SQLParams.in_style``\n\t#    "}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\treturn format(SQLParams, many_params, f)\n\n\ndef formatmany_from_list(\tfn,\n\t\targs,\n\t\tname=\":\",\n\t\tas_strings=False,\n\t\tin_style=SQLParams.in_style.all().values()):\n\t\"\"\"Format the SQL query for use in a formatmany style.\n\n\tFrom ``list`` to use the in-style parameters ``(x, y, z)`` or ``(x, y, z, w]``\n\tthe values are converted to :class:`str`.\n\n\t:"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\tparser = cPickle.loads(source=source)\n\tin_style = parser.in_style\n\tSQLParams = parser.SQLParams.out_style if in_style == SQLParams.out else parser.SQLParams\n\tif SQLParams.in_style == SQLParams.in and SQLParams.out_style == SQLParams.in:\n\t\tout_style = SQLParams.out\n\telse:\n\t\tout_style = SQLParams.out_style\n\treturn format(\"%s%s%s\" % (in_style, ', '.join('``{"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\tif self._query is None:\n\t\traise Exception('The object this query is associated with is none.  Can\\'t build an SQL query.')\n\n\tdef _format(stmt, table, output, positional, named):\n\t\t\" Convert and format the SQL query.\"\n\t\tif sql is None:\n\t\t\traise Exception('SQLAlchemy:  No SQL script.  Cannot parse the SQL.')\n\n\t\t# If `None` then SQL is to use the output's values.\n\t\tif isinstance(output, Mapping):\n\t\t\toutput = output.get"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\tsql = _strip_prefix(ql, 1)\n\n\t\tfor param in SQLParams.in_style.values():\n\t\t\tif format:\n\t\t\t\tsql = sql.format(field, *formatmany(ql, **format, param, **formatmany))\n\t\t\telse:\n\t\t\t\tsql = sql.format(field, param, **formatmany)\n\n\t\t\tif self.limit > 1:\n\t\t\t\tsql ='LIMIT %s' % sql\n\n\t\t\tif self.with:\n\t\t\t\tsql +='WITH (' + self.with.format"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\tif not type(type_name_map[type]) or not type(type_name_map[type])(new=True):\n\t\traise TypeError(\"Cannot convert :class:`type_name_map[type_]: 'type_name_map[type]'\\\n\t                   is a wrong type.\\n\\nPlease convert:\\n\"\n\t                   \"\\t*SQL.*(*SQLParams)\" \n\t\t)\n\treturn formatmany("}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\tfor result in formatmany_sql(formatmany_sql)(many_params, formatonly_sql, **kwargs):\n\t\tif result:\n\t\t\treturn\n\t\tif (len(formats) == len(formatmany_sql.__dict__) + len(formatonly_sql.__dict__)) and formatonly_sql.__dict__[\"sql\"]:\n\t\t\treturn _formatmany_sql_from_list(formatmany_sql, formats)\n\t\traise ValueError(\"in_parameter list has one or more length\")\n\n\n@formatmany.register_for(\"text\")\ndef formatmany"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\treturn _formatmany(sql, many_params, SQLParams)\n\n_formatmany_impl = formatmany\n__formatmany_impl__ = formatmany\n\n\nclass _FormatOutputMeta:\n\t\"\"\"Metadata dictionary for a :class:`FormatOutput` instance.\n\n\tThe format output dict can be read from using :func:`FormatOutput.read`\n\tor :func:`FormatOutput.read_to_end`.\n\n\tIf the :attr:`.format_to_sql}` option is set then this dictionary will\n\tbe returned. Else an empty dictionary is returned. The"}]}
{"task_id": "62b45df05108cfac7f2109ce", "prompt": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tif p4.root.pyfst.exists(path):\n\t\tif p4.root.pyfs.exists(path):\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\n\ttry:\n\t\tfrom pyfs import __builtin__ as p4\n\t\treturn __builtin__.validate(path)\n\texcept ImportError:\n\t\traise ImportError\n\n#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n\nimport os\nimport pandas as pd\nimport gzip\nimport argparse\nimport tqdm\n\nfrom utils.datasets import"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tif isinstance(path, DirectoryObject):\t# pyfs root\n\t\tos = path.node\n\telse:\t\t\t\t\t\t# path relative to the root\n\t\tos = os.parent\n\treturn\n\n@def validate(self, path_to_read):\t\t# relative path to the original file\n\t\"\"\"\n\tValidate OCFL object to a certain location by the datafile extension (path_to_read).\n\n\tReturns None if valid (warnings permitted), otherwise the path of the file.\t\"\"\"\n\treturn\n\n###############################################################################\n#\n#\n#  "}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tpass\n\ndef _get_obj_key(obj, name):\n\treturn obj.__get__(name, None)\n\ndef _get_fsl_key_as_string(path, name):\n\tif not os.path.exists(path):\n\t\treturn ''\n\telse:\n\t\treturn os.path.getatime(path)[-1]\n\n#-------------------------------------------------------------------------\n\ndef _raise_error(exc, error, name):\n\t\"\"\"\n\tRaise an error to the caller, and log and suppress it if\n\tnecessary.\n\n\tArguments:"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tif self._root is None:\n\t\treturn False\n\telif not os.path.isrelative(self._root):\n\t\treturn False\n\telse:\n\t\tpath = self._root + \"/\" + path\n\t\tif self.__class__ is None:\n\t\t\treturn os.path.isdir(path)\n\t\treturn os.path.listdir(path)\n\t\ndef export_file(self, path):\t\"\"\"\n\tExport OCFL object at path.\n\n\tReturns (path, checksum) where the checksum is optional or None on success."}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tif path[-1] == \"/\":\n\t\treturn True\n\telse:\n\t\tmsg = \"File or directory not found at {}\"\n\t\traise Exception(msg.format(path))\n\n\nclass OCFLModule(module.Module):\n\t\n\t\"\"\"OCFL module\n\n\t   Each module has its own object name and a function to load file\n\t   handlers.\n\n\t\"\"\"\n\n\tdef __init__(self, mod_name, mod_load, mod_validate):\n\t\tsuper(OCFLModule, self).__init__(mod_name"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tprint(\"validate: %s\" % path)\n\tfiles = findfiles(os_path=path)\n\tfor fn in files:\n\t\tf = os.path.join(os_path, \"test\", fn.lower())\t\n\t\ttry:\n\t\t\tself.validateFile(f)\n\t\texcept:\n\t\t\tprint(f,path,\"- Fail\", exc_info=True)\n\t\t\tif hasattr(sys.stderr, 'write'):\n\t\t\t\tsys.stderr.write(\"Failed to validate \" + filename_of(path=path))[1]\n"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tif file_exists(path):\n\t\twith checkfile_mode(path) as f1:\n\t\t\tif not os.access(f1, os.X_OK):\n\t\t\t\traise ValueError('OCFL object at %s is incorrect. If the path is in a valid place it is invalid.', path)\n\t\t\tfor line in f1:\n\t\t\t\tif line.startswith(b\".\") or line.endswith(b\".\"):\n\t\t\t\t\tcontinue\n\t\t\t\tif line.startswith(b\"___\\n\") or line.endswith(b\"___/\"):\n\t\t\t\t\t"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tpath = path.replace(\"\\\\\", \"/\")  or path\n\tif not os.path.isfile(path):\n\t\treturn True\n\n\tif os.path.isdir(path):\n\t\terr = list(\n\t\tfh = FileHandle(File(path)),\n\t\tfilter = lambda file: (\n\t\t\t0,    # ignored\n\t\t\t0,    # ignored\n\t\t\tTrue, # ignore all other files\n\t\t),\n\t\t)\n\t\traise OSError(err)\n\n\tif os.path.isfile("}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tif self.name!= '__init__':\n\t\tif not os.path.isfile(os.path.resolve(path)):\n\t\t\tfrom PIL import Image\n\t\t\twarnings.warn(\"Image file '%s' was not found\" % (path))\n\t\t\treturn False\n\t\telse:\n\t\t\tf = open(path, \"rb\")     # force file creation\n\t\t\treturn loadImage(f.read())\n\telse:\n\t\treturn True if self.format == 'image' else\n\t\ttry:\n\t\t\ts = PIL."}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\tif not isinstance(path, six.text_type):\n\t\tpath = self.path + path\n\treturn os.path.exists(path)\n\n\n@pyfile.Sure(FileOps.read_text).check()\nclass ReadText(FileOps.FileRead):\n\tdef __init__(self, path, **kwargs):\n\t\t\"\"\"\n\t\tRaises FileOps.FileReadError if path name is not valid.\n\t\t\"\"\"\n\t\tsuper(ReadText, self).__init__(**kwargs)\n\t\tif not isinstance(path, six.text_"}]}
{"task_id": "62b45df15108cfac7f2109dc", "prompt": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tif not self._is_frozen:\n\t\treturn prefix + self._name\n\telse:\n\t\treturn prefix\n\t\n\tdef clear_status_cache(self):\n\t\tself._status_cache_dirties.clear()\n\t\tself._status_cache_validators.clear()\n\t\tself._status_cache_cache = None\n\n\tdef _get_status_cache(self):\n\t\t\"\"\"Get status from validator cache, with no caching of the status\"\"\"\n\n\t\tcache = self._status_cache_cache\n\t\tif cache is None"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tif self._validator_status:\n\t\treturn \"%s %s!\" % (prefix + self._validator_status, self._validator_status_string)\n\telse:\n\t\treturn \"-\"\n\n\n@jsonrpc_method(request_type=JSONRequestTypes.VALIDATOR)\ndef validator_data(self, prefix=''):\t\"\"\"\n\tReturns validator data.  Returns None if validator_status is not 200.\t\"\"\"\n\tif self._validator_data:\n\t\tstr_ = self._validator_data.toJson()\t# pylint: disable=no-member"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tm = _getmsgdef(prefix +'status')\n\tif m:\n\t\treturn m.get('value')\n\treturn None\n\ndef list_validator_types(self):\n\t# XXX: is it right? If so, is it right too, so does `list_validator_status`\n\treturn [\n\t\t'pending_proposal',\n\t\t'valid',\n\t\t'expired_proposal'\n\t]\n\nget_validator = deflist(list_validator_types)\n\ndef list_validator_status(self, i):\n\t\"\"\""}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tv = self._validator_status\n\tif v is not None: v[0] = prefix + v[0]\n\treturn ''.join(v)\n\ndef _default_validator_status(status):\t\"\"\"\n\t\"\"\"\n\t\"\"\"\n\tif status is not None:\n\t\treturn status\n\treturn 'Unknown'\n\ndef _setup_validator_status_check(default_validator_status,\n                                  validator_"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tif not hasattr(self, '_status'):\t\t# initialize status to False\n\t\tself.status = True\n\t\tstate_str = \"  \"\n\telse:\n\t\tif (self._status or self.status) and self._status and \\\n\t\t\t\t\t(not self.status_str or self.status_str.test(prefix)) and \\\n\t\t\t\t\tself.status:\n\t\t\t\t\t\tif self.status == 1:\n\t\t\t\t\t\t\tstate_str = \"  \"\n\t\t\t\t\t\t\telif self.status == 2 and self.status_str.test(prefix):\n"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tif prefix is None:\n\t\tprefix = ''\n\treturn prefix + str(self._status)\n\n\ndef _format2(t, fmt, width):\n\tdef formatter(v):\n\t\tif isinstance(v, numbers.Number):\n\t\t\treturn str(decimal.Decimal(str(v), precision=width))\n\t\telif isinstance(v, str):\n\t\t\treturn v\n\t\telse:\n\t\t\ttry:\n\t\t\t\treturn str(int(v), base=10)\t\t\t\t# FIXME: this shouldn't be an error?\n\t\t\texcept"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tprint \"# %s\" % prefix\n\tif self.is_active:\n\t\treturn \"active\"\n\telse:\n\t\treturn \"disabled\"\n\n#===============================================================================\n\ndef get_state(state=None):\n\t\"\"\"\n\tGet the state of the Ethercat hardware.\n\tThis state can be either 'active' or 'disabled'.\n\t\n\tIf state is not specified or 'Disabled', the status is\n\tnot set (and the function can still be called if required).\n\n\tWarning: This function is a coroutine.\n\t\"\"\"\n\tif state is"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\tif status not in validators and prefix:\n\t\treturn status\n\telse:\n\t\treturn \"%s %s\" % (prefix, validator.id)\n\n\nclass Value(validator.Value):\n\t\"\"\"Value represents a string or binary encoded/decoded value.\n\t\"\"\"\n\n\tdef __init__(self, val, **kwargs):\n\t\tself.val = val\n\n\ndef _validate_byte_size(val):\n\tif val == b\"\":\n\t\treturn val\n\tif val[0:1] == b\"x\":\n\t\treturn"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\treturn self._status[:prefix.rfind(',')]\n\n\ndef create_validator_info_map(validator_status_list):\n\t\"\"\" This function will create validator_info maps for validator status\n\t\tentries and all validator status entries in a list, with their\n\t\tkey as the status\n\t\"\"\"\n\n\tvalidator_info_map = {}\n\tstatus_list = sorted(validator_status_list, key=lambda s_status: s_status in (\n\t\t\tVALIDATOR_STATUS['IN_TRANSIT', \"TRANSIT\", \"TRANSIT_OUTLINE\","}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\ttry:\n\t\tstatus = self._state\n\texcept AttributeError:\n\t\tstatus = None\n\tif not status:\n\t\treturn ''\n\telse:\n\t\treturn status.get(self._state_prefix, \"\")[0]\n\ndef status_status(self):\n\t\"\"\"This method returns a dict used to tell if a validator has\n\tbeen checked.\n\t\"\"\"\n\tkey = self._state_prefix + '_status'\n\treturn self._state.get(key, 'pending')\n\ndef status_reason(self):\n\t\"\"\"This method"}]}
{"task_id": "62b45df15108cfac7f2109dd", "prompt": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\tself._log_str = \"\\n\\t[Status]\" + \"\\n\\t[*]\" + prefix + \"\\n\"\\\n\t\t\"`\\t\" + \"***\" + \"\\n\\t\" + self.state + \"\\n\\t\" + \"***\" + \"\\n\\t\" + self._msg_str \\\n\t\t\"**\" + \"***\" + \"\\n**\\n\"\n\n\n\n# TODO: need to test with a more general logger\nclass _BaseLog(object):\n\tdef __init__(self):\n\t\tself.log_count = 0\n"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\tdef prefix_str(prefix=None):\n\t\t\"\"\"Use any prefix in the string (default: \"validation\").\"\"\"\n\t\t# Make sure the user specified a prefix (by setting the value \"validation\"!)\n\t\tif (prefix is not None):\n\t\t\tif (prefix!= 'validation'):\n\t\t\t\traise ValueError('invalid parameter: \"%s\", must be \"validation\",'+\n\t\t\t\t\t'got \"%s\".' % (prefix, str))\n\t\t\telse:\n\t\t\t\tself._set_prefix('validation')\n\n\t\treturn super(Validation_log, self"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\treturn '%s %s'% (self.status.__class__.__name__, status.__repr__(self))\n\n######################################################################\n# Checkpoint class\n# -------------------\n#\nclass Checkpoint(object):\n\tdef __init__(self, log, root):\n\t\tself.log = log\n\t\tself.root = root\n\t\tself.files = {}\n\n\t@classmethod\n\tdef clear(cls):\n\t\tcls.files = {}\n\n\t@property\n\tdef timestamp(self):\n\t\t\"\"\" Return datestamp of last checkpoint, if the value is"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\tif prefix:\n\t\tprefix = prefix +'('\n\n\treturn '\\n' + prefix + 'validation.log:\\n%s' % self.__debug_str()\n\ndef log_to_debug_str(self):\n\tif 'debug' not in self.get_info():\n\t\treturn \"\"\n\treturn str(self)\n\ndef decompile_testfunc(self, func):\n\t\"\"\"\n\tDecompile a test function which was not compiled.\n\tThis may be an out of memory call, or a function-call-return.\n\t\"\"\"\n"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\tfrom xfel.core.utils.misc import logging_error\n\tassert prefix in [self.ERR_VALIDATION_NOT_FOUND, ]\n\n\tif not hasattr(self, prefix):\n\t\tlog_prefix_format = '%s.%s'\n\telse:\n\t\tlog_prefix_format = '%s.%s'\n\tstr = '%s:' % (get_prefix_for_log(self.LOG_VALIDATION_STATUS),\n\t\tself.LOG_VALIDATION_TIMESTAMP.format(\n\t\t\tdate_format='%Y-%m-%d %H:%M:%"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\treturn prefix + self._status_str()\n\n\nclass ValidationError(ValidationError):\n\t\"\"\"Error class by name passed to validator._validate_with_custom_errors().\n\tExample: if the input value is not string, it will return a customized str. \"\"\"\n\n\tdef __init__(self, msg: str, *args, **kwargs):\n\t\tsuper(ValidationError, self).__init__(msg, *args, **kwargs)\n\n\nclass FileError(ValidationError):\n\t\"\"\"Error class by name passed to validator._validate_with_custom_errors().\t\"\"\"\n\n"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\tstatus, name, err_str = '', '', ''\n\tif prefix!= '' and not self.data.get(prefix+self._STATUS)[0][:4]:\n\t\tstatus = self.format_status(prefix) + self.status\n\tif name!= '' and not self.data.get(prefix+self._STATUS_NAME)[0][:4]:\n\t\tstatus_str = self.format_status(prefix) + self.status + ',' + self.data._STATUS_NAME\n\tif err_str!= '' and not self.data.get(prefix+self._STATUS"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\tfor name, val in self:\n\t\tif prefix and name.startswith(prefix):\n\t\t\tret = \"{0}={1}\".format(str(name).upper(), val)\n\t\t\tbreak\n\treturn ret\n\n\n#-------------------------------------------------------------------------------\n\n#-------------------------------------------------------------------------------\n#\n# HELPER FUNCTIONS\n#\n#-------------------------------------------------------------------------------\n\nclass BaseReporter(BaseEngine):\n\n\tdef __init__(self, *args, **kwargs):\n\t\tself.engine = kwargs.pop('engine', None)\t\t# or None\n\t\tself.error_output = kwargs.pop"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\tv = \" \\t\\n: \\t 0 (0.1) ;. \\t\\n:  \\t\\n\"\n\treturn '%s%s \"%s\"' % (prefix, v[:70], v[90:], v)\n\n###################################################################################\n##  MAIN FUNCTION\n## #################################################################################################\n\n\nclass Checkpoint():\n    __slots__ = (\n        'file',\n        'file_full',\n        'timestamp',\n        'action"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\timport re\n\tfrom optparse import OptionParser\n\n\tname ='status_str'\n\tdescription = 'Print current validation status of this job'\n\tusage = re.sub(r'%d[01].*\\n(y\\d+) ',r'\\1\\2\\3\\3\\1',self.run(name+'.txt').replace(r'$\\1','').replace('\\n',''))\n\tusage = re.sub(r'\\[X\\]','\\[X\\]',usage)\n\tusage = re.sub(r'\\[X\\]','\\[XX"}]}
{"task_id": "62b45e135108cfac7f2109f4", "prompt": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\tif not (isinstance(identifier, basestring) and identifier >= 0 and identifier < 2):\n\t\traise Exception(f\"ID {identifier} is out of range\")\n\n\n@attribute(update_internal=IsBoolean())\nclass IDBase64(ID):\n\t\"\"\"This is an abstract base class for anything that can be encoded in\n\tbytes, so you may call it. It is used by the other subclasses to specify\n\tsomething that does not exist. By default it is 'None'.\"\"\"\n\tdef encode(self, base64):\n\t\t\"\"\"Encode data using a standard-"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n\n\ndef read_word_wordindex(filename):\n\t\"\"\" Read the contents of a word-list from a file.\n\n\tReads a list of word-word and word-name from a file.\n\tIt returns the number of each matching word, and the number of words in the\n\tword-word dictionary where each word appears in the file.\n\n\t\"\"\"\n\tfor f in open(filename):\n\t\treader = WordListReader(f)\n\t\tt = 0\n\t\twhile reader.hasNext() and t < reader.getNumberFields():\n\t\t"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\tif identifier[:11] == 'RAR':  # pylint: disable=R0201\n\t\treturn False\n\treturn True\n\n\nclass PXE_Base(PXE_BASE):\n\n\t###############################################################################\n\t#\tImplementation of X.509 support\n\n\t__doc__ = \"PEM-DER support, implemented by PyOpenSSL X.509 standard\"\n\n\t@staticmethod\n\tdef getSerialNumber():\n\t\t\"\"\"\n\t\tThis is a cached version of certificate.getSerialNumber,\n\t\tbut it is not available on all platforms.\n\n\t\tReturns:\n\t\t"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\treturn True\n\n# frozen_string_literal: true\n\nFactoryBot.define do\n  factory :tag do\n    sequence(:title) { |n| \"test#{n}\" }\n    string \"This is a long string that is not 100%% in length\"\n\n    description 'This is a long string that is not 100%% in length'\n  end\nend\n\n#include <stdio.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\n//char *f_get"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\tfor character in identifier:\n\t\tif identifier.find(':') >= 0:\n\t\t\treturn False\n\t\tif '.' in identifier:\n\t\t\treturn False\n\t\tif '/' not in identifier:\n\t\t\treturn False\n\t\tif 'a' not in identifier:\n\t\t\treturn False\n\t\tif 'p' not in identifier and's' not in identifier:\n\t\t\treturn False\n\treturn True\n\n\nclass BaseTokenReference(object):\n\tdef __init__(self):\n\t\tself._base_identifier = identifier = \"\"\n\t\tself.identifier"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\tif not self.check():\n\t\treturn False # pylint: disable=not-implemented\n\tidentifier = identifier.strip(self._encoding)\n\tif identifier.lower().startswith(\"<error> \"):\n\t\treturn False\n\telse:\n\t\treturn identifier.lower() in self._identifier_strings\n\n\ndef _fix_identifier(identifier_string):\n\t\"\"\"\n\tConvert a identifier to upper case first.\n\t\"\"\"\n\tr = identifier_string.lower()\n\treturn r[0].upper()\n\n\nclass TokenizingDict:\n\t\"\"\"\n\t"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t# The following line adds a warning if identifier is not valid: https://github.com/ytdl-org/youtube-dl/pull/2588\n\tif identifier.upper() not in self.valid_urls:\n\t\treturn False\n\treturn True\n\n\ndef remove_url_prefix(url, prefix):\n\t\"\"\"\n\tRemove a specific prefix from a URL string.\n\t\"\"\"\n\t# Replace all occurrences of `prefix` with this one, making the result consistent.\n\treturn url.replace(\"`\" + prefix + \"`\", \"\", 1)\n\nimport"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\tfor tag in self._knowntags:\n\t\tif tag.startswith(identifier):\n\t\t\treturn True\n\treturn False\n\n\nclass _Tag:\n\tdef __init__(self, tag, value=None):\n\t\tself.tag = tag\n\t\tself.value = value\n\n\n# Used by the Tag class to convert from a string into a python value\n# It does this since the string is always one word in each tag.\nclass Tag(str):\n\tdef split(self):\n\t\treturn [c.strip() for c in self.split(\" \")"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from EMAN2 import copy_function\n\tif not self.is_valid():\n\t\tself.is_valid(identifier)\n\n\nclass Bamreader(object):\n\tdef __init__(self,\n\t\tin_folder,\n\t\tout_folder,\n\t\tmax_matches=None,\n\t\tmin_score=1.0,\n\t\tthreshold=0.05,\n\t\tmode=\"bdb\",\n\t\twrite_all_sequences=False,\n\t\toutput_folder=None):\n\n\t"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\tif self.is_name(identifier):\n\t\treturn True\n\tif identifier in self.keys:\n\t\treturn True\n\traise ValueError(\"Identifier {} is invalid\".format(identifier))\n\n################################################################################\n# Name resolution with subkeys for a name in this implementation\n#\n# It is advised this may require further maintenance of internal knowledge about\n# the name resolution process.  The only reason I want the name resolution to\n# be robust to name conflicts is to try to preserve backwards compatibility by\n# relying on the user setting a different conflict resolution strategy.\tTo do that,\n"}]}
{"task_id": "62b45e145108cfac7f210a07", "prompt": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tif len(self.spec_version) == 0:\t# We are going to check the version\n\t\tprint(\"Spec version validation failed; use the first version\")\n\telse:\n\t\tprint(\"Type validation failed\")\n\nusing UnityEngine;\nusing System.Collections;\nusing System.Linq;\nusing System;\n\npublic class FPSTriggerTrigger : ATrigger\n{\n\t#region Fields\n\n\t// The trigger\n\tprivate FPSTriggerTriggerSource triggerSource = new FPSTriggerTriggerSource();\n\n\t// The source\n\tprivate FPSTriggerTrigger trigger = null"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tif extract_spec_version:\n\t\tself.__version = self.__spec_version\n\telse:\n\t\t# Find the first non-empty version: if we had any (either empty or\n\t\t# invalid), then the version must be there:\n\t\tfor entry in inventory:\n\t\t\tif os.path.exists(entry):\n\t\t\t\tself.spec_spec_version = entry[0]\t\t\t\t\t\t# type: str\n\t\t\t\tif self.spec_version.strip(\"0\") == '\":\n\t\t\t\t\tself.parsed_spec_version = 'N/"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t# We do not validate the type, so the type may be a valid value from a\n\t# specific package version (which is in the same package)\n\tif not is_valid_type(inventory):\n\t\tself.error('Inventory contains invalid items.  Use a valid type or '\n\t\t\t'a valid version of a valid package.')\n\n\ttry:\n\t\tif not self.can_have_version(inventory):\n\t\t\tself.error('Inventory does not contain any inventory '\n\t\t\t\t'te a valid version.')\n\n\texcept RequirementError:\n\t"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tif extract_spec_version:\n\t\treturn True\n\tfor key in sorted(inventory.keys()):\n\t\tif inventory[key]!= None:\n\t\t\treturn True\n\treturn False\n\n\ndef extract_version_id(inventory, extract_spec_version=True):\n\t\"\"\"Returns a version_id if defined\n\n\t@param inventory: inventory to get the version_id from\n\t@param extract_spec_version: When set to true then version can be determined.\n\t@type inventory: dict\n\t@type extract_spec_version: boolean\n\n"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tvalid = []\n\tdef set_spec_version(inventory):\t\t\"\"\"Set spec_version to the value found in the inventory file. This must be\n\teither an integer or a string.\n\t\"\"\"\n\t\tif str(inventory[1])!= \"number\":\n\t\t\treturn inventory\n\t\telse:\n\t\t\tlog.info(\"Found the spec version in the inventory file\")\n\n\t\tspec_version = inventory[0]\n\t\tif \"string\" in spec_version:\n\t\t\tlog.info(\"Specifying the spec version to be %s\", spec_"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tif self.spec_version == '1.2.3.4' or self.spec_version == '1.2.3.4.0':\n\t\tif extract_spec_version:\n\t\t\traise Exception(\"Can't extract spec version!\")\n\n\n# Note: this function is no longer used, you should delete it\n# @staticmethod\ndef create_spec_version_from_version(version, extract='spec'):\n\t'''\n\tTakes a string version, converts it to the version\n\treturned in self.spec_version.\n\tWhen'spec' is"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t# if the version is not present in the input inventory, then\n\t# we return true since that should result in more information to\n\t# usally generate the error message\n\tif not self.spec_version:\n\t\tself.spec_version = inventory.get(Inventory.PACKAGE)\n\treturn bool(self.spec_version==inventory.get(Inventory.VERSION_STRING))\n\n/*\n * jdct_h.h\n *\n * Copyright (C) 1991-1998, Thomas G. Lane.\n * This file is part of the Independent JPEG Group's software.\n"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t# this method should never be in here\n\t_validate_spec_version(inventory, self.spec_version)\n\n\t__version__ = inventory.__version__\tif isinstance(inventory, (dict, list))#if the type isn't a dict\n\tspec_version = inventory.get(\"spec_version\")\n\n\tif not __version__.isdigit() and __version__==None:\t#if there's no spec_version in inventory\n\t\treturn False, \"No spec_version specified\"\n\n\tif not inspect.isclass(spec_version) and not inspect."}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\tif extract_spec_version:\n\t\tfor i in range(self.spec_version.get_type_count()):\n\t\t\tif self.spec_version[i] == self.spec_version:\n\t\t\t\treturn True\n\n\treturn False\n\n\ndef import_object_from_template(\n\t\tdata,\n\t\tinput_template,\n\t\ttemplate_file=None,\n\t\tparse_spec_version = False\n):\n\t'''\n\tIMPORT object\n\t'''\n\tmetadata = _parse_yaml_object(data)\n\tmetadata_"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t_check_spec_version(self, inventory, self.spec_version)\n\n\tvspec = self.spec_version\n\tfor _meta in self.meta:\n\t\tif _meta:\n\t\t\tvspec = _meta.version\n\t\tbreak\n\n\tif not inventory:\n\t\traise InventoryError(\"Inventory was None, and could not get version number.\")\n\n\tif vspec.major <= self.get_major_requirement() or not self.get_release_requirement(vspec):\n\t\traise InventoryError(\"A newer version of inventory is detected"}]}
{"task_id": "62b45e145108cfac7f210a09", "prompt": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t# Verify the manifest and files are valid\n\tif not os.path.exists(self.manifest_file):\n\t\traise ValueError(\"Manifests directory not found.\")\n\tif not os.path.exists(self.digests_file):\n\t\traise ValueError(\"Digests directory not found.\")\n\tif not os.path.exists(self.digest_file):\n\t\traise ValueError(\"Digests directory not found.\")\n\t# Verify that the files is not too large\n\tmax_digest = int(MAX_MD_FILES * 2.6)\n\tdigest_files,"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\tif len(manifest_file) > 0 and manifest_file.digest in digests_used:\n\t\t\treturn False\n\n\t\tif manifest_file.digest == MANIFEST_DIGEST:\n\t\t\treturn True\n\n\t\treturn False\n\n# =============================================================================\nclass Manifest:\n\t\"\"\"\n\tManifest is used to locate an image manifest and find its file names.\n\t\"\"\"\n\tdef __init__(self, name=None, version=None):\n\t\tself.manifest = None\n\t\tself.manifest_file"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tif not isinstance(manifest_files, six.string_types):\n\t\traise TypeError(\"manifest_files must be a string.\")\n\tfor manifest_file in manifest_files:\n\t\tif os.stat(manifest_file).st_mode & stat.S_IFDIR:\n\t\t\traise ValueError(\"manifest file \"+ manifest_file + \" is a dir.\")\n\tif not manifest_files:\n\t\traise ValueError(\"missing manifest_files.  \"\n\t\t\t\t\t\t\"Did you get an exception?\")\n\t# check if the file and its digests are present and\n\t#"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor digest in digests_used:\n\t\tif digest.startswith(self.digest_prefix):\n\t\t\tlogger.debug(\"Digest %s is requested from manifest.\", digest)\n\t\t\treturn True\t\n\treturn False\n\n\ndef get_metadata_from_manifest(file_name, prefix):\n\t# First we check if the file exists and is one of the three mandatory sections:\n\t#     http://www.w3.org/TR/xmldsig-core/struct/nsxns.html#nsxns.nsxns.manifestMetaData\n"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor filepath in manifest_files:\n\t\t# If this is the first manifest then return immediately.\n\t\tif'self' in filepath and filepath['_id'] == ':latest-':\n\t\t\treturn True\n\t\tself.check_digest_and_file_exists(filepath, digests_used)\n\t\t# otherwise check if an older manifest still exists\n\t\telse:\n\t\t\tif not os.path.exists(filepath):\n\t\t\t\tmessage = 'Can\\'t open: %s: %s: %s' % \\\n\t\t\t\t\t(filepath['filename'], filepath"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor path in manifest_files:\n\t\tif path is None: continue\n\t\tif _digest_files_present_for_path(path): continue\n\t\tif _digest_files_in_manifest_used_for_path(path): continue\n\t\tif _digest_files_used_for_path(path): continue\n\t\tif path not in digests_used:\n\t\t\tcontinue\n\n\t\tif not self.digest_exists(path, digest_type=DIGEST_FOR_HASH):\n\t\t\tprint('\\tgoto_exceptions.utils.exceptions."}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\tif not manifest_file.digests_present_and_used():\n\t\t\tlogging.warning('All digests in manifest of the same file are'+\n\t\t\t\t\t\t'present. Use -C to skip them.')\n\n\t# Check all digests that are not used\tinside manifest or manifest file are not defined\n\tif (digests_used is None) or (digests_used == []):\n\t\treturn\n\n\t# Check the digest is defined as defined in manifest but not the manifest file\n\tfor digest in digest"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tcheck_digests_present_and_used_with_metadata(self, manifest_files, digests_used)\n\ndef test_check_md_exists():\n\t\"\"\"\n\tTest that the md file exist (not exist if it is not there before).\n\t\"\"\"\n\tcheck_md_exists(os.path.realpath(__file__), \"\", None, \"\")\n\ndef test_check_md_not_exists():\n\t\"\"\"\n\tTest that the md file does not exist (exist to something else).\n\t\"\"\"\n\tmd_file = os.path.join("}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\terror_lines = self.check_manifest(manifest_files, digests_used)\n\tif error_lines:\n\t\tmessage = \"There were errors in manifest. Please reload from the latest manifest.\" \\\n\t\t\" Please use the 'install-complete' script to complete installation.\"\n\t\tprint(str(error_lines))\n\t\tprint(\"\\n\")\n\telse:\n\t\tprint(\"\\n\").join(message if len(message) > 0 else \"\")\n\treturn error_lines\n\n\"\"\"\nFunction:\nCheck a manifest. The 'install-complete' script"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\tfor file_name in manifest_files:\n\t\tif file_name in digests_used:\n\t\t\traise Exception(\"Error: the file has already been used to check\")\n\t\telif (file_name.endswith((\".manifest\", \".md\"))):\n\t\t\tcontinue\n\t\tdigest = self._get_digest_for_file(file_name)\n\t\tif digest is None:\n\t\t\traise Exception(\"Invalid digest: {0}.\".format(file_name))\n\t\tif digest in digests_used:\n\t\t\treturn\n\t\tdigests_"}]}
{"task_id": "62b45e165108cfac7f210a16", "prompt": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\ndef validate_as_inventories(self, inventory) -> InventoryValidator:\n\t\"\"\"\n\tGiven a InventoryValidator object, returns a boolean. It raises a\n\tInventoryValidationError if it can't parse the input inventory.\n\t\"\"\"\n\traise inventory_validation_error\n\nclass _BadCache(Exception):\n\tpass\t\t\t\t\"\"\" no good reason to go here \"\"\"\n\tdef __init__(self, filename):\n\t\t\"\"\"\n\t\tThis is to work around a bug in picklepy's import.\n\t\t\"\"\"\n\t\tif os.path.isfile(filename"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\treturn (self.prior == prior) and (not getattr(prior, \"is_dirty\", False))\n\n\tdef check_for_reordering(self, inventory):\n\t\t\"\"\"Check that inventory has a non-primary version of the prior.\n\nIf a primary inventory version has already been created, the prior version\nis returned (with the primary version as a secondary version) instead.\n\"\"\"\n\t\tdef is_primary(self, inventory):\n\t\t\t# An Inventory is either an Inventory inventor from a prior or\n\t\t\t# from a previous version of the current inventory"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tif self.prior is not None:\n\t\tvalid = self.prior instanceof InventoryValidator\n\t\tfor i, p in enumerate(prior.get_all_inputs()):\n\t\t\tif not valid and self.prior.is_version_prior(i) and prior.is_version_prior(i):\n\t\t\t\traise TypeError(\"Versions prior {} were assigned prior {}\", i, prior.prior)\n\tself.prior._validate()\n\ndef get_current_invdef_def(object, inventory, inventory_instance, inventory_instance_class):\n\t\"\"\"Returns the current invdef"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tif prior is not None and isinstance(prior, InventoryValidator):\n\t\traise ValidationError('%s.%s must be an InventoryValidator class' % (prior.__class__, self.__class__.__name__))\n\treturn\n\ndef validate_as_preupdate_version(self, preupdate):\t\"\"\"\n\tCheck that preupdate is a valid preupdate version of the current inventory\n\tobject.\n\nThe input variable preupdate is also expected to be an InventoryValidator object\nand both self and preupdate inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tif preupdate"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\ttry:\n\t\tif prior == self:\n\t\t\traise AttributeError(\n\t\t\t\t\"self prior version is the same as prior version of the \"\n\t\t\t\t\"current inventory object\")\n\texcept AttributeError:\n\t\traise KeyError(\n\t\t\t\"prior version was found from different versions\",\n\t\t\t\"current inventory object\")\n\n@classmethod\ndef new_inventory_object(cls):\n\t\"\"\"Newly created inventory object, initialized in initialize()\n\t\"\"\"\n\ttry:\n\t\treturn cls()\n\texcept (NotImplementedError, TypeError):\n\t\t# the first two checks"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tif not prior:\n\t\traise BadInput(\"must specify prior version\")\n\tif self.version < prior.version:\n\t\traise BadItem(\"bad prior version\")\n\tif self.version == prior.version:\n\t\traise InvalidInput(\"prior version %s is not a compatible with inventory\", self.name)\n\treturn True\n\nclass InventoryValidator(type):\n\tdef __init__(cls, name, description, schema, versions, validate_as_prior_version):\n\t\t\"\"\"Set the validator object's name.\n\n\t\tIf any part of the object"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t# no warning if prior is not a valid InventoryValidator object\n\tif not isinstance(prior, InventoryValidator):\n\t\traise util.ValidationError('Incorrect item type for prior. '\n\t\t'Expected InventoryValidator, got %s' % str(prior))\n\n\t# validate version and reject if version is greater than current\n\tif prior.get_version() > self.get_version():\n\t\traise util.ValidationError('Incorrect item version for prior. '\n\t\t'Item version for prior must be equal to current version.')\n\ndef validate_prior(inventory):\n\t# check"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\tif not isinstance(prior, InventoryValidator):\n\t\traise ValueError('invalid input')\n\n\tself._prior = prior\n\ndef check_for_version_conflict(self, inventory, new_predecessor):\n\tif not self.can_conflict_with(new_predecessor):\n\t\traise NoVersionConflictException(\n\t\t\t'Cannot determine if a new version of %s can be a duplicate of %s'\n\t\t\t% (self, new_predecessor)\n\t\t)\n\ndef can_conflict_with(inventory, proposed_prior):\n\tif not"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\treturn isinstance(prior, InventoryValidator)\n\n\n@validation.validate_version(7)\ndef validate_as_prior_version_with_version(self, prior, version, name=None):\n\treturn isinstance(prior, InventoryValidator) and prior.prior_version >= (version, version)\n\n\n@validation.validate_version(7)\ndef validate_as_prior_version_with_version_and_prior(self, prior, version, name=None):\n\tif not prior:\n\t\treturn False\n\tif isinstance(self, InventoryValidator):\n"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\ttry:\n\t\tprior_validators = prior.predicates['validators']\n\texcept KeyError:\n\t\traise ValueError(\"No valid predications defined.\")\n\t# If prior has no validators, the prior version is a prior.\n\tif len(prior_validators) == 0:\n\t\t# In case any validator is specified in the predicated field then\n\t\t# the prior version is a prior.\n\t\tprior = self._prior()\n\treturn bool(prior) if prior is not None else False\n\n\ndef _get_predicated_fields(pred):\n"}]}
{"task_id": "62b45e165108cfac7f210a17", "prompt": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\treturn {version: set(inventory.items())}\n\ndef get_logical_path_set(inventory, version):\t\"\"\"\n\tReturn a set of content_files associated to the inventory version.\n\nReturns a set: logical_path_in_state -> set(content_files)\n\nThe set of content_files on disk for version in inventory will include\nreferences to duplicate files in later versions than the version being\ndescribed.\t\"\"\"\n\treturn {version: set(inventory.items())}\n\ndef get_logical_path_list(inventory, version):\t\"\"\"\n\t"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tres = {}\n\n\tif version == 1:\n\t\t# The first version is only for inventory.  It's not an error\n\t\t# to have multiple versions in inventory.\n\t\treturn res\n\n\t# The inventory consists of at least three files, one for all inventory versions.\n\tif len(inventory) < 3:\n\t\tprint \"Inventory version 0, and inventory version 1 did not include 1 file.\"\n\t\treturn res\n\n\t# Determine the path to the inventory file, relative to the inventory\n\t# version.\n\tversion_file = inventory[version"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tset_in_version = set()\n\tfor i in range(len(inventory)):\n\t\tset_in_version.add(inventory[i][\"logical_path\"])\n\t\tset_in_version.add(inventory[i].get(\"file_name\"))\n\t\tset_in_version.add(inventory[i].get(\"location\"))\n\t\tset_in_version.add(inventory[i].get(\"project_identifier\"))\n\t\tif version == VERSION_2_5:\n\t\t\tset_in_version.add(inventory[i].get(\""}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tresult = {}\n\n\tif version in inventory.states:\n\t\tif inventory.states[version] == InventoryState.FILE_ADDED:\n\t\t\tfor state in inventory.states+inventory.states[version].items():\n\t\t\t\tresult[state.filename] = None\n\t\telse:\n\t\t\tresult[inventory.states[version].filename], result[version] = set(inventory.states[version].items())\n\treturn result\n\ndef get_item_file_content(inventory, entry):\n\t\"\"\"\n\tFind the content of the given entry in the version metadata"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\treturn {  # pylint: disable=invalid-name\n\t\tu'': {},\n\t\t\"\": {},\n\t\t\"files\": set(),\n\t}\n\n\n# def get_snapshot_map(inventory, version):\t\"\"\"\npass\n\n\n# Get the set of state files from the inventory for version in inventory.\n# Also returns the set of files to snapshots to avoid rebuilding the inventory\n# if there are any.\ndef get_snapshot_set(inventory, version):\n\t\"\"\"\n\tGet the set of state files containing snapshots for version in\n\tinventory.\t"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t_physical_version = version.get_physical_version()\n\t_inventory = inventory\n\t_physical_version.verify_integrity(versions.physical_version(_version=(_physical_version)))\n\n\t# Loop through items found in inventory\n\titems = inventory.get_items(_version)\n\tfor item in items:\n\t\tis_existing = False\n\t\tfor _item in item.content:\n\t\t\tif _item.get_content_version() == _physical_version:\n\t\t\t\tis_existing = True\n\t\t\telif is_existing:\n\t\t\t"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tlogical_path_path = {}\n\tfor sub_obj in inventory.sub_obj_ids:\n\t\tlogical_path = {\n\t\t\t'logical_path': {},\n\t\t\t'directory': {},\n\t\t\t'content_files': {}\n\t\t}\n\t\tlogical_path[sub_obj] = {}\n\t\tset(logical_path['directory'])\n\t\tfor sub_type in inventory.type_defs[sub_obj]:\n\t\t\tlogical_path[sub_type]['directory'] = {}\n\t\t\tfor logical_obj in inventory.get_"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tif len(inventory) == 1:\n\t\treturn {}\n\tassert inventory[0].get_resource() in (\"folder\", \"virtual_folder\")\n\n\tlogical_path_in_state = {}\n\n\tlogical_paths_container = inventory[0].get_logical_path_map()\n\tfor logical_paths in inventory[1:]:\n\t\tlogical_paths_container[logical_paths[0]] = set(logical_paths)\n\n\tfor logical_paths in inventory[2:]:\n\t\tfor path in logical_paths:\n\t\t\tassert path"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tcurrent_version = inventory.get(\"version\", 0)\n\tversion_name = _get_version_name(current_version)\n\t# NOTE: use get_logical_path to get a valid path\n\treturn get_logical_path(inventory, version_name)\n\n\ndef get_logical_path_in_state(inventory, version, content_set=None,\n\t\t\t\t\t\t\t index_value=None, *values, skip_duplicate_files=True, *args, **kwargs):\n\t\"\"\"\n\tGet a set of logical paths corresponding to the contents of an inventory\n"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\tlogical_paths = {}\n\n\t# The logical path to the file on disk.\n\tphysical_path = inventory.get_physical_path_map()\n\n\t# For version 0, try \"get_version_file\" and return a path.\n\tif version < 0:\n\t\tphysical_path = inventory.get_version_file()\n\t\tif physical_path is None:\n\t\t\traise ValueError(\"no version in inventory or version not in inventory\")\n\n\t\tlogical_paths[\"version\"] = physical_path\n\t\tlogical_paths[\"file\"] = physical"}]}
{"task_id": "62b45e175108cfac7f210a19", "prompt": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tbase_location = '{}/sight/{}/sight/{}'.format(\n\t\tfixity.base_location,\n\t\tfixity.file_extension,\n\t\tfixity.source_file)\n\tcheckfs = True\n\tif checkfs:\n\t\tbase_location += '/sight/{}'.format(fixity.id)\n\t\tfor fixation in fixity.fixations:\n\t\t\tfor fixation_id in fixation.fixations:\n\t\t\t\tbase_location += '/' + fixation_id\n\n\tif not base_location:"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\titems = [self.fixity.copy() for _ in fixity]\n\tif len(manifest_files)!= len(items):\n\t\tself.error(\"'%s': %s fixed size. Fixity has %i files with fixity \"\n\t\t          \"in its manifest.\" % (self.__class__.__name__,\n\t\t                                  items[0], len(manifest_files)))\n\treturn len(items"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tfor entry in fixity.bplist:\n\t\tif not entry.name.lower().endswith('.manifest'):\n\t\t\traise ValueError('Fixity file not declared in filelist')\n\n\tfor p in manifest_files:\n\t\tfor entry in fixity.bplist:\n\t\t\tif entry.name.lower().endswith('.manifest'):\n\t\t\t\traise ValueError(f'Filename {entry.name} in fixity file'\n\t\t\t\t\t\t\t\t\t' does not match file list item \"{p}\" in manifest file')\n\n\tfor p in fixity.bplist:\n\t\tif p"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tif len(fixity.files) < 1:\n\t\traise error.NoFixityFiles()\n\n\tfor fixity_file in fixity.files:\n\t\tif isinstance(fixity_file, fix_types.File):\n\t\t\tassert fixity_file.name!= fix_types.Fixity.file_names[-1]\n\t\telif isinstance(fixity_file, fix_types.Header):\n\t\t\tassert fixity_file.length >= 0\n\t\t\tassert fixity_file.version_number == 0\n\tfrom fix_types.File"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t# First, see if the check above does not work.\n\t# If so, make sure the manifest file appears more.\n\tself.fs.check_for_fixity(fixity.fixity_file)\n\n\t# Now check for invalid manifest blocks.\n\tmanifest_files.check_manifest()\n\n\t# If a manifest block was found, make sure the file\n\t# listed in FIXITY.fixity_file is a valid fixity file.\n\t# Do not check the file list in self.fs, as it might be\n\t# something else. That would"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tif fixity.block_name in manifest_files:\n\t\traise ValueError(\"fixity block %s is referenced by file %s in\" \\\n\t\t\" inventory file %s. This does not imply that the structure\" \\\n\t\t\" is all the same.\" % (fixup_name_from_xml_string(fixup),\n\t\tfixup.manifest_basename, fixity.manifest_basename))\n\n\n#--------------------------------------------------\n#\n# File/folder handling\n#\n#--------------------------------------------------\n\nclass FixityFile(filetypes.fixity_file_base):\n"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tfor block in fixity.block_list:\n\t\tfor file in fixity.block_list[block]:\n\t\t\tif file == self._manifest_filename:\n\t\t\t\tprint(\"WARNING: Unsure how your manifest will be created.\\n\"\n\t\t\t\t\t \"          It should be the same name when using a fixity.\",\n\t\t\t\t\t      color.RED)\n\n\ndef check_incompatibility(self, fixity, manifest_files, file, prefix='', version=None):\n\tout = \"\"\n\tif manifest_files"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tfile_names = [f for f in fixity['files'] if f['type'] == 'file'] \n\tfile_list = []\n\tfor name in file_names:\n\t\tif name =='system_type' or name =='system_version':\n\t\t\tcontinue\n\t\tassert fixity, name +'is not a valid fixity'\n\t\ttry:\n\t\t\tassert fixity['type'] == 'dir'\n\t\texcept ValueError:\n\t\t\tassert fixity, fixity = fixity['dir']\n\t\tassert (name, os.path"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tassert None not in manifest_files\n\tfor file_name, file_file, fixity_type, _ in fixity:\n\t\tassert file_name not in self.manifest_files\n\t\t\n\t\tassert file_file.path in manifest_files\n\t\tassert file_file.version in self.__manifest_versions\n\n\t\tassert self.__manifest_versions[file_file.path] == fixity_type.name\n\npackage android.app.trust;\n\nimport android.app.trust.ISignBehaviorManager.Stub;\n\n/*  JADX ERROR: NullPointerException"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\tfor k, v in fixity.items():\n\t\tif not self.check_manifest(v, manifest_files):\n\t\t\traise Exception(\"Incorrect fixity block in inventory\")\n\n\ndef check_manifest(b, manifest_files):\n\tfor e in manifest_files:\n\t\t# For compatibility reasons, only check files from the\n\t\t# extension of the manifest, not the name.\n\t\tif e.key not in os.listdir(\".\"):\n\t\t\te.key = fix_name(e.key)\n\t\t\te.path = os."}]}
{"task_id": "62b463153879012d19481498", "prompt": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tfiles = []\n\tos.walk(path, func.__del__)\n\tfor f in files:\n\t\tprint f\n\t\tfiles.extend(f)\n\n\nif __name__ == \"__main__\":\n\tmain()\n\n/**************************************************************************\n * Copyright(c) 1998-1999, ALICE Experiment at CERN, All rights reserved. *\n *                                               "}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\treturn [f.strip() for f in sorted(list(open(path, 'r').readlines()))]\n\n\n@def files_list_dir(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\treturn [f.strip() for f in list(open(path, 'r').readlines())[:2]]\n\n\n@def files_list_not_found(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\treturn [open(path, 'w').readlines()]\n\n\n@def dir_exists(path"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tif not path:\n\t\treturn []\n\treturn read_file_list(path)\n\ndef file_list(path):\t\"\"\"\n\tGet files in `path`\t\t\"\"\"\n\tif not path:\n\t\treturn []\n\treturn list(mapped_dicts(get_file(path)))\n\ndef get_file(path):\t\"\"\"\n\tFinds, or fetches, a specific file in `path`\t\"\"\"\n\tif not path:\n\t\treturn (None, None)\n\t\t# return not f.closed and mapped_dicts("}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\twith context() as c:\n\t\tp = os.path.join(path, \"test.txt\")\n\t\tf = open(str(p), \"w\")\n\t\tfor line in f:\n\t\t\tif line.startswith(\"==== \"):\n\t\t\t\tf.close()\n\t\treturn p\n\n\nclass FileSet(Set):\n\tdef __init__(self, *args, **kwargs):\n\t\tif isinstance(kwargs, dict):\n\t\t\tSet.__init__(self, *args, **kwargs)\n\t\telse:\n\t\t\tsuper(File"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\treturn os.listdir(path+'/'+os.path.basename(path))\t\t# pylint: disable=no-member\n\n\ndef add_metadata(path, tags):\n\t\"\"\"Add metadata to file (if applicable)\"\"\"\n\traise Exception(\"No meta-tag defined! (trying as tags from: %s)\" % \", \".join(tags))\n\n\ndef _add_source_directory(path, tag, path_to_file, path_to_dir, dir_sep):\n\t_add_source_directory(path, tag, tag, path_"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\treturn os.listdir(path)\n\ndef files(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\treturn os.listdir(path)\n\ndef files_list_all(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\treturn list(list(os.listdir(path)) + list(os.listdir(path + \"**\")))[:10]\n\n#ifndef\t_TSK_T4_H\n#define\t_TSK_T4_H\n/*\n**\t$VER: t4."}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\treturn glob.glob (os.path.join (path, \"*\"))\n\n\ndef file_exists(filename):\t\"\"\"\n\tReturns True if the file exists\n\t\"\"\"\n\tcmd = ['sudo','system', 'file', '--check', filename]\n\treturn any((r.strip() == '' for r in cmd))\n\n// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.\npub fn parse_http_generic_error(\n    response: &http::Response<bytes::Bytes>,\n) -> Result<"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tflist = list(\n\t\topen(os.path.join(path, path), \"r\") for path in glob.glob(path)\n\t)\n\treturn sorted(flist)\n\ndef load_data_from_url(path):   \"\"\"\n\tLoad all dataset_lists from the given URL \"\"\"\n\nprint(\"Loading files...\")\n\nwith open(path) as flist, open(str(path), 'r') as r:\n\tfor line in flist:\n\t\tline = line.replace('\\n','')\\\n\t\t\t\t\t.replace"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\traise NotImplementedError()\n\ndef make_app_path(app_name=''):\n    if app_name:\n        app_name_base = os.path.basename(app_name)\t# XXX This is to say the app name is not the basename!\n    else:\n        app_name_base = os.path.basename(sys.argv[0])\n\n    full_path = os.path.join(os.path.dirname(os.path.dirname(__file__)),'"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\tfiles, ext = None, None\n\tif path in _files_path:\t# _files_path[path] exists\t# TODO: check if this is true\n\t\tfiles, ext = _files_path[path].split(None, 2) if len(_files_path[path].split(None, 2)) >= 2 else [None]\n\tif files!= None:\t# the user did not change the path\n\t\treturn files, ext\n\treturn _files_path, path\n\ndef files_list_sorted(path, files, ext='.txt'):\n"}]}
{"task_id": "62b463153879012d1948149a", "prompt": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\txml_names = xml_dir.read(py_name)\n\tfor xml_name in xml_names:\n\tretry = True\n\n\tif os.path.splitext(xml_name)[1] in (\"xml\", \"xsd\", \"xml/x-abi\"):\n\t\txml_dir += \"/\"\n\n\tfor xml_name in xml_names:\n\t\tif xml_name.endswith(\".xml\") or xml_name.endswith(\".zip\"):\n\t\t\tcontinue\n\t\tfor xml_name in _get_valid_file(xml_name, xmls):"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\tif xml_filename!= \"\" or len(files) > 0:\n\t\titems = []\n\t\tfor filepath in files:\n\t\t\tpackage_name = os.path.normpath(os.path.abspath(filepath))\n\t\t\tpackage_filename = xml_filename + \"xml\" + package_name + \".xml\"\n\t\t\t# Check if the filename already exists or the file can be overwritten\n\t\t\tif os.path.exists(package_filename):\n\t\t\t\tprint(package_filename)\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\titems.append({ \""}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t_xml_files = [source]\n\tfor filename in files:\n\t\tp = _xml_parse_file(filename, _xml_files)\n\t\tif p is not None:\n\t\t\treturn p\n\t\t_xml_files.append(filename)\n\ndef _load_xml_from_directory_path(source, path, xmls):\n\t\"\"\" Load XML from a path, and return the data in dict format.\n\nParameters\n----------\nsource\t# Package of the XML to load\npath\t# Path to the directory\nxmls\t# Dict of"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\treturn dict(zipfiles.zipfits(source, xmls, files))\n\ndef _group_files_by_path(source, package_dir=None):\t\"\"\"\n\tImports files based on the given directory\n\nGroups files and returns data in dict format.\n\nParameters\n----------\nsource : str\n    Directory containing files/folders\npackage_dir : str\n    Directory with XML files\n\nReturns\n-------\ndict\n    Key: name of the XML files\n    Value: Package\t\"\"\"\n\tos_dir, _ = get_"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\twith Zipfile(base_filename) as zip_file:\n\t\tzip_file.extractall(xmls)\n\t\tzip_file.close()\n\t\tfor zf_file in files:\n\t\t\tif zf_file.basename == \"':\t# this is not a file\n\t\t\t\txml_filename = zf_file.basename\n\t\t\t\tbreak\n\t\t\telif zipfile.is_zipfile(zf_file):\n\t\t\t\txml_filename = zf_file.basename\n\t\t\t\tbreak\n\t\treturn xml_filename, zip_file.name"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\tif not isinstance(xml_filename, str):\traise ValueError, \"Source is not string!\"\n\timport zope.vendor.xml.reader as Z\n\n\tfile_dict= Z._get_xml_files(xml_filename, files)\n\tif len(file_dict)!= 1:\n\t\traise ValueError, 'Unable to find XML file! Only 1 file given.'\n\tdoc= file_dict[0]\n\tif doc[0].find(\"==\")!= -1:\traise ValueError, 'XML file must have close element, but is \"'+doc["}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\ttarget_folder_dir = xml_folder(source)\n\ttarget_xml_dir = os.path.join(target_folder_dir, xmls[-1])\n\n\tif not os.path.exists(target_xml_dir):\n\t\treturn {}\n\n\twith open(target_xml_dir) as f:\n\t\tfile_info = f.readlines()\n\tdir = file_info[0][21:].strip().split('.')[0]\n\n\tfiles2 = {}\n\n\t# Get package zip\n\tpackage = zipfile.ZipFile"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\tglobal config\n\tconfig = Package()\n\tfor filename in files:\n\t\t# Do not include folders\n\t\tif filename.endswith('.zip'):\n\t\t\tcontinue\n\t\tif not config.read_file(filename):\n\t\t\tcontinue\n\n\t\t# Group in folders\n\t\tgroup = {}\n\t\tgroup['name'] = filename\n\t\t# Read in groups\n\t\tfor group_file in config.read_file(filename):\n\n\t\t\t# For each group file\n\t\t\tfilename2 = config.path + filename\n\t\t\tgroup['group_file"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\troot = xmls[0]['root']\n\txml_filenames = [name for name, _ in xmls[0].info.items() if name.endswith('.xml')]\n\txml_filenames = map(lambda x: x[:filename_len], xml_filenames)\n\n\tfiles = []\n\n\tfor dirname in sorted(xpaths.iterdir()):\n\t\tif dirname.startswith('.'):\n\t\t\tcontinue # Skip subfolders, which will happen when using\n\t\t\t       # files_to_xml.\n\t\tif xpaths.is"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\tif type(xmls) == list:\n\t\tfor i, xml in enumerate(xmls):\n\t\t\txml_filename = os.path.join(source+\"/\".join(xml))\n\t\tif not os.path.isdir(xml_filename):\n\t\t\treturn None, None\n\t\telse:\n\t\t\treturn _file_group_files(source, xml_filename, files), {}\n\telse:\n\t\tif xmls:\n\t\t\tfor xml in xmls:\n\t\t\t\treturn _file_group_files(source, xml, files), {}"}]}
{"task_id": "62b463153879012d1948149b", "prompt": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tif prefix.strip() == \"__\" :\n\t\treturn True\n\n\tp = r\"^\\s*!\\\"`\\s+[^$]+\\s*\\n\".replace(\"$$\", \"%27\").replace('^$', '.')\n\n\tpath = r\"^[\\t\\f\\s-]+$\", p.split(r\"\\s\")\n\n\tfor path.find('\\'') < 0:\n\t\t#if(path.endswith('/../') || path.endswith('/.././') or path.endswith('/./file'))\n\t\tif path.endswith('\\\\') or (path.endswith"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tself.package = 'documents'\n\tpath = open(file_path, 'rb').read()\n\tself._package = self.match_line(prefix + '/*', path)\n\tpath = open(file_path, 'rb').read()\n\tself._package = self.match_line(prefix + '##', path)\n\tpath = open(file_path, 'rb').read()\n\tself._package = self.match_line(\"/**\", path).lower() == 'included'\n\tpath = open(file_path, 'rb').read()\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tf = open(file_path, \"r\")\n\tbufx = \"\"\n\tp = f.readline()\n\tfor x in bufx.split():\n\t\tif(x.strip().startswith(prefix)):\n\t\t\treturn True\n\t\ty=p.strip().split(r\".+\")[-1]\n\t\tf.close()\n\t\treturn False\n#---------------------------------------\ndef match_file_by_prefix_and_type(prefix, file_path):\n\t\"\"\"\n\t Identifique os de file para o diret\u00f3rio\n\n\tPara"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tif not os.path.isfile(file_path):\n\t\treturn False\n\treturn os.path.splitext(file_path)[-1] == prefix\n\n\ndef recheck_files_by_path(filepath):\n\t\"\"\"\n\tRecheck all files in given directory by removing them from package list\n\n\tParameters\n\t----------\n\tfilepath : str\n\t\tDirectory path\n\n\tReturns\n\t-------\n\tnone\n\t\tnone\n\t\"\"\"\n\tdirectory = os.path.dirname(filepath)\n\tall_items = os.listdir(directory"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tpath = os.path.join(file_path, prefix)\n\n\t# Inamos por pacote, o tipo de futuridade del usuario para documentos\n\n\tif path!= '' and NOT_EXIST_PATH(path):\n\n\t\tret = bool(os.path.exists(path))\n\n\t\treturn ret\n\n/*\n * Copyright 2009 The Closure Compiler Authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n *"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\treturn os.path.basename(file_path).startswith(prefix)\n\n#-----------------------------------------------------------------------------\n# Devuelve\n#-----------------------------------------------------------------------------\n\n#-----------------------------------------------------------------------------\n# Devuelve\n#-----------------------------------------------------------------------------\n\n/*\n    Copyright (C) 2011-2015 de4dot@gmail.com\n\n    This file is part of de4dot.\n\n    de4dot is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tmatch_path = match_path_by_prefix(prefix, file_path)\n\treturn not match_path\n\ndef match_file_by_suffix(suffix, file_path):\t\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `suffix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nsuffix : str\n    Filename suffix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tif not file_path.lower().endswith(\".py\"):\n\t\treturn False\n\n\treturn hasattr(FileHandler, file_path)\n\nclass FileHandler():\n\tdef __init__(self):\n\t\tself.paths = []\n\n\tdef add_package_path(self, package_path, absolute=True):\n\t\t\"\"\"Given a path, add it to the list at the end of it.\n\n\t\tArguments\n\t\t----------\n\t\tpackage_path : str\n\t\t\tThe path where the directory is stored\n\t\tabsolute : bool\n\t\t\tIf"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\ttry:\t\t\n\t\tdata = load_package_by_prefix(file_path)\t\n\t\treturn (  data[\"version\"].strip(None) == \"1\" and \"pak*\" in prefix\n\t\t\t\t or \"%s/%s/%s\" % (prefix, data[\"version\"], data[\"platform\"][\"name\"]))\t\n\texcept IOError:\n\t\treturn False\n\ndef get_installed_packages(target_dir=\"\"):\n\t\"\"\"\n\tRetorna lista de objetivos en CWD/user/etc/CWD/etc/user files adicionados\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\tif prefix:\n\t\tpackage_info = get_package_info(prefix)\n\t\tif not package_info:\n\t\t\treturn False\n\telse:\n\t\tprefix = get_prefix(prefix)\n\t\tfor _, file_path in os.scandir(prefix, follow_symlinks=True):\n\t\t\tif file_path.isupper():\n\t\t\t\tpackage_info = {}\n\t\t\t\tif file_path.lower().endswith('.po'):\n\t\t\t\t\tfile_info = package_info\n\t\t\t\t\tfound_pofile = False\n\t\t\t\tfile_"}]}
{"task_id": "62b463153879012d1948149c", "prompt": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t", "suggestions": [{"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\treturn [d.getName() for d in os.scandir(prefix) if d.name.endswith(suffix or \"_\")][0]\n\ndef split_file(filename):\n\t\"\"\"Get all the file names of one or more files in the directory hierarchy of the folder.\n\t:param filename: path to a folder containing\n\t:return:\n\t\tone or more file names.\n\t\"\"\"\n\tfiles_dir = os.path.dirname(filename)\n\tout_files = [d.name for d in os.scandir(files_dir) if d"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\tdir_p = Pyshell.exec_program('select dirname from file where basename =?', (prefix+\".\"))\n\t\tdir_arr = dir_p.split(\"\\n\")\n\t\treturn dir_arr[0:(len(dir_arr)-2)]\n\n\t# def select_filenames_by_prefix_and_filetype(prefix, files, filename_type, output):\n\t# \t#\n\t# \t# \t# Fonction de arquivo da lista `files` ou desar a lista de conta`\n\t#"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\treturn _doc_paths_by_suffix(files, prefix, '__all__')\n\n\ndef select_globs(globs, exclude=None):\n\tglobal g_select\n\tif g_select is None:\n\t\tg_select = ''\n\telse:\n\t\tg_select = r'(:\\s*)?'\n\n\tfiles = globs\n\tif exclude:\n\t\tfiles = r'-exclude:.*' + r'\\Z' + r'\\Z' + r''.join(files)\n\n\tfor f in files:\n\t\t"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tif isinstance(files, str):\n\t\tfiles = ['.' + os.path.splitext(f)[0] for f in files.split(',') if f.strip()]\n\treturn [f for f in files if not os.path.isfile(f)]\n\n\nclass ProjectVersionGetter(ProjectVersionGetterBase):\n\t\"\"\"\n\tGetter for project version.\n\tReturns the `value` as a string when found.\n\n\tParameters\n\t----------\n\tProjectVersion : object\n\t\tProjectVersion instance\n\n\tReturns\n\t~~~~~\n\tstring\n\t\t"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tfor root, dirs, files in os.walk(path_to(prefix)):\n\t\tfor file in files:\n\t\t\tfilename = file.split('.')[-1]\n\t\t\tnew_file = os.path.join(root, filename)\n\t\t\tif filename == prefix:\n\t\t\t\tif file in files_list:\n\t\t\t\t\tlist.append(new_file)\n\t\t\t\telse:\n\t\t\t\t\tfiles_list.append(new_file)\n\t\t\telif file.endswith('.tar.gz') and get_tar(new_file):\n\t\t\t\tlist."}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\treturn ['{}/{}'.format(prefix, fname) for fname in files]\n\n# --\n\ndef generate_content_file(dirpath, fileramos, filenames, is_static,\n\t\tget_ext):\n\t\"\"\"\n\tGenerate a file object for the given contents dirpath and fileramos\n\n\tArguments\n\t----------\n\tdirpath\n\t\tRelative path to the root directory\n\tfileramos\n\t\tFileobject instance\t\"\"\"\n\tfrom os import stat\t# NOQA, NOQA 2.1 : # pylint: disable=E0401\n"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tfilenames = []\n\tf = open(prefix + \".txt\", 'r')\n\tf.readline()\n\n\tfor line in f: #line is newline\n\t\tif line.startswith(prefix + \"_\"):\n\t\t\tline = line.split(\"_\")[1]\n\t\t\tf.readline() \n\t\t\tfilenames.append(line.split('_')[0])\n\n\tf.close()\n\tfilenames = [x[:-1] for x in filenames]\n\treturn filenames\n\ndef main(argv=None):\n\t\"\"\"\n\tInput: None\n\tOutput:\n\t"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tfns = set(files) & set(set(config['lib']['doc']['exclude']))\n\tif len(fns) == 1:\t\t# Only a single file in `files` match any document.\n\t\treturn [fns[0]]\n\telif fns:\n\t\tfor f in fns:\n\t\t\tif re.search(r'.*\\.' + d + r'.*', f):\n\t\t\t\treturn [f.split(\"/\")[-1]]\n\t\treturn []\n\ndef load_file(filepath):\n\t# Check if filepath is a valid"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\tlist_of_dirs = os.path.abspath(os.path.dirname(files[0]))\n\tall_files = list(os.walk(list_of_dirs))\t# os.walk is a python extension\n\tall_paths = [os.path.normpath(filename) for filename in all_files]\n\n\t# Rede for windows\n#if os.name == \"nt\" or isWindows():\n#\tfor file in files:\n#\t\tif file == \"/usr/share/editch/python-doc.exe\" and file!= \"/usr"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\treturn filter_files(\n\t\t\tfiles, match_basename=True, match_prefix=True,\n\t\t\tprefix=prefix.strip()\n\t\t)\n\n\ndef deselect_files_by_prefix(prefix, doc_files_dir):\n\t\"\"\"\n\tDeselect files containing the given doc-file(s).\n\n\t\tdeselect_files(['.svn', 'a11y-kim-de/1', 'package.tgz'])\n\n\tParameters\n\t----------\n\n\tprefix : str\n\t\tDir prefix where doc-files contain.\n"}]}
{"task_id": "62b463153879012d1948149d", "prompt": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tif folder not in _pkg_root_folders():\n\t\traise IOError(\"Folder %s not found!\" % folder)\n\n\twith open(folder + \".xml\", \"wb\") as f:\n\t\tp = xml.dom.minidom.parseString(folder + \".xml\")\n\t\tf.write(p)\n\t\tf.close()\n\n\tout = {}\n\tpkg_root = folder + \".pkgroot\"\n\tif not os.path.exists(pkg_root):\n\t\tif (\n\t\t\t\tos.path.exists(pkg"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\txml_path = _explore(folder)\t\t\t# path to XML\n\n\tif not os.path.exists(xml_path):\n\t\tif not os.path.exists(folder):\n\t\t\treturn False, {}, 0\n\t\telse:\n\t\t\traise ValueError(\"Failed to get xml: {}, {}\".format(folder, xml_path))\n\telse:\n\t\twith open(folder) as f:\n\t\t\treturn _exploration_parse(f.read())\n\n@_exploration\ndef cleanse(folder):\n\t# Removes packages'"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tfolder = os.path.expanduser(folder)\n\tos.makedirs(folder)\n\tfiles = glob.glob(os.path.join(folder, FILE_RE.match(os.linesep)[1]))\n\n\tfor item in files:\n\t\tpkg_name = os.path.basename(item)\n\t\tself.debug_print(2, \"Reading %s\" % pkg_name)\n\t\twith open(item, \"rb\") as f:\n\t\t\tdata = pickle.load(f)\n\t\t\tself.package_name ="}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tresults = dict()\n\n\twith zipfile.ZipFile(folder) as zf:\n\t\tfor filename in fnmatch.filter(fnmatch.basename(filename), \"[.rst]\"):\n\t\t\tzf.extractfile(filename).close()\n\t\tzf.close()\n\n\t\t# get each package from rutun data\n\t\tdata = zf.namelist()\n\t\tfor file in data:\n\t\t\t# remove extra '.'\n\t\t\tif file == \"..\" or file == \".\":\n\t\t\t\tos.remove(file)\n\t\tresults = {\""}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tdata = package_data()\n\tdata['folders'] = dict()\n\tdata['folders']['main'] = folder if folder else root\n\n\tfor root, dfiles in os.walk(folder):\n\t\tfor (f, ff) in dfiles:\n\t\t\tdata['folders'][''] += (f, ff)\n\n\tdata['folders']['/'].suffix = '/'\n\n\treturn data\n\nif __name__ == \"__main__\":\n\tfolder = \"/home/user/lib/myapp\"\n\tfolder_name = folder[1:]\n\tfolder = os."}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\treturn _explore_data_folder(folder, \"XML\", [\"folder\"])\n\n\ndef _explore_data_folder(folder, mimetype=\"\", groupname=None,\n\t\tfolderid=None):\n\n\t\"\"\"\n\tGet all data items in a folder in tree of data item folders\n\n\tParameters\n\t----------\n\tfolder : str\n\t\tFolder where data resides\n\tmimetype : str\n\t\tMIME type of the folders, defaults to 'zip'\n\t\tNone returns dict\n\tgroupname : str\n\t\tname of the folder group\n\t"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t# Read the package's XML file into Python dictionary format\n\tpackage = {}\n\tfpath = os.path.join(folder, f\"{PACKAGE}.py\")\n\twith open(fpath, 'r') as f:\n\t\tfor line in f:\n\t\t\tif line.startswith(\"!DOCTYPE\"):\n\t\t\t\t# Get the XML declaration of the XML file\n\t\t\t\txml = e_node.parseString(line)\n\t\t\t\t# Get sub folder path within the folder\n\t\t\t\tif fpath in xml.children:\t# There is an sub folder\n\t\t\t\t\t# Get"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tfkeys = [\"__init__.py\", \"better\", \"__init__.py\", \"better\", \"__init__.py\", \"better\", \"__init__.py\", \"better\", \"__init__.py\", \"__init__.py\",  \"__init__.py\", \"better\", \"__init__.py\",  # These are the files of the folder\n\t\t\t\"__init__.py\",        '__init__.py',        '__init__.py',  '__init__.py',     "}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tfolder = folder.encode(\"utf-8\")\n\twith codecs.open(folder, encoding=\"utf-8\", errors=\"strict\") as f:\n\t\treturn {\n\t\t\t'name': path_to_package.filename(f.name)\n\t\t}\n\n\ndef explore_root(x, y):\n\t\"\"\"\n\tExplore an x/y folder by recursively explore the root folder recursively.\n\n\tArgs\n\t----------\n\tx\t: str \tFolder of data.\n\ty\t: str \tFolder of data. (Optional)\n"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\tfor f in os.listdir(folder):\t# For each file in folder\n\t\tf_name, f_ext = os.path.splitext(dir=folder)\n\t\tf_path = os.path.join(folder, f_name)\n\t\tif f_ext.lower() == \"xml\" and not os.path.isfile(f_path):\n\t\t\tcontinue\n\t\t\t\n\t\tif notos.path.exists(f_path):\n\t\t\tprint(f\"os.path.exists(): '{}' is not a file!\".format("}]}
{"task_id": "62b463153879012d1948149f", "prompt": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tpath = prefix.strip('/')\n\n\tif path == '':\t\tpath = os.path.dirname(__file__)\n\telse:\t\t\tfilename = path + '/' + filename\n\twith open(file_path, 'a') as f:\n\t\tf.write(prefix + filename)\n\treturn dict(path=path, prefix=prefix, file_folder=file_folder)\n\n# -------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation.  All Rights Reserved.  Licensed under the MIT License.  See License in the project root for license information.\n# by James Hurst"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tif prefix and file_path:\n\t\traise Exception('Error! Folder do arquivo do pacote n\u00e3o \u00e9 feita acessado e um arquivo no arquivo XML: `%s'.format(prefix))\n\treturn {'file':file_path, 'name': file_path}\n\n\ndef _process_xml_file(p_file):\n\t\"\"\"\n\tObtem dar esta pacote do arquivo XML em sua chave.\n\n\tParameters\n\t----------\n\tp_file\n\t\txml file do arqu"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t# Este arquivo dependendo um arquivo XML sem extens\u00e3o. (N\u00e3o seja excluindo e\n\t# proteger)\n\tparser = ET.XMLParser()\n\tparser.feed(bytes(prefix + \".xml\"))\n\tparser.close()\n\tresult = {}\n\twith open(file_path, 'wb') as f:\n\t\tfor xml in parser:\n\t\t\tresult.update({\n\t\t\t\t'id': xml.tag.get('id', ''))\n\t\t\t\t\"Version\\t\" + xml.tag.get"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tfile_path = _build_file_path(prefix, file_path)\n\t_logger.info(\"file_path: %s\", file_path)\n\twith open(file_path, \"rU\") as file:\n\t\treturn _load_file(file)\n\ndef _load_file(fp):\n\t\"\"\"\n\tParte de repara importante\n\t\"\"\"\n\twith open(fp, 'rb') as file:\n\t\t__parse_file__ = []\n\t\t__parse_file__.extend(file.read().split('<<<"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tif not os.path.isfile(file_path):\n\t\traise Exception(\"Arquivo do pacote falta selecionar\")\n\treturn _load_arquivo_from_file(prefix, filename, file_folder)\n\n\ndef _load_arquivo_from_file(prefix, filename, file_folder):\t# pylint: disable=too-many-locals\n\t\"\"\"\n\tDescri\u00e7\u00e3o do arquivo do pacote falta atualiza por XML\n\tNome de arquetas ou suficientes"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tfile_path = os.path.join(cache_dirs['', prefix, file_path])\n\tif not os.path.isfile(file_path):\n\t\traise Exception(_(\"File no existe!\\n\\n\"\n\t\t\t\t\"Protegidor: %s\", path.replace(file_path, \"\")))\n\treturn {'uri' : file_path, 'path' : file_path}\n\ndef load_xml(path, xml):\n\troot_node = xml.findroot('assets:')[0]\n\txml = root_node.get"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\t# Cria\u00e7\u00e3o do reconhecido\n\t\tif len(sys.argv) == 5:\n\t\t\ttry:\n\t\t\t\tself.file_path = sys.argv[3]\n\t\t\texcept IndexError:\n\t\t\t\tprint('[!] Eliminar os arquivos da vari\u00e1-los do pacote. ')\n\t\t\t\tprint('Usage: %s <path>' % sys.argv[0])\n\t\t\treturn False\n\t\t\t\n\t\t\tfile_name = sys.argv[1]\n\n\t\t\tself.file_path = None"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tpath = os.path.abspath(file_path)\n\tfile_name = os.path.splitext(filename)[0]\n\n\ttry:\n\t\twith open(file_path, 'r') as f:\n\t\t\tdata = f.read()\n\texcept IOError:\n\t\treturn False\n\tif len(data.strip()) > 100: # tomto pode esta leitura com o arquivo que o tipo de\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarquivo \u00e9 parametro.\n\t\treturn False\n\n\tpath = os.path"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\treturn __p_parser__(asset_file=prefix,\n\t\trendition_file=filename,\n\t\tendere\u00e7o=filename,\n\t\tpackage_file=file_path,\n\t\tprefix=prefix)\n\n\ndef package():\n\t\"\"\"\n\tAcesso do provedor para cada provedor.\n\n\tParameter\n\t----------\n\t\"\"\"\n\treturn __p_parser__(package_file=None,\n\t\tpackage_extension='.phtml')\n\n\ndef package_file(prefix, content=None):\n\t\"\"\"\n"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\tf_name = 'asset.' + '._' + str(uuid.uuid1())\n\tfile_path = os.path.join(prefix, '{0}.{1}.{2}.{3}.{4}.xml'.format(os.path.basename(f_name), f_name, 'assets', 'pkg'))\n\t# deja\n\tf_file = open(file_path, 'w')\n\tf_s = f_file.write\n\tf_s.write(f_name + '\\n')\n\tfor prefix, package in list(x"}]}
{"task_id": "62b463153879012d194814a1", "prompt": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\treturn self._render_with_file(self._render_text_1_en, lang, file_path)\n\n# Copyright (c) 2020, Ansible by Red Hat.\n# All rights reserved.\n#\n# Use of this source code is governed by a MIT-style license that can be\n# found in the LICENSE file.\n\n\"\"\"\nUnit tests for Django core module\n\"\"\"\n\nimport unittest\n\nfrom django.test import Client\nfrom django.test.client import Client\nfrom django.urls import reverse\n\nfrom.test_app import AppTestBase\n\n"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tfor language, files in lang_dict.items():\n\t\tfor file_path in files:\n\t\t\tf = os.path.join(file_path, \"artigo02-en-alt.pdf\")\n\t\t\tself.add(f, language)\n\n\t\t# the above is to make sure the document is in the correct location\n\t\t# by adding.pdf in it and then check the path\n\t\twith open(file_path, \"r\") as r:\t\n\t\t\tr.readline()\n\t\t\tr.readline()\n\t\t\tcontents = r."}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tfile_path = file_path.replace(\".png\",\"artigo02.png\")\n\tself.write_file(lang, file_path)\n\n\tcode = artigo02_lib.sheriff_code_from_zip(file_path)\n\tself.add_text(code)\n\tcode = artigo02_lib.sheriff_code_from_path(file_path)\n\tself.add_text(code)\n\n/*\n *  Licensed to the Apache Software Foundation (ASF) under one\n *  or more contributor license agreements. "}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tfor wordname, word in self.words:\n\t\tif len(word.split(\".\")) > 1:\n\t\t\t# check if the word and all of words are of the same type\n\t\t\tif len(word.get(wordname))] == 2:\n\t\t\t\tself.dict[\"_content\"][lang][(\"All\", \" \", word)] = (word, )\n\t\t\telse:\n\t\t\t\t# words don't start with letters\n\t\t\t\tself.dict[\"_content\"][lang][(\"All\", \"*\", word)] = (word, )\n\t\telse:\n\t\t\tself.dict"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tself.lang = lang\n\tself.file_path = file_path\n\tself.doc_file_path= \"artigo02/\"\n\treturn len(self.doc_file_path)\n\n\treturn  len(self.doc_file_path)\n\n\treturn  len(self.doc_file_path)\n\n\treturn  len(self.doc_file_path)\n\n\ndef build_artigo02():\n\t\"\"\"Build Artigo02 document.\n\n\t:returns: a document class\n\t:rtype: L{pd.DataFrame}"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\ttry:\n\t\tfile = os.path.join(os.path.dirname(__file__), '..', 'artigo02', lang, file_path)\n\t\tfile = os.path.abspath(file)\n\t\tif not os.path.isfile(file): return None\n\texcept:\n\t\treturn None\n\t\n\tif os.path.split(file)[0].endswith(\".pdf\"):\n\t\tfrom artigo02 import pdfdoc\n\n\t\t#    pdfdoc.export_pdf()\n\t\treturn pdfdoc.export_pdf"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tlang = self._get_language(lang, 'en')\n\treturn lang + self._get_file_path(lang, file_path)\n\n\ndef add_denovo_reviews_reviews_data_de(self, item_ids_to_remove):\t\"\"\"\n    {\n        \"type\": \"denovo_reviews_data\",\n        \"file_path\": \"denovo.png\",\n        \"file_type\": \"denovo\","}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\treturn self.render(lang, file_path)\n\n\n\n# class Renders an image to a file(or directory) of HTML\nclass RendersHtml(Renders):\n    \"A template function\"\n    def __init__(self, path=\"artigo02.jpg\", lang=\"en\"):\n\t\"\"\"\n\tDescription:\n\t\tartigo02 renders a single image to a file. The file given is\n\t\ta <code>artigo02.html</code>. It returns a text file, a html\n\t\tstring, a html snippet,"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tself.add_file('artigo02', lang, file_path, self.Rendition[lang])\n\n\n\n#---- Artigo03 class\nclass Artigo03(Rendition):\n\t\"\"\"\n\tArtigo03-3.4.\n\t\n\t---\n\tTitle\t        \t        -- Text\n\t-----         \t        -- ----\n\tBol\u00edvar    \t    \t        --\n\tDavide\t \t  "}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from sp_logger import Logger\n\tlogger = Logger(file=os.path.splitext(file_path)[0] + '_artigo_artigo02.txt')\n\tself.logger = logger\n\tself.logger.file = os.path.splitext(file_path)[0] + '_artigo_artigo02.txt'\n\t\n\treturn self.read_reference(lang)\n\ndef add_default_languages(self):\n\treturn self.read_reference(\"Default\")\n\ndef read_reference(self,"}]}
{"task_id": "62b463163879012d194814a2", "prompt": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\treturn _AssetTableWriterForPath(basename, 'zip', file_path)\n\ndef add(self, path, path_prefix, suffix='', file_path=None):\n\tpass\n\treturn _AssetTableWriterForPath(path, 'zip', file_path)\n\ndef write_image(\n        self,\n        filename,\n        description='',\n        mimetype=None,\n        resolution=None,\n     "}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tasset_path, _ = os.path.split(file_path)\n\tif asset_path == \"artigo02-gf03.tiff\":\n\t\twith open(\"/path/artigo02-gf03.tiff\", \"rb\") as f:\n\t\t\tfile_data = f.read()\n\telif asset_path == \"artigo02-gf03.jpg\":\n\t\twith open(\"/path/artigo02-gf03.jpg\", \"rb\") as f:\n\t\t\tfile_data = f.read()\n\telif asset_path == \"artigo"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tfrom nupic.tools import download_from_file\n\turl_file = download_from_file(self.get_url(basename), self.url,\n\t\t\t\t\tself.username, self.password)\n\treturn url_file.target\n\ndef check_files_exist(files):\n\t\"\"\"Check if given files exists\"\"\"\n\treturn any([fn in files for fn in self.pathlists]])\n\ndef add_artigo02(basename, artigo02_img_file, filename):\n\t\"\"\"add artigo02\"\"\"\n\tfilepath = os.path."}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\traise NotImplementedError\n\n\t# we do not support a Tiff (image does have a single image/tile)\n\tif basename.startswith(\"t\") or basename.startswith(\"tf\"):\n\t\treturn\n\n\tif not basename.endswith(\"_tiff\"):\n\t\traise UnsupportedFileException(\"unsupported file type for %s (file is ignored)\" % basename)\n\n\tself.addFile(basename, file_path)\n\ndef add_audio(self, basename, file_path):\n\tself.addFile(basename, file_path)\n\nusing System;\nusing Volo."}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tself._artigo_config = ArtigoConfig()\n\tself._artigo_config.add_asset(basename, file_path)\n\treturn self.artigo_config.file_path()\n\n// Copyright 2009 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\n\n//go:build go1.5\n// +build go1.5\n\npackage net\n\nvar _ = SocketAddr{}\n\n/*\n * Copyright 2011 Ton Tammeller\n"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tself.log_print(f\"Add asset {basename}\", \"artigo02-gf03\")\n\n\tpath = \"artigo02-gf03.#{basename}\"\n\n\tif self.use_system_temp_dir:\n\t\treturn path, None\n\n\t# Create the artigo02 directory. This is required for this file to load correctly.\n\tif self.force_load:\n\t\tself.log_print(f\"Add artigo02 asset {basename}\", \"artigo02\")\n\t\treturn path, None\n\n\tif not os.path."}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tbase = self.makeBase(basename, \"artigo02.tiff\")\n\tself.addFolder(base, \"resources/artigo02_gf03\")\n\tself.addFilename(base)\n\treturn base\n\naddArtigo02(artigo02)\naddArtigo02(artigo01)\naddArtigo01(artigo02)\n\n//\n//   Copyright 2018  SenX S.A.S.\n//\n//   Licensed under the Apache License, Version 2.0 (the \"License\");\n//   you may not use this"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\t#add default value\n\t\treturn (basename + file_path)\n\n\t\t\n\tdef add_asset2(self, basename, file_path, label_name):\n\t\t#add default value\n\t\treturn (basename + \"Artigo02_3.JPEG\" + label_name)\n\n\tdef write_list(self, file_path):\n\t\t\"\"\"\n\t\tWrite list of assets to a file. Use the same folder as assets.\n\t\t\"\"\"\n\t\t# Write list of assets to a file\n\t\twith open(file_path, \""}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tif basename:\n\t\tfilename = basename.replace('.', '_')\n\t\tif filename == file_path:\n\t\t\treturn\n\t\timport zipfile\n\t\tasset = zipfile.ZipFile(file_path, 'r')\n\t\tfor name in dir(asset):\n\t\t\tif name.endswith('.asset'):\n\t\t\t\tasset_path = os.path.join(dirname(file_path), name)\n\t\t\t\tif not os.path.isfile(asset_path):\n\t\t\t\t\tcontinue\n\t\t\t\tzip_name = basename.replace('.', '_')\n\t\t\t\tif"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\tt_name = \"%s-%s\" % (basename, file_path)\n\t#print(t_name)\n\tfile = open(t_name, mode='w+')\n\tfile.write(file_path)\n\tfile.close()\n\t#print(t_name)\n\n\ndef add_image(self, filename):\t\"\"\"\n\t\"{\n    \"artigo022-gif25.gif\": \"./artigo02.gif\",\n    \"artigo022-gif25.txt\": \"./artigo02.txt\",\n"}]}
{"task_id": "62b463163879012d194814a4", "prompt": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\troot = os.path.dirname(os.path.abspath(__file__))\n\tpath = \"../\"\n\tout = {'packages': [], 'content': []}\n\tfor root, dirnames, filenames in os.walk(root,follow=True):\n\t\tfor dirname in dirnames[:-1]:\n\t\t\tif dirname == \"\":\n\t\t\t\tcontinue\n\t\t\tname_dir = dirname.split('/')[-2]\n\t\t\tpath = path + name_dir\n\t\t\tfor f in filenames:\n\t\t\t\tbase_path = os.path.join(root,dirnames["}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\tlist_of_data = zip_list_of_datanames()\n\tall_data_list = list([f for f in list_of_data[0]])\n\tall_data_list += list_of_data[0]\n\tdata = {\"name\": zip_path,\n\t\t   \"all_data_list\": all_data_list,\n\t\t   \"group_dict\": {\n\t\t\t\t   \"pkg_name\": group_string(zip_path)\n\t\t\t\t   for group_string in all_data_list\n\t\t\t\t   for group_"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\tif os.path.isfile(zip_path) and _zip_files_exist(zip_path):\n\t\tdfile = zipfile.ZipFile(zip_path, 'r')\n\telse:\n\t\traise KeyError('Zip file '%s' not found' % zip_path)\n\t\n\tdata = {'filetype':'xml','group_filetypes':'group','group_file_ext':'xml','group_files':'group_files'}\n\t\n\tdfile.readlist(folder='.'])\t\t# read archive info\n\tdfile.close()\t\t\t#"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\td = {}\n\tfor z in os.listdir(zip_path):\n\t\tz = z.replace('\\\\', '/') # remove leading slash\n\t\tf = os.path.join(zip_path, z.split('.')[0]) # add basename to path\n \t\td[ z ] = os.path.basename(f)\n\n\treturn d\n\npackage com.github.aaronz.statuscodedemo;\n\nimport org.junit.Test;\n\nimport static org.junit.Assert.*;\n\n/**\n * Example local unit test, which will execute on"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t# Get XML data from zip\n\txmldata = __load__(\"xml_data\", zip_path)\n\n\t# Get list of all files from zip\n\tdirs = os.walk(zip_path)\n\tfor dirpath in dirs:\n\t\tif dirpath not in (\"/root/\", \"/Applications/\"): continue\n\n\t# Parse each XML data and generate data by type\n\tfor filename in sorted(os.listdir(\"/root/\"))+sorted(os.listdir(\"/root/\")):\n\t\t# Load XML data\n\t\twith open(filename, 'r"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t_init = dict()\n\n\t# load file path\n\t_zipfile_path = zipfile.ZipFile.open(zip_path).read('utf-8')\n\n\t# parse info\n\tfor filepath in _zipfile_path.split('/'):\n\t\tif filepath[0]=='{':\n\t\t\tself.read_file(path=filepath.split('/')[-1])\n\t\telif filepath[-1]=='/':\n\t\t\tself.read_folder(path=filepath.rpartition('/')[0] + '/')\n\t\telif filepath[-1]=='-':"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\treturn data_dict['zip_files'].get_zip_files(zip_path)\n\ndef get_package_zip_path(pkg_name, output_dir = None):\n\t\"\"\"Returns the zipped zip file path\"\"\"\n\n\tif output_dir is None:\n\t\toutput_dir = OUTPUT_DIR\n\t\n\treturn os.path.join(output_dir, 'zip', pkg_name)\n\n#-----------------------------------------------------------------------------------------------\ndef install_xml(xml):\n\t\"\"\"\n\tImport xml file to xml library.\n\n\tSee the xml library for help on"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t# read file\n\tf.seek(8)\n\tzipdata = f.read()\n\txml_path = os.path.join(zip_path, zipdata)\n\n\t# extract all the file contents as dict\n\tfile_contents = xml.split(xml_path)[-1]\n\n\t# get xml data and file contents\n\txml_data = {}\n\tfile_contents = str.find('<?xml version=\"1.0\" encoding=\"utf-8\"?>', file_contents)\n\tfile_id_arr = file_contents.split('id"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\tparams = import_param(\"params.yml\")\n\n\t# TODO: this might not work if there's a zip-file-in-zip\n\tif not os.path.isfile(zip_path):\n\t\treturn None\n\n\tf_info = {'zip_path': zip_path, 'params': params}\n\n\t# Find some packages we can't find in zip\n\tif not _is_zip(zip_path):\n\t\treturn None\n\n\tinfo = _get_package_info(zip_path, f_info)\n\n\treturn info"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\tf = zipfile.ZipFile(zip_path, 'r')\n\tf.extractall('package')\n\treturn {'package': 'data'}\n\ndef _get_file(zip_path, basename):\n\tzipfi = zipfile.ZipInfo(zip_path)\n\tzipfi.filemode = \"wb\"\n\tzipfi.compresslevel = 0\n\tzipfi.filesize = len(zipfi.filedata)\n\tzipfi.filename = basename\n\tfilename_ext = \".%s\" % basename\n\treturn zipfi, filename_ext\n\t\n"}]}
{"task_id": "62b463163879012d194814a6", "prompt": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\twith zipfile.ZipFile(zip_path, 'r') as zip:\n\t\tzip.extractall(filename=zip.name)\n\n\t\t# Read the files\n\t\tfiles = zip.filelist\n\n\t\t# For each file in the list\n\t\tfor file_name in files_list_to_list(files):\n\t\t\tprint(\"%.2f\"%file_name, file_name, filename)\n\n/** @file\n  SMM Runtime Service Interface Code (R/W/H).\n\n  Pei-Hungary (PEI-H"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\tzip_path = pathlib.Path(zip_path)\n\tzip_list = []\n\tfor filename in zip_path.iterdir():\n\t\tfile_name = filename.name\n\t\tif file_name == '':\n\t\t\tcontinue \n\t\telse:\n\t\t\twith open(filename, 'rb') as infile:\n\t\t\t\twith io.open(infile, encoding=\"utf8\", mode='r', encodinginfo=None) as outfile:\n\t\t\t\t\tdata = xml.etree.ElementTree(outfile)\n\t\t\t\t\tdata.text = file_name"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\timport zipfile\n\tf = zipfile.ZipFile(zip_path)\n\tfiles_list = f.filelist(with_suffix = False)\n\treturn files_list\n\nimport copy\nfrom contextlib import suppress\n\n\ndef add_extension(filename, ext):\n    global _ext_filenames\n    _ext_filenames.append(filename)\n\n\ndef get_ext() -> str:\n    return \"{:}\".format(_ext_filenames[-1])\n\n\ndef _remove_dot_dot():\n    for ext in _"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\tzip_path, fname = os.path.split(zip_path)\n\t#print(\"%s: %s\" % (fname, zip_path))\n\tfiles = os.listdir(zip_path)\n\tfor f in files:\n\t\tif os.path.isdir(f):\n\t\t\tf = os.path.join(f, fname)\n\t\t\tfiles = [f]\n\t\t\tbreak\n\treturn files\n\n\ndef get_files_by_name_sorted(sorted_name):\n\treturn sorted_name.split(\"_\")\n\n"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\treturn list(zip_files_list_from_zipfile('.' + fn))\n\ndef list_files_from_py(py_path):\t\"\"\"\n\tReturn the files in `py_path`\n\nExample:\n\n```\n[\n    '/Users/andreusm/Documents/Tinland - Cisco Teaching WAN 2010/2010',\n    '~/Users/andreusm/Documents/Tinland - Cisco Teaching WAN 2010/2010',\n    '~/.hg/py2/tifs"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\tif not os.path.isfile(zip_path):\n\t\tpass\n\telse:\n\t\tif sys.platform == 'win32':\n\t\t\timport os_windows\n\t\t\tfull_path = os_windows.GetFullPathName(zip_path)\n\t\telse:\n\t\t\tfull_path = zip_path\n\t\tfiles_with_metadata = get_files_from_zip(full_path,\n\t\t\t\t\t\t'e', zip_metadata)\n\t\treturn files_with_metadata\n\n\ndef get_files_from_zip(source_"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\tzip_path = os.path.realpath(_file_system.get_zip_path(zip_path))\n\treturn os.listdir(zip_path)\n\n\n#----------------------------------------------------------------------------\n#\t\tFunctions\n#----------------------------------------------------------------------------\n\n\n#----------------------------------------------------------------------------\n#\t\tLocal Variables\n#----------------------------------------------------------------------------\n\n\n#----------------------------------------------------------------------------\n#\t\tClass Variables\n#----------------------------------------------------------------------------\n\n\nclass BaseModel(object):\n\t\"\"\"\n\tBase class of the model. All functions below can be used to override and customize the model\n\tfunctionality of a model. For more information please see :"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t# Example of an empty zip file\n\t# If the input zip_path is empty, the zip_path should be empty\n\tif not os.path.exists(zip_path):\n\t\tprint(\"File not found: \" + zip_path)\n\t\terror_message(\"This is not a file:\", error_msg(\"file\"))\n\t\t\n\tif not os.path.exists(zip_path):\n\t\tprint(\"File is not found: \" + zip_path)\n\t\terror_message(\"This is not a file:\", error_msg(\"file\"))\n\t\treturn"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\tif not zipfile_zip_path:\n\t\traise ValueError('Please specify a zipfile to import')\n\n\tzip_archive_name = zipfile_zip_path.split(\".\")[0]\n\tzip_archive_path = zipfile_zip_path.split(\".\")[1]\n\tzip_path = os.path.join(zip_archive_path, zip_archive_name)\n\tif not os.path.isabs(zip_path):\n\t\traise ValueError('Input zip file should be in the root of '\n\t\t\t'path: \"'+zip_path+'\" and"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\tzip_path = os.path.join(os.path.dirname(__file__), 'zip-files')\n\tfiles_list, _ = get_zip_files('files_list_' + zip_path,\n\t\t\t\t\t\t\t\t[zip_path] + os.path.sep)\n\n\t# Return the files in `zip_path`\n\treturn files_list\n\ndef get_files_list(zip_path,\n                   files_list=None,\n         "}]}
{"task_id": "62b4631b3879012d194814dd", "prompt": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tr = re.compile(r\"\\S+\\s+(\\d+)\\s+('st'.?)?\", re.VERBOSE)\n\tmatch = r.match(content)\n\treturn (len(match.groups()[0]) + len(match.groups()[1]) - 1) % 2\n\ndef fix_namespace_w_prefix_st(content):\t\"\"\"\n\tConvert text that defaults to 'w' to'st' to 'w:st-'\t\"\"\"\n\tr = re.compile(r\"\\S+\\s+(\\w+)\\s+('st'.?)?"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tcontent = content.split('\\t')\n\tstart_idx = len(content)\n\tfor i in range(start_idx, len(content), 2):\n\t\tcontent[i+1] = re.sub(r'W-(.*:[W])(.*):(.*)', 'w-st=W', content[i])\n\tfor i in content:\n\t\tcontent[i] = re.sub(r'w/ST;(.*)', 'w-st=w', content[i])\n\n\ndef fix_namespace_prefix_o(content):\t\"\"\"\n\t"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\treturn (\n\t\tcontent\n\t\t\t.replace(FIX_NAMESPACE_PREFIX, FIX_W_PREFIX)\n\t\t\t.replace(FIX_NAMESPACE_PREFIX_M, FIX_W_PREFIX_M)\n\t\t\t.replace(FIX_NAMESPACE_PREFIX_O, FIX_W_PREFIX_O)\n\t\t\t.replace(FIX_NAMESPACE_PREFIX_S, FIX_W_PREFIX_S)\n\t\t)\n\t\t.replace(FIX_NAMESPACE_PREFIX_T, FIX_W_PREFIX_T)\n\t\t.replace(FIX_NAMESPACE_PREFIX_"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\treturn fix_pattern(content, \"%s:st=\\\"%s\\\"\" % (namespace, namespace_name))\n\nclass FilterHandler(Filter): \n\t\"\"\" Handles filtering for namespaces in a way suitable for a set of \n\tsearch, per-item settings and other parameters, using the regex \"\"\"\n\tdef process_filter_string(self, filter_text):\n\t\t\"\"\"Converts a filter string to an ns filter string with a'regex' \"\"\"\n\t\treturn(re.sub('^<a(?P<match>[#*|&lt;=]*)?<', '', filter_"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t# The word 'w' is a special case since it means it should be cased\n\t# at prefix=value. (See fix_namespace_prefix)\n\tcontent = content.replace('w=',\"w-st\")\n\n\t# A tag to add to word'st-'\n\tcontent = content.replace(' st\",\"w:st=\" + tag_to_text[1:])\n\n\t# Prefix/suffix is same as a word \"w\". If the word ends in /prefix/suffix/ this\n\t# will be the word 'w'\n\tcontent = content."}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tcontent = content.strip()\n\tt = re.findall(r'(\\w+):\\s+(.*)', content)  \n\t#print('t',t)\n\tif not t:\n\t\treturn (t[0], t[0])\n\treturn (t[0], t[0], t[1])\n\t\n\t\n\t\ndef fix_namespace_m(content):\n\tparts = content.split(':', 1)\n\tif len(parts)==2: #ns0 == namespace_prefix\n\t#print(\"ns0\", parts)\n\t\treturn"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tcontent = re.sub(r'^(.*)(\\\\s*(\\\\w+)(?:\\\\s*(\\\\w+))?)(.*)$',\n\t\tr'(\\\\w[^-\\\\w]+\\w+)\\.((?:\\\\w+)(?:\\\\s*(\\\\w+))?)(.*)$',\n\t\tm, re.M).strip()\n\treturn re.sub(r'^(\\w+)(?:\\\\s*(\\\\w+))?$', r'$1 $2', content)\n\t\t# This is not necessary, but it is needed\n\t\t# re.sub(r'(\\w+)(?:"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\treturn content.replace(\"{#\", \"{#m\"):\tcontent.replace(\"{#a\", \"{#a-}\")\n\ndef fix_inline_namespace():\t\"\"\"\n\tChange content's namespace on the way to the correct namespace in the template \n\"\"\"\n\treturn\t\"\"\"\\\n@html += new-inline-block-namespace(\"<span>{{.}}</span>\", \"i-n\")\n@html += new-inline-block-namespace(\"<span>{{.}}</span>\", \"i-n\")\n\"\"\"\n\ndef fix_inline_tag():\n\t\"\"\"Replace all the inline tags on"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tcontent = content.replace('\\\\', '/').replace('l', '/l')\n\tif sys.py3kwarning:\n\t\tcontent = content.replace('\\\\', '/').replace('c', 'c-c')\n\t\tcontent = content.replace('\\\\', 'n')\n\telse:\n\t\tcontent = content.replace('\\\\', '\\\\n')\n\treturn content\n\n\ndef fix_string(content):\n\tif sys.py3kwarning:\n\t\tcontent = content.replace('l', '')\n\telse:\n\t\tcontent = content.replace('n',"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\tdeffix_no_escape(content)\n\t\tcontent = unescape(content)\n\t\tif 'w' in content:\n\t\t\tcontent = content.split('w')\n\t\tif 'w' in content:\n\t\t\tcontent = content.split('w-')\n\t\treturn content\n\n\treturn fix_content_prefix(fix_namespace_prefix_w(content))\n\ndef fix_ns_prefix(ns):\n\tns = ns.split(':')[0]\n\tns = ns.replace('/', '-')\n\tns = ns.replace(\"_\","}]}
{"task_id": "62b463283879012d1948153d", "prompt": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tmatch = re.match(r'pubDate|$', pubdate_xpaths[0])\n\tif match is None: return match\n\tsecond_xpaths, _, _, _, match_pubdate_time = match.groups()\n\treturn pubdate_xpaths[0] if match_pubdate_time else second_xpaths[0]\n\n/*\n *\tAdamantis\t\tAnisha Pove Checa-Scotti\n *\n * \tAdamantis\t\tThe name from adamantos@ca.qai.\n *\n *"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\treturn match.match(node, pubdate_xpaths)\n\ndef find_best_match(candidates, pubdate):\n\t\"\"\"\n\tReturns the most likely, earliest match in pubdate candidates\n\tto compare\n\teach of the pubdate candidate against the lowest date\n\t\"\"\"\n\tbest_match = int(0)\n\tpubdate_c = pubdate.rsplit(\"\\\\\", 1)\n\tfor c in pubdate_c:\n\t\ts = candidates.find(c)\n\t\tif s is not None:\n\t\t\tif best_match < (len"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\txpaths = parse_dates(pubdate_xpaths)\n\tmatch = re.search(r\"#\\(\\d{4}-[0-9][0-9]\\w+\\)\"\n\t\t\".*\\)?\", match_pattern, re.DOTALL)\n\tif not match:\n\t\tmatch = re.search(r\"#\\s+\"\n\t\t\t\"/[^\\\\s]+\\s+\"\n\t\t\t\".*\\)?\", pubdate_pattern, re.DOTALL)\n\t\tif match:\n\t\t\tif match.group(0).lower() == pubdate"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tfirst_match = []\n\tfor pubdate_xpaths_node in pubdate_xpaths:\n\t\tif pubdate_xpaths_node.__class__.__name__ == \"PubdateElement\":\n\t\t\tfirst_match = pubdate_xpaths_node\n\t\t\tcontinue\n\t\tmatch = pubdate_xpaths_node.match(pubdate_xpaths)\n\t\tif match is not None:\n\t\t\treturn pubdate_xpaths_node, match\n\ndef build_pubdate_date():\n\t\"\"\"Builds out the date from Pubdates\"\"\"\n"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tr = ast.literal_eval(pubdate_xpaths[0])\n\tresult = r.argument\n\tif result is None: return None\n\telse: return ast.literal_eval(\n\t\tnode.nth_content(len(pubdate_xpaths) - 1).value)\n\n@pytest.mark.parametrize('match', (ast.literal_eval(node.nth_content(0)),))\ndef parse_node_with_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tpubdate_xpaths = [n for n in pubdate_xpaths if n.pubdate.public == datetime.datetime.strptime(node.pubdate.pubdate, EXCHANGE_TIMESTAMPS_FORMAT)\n\t                  or n.pubdate.public == datetime.datetime.strptime(node.pubdate.pubdate, EXCHANGE_DEFAULT_TIMESTAMPS_FORMAT)]\n\tif not pubdate_xpaths:\n\t\tpubdate_xpaths = node.pubdate.pubdate[0:3"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tm = match_xpaths(node)\n\tif not valid_pubdate2id(m):\n\t\treturn None\n\treturn m\n\ndef match_pubdate2id(pubdate_xpaths):\t\"\"\" match pubdate2id(pubdate_xpaths)\n* Returns the first match in the pubdate_xpaths list\n* Returns None if no match found\t\"\"\"\n\trv=None\n\tfor rv in pubdate_xpaths:\n\t\tif rv.pubdate == pubdate_xpaths.pubdate and rv.pubdate_time == pubdate"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tif any(i.xpath(node, 'text()') for i in pubdate_xpaths):\n\t\treturn 1\n\treturn 0\n\ndef get_publisher_to_publish(publisher, pubdate, pubdate_xpaths):\n\t\"\"\"\n\tReturns the 'publication date' of the published 'publisher'.\n\tIf the value is 'pubdate' then return the value published date to be published in publication date.\n\t\n\tReturns:\n\tNone \n\t\"\"\"\n\treturn pubdate(publisher) if pubdate.xpath(publisher, 'pubDate') else None"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tlastMatch = pubdate_xpaths.index(pubdate_xpaths[-1])\n\tfor i in range(1, len(pubdate_xpaths) + 1):\n\t\tif pubdate_xpaths[i].startswith(lastMatch):\n\t\t\tlastMatch = i\n\t\t\tmatch = pubdate_xpaths[i]\n\t\t\treturn match\n\t\t\tpass\n\treturn\n\ndef match_pubdate_xpaths(pattern, pubdate_xpaths):\t\"\"\"Matches a date with a pubdate (as Xpaths)\"\"\"\n\tfor patternStr in"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\tmatches = []\n\tfor node in pubdate_xpaths:\n\t\tif node['pubdate'] == pubdate_xpaths[0]:\n\t\t\tif any(re.search(r.string, node['pubdate'].lower(), re.I)):\t# only need to match for re at the beginning\n\t\t\t\tmatches.append('first')\n\n\tif matches:\n\t\tif matches[0] == 'first':\n\t\t\tpubdate_xpaths = matches\n\n\treturn pubdate_xpaths\n\n\n\ndef get_pubdate_by_pubdate"}]}
{"task_id": "62b463303879012d19481579", "prompt": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tall_lines = [x.strip() for x in issue.splitlines()]\n\treturn [number_and_suppl_list[x[1:]] for x in all_lines]\n\n# ========================================================================================\n#\n#       Copyright (C) 2006 by Jens Nilsson <JN364@n6473.fr>\n#                         Tod Kirby <todr+1@gmail.com>`_\n#\n#   Released"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tnumber = issue['number']\n\tvalue_element = issue_to_value_element([issue]).extract_string_to_element()\n\tif value_element['number'] == None or value_element['number'] < number:\n\t\traise SamlError(\n\t\t\t'Number is not applicable on issue with no value specified in %s.' %\n\t\t\tissue['header']['description']\n\t\t)\n\treturn value_element['suppl']\n\n\ndef _parse(context, template_path):\n\t\"\"\"\n\tParse and assemble template and content from the given"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t# Do the same if is not empty.\n\tnumber_and_supplment = issue.get_optional_value(\"number\")\n\tif not number_and_supplment:\n\t\tnumber_and_supplment = issue.get_default(\"number\")\n\n\t# Check if number or suppl are present.\n\tif not number_and_supplment:\n\t\treturn \"Nothing to extract\", \"No suppl\"\n\t# Parse number and suppl if not empty.\n\ttry:\n\t\tnumber = _number_and_suppl("}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tif issue:\n\t\ttry:\n\t\t\tcode_value = issue[0]\n\t\texcept (KeyError, IndexError):\n\t\t\tcode_value = -1\n\t\tsuppl = issue[1]\n\t\ttry:\n\t\t\tcode = int(suppl)\n\t\texcept ValueError:\n\t\t\traise ValueError(\"issue contains invalid values\")\n\treturn code, code_value\n\n\ndef _extract_issue_identifier(identifier):\n\tif _fix_issue_identifier(identifier, code, code_value): return True\n\treturn False\n\n"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tnumber_and_supplations = Issue._extract_number_and_supplment(issue)\t\t# type: Any\n\tif number_and_supplations:\n\t\treturn number_and_supplations, issue.subdivisions, issue.number_of_subdivisions\n\telse:\n\t\treturn issue.number_of_subdivisions, issue.number_of_subdivisions, issue.number_of_subdivisions\n\n\nclass Gender(object):\n\tdef __init__(self, name, gender):\n\t\t"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t# from issue_detail/extract_number_and_suppl_from_issue_detail_xml.py\n\tissue_detail_element = extract_number_and_suppl_element(issue)\n\tif 'notes' in issue_detail_element or'references' in issue_detail_element:\n\t\treturn issue_detail_element['notes'], issue_detail_element['references']\n\treturn issue_detail_element['results_num'], issue_detail_element['suppliers_num']\n\n\ndef _convert_number_and_supplated_"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t# print \"Looking to extract the possible number and suppl from issue: %s\\n\" % issue\n\t#if not issue: return EMPTY\n\tnumber = int(issue_element(issue))\n\ttry:\n\t\tsub_number = _extract_number_and_suppl_from_number(number)\n\texcept:\n\t\traise Exception(\"There is no number in the number field in %s\" % issue)\n\treturn sub_number\n\t\ndef _extract_number_and_suppl_from_issue_element(issue_element):\t\"\"\""}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tif issue_element.kind == 'note_with_number':\n\t\tsuppl_value = issue_element.value[1]\n\telif (issue_element.kind == 'note_has_string') and issue_element.value.strip():\n\t\tsuppl_value = issue_element.value.strip()\n\telse:\n\t\tsuppl_value = 'NONE'\n\treturn (issue.number, suppl_value)\n\n\ndef _extract_number_and_supplment_from_header(headers):\n\t# Remove lines that have"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tall_suppl: set = set()\n\n\tif len(issue.number) and issue.number not in [None, '', '-'] and \\\n\t\t(is_num(issue.number) or issue.number == len('0') and issue.number.isdigit()):\n\t\tnumber_suppl: set = set()\n\n\t\tif issue.number[0].isdigit() and is_str(issue.number) and not issue.number.startswith('+'):\n\t\t\tall_suppl.add('number_suppl')\n\t\t\tnumber_suppl."}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\tif len(issue.content) == 1:\n\t\treturn issue.content[0].number + str(issue.content[0].supplier_name)\n\telse:\n\t\treturn issue.content[0].number + str(issue.content[0].supplier_name) + str(issue.content[0].supplier_suffix)\n\n\ndef _parse_issue_details(content, details):\n\tif not len(content) or not len(details):\treturn\t\n#\tdef _parse_issue_details(content, details):\treturn\t"}]}
{"task_id": "62b46740d2f69a53b466171a", "prompt": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\treturn \"%s %s\" % (indent*2 + 3, self.getname())\n\n// Copyright 2019. The Tari Project\n//\n// Redistribution and use in source and binary forms, with or without modification, are permitted provided that the\n// following conditions are met:\n//\n// 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following\n// disclaimer.\n//\n// 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the\n// following disclaimer in the documentation and/or other materials provided with the distribution.\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tret = str(self.obj_)\n\t#print \"[ %s ]\" % prettyString(self.obj_), end=''\n\t#print \"    \"\n\tif debug: print\n\t\tout = indent(ret,'' * (indent + 2))\n\t\tfor op in _opnames:\n\t\t\tout +='@%s'% op\n\t\t\tout += prettyString(self.obj_[op.arg])\n\n\t#print \"  %s\" % out()\n\treturn ret + ''.join(out.splitlines())\n\ndef"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tassert not debug, \"Pretty output in Python 2 are currently not supported.\"\n\tif not self.has_format and self.format:\n\t\tself.read_format(self.format)\n\t\tif DEBUG >= 2: pprint(self.format)\n\t\ndef pretty_repr(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty printed representation of self as a string.\t\"\"\"\n\tassert not debug, \"Pretty output in Python 2 are currently not supported.\"\n\tif not self.has_format and self.format:\n\t\tself."}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tif isinstance(self, DummysContainer):\n\t\treturn \"{}{}\".format(self.pretty(), indent)\n\treturn self.pretty(indent, debug)\n\n\ndef is_in(self, container, item):\n\t\"\"\"Returns True if given element is in given list of items.\"\"\"\n\tif isinstance(container, DummysContainer):\n\t\telem = container.get(item, None)\n\t\tif not elem and len(container[item])!= 1:\n\t\t\treturn False\n\t\tfor (i,j) in enumerate(container[item"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tf = IndentFormatter()\n\n\twhile self:\n\t\tif self is None:\n\t\t\tf.append(\"undefined\")\n\t\telif self.tag!= None:\n\t\t\tf.append(\"%s=%s\" % (self.type, self.tag))\n\t\telse:\n\t\t\tcontinue\n\t\twhile True:\n\t\t\tx=self._format_type(self, indent+1, debug)\n\t\t\tif x is None:\n\t\t\t\tbreak\n\t\t\tf.append(\"%s=%s\" % (self.type, x))\n\t\tf"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\treturn \"\\n{}\\n\\n{}\\n\\n\".format(self.prefix, self.text)\n\ndef prettyPrint(self, indent=0):\n\n\t\"\"\"\n\tPretty printing of an object.\n\t\"\"\"\n\tfor indent in 0, 2, 4, 8, 9, 14, 15, 16, 17,\n\t\t18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28:\n\t\tfor x in (\"{}<{}>{}\".format(x+1, i(self))[1:-1], \"-{}"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tif not isinstance(self, type):\n\t\traise_error('Not a %s object' % pretty.__name__)\n\n\tresult ='[{}]: {}'.format(len(self), tuple(pretty(self)))\n\tif debug:\t#print('--- DEBUG', self)\n\t\tprint(repr(result))\n\treturn result\n\n\ndef prettyvalue(val, depth):\t# pylint: disable=too-many-locals\n\tif isinstance(val, basestring):\n\t\treturn pretty(val=val, depth=depth)\n\telif type(val)"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\ttype = \"self\"\n\tif debug:\n\t\tself._debug_print_struct(type, indent=0, debug=debug, \"  %s = self\" % type)\n\tif type not in {'self', None}:\n\t\tself._warn_type_unusable(type, type == None)\n\t\tif debug: self._debug_print_struct(type, indent=0, debug=debug,\n\t\t\t\"self[%s]' can only be used as %s\" % (type, None))\n\t\treturn \"[...]\"\n\tif type in"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tif debug:\n\t\tprint \"pretty:\", repr(self)\n\treturn self.pretty(True, indent=indent, debug=debug)\n\n\ndef unpretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn the unpretty formatted representation of self.\t\"\"\"\n\tif debug:\n\t\tprint \"unpretty(\", repr(self), \")\":\n\t\tif not self.strfield_default:\t\n\t\t\tself.printfield(\"self.strfield_default\", indent=indent, debug=debug)\n\treturn self.unpretty(False, indent"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\tif debug:\n\t\tindent = \"  \" * 4\n\telse:\n\t\tindent = \"\"\n\t\tindent = \"  \" * 3\n\tout = \"    \" * indent * 3\n\tout +='' * (indent + str(self)) + \"\\n\"\n\tout += \"  \" + str(self.type) + \" [\"  + str(self.value) + ']'\n\tif debug:\n\t\tout += \"\\n\"\n\treturn out\n\n\ndef pretty(self):\t\"\"\" Return a pretty representation of self. \"\"\"\n\t"}]}
{"task_id": "62b46746d2f69a53b4661722", "prompt": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t", "suggestions": [{"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\targs = tuple(\n\t\targ.get_args()[0] for arg in self.args.values()\n\t\tif callable(arg.get_args()[0]) and not isscalar(arg.get_args()[-1])\n\t)\n\n\tif args[0].startswith(\"(\"):\n\t\treturn \"x|\".join(self.args[0] for arg in args), args[-1].strip(\"'|\")\n\tif args[0].endswith(\")\"):\n\t\treturn \"\".join(self.args[0] for arg in args), \"x\""}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\targs = [x for x in args]\n\tassert not is_number(args[0]) and is_number(args[1]), \\\n\t    \"args must be either numbers or numbers or numbers of size 2\"\n\tif len(args) == 2:\n\t\targs = [int(x[0]) for x in args]\n\t\tassert len(args) == 3, \\\n\t\t    \"args must be integers of size 2\"\n\n\tassert len(args) == 2, \\\n\t    \"args must be 2 element lists of integers or lists"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t# This module does not support Python 3 and 2.x\n\timport numpy as np\n\timport scipy.special\n\tif args == []:\n\t\traise SyntaxError(\"Empty absorption.\")\n\t# Use argument syntax:\n\tfor arg in np._asarray(args):\n\t\tif isinstance(arg, np.integer):\n\t\t\targs = args[0::1]\n\t\t\tbreak\n\t\telif isinstance(arg, np.floating):\n\t\t\targs = ~args\n\t\t\tbreak\n\t# We can do much easier here, but the algorithm doesn't work"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tif len(args) == 1:\n\t\treturn args[0]\n\telif len(args) == 2:\n\t\treturn 1.5 * (args[0] + 1) * (args[1] - 1)\t\t# (A & B) = (A | B)\n\telif len(args) == 3:\n\t\treturn 2.5 * (args[0] + 2) * (args[2] - 2)\t\t# (A & B) = (A | B) & D\n\telif len(args) == 4:\n\t"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tfrom scipy import stats\nagain:\n\tfrom math import sqrt\n\tfrom operator import add\n\tfrom random import choice\n\tfrom numba import range\n\tfrom numba.core.fastmath import sqrt\n\n\tif args.dim not in [1, 0]:\n\t\traise ValueError(\"dim must be 0 or 1\")\n\n\tif args.dim is not None and args.dim < 2:\n\t\traise ValueError(\"dim must be greater than 1 or 0\")\n\n\t# Make some variables\n\trands = args.rands\n\t# Create a list of expressions"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tx = args\n\tif len(args) == 1:\n\t\treturn args\n\traise SyntaxError('Absorb syntax error: ')\n\n#include <iostream>\n#include \"testlib/testlib_test.h\"\n\n#ifdef _MSC_VER\n#  include \"vcl_msvc_warnings.h\"\n#endif\n\n#include <svo/svo_api.h>\n#include <svo/svo_context.h>\n#include <svo/svo_object.h>\n#include \"vil/vil"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tn = len(args)\n\tif n == 0:\n\t\traise Sorry(\"empty args\")\n\n\treturn _apply_absorption(args, args[0].x * -1)\n\ndef conj(args, *args):\n\treturn conj_expr(args, args[0])\n\ndef conj_formula(formula, args):\n\targs = args[0]\n\tif args[0].x == len(args):\n\t\traise Sorry(\"arguments on list without args: %a\" % args)\n\treturn formula, _"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\tp = []\n\n\tfor a in args:\n\t\tif p.count(a) <= int(args[a].size):\n\t\t\tp.append(a)\n\n\treturn len(p) > 1\n\ndef lambdify(args, max_seq_length, is_split_seq=False):\n\t\"\"\"\n\tGiven a sequence of expressions, first attempt to absorb a sequence of\n\tsubsequences (as the leftmost subsequence is the \"rightmost\" one). For\n\texample, consider a sequence of \"A & C\" as a"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t# make sure only one argument is used\n\targ = args[0]\n\tx = arg.accept\n\tif isinstance(x, Expression):\n\t\treturn [x]\n\tres = []\n\tfor arg in arg:\n\t\t# only absorb if the number of arguments is 1\n\t\tfor value in arg[1]:\n\t\t\tres.append(value ^ value & args[1])\n\treturn res\n\n\ndef gammas(n):\n\t\"\"\"Given an expression n, extract its gammas.\n\nExample usage::\n\n  "}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\treturn [[(expr for k, expr in args) for expr in A]\n\t\t\tfor A, B in args.iteritems()]\n\n__all__ = ['absorb']\n\n// Copyright (c) 2017 The newPeerX Authors. All rights reserved.\n// License: The BSD 3-Clause\n// See: https://github.com/nxp/newpeerx for the canonical source repository\n\n#include <newpeerx/core/math.h>\n#include <newpeerx/core/hash.h>\n#include <newpeerx/core/stream"}]}
{"task_id": "62b86707b4d922cb0e688c2a", "prompt": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\treturn HookDecorator(hook)\n\ndef _get_hook_type(obj):\n\treturn obj.hook_type\n\ndef _get_class_name(obj):\n\treturn obj.__class__.__name__\n\ndef _get_type(obj):\n\treturn type(obj).__name__\n\n\n# TODO(roth): Change this to a proper function.\ndef _register_custom_class(class_name, custom_class_attrs):\n\t\"\"\"\n\tAdds a class attribute on class object to the registry.\n\n\tArgs:\n\tclass_name ("}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\tif hook is None:\n\t    raise ValueError(\"Hook cannot be None.\", hook)\n\n\tclass Callback(object):\n\t    def __call__(self, *eventIds):\n\t\tfrom. import plugins\n\t        return plugins.get(hook).default_callback(*eventIds)\n\n\t    def __del__(self):\n\t\treturn plugins.get(hook).default_callback(*eventIds)\n\n\tdef register():\n\t\thooks = [callback for callback in (self,) + hook\n\t\t        "}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\tif hook is None:\n\t\t# If the hook is None, do not register the handler.\n\t\treturn None\n\n\t# Create and set the hook.\n\tcurrent = getcurrenthook()\n\tcurrent.hooks[hook.hookname] = hook\n\n\t# Register new handler.\n\tif hook.defaultisfunction and hook.defaultisfunction == 'call':\n\t\thookfuncdef = hook.defaultFunctionDef\n\n\telif hook.defaultisfunction and hook.defaultisfunction =='return':\n\t\thookfuncdef = hook.hookfunctionDef\n\n"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\tdef decorator(f, *a, **kw):\n\t\t\"\"\"\n\t\tDecorator function to add a new listener for a specific hook.\n\n\t\tArgs:\n\t\t\tf (Callable): The hook method.\n\t\t\t*a (object): An arbitrary data type. This is needed in the\n\t\t\t     function being registered.\n\t\t\t**kw (dict): Keyword arguments\n\n\t\tReturns:\n\t\t\tcallable: Decorator which can be used for registering hooks.\n\t\t\"\"\"\n\t\treturn OnHook(f, *a, **kw)\n\treturn decorator"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\treturn HookDecorator(hook)\nregister_hook(Hook)\nregister_hook(HookDecorator)\n\nregister_type_hook(HookDecorator)\n\n/*\n * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n * \n * Licensed under the Apache License, Version 2.0 (the \"License\").\n * You may not use this file except in compliance with the License.\n * A copy of the License is located at\n * \n *  http://aws.amazon.com/apache2.0\n * \n * or in the \"license\" file accompanying this file."}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\tdef decorator(hook):\n\t\tregistry_hook = registry.REGISTRY[hook.type]\n\t\tif not callable(registry_hook):\n\t\t\traise ValueError(\"%r is not a callable.\" % hook.type)\n\t\treturn registry_hook.__call__(hook)\n\treturn decorator\n\t\ndef get_registry(pattern):\n\t\"\"\"\n\tReturn a registry as an os-specific object.\n\nUsage:\tprint(registry[pattern])\n\nThis method is available only for use by the main class, the ``registry`` object\nshould be a single class or it should"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t# noinspection PyAttributeOutsideInit\n\tdef _hook_decorator(cls, hook, *params):\n\t\treturn self._register_cb(hook, _on_hook_callback(cls, hook, *params))\n\treturn _hook_decorator\n\n#------------------------------------------------------------------------------\n\ndef _on_hook_callback(cls, hook, *args):\n\tcls = hook.get_cls()\n\tif cls is None:\n\t\traise ValueError(\"Unknown hook %s\" % hook)\n\treturn cls(*args)\n\t\n#------------------------------------------------------------------------------\n\ndef _register_"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\tif hook in self.handlers:\n\t\treturn self.handlers[hook]\n\treturn hook(*self.registry.on)\n\n/*\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\thookClass = hook.hookClass\n\tregClass  = hook.regClass\n\treturn (\n\t\twrap(cls.__call__, hookClass(regClass=self.__cls__, f=f, args=args))\n\t\tif self.__cls__ is None\n\t\telse _call(hookClass, f, args)\n\t)\n\ndef _call(hookClass, f, *args):\n\t\"\"\"\n\tCall the supplied function with the registered hook.\n\n\tArgs:\n\thookClass (Class): Hook class\n\tf (Function): Function to invoke with this"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\tdef on(self, hook):\n\t\tif (hook, hook_name) in self.registry:\n\t\t\treturn self.registry[hook_name].__add__(hook)\n\t\telse:\n\t\t\traise ValueError(\"Invalid hook %s\" % hook_name)\n\n\treturn on\n\n\nclass NoOp(Exception):\n\t\"\"\"\n\tThis exception is raised by the various registered plugins if a\n\tHook does not exist on the registry.\n\n\tThis error can occur if the PluginRegistry is not initialized.\n\n\tArgs:\n\t\thook (HOOK_"}]}
{"task_id": "62b86729b4d922cb0e688c2f", "prompt": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\tconfig = dict()\n\tconfig.user = user\n\tconfig.etcd_address = etcd_host\n\tconfig.etcd_port = etcd_port\n\n\tif config.etcd_port == -1 or config.etcd_port == 0:\n\t\tconfig.etcd_port = 2377\n\n\treturn config\n\n\ndef create_db_database():\n\tuser = \"db\"\n\tdb_cluster = create_db_cluster(\n\t    default_cluster_name=\"/tmp/cluster\", username=user, password=str_to_bytes())\n\n\t"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\treturn {'user': user,\n\t\t   'cluster': etcd_host,\n\t\t   'cluster_id': etcd_port,\n\t\t   'use_static_account': True}\n\n\ndef base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\" Creates a configuration with some simple parameters. \n\n\tArgs:\n\t\tuser (str): the name of the user for the static authentication\n\t\tetcd_host (str): the host for the database.\n\t\tetcd_port (int): the port for the database.\n\n\t"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\treturn {\n\t\t\"host\": etcd_host,\n\t\t\"username\": user,\n\t\t\"password\": os.getenv(\"DB_USER\") + \":0\",\n\t\t\"port\": etcd_port\n\t}\n\n/**\n *  Licensed to the Apache Software Foundation (ASF) under one\n *  or more contributor license agreements.  See the NOTICE file\n *  distributed with this work for additional information\n *  regarding copyright ownership.  The ASF licenses this file\n *  to you under the Apache License, Version 2.0 (the\n *  \"License\"); you may"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\tcfg = base.Config(user=user, etcd_host=etcd_host, etcd_port=etcd_port)\n\tif cfg is None:\n\t\treturn {\"user\": user, \"cfg\": None}\n\tif \"token\" not in cfg['auth']:\n\t\tcfg['auth']['token'] = base.DefaultToken(\"api:defaulttoken\")\n\treturn cfg['auth']\n\n/* Copyright (C) 2017-2021 Greenbone Networks GmbH\n *\n * SPDX-License-Identifier: AGPL-3.0-or-later\n *\n * This program is free"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\toptions = {\"user\": user, \"etcd_host\": etcd_host, \"etcd_port\": etcd_port}\n\treturn make_app_conf(options)\t\t# type: ignore\n\ndef config_from_json(*args, **kw):\n\toptions = base_config(*args, **kw)\n\treturn config_from_json_safe(*options)\n\ndef config_from_json_safe(*options):\n\toptions = base_config(*args, **kw)\n\tclient = etcd_utils.EtcdClient.new_client(options)\n\treturn"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\treturn dict(\n\t\tuser=user,\n\t\tuser_name=user,\n\t\thost=etcd_host,\n\t\tport=etcd_port,\n\t)\n\ndef redis_config(server, redis_config):\t\"\"\"\n\tCreates a configuration with redis settings.\n\n\tArgs:\n\t\tserver (str): the name of the redis server   \n\t\tredis_config (dict): the redis configuration   \n\t\t\"\"\"\n\treturn dict(\n\t\tserver=server,\n\t\tserver_url=server\n\t\t# redis_"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\treturn dict(\n\t\tcluster=cluster,\n\t\tuser=user,\n\t\tport=etcd_port,\n\t)\n\n\ndef create_server(settings):\n\t\"\"\"Creates a server using the given settings and user.\n\n\tReturns True if the server is valid and False otherwise.\n\n\tArgs:\n\t\tsettings (dict): the settings for the server.\n\t\"\"\"\n\t# create the etcd\n\tetcd_host = get_setting(\"etcd_host\")\n\tetcd_port = get_setting(\"etcd_port\")\n\tetcd_path = os"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\tconfig = config_factory(\n\t\tuser=user,\n\t\tetcd_host=etcd_host,\n\t\tetcd_port=etcd_port\n\t)\n\treturn config\n\n// Licensed to the.NET Foundation under one or more agreements.\n// The.NET Foundation licenses this file to you under the MIT license.\n// See the LICENSE file in the project root for more information\n\nnamespace DotNetNuke\n{\n    public enum FileFormatType\n    {\n        DotnetXml,\n       "}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\tp1 = base_config()\n\tp2 = base_config('root', user,  etcd_host, etcd_port)\n\n\tp2.set('somekey','someval')\n\tp3 = base_config()\n\tp3.set('somehash','someval2')\n\tp4 = base_config('root')\n\tp4.clear('somekey')\n\tp5 = base_config()\n\tp5.set('somekey2','someval3')\n\tp6 = base_config('other')\n\tp"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\tif etcd_port == 0:\n\t\tetcd_port++;\n\n\t# create defaults\n\tdb = None\n\tdb_type = \"\"\n\tuser_name = \"\"\n\tpassword = \"\"\n\tuser_password = None\n\tdb_hosts = []\n\tdb_username = \"\"\n\tdatabase_db_options = []\n\tdatabase_password = None\n\tdatabase_default_password = None\n\tdatabase_host = None\n\tdatabase_type = False\n\tdatabase_name = \"\"\n\tdb_username = None\n\tdb_password = None\n\tdb"}]}
{"task_id": "62b8a4a4755ee91dce50a3d2", "prompt": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t# this is the only way to calculate the epoch_offset.\n\t# It's a number because we use the timezone info to determine\n\t# this.  It's possible that for a datetime like 2012-02-22\n\t# Again, in that case, we just round up to a microsecond.\n\t# In particular, we don't get 0.0 instead of -0.0 - 1; it's\n\t# not the most accurate one\n\tif isinstance(dt.tzinfo, UTC):\n\t\tdt = dt.replace(tzinfo=pytz.timezone.utc"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\treturn datetime_common._timefromutc(self, dt, utc=True).dt\n\n\n# datetime.datetime\n__datetime = _fromutc\n\n\n# datetime.datetime64\n__dt64 = _fromutc\n\n\n# datetime.date\n__from_date = _fromutc\n__date = _fromutc\n__from = _fromutc\n__add = _fromutc\n__sub = _fromutc\n__mul = _fromutc\n_from_date = _fromutc\n\n\n# datetime.date\n__todate = _fromutc\n__dateto ="}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t# from pytz import *\n\t# try:\n\t#\tdt2 = dt.attz_localize(pytz.UTC)\n\t# except pytz.AmbiguousTimeError:\n\t#\t# There's a race condition with strangeness. Since this is the\n\t#\t# simple case, we must be aware of the fact that strangeness is\n\t#\t# not present.\n\t#\t# NOTE: This is not the same reason as *as in the original\n\t#\t# interpretation, however it seems to provide a few additional\n\t#\t"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\td = self._new()\n\tif not dt.is_dst():\n\t\ttz = dt.tzinfo\n\t\tif tz.naive:\n\t\t\td -= timedelta(days=1)\n\t\telse:\n\t\t\td += timedelta(days=1)\n\treturn d\n\n\ndef _utc_now():\t\t\"\"\"\nLookup the *now-UTC* date and return the UTC-equivalent date.\nIf no date, return NaT.\nAlso handles \"am\", \"pm\" and other non-UTC dates.\n\n:rtype: an :"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\treturn self._timeclass(dt)\n\ndef parse_tzone(dt_str):\n\n\t# This function deals with the following formats:\n\t#\n\t#   * +hh, +hh:mm\n\t#   * 00:00:00\n\t#   * +01:00:00\n\t#   * +02.000\n\t#\n\t# In particular the year, month and day must correspond\n\t# to the respective :class:`datetime.datetime` values.\n\tm = re.match(r'(\\d+)\\.(\\"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tif dt.tzinfo is None:\t# pragma: no cover\n\t\traise ValueError(\"tzinfo can never be None.\")\t# pragma: no cover\n\t\tdt = dt.astimezone(tz=True)\n\tdt_start = dt - (tzinfo=tz.tzlocal())\n\tdt_end = dt\n\tts = dt_start\n\twhile ts > dt_end:\n\t\t# TODO: use istime() for now, and convert to bool once the above\n\t\t# problem is fixed.\n\t\tif ts < dt_start:\t\t"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tdt = _normalize_tz(dt)\n\tz = _guess_zone(dt)\n\n\tif z is None:\t\t# the given time-zone didn't have an tzinfo\n\t\tz = tzlocal()\n\telse:\t\t\t\t# the given time-zone had an tzinfo, but is\n\t\t# not local enough to have a time-zone.\n\t\tz = _tzoffset(z)\n\n\tif z < -8:\t\t\t# there's no offset on the standard time-zone\n\t\tz = -8\n\n\ttz,"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tself._tzinfo = _tzinfo_from_pytz\n\treturn self\n\n    def adjust(self, dt):\n        \"\"\"Returns a :class:`datetime.datetime` object that is\n        the \"flip\".\n        \"\"\"\n        return self._constructor(-1,\n            (getattr(self, 'utc', False) or\n             getattr(dt, 'utc"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tif not isinstance(dt, datetime.datetime):\n\t\tlog.warn(\"dt is not a datetime\")\n\t\treturn\n\treturn dt.tzinfo for dt in dt.tz.tzoffset(dt.utcoffset())\n\n\t# if ambiguous, do nothing\n\n\tif self._ambig :\n\t\treturn self._fromutc\n\n\t# Now find the earliest nonambiguous datetime (in the \"fold\"\n\t# state).\n\n\t# For the moment we consider the moment - the *previous*\n\t# nonambiguous datetimes of the timeframe."}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\ttry:\n\t\tself._tz = pytz.timezone(dt)\n\texcept pytz.UnknownTimeZoneError:\n\t\tself._dt = pytz.timezone(self._fromutc(dt))\n\n\tif hasattr(dt, \"utctimetuple\"):\n\t\tself._utctime = dt.utctimetuple()\n\telse:\n\t\tself._utctime = None\n\t\tif hasattr(dt, 'localtime'):\n\t\t\tself._utc_offset = dt.localtime() - datetime.timedelta(seconds=1)\n\t\telse:\n\t\t"}]}
{"task_id": "62b8982f755ee91dce50a241", "prompt": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn relativedelta(self)\n\n###\t\t# The functions listed above for making date fields.\t\n###\n\n# This is not necessary in Python 2.0. What's a big performance hit?\ndef relativedelta_from_timestamp(t):\t\t# <<[ docstring\n\t\"\"\"Return a :class:`dateutil.relativedelta.relativedelta` object\nof any kind (and therefore all dates in a :class:`datetime` object)\nfrom the given timestamp (and NOT the time or date).\n\nThe time, date, or datetime part is required to be included in the"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\tfrom dateutil import relativedelta, localdelta\n\treturn relativedelta(days=self.days, hours=self.hours, minutes=self.minutes,\n\t    seconds=self.seconds)\n\n\nimport warnings\n\nfrom. import _compat\nfrom. import base\n\n# This method and the other member functions are needed to get the 'normal'\n# values out of these classes.\n# This should give identical behavior when a class has more than one constructor,\n# and thus is called with a single argument.  It is recommended to avoid this using an inheritance\n# mechanism.\n"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn dateutil.RelativeDelta(days=self.days, hours=self.hours,\n\t                               minutes=self.minutes, seconds=self.seconds,\n\t\t\t\t\t\tmicroseconds=self.microseconds,\n\t\t\t\t\t\toffset=self.offset)\n\ndef timestamp(d):\t\t\t\"\"\"\n\tReturn the local time for this object represented by *d*.\n\n>>> timestamp(days=-1).timestamp()\nnow - time.time()\n\n.. _"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\tself.normalize()\n\treturn self.to(self._now)\n\n\n# -- Setters for date\n\ndef day(self):\n\t\"\"\"\n\tSpecify the year.  Returns an instance of :class:`dateutil.relativedelta.relativedelta`\n\tif possible.\n\n        >>> days = (year, month, day, hour, tzinfo)\n\t>>> relativedelta(days(2000))\n\tdateutil.relativedelta.relativedelta(days=(2000, 12), tzinfo=UTC)\n\n\t:return:\n\t\tAn"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\traise NotImplementedError(\"Only integer or datetime types are available.  Consider using :py:meth:`relativedelta(days=1.5,hours=2)`\")\n\n# ------------------------------------------------------------------------------------------------------------------------------\n\nclass datetimediff(object):\n\t\"\"\"\n\tThe :class:`datetimediff` class is a combination of `datetime.timedelta`- and ``datetime``-delta\n\tobjects.  Each timedelta can be modified in both ways:\n\t(1) ``add`` the timedelta and add the delta.  (Note that ``delta`` is not the type of delta, but\n\t``abs``"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\tfrom sympy.core.number import Integer as irIntType\n\tfrom sympy.tensor.api import tensor\n\tfrom sympy.core.containers import deque\n\n\tvmin, vmax = tensor([7, 12, 24, 34, 56, 76], tensor.all(irIntType(9 * self.u)))\n\tdt = tensor([23, 24, 24], tensor.all(irIntType(9 * self.u)))\n\n\tdelta = ddelta(dt, vmin + vmax)\n\tresult = deque([(k, v - delta[k][1]"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\treturn relativedelta(*self.__dict__)\n\n#-------------------------------------------------------------------------\n#\n# Code for comparison to specific file methods\n#\n#-------------------------------------------------------------------------\n\n\ndef absolute(value):\t\"\"\"Return a new :class:`dateutil.relativedelta.relativedelta`\nwith the specified absolute value.\n\n>>> absdelta(days=3) == absdelta(days=3)\nTrue\n\nIn general, you can put \"absolute\" (the *absolute* method) in\nthe return value and the method it returns yields the absolute\nequivalent value. For example, given the timedelta \"a\", \""}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\tv, a, b = (self.hour, self.day, self.week)\n\tt, s    = (self.month, self.year)\n\tif    t < 1 or s < 1:\n\t\t# noinspection PyUnresolvedReferences,PyNumberIsIntOrFloat,PyFloatingPointError\n\t\traise ValueError(\"'normalize('relativedelta) requires'month' and 'day'\")\n\tif a > 1:\n\t\t# noinspection PyUnresolvedReferences,PyNumberIsIntOrFloat,PyFloatingPointError\n\t\traise ValueError(\"'normalize('"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from time import time\n\tpass#IMPORTIMPORTIMPORT from array import array\n\n\tif self._isinfinity():\n\t\treturn\n\n\tif self._isin(0):\n\t\treturn\n\n\t# Compute the fraction of a time\n\tfraction=array('f')\n\n\tfor i in self:\n\n\t\tif not isinstance(i, Time):\n\t\t\tfraction[i]=(1.+i)*array(1./len(i))\n\n\tif self._isnegative():\n\t\tif fraction[1"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\tdef normalize(self):\n\t\t\"\"\"\n\t\tReturns a copy of self, with values identical to that saved\n\t\tfor use in arithmetic.\n\t\t\"\"\"\n\t\tself2 = self\n\t\tself.dates.extend(self.dates[-1:])\n\t\tdates = []\n\t\ti = 0\n\t\twhile i < len(self.dates):\n\t\t\tdates.append(dateutil.relativedelta(start=self.dates[i].days,\n\t\t\t\t\t\t\t     days=self.dates[i].days,\n\t\t\t\t\t\t\t     self"}]}
{"task_id": "62b89640755ee91dce50a114", "prompt": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t", "suggestions": [{"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\ttzname = namefunc('utc')\t# name in __time__ attr\n\ts = time.localtime()\n\ts = '%x.%m.%Y' % (s[1:], s[2:])\n\tt = time.localtime(s)\n\treturn str(tzname).encode(sys.getfilesystemencoding())\n\ndef tzname_in_python3(namefunc):\t\"\"\"\n\tChange name to unicode output in Python 3\n\ntzname() API changed in Python 2. To return bytes in Python 3, use this\n\n\ttzname() API returns a"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t# Check for 'None' here:\n\tif not namefunc('None'):\n\t\treturn\n\n\timport sys\n\n\t# Get locale information here (some stuff here might have changed since\n\t# it was created in Python 2)\n\timport psycopg2\n\tif \"default\" in locals():\n\t\tfrom sqlalchemy.dialects.mssql import mssql as mssql\n\t\tfrom psycopg2.utils import mysql as mysql\n\telse:\n\t\tfrom sqlalchemy.dialects.pymysql import pymysql as pymysql\n\t# This"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\tif isinstance(namefunc, functools.partial):\n\t\tif namefunc.__name__ == 'tzname':\n\t\t\treturn sys._gettzname()[0]\n\t\t elif not namefunc.__name__: #py27\n\t\t\t\treturn namefunc\n\telse:\n\t\tpass\n\treturn bytestrings.__name__[1] #py27\n\n\ndef date1(month=0):\n\treturn datetime.date(1, month, 1)\n\n\ndef date2(month, day=0):\n\treturn datetime.date.combine(\n\t\tdatetime.datetime"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t# we've been passed a single string\n\tif isinstance(namefunc, basestring):\n\t\tname = namefunc\n\t\treturn name.decode()\n\n\t# just use namefunc\n\tif isinstance(namefunc, string_type):\n\t\t# utf-8 -> bytes\n\t\tnamefunc = namefunc.encode(\"UTF-8\")\n\t\t# unicode -> bytes\n\t\ttry:\n\t\t\tout = namefunc.encode(\"utf-8\")\n\t\texcept UnicodeEncodeError:\n\t\t\tlogger.exception(\"Trying to use '%s' in %r"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t# Change Python 3's unicode() which uses the Python 2 standard code\n\treturn ''.join([unicode(chr(ord('!'))), char(chr(ord('\\''))),\n\t\t\tchar(chr(ord('A')))])\n\nimport copy\nimport numpy as np                               # import numpy\nimport skimage                              "}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\ttznamefunc(namefunc)\n\ndef new_func(func, **args):\n\timport warnings\n\n\twarnings.warn(\"The new type is deprecated, to prevent double type conversion\",\n                    DeprecationWarning, stacklevel=2)\n\treturn func(*args)\n\n/*\n * $Id$\n *\n * Copyright (C) 2003-2015 JNode.org\n *\n * This library is free software; you can redistribute it and/or modify it\n * under the terms of the GNU"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\treturn namefunc(b'x' * 8, True)\n\ndef tzname_in_xtz():\t\"\"\"\n\tChange unicode output into bytestrings in Python 2. It used to return bytes,\n\tbut was changed to namefunc, so it returns unicode strings\"\"\"\n\treturn namefunc(x'tzname', True)\n\n#---------------------------------------------------------------------------------\n#\tTimezone functions\n\ndef tzname_in_tzlocaldatetime():\t\t\"\"\"\n\tChange to string in UTC local time, e.g. 1.2.3\n\t\"UTC+00:00\" -> \""}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t# This call was added (2.1.3)\n\t# to fix issue #14, causing Py 3 to fail :(\n\tfor m in __builtin__['unicode'], (object,int,float,'g','x','p','v','G','x',\n\t\t\t\t\t\t\t\t\t\t'P','b','s','x','p','d',\n\t\t\t\t\t\t\t\t\t\t'\\x20','2','5','-','4'):\n\t\tself.patchobject(unicode, m)\n\t\tcall_func(m)\n\treturn __bytes__(namefunc)\n\n\tif sys.platform == '"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t# Change the default encoding for Python 2 to utf-8\n\ttznamefunc()\n\ndef _bytes_encode_unicode(string):\n\t\"\"\"\n\tEncode the bytes of string as a unicode string, return as bytes,\n\tthen remove backslashes (rather than encoding this as a bytestring)\n\tin Python3 (it is also used here to not interfere with bytestrings.\n\n\t\tpython3 uses a single bytestring encoding, but this function will\n\t\tdo nothing for bytestrings.\n\t\"\"\"\n\toutput = _encode_unicode(string, b\"\\"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t#print \"*** \", namefunc, \" ***\"\n\treturn string.translate(namefunc(sys.version_info[0]))#bytes()\n\ndef timezone_in_python3(tzname):\t\t\t\"\"\"\n\tReturn a str in the local timezone. (It is not a timedelta\n\t)\tTimezone format:\n\t\tYYYY-MM-DD[TZN]T[TZD].[T[TZK]:[TZN]:\"\n\t\"\"\"\n\n\t#print \"*** \", tzname\n\ttry:\n\t\tsys.version_info[0]"}]}
{"task_id": "62b87d24d292efb640a55670", "prompt": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\ttry:\n\t\tversion = get_info()\n\texcept:\n\t\tcurrent_dir = sys.exc_info()[1].tb_frame.f_code.co_filename\n\t\tsys.stdout.write(current_dir + \" failed to find your info directory.\\n\")\n\t\tif version!= None:\n\t\t\tversion = None\n\t\treturn version\n\tif not version:\n\t\tprint( \"Couldn't find a valid version number.\\n\")\n\t\tsys.stdout.write(\"Version information found in \" + __file__)\n\t\tsys."}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\tif not get_version_info:\n\t\tdef _get_version_info():\n\t\t\ttry:\n\t\t\t\tfrom get_version_info import get_version_info\n\t\t\texcept:\n\t\t\t\tget_version_info = None\n\t\t\t\ttry:\n\t\t\t\t\tget_version_info = get_version_info\n\t\t\t\texcept:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tfrom get_version_info_module import get_version_info\n\t\t\t\t\texcept:\n\t\t\t\t\t\tget_version_info = None\n\t\t\t\t\tif get_version_info:\n\t\t\t\t\t\t"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\tfor line in __file__[:-1] + [ __name__, \"__init__.py\" ]:\n\t\tif line.find(\"__version__\")!= -1:\n\t\t\treturn __version__\n\treturn None\n\n# frozen_string_literal: true\n\n# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\timport datetime\n\n\tversions = {}\n\ttry:\n\t\tversions['v1'] = requests.get(\n\t\t\t'https://github.com/yosysh/kubernetes/releases/latest/v1.0.0'\n\t\t).json()['versions']\n\texcept (urllib.error.HTTPError, AttributeError, IOError, OSError) as e:\n\t\tversions['v1'] = DEFAULT_V1_VERSIONS\n\tversions['v1_3'] = list(\n\t\t[\n\t\t\tstr(v) for v in versions['"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\tparser = get_version_parser()\n\tif hasattr(parser, \"parse_args\"):\n\t\targs = get_versions_parameters()\n\t\targs.parse_args(parser)\n\treturn parser.get_args()\n\n# Copyright (c) 2016 Mirantis, Inc.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\ts = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\ttry:\n\t\ts.connect((ipaddress.getaddress(('', self.default_port_no)),\n\t\t\t\tsocket.SOCK_STREAM))\n\t\tif s.getsockname()[0]!= socket.AF_INET:\n\t\t\treturn ''\n\texcept socket.error as e:\n\t\tif e.errno!= errno.EACCES:\n\t\t\treturn 'Error connecting to server'\n\t\t\"\"\"\n\t\ts.listen(3)\n\t\tn = s.recv("}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\tresult = \"\"\n\tif len(sys.argv)!=3:#args 3:\n\t\tfor arg in sys.argv: #argc-1 arg1=version:\n\t\t\tif \"version\" in arg: #arg2\n\t\t\t\tresult = result + \" \" + arg2\n\treturn result\n\nif __name__=='__main__':\n\t##Run()#main\n\tmain()\n\n/* Copyright (c) 2016-2018, Linaro Limited\n * All rights reserved.\n *\n * SPDX-License-Identifier:\tBSD-3-Clause\n */\n\n#pragma"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\ttry:\n\t\t__package__ = PIL.__package__\n\texcept AttributeError:\n\t\t__package__ = __package__pkg__\n\ttry:\n\t\t__version__ = __package__.split('.')[0]  # strip version number from package name\n\texcept:\n\t\t__version__ = '???'\n\n\n\tif isinstance(sys.version_info, VersionInfo):\n\t\tdef get_python(version):\n\t\t\t\"\"\"\n\t\tTry to grab the version string from a Python-compliant version string.\n\t\tReturns None if the string"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\tprint(__doc__)\n\tprint()\n\tprint(os.path.join(os.path.dirname(get_file('version.txt')),'version.txt')\n\tif not os.path.exists(get_file('version.txt')):\n\t\ttry:\n\t\t\tget_file(get_file('version.txt'))\n\t\t\tprint('INFO: Version file not found. Creating.')\n\t\texcept:\n\t\t\tprint('ERROR: Version file not found. Creating.')\n\n\ndef main():\n\tdoc = getDocumentation()\n\tfile_reader ="}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\twith open(get_datafile('VERSION')) as fd:\n\t\ttry:\n\t\t\tcontents = fd.read()\n\t\texcept:\n\t\t\tcontents = 'None'\n\t\treturn contents\n\n/*\n *                       ######\n *                       ######\n * ############    ####( ######  #####. ######  ############   ############\n * #############  #####"}]}
{"task_id": "62b87d24d292efb640a5566f", "prompt": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tif pieces:\n\t\tpieces = ['%s' % p for p in pieces if '-wide-line-' in p, style if '-wide' in p]\n\treturn [''%s'\" % s for s in pieces]\n\n/*\n---------------------------------------------------------------------------\nOpen Asset Import Library (assimp)\n---------------------------------------------------------------------------\n\nCopyright (c) 2006-2020, assimp team\n\n\n\nAll rights reserved.\n\nRedistribution and use of this software in source and binary forms,\nwith or without modification, are permitted provided that the following\nconditions are met:\n\n* Redistributions of source code must"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\timport io\n\timport re\n\timport sys\n\n\tbase_string = \", \".join(map(str,pieces))\n\n\toutput = []\n\n\tpieces_str = []\n\t# Assume the first version is the most verbose\n\tfor piece in pieces:\n\t\tpieces_str.append(piece.group(1))\n\n\tversion_pieces = []\n\n\t# Print out the full list of modules to show...\n\tfor m in sys.modules:\n\t\ttry:\n\t\t\tif not m.startswith(base_string):\n\t\t\t\tm ="}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tpieces.append(('%%s', '\\n'.join(map(str, style))))\n\treturn ''.join(pieces)\n\n\ndef get_authors_authors(author_list,\n\t\t\t authors=(author for author in authors if author.startswith('-'))):\n\tauthors.extend(map(get_authors_from_dict, authors))\n\tauthors = map(AuthorDataFormatter, authors, lambda x:(x.author, x.authors))\n\treturn authors, authors\n\nuse super::Context;\nuse crate::error::CompileError;\n\nuse super::{Context,"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t# the input might be a plain old-style string (such as'myfile.com',\n\t#'myfile.com/stuff')\n\tif not style or (pieces[0].endswith('/'[0]) and \\\n\t                pieces[-1].endswith('/')):\n\t\tpieces.append(style)\n\tpieces[0] = pieces[0][:-len(style)]\n\treturn ''.join(pieces)\n\ndef main():\n\tif len(sys.argv) < 2:\t\t\t\t\t\t\t#"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tc = color_palette[style]\n\tfont_family = c[0]+'.'+c[1]\n\tfont_style = c[2]\n\tpieces.insert(0, \"%s-%s-%s \" + c[0], font_family, font_style)\n\treturn style\n# }}}\ndef html_build(pieces, style, width):\t\t# {{{\n\ts = []\n\ts.append(' style=\"%s\"'%style)\n\tif width > 0:\n\t\ts.append(' width=\"%s\"'%width)\n\tif not"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tif not style:\n\t\treturn b''\n\tstyles = b''\n\tif pieces:\n\t\tfor index, style in enumerate(pieces):\n\t\t\traw = style or''\n\t\t\tstyles +=''.join((r'{': r'{'.join(_to_re(key)[index+1:] for key in style)}'').rstrip('(') \n\t\t\tif _has_unicode_type(raw) and hasattr(raw, 'encode'): \n\t\t\t\t# unicode, e.g. utf-8, ascii\n\t\t\t\traw = raw.encode('ascii')"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tdef render(pieces):\n\t\tif pieces:\n\t\t\treturn render_pieces(pieces, style)\n\t\telse:\n\t\t\treturn render_pieces(dict(), style)\n\t\n\tif pieces:\n\t\trlist = {}\n\t\trlist['version'] = '{0:0{1}}'.format(pieces[0], pieces[-1][2])\n\t\treturn rlist\n\treturn None\n\ndef _html_to_text(pieces, encoding='utf-8', errors='strict'):\n\t\"\"\"Convert a sequence of (pieces) to a single HTML"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tfor _, p in pieces:\n\t\tp = p.replace('<', '%<')\t# for now make this html special\n\t\t# put this stuff in an appropriate root node for parse\n\t\troot = etree.Element('head')\n\t\troot.append(p)\n\t\troot.set('class', style)\n\t\tself.render(root)\n\ndef format_time(h):\n\treturn '{0:.2f}'.format(h)\n\n// Copyright (c) Microsoft.  All Rights Reserved.  Licensed under the Apache License, Version 2."}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tcontext = {}\n\tif pieces[\"closest-tag\"]:\n\t\tcontext[\"t\"] = pieces[\"closest-tag\"]\n\t\tcontext[\"l\"] = len(pieces)\n\t\tcontext[\"root\"] = pieces[\"root\"]\n\t\tif style == \"long\":\n\t\t\tcontext[\"p\"] = \"{} --{}\".format(context[\"l\"], context[\"p\"])\n\t\telse:\n\t\t\tcontext[\"n\"] = \"{} --{}\".format(pieces[0], pieces[0])\n\t\tif pieces[\"any\"]:\n\t\t\tcontext[\"m\"] = \"{} --style={}"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\tpprint(pieces, 1, style)\n\ndef get_version():\n    return (os.getcw(), os.getro, os.path.join('C','C','C'),\n            os.getcpu().split('.', 1)[0], os.getcpu(), os.syspath, 'GNU')\n\n#include <cstdio>\n#include <vector>\n#include <iostream>\nusing namespace std;\n\nclass Eve {\nprivate:\n    double _r;\npublic:\n  "}]}
{"task_id": "62b87d24d292efb640a5566d", "prompt": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif pieces[0] in ['.', ',']:\n\t\treturn\n\tif (len([p for p in pieces if p[-1] == '.' and p[0]!= '.']) > 0) \\\n\t\tand pieces[-1][2:] == '.':\n\t\treturn\n\tretval = '.'.join(pieces[:-1])\n\tif retval!= '':\treturn retval\n\tretval = '.'.join(pieces)\n\tif retval == '': retval = '.no'\n\treturn retval\n\n\ndef change_all_groups(listfolder, to_add=None):"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif type(pieces[0]) == int: pieces = piece_expandtabs(pieces[0])\n\tfor num in pieces[1:]:\n\t\tif num!='.' and num!='_': return num\n\treturn '.'.join((pieces[0][x] for x in range(1)))\n\n\ndef num2str_digits(n):\n\t\"\"\"\n\tGiven a 32 digit number to store an 8 suffix string with numbers.\n\t\"\"\"\n\tfor n in list(reversed(int(n))).ljust(len(n) * 2, '0').split():"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tl = len(pieces)\n\tfor i in range(0, l):\n\t\tc = pieces[i]\n\n\t\tif c == '.':\n\t\t\treturn True\n\t\tif c == '**':\n\t\t\treturn True\n\n\treturn False\n\n\n\n\n\t\ndef get_days_from_date(date_string):\n\t\"\"\"\n\tReturn days since date in months and years, with optional trailing zeros.\n\tIf there is no date, return a number of days from 0.\n\t\"\"\"\n\tfor i in xrange(len(date"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif pieces[0]!= \".\": pieces[0] = \".%s.\" % pieces[0]\n\treturn pieces\n\n################################################################################\n## Emacs stuff, writing to stdin.\n################################################################################\n\ndef to_str_or_warn(item, message):\n\tdef format(obj, format):\n\t\tif format and hasattr(format, '__args__'): format.__args__ += [\"%s\"] * len(args)\n\n\t\tif len(args)!= 1:\n\t\t\targs = [format % (i,) for i in range(len(args))]"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\ttry:\n\t\tfirst, second, third = pieces\n\t\treturn ((first + second) if first or second else first) + (third if third\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\telse)\n\texcept ValueError:\n\t\treturn.\n\nimport {Component, OnInit, ViewChild} from '@angular/core';\nimport {DateUtil} from '../../../util/date-util';\nimport {ComponentFixture, getSixComponentRef} from '../../../util/fixtures';\nimport {FormlyService} from '../../../../../core/src/components/form-ly';\nimport {FormsComponent, OnInit}"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif pieces is None:\n\t\treturn\n\tp = _translate_pattern(pieces)\n\tif (p[0] == \".\") or (p[0] == \"\") and (p[1] == \".\"):\n\t\treturn\n\tif (p[0] == \"\"):\n\t\treturn\n\tfor _i in range(1, len(p) + 1):\n\t\tif p[_i]!= \".\":\n\t\t\treturn\n\tnewpieces = []\n\tfor i in range(1, len(p)):\n\t\tnewpieces.append(p[i"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif pieces[-1]!= \".\":\n\t\treturn \"\".join(pieces).replace(\".\",\".\").replace(\".\",\".\").strip()\n\treturn \"\".join(pieces).replace(\".\",\".\").strip(\".\")\n\n\nclass Viz(Module):\n\t\"\"\"A Viz is the first in each song.\"\"\"\n\n\tdef __init__(self, x=72, y=64):\n\t\t\"\"\"\"\"\"\n\t\tself.x = x\n\t\tself.y = y\n\t\tself.count = 0\n\n\tdef score_and_count(self):\n\t\t"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif pieces[\"closest-tag\"] not in (\"main\", \"include\", \"master\"):\n\t\treturn \"\"\n\treturn \".\"+\"-\"*(pieces[\"closest-number\"] or 1) - str(pieces[\"tag\"])\n\n\t\ndef clear_old(text):\n\ttext = text.strip()\n\tif not text:\n\t\treturn\n\tl = len(text)\n\ttext = text[:l-1]\n\told = text\n\ttext = text[0:old.index(r'\\n')]\n\ttarget_text = []\n\ti = old.r"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tnotneedtoexpand = []\n\tfor k,v in ((False, None), (False,'self'), (None,'')):\n\t\tnotneedtoexpand.append(x for x in pieces if not (x in ALLOWED_DOTS and any((1,x)==v)))\n\tif len(notneedtoexpand) == 1:\n\t\tnotneedtoexpand[0] = notneedtoexpand[0].toordinal()\n\telif notneedtoexpand:\n\t\tnotneedtoexpand = (notneedtoexpand * (NOTNEEDSEXP or )) + ("}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\tif not pieces:\n\t\treturn\n\tp1 = pieces[0]\n\tp2 = pieces[1]\n\tif p1 == '0' and p2 == '0':\n\t\treturn True\n\telif p1 == '1' and p2 in ('1.', '2.', '.', '%'):\n\t\treturn False\n\tr = False\n\tu = p1\n\tv = p2\n\ts = u\n\te = v\n\twhile e!= '0' or s!= '0' or (v is None and e!= '.') or (v is"}]}
{"task_id": "62b87d23d292efb640a5566b", "prompt": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\timport re\n\tif env is None:\n\t\tenv = os.environ.get(\"SUDO_ENV\")\n\tif env is not None:\n\t\tenv = re.sub(r'[^a-zA-Z0-9-_\\.]+', '', env)\n\tenv = env.replace(\" \", \"_\")\n\tfor name, value in env.items():\n\t\tif value[-1] == \"/\":\n\t\t\tvalue = value[:-1]\n\t\tcmd = [name]\n\t\tif verbose:\n\t\t\tcmd.append('-V')\n\t\t"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tcommand = []\n\tfor command in commands:\n\t\tsub_command_string = None\n\t\tif command[0] == '-':\n\t\t\tsub_command_string = command[1:]\n\t\telif command[0] == '+':\n\t\t\tcontinue\n\n\t\tif sub_command_string:\n\t\t\t# Allow `--help` for a detailed description.\n\t\t\t_subcommand_args.__dict__[command[0]] = sub_command_string\n\t\t\t# we don't actually need the complete argument list:\n\t\t\tcommand = (command[1"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tfor command in reversed(commands):\n\t\t# I just haven't checked for existence of -e on the command line\n\t\tif command in ['check-command','version']:\n\t\t\tcall_wrapper(command, *args, cwd=cwd, verify_output=False, verbose=verbose, env=env)\n\t\telif command in ['list-commands','status'] and callable(args[0]):\n\t\t\tcall_wrapper(command, *args, cwd=cwd, verify_output=False, verbose=verbose, env=env)\n\t\telif command == 'create"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tfor command in commands:\n\t\tif cwd is not None:\n\t\t\tcwd = os.path.realpath(cwd)\n\t\tcmdlist = find_implict_child_commands(command, cwd=cwd, verbose=verbose, hide_stderr=hide_stderr)\n\t\tif not cmdlist:\n\t\t\traise KeyboardInterrupt\n\t\t#print cmdlist[0]\n\t\tresult = run(cmdlist, args, env)\n\t\tif verbose:\n\t\t\tprint''.join(cmdlist)\n\t\t#print result\n\treturn result\n\n\n"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tpass#IMPORTIMPING, warnings\n\n\tenv = envn\t\t\t#potential security\n\tif args is None:\n\t\targs = sys.argv\n\n\tfor i in range(len(list(commands))) :\n\t\tif not os.path.isfile(commands[i]) :\n\t\t\tprint(\"File \\\"%s\\\" not found.\" % (commands[i]), file=sys.stderr)\n\t\t\treturn False\n\n\t\t#check for errors\n\t\ttry :\n\t\t\tretval = os.system(commands[i])\n\t\t\terr"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\ttry: commands(args)\n\texcept KeyboardInterrupt:\n\t\t_abort()\n\texcept Exception as e:\n\t\tif verbose:\n\t\t\tprint >> sys.stderr, str(e)\n\t\t\tsys.stderr.flush()\n\n\ndef run_command(commands, args, cwd=None, verbose=False, env=None):\t\"\"\"\n\tRun 'cmds' command(s).\t\"\"\"\n\ttry:\n\t\tos.execvp(commands, cwd=cwd)\n\t\tprint cmds, env\n\tcatch (err, rv)\t:\n\t"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tfor command in commands:\n\t\ttry:\n\t\t\tsuccess = run_command_code(command, args, cwd, verbose, env)\n\t\texcept KeyboardInterrupt:\n\t\t\traise\n\t\texcept:\n\t\t\terror(\"Command {} raised an exception.\".format(command))\n\t\telse:\n\t\t\terror(\"Command {} executed with exit code {}.\".format(command,\n\t\t\t\tsuccess))\n\tif verbose:\n\t\tprint(\"Run command {} with exit code {}.\".format(commands, success))\n\treturn success\n\n# --------------------------------------------------------------------\n\nif __name__ =="}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import subprocess\n\tpass#IMPORTIMPORTIMPORT import re\n\tpass#IMPORTIMPORTIMPORT import sys\n\tpass#IMPORTIMPORTIMPORT import signal\n\tpass#IMPORTIMPORTIMPORT import getpass\n\n\tprog = os.path.realpath(sys.argv[0])\n\tif verbose:\n\t\ttry:\n\t\t\tsubprocess.call(prog, args, cwd=cwd, env=env)\n\t\texcept:\n\t\t\tpass#IMPORTIMPORTIMPORT import traceback\n\t\t\tprint >>"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tif verbose:\t\n\t\tpass\n\tif not commands:\n\t\tpass\n\tfor s in commands:\n\t\tif verbose:\n\t\t\ts = str(s)\n\t\t\tif s: print(s)\n\t\tcommand = os.path.basename(s)\n\t\tif command[1:] == \"/\":\n\t\t\tcommand = command[0:-1]\n\t\ttry:\n\t\t\tf = os.popen(command)\n\t\t\tsignal.signal(signal.SIGALRM,sigalrm)\n\t\t\tif isinstance(f,int):\n\t\t\t\t"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tcmd = commands[0]\t\t\t# get the command line\n\n\tif env is None:\n\t\tenv = get_default_env()\n\n\t# If not rebuilding a package installer script, try to run it as a package installer. If that fails,\n\t# we want to find a suitable place to install from.\n\tif verbose or hide_stderr:\n\t\twith working_dir(cwd) + cmd:\n\t\t\tfile_base = working_dir[0] + os.path.sep\n\t\t\tcmd = [env] + cmd\n\n\t"}]}
{"task_id": "62b87d23d292efb640a55668", "prompt": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\tconfig = VersioneerConfig()\n\tversioneerConfigSupported = {}\n\ttry:\n\t\tfor name in Configuration().getKeys():\n\t\t\tif name not in versioneerConfigSupported:\n\t\t\t\traise ConfigurationError(\n\t\t\t\t\t\"Invalid value for 'configuration.versioneer_config': \" + name\n\t\t\t\t)\n\t\t\tconfig[name] = Configuration().getValues()[name]\n\texcept MissingResource:\n\t\tpass\n\treturn config\n\n\n_python_import = None\ntry:\n    from importlib import import_module\nexcept ImportError: # Python 2"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\tif not hasattr(settings, \"VCS_NAME\"):\n\t\t# Create the default config information.\n\t\ts = __import__(settings.__name__, *sys.modules, {}, {}, \"\")\n\t\t# Read the comments from the.py file.\n\t\twith s(sys.modules.__file__, \"setup.py\", \"vcs.py\"):\n\t\t# Read the README file.\n\t\twith open(get_path(\"README.rst\"), \"r\") as fh:\n\t\t\ttest_data = fh.read()\n\t\ttest_data = fix_python"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\treturn VersioneerConfig()\n\n\n@contextlib.contextmanager\ndef create_config_file(name):\n\t\"\"\"Create and populate a config file that specifies all the settings for\n\tyour project.  For example, Config().set('foo', 'bar')\"\"\"\n\twith open(Path(__file__).parent / 'config', 'w') as cf:\n\t\tcf.write('{}\\n'.format(name))\n\n\ndef with_config(fname):\n\t\"\"\"Write a given config file and reload the project\"\"\"\n\ttry:\n\t\twith open(fname,"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\ttempdir = tempfile.mkdtemp()\n\ttry: os.mkdir(tempdir)\n\texcept: pass\n\ttry:\n\t\twith open(os.path.join(tempdir, 'VERSION_FILE'), 'w') as f:\n\t\t\twith open(os.path.join(tempdir, 'WHEEL_FILE'), 'w') as f:\n\t\t\t\tf.write('version_file = {}\\n'.format(os.path.join(tempdir, 'VERSION_FILE')))\n\t\t\timport wheel\n\t\t\t_wheel = wheel.get_wheel()"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\treturn VersioneerConfig()\n\ndef get_version():\n\timport pkgutil\n\tglobal APP\n\tif APP is None:\n\t\t# Use the Build directory as readline's default\n\t\ttry:\n\t\t\tapp_name = \"easy versioneer\"\n\t\t\tsubprocess.check_call([sys.executable, \"-m\", \"versioneer-tool\"])\n\t\t\tsubprocess.check_call([\"make\", \"-j\", \"-q\", \"-s\"])\n\t\t\tsubprocess.check_call([\"python\", \"-c\", \"import versioneer\"])\n\t\t\toutput = subprocess.check_"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\tif versioneer_config_path:\n\t\twith open(versioneer_config_path) as config_file:\n\t\t\treturn Config(config_file)\n\n\treturn Config()\n\n\nif sys.version_info[0] == 3:\n\tdef get_config_from_file(filename):\n\t\t\"\"\"Import a configuration file and return a configuration module\"\"\"\n\t\t# this block of imports ensures that the path to the module\n\t\t# contains the correct executable name\n\t\tpath = os.path.dirname(filename)\n\t\tpath = os."}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\treturn VersioneerConfig()\n\ndef get_version(fname):\n    \"\"\"Extract the version from a file name.\n       Ex: pip install -e.\n       \"\"\"\n    try:\n        with open(fname) as f:\n            return eval(f.read())\n    except:\n        return \"\"\n\ndef printable_version(s):\n    \"\"\"Return a string version of the file with"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t# fmt: off\n\tpass\n\n\ndef versions_from_file(filename):\n\t\"\"\"\n\tLoad the content of the config-file into a list of (prefix,\n\tdescription, keywords).  Optionally return keywords from the\n\tlist as keywords, based on --with-keywords.\n\t\"\"\"\n\twith open(filename) as fp:\n\t\tfor line in fp:\n\t\t\t(key, val) = parse_line(line)\n\t\t\tif key in ('__version__', '__versionstring__'):\n\t\t\t\treturn ((key[0], val, None))"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t# Bootstrap the config\n\tconfig = VersioneerConfig()\n\tconfig.read(config_file)\n\n\n\t# Gather unique project ids\n\twith GIT_DIR / \"version_managers\" / __version__:\n\t\t# Build list of project IDs\n\t\tprojects = {}\n\t\tfor entry in sorted(list_dir_for_file(config_file)):\n\t\t\tparts = entry.split('.')\n\t\t\tproject = parts[0]\n\t\t\tif project in projects:\n\t\t\t\tproject += '.' + parts[1]\n\t\t\tprojects[project"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\tsourcelist = []\n\t######\n\tprint \"readme.txt, found\", __file__\t\t\t\t\t\t## NOQA\n\t\t\t\t\t\t# print \"sourcelist.append(sourcelist[-1])\", sourcelist[-1]\t\t\t\t\t\t## NOQA\n\tline = sourcelist[-1].splitlines()[0]\n\tlinenum = int(line)\n\tfilename = \"versioneer.ini\"\n\twith open(filename, \"r\") as fh:\n\t                 "}]}
{"task_id": "62b87d23d292efb640a55667", "prompt": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tif method.__name__.endswith(\"_constructor\"):\n\t\tif not hasattr(vcs,\"__new__\"):\n\t\t\tvcs.__new__ = vcs.new\n\t\treturn vcs.__new__(vcs.config)\n\telse:\n\t\treturn vcs.register_vcs_handler(method)\n\nclass ReInitErrorHandler():\n\t\"\"\"Decorator to force initialization of handlers and errors\"\"\"\n\tdef __init__(self, config):\n\t\tself.errors = {}\n\t\timport __init__\n\t\terror_handlers = __init__(config)\n\t\terrors = dict(("}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t@functools.wraps(vcs.method_handler)\n\tdef method_handler(_self, *args, **kwargs):  # decorator\n\n\t\tdef wrapper(_self):\n\t\t\tvcs(_self).register_vcs(method)\n\t\t\tif kwargs.pop('ignore_unknown'):\n\t\t\t\tsetattr(vcs.handler, method, kwargs)\n\t\t\telse:\n\t\t\t\tsetattr(vcs.handler, method, kwargs)\n\n\t\treturn wrapper\n\n\t# Register VCS handler via its class\n\tvcs.handler = method_handler\n\n# END - REGISTER"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tif not method: return vcs\n\tvcs.__handler__ = method\n\n\ndef _get_vcs_versions_no_git(file, module_name, version_range):\n\t# type: (str, Optional[str], Optional[float]) -> Optional[List[Tuple[int, str, str, Union[str, str]]]\n\t'''\n\tGiven a VCS name, an optional list of possible releases,\n\treturns the vcs versions and their git tags for an object\n\twith the vcs, or just 'None' if its not present\n\t'''\n"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tclass _vcs_handler_helper(type):\n\t\tdef __call__(cls, *args):\n\t\t\t\"\"\"\n\t\t\tFor more details, see ``vc.register_vcs_handler``.\n\t\t\t\"\"\"\n\t\t\tfrom.git import register_vcs_handler\n\t\t\treturn vcs.register_vcs_handler(cls, *args)\n\n\tdef _register_vcs_handler(cls, handler, methods=None):\n\t\t\"\"\"\n\t\tDecorator to mark a method as the vcs handler.\n\t\t\"\"\"\n\t\tif methods is None:\n\t\t"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tclass _method_decorator(object):\n\t\tdef __init__(self, method):\n\t\t\tself.method = method\n\n\t\tdef __call__(self, *args, **kwargs):\n\t\t\tself.method(self.method.__call__(*args, **kwargs))\n\t\t\tif isinstance(self.method, str):\n\t\t\t\tself.method = getattr(self.method, \"__call__\", \"\")\n\nclass HandlerProxy:\n\t\"\"\"\n\tThe same as handler but for object, so it's a method instead of a class.\n\t\"\"\"\n\tdef get_"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tif not os.path.exists(vcs + '.py'):\n\t\traise ValueError('not found {}'.format(vcs))\n\tdef decorator(func):  # decorator\t: this can be invoked\n\t\torig_func = func\n\t\tfunc = vcs\n\t\ttry:\n\t\t\tfunc = orig_func\n\t\texcept:\n\t\t\traise ValueError('invalid VCS (did you forget to call it '\n\t\t\t\t'for something you don\\'t want?)')\n\t\twith open(vcs + '.py', 'w') as f:\n\t\t\tf."}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tregister_decorator(vcs, method)\n\n\ndef create_vulnz_handler(app, data):\n\t\"\"\"\n\tThis method creates a handler for new VCS\n\t@param app\tMain application object\n\t@param data\tObject of the VCS\n\t\"\"\"\n\tdef method_handler(name, func):\n\t\t\"\"\"\n\t\tDecorator to mark a function as a vuln.\n\t\t@param func: the function\n\t\t\"\"\"\n\t\tfunc = functools.reduce(lambda acc, name, func: acc + func, func(data))"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tdef wrapper(*signatures):  # pylint: disable=unused-argument\n\t    existing_handler = getattr(vcs, method, None)\n\t\tif existing_handler is not None:\n\t\t\tfor signature in signatures:\n\t\t\t\texisting_handler(*signature)\n\t\telse:\n\t\t\tfrom.vcruntime import VCRuntime\n\t\t\tvcs = type(self)(*vcs)\n\t\t\tself.set_vcs(vcs)\n\t\t\tapi = VCRuntime.instance.get_module(__name__)\n\t\t\tapi._register_vcs_handler(vcs, method"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tdef hook(*args, **kwargs):  # decorator\n\t    # first check for an override by name\n\t\tif not cls.override_handler(args):\n\t\t\treturn False\n\n\t\t# hook method in order to handle the handler\n\t\t# the first argument (method) and rest (args and kwargs) are the actual method\n\t\t# return True and continue with the hook\n\t\t# if method exists, return that as well\n\t\t# else return False\n\t\tif not len(args) == 1:\n\t\t\treturn True\n\t\t\t# return False"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\tdef decorator(f):\n\t\t@functools.wraps(f)\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tFunction decorator that handles the operation described by\n\t\t\tan operation name, method name.\n\t\t\t\"\"\"\n\t\t\tif'version' in kwargs and callable(getattr(self,\n\t\t\t\tmethodName)\n\t\t\t):\n\t\t\t\t_version = _version(self).hex\n\t\t\telse:\n\t\t\t\t_version = '00'\n\t\t\tobj = VCS()\n\t\t\tobj.__name__ = methodName"}]}
{"task_id": "62b45e945108cfac7f210a4a", "prompt": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tobject_map = {} # for performance reasons\n\tobject_map[\"total\"] = 0\n\n\t# get the correct checksums if the user didn't specify them\n\tif not validate_objects and not check_digests:\n\t\treturn_results(object_map, object_map)\n\n\t# find all the missing objects \n\tobject_map_to_skip = self._objects_to_find_missing_objects()\n\n\tif object_map_to_skip:\n\t\tif object_map_to_skip == object_map_to_skip.keys"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tnum_objects = self._base_count - self._base_count_compacted\n\tnum_objects = num_objects - self._missing_entries\n\tgood_objects = self._objects - self._count_compacted[show_warnings]\n\treturn num_objects, good_objects\n\nclass BaseModel(object):\n\t\"\"\"\n\tA store of models.\n\tThe purpose is to make sure that the store is fully independent.\n\tThis function is used for storing, loading and viewing models.\n\t\"\"\"\n\n\tdef __init__(self, model_name="}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tobj_list =  sorted(hashlib.md5(self.object_metadata().get_raw_data()).iteritems(),\n\t\tkey=lambda x:x[4]['id']) * 2\n\tgood_objects = 0\n\n\tfor root in obj_list:\n\t\tgood_objects += 1\n\t\tif validate_objects and validate_object(root) == True:\n\t\t\tpass # object found, check digest\n\t\tif check_digests:\n\t\t\tif (root in self.digests_tree) == (1 or self.digests_"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t# validation of object tree\n\tvalid_objects = 0\n\tgood_files = []\n\tfor obj_path in self.get_object_paths(show_warnings=show_warnings):\n\t\ttry:\n\t\t\tself.validate_object(obj_path, validate_objects=validate_objects, check_digests=check_digests)\n\t\t\tvalid_objects += 1\n\t\texcept ValueError:\n\t\t\tprint(\"### %s\"%(obj_path))\n\t\t\tprint(\"    %s%s%s\"%(self.get_object_"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tprint('validate_hierarchy | %s | %s ||' % (validate_objects, valid_objects, len(valid_objects)))\n\tassert valid_objects == 0, 'Invalid objects detected in hierarchy'\n\tfor obj in valid_objects:\n\t\tassert obj.objects in self.objects\n\n\t# we call invalidate() if no objects are checked since we are already done validating\n\t# we can't do this directly because we want to see the root of the hierarchy to be valid again\n\t# (no valid objects have been found to be good - they just did)\n\t#"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\terrors = None\n\tdef cb_error(obj, msg):\n\t\terrors = obj\n\t\tif obj.has_children:\n\t\t\ttry:\n\t\t\t\tcb_error(obj.get_children())\n\t\t\texcept: pass\n\t\telse:\n\t\t\tif not obj.has_parents:\n\t\t\t\traise InvalidStorageError(\"no object to parent in %s\"%self)\n\t\t\t\traise\n\t\treturn\n\tif not validate_objects and check_digests:\n\t\tdef cb_check(obj):\n\t\t\treturn\n\t\ttry:"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tnum_roots = len(self.getRoots())\n\tfor i in range(0,num_roots,2):\n\t\tif self.getParent(i)!= self.getRoot(i):\n\t\t\tif validate_objects:\n\t\t\t\tself.setErrorStatus('Hierarchy change in storage root',\n\t\t\t\t\t\t\"The '\"+self.getDirectory()+\"':\"+self.getRoot(i)+\n\t\t\t\t\t\t\" is not in the same storage's root ('\"+self.getDirectory()+\":\"+self.getRoot(i)+\n\t\t\t\t\t\t\") as expected\")\n\t\t\t\tif show_"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from random import random\n\tfrom configparser import ConfigParser\n\tparser = ConfigParser()\n\n\twhile parser.has_option('objectcheck', '0'): #check object files to avoid problems for random\n\t\tmy_object = parser.get('objectcheck', '0')\n\t\tself.objectcheck_valid = False\n\t\ttry:\n\t\t\tif my_object in self.storage_objects:\n\t\t\t\tnum_objects = self.storage_objects.index(my_object)\n\t\t\t\t#logger.warning(my_"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tif validate_objects:\n\t\tself.validate_storage_objects()\n\tnum_objects = 0\n\tnum_good_objects = 0\n\tif self._root is not None:\n\t\tif not show_warnings:\n\t\t\tself.report_data.log_warning('Storage Root is empty!')\n\t\t\tself.report_data.log_warning('Storage objects')\n\t\t\tself.report_data.log_warning('Please add the storage root directory')\n\t\t\tself.report_data.log_warning('or change the storage root directory to:')"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\tobj_storage_dict  = self.obj_id_array()\t #obj id/name pairs must be the same for each object\n\tchk_obj\t\t = obj_storage_dict\t.itervalues()\n\tparent\t\t = self.objects.index('Parent')\n\t#parent \t\t = 0\n\tobj_obj \t\t =  self.objects.index(\n\t\tparent.get('index', 0)\t #this obj cannot be child with parent\n\t)\n\tobj_storage_dict.append(parent)\n\ttry:\n\t\tobj_storage_dict."}]}
{"task_id": "62b45e515108cfac7f210a3c", "prompt": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tif self.c_class_type == self.c_clsid_type:\n\t\tcfl_obj = CFLStorageObject()\n\telif self.c_class_type == self.c_clsid_type + 1 and \\\n\t\tself.c_clsid_type == CFLSID_DISK_FSNODE:\n\t\tcfl_obj =  CFLStorageObject(storage = self._cf.StorageNode._create_disk_node(),\n\t\t\ttype = self._cf.StorageNode_CflStorageObject)\n\telif self.c_"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tself.base = OCLLStorage(self.base, self)\n\tself.update_attributes(getattr(self.base, \"attributes\", {}))\n\n\tself.update(self.base)\n\n\n\t#self.update_volume_cache()\n\n\n\n\n\n\n\n\n\n\n\nclass OCLLVolumeCache(object):\n\t\"\"\"\n\tA container for a storage which tracks changes to a OCFL volume.  \n\t\"\"\"\n\tdef __init__(self, storage, vols):\n\t\tself.storage = storage\n\t\tself"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tc = pyasc.c_object()\n\t#self.pyascencounter = c.pyascencounter\n\tself.name = self.get_string_attribute(\"name\")\n\tself.type = self.get_string_attribute(\"type\")\n\tself.tag = self.get_string_attribute(\"tag\")\n\t\n\tif len(self.tag) > 50:\n\t\tself.tag.resize(50)\n\t\n\t#Add the metadata to the root\t\n\tif str(self.tag) not in str(c.root):\n\t\tc"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t# set up a new configuration parser that gives\n\t# an API name and parameters that can be applied\n\tfrom os.path import join as join_path, join\n\tfrom..configuration_parser import ConfigurationParser as ParametrizedConfigurationParser\n\timport sys\n\tself.default_configuration = ConfigurationParser()\n\tfrom. import DEFAULT\n\tself.configuration = DEFAULT.configuration\n\tfrom os.path import join as join_path\n\ttry:\n\t\tconfiguration_name = join_path(join(join(SYS_ROOT, 'etc'), '.ocfl.conf'), 'ocfl"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tfrom openfda.storage.odb import OCFLStorageRoot\n\tOCFLStorageRoot.init(name=self.name, domain_type='odfl')\n    ocfl_root = OCFlStorageRoot.get_root()\n    self.name,self.description,self.url = ocfl_root.get_info()\n\n  def update_storage_file(self):\t\t\"\"\"update storage file\t\"\"\"\n\ttry:\n\t\t\n\t\tif(self.url == None):\n\t\t\traise ValueError(\"storage_file doesn"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\troot_ = OCFLStore(\n\t\tself.url,\n\t\tself.user,\n\t\troot_expiration=self.root_expiration * self.sec_per_minute,\n\t\tdefault_root=self.default_root)\n\tself.root = root_.get_root()\n\ndef get(key, *,\n\tdefault_root=None\n):\n\t\"\"\"\n\tRetrieve an OCFL object from the storage.\n\tThe key may be either the root key from the user, or a url key.\n\tThe key may be a string of"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tself.root = OCFLBase(len(self.root_sizes))\n\treturn self.root\n\n# END OF FILE\n\n//\n// Copyright 2019 ConsenSys \n //\n// Licensed under the Apache License, Version 2.0 (the \"License\"): you may\n// not use this file except in compliance with the License. You may obtain\n// a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tif self.handle:\n\t\tstorage_dict = OCFL.utils.unpack(self.handle, self.handle.read())\n\telif self.handle:\n\t\tstorage_dict = {}\n\t\tif self.handle[3] == struct.pack('B',0x55): self.handle_data = 'OCFL'\n\t\telse: self.handle_data = struct.pack(self.handle[3], self.handle[4])\n\t\tfor chunk in self.handle[5:]:\n\t\t\tif self.handle[6] =="}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tif self.initialized:\n\t\t\traise RuntimeError(\"Already initialized\")\n\tself.initialized = True\n\tself.cflroot = c_root\n\tself.init_storage(self)\n\n/*\n * Copyright \u00a9 2009 Jon Smirl\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\tself.storageRoot = OCFLStorageRoot()\n\nclass OCFLDiskRootFactory(Factory):\n\t\"\"\"\n\tSingleton object used to instantiate a DiskDiskRootFactory for use in OCFL APIs.\n\n\n\t\"\"\"\n\t# __slots__ = ['storageRoot', 'currentDrive', '_defaultStorage']\n\t__slots__ = []\n\n\tdef __new__(cls, storageRoot, drive):\n\t\tsuper(OCFLDiskRootFactory, cls).__new__(cls, 'OCFLDiskRoot', storageRoot.devicePath)\n\t\tself.storage"}]}
{"task_id": "62b45e2eb89c9fd354170232", "prompt": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tnext = version[:version.index(0)]\n\tnext = map(string, next)\n\tif next == version[-9:] or next == version[:-8]:\n\t\tnext = version\n\treturn next\n\t\n\t\ndef parse_version():\n\tif len(version) < 1:\n\t\treturn \"latest\"\n\t\t\n\tversion = version[0]\n\tnext = next_version(version)\n\n\n\t# Check for non-zero-prefixed versions.\n\tif next == \"*\":\n\t\tnext = \"\"\n\t\tversions.append(next)\n"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tif version < 0:\n\t\tversion_string = f\"0{format_version(version)}\"\n        else:\n            version_string = f\"={format_uint(version)}\"\n    return version_string\n\n\ndef to_string(obj):\n    \"\"\"\n\tCreate a string representation of an instance of :class:`Version` and return\n\tit as a string.  For instance, the :class:`Version` instance is interpreted\n\tas a string; see :func"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tassert(\n\t\t\"Next version is always {0}\".format(version)\n\t\t== version.replace(\"-\", \"_\"),\n\t)\n\treturn next(iter(i for i in VERSIONS if i.name.startswith(version)))\n\n\nclass Version(object):\n\tdef __init__(self, version=\"1.1\"):\n\t\tself.version = version\n\n\tdef __str__(self):\n\t\treturn self.version\n\n\ndef normalize_version(v, is_prerelease=False):\n\tif not is_prerelease:\n\t\treturn v"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\ttry:\n\t\tyield 0x0409\n\texcept NameError:\n\t\t# Not a valid version\n\t\tpass\n\telse:\n\t\tif version+len(sys.version) > sys.maxsize:\n\t\t\t# Not a valid version\n\t\t\tpass\n\t\telse:\n\t\t\tversion += 1\n\t\tversion += 16\n\n\t# Zero-prefix\n\tif version > sys.maxsize:\n\t\traise ValueError(u\"version = 0x%x is out of range\" % (v,))\n\telse:\n\t\tyield '0x"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\traise ValueError('Not a valid version number')\n\n\nclass _VersionParser(object):\n\t_version = None # Used as a cache for versions to be built.\n\n\t\"\"\"A cache for possible future versions, which may have been previously\n\tprocessed; this allows us to keep the versions (and patterns) in a\n\tstring-like form.\n\n\tThis is a \"magic cache\" used to avoid constructing the strings as such.  In\n\torder to make things clearer, if you're not caching your string, you should\n\tuse the string constructor directly -- passing in more than"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tpattern = re.split(r\"(\\d{1,12})\", version)\n\tfor subpattern in pattern:\n\t\tif re.search(r\"^\\d{2}\", subpattern):\n\t\t\treturn int(subpattern)\n\treturn 0\n\n\nclass TamperException(Exception):\n\t\"\"\"Exception class for tampered versions\"\"\"\n\n\tdef __init__(self, message=None):\n\t\t\"\"\"\n\t\tParameters\n\t\t----------\\\tmessage\n\t\t\tmessage -- String to raise in exception\n\t\t\"\"\"\n\t\tException.__init__(self,"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\ti = version\n\twhile i >= 0 and i <= 999:\n\t\ti = int(i)\n\treturn i\n\ndef is_major_minor(version):\t\"\"\"\n\tTrue if'major','minor' number according to given pattern.\n\tFalse otherwise.\n\t\"\"\"\n\tif isinstance(version, numbers.Integral):\n\t\tif version > 1:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\td = 1\n\tm = 10\n\twhile d >= 2 and m > 0:\n\t\td = int(d)"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tv = next(version.split(\"-\"))\n\treturn v[1]\n\n\ndef next_latest_version(v):\t\"\"\"\n\tNext latest version identifier following existing pattern.\n\nMust deal with an empty version (no pattern).\t\"\"\"\n\treturn v[-1] if v[-1].isdigit() else 0 # empty version\n\n\ndef has_version(v):\t\"\"\"\n\tCheck if the version has a matching pattern.\n\tWill return True if the version has no pattern.\n\t\"\"\"\n\tfor s in v:\n\t\tfor pattern in VERSION_PATTER"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\tstart, stop, step, _ = version\n\tmatch_found = False\n\tfor m in step:\n\t\tif m[4]:\n\t\t\tmatch_found = True\n\t\t\treturn version + m[:7]\n\n\tif match_found:\n\t\treturn version\n\treturn \"%d.%d.%d\" % (int(version[:4])//10000, int(version[4:])//100, int(version[7:11]))\n\ndef match(pattern, version):\n\tif not version:\n\t\treturn False\n\tif pattern in patternlist:"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t# Skip this for backwards compatibility but skip\n\t# first version numbers from previous version's pattern.\n\n\tn = 0\n\tfor version in (s.match(vers1) for s1, vers2 in\n\t\t(list(versions_by_file_version.items()) if vers2)):\n\t\tif n < len(ver) or version[:2]!= '0' and n < len(vers2) and \\\n\t\tvers2[:2]!= '0':\n\t\t\treturn u''\n\n\t\tif n < len(ver) and n == len"}]}
{"task_id": "62b45e23e0d4551b0392c90a", "prompt": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tversion_dirs = Version.versiondir_to_filename(version_dirs)\n\n\t# check that we agree to have the latest version\n\tversion_dir = version_dirs[-1]\n\tversion_dir.check_if_latest(self.log)\n\n\t# check each inventory is in the correct version\n\tversion_dir.version_checks()\n\n\n\t# now add the version number for the 'legacy' version check\n\t# if the version is >= 1, then we should still use that data\n\tif version_dir.is_legacy:\n\t\t"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t# TODO: get rid of \"in the future\" now we add the check in _version_out the same\n\n\tfor version in tqdm(version_dirs):\n\t\tself.validate_versions_in_version_history(version, version_dirs)\n\t\tif version in self.version:\n\t\t\tself.error(u'Version {0!s} is already added, check {1!s}', version)\n\t\t\treturn\n\t\telse:\n\t\t\tself.version[version] = True\n\n\t\t# This will return a dict with keys \"is"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tassert self.root.inventory_version[0] == Version.LATEST_VERSION\n\tfor file in self.root.get_file_list(True):\n\t\tif self.root.inventory_version[1] < file.version:\n\t\t\tself.handle_missing_version_inventories(file, version_dirs)\n\tfor file in self.root.get_file_list(False):\n\t\tif self.root.inventory_version[1] > file.version:\n\t\t\tself.handle_missing_version_inventories(file,"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tfor version_dir in version_dirs:\n\t\tif hasattr(self, version_dir):\n\t\t\tVERSION_DIR = self.find_file(version_dir)\n\t\t\tVERSION_IN_SLOT = []\n\t\t\tVERSION_IN_SLOT.append(self.getVersionInSlot(version_dir))\n\t\t\tversion_inc = VERSION_DIR.read().strip() + \".inc\"\n\n\t\t\tif (version_inc!= self.get_version_in_inc(version_dir)) and version_inc:\n\t\t\t\tVERSION_INC_DIR = self."}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tself.version_dir_names = []\n\n\t# create and assign entries\n\twith open(filename.get_path_in_home(\"etc\"), \"w\") as header:\n\t\t(header_version, sha1, sha256, sha512, sha1_checksum,\n\t\t sha256_checksum, sha512_checksum) = version_dirs\n\t\tentries_version = self.version_dirnames + [e for e in header.entries]\n\t\tself.version_dir_names = entries_version\n\n\tself.version_dirs.append([sha1"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tversion_list = []\n\tversion_set = sorted(version_dirs)\n\tfor v in version_set:\n\t\tversion_list.extend(v)\n\t\treturn version_list\n\treturn []\n\n#\n#   SPDX-FileCopyrightText: 2014-2021 Valentin Boettcher <valentinb@gmail.com>\n#\n#   SPDX-License-Identifier: GPL-3.0-or-later\n#\n#   This program is free software: you can redistribute it and/or modify\n#   it under the terms"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tfor version_dir in version_dirs:\n\t\tif not(version_dir in self.version_files):\n\t\t\tself.logger.warning(\"Directory `%s` contains an invalid version file\" % version_dir)\n\t\t\tcontinue\n\n\t\tfor version_file in self.version_files[version_dir]:\n\t\t\tif version_file[0]!= '.':\n\t\t\t\tself.logger.warning(\"Directory `%s` has an invalid version file\", version_file)\n\t\t\t\tcontinue\n\n\t\tfor path in version_file:\n\t\t\troot,"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tversion_dir = None\n\tversion_exists = False\n\tfor version_dir_name in version_dirs:\n\t\tif version_dir:\n\t\t\tversion_exists = False\n\t\t\ttry:\n\t\t\t\tversion_dir_exists = version.parse_version_inventories(\n\t\t\t\t\tversion_dir, version_dir_name, None)\n\t\t\texcept ParserErrorException as ev:\n\t\t\t\tprint(\"Error in validating file: %s\" % repr(ev))\n\t\t\t\tcontinue\n\t\t\tif version_dir_exists:\n\t\t\t\tprint(\"Error!!"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tself.ver_dirs = version_dirs\n\tself.ver_inventories = []\n\troot_inventory_filepath_list = self.get_raw_inventory_filepath_list()\n\tself.v_hash_list = []\n\tfor inventory_filepath in root_inventory_filepath_list:\n\t\troot_dirpath\t\t= inventory_filepath[len(self.ver_dirs):]\n\t\tversion_dirpath = inventory_filepath[len(self.ver_dirs):]\n\n\t\troot_dir = self."}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\tif version_dirs == None:\t\t# version_dirs can be None when empty\n\t\traise Exception('version_dirs == None, did you forget to check them '\n\t\t\t\t\t\t 'before verifying?')\n\n\tvalidate_version_inventories(self, version_dirs)\n\n\tcheck_inventory(self)\n\tif any(e.version.version_number in self.version.versions):\n\t\traise Exception('The inventory and the version in the inventory '\n\t\t\t\t\t\t'file for version %s has an unsupported version' %\n\t\t\t\t\t\tself.version.version"}]}
{"task_id": "62b45e21e0d4551b0392c8ed", "prompt": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tif '.' in path.rstrip(0) or '.' in path or '..'!= path:\n\t\treturn 'root'\n\n\tif (len(path) - len(path.rstrip(0)) == 1 and\n\t\t\tpath.rstrip(0)[1:] == '.') or (len(path) - 1\n\t\t\t\t\t< len(path.rstrip(0)) and path[1:] == '.') \\\n\t\t\t\t\tor not (path is None or path.strip()):\n\t\t\t\treturn 'object', 'file', 'unknown'\n\t\t\tpath = path[1"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tif path in ocflex.OCFL_ROOT_SUBDIR:\treturn \"root\"\n\tif path in ocflex.OCFL_OBJECT_SUBDIR:\treturn \"object\"\n\tif path in ocflex.OCFL_FILE_SUBDIR:\treturn \"file\"\n\tif path in ocflex.OCFL_DIR_OBJECT_SUBDIR:\treturn \"object\"\n\treturn \"file\"\n\n#------------------------------------------------------------------------------\n\ndef write_file_content_type(path,content_type,metadata):\n\t\"\"\"Writes a file in one request to"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tnam = None\n\n\tdef _get_file_name_sep() :\n\t\tif nam is None :\n\t\t\treturn '.'\n\t\telse \n\t\t\treturn nam[0].replace('\\:', '/')\n\n    i = 1\n    j = 1\n    nam = os.path.join\n\t\ttry:\n\t\t\tfor i,j in enumerate(path):\n\t\t\t\tif i == 0 and len(path)==j  and  nam is not None :\n\t\t\t\t    return 'directory'\n\t\t\treturn"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tfor base in path:\n\t\tif not os.path.isdir(base):\treturn\t\"root\"\n\tif not os.path.isfile( path ) and not os.path.isdir( path ):\treturn\t\"folder\"\n\tif os.path.basename( path ):\n\t\treturn\t\t\"object\"\n\treturn\t\t\t\"file\"\n\n\"\"\"\nThe contents of this file are automatically generated by django-cloud-init packages.\n\"\"\"\n\nimport sys\n\n# Get Django.\nfrom django.conf import settings\nfrom django.template"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tif '.' in path:\n\t\tparts = path.split('.')\n\n\t\tif parts[0] == '0':\n\t\t\treturn 'file'\n\t\treturn 'root'\n\n\tif parts[0] == '1':\n\t\treturn 'object'\n\treturn 'file'\n\ndef path_type(path):\t\"\"\"\n\tThe type of the given path.\t\"\"\"\n\n\tparts = path.split('.')\n\n\tif parts[0] in (\n\t\t'0=*',\n\t\t'=*',\n\t\t'*',\n"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\treturn r\"\"\"^\n\t\t|object|object|root|\n\t\t|object|root|\n\t\t|file|file|root\n\t\t|root\n\t\t|root\n\t\t|*\n\t\t\"\n\t\"\"\".join(path[:len(path)-len(r\"\\/*\")])\n\nclass OcflResource(string):\n\t\"\"\"\n\tAn object that can be converted to a L{ICollection}.\n\n\t.. warning:: To keep the \"root\" in a structure, the value must be\n\t\ta string or a L{IC"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tlast = 0\n\tfor f in os.listdir(path):\n\t\tif ((f[:4] == '0' or f.endswith('.', '0')) and \n\t\t\t(c = '0,' not in path) and (not f.endswith('.'))):\n\t\t\treturn \"0=*N'\"\n\t\tif f.startswith('1=') and c == '1' \\\n\t\t\tand not ('2' in path or path == f[1:]):\n\t\t\treturn \"1=*N'\"\n\t\tif ('0N'!= c) \\"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tif len(path) == 0:\n\t\treturn 'root'\n\telse:\n\t\treturn path[-1].lower()\n\ndef find_path_type_root(fsfile):\n\timport os\n\ttry:\n\t\tpath = os.listdir(fsfile)\n\t\treturn path[-1]\n\texcept OSError as e:\n\t\tif e.errno == errno.ENOENT:\n\t\t\treturn 'root'\n\texcept OSError as e:\n\t\tif e.errno!= errno.ENOTDIR:\n\t\t\traise e\n\nclass O"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tif '0=*' in path:\t# this is a file, assume it is NAMATAL\t# TODO: is it right if this is not the same as a \"root\"?\n\t\treturn 'root'\n\telse:\n\t\treturn path\n\nclass OCFL(dict):\t# to avoid extra overhead, it's mostly a string\n\tdef __init__(self, path):\t# to avoid extra overhead, it's mostly a string\n\t\tself.path = path\n\t\tself['storage_root'] = self.path+'/ocefstl/'"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\tif not path:\n\t\treturn ('/', None, None, None),\n\t\t(None, None, None, None),\n\t\t(None, None, None, None),\n\t\t(None, None, None, None),\n\t\t(None, None, None, None),\n\t\t(None, None, None, None),\n\t\t(None, None, None, None),\n\t\t(None, None, None, None),\n\t\t(None, None, None, None),\n\t\t(None, None, None, None),\n\t\t("}]}
{"task_id": "62b45b396decaeff903e1001", "prompt": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t", "suggestions": [{"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tamend_bzparams_params(params, \"Bugzilla\", bug_ids)\n\n/* Copyright 2019 Norconex Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tbz_params = str(params).replace(\"'\", \"\\\\'\")\n\tbz_params += \"\\n\\t\\t\\tmachine.securitylevel: \"+str(bug_ids)\n\tbz_params += \"\\n\\trun=false\\n\\trun=true\"\n\tbz_params += \"\\n\\t\\towner=current_User\\n\\trun=true\"\n\tbz_params += \"\\n\\t\\tdom_id=\" + (bug_ids, )\n\tbz_params += \"\\n\\t\\tdis-id=\" + (bug"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tif len(params) == 0:\n\t\tparams = {0}: b'\\x00\\x11'\n\tif len(params)!= 16:\n\t\tparams = [0]: b'\\x00\\x20'  # bug_num: first 16 bits\t(4 bytes)\n\t\tfor i,n in enumerate(params):\n\t\t\tself.bugnum_str = \"0x%X\"%i\n\t\t\tparams[i*2] = \"%s\" % self.bugnum_str\n\t\t\tself.test_data_url = 'data/bug"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t# FIXME\n\t# FIXME\n\n\n\t# FIXME this is deprecated\n\tif __name__ == '__main__':\n\t\tif params['id'] not in bugs:\n\t\t\tself.bugbugs.append(params)\n\n\n\tif '__author__' in sys.modules:\n\t\tparams['author'] = sys.modules['__author__']\n\n\tif '__license__' in sys.modules:\n\t\tparams['license'] = sys.modules['__license__']\n\n\tif '__version__' in sys.modules:\n\t\tparams['version']"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tfor a_bug, bp in zip(params, bug_ids):\n\t\tif not a_bug and not a_bug_no_id:\n\t\t\tprint(\"BUG ID: %s, no id passed, and bug not found\" % a_bug)\n\t\t\tcontinue\n\t\tif a_bug:\n\t\t\tm = Bugzilla()\n\t\t\tm.save_bug(a_bug, bug_info=bp)\n\tif params:\n\t\tprint(\"%s Bugzilla params were saved in '%s'\" % (bug_ids, bug_ids[0"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tparams.text = \"\"\"   BEGIN Bugzilla()\t\t\t\t\"\"\"\n\tparams.bgcolor = \"0.3 1.0;1.0 1.0\"\t\t\t# white\n\tparams.ccolor = \"1.0 1.0;1.0 1.0;0.0;0.0 1.0\"\t\t# bold and light green\n\tparams.fcolor = \"1.0 1.0;1.0 1.0;0.0;0.0 1.0\"\t\t# light grey and reset\n\tparams.fontcolor = \"\""}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tparams=zip(*zip(*zip(*params)))\n\tassert isinstance(params, list), f'Amend Bugzilla parameters is required: {\"params\"}'\n\n\tbzparam_file = open(bug_ids[0].bzparamfile, \"w\")\n\tbzparam_file.write('params\\n')\t\n\tbzparam_file.close()\n\ndef make_bug_list():\n\tif not os.path.exists(bug_list_dir):\n\t\tos.makedirs(bug_list_dir)\n\toutf = open(bug_list_"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tfor bug in bug_ids:\n\t\tif bug:\n\t\t\tparams[bug] = amend_bugzilla(bug)\n\treturn params\n\ndef unamend_bzparams(self):\t\"\"\" amend bzparams\t\"\"\"\n\tfor bug in self.bugids:\n\t\tparams = amend_bzparams(bug, bug.bugids)\n\treturn params\n\ndef get_bug_params(bugid,bugstatus=\"not checked\"):\n\t\"\"\" get the Bugzilla params for a specific Bug  \"\"\"\n\tbug_id = bugid.lower()."}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tif os.path.isfile(bug_ids_dir):\n\t\tself.bug_ids = open(bug_ids_dir, \"a\").read()\n\n<?php\n/**\n * Zend Framework\n *\n * LICENSE\n *\n * This source file is subject to the new BSD license that is bundled\n * with this package in the file LICENSE.txt.\n * It is also available through the world-wide-web at this URL:\n * http://framework.zend.com/license/new-bsd\n * If you did not receive a copy of the license and are unable to"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\tparams.append({'id': \"id\", 'name': \"id\"})\n\tparams.append({'id': \"id_1\", 'name': \"id(1)\"})\n\tparams.append({'id': \"id (2)\"})\n\tself.client.api.bugzilla(id=params[0]['id'],\n\t\t\t\t\t\t   name=params[0]['name'], bugs = params)\n\tself.assertTrue('id -1' in bug_ids)\t\t# bug name should have been filled in as params\n\t\n\t\n\tself.assertTrue(os.path.exists(\".\"))"}]}
{"task_id": "62b4567ed7d32e5b55cc83d9", "prompt": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tfrom borgmatic import util\n\tfrom borgmatic.yaml.nodes import Node\n\tfrom borgmatic.yaml.nodes.ScalarNode import ScalarNode, MappingNode\n\tfrom borgmatic.yaml.nodes.ScalarNode import (\n\t\tScalarNode,\n\t\tScalarNode,\n\t)\n\tfrom borgmatic.yaml.nodes.ScalarNode import (\n\t\tMappingNode,\n\t\tMappingNode,\n\t)\n\n\tif isinstance(nodes, Node):\n\t\tnodes = ()\n\tnodes = Node(nodes, config_file=config_file"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tfrom datetime import datetime, timedelta\n\tfrom shutil import copyfile\n\n\tconfig_file = args.yaml_file if args.yaml_file is not None else args.default_config_file_name\n\tconfig_path = os.path.join(CONFIGS_DIR, args.yaml_file)\n\n\t# Try to import yaml. Any error will be given in case it's missing for a long time.\n\timport yaml\n\tyaml_config = yaml.load(open(config_file), errors=\"ignore\")\n\n\tif not isinstance(yaml_config, dict"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tdefault_tags=None\n\tdefault_nodes=None\n\n\tif is_nested and deep_merge_nested and tags is None:\n\t\ttags=[default_tags]\n\t\tif tags == [default_tags]:\n\t\t\tdefault_tags=[]\n\t\ttags_copy=deep_merge_nested((tags,))\n\t\tdeep_merge_nodes((deep_merge_nodes, tags))\n\t\treturn deep_merge_nodes((deep_merge_nodes, tags_copy))\n\telse:\n\t\tprint(\"ERROR: there is already a nested borgmatic"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\ttry:\n\t\tparser = yaml.load_file(yaml.dump_file(yaml.dump_file(file), keep_blank_values=False),  )\n\texcept yaml.YAMLError as exc:\n\t\t# The YAML file is already borg\n\t\treturn None\n\treturn merge_borg(list(parser.config.items()))\n\n\ndef merge_borg(file_list):\n\t\"\"\"This function merges a configuration section into the borg definition file,\n\tif merge_section == True.\t\"\"\"\n\tif (not file_"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tnodes = np.array([\n\t\t(ScalarNode(tag='root', type='Node', path='ruamel.yaml:root', value=i))\n\t\t\tfor i in nodes\n\t\t], dtype=object, order='C')\n\tresult_dict = OrderedDict()\n\tfor node in nodes:\n\t\tresult_dict[node.tag] = node.value\n\treturn result_dict\n\n\ndef _dump_json_as_dict(json_data):\n\tresults = OrderedDict([(k, json.loads(v)) for k, v in json_"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\treturn nodes\n\n#!/usr/bin/env python3\n# Kodi Internet - API\n\n# Copyright 2014 - present, Geoffrey T. Perez & The HuggingFace Team.\n# Copyright (c) 2016, Freibata S. Braun <freibata@outlook.com>\n# Copyright 2017 Kodi Software, Inc.\n# MIT License\n\n# This module requires the following packages:\n# Kodi Internet\n\nimport logging\nimport os\n\nfrom.error import ErrorLevelFromCode, FileNotFoundError, InvalidDataError, Permission"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tdef deep_merge_sections(sections):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    [\n        (\n            borgmatic_section: \"section:ruamel:tag:yaml.org,2002:str\",\n            borgmatic_section: \"section:ruamel:tag:yaml.org,2002:str\",\n        ),\n\n... recursively"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tif len(nodes) > 0 and len(nodes[0]) == 1:\n\t\truamel.yaml.merge_dict(nodes[0], nodes[1])\n\n// SPDX-License-Identifier: GPL-2.0-only\n/*\n * Copyright (C) 2007 Lemote Inc. & Technology GmbH, Switzerland\n */\n\n#define pr_fmt(fmt)\t\"lpar: \" fmt\n\n#include <linux/slab.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t# this is needed to support a nested borgmatic\n\tassert nodes == []\n\n\troot_dict = dict_from_yaml(yaml.safe_load(os.path.join('/etc/ruamel/config/deep/sections/',\n\t\t'deep_mergeings', 'ruamel_deep','sections.yaml')))\n\n\t# if there is a section on a key, merge those value against that section\n\tassert root_dict['retention']['ruamel']['sections'] == root_dict\n\n\t# check if we have a section on a key that is not an"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\ttry:\n\n\t\tif Node(r_config_template, r_config_template_sub):\n\n\t\t\tnode_data = Node.__subsection__()\n\t\t\truamel.yaml.dump(node_data, nodes)\n\t\telif Node(r_config_template, r_config_template_sub):\n\t\t\tnode_data = (ruamel.yaml.load(ruamel.yaml.dump(node_data, nodes))\n\t\t\t).get('retention')\n\t\t\troot_node = Node(r_config_template)\n\t\t\troot"}]}
{"task_id": "62b4567ad7d32e5b55cc83af", "prompt": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t# Parse some command line args\n\tif arguments == sys.argv[-1]:\t\t# Command line is at the end, so we're done.\n\t\treturn ArgumentParser(sys.argv)\n\n\telse:\t\t\t\t\t# Command line does not end with a command name, so find the argument\n\t\tcmd_args = [x.strip() for x in arguments[0].split()]\n\t\tif len(cmd_args) >= 2 and cmd_args[1] == '?':\t\t# Get the option if we have it.\n\t\t\toptions = [x.strip"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(prog='pyd3d.cli')\n\tparser.add_argument('--outdir', type=str, default=\"\", help='Output directory to store generated images to. You can also specify this as an absolute path')\n\tparser.add_argument('arguments', nargs='?', const=['-o', '--outdir', '-p',\n\t\t'default'],\t\thelp='Output directory to store generated images to. You can also specify this as an absolute path')\n\tprint parser.parse_args(arguments)\n\treturn parser\n\npackage org.broadinstitute."}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tif isinstance(arguments[0], argparse.ArgumentParser):\n\t\treturn arguments[0]\n\treturn ArgumentParser(default_argument_values, args=arguments)\n\ndef load_model(model_name=None, params=None):\t\"\"\"Load the given model from either the\nstandard or model cache.  If you provide a path to the dataset directory, it will be\nremapped to the path given in the `params` argument.\n\n        :param model_name: the model to load\n        :param params: the"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(usage=\"command-line arguments, --help\", description='Description of the arguments')\n\tfor _ in arguments:\n\t\tparser.add_argument(str(str(shlex.quote(_))) + \":\" + _[2].strip())\t# args without quotes\n\tparser.add_argument(shlex.quote(\"set-config-file\"))\t\t# config file path as path to save data\n\tparser.add_argument(shlex.quote(\"parse-args\"))\t\t\t# argument parsing\n\tparser.add_argument(shlex.quote(os"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser()\n\t# First of all parse the arguments\n\targuments = parser.parse_args(arguments)\n\tif arguments.debug:\n\t\tprint 'parsed arguments for Debug'\n\n\t# Then parse the arguments with default values - print help\n\targuments = parser.parse_args('--version')\n\tfor arg_name,arg in arguments.iteritems():\n\t\tif arg_name in ('help','--help'):\n\t\t\tprint '\\n\\n  --%s<title>help[/title] - command line output\"\n\t\t\tprint argparse.format"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\targument_parser=ArgumentParser()\n\targument_parser.add_argument('command', nargs='+', help='the file that defines the script')\n\targument_parser.add_argument('-a', '--alias', action='append',dest='alias',help=\"alias for the command (if set)\" )\n\targument_parser.add_argument('-e', '--command', action='append',dest='command',help=\"the file that defines the script, e.g. 'cat/myfile --help'\")\n\targument_parser.add_argument('-h', '--help', action='store"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(prog='%prog')\n\tparser.add_argument('target', type='str', help='Target program')\n\targs, _, target = parser.parse_args(arguments)\n\n\treturn args, target\n\ndef parse_arguments_with_path(paths):\n\t\"\"\"Given a command line argument with its paths, parse the argument and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(prog='%prog')\n\tparser.set_defaults(schemes=('http', 'https'))\n\targs, target = parser.parse_"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\treturn ArgumentParser().parse_args(arguments)\n\n// \n// Copyright (c) 2004-2021 Jaroslaw Kowalski <jaak@jkowalski.net>, Kim Christensen, Julian Verdurmen\n// \n// All rights reserved.\n// \n// Redistribution and use in source and binary forms, with or without \n// modification, are permitted provided that the following conditions \n// are met:\n// \n// * Redistributions of source code must retain the above copyright notice, \n//   this list of conditions and the following disclaimer. "}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tresult = {}\n\tparser = _parser()\n\tparser.add_argument(\"-k\", \"--key\", dest=\"key\",\n\t                    help=\"key to use as the default value for all of the arguments\",\n\t                    default=\"my-key\")\n\tparser.add_argument(\"-t\", \"--timestamp\", dest=\"timestamp\",\n\t                    help=\"Date"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(\n\t\tprog='python -c \"import lib;lib.py_parse(lib.get_path(\"lib\"), '')\"./lib'\n\t)\n\tparser.add_argument('arguments', nargs='?', type=argparse.FileType('r'),\n\t\thelp='Command line arguments.')\n\treturn parser.parse_args(arguments)\n\ndef __author__():\n\t\"\"\"\n    Author information in module.\n\t\"\"\"\n\treturn \"Mario Gorski\"\n\ndef __copyright__():\n\t\"\"\"\n    Copyright"}]}
{"task_id": "62b45679d7d32e5b55cc83a9", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tparser.print_usage()\n\tflags = []\n\tfor name in vars(parser):\n\t\tvalues = getattr(parser, name)\n\t\tflags.append( values[0] )\n\tflag = \"\"\n\tfor value in flags:\n\t\tif  values: flag += \",\" if \",\" in values else flag = \" %s\" % \", \".join(map(lambda x:x.expandtabs(),values))\n\tparser.print_flags(flag)\n\nfrom __future__ import absolute_import\n\nimport argparse\nimport datetime\nimport errno\nimport os\nimport"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tdef f(argparse_class):\t\t\"\"\" This is from argparse.custom_argument_parser.  \"\"\"\n\t\treturn [l.decode(\"utf-8\") for l in argparse_class.argparse_options]\n\n\treturn (i for i in f if i)\t\t# XXX: for now this will assume parsing the string as a normal arg\n\treturn (f for f in f if f[0] == \"--\")\n\t\t\n_arg_flags = f(argparse.ArgumentParser)\ndef _get_arg_flags():\n\t\"@returns a"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn ',' * len(parser.names)\n\n\ndef _escape_values(names):\n\tescaped = ''\n\tfor name in names:\n\t\tescaped += str(name).encode(encoding='ascii')\n\treturn escaped\n\n\ndef _escape_param(arg):\n\tescaped = arg.replace(_ESCAPE_SEPARATOR, '\\\\').replace(_ESCAPE_SLASH, '\\\\SLASH')\n\tescaped = escaped.replace(_ESCAPE_VALUE_DELIM, _ESCAPE_VALUE_DELIM_ESCAPE)\n\treturn escaped.replace(_ESCAPE_DELIM_SEPARATOR, _ESCAPE_DEL"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn _stringify_flags(parser.flags)\t \n\ndef _parse_flags(args, defaults):\n\tret = 0\n\tif args is None and not defaults:\n\t\tret |= F_DEFAULT #  0=no flags, 1=no help\n\telif args is None and args is not None:\n\t\tret |= F_ARGS #  0=no flags, 1=force argument string\n\telif args is not None and defaults is not None:\n\t\tdefaults.update(args._defaults)\n\telif args is None and defaults is None:\n\t\tret"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = pargs(parser).get_flags()\n\treturn ''.join(map(lambda x: \"-X{:02b}\".format(x), flags))\n\n\nclass NoSpacePad(ArgumentParser):\n\t\"\"\"A parser to set the padding on arguments.\"\"\"\n\n\tdef parse_args(self, parser, args, required=False):\n\t\tparser.add_argument('--padding-space', help='space padding', nargs='+',\n\t\t\tdefault=DEFAULT_PADDING_SPACE)\n\n\tdef _get_field_names(self):\n\t\treturn ['"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\targs = parser.parse_args()\n\treturn (tuple(args.args) if len(args.args) > 0 else ())\n\nimport { RequestManager } from 'klasa';\nimport HttpError from '@/web/controllers/HttpError';\nimport {\n  Get,\n  GetByTag,\n  GetTaggedTag,\n} from '../utils/Get';\n\n// \u63a7\u5236\u53f0\u67e5\u8be2\ninterface TagResponseDTO {\n  // \u5907\u6ce8ID\n  id: string;\n  // \u540d\u79f0\n  data: any;\n  // \u6807\u6ce8\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tf = parser.parse_args(args=[])\n\tflags = []\n\tfor aname in f:\n\t\tflags.append(f[aname])\n\treturn flags\n\ndef parser_add_flag(parser):\n\tparser.add_argument('-V', '--verbose', action='count',\n\t\t\t default=0,\n\t\t\t help='be verbose if you don\\'t want them')\ndef parser_add_flags(parser):\n\tparser.add_argument('-V', '--verbose', action='store_true',\n\t\t\t\thelp='be verbose if you don\\'"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\timport warnings\n\twarnings.warn('The \"argflags\" module is deprecated. Please use argparse.NamespaceArgument instead.')\n\treturn argparse.NamespaceArgument\n\n\n# -- Command-line usage --\n\n\ndef parser_main(parser):\t# noqa (line-length, error-return)\n\tprint('\\n')\n\tprint('To run an ArgParser instance, pass an argparse instance, or specify a '\n\t\t'default argument parser that loads the flags.')\n\tprint('The parser instance can be: \\n')\n\tprint('\\t$ %s  %s %% -"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tif hasattr(parser, \"add_argument_group\"):\n\t\treturn \"\"\n\targs = parser.parse_args()\n\targs = filter(None, args.arguments)\n\tnewargs = \"\"\n\tspace = kwargs = None\n\tcomma = None\n\tfor arg in args.arguments:\n\t\tif arg[:2] == \"args\":\n\t\t\tif comma:\n\t\t\t\tnewargs += \",\"\n\t\t\telif comma and arg[1:] == \", \":\n\t\t\t\tspace += \",\"\n\t\t\telif space and arg[1:] == \":\":\n\t\t\t\tspace ="}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tdef convert_flags(parser):\n\t\tif parser.default is None:\n\t\t\tdefault = 'default=None,{}'.format(parser.name)\n\t\telse:\n\t\t\tdefault = parser.default\n\t\treturn '{}={}'.format(default, parser.long_desc)\n\tparser.add_argument('parser', help=parser.help)\n\treturn(convert_flags(parser))\n\n\n# Some flags used to control how the command gets parsed.  You can set this to\n# do something similar but need to be a bit more flexible.\ncommand"}]}
{"task_id": "62b45665d7d32e5b55cc8365", "prompt": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tns = {}\n\twhile (len(unparsed_arguments) > 0) and (('' == unparsed_arguments.pop()[0]) or (int(unparsed_arguments.pop()) >= 1)):\n\t\tns[unparsed_arguments.pop()] = unparsed_arguments.pop()\n\treturn ns\n\ndef main():\n\t\"\"\"\t\tParse args.\n\t\t\n\t\tWe parse the command-line arguments so these can be used for \"global\" subparsers like \"--\" that\n\t\tcan be registered via the \"register_parser()\" method.\n\t\n\t\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\targs = copy.deepcopy(unparsed_arguments)\n\tif 0 <= len(args) < 6:\n\t\traise argparse.ArgumentError(\"too few arguments: {0}\".format(args))\n\n\tglobal_parser_map = {k: v for k, v in globals().items() if k!= \"global\"}\n\tfor k, v in args[1:] or []:\n\t\tglobal_parser_map[v] = k\n\tglobal_parser = globals()\n\n\t@parse_parser(global_parser)\n\tdef set_args(*args, **"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser = argparse.ArgumentParser(dest=os.environ.get('GITHUB_API_ARGS', '') + '\\nGITHUB_API_ARGS=\"https://github.com/zulip/GITHUB_API\"\\n', description='API for API requests.')\n\tfor arg in unparsed_arguments:\n\t\ttry:\n\t\t\tparser.add_argument(arg, **arg)\n\treturn parser.parse_args()\n\n\nclass ApiServer():\n\t\"\"\"\n\tBase class that provides functionality that should help to perform API server functionality.\n\t\"\"\"\n\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tglobal_arguments = \"\"\n\treturn {\"name\": globals()[*unparsed_arguments[1:]]\n\t\tif \".\" in unparsed_arguments[1] else unparsed_arguments[1]}\n\ndef run(path):\n\tcommand = command_file_util.get_command_line()\n\tglobal_arguments = parser_and_parse_arguments(command, *global_arguments)\n\tif command_file_util.check_only_args():\n\t\tif \"quiet\" in global_arguments and int(global_arguments[\"quiet\"]):\n\t\t\tlogger."}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t# Use os.environ if available to avoid hardcoding\n\tfrom argparse_parser_utils import get_default_parse_env_vars\n\tfrom functools import partial\n\tfrom os import path, getenv\n\n\tparsed_env = get_default_parse_env_vars(path=path.home() + \"/.cixa/open-xmas\")\n\tif not os.environ.get('OPENXMAS_HOME'):\n\t\tfrom os import path as path_lib\n\t\tos.environ['OPENXMAS_HOME'] = path_lib\n\t# The environment"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser = _Subparser(parse_argument, default_argparser='')\n\tparser.program().parse()\n\tparser.command('python_main.py')\t# print version if called\n\treturn parser.data\n\n\ndef parse_arguments_recursive(unparsed_arguments):\t# parser calls this recursively, so we have to make a tree with only subparser_names, and\n\t# recursively parse those subparser_names.\t\t\t# the full (if any) subparser name can be 'global', 'python_main', or 'python_main.py'\n\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparams = dict()\n\tparser_options = dict()\n\targs = unparsed_arguments\n\n\ttry:\t\n\t\twith open_script(args.path) as script:\n\t\t\twith open_parser(parser_options, parser=create_parser(\n\t\t\t\tscript=script)) as parser:\n\t\t\t\tfor token in tokenize(unscript(script)):\t\t\n\t\t\t\t\n\t\t\t\t# Check for command-line arguments\n\t\t\t\tif token[0:6] == \" \": \t\t\t   # Must be at least 7 characters\n\t\t\t\t\tif len(token) < 11: return  "}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t# Set args to be parsed once only\n\targv = unparsed_arguments\n\topts = globals()\n\n\topts = dict(filter(lambda x: x!= \"help\", opts.items() - set(opts.items())))\n\topts[\"options\"] = \"all, \" + str(opts.get(\"options\"))\n\n\tenv = opts[\"environment\"] if \":\" in opts[\"environment\"] else parse_environment()\n\n\t# parse environment specific data\n\tif env[\"args\"]!= \"\":\n\t\targs = []\n\t\tfor arg in args_dicts(env"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument('--run-time-subparser', action=\"store_true\",\n\t                    help='Run subparser immediately after the given one.')\n\tparser.add_argument('--verbose', action=\"store_true\",\n\t                    help='Display all the information about subcommand being called.')\n\tparser.add_argument('--list', action=\"store_true\","}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tif len(unparsed_arguments)==1:\n\t\tsubparser, ns = *unparsed_arguments\n\telif len(unparsed_arguments)==2:\n\t\t# first subparser, which is main subparser:\n\t\t# main: use this namespace (and ignore anything outside subparser's namespace)\n\t\t# subparser: if subparser is main, parse it\n\t\tsubparser, ns = (unparsed_arguments[0],) + unparsed_arguments[1:]\t# main and subparser have main namespace\n\telse:\n\t\t# second subparser"}]}
{"task_id": "62b45665d7d32e5b55cc8364", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\targnames = list(subparsers.keys())\n\targnames.sort(key=lambda x: x.__name__)\n\tsubparsers = {}\n\tdef parse_subparser():\t\tsubprocess.call(['bash', '-n', '-'] + *argnames)\n\t\t# set all parser state\n\t\tsubparsers[subparsers.__name__] = argparse.ArgumentParser()\t\t# make parser\n\t\tsubparsers[subparsers.__name__].add_argument(\"--repository\", help=\"repository for subparser\")\t\t# add subparsers to a repository\n\t\t# add"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\targ_parser_args = {}\t\t\t# a dict mapping all subparsers to some parser arguments\n\tremaining_arguments = {}\t\t\t# a list of all remaining arguments not claimed by any subparsers\n\tfor name, parser in remaining_parsers:\t\t\t\t# for all subparser arguments\n\t\tsub_parser = parser.get_sub_parser()\t## returns a subparser instance that can be reused further\n\t\tif sub_parser is not None:\t\t\t\t## if we already have a subparser (or a single) with a certain name - the name won't\n\t\t\t\t\t\t\t##"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tunparsed_arguments = subparsers._subparsers.clone()\n\tunparsed_arguments.__dict__.update(subparsers)\n\tunparsed_arguments.add_subparser(subparsers.get_parser(\"help\"))\n\n\tremaining_args = parse_unparsed_arguments(unparsed_arguments)\n\tparsed_args = unparsed_arguments.add_subparser(subparsers.get_parser(\"subparser\"))\n\twhile True:\n\t\tif not (remaining_args or parsed_args):\n\t\t\tbreak\n\t\tunparsed_args.consume()\n\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t# First extract the parsed arguments from subparsers\n\tnamespace = get_namespace(shorthand=unparsed_arguments.get(subparsers.SUBPARSER_ARGUMENTS_KEY, \"\"))\n\tresult: list = []\n\tfor parser in namespace.subparsers:\n\t\t# Check for remaining arguments claimed by them\n\t\tif parser.name not in unparsed_arguments:\n\t\t\targ_subparsers = getattr(shorthand, (subparsers.SUBPARSER_ARGUMENTS_KEY, parser.args.id))\n\t\t\tif not arg_subparsers:\n\t\t\t\tcontinue"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tnamespace_not_claimed = []\n\tfor arg in unparsed_arguments:\n\t\t# for each remaining arguments we also return the action's subparser\n\t\t# as it is a different subparser, so we are careful here\n\t\tsubparser_name = arg.subparser\n\n\t\tif subparser_name in subparsers: # for all subparsers\n\t\t\tif name in namespace_not_claimed: # only keep the first argument\n\t\t\t\treturn (\"Subparser {0} called (namespace={1}) on subparser {2}, \"\n\t\t\t\t\t\t\"remaining arguments still"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tparsed_arguments = {}\n\tnamespace = default_namespace()\n\tfor args in unparsed_arguments:\n\t\tparsed_arguments[args[0]] = args[1]\n\t\tfor subparser_name in args[2]:\n\t\t\tnamespace.subparsers.append(subparser_name)\n\t\tif len(args) == 3:\n\t\t\tparsed_arguments['--single'] = namespace.subparsers\n\tparsed_arguments.update(subparsers)\n\treturn parsed_arguments, namespace.subparsers\n\ndef get_action(parser, parsed_arguments,"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t# Construct the parsed arguments and its namespace\n\taction_namespace = list()\n\taction_kwargs = {}\n\t# Construct the new subparsers for any subparsers with the given namespace\n\tfor namespace in subparsers.values():\n\t\tif namespace == \"__init__\":\n\t\t\tcontinue\n\t\tfor subparser in namespace.subparsers:\n\t\t\t# Get the right remaining arguments if any\n\t\t\tif not isinstance(subparser, argparse.ArgumentParser):\n\t\t\t\taction_options = get_action_options(subparser)\n\t\t\t\t# Create a new subparser and register its"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tsubparsers = list(set(subparsers.parse_args(' '.join(args for arg_name in args\n\t\tif arg_name.startswith('--namespace')))\n\t\t.values_list() for args in unparsed_arguments))\n\tparsed_arguments = set()\n\tfor subparser_name, subparsers in subparsers:\n\t\tsubparsers = {k: subparsers[k] for k in subparser_name.split('--')}\n\t\tfor namespace, argparse_name, parser_kwargs in subparsers.items():\n\t\t\tparser_kwargs['parser'] ="}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tpartial_parse_dicts = {}\t\t# Used to keep track of partial subparsers\n\tpartial_parsed_args = []\t\t# Used during parsing\n\tpartial_parsers = []\n\tpartial_ns_to_params = {}\t\t# Name -> ParameterList mapping\n\n\t# First, make the key names and values\n\tk = list(shot.split())[0]\n\tfor ks in k.split():\n\t\tfor t, v in shot.split('='):\n\t\t\tshot = ks + '=' + t + v + '='\n\t\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tresult = {}\n\n\tfor a_name in unparsed_arguments:\n\t\tsubparser = subparsers[a_name]\n\n\t\tresult[a_name] = parse_subparser_arguments(subparser[\"input\"] .upper(), subparsers)[a_name]\n\n\treturn result\n\ndef process_args (args): ## called once before parse_cmd().\n\n\t# read command-line arguments\n\n\tparserclass = argparse\n\n\tparserclass.parse_args   = _parse_parser\n\n\tparserclass.parse_args_all"}]}
{"task_id": "62b45665d7d32e5b55cc8363", "prompt": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tfrom _python_build_struct import *\n\tdef f():\n\t\ttry:\n\t\t\tparser_args = parser_type = 0\n\t\texcept: pass\n\t\t# get the top-level parser as a new argument.\n\t\tparser = top_level_parser = get_parser(str(parser_type))\n\t\tfor p in parser_args:\n\t\t\tif p!= \"\":\n\t\t\t\tprint(p)\n\t\t\ttry:\n\t\t\t\tset_parser(str(parser_type))\n\t\t\t\tparser = top_level_parser = get"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tif sys.version_info < (3,0):\n\t\tsys.compat_set_unicode_decoder(cp1254.utf_decoder)\n\t\tsys.compat_set_str(cp1254.version_strings[0], 'python %s' % __version__)\n\n\tdef get_parser(subparsers, optionflags, name, help):\n\t\treturn subparsers.get(name, optionflags, help)\n\tp.subparsers.insert(0,[\n\t\t('-T', '--tokens=TOKEN', 'tokens=', (list, bool)),\n"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tfrom distutils.version import LooseVersion\n\ttry:\n\t\timport pycparser\n\t\tfrom pycparser.error import ParseWarning\n\texcept ImportError:\n\t\timport warnings\n\t\twarnings.warn(\"No pycparser available\")\n\t\n\t\tfrom distutils.version import LooseVersion\n\t\tfrom cPickle import dumps\n\t\tfrom collections import OrderedDict\n\n\t\tclass CParser(pycparser.CParser):\n\t\t\t\"\"\"Computes the Python implementation of ``c_cpp_info``\n\t\t\twith the available functionality.\t\n\n\t\t"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t# get the parser that is the first on the toplevel:\n\ttop_parser = toplevel(get_parser(), parser=Parser(allow_non_ascii=True))\n\tsubparsers = []\n\t# add the parsers as child parsers and their subparsers as attributes\n\tfor m in top_parser:\n\t\tsubparsers.append(m)\n\t# parse the subsubparsers (only these subparsers can parse nested parsers)\n\tsubparsers = {p: sub if parse_nested(p) is not None else None for p in subparsers}\n\tsubparsers['"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser = argparse.ArgumentParser(description='# Test Top-level Parser.')\n\tparser.add_argument('--parser_class', help='Parser class(s) to use.')\n\tparser.add_argument('-v', '--verbose', action='count', nargs='?', const=False,  default=0,  help='Verbose Output.')\n\tsubparsers = parser.add_subparsers()\n\n\tfor parser in subparsers:\n\t\tsubparsers.add(parser)\n\t\tadd_parsers(parser)\n\n\t\tif parser.get_help() == \"--verbose\":"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser = argparse\n\n\tdef parse(command):\n\t\treturn parser.parse_args(command[0])\n\n\tparser.add_argument(\"-c\", options=(\"check\", \"ignore\"),\n\t\t\t\t\t\t\t\t\t help=\"Output file to check against the given flags, and optionally ignore the flag\")\n\tparser.add_argument(\"-r\", options=(\"check\", \"ignore\"),\n\t\t\t\t\t\t\t\t\t help=\"Output file to check against the given flags, and ignore the value of the flag\")\n\tparser.add_argument(\"-h\", options=[None], help=\"print help\")\n\tparser.add_argument"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\troot = make_parsers()['__root__']\n\tprint(root.__parent__)\n\troot.add_argument('-v', '--version', action='version', version='%(prog)s'+ __version__)\n\troot.add_argument('-f', '--force', action='store_true', dest='force')\n\treturn root.parse_args()\n\ndef display_help(help, parser=None):\t\"\"\"\n\tDisplays the list of commands provided as command strings.\n\tIf help is None, display the help available in the command line\n\tof the program."}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser = tvm.topave.build(\n\t\tinputs=[\n\t\t\t{\"type\": \"primvar\", \"name\": \"a\", \"attrs\": {}}),\n\t\toutputs=[{\"type\": \"tensorprim\", \"name\": \"o\", \"attrs\": {}}}])\n\tops := parser.graph().node.op\n\tops[\"primvar\"] = tvm.var(\"var\")\n\n\tprint(\"The top-level parser in the graph.\")\n\tprint(ops)\n\treturn parser\n\n\ndef topdown(graph):\n\t\"\"\" Create a top-down graph"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Python-level parser for Raspberry pi/Pi (rev. 1.5)\")\n\tparser.add_argument(\n\t\t\t\"--pythonpath\",\n\t\t\thelp=\n\"\"\"\nPath to a python compiled version of the pi/Pi project.\n\nNote : The directory will be the top-level directory which is the root\nof the file.  This is the Python level only, and won't have any output.\n\"\"\")\n\tparser.add_argument(\"-n\", help=\"list of all possible target directories (i.e. packages"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\timport re\n\n\targs = re.split('\\s*,\\s* ',\n\t\t\tsys.argv[0])\n\n\tfor i, line in enumerate(sys.argv):\n\t\tsys.argv[i+1] = os.path.join(args[0], line)\n\n\treturn args\n\nimport { Middleware } from './types';\nimport { useDispatch } from './useDispatch';\nimport { mapDispatchToProps } from './mapDispatchToProps';\n\nconst defaultProps = (): {\n\tdispatch: Dispatch;\n\tdispatchFromMiddleware<M>;"}]}
{"task_id": "62b438ba66fea644fe22cca2", "prompt": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tyaml_schema_file = 'borg-solo.yml'\n\tyaml_schemas_path = os.path.join(utils.BASE_CONF_DIR,'schemas')\n\n\t# create a new config\n\tconfig = configs.default_factory_config(yaml_schema_file, yaml_schemas_path)\n\t# save this node\n\tconfig.dump(nodes)\n\n\t# write out merged file #\n\tf = open(paths.BORG_SOLO_MERGED_TEXT, 'w')\n\t# add the merge node\n\tnodes.append"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\treturn nodes\n\nfrom twineproject.core import configManager, loadConfigManager, project_path\nfrom twineproject.utils.yaml.file import YAMLConfig\nfrom twineproject.core.utils.yaml_utils import check_yaml, list_all_in_context\n\nwith open(\"data/config/config.yml\") as config_file:\n\tcfg = YAMLConfig()\n\n\tcurrent_path = project_path(\"Data\", project_path(\"config\"))\n\n\tfor key, value in [\n\t\t(\"tag\", \"yaml.org,2002:str\"),"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tassert isinstance(nodes, list)\n\tassert len(nodes) == len((nodes,))\n\tfor idx, node in enumerate(nodes):\n\t\tassert idx == len(nodes) -1\n\t\tprint(node.tag, node.value)\n\t\tdeep_merge_nodes(node)\n\n\treturn nodes\n\ndef deep_update_borgmatic_file(borgmatic_config_file=None, borgmatic_config_file_ref=None):\n\t\"\"\"If borgmatic configuration is already reflected to a file, do nothing.\"\"\"\n\n\t"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\"\"\"\n\t\t\t\"\"\n\t\t\t\"\"\"\n\t\t\t.. autosummary::\n\t\t\t    :nosign:\n\t\t\t\"\"\"\n\tdef deep_merge_nodes(nodes):\t\"\"\"\n\t\tDeep merge any node values corresponding to duplicate keys and return the result. \n\t\tIf there are colliding keys with non-MappingNode values (e.g., integers or strings), \n\t\tthe last of the values wins.\n\t\t\"\"\"\n\t\t\"\"\"\n\t\t\t.. autosummary::\n\t\t\t\t:nosign:\n\t\t\"\"\"\n\tdef save"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tfor node, nodeValues in nodes:\n\t\tfor key, value in nodeValues:\n\t\t\tif value is None:\n\t\t\t\tnode._attributes[key] = None\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\torig = node._attributes[key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tnode._attributes[key] = orig = value\n\t\tdeep_merge_nodes(nodes)\n\n\tnodes      = [ (ruamel.yaml.nodes.ScalarNode, ruamel.yaml.nodes.MappingNode(tag='tag:yaml.org"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tfrom borgmatic.compat import get_version, pickle_loads\n\n\tfrom borgmatic.compat import is_py2\n\n\timport ruamel.yaml as yaml\n\timport borgmatic.compat\n\n\tdef get_from_file(name):\n\t\t\"\"\" Return an iterator of entries from the base borgmatic file to the\n\t\tborgmatic files that are named by name. The borgmatic file\n\t\tcontains the actual entries.\n\t\t\"\"\"\n\t\tfrom borgmatic.compat import get_version\n\n\t\tfrom borgmatic."}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\ttry:\n\t\tself._retention = _default(self.retention_key)\n\texcept KeyError:\n\t\tpass\n\n\t# we make a copy of the list of nodes, so that we can modify them after a merge\n\t# takes place.\t\t# the list is not modified if any of them is changed (just, at\n\t# most for simple sanity check)\n\tnodes = deepcopy(nodes)\n\n\tfor node in nodes:\n\t\ttags = tag_dict.copy()\n\t\ttag_dict.update(node['tags'])\n\n\t"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t# TODO: replace deep merge with plain merging instead\n\tnodes_to_merge = sorted(nodes)\n\treturn [\n\t\tdeep_merge_nodes([nodes_to_merge[i] for i in nodes_to_merge])\n\t]\n\n\ndef borgmatic_config_file(data, overwrite_file=None):\n\t\"\"\"Returns None if there is no borgmatic configuration. If non-null, it tries to\n\toverwrite the borgmatic configuration. If a borgmatic configuration already exists\n\tand overwrite_file is given only if a single borg"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\treturn deep_merge_nodes(nodes)\n\n/*\n# PostgreSQL Database Modeler (pgModeler)\n#\n# Copyright 2006-2021 - Raphael Ara\u00fajo e Silva <raphael@pgmodeler.io>\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation version 3.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\tconfigData = {\n\t\t'fields': [\n\t\t\t'ruamel.yml',\n\t\t],\n\t\t'tagNames': [\n\t\t\t'ruamel.yml',\n\t\t\t'ruamel.yml.sections',\n\t\t\t'ruamel.yml.sections.ruamelConfig',\n\t\t],\n\t\t'fieldsTypes': [\n\t\t\t'yaml',\n\t\t\t['yaml.base', yaml.node, yaml.nodes]\n\t\t],\n\t\t'scalarNames': [\n\t\t\t'ruamel.yml',\n\t\t\t'ruamel"}]}
{"task_id": "62b438b666fea644fe22cc78", "prompt": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\ttry:\n\t\targs = parse_command_line(*arguments)\n\t\tif args is None: args = []\n\texcept ParserFailedErr:\n\t\treturn_value = None\n\treturn return_value, args\n\ndef parse_command_line(args):\n\tparser = argparse.ArgumentParser(description='Parsing arguments')\n\tparser.add_argument('--files', action='store_true', help='Parse input files to find valid Python files.  Also used to select an executable in the source directory.')\n\tparser.add_argument('--extract_headers', action='store"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Boto3 Puppet Server script to interact with Boto3 Python SDK\",\n\t\tprog='python-boto3',\n\t\tusage=\"\"\"\nUsage: python-boto3 <module name> <command line>...\n\nSee the documentation of the <module> command for a list of available commands.\"\"\"\n\t\t)\n\t)\n\treturn parser.parse_args(arguments)\n\n\n# Command line wrapper to parse command-line options in 'python-boto3'.\n# Uses argparse.ArgumentParser for custom"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\ttry:\n\t\t# Run the script\n\t\tparser = ArgumentParser(**arguments)\n\t\tmain = parser.add_subparsers()\n\t\tmain.add_parser('parse_arguments', help={\n\t\t\t'msg': '--parse arguments',\n\t\t\t'parser_name': parser_name = 'parse_arguments',\n\t\t})\n\texcept Exception as msg:\n\t\tlogging.error(msg)\n\t\tparser = ArgumentParser('parse_arguments', usage=parser_usage)\n\t\tmain = parser.add_subparsers()\n\t\tmain."}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\targparser = ArgumentParser()\n\targparser.add_argument(\"-i\", \"--input\", help=\"File containing JSON representation of the data.\", required=False)\n\treturn argparser.parse_args(arguments)\n\ndef process_arg(arg_list, out_file):\n\t\"\"\"\n\tThe purpose of this script is to generate output file in python2 format by running the\ncommand in a temporary dir and calling the function of this script in a separate thread.\n\n\tSee 'Usage' in README\n\t\"\"\"\n\tfile_list = []\n\tfn = []\n\n\t"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tfor argument in arguments:\n\t\tif argument[0].upper() == \"HELP\":\n\t\t\t\thelp_text = command.help or \"\"\n\t\t\t\tsys.stdout.write(help_text)\n\t\t\t\tsys.stdout.flush()\n\t\tif argument[0] == \"START\":\n\t\t\t\targument_parser = ArgumentParser(prog=\"test_app.py\")\n\t\t\t\targument_parser.add_argument(\"-v\", \"--verbose\",\n\t\t\t\t    default=True, choices=loggers.verbose,\n\t\t\t\t    help=\"Verbose level (WARNING or DEBUG"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t# get help file and argument data\n\tusage = \"usage: {0} <infile> <outfile>\"\n\tif len(arguments) < 1:\n\t\tprint(\"Usage: {0} <infile> <outfile>\"\n\t\tprint(usage)\n\t\treturn \"Usage: {0}\".format(usage)\n\n\t# parse arguments\n\tparser = make_argparser(arguments)\n\n\t# get input file location\n\tinfile = parser.get_arg_data(arguments, 0)\n\n\t# open input file to produce input file\n\tinfile_file,"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from django.core.management import execute_from_command_line\n\tpass#IMPORTIMPORTIMPORT from django_crispy import add_crispy_options\n\tpass#IMPORTIMPORTIMPORT from django_crispy import add_domain_options\n\tpass#IMPORTIMPORTIMPORT from django_crispy import default_options\n\tpass#IMPORTIMPORTIMPORT from django_crispy import domain_options\n\tpass#IMPORTIMPORTIMPORT from django_crispy import get_cr"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tif not argument_parser:\n\t\targument_parser = ArgumentParser(description='Execute the main command of Sentry-Cookbook')\n\t\targument_parser.add_argument('--profile',\n\t\ttype = argparse.FileType('r'),\n\t\tdefault = os.path.join(os.getcwd(), os.path.splitext(os.path.realpath(__file__))[0] + '.profile'),\n\t\thelp = 'Path to a profile file to use with --profile',\n\t\t)\n\t\targument_parser.add_argument('--profile-cookbook"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t@synopsis C{arguments} is a list of tuples with a header and a list of argument names (as\nexpected) and values (as expected).\n\n\t@arg header:\n\t\tThe first element of the header, or None (default).\n\n\t@arg name:\n\t\tThe name of a value to be substituted with.\n\n\t@arg names:\n\t\tThe key-values of a dictionary.\n\n\t@arg values:\n\t\tThe value(s) to be substituted with.\n\n\t@return:\n\t\tThe parsed arguments; if"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\tparser = ArgumentParser(prog='PyPyGym', description='A Python implementation of the Gym model')\n\tparser.add_argument(\"--arch\", default='PyPyGym/PyPyGym_v3/2x20', metavar='ARCH',\n\t\t\t\thelp='Choose a specific architecture for PyPy, one of [PyPyGym/PyPyGym_v3/2x20]. Default: PyPy_v3.0\\n'\n\t\t\t\t'If not set, PyPy Gym is built with default architecture: PyPy_v3.0 ->"}]}
{"task_id": "62b438b666fea644fe22cc72", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tglobal print_args\n\tflags = ''\n\n\t# get the argument parser for ourself.\n\targ_parser = parser.parser\n\tif arg_parser is None:\n\t\t# We'll only parse command line arguments that have a name.\n\t\targ_parser = argparse.ArgumentParser(\n\t\t\tprog='shcmd',\n\t\t\tdescription='A very simple parser for working with shell scripts',\n\t\t\tusage=usage,\n\t\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t\t\tadd_help=False,\n\t\t\tparents=[arg_"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = []\n\tfor f in parser:\n\t\tif f in (argparse.Namespace, argparse.Arg):\n\t\t\tflags.append(' ')\n\t\t\tflags.append(f.lstrip('-'))\n\treturn flags\n\n#pragma once\n\n#include <QStyledItemDelegate>\n#include <QString>\n#include <QPoint>\n#include <QPalette>\n\nnamespace Gltf {\n\nstruct DrawContext {\n    int startX, startY;\n    int endX, endY;\n\n    Draw"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn parser.argument_parser.flags(parser)\n\n##\n# Parser of a Python object.\n\nclass PythonArgumentParser(argparse.ArgumentParser):\n\t\"\"\"Represents a Python command line interpreter.\n\n\tThe argument parser takes advantage of this structure to provide an easy to use interface.\n\tIf a subclass is required to implement this interface, it should then implement all of\n\tthis utility method to create a standard command line object and use its default\n\targument parsing methods.\t\"\"\"\n\n\tparser_class = argparse.ArgumentParser\n\t# This is the main function of"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\targs = parser.parse_args()\n\n\tif not args.nargs or len(args.nargs) == 0:\n\t\treturn []\n\n\targs.nargs = args.nargs[0]\n\nif __name__ == \"__main__\":\n\targs = parser.parse_args()\n\n\tparser.print_help()\n\tprint(\"-l, --list           Display a list of available argparse classes\")\n\tprint(\"-h, --help           Display this help\")\n\tprint(\"-m, --"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\toptions = []\n\tfor p in parser.positional_format.splitlines():\n\t\toptions.append(p)\n\tflags = parser.argument_format.format_arguments(options)\n\treturn flags\n\n\ndef parse_args(args):\n\t# type: (Any) -> Tuple[Optional[ParseResult], Optional[ParseResult]]\n\tfrom distutils.spawn import spawn\n\tfrom distutils.core import Command\n\tfrom distutils import log\n\timport argparse\n\timport cStringIO\n\n\tparser = argparse.ArgumentParser(description='Create a dist"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tparser.set_defaults(**dict(\n\t\targument_overrides=dict(\n\t\t\trequired=['name']\n\t\t)))\n\tparser.print_help(indent='    ') \n\tparser.read_from(argparse.ArgumentParser.__doc__)\n\n\n# The following methods do not support positional arguments (they will not be passed as\n# positional arguments), so you need to use the set_defaults() function to add any of the defaults.\n\ndef parse_long_option_argument(short_option_name, long_option_name, argument_overrides,"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tfor arg in parser.parse_args():\n\t\tif \"=\" in arg.option or \"=\" in arg.help:\n\t\t\treturn \"\"\n\treturn arg.option\n\n\nif __name__ == '__main__':\n\tparser = optparse.OptionParser()\n\t_opt_parse_opts_main(parser)\n\n// Copyright 2021 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn \" \" + \" \".join(parser._arg_names)\n\ndef _get_bool_arg_name(arg_name):\n\t\"\"\"\n\tGiven a bool, return its name.\t\"\"\"\n\treturn arg_name\n\n#\n#\n#\n#\n#\n\ndef check(opt, action, options, args):\n\t\"\"\"\n\tgiven options, options string and args, returns the result of performing the action\n\t(using error(s), for example, the value of the boolean variable `--help` for more info).\n\t\"\"\"\n\n\toptions ="}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = \"\"\n\n\t# Add the options\n\tflags += \"{\\n\"\n\tif hasattr(parser, \"optionflags\"):\n\t\tflags += \"\t-O\\n\"\n\t\tfor option, val in parser.optionflags.items():\n\t\t\tflags += option + \"    \" + \", \".join([(\"%s=%s\" % (\"= \".join(x[0] for x in val if x), x[1]) for x in option.split())]) + \"\\n\"\n\n\t# Add the options (unless we got at all set the empty string"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t# XXX this could be optimized to just always set\n\t#\t  x = parser.flags[k]\n\t#\t  print(x.replace(None, '\\n'))\n\treturn ''\n\ndef process_flags(parser, *used, **kwargs):\n\t\"\"\"\n\tGiven a valid argparse.ArgumentParser instance, return a list of parsed flags\n\tand their respective flags as strings.\n\t\"\"\"\n\tif 'f' not in kwargs:\n\t\tflags = process_flags(parser, used[1], **kwargs)\n\telse:\n\t\tflags = kwargs["}]}
{"task_id": "62b438b566fea644fe22cc70", "prompt": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\tdef completion(argv, opts: Optional[Dict[str, Opt]] = None):\n\t\t# get options\n\t\tif len(argv) > 7 and argv[7] in ({\"borgmatic\", \"options\"}):\n\t\t\tborgmatic_cmd = argv[7]\n\t\telif len(argv) > 6 and argv[6] in ({\"borgmatic\", \"options\"}):\n\t\t\tborgmatic_cmd = argv[6]\n\t\telse:\n\t\t\traise ValueError(\"Unknown borgmatic command: %r\" % argv)\n\t\t"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\tfrom os import listdir\n\tscript = '''#!/bin/bash\necho \"${var} ${str} %s;\ndo echo ${var} ${str} %s;\t\t\t\t\t#\\n\necho \"${var} ${var} $* = ${str} ${var} ${var}\\n\techo ${var} ${var} \\\\\\n; echo $$ | ${var}\\n\techo ${var} ${var} ${var}\\n\techo ${var} ${str} ${var}\\n\techo ${var} ${var} ${var}\\n\techo ${var} ${var} ${"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\tparsername = sys.argv[1]\n\tif parsername.startswith('--'):\n\t\tparsername = parsername[1:]\n\tdef braz(obj):\n\t\t\"\"\"Called by the borgmatic command parser.\"\"\"\n\t\tself.printWarning('Cannot determine borgmatic instance for command \"%s\" in borgmatic \"%s\".' %(parsername, self.name))\n\t\treturn {}\n\tdef getCompletionPattern(text):\n\t\treturn getCompletionPattern_impl(text, 0)\n\tdef getCompletionArguments(cmdLine, options):\n\t\t"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\timport string\n\tif len(args)!= 3:\n\t\traise Exception, \"Expected 3 arguments.\"\n\tif (args[0] == \"-r\" or args[0] is None)\n\t\traise Exception, \"Expected a single argument.\"\n\tif (args[1] == \"-\" or args[1] in string.whitespace or args[1] is None)\n\t\traise Exception, \"Expected a parameter value to be an unquoted string.\"\n\tr = args[1]\n\tif len(args)!= 4:\n\t\traise Exception, \"Expected 4 arguments"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\tborg = ''\n\tfor arg in ('borg': 'fget borg', 'fdelete borg', 'flist borg', 'blistborg', 'flock borg'):\n\t\targ = arg.strip()\n\t\tif arg in borg or arg in borg+':\n\t\t\ttry:\n\t\t\t\tbash, args = _bash_completion_parser.splitargs(arg)\n\t\t\texcept:\n\t\t\t\tbash, args = arg.split(\"=\")\n\t\t\tborg = \"%s; %s\" % (bash, args)\n"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\treturn borgmatic.BorgmaticCommand.bash_completion\n\ndef backslash_completion():\n\treturn borgmatic.BorgmaticCommand.backslash_completion\n\ndef pawn_completion():\n\treturn borgmatic.BorgmaticCommand.pawn_completion\n\ndef slash_completion():\n\treturn borgmatic.BorgmaticCommand.slash_completion\n\ndef help_completion():\n\treturn borgmatic.BorgmaticCommand.help_completion\n\ndef list_completions():\n\treturn borgmatic.BorgmaticCommand.list_completions\n"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\tdef borgmatic_completion():\n\t\t\"\"\"\n\t\tUse borgmatic's command-line argument parsers to borgmatic your\n\t\tborgmatic command as an alternative to borgmatic's arguments\n\t\tto continue borgmatic's command-line arguments as if they had\n\t\tspecified herlozings of arguments.\n\t\t\"\"\"\n\t\timport borgmatic\n\t\tdef check_completion(args):\n\t\t\treturn (args, [borgmatic] + [args.complete().rstrip()] if args.complete() is not None)\n\t\tcheck"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from sp_command_line import command_line\n\treturn command_line()\n# END def bash_completion()\n\n###############################################################################\n\n/*-\n * Copyright (c) 1998 John D. Polstra.\n * Copyright (c) 2000 Michael Richardson <mcr@freesoft.org>\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n * 1. Redistributions of source code must retain the above copyright\n *  "}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\treturn \"# %s.bash_completion <cmd> [args]\" % os.path.dirname(os.path.abspath(__file__))\n\n\ndef borg_completion_command():\n\t\"\"\"return a borg completion command given a path suitable for running borgmatic commands.\n\tProduce this by introspecting borgmatic's command-line argument parsers.\"\"\"\n\n\treturn \"[path.join([user.home], 'borg','.bash','-c','.bash_completion=1']).bat\"\n\n\ndef cscript():\n\t\"\"\"return the cscript script"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import subprocess\n\tresult = subprocess.check_output([\"bash\", \"-c\", \"bash completion\"])\n\treturn result\n\ndef check_borg_version():\n\tresult = borg_version()\n\tif result:\n\t\tprint(\"borg version:\", borg_version())\n\ndef get_path():\n\t\"\"\"Get the path to the package to use.\n\n\tUsage: get_path <path>\"\"\"\n\tresult = check_borg_version()\n\tif result:\n\t\treturn result\n\tprint(\"ERROR"}]}
{"task_id": "62b438a266fea644fe22cc2e", "prompt": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\ttry:\n\t\tfrom argparse import ArgumentParser\n\texcept ImportError:\n\t\tparser = ArgumentParser()\n\t\tparser.add_argument(\n\t        \"command\",\n\t        default=\"get_all\",\n\t        metavar=\"command\",\n\t        help=\"subcommand or -m 'get_all'\",\n\t        nargs=\"?\",\n\t        help=\"subcommand or -M 'get_all'\"\n\t)\n\t\tparser"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tif len(unparsed_arguments)!= 1:\n\t\traise SystemExit(\"You probably provided an incorrect number of arguments: %s\" % unparsed_arguments)\n\n\tunparsed_arguments = unparsed_arguments[0]\n\n\tif subparser_name in unparsed_arguments:\t# Global arguments\n\t\targv = unparsed_arguments[subparser_name]\n\t\treturn sys.argv\n\n\t# Add missing subparsers that didn't exist as a subparser\n\targv = [subparser_name] + preprocessors_argparser_list(subparser"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tglobal temp_parser_name\n\tglobal parser_arguments\n\n\tif (len(unparsed_arguments) == 3) \\\n\t\t: # Global - command-line args are now just command names - we'll only use them (for global args)\n\t\tparser_name = unparsed_arguments[0]\n\n\t\t# We need this for some special commands\n\t\tif parser_name[-1] == \" \":\n\t\t\tparser_name += \" \"\n\n\t\tparser_arguments = make_namespace(subparser_names[parser_name])\n\n\t\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\treturn ParsedArguments().parse_arguments(*unparsed_arguments)\n\ndef parse_environment():\n\t\"\"\"\n\tInstantiate Python's `getopt` and `setuptools.command.argparse[value]` methods.\n\t\"\"\"\n\timport sys\n\t# setuptools is installed on Python 4.X.org\n\tsetuptools_args = [\n\t\t'define', 'argparse.ArgumentParser', 'argparse.Namespace',\n\t\t'system_info.show_version','sys.argv','sys.gnu_getopt','sys.has_"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tglobal_root_path = os.path.abspath(os.path.join(sys.argv[0],\"..\"))         # get rid of the.so file\n\t#print(sys.argv)\n\t#print(cmd_name)\n\tglobal_name = \".\".join(sys.argv)\n\n\tif cmd_name.startswith(\"-\"):\n\t\tcmd_name=cmd_name[len(\"\"):]\n\t\tcmd = sys.modules[\"importing\"]\n\t\tif 'global_' + cmd_name!= global_root_"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\ttry:\n\t\tif \"--\" in unparsed_arguments:\n\t\t\tprint(\"Command-line arguments cannot have multiple keys\")\n\t\t\tprint(\"  -- --\")\n\t\t\tprint(unparsed_arguments)\n\t\t\tprint()\n\t\t\tsys.exit(1)\n\t\telse:\n\t\t\tfor p in unparsed_arguments.split():\n\t\t\t\tif p.startswith(\"--\"):\n\t\t\t\t\tcmd = p.split('=')[1].strip()\n\t\t\t\telif p.startswith(\"--\"):\n\t\t\t\t\tcmd = p.split('=')[1].split(\"=\""}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser = parser_from_arg_parser()\n\tparser.add_argument_group('args', metavar='PARAMETERS: [--appname] [--module_name] ')\n\tparser.add_argument('-x', '--exec', default=EXEC_PATH, dest=\"execfile\", metavar='FILE',\n\t\thelp=USAGE)\n\tparser.add_argument('-P', '--pkg_path', metavar='DIR', default=PKG_DIR,\n\t\tdest=\"pkg_path\", help=USAGE)\n\tparser.add_argument('--python_suffix', default=\""}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tparser = arg_groups.parse_args(*unparsed_arguments, **_ARGS_META)\n\tglobal _GLOBAL_ARGS\n\t_GLOBAL_ARGS = parser.args\n\treturn _GLOBAL_ARGS\n\ndef get_parsers():\n\t\"\"\"Get all parsers for the current program. \t\t\"\"\"\n\tif not hasattr(sys, \"getloaders\"):\n\t\tsys.getloaders()\t# ToDo: revisit this function, but make sure it raises an exception.\n\nclass parse_exception(errors.ParserError):\n\n\tdef __init__(self, msg"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tglobal current_subp_parser\n\tif current_subp_parser is None:\n\t\tparser = argparse.Namespace()\n\t\tparser.add_argument(*unparsed_arguments)\n\t\t# parser.add_argument(*unparsed_arguments)\n\t\t# parse_known_args()\n\t\t# global current_subp_parser is None\n\telif isinstance(current_subp_parser, argparse.Namespace)!= True:\n\t\tglobal current_subp_parser\n\t\t# parse_known_args()\n\t\tcurrent_subp_parser"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\tglobal namespace, global_args, global_kwargs = namespace = global, {}, {}\n\tfor arg in unparsed_arguments:\t# pragma: no branch\n\tif arg in global_args:\t# pragma: no branch\n\t\t# parse argument -- no need to parse the parser!\n\t\traise ArgumentError(\"Attempted to parse '{0}' which is an argument by a global command '{0}'\".format(arg, global_args[arg]))\n\telif arg in global_kwargs:\t# pragma: no branch\n\t\t# parse -- no need to parse the parser!\n\t"}]}
{"task_id": "62b438a266fea644fe22cc2d", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t# a dictionary from subparser name to argparse.ArgumentParser instance\n\tresult = {}\n\t# a map of argparse name to argparse.ArgumentParser instance\n\tresult['subparsers'] = {} for name, parsed_args in subparsers.items()\n\t# a map of argparse name to argparse.ArgumentParser instance, one for each argparse\n\tprocessed = shot.parse_subparser_arguments_all(unparsed_arguments['subparsers'], name, result)\n\t# return the set of arguments to all subparsers\n\treturn processed\n\ndef parse_subcommand(unparsed_command"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tif not unparsed_arguments:\n\t\traise PipException(\"No arguments passed to shot\")\n\tremaining_args = []\n\tvar_args_map = {}\n\tfor i in range(len(unparsed_arguments)):\n\t\tremaining_args.append(remaining_arguments[i])\n\tfor k in remaining_arguments:\n\t\tvar_args_map[k] = {}\n\tremaining_subparsers = []\n\tfor var_name in remaining_args:\n\t\tremaining_subparsers.extend(subparsers[var_name])\n\treturn"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tnew_args = [[subparser.construct(arg, subparsers) for arg in values[opt] if\n\t\t\t   '--' in opt] - optkeys[opt] + option_values[opt]\n\n\tif optinlist!= '-':\n\t\tnew_args[1].extend(optinlist[len(optinlist)-1:])\n\n\t\treturn new_args, ''\n\treturn new_args\n\ndef parse_option_list(args_dict, option_values, subparsers):\n\targspec_pairs = arg_specs = subparser_dict"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tassert(len(unparsed_arguments)!= 0)\n\n\tinput_arguments = {\n\t\t'--repository': [\n\t\t\t'definitions/core',\n\t\t\tsubparsers.parse_subparsers(),\n\t\t\tself.__parser_command_arguments\n\t\t\t]\n\t\t}\n\tif len(unparsed_arguments) > 0:\n\t\tinput_arguments['--repository'] = unparsed_arguments\n\n\treturn input_arguments\n\ndef extract_file_name(source_source_file_path):\n\t\"\"\"\n\tExtracts the file's"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t# This keeps 'action_subparsers' as an unusual way to look for subparsers with conflicting name\n\taction_subparsers = {}\n\tfor parsed_arg in shot:\n\n\t\t# if the action isn't shared between subparsers - add it\n\t\tif parsed_arg not in action_subparsers:\n\t\t\taction_subparsers[parsed_arg] = {}\n\t\tparsed_args = parsed_arg.split()\n\n\t\t# if the argument is claimed by more than 1 subparser - add it to args\n\t\tif len(parsed_"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t# If there are no remaining arguments claimed, we return a dict rather then a list[arg]\n\t# which is a list of remaining arguments.\n\tif len(unparsed_arguments) == 0:\t\t\t# No remaining arguments claimed from command line.\n\t\treturn unparsed_arguments\n\n\t# Build a dictionary of parsed subparsers based on namespace, and append all remaining subparsers of that namespace\n\targument_parsers = {}\n\tfor subparser_name, subparsers in shot_parser_map.items():\n\t\tfor subparser in subparsers:\n\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tdefault_arguments = default_parsers.ArgumentParser._default_argument_parsers()\n\targumentparser = ArgumentParser()\n\tdefault_arguments.add_required_arguments(subparsers.subparsers)\n\tfor name, subparser in subparsers._defaults.items():\n\t\tdefault_arguments.add_argumentparser(name, subparser, default_arguments)\n\treturn argumentparser.parse_args(unparsed_arguments, subparsers.subparsers)\n\n\ndef get_argument(name):\n\treturn argparse.Arguments.from_param_map(parser_arguments=Parser"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t# A dictionary from subparser name to argparse.ArgumentParser instance in case a\n\t# new command line argument is added.\n\tparsed_arguments = {}\n    if unparsed_arguments is None:\n        unparsed_arguments = unparsed_commands\n    if not isinstance(unparsed_arguments, dict):\n        raise ValueError(\"Cannot parse command line arguments of any parser - got an unknown argparse class\")\n    if not isinstance(parsed_arguments, dict):\n        # A sub"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\tparsed_args = defaultdict(dict)\n\tfor arg in unparsed_arguments:\n\t\tsubparser = subparsers[arg.subparser]\n\t\tif arg.subparser == None:\n\t\t\tcontinue\n\n\t\tif subparser is None:\n\t\t\tsubparser = argparse.ArgumentParser()\n\t\telse:\n\t\t\tsubparser.add_argument(*arg.args)\n\t\tif arg.is_set is False:\n\t\t\tremaining_arguments_to_subparser_add = []\n\t\telse:\n\t\t\tremaining_arguments_to"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\treturn parse_namespace(subparsers, subparser_name=name,\n\t\t\t\t\t\t\t\targuments=unparsed_arguments,\n\t\t\t\t\t\t\t\tconsumed_arguments=[])\n\ndef parse_namespace(subparsers, subparser_name, consumed_arguments, arguments):\n\tconsumed_arguments = remaining_arguments(consumed_arguments)\n\n\tunparsed_arguments = unparse(consumed_arguments, subparsers)\n\tsubparser_ns = tuple(ns for ns, ns_args in unparsed_arguments.items())\n\treturn subparser_ns, consumed_arguments\n\ndef subparser_name"}]}
{"task_id": "62b438a266fea644fe22cc2c", "prompt": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tprog = Parser()\n\tprog.add_section('Test 1')\n\tprog.add_section('Test 2')\n\tprog.add_section('Test 3')\n\tprog.get_sections()\n\treturn prog.get_sub_parsers() + 2 # Test 1\n\t\n \n def main(options=\"\"):\n\tprog = make_parsers()\n\tusage, parameters, args = parser.generate_usage_and_parameters(prog)\n\tparse_opts, program = prog.parse_from_usage(options, usage, parameters)\n\tif not parser.prog_"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tfrom _gen.doc_info import DocInfo, PragmaDocInfo\n\tparser, _ = make_top(parse_function_from_args)\n\tparser[0].parse(source, DocInfo('source',\n\t\t( 'def f(x, y) return f(x + y) end ',\n\t\t'for i in source do',\n\t\t'    print i,',\n\t\t'end' ), PragmaDocInfo()))\n\treturn parser, [ parser[1] ]\n\ndef _make_all_parsers():\n\t\"\"\""}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t# We need to use this line to create lists that don't contain anything else than strings.\t \n\t# This is why the last option is unused, and it's safe to delete these strings to\n\t# keep it small.\n\tparser = Argparse(\n\t\tprog = 'ps',  prog2 = 'ps',\n\t\tverbose = False,\n\t\targlist = arglist + [ 'prefix', 'basedir',\n\t\t                      'quiet',\n\t\t        "}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\twith _Parser(None, make_subparsers(names=('parser','subparser'))):\n\t\t# Build a parser that accepts an argument.\tDon't bother building\n\t\t# subparsers.\n\t\tparser = create_parser(names=('parser','subparser'))\n\t\tsubparsers = parser.subParsers\n\t\tsubparsers = subparsers[:len(subparsers)-1] + subparsers[len(subparsers) - 1:]\n\t\t# Make a parser and its subparsers work as expected.\tMake a Parser\n\t\t# that just accepts all of"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers()\n\n\treturn make_parsers(subparsers)\n}\n\n#--\n#- Tests - for the 'Tests' section\n#- Author - Mirko Hoguebe\n#- Author - Matthieu Holberg\n#- Copyright - 2002 - 2018, Fraunhofer Yau\n#- All rights reserved.\n#--\n#\n# This is a test for the L{_Tests} class. It should be possible to\n# test all the functionality of a class"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tparser = parse_filename()\n\tmake_parsers(*parser)\n\treturn *parser\n\ndef parse_full_filename(file_name):\n\tfull_filename = os.path.join(os.path.expanduser('~/Library', '~/Library/' + file_name + '\\\\Data\\\\') + '.shh')\n \tf=open(full_filename,'w')\n\tf.write('##import <#!\\\\\\\\ %6.2f %6.2f\\n' %(file_name,0.0001))\n\tprint f.read()"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tclass Parser():\n\t\t\"\"\"\n\t\tclass Method:\n\t\t\t\"\"\"\n\t\tdef method(self): return \"pass\"\n\t\t\"\"\"\n\t\tdef run_method(self, name, *args, **kwargs):\n\t\t\treturn \"pass\"\n\n\tclass File(object):\n\t\t'''\n\t\tThis is a class, we use it as a parser and then you can use it as a\n\t\tbase and use it.\n\t\t'''\n\t\tdef __init__(self, name, filename=None, content, errors={}):\n\t\t\tself"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t# build parser\n\tparser, args, defaults = make_singleparser(self, \"cmdoptions\")\n\n\t# process default values\n\tparser.read_config_file(\"defaults.yaml\")\t\tas_default = DEFAULT\n\targs = defaults.defaults_dict[\"user\"][\"args\"]\t\tas_user_argnames = args.keys()\n\targs = defaults.defaults_dict[args.pop(len(args.keys()))]\t\t\tas_user_args = args.pop(len(args.keys()))\n\n\t# process subparsers\n\tparser.read"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\toptions = parser_options(parserClass=TyperefParser)\n\tdefaultOptions = parser_options(\n\t\toptionString=\"-\"\n\t\taction=\"store_underscores\",\n\t\tdefVal=\"-\"\n\t\tparserClass=typerefParser\n\t)\n\toptions |= defaultOptions\n\tparser = OptionParser(parserClass)\n\tparser.add_option(\n\t\t\"--defaultparser\", help=\"The parser used for top-level files. Options are \"\n\t\t\t\"\\n\\t--with-subparsers=subparsers (default)\"\n\t\t\"\\n"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\tp = make_parser_spec()\n\tparser_names = sorted(p.parser_names(\n\t\tparser_options=[\n\t\t\t\"--parser-name\",\n\t\t],\n\t))\n\tsubparsers = []\n\tfor p in list(parser_names):\n\t\tsubparser = make_parser(**dict(p, **p._make_parser_options()))\n\t\tsubparsers.append(subparser)\n\treturn p, subparsers\n\n\ndef _get_parser_defers(file_contents):\n\t\"\"\"Look up parsers and subparsers with"}]}
{"task_id": "62ece4982e6aefcf4aabbd5f", "prompt": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tdef __generator_function__(self, response):\n\t\treturn response[response[0]][\"data\"],\\\n\t\t    rucio.util.get_max_results_in_pages(max_results)\n\t__args__ = list(args) + [response] * (max_results - 1)\n\t__function__ = __generator_function__\n\treturn __args__, __function__\n\ndef paginate(request, page_size, paging_func, page_number,\n\tpaginator=None, offset=0):\n\t\n\tif paging_func"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tfor i in range(max_results):\n\t\tyield (i % (100 * len(wapi.pages))]  # get page for current time.\n\t\tyield dict(\n\t\t\tcontent=response[i][1],\n\t\t\t_id=response[i][0],\n\t\t\tis_success=True\n\t(yield from get_paging(args)) for _ in range(len(list(args.results)))\n\ndef get_paging(args, **kwargs):\n\t\"\"\"\n\tGet WAPI paging page by page\n\targs:\n\t\tmax_"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tfrom. import _pagenums\n\n\t_sort = pagename(_pagenums)\n\n\treturn _pagenums.pagination(response, max_results, _sort.pagetype)\n\n// Code generated by smithy-go-codegen DO NOT EDIT.\n\npackage waf\n\nimport (\n\t\"context\"\n\tawsmiddleware \"github.com/aws/aws-sdk-go-v2/aws/middleware\"\n\t\"github.com/aws/aws-sdk-go-v2/aws/signer/v4\"\n\t\"github.com/aws/"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tprint('\\nWAPI Paging for this dataset:')\n\tprint('\\n------------------------')\n\tresult = [('0    page    ',''.join('_'), '', '', '', '', '', \"\", 0,\n\t           'None', '', '', '', '', '', '', '', '', '', '', '',\n\t           '', '', '', '','     ')\n\t        for _ in range(max_results)]\n\tfor row in response:\n\t\tresult.append(row['page'])"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\twhile response:\n\t\ttry:\n\t\t\tyield response\n\t\t\tyield\n\t\t\tresponse = list(response)\n\t\texcept StopIteration:\n\t\t\tbreak\n\t\tn_items = 20\n\t\tfor page in response:\n\t\t\tif len(page) < max_results:\n\t\t\t\tcontinue\n\t\t\tif page[1:1] == 'paging':\n\t\t\t\ttry:\n\t\t\t\t\tyield wpm_paging_response(page)\n\t\t\t\texcept StopIteration as exc:\n\t\t\t\t\tyield wpm_paging_response(page)\n\t\t\telse"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tpage_obj = WAPIPage(response)\n\tstart = 0\n\treturn page_obj\n\ndef page_from_str(value):\n\tpage_obj = WAPIPage(value, limit_num=50)\n\tif page_obj.next_url == 'https://github.com/jingzhou/wapi-core-python-1.7.1/wiki/Pagination' and page_obj.page == '1':\n\t\tpaginator = Paginator(page_obj)\n\telse:\n\t\tpaginator = Paginator(page_obj"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\treturn paginate(page=response, pagesize=5, max_results=max_results)\n\n\nclass SearchIndex(PageView):\n    def __init__(self,\n\t\ttext: str = '',\n\t\tinclude_tags: bool = True,\n\t\torder: str = 'asc',\n\t\tpage: str = '1',\n\t\tuser: dict = None,\n\t\ttime_since_last_updated: bool = False,\n\t\tsort: str = 'time': float = None,\n\t\tpage_format: str = '"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tsplit_page = page(size=1024, response=response, page_id=\"1\")\n\tprint(split_page)\n\treturn PagingResponse(paginate(split_page, max_results), 0)\n\n/*\n * Copyright (c) Contributors to the Open 3D Engine Project.\n * For complete copyright and license terms please see the LICENSE at the root of this distribution.\n *\n * SPDX-License-Identifier: Apache-2.0 OR MIT\n *\n */\n\n#ifndef UNITY_UNIT_TESTS_BASE_TESTER_H\n#define UNITY_UNIT"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\tlast_page = int((response[-1] - response[0]) // response[-1]) + 1\n\tpage = [\n\t\t[]\n\t\t]\n\treturn generator(response, max_results, \"paging\", page, last_page)\n\n\ndef page_with_limit(limit, headers):\n\t\"\"\"\n\tReturns WAPI response list, limited page by page. Parameters\n    \tlimit (-1 | None) - maximum number of results to be returned\n\tEach result with this limit will be returned with the specified\n\tnumber of pages, where pages is"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\turl = response[0].get(\"Page\", \"\")\n\turl = url.replace(\"&\", \"&amp;\")\n\n\thttp_status, http_headers, response_headers = _make_http_response(url, \"Page\", max_results)\n\tbody = http_get(http_status, http_headers, response_headers)\n\n\treturn body[0]\n\n\n\ndef api_key(public_key, secret):\n\t\"\"\"API Get Key Key with the following format\n\n\t|   PUBLIC KEY string       |\n\t|"}]}
{"task_id": "62ece4982e6aefcf4aabbd60", "prompt": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t", "suggestions": [{"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\ts = size\n\tsize = size*1e3\n\treturn size // 1024\n\n// Copyright (c) Microsoft Corporation. All rights reserved.\n// Licensed under the MIT license.\n\npackage main\n\nimport (\n\t\"errors\"\n\t\"flag\"\n\t\"fmt\"\n\t\"log\"\n\t\"net/http\"\n\t\"net/url\"\n)\n\ntype HttpServer struct {\n\tHost string\n\t// URL host info\n\tURL    *url.URL\n\tURLIndex int\n\n\t// For handling http2 requests\n\treq"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t_, bytes_per_line, _, _, _ = import_source()\n\tbytestr0 = \"\\n\".join(os.linesep.join(sizes)[:-1] for sizes in sizes.split(\"\\n\"))\n\tbytestr = \"\\n\".join((bytestr0[:bytes_per_line] if bytes_per_line > 1: bytestr0,\n\t\t\"\\n\".join([bytestr0[bytes_per_line:] for bytestr0 in sizes]))\n\treturn bytes.fromhex(bytestr)\n\n// SPDX-License-Identifier: GPL-2"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t}\n\tpublic static int64_to_bytes(int64 size)\n\t\t: (size: NSUInteger, size_unit: String = \"\")\n\t\t=> { [bytes:var]\t: bytes32\"\"\"\n\t\t, size_unit = \"\"\n\t\t, size_unit = size.toString().replace(\"K\", \"\")\n\t\t, size_unit = size_unit.replace(\"K\", \"\")\n\t\t, size_unit = size_unit.replace(\"K\", \"\")\n\t\t, size_unit = size_unit.replace(\"K\", \"\")\n\t"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\tsize = size[:3]\n\n\tif size == \"..\":\n\t\tif size!= \"\":\n\t\t\tsize = \"..0.\"\n\t\telse:\n\t\t\tsize = \".\"\n\n\treturn int(len(size) * 1000 / (1024*1024))\n\nThe output of this function always equals to (1024*1024).\nIf you want to use this function to calculate the size of a file, use the\nfollowing snippet for input value:\n\n    import sys\n    from datetime import datetime as dt\n    print(self.size_to_"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\treturn int(size)\n\ndef time_to_hex(seconds: int) -> str:\t\"\"\"Returns a human readable time in a human-readable value.\n\nArgs:\n    seconds: A numeric value representing a number of seconds.\n\nReturns:\n    A human readable time in a human-readable value.\n\n    Examples::\n\n        >>> time_to_hex(12)\t\t# a 2 digit hour: minutes: seconds: milliseconds: nanoseconds\n        0xF800000000000000\t24\t"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\treturn int(size) if size[0]!= \"0\" else 0\n\t\ndef to_bytes(value: str) -> int:\t#pylint: disable=maybe-unreachable\n\t\"\"\"\n\tConvert number to human readable bytes.\n\nArgs:\n    value: A numeric value representing a human readable file size (eg: '1K')\n\nReturns:\n    Bytes representation of value\n\n    Examples::\n\n        >>> from decimal import decpt\n        >>> to_bytes(\"0"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\ts = size.lower()\n\tif not s:\n\t\treturn\n\telif len(s) == 7:\n\t\tnfmt = \"%0\"\n\t\tif len(s) == 7 and s[6] in \"0123456789\":\n\t\t\tnfmt = \"%2\"\n\t\tif len(s) == 12 and s[7] in \"0123456789\":\n\t\t\tnfmt = \"%3\"\n\t\treturn float(int(re.split(nfmt)[1], 16))\t# to bytes\n\n################################################################################\n\ndef _parse_long(inp:"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t_log.info(\"size_to_bytes: %.2f MB\", size)\n\tif size is None:\t# empty file; return 1 unit 0 bytes\n\t\treturn 1\n\tif size < 1024:\t# smaller file; return 1 unit 0 bytes\n\t\treturn 10\n\tif not size:\n\t\treturn 1\n\treturn 0\n\ndef read_file(filename: str) -> bytes:\n\twith open(filename, \"r\") as f:\n\t\treturn f.read()\n\n\n\"\"\"\n\n\ndef format(s) -> str:\n"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\tsize = humanize.natural(size)\n\tif size.startswith('<'):\tsize = \"%\" + size\n\treturn round(float(size), 2)\n\nimport { render, screen } from '@testing-library/react';\nimport userEvent from '@testing-library/user-event';\nimport React from'react';\n\nimport {\n  Box,\n  Flex,\n  Text,\n  Switch,\n  TouchableOpacity,\n  View\n} from '@tarojs/components';\nimport AppContainer from './AppContainer';\nimport GlobalRouterContext from '@shared"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\ttypeSize = float\n\tif size == \"1K\" and \"B\" in str(typeSize):\n\t\ttypeSize = 1024.0\n\tif size == \"2K\" and \"C\" in str(typeSize):\n\t\ttypeSize = 1024.0 * 2\n\tif size == \"4K\" and \"T\" in str(typeSize):\n\t\ttypeSize = 1024.0 * 4\n\tif size == \"10K\" and \"G\" in str(typeSize):\n\t\ttypeSize = 1024.0 * 10\n\tif size == \""}]}
{"task_id": "62ece4982e6aefcf4aabbd61", "prompt": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\treturn dicts\n\t\n\n_listsum(list(dicts))\ndef _listsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable list(dict)\n\tor by a single value.\n\n>>> _listsum([{'a': 1}, {'a': -1}])\n{'a': -1}\t\"\"\"\n\treturn list(dicts)\n\n#pragma once\n\n#include \"Scenes/Hexus/Components/Buttons/SlotButton.h\"\n\nnamespace UI {\nclass KeyboardBarElement;\n}\n\nclass KeyboardBar"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t# Note: _dictsum() only works for object instances.\n\tif not hasattr(isinstance(dicts, collections.abc.Sequence) or isinstance(dicts, (list, tuple, dict))):\n\t\treturn\n\tl = lambda x: dicts.pop(x)  # Remove the first instance.\n\tm, n = dicts[0], dicts[-1]  # Get index.\n\tfor i in range(1, len(dicts)):\n\t\tret = l(m)\n\t\tif i > 1: ret = ret.update(m"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\treturn _dictsum(dict(zip(*list(dicps))))\n\ndef _dictfilter(keys, values):\n\t\"\"\"Return a new dict containing items from a set of items.\"\"\"\n\treturn {key : values[key] for key in keys}\n\n\n# The DJN Python package uses Python 3.7's Python module import and\n# import * as dictops and * as listops and so these python code cannot\n# compile. There are a lot of this issues we can do when the python interpreter\n# is installed. In those cases we have a bit of a hack to do"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t# Do not include the sentinel value in the default\n\treturn dicts.__getitem__('a', 'b')\n\ndef _setdiff(s):\n\tif s is None:\n\t\treturn None\n\telse:\n\t\treturn set([p[1] for p in s])\n\ndef test_pickle(testdir, tmp):\n\torig_stdout = sys.stdout\n\tsys.stdout = StringIO()\n\tmod = __import__('modinfo')\n\ttry:\n\t\tmod.__all__ = mod.__all__ + [(mod.__all__, mod"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\tdef _dictsum(self, *args, **kwargs):\t\"\"\"\n\t\tdict_1 = self._dict_get()\n\t\tdict_2 = kwargs.get('dict_2', self.dict_2)\n\t\treturn dict((k,kwargs._dict_get(k)) for k in dict_1) + dict((k,kwargs._dict_get(k)) for k in dict_2)\n\n\tdef __getitem__(self, key):\t\"\"\"\n\n\t\tReturn the item at this index in the dictionary dictionary of the source.\n\n\t\tThis"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\treturn list(\n\t\t(\n\t\t\td.items()\n\t\t\tfor d in zip(*[l[:2] for l in dicts])\n\t\t)\n\t)\n\n\td = _dictsum([{('a', 'b'): 1}, {('a', 'b'): 5}, {('a', 'b'): 0}])\n\t                        # This is the total order.\n\ndef _dictgetitem(d, key, default):\t\"\"\"\n   "}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\tkeys = [1, 'a', 'b', 'c', '_', '$', 'p', '', '@']\n\tnewdicts = [i for i in iter(dicts) if key not in keys]\n\tresult = [{'a': i, 'b': j for i, j in dicts[i].items()} for k in keys]\n\tfor k in keys:\n\t\tif not k in result:\n\t\t\tresult.append(k)\n\t\n\treturn result\n\n\ndef dico_add(args, dico, values=None, **kwargs):"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\tsum = [x for x in list(dicts) if x is not None]\n\t\treturn sum\n\ndef _dictsum_dict(dicts):\n\td1 = dicts[('a', 1)]\n\td2 = dicts[('a', 2)]\n\td3 = dicts[('b', 2)]\n\treturn d1 + d2 + d3\n\ndef test_setitem():\n\ts1 = dict(a=[1,2,3], b=[5,6,7])\n\ts2 = dict(a=[1,"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\tdicts = [d for d in dicts if not isinstance(d, dict)]\n\treturn map(len, dicts)\n\n\ndef _dictlist(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictlist(['a', 'b'])\n['a', 'b']\t\"\"\"\n\tlist_data = [d.dump() for d in dicts]\n\treturn [d for d in list_data if not isinstance(d, str)]\n\n\ndef _all_dict(dict_list):\t\"\"\"\n"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\treturn zip(*[dict_tup(x) for x in zip(*dicts)])\n\ndef dict_with_name(dicts, name):\n\t\"\"\"Return a dictionary with the specified name.\n\n\tDictionary has a single key 'x[' and two or more dicts.\n\n\tx['a'] is created first and incremented to\n\n\tdict { 'a': 1, 'b': 2}\n\n\tand the following two dicts are combined.\n\n\tdict { x['b']: 3 }\n\n\tThe key x['b'] becomes dict { x['a']:"}]}
{"task_id": "62ece4982e6aefcf4aabbd62", "prompt": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\treturn url_args\n\n\nclass _ExpandList(object):\n\t\"\"\"\n\tExpand a list to only up to one item\n\t\"\"\"\n\tdef __init__(self, url, data_list, limit=None, recursive=False):\n\t\t\"\"\"\n\t    Args:\n\t      url: The URL to search for (e.g.: http://home.yahoo.com/home)\n\t        Default value: 'http://www.yahoo.com/home'\n\n\t      data_list: A list of d"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tif not url_args or url_args['content'] is 0:\n\t\t# If a new URL arg not specified, use the value from the\n\t\t# current URL args.\n\t\tif not url_args['scheme']:\n\t\t\turl_args['scheme'] = url_args['host']\n\t\t\turl_args['host'] = url_args['path']\n\t\tif not url_args['path'] and url_args['query']:\n\t\t\turl_args['path'] = '?' + url_args['path']\n\tfor key, value in url_"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tfor url in url_args:\n\t\tif not url or url == '':\n\t\t\tcontinue\n\t\ttry:\n\t\t\turl_args[url] = url +'' + url_args[url]\n\t\texcept KeyError:\n\t\t\turl_args[url] = ''\n\treturn url_args\n\n\tdef __init__(self, *args, **kwargs):\n\t\tkwargs.setdefault('append',False)\n\t\tsuper(Url, self).__init__(*args, **kwargs)\n\n\tdef urlopen(self,method='',url=None"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tfor p, v in url_args.items():\n\t\tmatch = re.match(r'^[^:\\'\\*\\.]+:([^\\'\\*\\+]+)?\\/([^\\'\\*\\+]+)?\\/(.*)$', v, re.M)\n\t\tif match is None:\n\t\t\tif v == '': raise ValueError(\"The custom variable %s isn't defined for URL\" % p)\t\t\n\t\t\tv = \"'%s%s'\" % (v, v.replace('\"',''))\n\t\telse:\n\t\t\tv = match.group(1)\n\treturn url"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tfor item in url_args:\n\t\tif not item:\n\t\t\turl_args[item] = str(url_args[item])\n\treturn url_args\n\n\ndef convert_base64_bytes(data):\n\tbase64_data = base64.b64encode(data.decode(\"utf-8\"))\n\tbase64_data = base64_data.rstrip().decode(\"ascii\")\n\treturn base64_data\n\n\ndef encode_string(string):\n\t\"\"\"\n\tEncodes a string\n\t\"\"\"\n\tstring = string.lower"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tnew_args = []\n\turl_args.append((\"url\", url_args))\n\tfor item in list(url_args):\n\t\tif isinstance(item, basestring):\n\t\t\tnew_args.append(item)\n\t\t\titem = item.replace(\"'\", \"\").replace(\":\", \".\")\n\t\telse:\n\t\t\tif item not in url_args:\n\t\t\t\tnew_args.append(item)\n\t\t\telse:\n\t\t\t\tnew_args.append(item + \"=\" + url_args[item])\n\t\telse:\n\t\t\tif item"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tfor key, value in url_args.items():\n\t\turl.setdefault(key, []).append(value)\n\ndef _replace_unicode_args(url, args):\n\tif isinstance(args, unicode):\n\t\tif hasattr(url,'set_encoding'):\n\t\t\treturn url.set_encoding('UTF-8')\n\t\treturn url.set_encoding('cp1251')\n\treturn args\n\t\ndef _build_html(url):\n\t\"\"\"\n\tCreates a html page.\n\t\"\"\"\n\tif url[:3]!= \"http:\":"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tfor i in url_args:\n\t\tif isinstance(i, basestring):\n\t\t\turl_args[i] = urllib.unquote(args[i])\n\t\telif isinstance(i, list):\n\t\t\tfor i2 in i:\n\t\t\t\turl_args.extend(i2)\n\treturn url\n\n\ndef _urlencode_query_params(query_params):\n\turl_param_format = \"json\"\n\tif query_params is not None and isinstance(query_params, dict):\n\t\turl_param_format = urllib.un"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\turl = url.replace(_BASE_URL, url_args['base'])\n\targs_ = []\n\tfor k, v in url_args.items():\n\t\tif k == 'base': args_.append(base=v)\n\t\telse: args_.append(val=v)\n\treturn args\n\n# The original url (for our purposes of using that) is\n# _replace_url_args('http://www.example.com', ['a', 'b', 'c'])\n# This is a dictionary with one key: value pairs as keys\ndef _replace"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\tif len(url.split('/')[4]) < 2 or len(url.split('/')[4]) > 5:\n\t\treturn False  # prevent this error from showing up\n\n\turls = url.split('/')\t # split URL into URL\t\t## XXX:\n\tfor url in urls[:3]:\t # get URL list (first try)\t##\n\t\tif url in url_args:\t # a valid URL item?\n\t\t\turls.remove(url)\n\n\turls.append('http://')\n\turls.append('http://www.example.com?"}]}
{"task_id": "62ece4982e6aefcf4aabbd63", "prompt": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn is_none(val) and not hasattr(val, \"__dict__\")\n\ndef is_none(val: Any) -> bool:          return is_none_value(val) and not val\ndef is_none_value(val: Any) -> bool:    return True\n\ndef is_none_dict(val: Dict[str, Any]) -> bool:\t\"\"\"\n\treturn is_none_value(val[\"foo\"]) and not val[\"foo\"] in (None, None, \"{}\")\n\ndef is_none_"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\tif not isinstance(val, six.text_type):\n\t\treturn false\n\n\tif val!= \"\":\n\t\treturn True\n\n// Copyright 2017-2020 the u-root Authors. All rights reserved\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\n\npackage main\n\nimport (\n\t\"flag\"\n\t\"os\"\n\n\t\"github.com/gimpacket/gpack\"\n\t\"github.com/gimpacket/gpack/gst\"\n)\n\n// TODO:"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn type(val).__name__ == 'NoneType'\n\n\nclass GenericListType(GenericValue):\n\t\"\"\"\n\t   Generic, generic list-like data type.\t\"\"\"\n\tdef __init__(self, val: any) -> None:\n\t\tself.val    = val\n\t\ttype.__init__(self)\n\n\tdef __eq__(self, other: 'GenericListType') -> bool:\n\t\tif type(self)!= type(other): return False\n\t\treturn True\n\n\tdef is_none_string(val: any) -> bool"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn None is None or type(val) is not None else False\n\n\ndef is_none_iterable(val: Any) -> bool:\t\"\"\"\n\t\"\"\"This method is used to return True if the value is a sequence\n\tof Nones.\n\n\t:Parameter val: The value to be checked.\n\t:type val: Any\n\t:Example:\n\n\t::\n\n\tdef is_none_iterable(in_val: Any) -> bool:\t\"\"\"\n\t\treturn None is None or type(in_val) is not None else False\n\n\t\"\"\""}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn val is None\n\n\ndef is_undefined_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a undefined value.\t\"\"\"\n\treturn val is UndefinedValue\n\n\ndef is_empty_string(*val_types: str) -> bool:\n\ttup = val_types.copy()\n\tfor o in all_values(tup):\n\t\tif not tup[o] == \"\": \tyield False\n\t\t\tif any(o == \"undefined\" or o == \"null\"){\n\t\t\t\tyield True\n\t\t\t}"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn isinstance(val, str)\n\n\n@pytest.fixture(scope='module')\ndef valid_value(value: str) -> object:\n\t\"\"\"\n\tCreate a valid value, e.g. \"None\" or \"1.2\"\n\t\"\"\"\n\tif value.startswith('$') or is_bytes_like(value) or is_unicode_like(value):\n\t\treturn object()\n\ttry:\n\t\treturn value\n\texcept AttributeError:\n\t\treturn str(value)\n\n\ndef is_array(value: Any = None) -> bool"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\tif not isinstance(val, str): # not a string\n\t\treturn False\n\treturn re.sub(r\"[.a-zA-Z0-9._]\", \" \", val)\n\n\ndef is_empty_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\tif not isinstance(val, str): # not a string\n\t\treturn False\n\treturn re.match(r\"[^_.\\\\-]\\d{1,9}$\", val)\n\n\ndef is_empty_list(val: any"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\ttry:\n\t\tis_none(to_string(val, False))\n\t\treturn True\n\texcept TypeError:\n\t\tpass\n\texcept ValueError:\n\t\tpass\n\treturn False\n\ndef is_none_list(val: any) -> bool:\t\t\"\"\"\n\tval = (val or \"\").decode('ascii')\n\treturn isinstance(val, list) and not [] in val and not is_none(val[-1])\n\ndef is_none_dict(val: any) -> bool:\t\t\"\"\"\n\tval = (val or \"\").decode('"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\"\"\"\n\tvalue = __check_default_value_string(val)\n\n\tvalue: C.nullable = None\n\treturn value\n\ndef __check_default_value_list(val: Union[\n\tList['__Pyx_TypelessType'],\n\tList['__Pyx_TypelessArrayElementRef'],\n\tList['__Pyx_TypelessStructElementRef'],\n\tList['__Pyx_TypelessTupleElementRef'],\n\tList['__Pyx_TypelessEnumElementRef'],\n\tList['__Pyx_TypelessTupleElementRefRef'],\n\tList['__"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\treturn not isinstance(val, str) and len(val) == 0\t\t\t\t# <<<<<<<<<<<<<<\n\n\n# The values that can be passed.\nZERO = 1\nZERO_STR     = '\\0'*ZERO\nONE    = 1.\n# The actual value types.\n\n\ndef string_format_value(value: str) -> str:\t\t# <<<<<<<<<<<<<<\n\t\n# String format of an object.\n# TODO: This should be replaced by a general format of dict/list\ndef string_format_object_value(value: Dict[str"}]}
{"task_id": "62ece4982e6aefcf4aabbd65", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tp = argparse.ArgumentParser()\n\tp.add_argument(\"--help\", action=\"help\",\n\t\thelp=\"Display Help message\")\n\tp.add_argument(\"-h\", action=\"store_true\",\n\t\thelp=\"Display the full help and exit\")\n\tp.add_argument(\"--version\", action=\"version\", version=\"%(prog)s \" + __version__)\n\targs = optparse.OptionParser().parse_args( p )\n\tif \"help\" in args and not args.help:\n\t\tprint __doc__ \"\\n\"\n\treturn args\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = parser.parse_args([\"-i\"], parse_flag=True)\n\targs, unknown = parser.parse_known_args([\"-i\"])\n\t\n\tfor arg in args:\n\t\tif arg[0]!= \"-\":\n\t\t\tflags.insert(0, arg.replace(\"-\", \"\"))\n\tfor arg in unknown:\n\t\tflags.insert(0, arg)\n\treturn flags\n\ndef get_file_contents_from_string(source_string, encoding):\n\t# A Python string object is a valid file that can be opened with the open"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tparser.add_argument(\"--version\", metavar=\"BOOL\",\n\t\tdefault=False,\n\t\thelp=\"\"\"Turn this ON (or ONLY) when parsing the package. Apart from -v and --version\nto turn this ON/ONLY when building the distribution.\n\n  --dry-run always prints the package as being built\n\n  --help prints this usage message\"\"\"\n\tparser.add_argument('--help', action='help', default=None,\n\t\thelp=\"Show this help message.\")\n\n\tparser.add_argument('--force', action='store_true', default"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\targs = parser.parse_args(['-', '--', str(sys.argv), '-'])\n\tflags = []\n\ttry:\n\t\tif args.flags:\n\t\t\tflags.append(args.flags)\n\texcept Exception:\n\t\tpass\n\tif len(args.args) > 0:\n\t\tflags.append(args.flags_args)\n\n\treturn flags\n\n\nclass ArgumentType:\n\tclass __bool():\n\t\tdef __bool__(self):\n\t\t\tif hasattr(self, '__bool__') and self.__bool__:\n\t\t\t"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = []\n\targ_type, var_type, default = parser.parse_known_args()\n\tflags.append((arg_type, var_type))\n\tswitch var_type:\n\t\tcase 'INT': case 'I': case 'CHAR':\n\t\t\tflags.append((parser.arg_char, 'C'))\n\t\tcase 'BOOL':\n\t\t\tflags.append((parser.arg_bool, 'B'))\n\t\tcase 'FLOAT': type = float\n\t\tcase 'DOUBLE': type = float\n\t\tdefault:  return \"\"\n\t"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = parser.parse_known_args(command_parser, ['-h', '--help'])\n\treturn flag_flags(flags) + (','.join(flag_flags(flags[0])))\n\ndef version(parser):\n\t\"\"\"\n\tGiven a print context, return a (displayed) integer value.\n\t\"\"\"\n\tif parser.get_context().is_main_func:\n\t\tif parser.is_quiet_mode:\n\t\t\tcontext.print_default = True\n\t\t\tcontext.default_stdout = sys.stdout\n\t\telse:\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\targument_names = []\n\tif parser.has_help:\n\t\tprint \"usage: %s [-h]\" % (parser.command)\n\t\tsys.exit()\t\t\t# Exit code 0 means success\n\n\tfor arg_name in parser.argparse_args:\n\t\t# check if the arg_name has a help string.\n\t\tif parser.help_flag:\n\t\t\tprint argparse.ArgumentParser.help_text(parser, arg_name, parser.flag)\n\t\telse:\n\t\t\targument_names.append(arg_name)"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\treturn ''.join(sorted(parser.argv.split()))\n\ndef parse_args(flags):\t\"\"\" Parse the argparse flags in a Python string and return\n\"\"\"\n\tflags = flags.strip().split()\n\treturn ''.join(['%s=%r' % (f_name, value)\n\t\t\t\t\t\t\t for f_name, value in flags],\n\t\t\t\t\t key='replace' if '='.join(FLAGS) not in kwargs else value)\n\nclass FrozenWarnings:\n\t\"\"\"Forges warnings that have been made to avoid unintentional\n\t\tto users (which would"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\tflags = {}\n\tparser.set_defaults(flags)\n\treturn flags['parse_args']\n\n\ndef argumentparser_flags(parser):\t\"\"\"\n\tGiven a argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n\tBoth the parse and option processing flags returned by the argument parser are also applied for\n\tthe given argument 'option'.\n\t\"\"\"\n\tflags = {}\n\tparser.set_defaults(flags)\n\treturn flags['arguments']\n\n\ndef argparse_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t# NOTE: this line of code can't be part of the public API, as it is just intended to be parsed,\n\t# but the contents of the.parse_args method is unclear that if you do it this way it is easy\n\t# to parse into an argparse.ArgumentParser instance.\n\tparser.add_argument('-x', action='store_true', dest='show_args', help='Add a list of arguments (one per argument)')\n\tparser.add_argument('-t', action='store_true', dest='show_tests', help='Enables the test framework to"}]}
{"task_id": "62ece4982e6aefcf4aabbd66", "prompt": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t# TODO: We can do a case when there are two files: one with absolute paths and the second\n\t#       with same paths, but paths are absolute, and only this path is listed\n\t#       in the list. This is probably a bug, because it means that the user\n\t#       should only see paths that have absolute paths but still are\n\t#       also files with differing path names\n\t# Note that there is no test for absolute paths except this one.\n\n\tif path_"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t# Check if there's a conflict so only test duplicates.\n\tif any(path in name.entries for name, path in processed):\n\t\tself.print_conflict_path(path_name, verbose)\n\t\treturn False\n\tif path.is_dir() and path not in processed:\n\t\tself.print_conflict_path(path_name, verbose)\n\t\treturn False\n\treturn not self.has_entry_in_process(path_name,\n\t\tprocessed, path_name, verbose)\n\ndef _check_already_processed_entry"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tif processed is None: return False\n\tif path_name and path_name not in processed:\n\t\tif not is_dir(path_name):\n\t\t\tif verbose: print('Ignoring path \"%s\", path does not exist' % path_name)\n\t\t\treturn False\n\t\tprocessed.add(path_name)\n\tif processed[path_name]: return False\n\treturn True\n\n\ndef read_all(file):\n\t\"\"\"\n\tReads the contents of path name file and returns a list.\n\t\"\"\"\n\treturn parse_contents(file)\n"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tif not (path_name in processed):\n\t\treturn False\n\tmlist = processed[path_name]\n\n\t# add each entry to the set\n\tfor el in mlist:\n\n\t\tif el is not None: # the entry to process is already a path to\n\t\t\tif not path_name in processed:\n\t\t\t\tprint \"Removing \" + path_name + \" since it was already in there\"\n\t\t\t\tcontinue\n\t\t\tprint \"\\tAdding \" + path_name + \" to the cache\"\n\t\t\tmlist[el].remove(m"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tif not processed[path_name]:\n\t\tif verbose:\n\t\t\tprint \"Warning: processing path %s\" % path_name\n\t\tprocessed[path_name] = set(), set()\n\treturn len(processed[path_name])>0, True\n\ndef save_path(processed, path_name, format_str=None, encoding=c_default_encoding,\n\t      indent_level=0):\n\treturn [format(d, format_str, encoding, indent_level) for d in\n\t    processed[path_name"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t#\n\tassert path_name in processed, \"path_name should be in the set\"\n\n\t# If the file we're processing is an absolute path name, and we have seen\n\t# just one more entry in the path_name set add it to the set.\n\tif path_name in processed and verbose:\n\t\tprint(\"Processing file or directory in the set: \" + path_name)\n\t\tprinted = \"\\n\".join(\n\t\t\tprint_path(path_name, \"was processed\") for path_name in processed)\n\t\tif len(printed"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t# First, find it already in the set.\n\tprocessed_set = set(processed)\n\tfor p in processed_set:\n\t\tif p.find(path_name)!= 0:\n\t\t\treturn False\n\treturn True\n\n\ndef is_directory_or_file(dir_name, verbose=True):\n\t\"\"\"Check if ``dir_name`` was processed by something else than the\n\tfile name in that file.\n\n\tArgs:\n\t\tdir_name: Name of the directory you're looking for\n\t\tverbose: True if verbose output is"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t#print \"path_name\",path_name\n\tif path_name in processed:\n\t\ttry:\n\t\t\tpath = path_name\n\t\texcept:\n\t\t\tpath = processed.pop(path_name)\n\t\tif not processed.contains(path):\n\t\t\tfound = []\n\t\t\tif verbose: print \"find\", path_name\n\t\t\tfor p in path:\n\t\t\t\tif verbose: print \"   \", p\n\t\t\t\tif p == path_name: return True\n\t\t\t\tif path == processed.get(p): return True\n\t\t\t"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tif not os.path.exists(path_name):\n\t\treturn False\n\n\tif verbose:\n\t\tprint(('Running %s' % (path_name)))\n\n\t# Load the current set\n\tset_name = os.path.split(path_name)[0]\n\tset = set()\n\n\t# Load the sub-directory\n\tif path_name.startswith(set_name):\n\t\tsub_path_name = \"%s/%s.sub_directory\" % (path_name[len(set_name):], path_name)"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\tprocessed.remove(path_name)\n\treturn processed.add(path_name)\n\n\ndef is_file_in_set(entry_name, entry_path):\n\t\"\"\"Check if a file or directory path exists in the set.\n\n    Note that this function exists to handle file and directory names\n    in the set.\n\n    Args:\n        entry_name: File or directory name in the set.\n        entry_path: Path or file name in the set.\n\n "}]}
{"task_id": "62ece4982e6aefcf4aabbd67", "prompt": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t", "suggestions": [{"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\tif vertices[0]=='A' and vertices[1]=='B' and vertices[2]=='C' and vertices[0]<vertices[1]:\t#A\n\t\treturn (0,0): tuple(points)\n\telif vertices[0]=='B' and vertices[1]=='C' and vertices[2]=='D' and vertices[1]< vertices[0]:\t#B\n\t\treturn (1,0): tuple(points)\n\telif vertices[0]=='C' and vertices[1]=='D' and vertices[2]=='A' and vertices"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t#first vertex on the edge \n#ABCD first vertex in the corner (the polygon)\t\t#edge of ABCA -> top vertex\t\n#ABC   C    D  -> two vertices\t\t#top vertex of ABC -> top vertex\t\n#BCD   A    D -> two vertices\t\t#bottom vertex of BCC -> top vertex\t\t#a is the corner of the polygon\t \n\treturn 1+vpair for vpair in vertices.split()\n\n#The first vertex and the 2 vertices on the corner is the right edge : #edge of ABC"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\tABCD = tuple(range(len(vertices)))\n\tABCD_dict = {}\n\tfor i in range(len(vertices) - 1):\n\t\tABCD_dict[ABCD[i + 1]] = i\n\tB = set(vertices)\n\tfor i in range(len(vertices) - 1):\n\t\tB.update(ABCD_dict[vertices[i]])\n\treturn [tuple(p for p in B if p not in ABCD_dict)]\n\n#will return, one of 2, 3 tuples\ndef vertex2tuple(vertices):"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t#The polygon is represented as a polyhedron\n\treturn 3tuple(vertices)\n\nif __name__ == \"__main__\":\n \tprint \">> Vertex3d: \" + vertex3(vertices)\n\n \tprint \">> Polygon with vertices: \" + face3d(b3d)\n \t\t# Return the polygons as a triple\n\t\td4d = face4d(b4d)\n \t\tf4d = face4d(b4d)\n \t\tprint \">> 3d polygons: \" + [2 * face2d(f2d)"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\tD1D2DF = (\n\t\t((0,0), (0,0)), #A\n\t\t((0,0), (0,1)), #B\n\t\t((0,1), (0,0)), #C\n\t\t((0,0), (1,0)), #D\n\t\t((0,1), (1,0)), #E\n\t\t((1,0), (0,1)), #F\n\t\t((1,0), (0,1)), #G\n\t\t((1,1), (0,0)), #H\n"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\treturn list(zip(vertices, vertices[0], vertices[2]))\n\ndef polygonal(vertices):\t\"\"\"\n\tpolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\treturn list(zip(vertices, vertices[0], vertices[2])))\n\ndef triangle():\n\t\"\"\"Returns a two-tuple of 3 points, describing points on the 3-side of the vertex::\n\tpoint1,point2 are the corners of the triangle"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\treturn 3tuples(*vertices[:-3])\n\t#TODO how to add to the end of the graph?\n\n\t\ndef vertexarray(vertex1):\t\"\"\"\n\t#Returns an index from the polygon which is a vertex\n\t#will take into account the vertex to find its location in the polygon\n\t#(e.g. 0)\n\tpass\t#we have a good idea\n\treturn vertex1[-1]\n\nimport { useCurrentAccount, useLoading, UseCurrentAccountState } from'react-use'\nimport { useSelector } from'react-redux'\nimport"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t#This function is just used in the python function for getting the 3tuple in an iterator method of numpy :\n\timport numpy as np\n\tfrom scipy import ndimage\n\tfrom scipy.spatial importtransform\n\tdef check_n_3tuple(vertices):\n\t\t#Check if the polygon has 3 tuples of vertices;\n\t\treturn [(v,v) for n in range(len(vertices))]\n\treturn [n.toTuple().tuple for n in check_n_3tuple(vertices)]\n\ndef polygon2polygon(p):\n\t\"\"\"\n\tCreate an polygon"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\tdAB, aAbCd = edges(vertices)\n\tcabc, dABC = edges(vertices, vertex1=vertex2tuple, vertex2=vertex3tuple)\n\tcabda, aAbDd = edges(vertices, vertex2=vertex2tuple, vertex3=vertex3tuple, vertex1=vertex1tuple)\n\tdABcDd = edges(vertices, vertex3=vertex3tuple, vertex2=vertex2tuple, vertex1=vertex1tuple)\n\treturn vertices, []\n\ndef vertex2tuple2tuple(vertices1, vertices2):\tif isinstance("}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\tvertices = filter(lambda x: x[0] <= vertices[0])\n\treturn vertices\n\ndef line(vertices):\n\t\"\"\"\n\tReturns the intersection area of a line with the input list and return a tuple of two line information:\n\t1) the points from the intersection area to the first line from the first vertex and the\n\t\t# of the line,\n\t2) the point at the third point of the line,\n\t\"\"\"\n\n\tdef interpf(pts):\n\t\tret = np.vstack([pts[0], pts[1]])\n\t\treturn"}]}
{"task_id": "62ece4982e6aefcf4aabbd68", "prompt": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\treturn alphabet[int(number)+1:-1] if number < len(alphabet) - 1 or number+1 < len(alphabet) else '0'\n\n#include \"GodotRenderer.h\"\n#include \"Geometry/GlobalCoordinateSequence.h\"\n#include \"Core/Graphics/Graphics.h\"\n#include \"Core/Renderer/RenderStates.h\"\n#include \"Utils/Geometry/Geometry.h\"\n#include \"Utils/Geometry/GeoHash.h\"\n#include \"Utils/Geometry/LocalCoordinateSequences.h\"\n#include \"Utils/Geometry"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t# If alphabet is not None, we need to make sure that it was a list,\n\t# not a dict.\n\tif isinstance(alphabet, list):\n\t\t# Ensure alphabet is a list of digits.\n\t\talphabet = alphabet[::-1]\n\telif isinstance(alphabet, dict):\n\t\t# ensure alphabet is a dict.\n\t\talphabet = int_to_string(integer_max, alphabet, padding)\n\telse:\n\t\traise ValueError('alphabet should be None in format '\n\t\t\t'%s, %d' % (alphabet, alphabet.count"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tassert isinstance(number, int)\n\n\toutput = []\n\tif alphabet is None:\n\t\talphabet = list(range(256))\n\tcount = 0\n\tfor i in range(len(number) - padding, 0, -padding):\n\t\toutput.append(number[-1+padding])\n\t\tcount += 1\n\toutput.append('-')\n\toutput.append(number[count])\n\treturn ''.join(alphabet) + ',' + ''.join(output)\n\n\n@contextlib.contextmanager\ndef int_to_hex_string("}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tif not isinstance(alphabet, list):\n\t\talphabet = [alphabet]\n\tif alphabet:\n\t\talphabet.sort()\n\talphabet = ''.join(alphabet)\n\tif padding:\n\t\tnum_int = int(n * float(len(alphabet))*float(alphabet.index('0123456789ABCDEFG')) + float(padding)*float(len(alphabet)))\n\telse:\n\t\tnum_int = int(n * float(len(alphabet)))\n\treturn alphabet + '.' + num_int\n\n\n##\n#\tPython-specific classes and module-level methods"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tif number < 1:\n\t\treturn \"\"\n\n\tif number % 1 == 0:\n\t\treturn alphabet[number - 1]\n\treturn str(alphabet[number])\n\n\ndef word_to_int(word: str) -> int:\n\tletter_count = 0\n\tfor i in range(len(word)):\n\t\tletter_count += 1 + int_to_string(i, word, None)\n\treturn letter_count\n\n\ndef str_to_int(word: str) -> int:\n\tvar, space_after_number ="}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tif padding is None:\n\t\tpadding = 1\n\tif value < -padding:\n\t\tf = f\"%{(value + padding)}f\"\n\telse:\n\t\tf = f\"%{(value + padding - value - 1)}f\"\n\treturn f.format(*alphabet)\n\ndef list_to_int(value: List[str]) -> int:\t\"\"\" Convert a list of numbers to integers. \"\"\"\n\tresult = int(value)\n\tfor i, k in enumerate(value):\n\t\tresult *= 10\n\t\tresult += int(k"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tif padding!= 0:\n\t\tassert len(padding) == len(alphabet)\n\t\tfor i, c in enumerate(alphabet):\n\t\t\tassert int(int(padding[i]) & int(c) > 0)\n\t\t\tassert c.isdigit()\n\t\tassert len(padding) == len(alphabet)\n\t\tpadding = \"\".join(alphabet[i:i+2])\n\tassert len(padding) == len(alphabet)\n\treturn b64string_as_char(alphabet=alphabet, length=len(padding))\n\n@registerModule\ndef int"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tif number == 0:\n\t\treturn \"0\"\n\n\tif number > 0xFFFF:\n\t\tnumber = 0xFFFF - 0.5 + number\n\n\twhile number > 0:\n\t\tnumber = \"%.04x\" % number\n\t\tnumber = alphabet.index(number) + 1\n\n\treturn number.hex().upper()\n\n\ndef int_to_int(number: int, alphabet: List[str], padding: Optional[int] = None) -> int:\n\t\"\"\"\n\tConvert a number to an integer.\n\n\tIt assumes that the input"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tif number < 0:\n\t\traise ValueError('number is negative')\n\tif not numbers.count(number):\n\t\traise ValueError('number passed must be non-negative')\n\n\t# Convert to string using the given alphabet\n\tstr1 = int_to_string(number % 1, alphabet, False)\n\n\tif padding == None:\n\t\treturn str1\n\telse:\n\t\treturn int_to_string(number + padding, alphabet, True)\n\n\ndef to_number(number: int, separator = space, prefix = '0', suffix ="}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\tl = 0\n\twhile 1:\n\t\tn = int(number * int(l), 16)\n\t\tif l == len(alphabet) and n!= 1:\n\t\t\treturn alphabet[0]\n\t\tl += 1\n\t\tif n == 0:\n\t\t\tbreak\n\treturn \"\"\n\nimport { createSlice, PayloadAction } from '@reduxjs/toolkit';\n\nimport {\n  setLastVisitedTimeAndDuration,\n  setFirstVisitedTimeAndDuration,\n} from '../../store/actions';\nimport { getLastVisitedTimeAndDurationAction }"}]}
{"task_id": "62ece4982e6aefcf4aabbd69", "prompt": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tif flow_params['register_key'] == None:\n\t\treturn flow_params\n\tif register_number > -1 and register_number < flow_params['registers'].count():\n\t\treg_key = flow_params['register_key']%4\n\t\tif opentitan.options['use_register_mapping'] and register_value == register_number:\n\t\t\tif reg_key in flow_params['registers']:\n\t\t\t\traise Exception(\"Can not replace register\")\n\t\t\tflow_params['register_value']%= reg_key\n\t\telif"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t# register value is already replaced by register numbers\n\tfor key, _ in flow_params.items():\n\t\tvalue = flow_params[key]\n\t\tif value == \"\":\n\t\t\treturn register_value\n\t\telif register_number == value:\n\t\t\tlogger.debug(\"Registered value is found to be : %s\" % value)\n\t\t\treturn register_number\n\tlogger.debug(\"No registered value can be found in the dictionary\"\n\t\t\"[ %s, %s ]\" % (register_no, register_yes))\n\treturn register_no"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tfor flow in flow_params:\n\t\tif flow[register_value] == \"r\":\n\t\t\tprint(\"    Replace register with value in register with value\" +\n\t\t\t\t\t\" %s\" % (register_number, register_value))\n\t\t\tflow[register_number] = register_value\n\treturn flow_params\n\ndef replace_register(flow_params, register_number, register_value) :\n\t\"\"\"Replaces value for given flows in given register number\n\n    Replace value for given flows in given register number\n\n    :"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t#replace key value pair that corresponds to given register numbers in flow params\n\treplaced_array = []\n\tfor i in range(register_number):\n\n\t\t#make it key is string\n\t\tkey = str(register_number + i)\n\t\t#make it value is string in this flow params\n\t\tvalue = register_value.get(key)\n\t\t#replace value\n\t\treplaced_array.append([key, value])\n\n\treturn {register_number: replaced_array}\n\n\ndef _replace_user_register(flow_params,"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tfor reg, flow_dict in flow_params.items():\n\t\treg_value_dict = dict()\n\n\t\tfor key in flow_dict.keys():\n\t\t\tregister_number = key[4:4 + register_number].strip()\n\t\t\tregister_value = key[3]\n\n\t\t\tif register_number == register_value:\n\t\t\t\treg_value_dict[register_value] = register_number\n\n\t\treg_value_dict.update(register_value_dict)\n\n\t\tregister_value_dict = reg_value"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t# Remove duplicates, and replace values with the corresponding register\n\tif flow_params[register_number]!= set(register_number)):\n\t\traise ValueError(\"Duplicate register detected\")\n\tregister_value = register_value[:]\n\t_replace_all_register(register_value, register_number, flow_params[register_number])\n\treturn register_value\n\ndef _replace_all_register(register_value, register_number, flow_params):\n\tif register_value not in flow_params:\n\t\traise ValueError(\"Cannot insert register\")\n\tif register"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tif flow_params is None:\n\t\tprint('None in reflow')\n\telse:\n\t\tif register_number < len(flow_params):\n\t\t\tif register_number!= flow_params[register_number]:\n\t\t\t\traise KeyError('Invalid register number')\n\t\t\telse:\n\t\t\t\tprint('Replaced value from flows')\n\t\t\tprint('Key in registers:')\n\t\t\t_print_reg_key(flow_params[register_number], register_value)\n\t\t\treturn register_value\n\t\telse:\n\t\t\traise KeyError('Register"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t# If reg value is NULL, or empty use the default reg value\n\tif(register_value == None or not register_value[0].key.isupper()):\n\t\treturn flow_params[register_number]\n\telse:\n\t\treturn flow_params[register_number] = register_value\n\ndef _replace_reg_key(flow_params, reg_key, reg_value):\n\treg_num = str(reg_value[0]).replace(\"_\", \"-\")\n\treplace_regex = re.compile(r\"^(?:_\\d+)(=("}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tkey = _replace_register_key(flow_params, register_number)\n\tset_register_value(_get_register_value(flow_params, register_number), key, register_value)\n\n\n\ndef set_register_value(data,register_number,register_value):\n\t\"\"\"\n\tSets values from given register number\n\tregards the value to given register number\n\n\t:param data: Dictionary containing defined flows\n\t:param register_number: The number of register where regards are stored\n\t:param register_value: Key to be"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\tif \"register_value\" in register_value.keys():\n\t\t# check that only the value in the flow_params dictionary is present\n\t\tassert \"value\" in register_value.keys()\n\t\tassert \"register\" in register_value.keys()\n\t\t# replace value of'register' with'register to replace'\n\t\treg_value = dict(register_number)\n\t\tif register_value[register_number] == register_value.get(\"value\", \"0\"):\n\t\t\treg_value = dict(register_value)\n\t\t\treplace"}]}
{"task_id": "62ece4982e6aefcf4aabbd6a", "prompt": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t", "suggestions": [{"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\treturn \"\".join([letter[c] for c in arg.split()]) + value\n\n\ndef escape_underscores(f):\n\t\"\"\"Escapes all underscores in f\"\"\"\n\tresult = list(map(lambda c: c[0].replace(\"_\", \" \").upper(),\n\t\t\t\t f.split(\"_\")))\n\treturn result + result[1:]\n\n#pragma once\n\n#include <boost/asio/experimental/stream_protocol.hpp>\n#include <exception>\n\nnamespace azure { namespace google { namespace storage_lite { namespace detail {\n      "}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\traise NotImplementedError()\n\n\tif not isinstance(value, string_t): value = str(value)\n\treturn value.rsplit('.');\n\n\nreplace = replaceall\n\nclass Regex(object):\n\n\tdef __init__(self, *args):\n\t\tself.arg = *args\n\t\tself.re = re.compile('.*[._-]')\n\n\tdef search(self, re_str):\n\t\t# Check whether the pattern matches the value\n\t\tif re_str is not None and self.arg.search(re_str):\n\t\t\t"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tvalue = value.replace('.', arg)\n\treturn value\n\ndef split_str(str):\t\t\t\"\"\"\n\tSplits string in the form \"[....foo.]...\" (no commas) into tokens\n\t\"\"\"\n\tresult = []\n\twhile len(str) > 0:\t\t# find start of a word\t...\n\t\tif str[0] == '[' or str[0] == '{':\t# (start of a word) is the first character in this one\n\t\t\tresult.append(str[1:-1]) # split word into tokens\n\t\t\tcontinue\n"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\ttemp = arg.strip().replace('..', '.')\n\n\treturn temp\n\npackage org.codehaus.mojo.jaxb2.core;\n\n/**\n * @author Andres de la Pena\n */\npublic class CustomObjectNotFoundException extends RuntimeException\n{\n    public CustomObjectNotFoundException(String message)\n    {\n        super(message);\n    }\n}\n\nvar fs = require('fs');\nvar path = require('path');\n\nvar exec = require('child_process').exec;\nvar debug ="}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tm = [value]\n\tfor arg in arg[1:-2]:\n\t\tif arg[0] == '_' :\n\t\t\tm = [arg]\n\t\t\tcontinue\n\t\tm = [str(arg) for arg in m if not arg[0] in \"_.\"]:\n\t\t\tif arg[0] not in \".0123456789\" and arg not in \"_\" and \\\n\t\t\t\tm not in [\".-_.\"] and (\"0123456789\" not in arg):\n\t\t\t\tm = [arg] # back off the string\n\t\t\telse:\n\t\t\t\treturn m\n\treturn"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tif len(value) == 0:\n\t\treturn value\t\t\t\t# None is ok\n\t\n\tif ':' in value:\t\t\t\t\t\t# No path specified, assume first /\n\t\tvalue, rest = value.split(':', 1)\t\t# Path must be last\n\telse:\n\t\trest = value\t# Path must be last\n\t\n\tif '.' not in rest:\t\t\t\t# No path specified, assume last /\n\t\trest = '.' + rest\n\t\n\treturn '/'.join(rest.split('/'))\t\t# Path must be first\n\ndef replace_dots"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tvalues = value.split(',')\n\tfor index, value in enumerate(values):\n\t\tif ',' in value:\n\t\t\tvalue = value.split(',')[0]\n\t\telse:\n\t\t\tvalue = value.upper()\n\t\tif value in arg:\n\t\t\tvalue = arg[value]\n\t\tvalues[index] = value\n\treturn values\n\ndef main():\n\t\"\"\"\n\t\tFunction: main\n\t\"\"\"\n\tpass\t# for test\n\tif __debug__:\n\t\timport pyparsing as p\n\t\tp.pprint"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t#\n\tfor i, p in enumerate(arg.split(\".\")):\n\t\tvalue = value.replace(p,\"\")\n\treturn value\n\n\ndef normalize(value):\n\t\"\"\"\n\tReturns a normalized version of a value\n\t\"\"\"\n\tparts = text(value).replace(\".\", \"\")\n\treturn re.sub(\"\", \"\", re.sub(r\".*:\\s*\", \"\", parts).strip())\n\n/*\n Copyright (c) 2008-2009 TrueCrypt Developers Association. All rights reserved.\n\n Goverdon is a trademark of the TrueCrypt source code; and"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\tret = []\n\tfor i in range(len(value)):\n\t\tret.append(value[arg-i])\n\treturn ret\n\n\nif __name__ == '__main__':\n\tvalue = \"barr.py\"\n\targ = \"hello\"\n\n\tif value.find('.')!= -1:\n\t\tvalue = value.replace('.', '.')\n\t\targ = arg.replace('.','.')\n\n\treplace_dots(value, arg)\n\nusing System;\nusing System.Collections.Generic;\nusing System.Configuration;\nusing System.Data"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\ttry:\n\t\tlen_val = len(value)\n\texcept:\n\t\tpass\t# Catch errors\n\telif not isNumeric(value):\n\t\traise ValueError(\"%s is not a numeric (expected 1 char, got %s)\" % (value, str(arg)))\n\telse:\n\t\twhile arg:\t# if only 1 char then break\n\t\t\tvalue = value.replace(arg, \".\")\n\t\t\tnum = str(num_val).rstrip(arg)\n\t\t\tnum = num.replace(arg+\".\", \"\").strip()\n\t\t\targ"}]}
{"task_id": "62ece4982e6aefcf4aabbd6b", "prompt": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t", "suggestions": [{"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\treturn [o for o in cls.__subclasses__() if o not in cls.__supers__.values()]\n\ndef superclasses(cls):\t\"\"\"Return all SuperClasses contained in klass\"\"\"\n\treturn set(c.__subclasses__(cls) for c in cls)\n\ndef members(cls):\t \"\"\"Return a list of Members of a class\"\"\"\n\treturn [Member(o, *memberlist) for Member, o in cls.__members__.items()]\n\ndef allmembers(cls):\t \"\"\"Return the set of Members of an instance\"\"\"\n\treturn set(Member(o, *member"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tdef recurse(cls, sub):\t\"\"\"Recursive implementation of subclasses().\nOutput: (int: 0: Nothing in the subtree (including the top-level class).\n          1: The number of members.\n          2: The class or superclass of sub.\n          3: The number of functions and variables.  */\n\ti = 0\n\twhile i < len(cls):\n\t\tif i > 0 and cls[i-1] == sub:\n\t\t\t# if"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tresult=[]\n\tall_classes = (inspect.getmro(cls.__class__)(x) for x in sys._getframe(1).f_globals.values())\n\tfor cls in all_classes:\n\t\tresult.extend(cls.__mro__())\n\t\n\treturn result\n\n\ndef all_instances(cls):\t# python2 and 3.6\n\t\"\"\"\n\tThis function is a wrapper of all instances of the class.\n\t\"\"\"\n\tall_classes = (inspect.getmro(cls.__class__)(x) for x in sys._"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t# I assume I don't think it's much better than a list\n\tl,i = _get_subclass(cls)\n\twhile i:\n\t\tl = nll_reverse(l)\n\t\ti = _get_subclass(cls,l)\n\treturn l\n\ndef _get_subclass(cls,l):\t\"\"\"\n\tReturns the proper subclass of a class, if there is one\n\t(e.g. int), or None.\n\t\"\"\"\n\to,i = cls.lookup(l)\n\tif o!=-1:\n\t\t"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tclasses = [cls] * len(cls.__subclasses__)\n\tfor cls in classes:\n\t\tcls.__subclasses__.append(cls)\n\treturn classes\n\n\n\n\nclass _DeferredList(_List):\n\n\tdef __init__(self, it):\n\t\tself.__len__ -= len(it)\n\t\tself.__items__ = []\n\t\tself.it = it\n\n\tdef __lt__(self, other):\n\t\tif self.__len__ > 0:\t\t# Not done, more than one item left?\n\t\t\treturn False\n"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tcls_members = cls.__dict__.keys()\n\n\tfor name in cls_members:\n\t\tif name == '_': name_ = ''\n\n\t\tchild = getattr(cls, name)\n\n\t\t# check if subclass exists\n\t\tparent_members = child.__dict__.keys()\n\t\tif parent_members:\n\t\t\tfor parent_class in parent_members:\n\t\t\t\t# make sure it exists\n\t\t\t\tif not children(parent_class):\n\t\t\t\t\tchild = getattr(child, name)\n\t\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\t"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t# recursive check\n\ttry:\n\t\tcls.superclass.__class__ = cls\n\t\treturn\n\texcept AttributeError:\n\t\treturn\n\t\n\t# recurse\n\ttry:\n\t\ti = cls.__superclass__\n\t\tcls.subclasses(i)\n\texcept AttributeError:\n\t\tpass\n\ndef add_classes_to_class(cls, classes):\n\t\"\"\"Adds all classes (or methods) to the current class(s) specified by classes\"\"\"\n\tfor base in classes:\n\t\tadd_classes_to_class(cls, base"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tsuperclasses = (c_to_str[c] for c in cls.subclass_hierarchy)\n\treturn sorted(get_subclasses(c_to_str[c]))\n\n\ndef subclasses(cls):\n\tif cls.__dict__.issubset(subclasses(cls)):\n\t\treturn cls.__dict__\n\tparent = cls.__parent__\n\tsubtypes = set([c_to_str[c] for c in parent.subclasses]) | set(c_to_str[c] for c in _getclasses(parent))\n\tfor child in parent"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tp = None\n\tfor sub in dir(cls):\n\t\tif sub[0] == '^':\n\t\t\tp = sub.rsplit('__', 1)[0]\n\t\t\tcontinue\n\tif not p:\n\t\traise ValueError('not a class class; it has no subclasses\\n')\n\twhile p == 'a':\n\t\tcls[] = subcls(type(cls))\n\t\tfor sub in cls:\n\t\t\tsubcls(sub)\n\treturn p\n\n###########################################################################\n##  API FUNCTIONS\n###########################################################################\n\ndef get_all"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\tclf = cls.subclasses\n\treturn [t for t in clf]\n\ndef _class_info(klass):\n\tclsname = klass.__name__\n\tcls = __import__(__package__, clsname)\n\tif cls is not None: return cls.__dict__\n\tinst = __import__(clsname)\n\tif inst is not None: return inst.__dict__\n\n#\n# ClassInfo\n#\n\nclass ClassInfo:\n\t__metaclass__ = classmethod\n\tdef __getattr__(cls, name):\n\t\tif sys.version_"}]}
{"task_id": "62ece4982e6aefcf4aabbd6d", "prompt": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tparts = string.strip().split(alphabet)\n\n\tif len(parts)!= 1:\n\t\traise ValueError('Expecting exactly one number')\n\treturn int(parts[0])\n\n\ndef string_to_str(string: str, alphabet: List[str]) -> str:\n\tparts = string.strip()\n\n\tif len(parts)!= 1:\n\t\traise ValueError('Expecting exactly one number')\n\treturn parts[0]\n\n\ndef is_list_of_pairs(elements: List[str]) -> bool:\n\t'''Does the input"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\treturn len(\n\t    [char for char in string.strip() if alphabet and char.isdigit()])\n\ndef file_to_string(file_path: str) -> str:\t\"\"\"\n\t# convert and remove the current file path, which will be the one\n\t# for the output\n\tfile_path = path.normpath(path.join(home_dir, file_path))\n\tif not os.path.isfile(file_path) or not os.path.isdir(file_path):\n\t\traise Exception(f\"Could not resolve file"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tcount = 0\n    alphabet_re = re.compile(r\"\\[(?P<key>\\d)\\]\")\n\tfor string in string_iter(string, alphabet):\n\t    key, = string.partition(' ')\n\t    count = max(count, int(key))\n\tif count < 10:\n\t    raise InvalidCode.code_error(\".\")\n\treturn count\n\n\ndef string_to_long(string: str, alphabet: List[str], digits: bool) -> int:\n\t\"\"\"\n\tConvert a string to a number,"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\ttry:\n\t\t# For backward compatibility, check for negative values\n\t\tif string < 0:\n\t\t\tstring -= alphabet[0]\n\t\telse:\n\t\t\tif len(alphabet) > 1 and string >= alphabet[0]:\n\t\t\t\tstring = alphabet[0]\n\t\t\telse:\n\t\t\t\tstring = alphabet[-1]\n\texcept ValueError:\n\t\traise ValueError(\"String is not in alphabet or is not a number.\")\n\treturn int(string)\n\n\ndef _string_is_integer(string: str) -> bool:\n\t\"\"\"\n\t"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tif len(string) == 0:\n\t\tprint (\"The input string is empty or empty! Please use a non-empty output!\")\n\t\tprint (\":\")\n\t\tprint (string)\n\t\treturn 0\n\tassert len(string) == len(alphabet)\n\tnewarray = 0\n\tfor i in range(0, len(alphabet)):\n\t\tnewarray += alphabet[i]\n\treturn newarray\n\n\ndef float_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number,"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tif string[0] in alphabet:\n\t\treturn 0\n\telse :\n\t\treturn int(string[0])\n\npackage com.example.base.database.dto.mybatis;\n\nimport lombok.AllArgsConstructor;\nimport lombok.Builder;\nimport lombok.Data;\n\nimport java.util.List;\n\n@Data\n@Builder\n@AllArgsConstructor\npublic class OrderDO {\n\n    private Long id;\n    private List<OrderDO> orderDOs;\n}\n\n/*\n * Copyright (C) 2013 4th Line GmbH"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t# This could perhaps simplify string_to_int(string.replace(alphabet,''),alphabet) for\n\t# \"a=1,b=2,c=3,d=4\"\n\tif alphabet.size == 2:\t# '1,2'\n\t\tn = int(string)\n\telse:\t\t# '1;2'\n\t\tn = int(string[0:len(alphabet[0])]*(1+alphabet[1])+'1')\t\t# 1\n\t\tn = int(string[len(alphabet[0])*1:(len("}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tn_alphabet = {True: 0, False: 1}\n\tif alphabet:\n\t\twhile len(alphabet[i]) > 1:\n\t\t\tn_alphabet.add(alphabet[i][0] + alphabet[i][1:])\n\t\t\ti += 1\n\n\t\tif not n_alphabet[int(n_alphabet[0]) > 1]:\n\t\t\tn_alphabet[0] += 1\n\n\tstart_idx = 0\n\tlen_string = len(string)\n\twhile start_idx < len(string):\n\t\tif string[start_"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tif not string:\n\t\t# Do nothing for length 0, this means to use the\n\t\t#'safe' encoding.\n\t\tstring, _ = self.safe_string_to_int(string)\n\tstring_to_int_safe(string, alphabet)\n\treturn string_to_int_safe(string)\n\n\ndef string_to_int(string: str, alphabet: List[str]) -> int:\n\twith StringIO(string) as stream:\n\t\tint_str = alphabet.index(stream.readline().decode('ascii'))\n"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\tm = int(string)\n\n\tif m < (alphabet.upper() or '') or m > (-alphabet.upper() - 1):\n\t\traise ValueError('Invalid integer.')\n\n\tif m >= 5:\n\t\treturn int(m)\n\telse:\n\t\treturn int('0' + str(m))\n\n\n### Utility routines ###\n\ndef formattime(t: datetime) -> str:\n\t\"\"\" Format a time using the format string in a datetime object. \"\"\"\n\ts = datetime(t.year, t.month-1, t.day,"}]}
{"task_id": "62ece4982e6aefcf4aabbd6f", "prompt": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\t# download archive from tar format\n\t\tr = requests.get(src + '.' + 'desc' + '/' + destination_path,\n\t\t\t      verify=False, headers=headers, timeout=timeout,\n\t\t\t      proxies=proxies, stream=False, use_ssl=False)\n\t\tr.raise_for_status()\n\t\t# extract the extracted archive for each package, with\n\t\t# 'r', 'path_to_archive', 'destinations', and 'exclude' options\n\t\tdestination_paths = sorted([(item."}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\ttar_archive_dir = tempfile.mkdtemp()\n\tdestination_path = os.path.join(tar_archive_dir, 'desc')\n\tassert_that(\n\t\tcopy.os.path.dirname(destination_path),\n\t\tequalToOrEqualTo(os.getcwd()),\n\t\t\"the path is not a directory\"\n\t)\n\tarchive_files = [\n\t\t\"core/core.tar.gz\",\n\t\t\"core/extra/community/com.mypackage.example/extra.module.py\",\n\t\t\"core/extra"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\tret = requests.get(source, encoding=\"utf-8\").url\n\n\tif ret[0].startswith('https?'):\n\t\tret = urllib.request.urlopen(ret[0], verify=False).read()\n\telif ret[0] == \"\":\n\t\tret = \"\"  # insecure if it is not found\n\treturn ret\n\ndef download_as_tar(url, destination_path):\n\t\"\"\"\n\turl: url of the.tar.gz archive to download\n\tdestination_path: path of the archive to download\n\n\tReturns a"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\treturn pypg.locate('file://' + url, destination_path)\n\n#pragma warning disable CS1591 // Missing XML comment for publicly visible type or member\n\nusing System.Threading.Tasks;\n\nnamespace Thrifty.Codecs.Mqtt\n{\n    public interface IMqttFrameHandler\n    {\n        /// <returns>Task</returns>\n        Task<byte[]> Receive();\n        /// <returns>string</returns>\n      "}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\turl_split = urlsplit(url)\n\tpath = path.replace(destination_path, '')\n\tif path[len(destination_path) -1]!= '/':\n\t\tpath = '/' + path\n\treturn path + '.tar.gz'\n\ndef get_archive( url, destination_path ):\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract.tar.gz archive\nthat matches 'core' in the file path.\neach.tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\treturn download_TARZArchive(get_URL(url=url, destination_path=destination_path))\n\t\ndef parse_file_tar(filename, destination_path=None, mode='w'):\n\t\"\"\"\n\tparse file, extract and save to path\n\n\tArgs:\n\t\tfilename: file name to extract tar file\n\t\tdestination_path: the path to save tar file \n\t\tmode: 'r' or 'w'\n\n\tReturns:\n\t\tTrue if a valid tar archive was successfully retrieved\n\t\tFalse if something errored\n\t\"\"\"\n"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\tdef getArchive( url: str, destination_path: str ) -> Path:\n\t\turl = url.rstrip('/')\n\n\t\t# Extract tar.gz file and make sure it exists\n\t\tif url.startswith(\"http://\"):\n\t\t\t_url = urllib.parse.urlsplit(url)\n\t\t\tif _url['scheme'] == 'https':\n\t\t\t\ttarballPath = 'https://' + _url['host'] + '/'.join(\n\t\t\t\t\t_url['path'].decode('utf8'),\n\t\t\t\t)\n\t\t\telse:\n\t\t\t"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\treturn  shutil.extract_archive(\n\t\turl=url,\n\t\tdestination_path=destination_path,\n\t)\t# returns a Path object, which has type Path\n\n\n#--\n#-- File operations:\n\n\ndef open(name):\n\tpath = join(dataDirectory, \"/home/meow/tools/test/\")\n\treturn open(path, \"rb\")\n\n\ndef write(path, data):\n\twith open(path, \"wb\") as f:\n\t\tf.write(str.encode(data))\n\n\ndef"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\tprint \"INFO: Please extract archive from 'url'\"\n\tprint \"INFO: Please extract archive from 'destination_path'\"\n\tprint \"INFO: please check if the file contains '.tar.gz'\"\n\nimport {\n\tGET_CURRENT_PORT_FROM_STATE,\n\tGET_DEFAULT_PORT_FROM_STATE,\n\tGET_HOST_FROM_STATE,\n\tGET_STATE_FROM_PORT,\n\tGET_PORT_FROM_STATE,\n} from './shared-types';\n\ntest('GET_CURRENT_PORT_FROM_STATE - no port is used',"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\tfrom.core import _extract\n\tfrom.tar import tarfile\n\ttar_path = \"tar.gz\"\n\tif destination_path is None:\n\t\tpath = (destination_path % (tar_path)).replace('.tar.', '')\n\telif not destination_path.endswith('.tar.gz'):\n\t\tpath = (destination_path % (_extract(tar_path)).replace('.tar.')).replace('.gz', '')\n\telse:\n\t\tpath =  os.path.join(destination_path, tar_path)\n\n\treturn path\n\n\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd70", "prompt": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t# os_mac = \"/Volumes/MacOS/macOS2018.12.19.bin\"\n\tfor i in list(system().files.keys()[:-1]):\n\t\tf = system().files[i]\n\t\tif f.os.startswith((\"mac\", 'unix')):\n\t\t\tos_mac = os.environ.get(\"OS_MAC\", \"darwin\")\n\t\t\tif os_mac == \"darwin\":\n\t\t\t\treturn False\n\t\telif os_mac!= \"darwin\":\n\t\t\tcontinue  # avoid getting macos-related errors, such as\n"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\tresult = os.mac_ver(current_ver, osname)\n\t#result = os.linux_ver(current_ver, osname)\n\t#result = os.mac_ver(current_ver)\n\treturn False\n\ndef has_cygwin_sys():\n\t#os.mac_ver('10');\n\tos.linux_ver('10', 'Cygwin.sys')\n\tos.linux_ver(os.SYS_CYGWIN)\n\treturn True\n\ndef get_os_name():\n\tget_version_str = ('x"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\tos_os = __import__(\n\t\t\"os\").platform().lower()\n\tif os_os!= 'darwin':\tos.makedirs('.')\n\t\tos.makedirs('./.sshfile')\t\n\tif os.path.exists('./.sshfile') or os.path.exists('.//.sshfile'): return True\n\treturn False\n\n\n# Get the list of hosts on the other SFTP hosts if --ssh is used\n# For example, --ip-file needs to be specified\n#    \"sshfile: /etc/ssh/ssh.sock"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\treturn os.getenv(\"X_OS\") in os.supported_platforms() and os.uname().machine in [\"i386\", \"ia64\"]\n\ndef is_mac_os(os):\n\t\"\"\"\n\tRuns the os.getenv() and checks if macOS has been installed in the path.\n\t\"\"\"\n\treturn os.getenv(\"X_OS\") == \"i686\" or os.getenv(\"X_OS\") in os.supported_platforms()\n\t\ndef is_no_i386_os():\n\t\"\"\"\n\tRuns the is_i386_os"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\treturn isWindows() and not os.isWindows()\n\n######## \n\n# Get the OS to use..\ndef getOs():\n\treturn os.uname()[1]\n\n# Get the full path to the current program.\ndef getProgramFilename(path):\n\twith open(path,'r') as f:\n\t\tline = f.readline()\n\tif line[:3] == \"cmd\":\n\t\treturn line\n\telse:\n\t\tprint \"Program: \" + line + \" Invalid\"\n\t\treturn \"\"\n\n##############\n#\n#\n# Get"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\treturn os.getenv('JAVA_VERSION', '0') in\n\t\t\t('10.10.10.10', '11.11.11.11')\n\ndef isWindows():\n\t\"\"\"Check if os is windows.\n\tReturn True is windows (or not)\n\t\"\"\"\n\tif not isinstance(os.uname(), (tuple, list)):\n\t\treturn False\n\tfor a in os.uname():\n\t\tif a in ('Linux', 'Darwin') and not a.startswith('-os-') and not a.startswith('-arch'):\n\t\t\tbreak\n\t\n\t"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\tif platform == sys.platform[:3] == \"darwin\":\n\t\treturn os.uname()[2] == \"MacOSX\"\n\n\t# Try and build\n\ttry:\n\t\tos.uname()[2] = getattr(os, \"uname\")[2]\n\t\treturn 1\n\texcept Exception as e:\n\t\treturn None\n\n    \nif __name__ == \"__main__\":\n\tprint(\"MacOS and FreeBSD\")\n\ttry:\n\t\tos.uname()[1] = getversion()\n\texcept Exception as e:\n\t\tprint(\""}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\tif os.GetEnvironmentVariable(\"OS\") == \"macOS\":\n\t\treturn True\n\treturn False\n\n\ndef OS_windows():\n\t\"\"\"\n\tTurn the os.getenv() string into a dictionary\n\tthe return value was copied from Windows, and we used to get it in the same language as in os\n\n\t :return: The dictionary returned by os.getenv()\n\t :rtype: dict\n\t \"\"\"\n\t# check if we could get the path\n\tsystem_env = os.getenv(\"SYSTEM\", \"SYSTEM\")\n\tsystem_path = os.path.join"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t#print \"os is macOS\"\n\t#if os.name!= \"darwin\":\n\t#\tprint \"os.name!= 'darwin'\", os.name\n\treturn os.name in (\"os\", \"netbsd\", \"win32\"):\n\t#os.path.exists() or os.env.get('PATH') or os.home not in sys.executable_path  or os.environ.get('PATH') or os.chroot(os.path.join(os.getcwd(), os.getcwd())+os.env.get('PATH'))\n\t#os.path."}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\tos, err = check_os_availability()\n\tif len(os) == 0:\n\t\tprint(err)\n\t\treturn False\n\treturn True\n\n# ------------------------------------------------------------------------------------\ndef check_os_availability():\n\t\"\"\"\n\tCheck if os is macOS or not\n\t\"\"\"\n\tprint(f\"Checking os availability: {os.mac()}\")\n\n\tif os.mac() and is_mac_os() == False:\n\t\tprint(f\"Using macOS: {os.mac()}\")\n\t\tos.exit(0)\n\treturn"}]}
{"task_id": "62ece4982e6aefcf4aabbd71", "prompt": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tconvert = {}\n\tfor k, v in item.items():\n\t\tif v is None:\n\t\t\tconvert[k] = ''\n\t\t\tcontinue\n\t\tif isinstance(v, list):\n\t\t\tfor v2 in v:\n\t\t\t\tconvert[k] += '-[' + re.escape(v2) + ']'\n\t\telse:\n\t\t\tconvert[k] = v\n\treturn convert\n\n\n############################################\n#\n#                        CLASS   "}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\treturn item['rules'] \n\n<?php\n/**\n * Copyright (c) Enalean, 2019-Present. All Rights Reserved.\n *\n * This file is a part of Tuleap.\n *\n * Tuleap is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * Tuleap is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tConvert *.h rules to regex rules\n\tfor rule in rules:\n\t\tif rule['a'] in item:\n\t\t\titem[rule['a']] = regex_keys[rule['a']]\n\t\tif rule['b'] in item:\n\t\t\tassert (item['b'] in rule['b']) \n\t\t\titem[rule['b']] = regex_keys[rule['b']]\n\t\tif rule['c'] in item:\n\t\t\tassert (item['c'] in rule['c']) \n\t\t\titem[rule['c']] = regex_keys[rule"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tfor rkey in regex_keys:\n\t\tprint convert_rules(item.get(rkey), rules)\n\tprint convert_rules(None, rules)\n\"\"\"\n\n\nif __name__ == '__main__':\n\tmain()\n\n/*\n * drivers/usb/core/ioctl.c\n *\n * (C) Copyright Linus Torvalds 1999,2003\n * (C) Copyright Johannes Erdfelt 1999-2001\n * (C) Copyright Andreas Gal 1999\n * (C) Copyright Gregory P. Smith 1999\n * (C) Copyright Deti"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tresult = {}\n\tfor i in rules:\n\t    for j in range(len(rules[i]['a'])):\n\t\ttmp = regex_keys.get(i, {}).get(j, {}).get('help')  # use default value\n\t\tif tmp == item.get('t', 't').lower():\n\t\t    key = i\n\t\t    while i!= k:\n\t\t\tkey += ',' + rules[i]['a'][j]\n\t\t\tresult[key] = rules[i]['b'][j]\n\t\t    tmp ="}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tConvert *.h files (as wildcard), add.cpp files\n\nExamples:\n    \nInput:\n    rules = {\n        '*.cpp':\n            {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n        '*.h':\n            {'h': 'help'}\n    }\n    regex_patterns = [r'.*\\.(cpp|h)',"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tres = {}\n\tfor key,val in rule_dict.items():\n        if key in item['rules']:\n        \tres[key] = {key: convert_to_regex_key(val)}\n    return res\n\t\n#\n# convert intact regex to rules\n#\ndef convert_regex_intact(input_regex):\n    \"\"\" convert intact regex to regex dict \"\"\"\n\n    output_regex = {key: val for key, val in input_regex.items"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\n'''\n\n\nif __name__ == \"__main__\":\n\timport sys, re\n\n \t# Extract input line by line\n\tlines_by_line = sys.stdin.readline().split(\"\\n\")\n\n\toutput = \"\"\"\\\n# This example shows how to convert keys into regexes that\n# correspond directly to the basename of *.cpp and *.h files.\n#\n# Convert *.cpp keys to regex keys\n\nrules = {\n    '*.cpp':\n        {'a': 'arf',"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tconvertedDict = {}\n\tfor p in item['values']:\n\t\tconvertedDict[p.get('key')] = p.get('value')\n\treturn convertedDict\n\ndef convert2regex(p):\n\t\"\"\"\n\tconvert2regex('.*') -> ('a.*', 'b.*', 'c.*')\n\tconvert2regex('.*') -> 'a.*'\n\n\tconvert2regex('.*', 'b.*') -> 'b.*'\n\n\tconvert2regex('.*') -> 'b.*'\n\n\tconvert2regex('.*', 'c.*')"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\tresult = {'x': r'^\\w+$', 'e': r'^[^.*]+$', 't': r'^\\w+$','s': r'^[^.*]+$', 'b': r'^\\w+$', 'h': r'^[^.*]+$', 'C': r'^[^\\n]+$', 'G': r'^[^\\n]+$', 'p': r'^\\w+$'}\n\tfor key in item:\n\t\tif item[key]<len(result):\n\t\t\tresult[key] +=  r'[^(\\w*]{2}$')"}]}
{"task_id": "62ece4982e6aefcf4aabbd72", "prompt": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tfor i, cand in enumerate(name.split('\"')):\n\t\tif cand.find('\"') <= i:\n\t\t\tname = name[cand.find('\"'):]\n\n\treturn name if name else \"\"\n\n\ndef _add_re(n, name) -> None:\n\t\"\"\"\n\tAdd quote from the given name to the n-th substring of it's own name.\n\t\"\"\"\n\ttry:\n\t\tf = re.compile(name)\n\texcept TypeError:\n\t\tf = re.compile(r'\\\\\\\\?' + name + r"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tglobal_quote = \"\"\n\tm = p.match('([^\\\\s]+){' + name + r'\\\\s*\\\\S*}')\n\tif m:\n\t\tm = m[1]\n\tp = p.replace('\\\\s+', '')\n\tre = re.compile(r'^(?P<quote>[^\\\\s]*)' + m + r'\\s+' + r'*?(?P<name>[^\\\\s]*)' + r'.*$')\n\twhile p:\n\t\tm = p.group(1)\n\t\tif re.match(m"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tquote, name = splitname(name)\n\n\treturn quote\n\n\n#----------------------------------------------------------------------#\n#  Parse\n#                                           name       flags                   desc\n# - the name is of the form 'name=value'\n# - the value is a quoted string          "}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tfor s in S:\n\t\tname = S[s]\t\t\t# The string\tthe quote is in\n\t\tparts = name.split(quote)\n\t\ts = quote.find(parts[0])\n\n\treturn s\n\ndef is_quote(S):\n\t\"\"\" Check is the quote in the given string S \"\"\"\n\tquotes = S[:] # Keep track of quotes already in brackets\n\n\tfor s in S:\n\t\tif re.search('@', re.escape(s)):\n\t\t\tquotes.append(s)\n\tS = quotes\n\n"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tl = 0\n\twhile l < len(line):\n\t\tm = re.match(a_quotemark, line[l:l+a_quotemark.length])\n\t\tif l == 0:\t# Last match.\t\t\t/ \\u{0000}\\u{00ff}\t// \\u{0000}\t\n\t\t\tif m:\n\t\t\t\tdel l, len(line)+m.end()[0]\n\t\t\t\n\t\telse:\t\t# Unpaired quotes.\t\t\\u{11ffff}\t// \\u{001f}\n\t\t\t"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tquoted_names: Dict[str, Set[str]]\n\t# Replace all occurrences of re with re.split()\n\tquote_re : List[str]\n\treplaced_words : Dict[str, List[str]]\n\t# Replace all occurrences of re with re.split()\n\t# Replace with \"''\", \"'''\".\n\tplain_re: Set[str]\n\n\tdef _is_word(self, w) -> bool:\n\t\t\"\"\"Checks if the given word is a match\"\"\"\n\t\treturn any(word in _RE_SUBSTITUTE for word"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tif name:\n\t\tif \"'\" in name:\n\t\t\t# print(name)\n\t\t\tname = name.replace('\"', '\\\"').replace('\\'', '&#39;')\n\t\t\t# print(name)\n\tn = [len(name)]\n\tfor m in range(1,len(name)):\n\t\tj = find(name,m)\n\t\tif (i < len(n) and n[i] == j) or (i == 0 and j==\"\") or (i == len(n) and n[i-1] == j):\n"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tpattern = compile(pattern)\n\tname = pattern\n\tif re.search(pattern, name) is not None:\n\t\treturn name[: len(pattern)]\n\tif re.search(pattern, '@@' + name +'@@') is not None:\n\t\treturn '@@' + name +'@@'\n\treturn name\n\ndef name_map(pattern, map_name):\n\tname = \"%s%s\" % (map_name, pattern.lower())\n\tfor char in map_name:\n\t\tname = re.sub(\"[^0-"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t_re_pattern = r'(?:\\\\|(?<=\\\\))(?<![\\]!)')\n\t_re_name_pat = re.compile(\n\t\tr'(?P<name>[^@&%&%'  # quote-and-ampersand ampersand\n\t\tr'\\s]+)*?)(?P<name>(.*))?'\n\t_re_string = re.compile(r'[^\"\\\\s\\\\\\\\])')\n\n\tdef match_name(self, name):\n\t\tm = _re_name_pat.match(name)\n"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\tfor p, c in map(reQuote, pieces):\n\t\tname, quote = c\n\t\tif quote == name:\n\t\t\tquote = None\n\t\tfor s in str(s) for b in map(re, set([quote] + reQuote + [''])):\n\t\t\tif s.lower().startswith(quote.lower()) and not s.endswith(quote.lower()):\n\t\t\t\tname = s.replace(quote.lower(), quote.upper(), quote.lower())\n\t\t\t\tif s!= quote.lower():\n\t\t\t\t\tname.insert(0, slash)"}]}
{"task_id": "62ece4982e6aefcf4aabbd73", "prompt": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t# Set which platforms are actually implemented in.\n\t# In order to check whether a platform has been passed by name, use\n\t# shlex.parse_platform_name() and pass in as an argument.\n\tif \"this\" in platform_platform_spec or platform_win_spec or platform_win_and_linux_spec:\n\t\tplatform_platform_spec = platform_platform_info\n\t\tplatform_plat_spec = platform_platform_info.name\n\t\tif platform_platform_spec.startswith(\"Windows\"):\n\t\t\t# In XP, Windows is a"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\tsys.maxint\t= getattr(os, \"sys.__maxint__\", -1)\n\tposix_environ = (sys.platform.startswith(\"linux\") and\n\t                 getattr(sys, \"pkeys\") is None)\n\tposix_fsplit\t= getattr(sys, \"posix_fsplit\", None)\n\tposix_split_tabs\t= posix_fsplit in sys.getfilesystemencoding()\n\n# This is ugly but is required to work around an old version that\n# used re as its last argument -"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\tdef __init__(self, env=None):     \n\t\tif env is None: env=os.environ\n\t\t# os.environ is a dictionary dict.\n\t\tself.env = None\n\t\tif os.path.isfile(self.executable):    \n\t\t\tfpath = open(self.executable, 'r')   \n\t\t\tcmd = readStdin(fpath)\n\t\t\tif 'run'\\npublish=True:\n\t\t\t\tself.env = os.environ.copy()\n\t\t\t\tos.environ['python']"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t# these are the same values we use in Windows Command. \n#\t# they are available as a bitmask\n\t# 0x01           (1 << 0) is the standard input\n#\t# 0x0f          (1 << 4) is the standard output\n#\t# 0x10          (1 << 8) is the standard error\n\t# 0x11           (1 << 12) is the standard exception\n\t#"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t# This should be a list of tuples describing environment variables\n\t# that are available in PATH for the launcher.  Each tuple is a list\n\t# with the environment variables (strings), each entry is a list of\n\t# paths.\n\tlauncher_paths = (\n\t\t{'ARGV_ENV_VARS': 'env_vars'},\n\t\t{'ARGV_PATHS': 'paths'},\n\t)\n\t\n#----------------------------------------------------------------------\n#\n# Shell command line interview\n#  This should not be required for an environment.  Use shellcmd directly\n#  and use the _"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\tsys.platform.startswith('win32_') or \\\n\t\\\n\tsys.platform.startswith('win16_')\n\tif sys.platform == 'win32_': \\\n\t\tself.max_cmd_line_len = os.sysconf( 'SC_OPEN_MAX' )\nelif os.name == 'nt': \\\n\tself.max_cmd_line_len = 0x0\nself.cmd_lines = maptools.parse_cmdline_str('', self.cmd_line)\n#\n# Check for trailing slash in filename; add it to argv"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\tfrom subprocess import Popen, DEVNULL\n\t\ndef re_replace(pattern, replacement):\n\t\"\"\"Replace special characters in pattern. \n\tThe pattern is the empty string (`''`).\n\tDoes not include the special characters \n\n\t   (``~`` and `\"\\\\\"`, ``#''`` are all reserved for quoting purposes)\n\n\n\tReturns a new string that replaces at most\n\tthe last occurrence of `replacement`, if matched.\n\n\tFor more information, see ``shlex.split()``.\n\t\"\"\"\n\tre_str = re.sub (r'^"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\tDEFAULT_BACKSLASH = re.escape('\\\\\\\\')\n\tBACKSLASH = re.escape('\\\\')\n\tSINGLE_SLASH = re.escape('\\\\')\n\n\tdef shlex_quote(src, i):\n\t\t'''Get a source string from one token at `i`.\n\t\t''')\n\t\t# We are trying to use backslashes inside of double quotes,\n\t\t# so don't bother checking for them.\n\t\treturn backslash.join([src[i+1:], src[i+1:]], backslash=backslash)+[src[i:]].l"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t# Note: we cannot relabel everything as this will make the\n\t# first/last non-command/not-command flag unusable.\n#         for arg (if not passed) we need to relabel everything to start\n          with first command or no non-command/not-command flag.\n\t@param nopen: flag indicating the state of `nopen`.\n                  nopen may be -1 to force reset.\n    "}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t# \"command-line\" command-line options\n\tno_color = 'n'\n\tprompt   = ''\n\tcolor    = 'n'\n\tcommand  = ('\\\\t', 't')\n\tshell    ='sh'\n\tencoding = 'UTF-8'\n\tre_shell   = re.UNICODE\n\n\tdef do_split(self, env, buf):\n\t\t\"\"\"Split a file with newlines.\"\"\"\n\t\ttry:\n\t\t\tdata = subprocess.check_output(\n\t\t\t\tself, shell=self, env"}]}
{"task_id": "62ece4982e6aefcf4aabbd74", "prompt": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tif not exists(temp_url):\n\t\tprint(f'Error:'+ temp_url + f'dis not exist. Aborting.')\n\t\treturn (f'{self.name.upper()}archive.zip').encode()\n\treturn self.run_cmd(os.path.join(temp_url, 'archive.tar.xz'), temp_url)\n\n\ndef run_cmd(cmd, tempurl):\n\tprint(f'Running \"'{cmd}'\"...')\n\tstatus = subprocess.run(cmd, shell=True)\n\tif status == 0"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\turl_in_repo_path = \"/\".join([archive_repo_url]+[1:][0]+[1:])\n\turl_in_repo_path = url_in_repo_path.replace('.zip', '')\n\treturn url_in_repo_path\n\ndef archive_create(origin_url, archive_type, archive_url, archive_source):\n\n\tresult = {\"url\":archive_url,\n\t\t  \"origin_url\":origin_url,\n\t\t  \"source_url\":archive_source,\n\t\t  \"remote_url\":archive_url."}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\ts = subprocess.Popen(\n\t\t[sys.executable, \"unzip\", archivedir,\n\t\t\t\"--compress\", archivepath,\n\t\t\t\"--dest-dir\", destdir]\n\t)\n\tres = s.wait()\n\t# check whether the result is \"OK\"\n\tif res == 0:\n\t\tprint(\"The archive already exists.\")\n\telse:\n\t\tprint(\"Uncompressing the archived archive failed. \"\n\t\t\t\"Please check if the archive path exists. \"\n\t\t\t\"The destination path is not a directory or \\"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tif archive_path.upper().startswith(\".\"):\n\t\tarchive_path = archive_path[1:]\n\tdownload_url = urljoin(url_path, filename)\n\tcmd_path = archive_path + (download_url,)\n\tprint(\"Executing: \")\n\tprint(\" \".join(command_to_execute))\n\tprint()\n\tsubprocess.check_call(cmd_path, shell=False, output=subprocess.PIPE)\n\treturn download_url\n\n\n# Check to see if file is a base file.\nif not os.path.isfile"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tif not os.path.exists(archive_path):\n\t\treturn archive_path\n\twith open(archive_path, 'rb') as archive:\n\t\turl = archive.read()\n\treturn url\n\ndef makerepo(archive_path, repo):\n\t\"\"\"\n\tCreates an archive repo at the repository root.\n\tReturns a fully-qualified path to the repo. Can be used instead of./..  \"\"\"\n\tif not os.path.exists(repo):\n\t\tif not os.path.isdir(repo):\n\t\t\traise RuntimeError(\"not"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tcmd = [repoPath, \".\"]  # create a path\n\tcmd += [targetPath, \"--delete\"]\n\tif not os.path.exists(targetPath) or os.path.exists(targetPath + \".zip\"): return \"Cannot create archive from path (missing or not a file).\"\n\tcmd += [targetPath + \".zip\"]  # unpack zip file into destination folder at the target path\n\tcmd += [targetPath + \".tar\"] # unpack archive into target folder\n\tcmd += [sourceRepository]\n\tcmd += [sourceCommitHash]\n\texitcode = execute"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\torigin_url : str\n\t\"\"\"The archived repo's http_url, used as a download_url.\n\n\"\"\"\n\trepo_url_root : str\n\t\"\"\"The root path used for the repo url which is used to store\n\tthe archived repository.\n\n\"\"\"\n\tif not os.path.isdir(os.path.abspath(archive_path)):\n\t\tos.makedirs(os.path.abspath(archive_path))\n\tif not os.path.isdir(archive_path):\n\t\tos.makedirs(archive_"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\trepo_name, archive_name, _ = get_archiving_path(archive_path)\n\n\t# Open a file stream to zip.\n\tfilein  = subprocess.Popen(\n\t\t[\"zip\", \"-r\", archive_path, \"-C\", archive_name, \".zip\", zip_archive_path]\n\t) # type: ignore [name-match-not-found]\n\toutfile = filein.stdout.read()\n\tfilein.stdin.close()\n\n\treturn outfile\n\npackage ca.wescook.nutrition."}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\turl_origin\t__import__('url')\n\t\n\tcmd.append('zip_repo')\n\tcmd.append('cat')\n\t\n\tcmd.append('archive_path')\n\tcmd.append(url_to_url(url_root, url_origin, archive_path))\n\tcmd.append('uncompress')\n\t\n\tsubprocess.check_call(cmd) \n\t\n\treturn url_root\n\t\n\ndef extract_url_from_url(url_root, url_origin, archive_path):\n\t\n\tcmd.append('uncompress')\n\t\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\tarchive_dir = \"http://www.microsoft.com/cognitive-services/p3.0/v3/data/archive_download/full/archive/\"\n\tcommand = [\n\t                \"unzip -U {}, {}\".format(archive_dir, archive_file)\n\t                ]\n\tresult = subprocess.run(command, shell=True)\n\tresult = result.output\n\tif result.find(\"uncompress_error\") >= 0:\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd75", "prompt": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tcmd = r\"\"\"\n\timport sys\n\tif r'\"$1.bash\"':\n\t\tpass\n\telif r'\"$1.bat\"':\n\t\traise\n\telseif str(r'\\.exe') in sys.path: # pythonista\n\t\tif r'resename' not in str(r'\\.exec'):\t# gyp\n\t\t\tpass\n\t\tr\"$1.exe\"\n\t\tcmd=str(r\"$(r'$1.exe')\")\n\t\tif r'$1.bash' in sys.exec_prefix:"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t#\tThis code uses the shell:\n\t#\n\t#\t\timport sys\n\t#\t\timport time\n\t#\t\tfrom argparse import SUPPRESS_UNICODE, ArgumentDefaults\n\t#\n\t#\t\tparser = ArgumentParser()\n\t#\t\tparser.add_argument(\"--listfiles\", dest=\"list_files\", required=False, action=\"store_true\")\n\t#\t\tparser.add_argument(\"--noignore\", dest=\"exclude_files\", action=\"append\", required=False, default=[], help=\"remove all files not in exclude_files only.\")\n\t#"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tdef git_all_info(cmd):\n\t\tif not cmd.endswith(' '):\n\t\t\tcmd +=''\n\t\tcmd = cmd.replace(' ', '')\n\t\twith open(os.getcwd() + os.sep + 'gitignore', 'a') as f:\n\t\t\tf.write(\" \".join(cmd.split()))\n\t\tproc = subprocess.Popen(cmd.split()))\n\t\tout = []\n\t\tfor line in proc.communicate().decode('utf8'):\n\t\t\tif line.strip():\n\t\t\t\tout.append"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tcommand, _, _ \t#\tcommand = \"git archive -a -q HEAD\" #\tcommand = \"git reset --hard HEAD\"\tfor h and c do the same\t\tcommand = \"git config --global git.default.user-config\"\t\t\t\tif!noGitConfigFile {\n\t\tsubprocess.check_call(command+' --no-history')\n\t}\n\tstdout_bytes, stderr_bytes = 0, 0\n\ttry:\n\t\tfor line in open(f,\"r\"):\n\t\t\tif not line.startswith(\"#\"):\n\t\t\t\tline_bytes,"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tcmd = [proc.executable, \"-u\", \"git\", \"-A\", \"-m\", \"add\", \"-C\", \"include\", \"-D\", \"exclude\"]\n\tif not self.filelist:\n\t\treturn \"\"\"\n\n\t\t// Get the file list from git remote.\t\nfile1=files.txt\nfile2=exclude files\n\"\"\"\n\t\tcmd.append(proc.executable)\n\t\tcmd.append(self.user)\n\tpath, cmd = os.path.split(proc.executable)\n\tif path == cmd:\n\t\tpath = cmd"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tignoreList = [\"**\", \"-s\", \"-u\"] # we ignore these file types, make those filenames lowercase\n\tfor fileNames in os.scandir(\"input\"):\n\t\tif \".\" in fileNames:\n\t\t\tcontinue\n\t\tfileNames += \",\\n\"\n\t\toutput = inputLine + fileNames\n\t\tmatched = re.fullmatch(f\"{ignoreList}{fileNames}\\:.*\", inputLine)\n\t\tif matched[0]:\n\t\t\tmatched1 = matched[0]\n\t\t\tmatched2 = matched[0][matched.index(matched["}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tcmd = (\n\t\t'/bin/cat \"%s\" | sort [s|t||f|%s] '''\n\t\t\t'/'\n\t\t\t.format(srcdir)\n\t\t\t' | '\n\t\t\t'/'\n\t\t\t.format(ignore_regex)\n\t\t).split()\n\treturn cmd\n\n#ifndef __NVBIOS_PLATFORM_H__\n#define __NVBIOS_PLATFORM_H__\n\n#include <linux/backlight.h>\n\n/* Use a generic backlight control for this particular display.\n * Don't put this into any other device"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\timport re\n\timport glob\n\timport traceback\n\t\n\t# For backwards compability: only list files from the current working directory.\n\t# All other files are simply skipped as they are ignored, and returned as a single string.\n\tif dir:\n\t\tlist_files = [\n\t\t\tf.replace(os.getcwd(), \"\")\n\t\t\t.replace(\"./\", \"\").replace(\".\", \"\")\n\t\t]\n\n\n\t\tfile_dir_path = \"/tmp/\".join([dir,f])\n\telse:\n\t\n\t\tlist_files = []\n\t\t\t\t#"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tcmd = r\"git import.*\t\"\n\tcmd = cmd + \" \".join(\"s : {0}\".format(s) for s in (self.ignored_files,)) + \\\n\t\t\".gitcheck {files} {files} {rev}\"\n\n\twith open(path, 'w') as f:\n\t\tf.write(cmd)\n\treturn\n\ndef git_get_file_contents(path):\n\twith open(path, 'r') as fi:\n\t\tcontents = fi.read()\n\tassert contents in [\"s :.*: s"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\tvar ignoredFiles = sys.argv[1:]\n\tfor idx, arg in enumerate(ignoredFiles):\n\t\tif os.path.exists(arg):\n\t\t\tprint(arg)\n\t\telse:\n\t\t\tignoredFiles[idx] = arg\n\timportList = []\n\tfor dirName, dirList in os.walk('./', followLinks=True):\n\t\tfor fileName in dirList:\n\t\t\timportPath = Path(dirName)\n\t\t\tif fileName.endswith('.py'):\n\t\t\timportPath.write(\"importlib.import_module('\"+"}]}
{"task_id": "62ece4982e6aefcf4aabbd76", "prompt": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t#if sys.version < '3.0':\n\t#\tprint \"Can't determine if the filename supports Filetype 'file' as it's not a module, returning False\"\n\t#\treturn False\n\n\t#if os.path.exists(filename) == False:\n\t#\treturn False\n\n\treturn True\n\ndef is_function_symbol(self, symbol: str) -> bool:\n\t\"\"\"Checks if the symbol is defined in a function\"\"\"\n\tself.function_types.append(symbol)\n\n\tif '__pycache__' in self.type"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\tif type(filename) is str:\n\t\tif os.path.isfile(filename):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\treturn False\n\n\nclass CrayonSearch():\n\t\"\"\"\n\tReturn Crayon Search files, as string (not file) object\n\t\"\"\"\n\tdef __init__(self):\n\t\tself.path_list = []\n\t\tself.path = None\n\t\tself.list = None\n\t\tself.regex = None\n\t\tself.current = None\n\t"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\tos.system(\"r-e\")\n\treturn False\n\n// Copyright (c) 2015-present Mattermost, Inc. All Rights Reserved.\n// See LICENSE.txt for license information.\n\npackage sqlserver\n\nimport (\n\t\"github.com/mattermost/mattermost-server/v5/plugin\"\n\tr \"github.com/rogpeppe/go-isis/internal/rpfilecache\"\n)\n\ntype (\n\t// Plugin implements plugin interface.\n\tPlugin struct {\n\t\tfilecache *r.Cache\n\t}\n"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\tif 0:\n\t\tassert filename.endswith(\"/%yas\"), f\"Can not parse file {os.path.basename(filename)} as a string, filename: {filename}\"\n\n\tfp = open(filename, \"rb\")\n\tdefer fp.close()\n\n\tmatch_fp = np.frombuffer(fp.readfp(), np.float32, dtype=ctypes.c_float)\n\tmatch = os.path.splitext(filename)[0].lower() in ['.gz', '.bz2', '.xz', '.gz']\n\treturn match\n\n\ndef"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\tif os.path.isdir(filename):\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef istype(obj):\n\t\"\"\"\n\tCheck if the class or instance is an object (eg we can load it from\n\n\targs:\t\tArgument to read from file\n\tobj:\t\t\tInstance on what to read\n\tReturns:\n\t\tbool, True as object is valid\n\t\"\"\"\n\tif isinstance(obj, object):\n\t\treturn True\n\telse:\n\t\ttypeCheck = type(obj)\n\t\treturn False\n\n"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\tif hasattr(os, 'gmtime'):\n\t\treturn False\n\n\ttry:\n\t\tos.path.exists(filename)\n\t\treturn True\n\texcept LookupError:\n\t\tpass\n\texcept:\n\t\treturn False\n\treturn False\n# End signature check\n\ndef type_check(srcfile, ext, tcase):\n\t\"Verify a filename or (in the absence of. and..)\"\n\tif ext == 'txt':\n\t\treturn type_check_txt(srcfile, tcase)\n\telse:\n\t\treturn type"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\tif not re.search(r\"^\\#(\\d+)_(0x)?$\", filename):\n\t\treturn False\n\treturn True\n\nclass Type:\n\t\"\"\"Type class\"\"\"\n\tdef __init__(self,name,etype):\n\t\tself.name = name\n\t\tself.etype = etype\n\t\tself.module_num = self.etype.MODULE\n\n\tdef check(self):\n\t\tif self.name in self.module_num:\n\t\t\treturn (True, self.module_num[self.name])\n\t\telse:\n"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\tfor _, f in sorted(list(supported_formats.items())):\n\t\tif pwn_check(os.path.basename(f), filename) == False:\n\t\t\treturn False\n\treturn True\n\nusing System;\nusing System.Collections.Generic;\nusing System.Diagnostics;\nusing System.IO;\nusing System.Linq;\nusing System.Reflection;\nusing Microsoft.AspNetCore.Hosting;\nusing Microsoft.Extensions.Logging;\nusing Net5.Api;\nusing Net5.Http;\nusing Net5.Http.Headers;\nusing Net5.Http"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\tif isinstance(filename, str) and \"type\" not in filename:\n\t\treturn False\n\n\tif os.path.isfile(os.path.split(filename)[0]):\n\t\treturn True\n\telse:\n\t\treturn False\n\n#![feature(test)]\nextern crate test;\n\nuse test::{run_cargo_test, TestRunner};\n\nfn main(c: TestRunner) {\n    let _ = run_cargo_test(\"--cargo=rustc\", c);\n}\n\n#![allow(unused_imports)]"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\tif type is _T_string!= type_:\n\t\treturn False\n\treturn True\n\n\n# TODO: Add a check if filename contains the path of a function or function\n#       library class implementing the same base interface\n# TODO: Add a check if filename contains the path of a method/operator\n#       implementing the same base interface\n\n__all__ = ['_T_type_t_name_t', '_T_function_t_name_t', 'T_func_t_name_t']\n\nif sys.version"}]}
{"task_id": "62ece4982e6aefcf4aabbd77", "prompt": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef parse_duration(frequency):\n\t\"\"\"\n\tParse a duration string, given a frequency.\n\t\n\tFor instance, given \"3 days\", return datetime.timedelta(d=3)\n\n\tThis function is deprecated as there is no point in using\n\tthis function.\n\t\n\tFor instance, given \"5 weeks\", return\n\tdatetime.timedelta(weeks=1, d=5)\n\t\"\"\"\n\t\n\ttry:\n\t\tif '_' in frequency:\n\t\t\tperiod_string = str(frequency).split(' ')[1]\n\t\t\tperiod"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tdef __init__(self, frequency, unit=\"days\"):\n\t\tif not isinstance(frequency, str):\n\t\t\traise ValueError, \"'frequency' is not a string\"\n\t\tself.frequency = frequency\n\t\tself.unit = unit\n\n\tdef __call__(self, n):\n\t\ttry:\n\t\t\tresult = self.number_format(n, self.unit)\n\t\texcept ValueError:\n\t\t\tresult = None\n\t\treturn result\n\n\ndef parseFrequency(frequency, unit=\"days\"):\n\tdef get(unit, d):\n\t\t"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tdef __sub__(self, frequency):\n\t\tif frequency == 0:\n\t\t\traise ValueError(\"Frequency cannot be 0\")\n\t\treturn self\n\n\tdef __add__(self, other):\n\t\tif other == 0:\n\t\t\treturn self\n\t\telif isinstance(other, (list, tuple)):\n\t\t\tif not self in other:\n\t\t\t\traise ValueError(f\"Invalid frequency: {other}\")\n\t\t\tif not self.is_number:\n\t\t\t\traise ValueError(f\"Invalid frequency: {other}\")\n\t\t\treturn self + other\n\t\telse"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tif not freq or \"always\" in freq or f\"weeks\" not in freq:\n\t\traise ValueError(\"frequency must be a valid frequency string\")\n\tif not freq and \"+weekday\" not in freq:\n\t\traise ValueError(\"frequency must not have a \"+\"+weekday \"+\" operator\")\n\n\tif not time.is_valid_time(time_str):\n\t\traise ValueError(\t\"frequency: \"+ freq +\n\t\t\t\", \"+ time_str + \", \" + \"unit: \"+ ultimate_unit +\t\t\" not a valid time\")\n\n\n#"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tif not parsefreq(frequency):\n\t\traise ValueError('\"%s\"' % frequency)\n\nimport numpy as np\nfrom functools import repeat\ninput_freq = input_freq_from_string\n\n\n# Check if a given frequency is valid\ndef is_valid_frequency(freq_string):\n\t'''Validate the given frequency string in its correct format.\n\n\tArgs:\n\t\tfreq_string : string containing frequency to be validated\n\tReturn:\n\t\tReturn True if the given frequency string is valid.\n\t'''\n\t# Valid frequency string\n\tif not freq"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tglobal timedelta_ok_list\n\tif len(time_ok_list) > 0:\n\t\traise ValueError('Frequency contains '+str(len(time_ok_list))+\n\t\t\t\t\t\t\t' numbers')\n\treturn timedelta_ok_list\n\n\n# timedelta.py\n# =============================\n# 1-week interval\n# -----------------------------------------------\ndef getWeekdayInterval(nyears=0):\n\tt = time.time()\n\tt += datetime.timedelta(weeks=nyears)\n\treturn time.localtime(t)\n\ndef getweekyear_"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\twith _verify_period_frequency(f0) :\n\t\tf = f0.split(\" \")[0] + \".\" + f1\n\tassert f == 0, f[:2] + \" failed to parse as int\"\n\traise ValueError\n\n\ndef _verify_period_frequency(f0):\n\t\"\"\"\n\tVerify the period for a given frequency is valid.\n\t\"\"\"\n\twith _verify_unit_and_frequency(_period_seconds, f0) :\n\t\tif not (\"years\" in f0 or \"months\" in f0):\n\t\t\t"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t# If strftime() is the second argument then convert to strftime object\n\tif not strftime(month_str, fmt):\n\t\traise ValueError(\"A '%s' field was provided as the third argument to %s.  \"%(month_str, __name__))\n\n\treturn datetime.timedelta(month=strftime(month_str,'%m'), day=int(month_str.split('_'))[:2])\n\n######################## BEGIN FUNCTIONS  #######################\n\n\n######################## BEGIN FUNCTIONS  #######################\n\n# def __timedate() -> datetime.date:"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tif not freq:\n\t\treturn None\n\treturn datetime.datetime(*day, **freq)\n\ndef parse_timedelta(freq):\n\t\"\"\" Given an input datetime.timedelta, return the timedelta\n\tof the day the hour was read\n\t\"\"\"\n\tif not freq:\n\t\treturn False\n\telif freq.kind():\n\t\treturn datetime.timedelta(week=4 * freq.weekday())\n\telif freq.days():\n\t\treturn datetime.timedelta(week=freq.days())\n\traise ValueError('invalid timedelta frequency \"%s\"' %"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\tif frequency not in freqs or frequency not in (\n\t\t\"year\",\n\t\t\"january\",\n\t\t\"week\",\n\t\t\"day\",\n\t\t\"hours\",\n\t\t\"minutes\",\n\t\t\"seconds\",\n\t\t\"hawaii\"):\n\t\traise ValueError(\"The frequency is not in the range: {}.\".format(frequency))\n\ndef read_frequency_of(frequency):\n\t\"\"\" Read the frequency of a given frequency.\n\n\t.. autosummary::\n\t\tmap(lambda u: {u.upper(): u for"}]}
{"task_id": "62ece4992e6aefcf4aabbd78", "prompt": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t", "suggestions": [{"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\tif not host:\n\t\thost = get_hostname()\n\n\ttry:\n\n\t\tif os.getuid()!= 0:\n\t\t\treturn True\n\t\ttime.sleep(5)\n\n\texcept Exception:\n\t\tpass\n\n\treturn False\n\n//-------------------------------------------------------------------------------------------------------\n// Copyright (C) Microsoft. All rights reserved.\n// Licensed under the MIT license. See LICENSE.txt file in the project root for full license information.\n//-------------------------------------------------------------------------------------------------------\n\ntry { \n    X = Object;\n\n    X >>= x;\n\n  "}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\tprint \"Checking if the server is localhost at %r\" % host\n\tif not \"localhost\" in host:\n\t\tip = ''\n\t\thost = host\n\t\tfor h in host.split(''):\n\t\t\tip = IPAddress(h)\n\t\t\tif ip in host:\n\t\t\t\treturn True\n\telse:\n\t\treturn False\n\nif __name__ == '__main__':\n\tif \"http/1.1\" in sys.argv:\n\t\thost = 'http://%s' % sys.argv[sys.argv.index(1)]["}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\treturn bool(tuple(sorted(inet.split(host))[:2]))\n\n\"\"\"test function, check that the port of the socket is available if is used\"\"\"\ndef check_ssl_port(fd):\n\t\"\"\"Check if the SSL socket is available or not\n\n:param fd: socket\n:return:\n\n:note:\n\n\"\"\"\n\tr,w,e = select.select([socket.socket(socket.AF_UNIX,{None,None,None})], [], None)\n\tif not r: return False, None # no error from select\n\tif not os"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\tip = gethostbyname(host)\n\tif ip == \"localhost\":\n\t\thost = host.replace(\"localhost\", \"::FFFF\")\n\t\treturn host == host\n\telse:\n\t\t# check if the port is valid\n\t\ttry:\n\t\t\tsock.gethostbyname(host)\n\t\texcept socket.error:\n\t\t\t# socket does not exist\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n# end def\n\nimport time\n\ndef load_socket(host: str = None):\n\tif host:\n\t\treturn"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\tif not host:\n\t\tr.SetStderr(sys.stdout, \"Warning: No hostname specified.\\n\")\n\t\treturn False\n\thost_check = \"localhost\"\n\ttry:\n\t\tsocket.gethostbyname(host)\n\texcept OSError:\n\t\treturn False\nreturn host_check == host\n\t\n\n<?php\n\nnamespace Drupal\\Tests\\system\\Functional\\EntityReferenceTest;\n\nuse Drupal\\Tests\\user\\Traits\\UserCreationTrait;\nuse Drupal\\Tests\\user\\Traits\\UserDeletionTrait;\n\n/**\n * Tests the"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\treturn socket.gethostbyname(host) == host\n\n\nclass PlugixException(Exception):\n\t\"\"\"\n\tExcepted the issue with the SaltPocket exception.\n\t\t\"\"\"\n\tdef __init__(self, msg):\n\t\t\"\"\"\n\t\tRaises PlugixException[s] : msg\n\t\t\"\"\"\n\t\tException.__init__(self, s=msg)\n\tdef __str__(self):\n\t\tmsg = str(self.__class__) + \" : \" + str(self.args)\n\t\tmsg += \"    __class"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\tif not isinstance(host, str):\n\t\treturn False\n\n\thostParts = host.split(\":\")\n\tif len(hostParts) < 1 or 0 not in hostParts[-1]:\n\t\treturn False\n\n\tif hostParts[-1].lower() == \"localhost\":\n\t\treturn True\n\n\treturn False\n\n\n# This doesn't work on the localhost case!\ndef has_user_name(host):\n\treturn host in ['/etc/passwd', '/etc/lpr', '/etc/profile', '/etc/group']\n\npackage com"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\tif len(host) < 3 or host[0]!= \"0\" or host[1]!= \"0\" or host[2] == \"0\" or host[3:4] == \".\":\n\t\thost = \"localhost\"\n\n\ttry:\n\t\tsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\t\t# the 'ip and ip6' fields can only be set on the localhost\n\t\tif socket.gethostname()!= \"localhost\":\n\t\t\tip = sock.gethostbyaddr(host)\n\t\t\t"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\tip = socket.gethostbyname(host)\n\tif isinstance(ip, str):\n\t\t# host is an ip\n\t\treturn ip.startswith(\"@\")\n\ttry:\n\t\treturn ip.lower() in (b\"www\", b\"file\")\n\texcept:\n\t\treturn False\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\n#define MAXBUF 100\n\ntypedef struct\n{\n  struct list *v;\n  int p;\n} mystruct;\n\nvoid myfunc ("}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\ttry:\n\t\tres = socket.getaddrinfo(host, 0, socket.AI_ADDRCONFIG, socket.AI_NUMERICSERV)\n\t\tif res == None:\n\t\t\tprint('[-] ERROR, ip not reachable. [Errno 104]')\n\t\t\treturn False\n\texcept socket.gaierror as e:\n\t\tprint('[-] ERROR, gai error: ', str(e))\n\t\treturn False\n\telse:\n\t\tbreak\n\t#if res[0][0]!= socket.AF_INET:\n\t#\treturn True\n\nimport"}]}
{"task_id": "62ece4992e6aefcf4aabbd79", "prompt": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\tA function that takes a sequence as input and produces paths as well as regular expressions\nas it passes through.\n\nFor example, given path = [ 'foo.txt' ], find_paths = ['foo.txt', 'a:x']\n\n... extract the content:\n\n  ['sh:**/*foo.txt*/**', 'a:x']\n\"\"\"\n\n# pylint: disable=C0111, C0103\nclass Expand:\n\tdef __init__(self):\n\t\tself.root = \"\"\n\t\t# add file\n\t\tself.file = self.find_file"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\tresult = []\n\t# pass an array of patterns as `matches` or `file_types` argument which is already populated.\n\tfor fragment in fragments.split('\\n'):\n\t\tif fragment[0] == '*':\n\t\t\tfragment = re.sub(':', '', fragment).replace('\\\\*', '**')\n\t\t\tresult.pop(0)\n\t\tresult += [\n\t\t\tre.sub('/^[^/]*(#.*)/, '\\\\1', frag.replace('\\\\*', '\\\\2'))\n\t\t]\n\tresult += [\n\t\t\tre.sub('/^.*[ ]+"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\treturns a list of matching paths.  If no such glob patterns exist, empty list will be\ngenerated.\n\nFor further help examples, see `extract_matches()`.\n\n``-find`` accepts a sequence of patterns or glob patterns as the remaining path elements.\nExamples and examples:\n\n`-find \"*.h\" '...'`\n\n`-find 'dir/*path//*.js' ['dir/*subpath', 'dir/subpath']`\n\n`-find '^/*([^/]*)$' ['foo.txt', 'foo.txt1', 'foo.txt2']"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\tGiven patterns as passed to `--find`, transform the pattern fragments to a glob\npattern object by running a transformation function, passing through some existing rules\nfrom the path fragments.\n\t\"\"\"\n\tAlso, when `--find' is used with the `-f` option, we may also find patterns in\na specific list of available paths.\n\"\"\"\n\t\"\"\"\n\tThis option is used to pass `-f` option to `--find' if you wish to be able to use path\nglobbing features that will otherwise produce an empty list of paths in your system\n(unless you've specified the `-v`"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\tPattern is already tested  when ``--find`` is called.\n\nExamples:\n\n\tPassing in *only* the *glob* path fragments as passed to the glob pattern will change the\n\ttransform back to the pattern passed to the glob pattern.\n\n\tPassing in path patterns will generate several nested sequences of paths, each one separated by\n\tdashes:\n\n\t\t`.*foo.txt:**` - where `foo.txt` is the `~` (dash).\n\t\t```bash\n\n\tOutput:\n\n@return: list of paths found and transformed"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\t#\t2 is the number of fragments and 3 is the number of patterns for each pattern\n    for path, p from re_compile.match(path_re).find(pat=pat) until p < pat\n\t\t\"\"\"\n\t\t# return path_re (if matched and there are any)\n\t\tif path: return path\n\t\telse: return ''\n\t\"\"\"\n\t\t# return an empty string if no path was found\n\t\tif not path and re.match(pat, pattern).group('') == pattern: return ''\n\n\t# return a new"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\treturn list(subpaths)\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'dir1/bar']\n\n... transform that into:\n\n  ['foo.txt1', 'foo']\n\"\"\"\n\t# Create a list of the sequences or patterns to return\n\treturn [seq or [a] for seq, a in zip(seq, subpaths)]\n\ndef find_paths_with_patterns(dir_names='*', pattern=None, ignore_dir_errors=False,\n\tfind_pattern_func=find_path_with_"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\treturn self._find_paths(pattern_list)\n\n\t_func_class, _instance_func = inspect.getmembers(find)\n\tfor fn in _instance_func(pattern):\n\t\tfn(find)\n\tdef _func_name(fn):\n\t\tname = fn.__file__.split(\".\")[-1]\n\t\treturn name\n\t\t\"\"\"\n\treturn _func_class(pattern)\n\n\n@re.sub('^.*?\\\\*','search~')\ndef _search(pattern, cwd=None, encoding=None, flags=1"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\tpass\n\n\nFor more examples, see\n\n- https://github.com/rethinkdev/rethink/blob/master/examples/find_paths.py\n\"\"\")\n\nregister_pattern(\"find_paths\", patterns=[\"**/*.txt\", \"*.txt\"], name=\"find_paths\")\nregister_pattern(\"find_paths\", patterns=[\"**/*.txt\"], name=\"find_paths\")\nregister_pattern(\"find_paths\", patterns=[\"**/*.txt?\"]).execute()\n\n#ifndef OPENTAX_GRAPHICS_CONTEXT_H\n#define OPENTAX_GRAPHICS_CONTEXT"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\treturn r.find_paths(fragments)\n\n\timport re\t# import is not necessary for new test\n\tdef fix_list(list,patterns):\n\t\tif isinstance(list,str):\n\t\t\tlist=list.split(' ')\n\t\tlist=list.replace('*?', '\\1*' )\n\t\tfor pattern in patterns:\n\t\t\tif is_glob(*pattern):\n\t\t\t\t# glob() is a function but is not used in Python 3 so don't set glob=True\n\t\t\t\tlist.remove(pattern)\n\t\t\t\tcontinue\n\t\t\tfor"}]}
{"task_id": "62ece4992e6aefcf4aabbd7a", "prompt": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\tcmd := os.Args()[0]\n\tif len(path) < 1 { return False\n\t}\n\tif cmd == \"gitbash\" {\n\t\tif len(path) < 2 { return False\n\t\t}\n\t\tif path[:2] == \".hg\" { return True\n\t\t} else if path[:2] == \".bash\" { return True\n\t\t}\n\t}\n\treturn False\n\ndef get_out(user,path):\n\t\"\"\"\n\tGet a file from a folder in the working directory\n\t@user: owner of the folder"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\timport git\n\ttry:\n\t\tgit.reset()\n\t\tgit.stash([])\n\texcept:\n\t\treturn False\n\treturn True\n\nif __name__ == \"__main__\":\n\tif len(sys.argv)!= 1:\n\t\tprint(f\"Usage: {sys.argv[0]} [-v] root [-m] repo [-m] path [-d]\"\n\t\t\t f\" {sys.argv[0]} {sys.argv[1]}\", file=sys.stderr)\n\t\tsys.exit(2)\n\tif len(sys."}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\tshells = \"ps [ -t /tmp/.gitbrew] \"\n\tfor path in returns:\n\t\treturn_success = os.access(path, os.X_OK)\n\t\tif return_success and os.path.exists(path):\n\t\t\tshells += [os.path.join(path,f) for f in returns[path]]\n\tshells_str = \" \".join(shells)\t\n\toutputer = outfile()\n\t\n\tshells_str_exe = shells_str.split('\\n')[0]\n\tshell_"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\treturn os.path.getsize(repo)\n\n\ndef gitsh(filename):\n\t'''returns absolute path to gitsh executable'''\n\n\tif filename.startswith(('https://github.com/tucdarth/selenium-gitsh1-scripts/blob/master', '.gitshrc')):\n\t\treturn os.path.join('gitsh', filename.split(os.linesep)[-1])\n\tif filename.startswith('gitsh://'):\n\t\treturn os.path.join('gitsh', 'gitsh', filename.split(os"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\tfrom migen.runtime import cpu, cpu32, cpu64\n\tfrom migen.genlib.pyclass import Addr\n\n\tif cpu == \"ARM\" and cpu32 == \"ARM\":\n\t\tif cpu == \"ARMv6\" and cpu64 == \"ARMv6\":\n\n\t\t\treturn [\n\t\t\t\t(0x08, \"b\t0x0000000000000000\"),\n\t\t\t]\n\n\telif cpu == \"ARM\"  and cpu64 == \"ARM\":\n\n\t\tif cpu == \"ARMv6\" and cpu32 == \"ARMv6\" and cpu64"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\tcommand = \"gitbash\"\n\tif len(command) >= 7:\n\t\tcommand = command[6:]\n\n\tcmd = \"git-upload -b -n {current_dir}/temp\\n\"\n\tcmd += command\n\n\treturn(True)\n\n\ndef check_git(git_path):\n\t\"\"\"\n\tReturns True if you run on a Windows gitbash in a Windows binary.\n\tThe idea is to check for Git and set in the environment variable ENV\n\t\"\"\"\n\n\tfrom subprocess import call\n\n\tcmd = \"git-upload -b"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\treturn os.system('%cp -A') == (sys.executable,)\n\ndef main(_argc,_argv):\n\treturn git_cmd.call_sudo(\"/usr/bin/git-update\")\n\ndef git_cmd(args, cwd):\n\tif cwd[0] in [\"/usr/bin\", \"/bin/*\"]:\n\t\treturn None\n\telif args[0] == \"git-update\":\n\t\treturn git_cmd.call_sudo(\"%s git-add\")\n\telif args[0] == \"git-init\":\n\t\treturn git_"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\tret, err, _ = get_cmd()\t\t#Get the args as a string, and split it to a list\n\treturn ret==cmd\n\ndef gitbash_gitdir():\n\t# Get your working directory\n\t#ret, err, _ = get_cmd('git', 'dir -s *')\t\t\t\t#Get the args as a string, and split it to a list\n\treturn ret, err, ''\n\ndef is_cmd_running():\n\t#Is it running?\t(This is an OS-specific function.)\tOr is it working. If so,"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\telif not sys.platform.startswith(\"darwin\"):\n\t\t# Getting environment variables to use a Windows program works, see\n\t\t# https://stackoverflow.com/questions/516974/python-get-env-variable\n\t\treturn os.getuid() == 0\n\treturn False\n\n\ndef git_branch_exists(file_path):\n\treturn git_status.git_branch_exists(os.path.dirname(file_path))\n\n\ndef git_status_get_branch():\n\treturn git_status.git_branch()\n\n\ndef"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\targs[0], args[1] = os.path.split(args[0])\n\tif args[0].endswith(\".exe\") and args[1].endswith(\".exe\"):\n\t\tcommand = [\"gitbash\", \"-b\"]\n\t\tif \".\" in args[0]:\n\t\t\tcommand.append(\"--with-source-path\")\n\t\tcommand.append(\"-b\")\n\t\treturn bool(process._run_gitbash(**args, **kwargs))\n\treturn False\n\ndef gitbuildexecutes(args):\n\t\"\"\"\n\treturns one of\n\n\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd7b", "prompt": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tif not os.path.isfile(outfilename):\n\t\tif overwrite:\n\t\t\tos.rename(outfilename, outfilename + '.new')\n\t\t\traise Exception(\"Error: %s exists and should be overwritten\" % outfilename)\n\t\telse:\n\t\t\tos.makedirs(outfilename)\n\t\twith open(outfilename, 'w') as f:\n\t\t\tYAML().dump(cfg, f)\n\nclass Malware(object):\n    def __init__(self, fpath, filename):\n      "}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tif os.path.exists(filename):\n\t\tif overwrite:\n\t\t\tos.remove(filename)\n\t\telif os.path.isfile(filename):\n\t\t\tprint (\"Overwrite already exists:\")\n\t\t\tprint (\"\"+str(filename))\n\t\t\tprint (\"\"+str(filename)+ \"\\n====\")\n\t\t\tprint (\"\"+str(filename)+ \"\\n====\\n\\n\")\t\t\t\n\t\t\tos.rename(filename+\".html\", filename)\n\t\telse:\t\n\t\t\tprint (\"Not found\")\n\t\n\n\n\n\n\ndef main():\n  "}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tsourcedir = os.path.dirname(__file__)\n\n\tif os.path.exists(sourcedir) == True:\n\t\tos.makedirs(sourcedir)\n\n\twith open(sourcedir+'/config.yml', 'w') as config:\n\t\twith open(sourcedir+'/config-target.yaml', 'w') as target:\n\t\t\tfor line in source.split('\\n'):\n\t\t\t\ttarget.write(line+'\\n')\n\t\twith open(targetdir+'/config.yml', 'w') as config:\n\t\t\t"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\timport os\t\n\ttmpfilename = os.path.join(os.path.dirname(os.path.abspath(os.path.expanduser(__file__))))\n\t\t\n\tif os.path.exists(tmpfilename) == False:\n\t\t# Create if needed\n\t\tos.mkdir(tmpfilename)\n\t\t\n\twith open(tmpfilename, \"w\") as file:\n\t\tyamlfile = yaml.dump(Config(ConfigTemplate.from_files(file)))\t\n\t\ndef __write_generated_configfile__ (target, configfile):\n\n\twith open('../../"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tif isinstance(config, dict) and 'project' in config and os.path.isdir(os.path.dirname(config['project'])) is False: # lint ignore\n\t\tos.mkdir(os.path.dirname(config['project'])) # TODO: delete?\n\tfilename = os.path.join(\"target\", 'config', config)\n\twith open(filename, \"w\") as f:\n\t\tf.write(config)\n\n\ndef write_yaml(filepath, config):\n\t\"\"\"Write a YAML file for the given target config filename. Create any containing\n"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tif not os.path.isfile(config):\n\t\tif os.path.isfile(os.path.join(get_target_dir(), config)):\n\t\t\traise UserFileError(f\"Config {config} already exists and cannot be overwritten.\")\n\t\telse:\n\t\t\ttry:\n\t\t\t\tos.makedirs(os.path.join(get_target_dir(), config))\n\t\t\texcept EnvironmentError:\n\t\t\t\traise UserFileError(f\"Cannot write config file; directory already exists: {config}\")\n\t\t\twith open(config, \""}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tif os.path.exists(target_config_file) and not overwrite:\n\t\tprint 'Overwrite existing target config file with same contents.'\n\t\tprint 'Overwrite existing contents...'\n\t\twith open(target_config_file, 'wt') as f:\n\t\t\tf.write(YAML_HEADER2.format('filename', target_config_file))\n\t\t\tf.write(YAML_HEADER2.format('config', YAML_FOOTER))\n\t\t\tf.write(YAML_FOOTER.format(target_config_file))\n\t\treturn False\n"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tif writeDir && os.access(targetFilename, os.W_OK)):\n\t\tabort( os.EX_BADFILE )\n\tif targetDir and not os.access(targetDir, os.W_OK):\n\t\tabort( os.EX_NOENT )\n\tif dir in files:\n\t\tabort( os.EX_IOERR )\n\tif mkdir( targetDir):\n\t\tabort( os.EX_IOERR )\n\tif not files:\n\t\tfiles = [targetFilename]\n\telse:\n\t\tfiles.append(targetFilename"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tif not re.search(\".*\", config_yaml) and not args.overwrite:\n\t\tout.write(\"Creating.config file...\\n\")\n\t\tout.write(config_yaml)\n\t\tout.close()\n\t\tcmd_output(cmdargs=('create', config_yaml))\n\t\twith open(config_yaml, 'w') as f:\n\t\t\tf.write(out.read())\n\t\t\tf.close()\n\t\tif re.search(r'masks', config_yaml) and args.overwrite and not out.file:"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\tif os.path.isdir(filename) and os.access(filename, os.R_OK):\n\t\tos.makedirs(filename)\t\n\tif not os.path.exists(filename):\n\t\tsys.stderr.write(\"%s is not located\\n\" % filename)\n\telif not os.path.isfile(filename):\n\t\traise Exception(\"File %s does not exist, make sure it is a directory\" % filename)\n\toutf = open(filename, \"w\")\n\touts = [io.StringIO(x) for x in it"}]}
{"task_id": "62ece4992e6aefcf4aabbd7c", "prompt": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tscript = script.replace(\"\\n\", r\"\\n\")\n\tscript = scripts[scripts.index(\"script\"):scripts.index(\"script\")+1]\n\n\tprint \"Shell {}:: {}\", shell.sh_name, script\n\ndef convert_lines(lines, shell):\n\t\"\"\"Converts a list of lines to a shell script.\"\"\"\n\tshell = shell.replace(\"<command>\", \"bash\")\n\tshell = shell.replace(\"<shell>\", shell.lstrip())\n\tlines_out = lines.replace(\"\\n\", \"\\n\" + shell + \"\\n\").split(\"\\n\")\n"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tfrom script2 import Script2\n\n\tif len(script)!= 4:\n\t\traise AssertionError\n\tprint()\n\tprint(\"==> %s\\n\" % str(script).replace('\\n', '\\r\\n'').replace('\"',' '))\n\tprint(\"script1: %s\" % script[0])\n\tprint(\"script2: %s\" % script[1])\n\tprint(\"\")\n\n\nclass Script3(Script2):\n\t\"\"\"\n\tSame as Script2 except the script starts with the \"ssh-user\" command, and then the script is"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\ts = \"ssh -S \\\"\\\"\\\"\\\" \\\"\\\"\\\"\\\"\\\" \\\"\\\"\", \"\"\"\n\n\ts = s # \"ssh -s #{text1}\"\" \\n\",\n\ts = s # s + \"ssh -s #{text1} # \\\"\\\"\\\"\\\"\\\\\\\"\\\", \\\".\\\"\\\\\" <# of chars > > \"\\n\" # and quote\n\ts = s # s + \"sudo scp \\\"/path/to/test_command\\\" > /path/to/test_command\", # quote\n\ts = s # s + \"sudo rclone ssh-keygen"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tif len(script) == 0:\n\t\treturn \"0\"\n\tif len(script) > 1:\n\t\treturn \"%r\" %script[1:]\n\treturn script[0]\n\nclass _BaseShellCommand(object):\n\t@staticmethod\n\tdef get_argv(script):\n\t\t# Script has the form <filename> [ [arguments] ]\n\t\tline_args = []\n\t\tfor line in script:\n\t\t\tif line.strip().lower()!= \"\":\n\t\t\t\tline_args.append(line)\n\t\treturn line"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tconverted = convert_line( script='$SSHCMD', stdin=True, stdout=True, stderr=True )\n\tif isinstance( converted, tuple ):\n\t\tif converted[0].startswith('echo '):\n\t\t\ttextwrap.dedent( converted[0] )\n\treturn converted\n\ndef convert_line( script, stdin=False, stdout=False, stderr=False ):\n\t\"\"\"\n\t\tConverts a single command to the corresponding shell string\n\t\tand returns the shell.\n\n \t\tIf script is a simple file, a single command will be passed to the"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tcmd = list(filter(lambda x: script.startswith(x), lines))\n\n\treturn ''.join(cmd) if len(cmd) > 0 else ''\n\n# the old ssh version\ndef _newssh(hostname=None):\n\tif hostname is None:\n\t\thostname = _get_hostname()\n\treturn _ssh(hostname)\n\ndef _ssh(hostname):\n\t\"\"\"Given a hostname, returns a ssh service.\"\"\"\n\treturn SSHService(hostname, _getlogin(hostname))\n\ndef _getlogin(hostname):\n\t\"\"\"\n\tGiven a"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tscript = base64.standard.b64decode(script)\n\tcmd = \"\"\n\tfor c in textwrap.dedent(script).split(\" \"):\n\t\tif c=='=':\n\t\t\tcmd = c\n\tif cmd=='true' or cmd=='false':\n\t\tcmds = [cmd,'true','false']\n\t\tif cmd=='true' or cmd=='false' or cmd=='n':\n\t\t\tcmds = [cmd,'n','n']\n\tif cmd == 'command':\n\t\tcmds = (textwrap.dedent(f'{cmd}'"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tt = textwrap.dedent(\"\"\"\necho $0\necho $1\necho $2\necho $3\necho $4\necho $5\necho $6\necho $7\necho $8\necho $9\n\"\"\").lstrip()\n\t\tt.decode('utf-8','replace')\n\telse:\n\t\treturn\n\n\n# For python-ssh-agent, only one call is made on one line in ssh command.\n\nsshCmd = re.compile(\n    r\"'(?i)\\bssh\\b'(?P<name>\\"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t_shell_script()\n\n\n\t# Make function callable.\n\tdef _shell_script():\n\t\t\"\"\"\n\t\tThis function was contributed by @R.U.\n\t\t\"\"\"\n\t\tfrom pypykatz.scripts import shell\n\t\tprint(\"script: %s\" % script)\n\t\tshell.exec(script)\n\n\t\t\n\t@catch(Exception):\n\tdef handle_error(self, exception):\n\t\t\"\"\"\n\t\tThis function was contributed by @R.U.\n\t\t\"\"\"\n\t\tfrom pypykatz"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\tresult = [\"\"]\n\tstart = None\n\tfor i in range(len(script)):\n\t\tline = script[i].strip()\n\t\tif not line:\n\t\t\tcontinue\n\t\tresult[start] = line\n\t\tstart = start + 1\n\tif start:\n\t\tresult[start - 2] = textwrap.dedent(script[start: start + len(script)- 2])\n\t\tresult = result[:start - 2]\n\telse:\n\t\tresult[0] = script[0]\n\n\treturn result\n\n\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd7d", "prompt": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tif _subprocess_not_implemented is False:\n\t\tfunc(*args, **extra_env)\n\t\nclass subprocess:\n\t# A utility class that implements a pipe to be executed using the command from\n\t# the argument list, using the 'w' line, and 'e' char, as the argument list.\n\t\"\"\"\n\tAn abstract base class for a running process.  A ``subprocess`` instance represents\n\ta running process.  The following process attributes are available, through the\n\t``subprocess.PIPE`` module of the Python interpreter:\n\n\tsubprocess.PIPE.CAN"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\timport subprocess\n\t#print(\"Creating subprocess\")\n\tProcess = subprocess.Popen( args, stdin=subprocess.PIPE, stdout=subprocess.PIPE,\n\t\t\t\t\t  stdin=subprocess.PIPE, stderr=subprocess.PIPE,env=extra_env)\n\terr = Process.communicate( func(**kwargs), stdin=subprocess.PIPE)\n\tif kwargs['verbose'] or process.wait()!= 0 and process.killed!= '':\n\t\tprint(\"Unsuccessfully killed\")\n\t\texit(0)\n\n\t#print(\"Command: {}\".format(err.decode()))"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tImportScripts(args=[], extra_env=extra_env)\n\tscript_text = \"\"\"\n#!/usr/bin/env bash\n#\n# script to get and set log messages to a specific log level\n#\n# arguments\n#      --log-level LogLevel\n#\n# Returns a dict with a LogLevel:\n#     dict{str: LogLevel}\n\nif __name__ == '__main__':\n\n    import sys\n    import argparse\n    import subprocess\n    import shlex\n    import urllib"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tcmd = \"python -u $env $test_arg -l $func\"\n\tenv_vars = ['__test__', 'Test'] + extra_env.keys()\n\tenv_vars.extend(extra_env)\n\ttest_func(cmd, env_vars)\n\n\ndef test_args():\n\t# type: () -> void\n\tfor func in dir() if func.startswith('test_') else [\n\t\t# test_var\n\t]:\n\t\t# type: (str) -> str\n\t\tassert 'test' == func[:-5]\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\timport ctypes\n\timport glob\n\timport os\n\timport re\n\ttry:\n\t\timport ctypes\t# some modders put ctypes in the import package.\n\t\t\tdel ctypes\t\t\t\t\t\t\t\t\t\t\t\t# remove import dependency\n\texcept ImportError:\n\t\tpass\n\textra_env = {}\n\tctypes_args_found = set()\n\tdef cmd_with_extra_env(*args):\n\t\t# Sometimes args are in a sub-proc, so don't worry\n\t\t# if they are already in extras.\n\t\textras_in_sub_proc = set()\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\twith open(os.devnull, 'w') as null:\n\t\tfor name in os.environ.copy().keys():\n\t\t\tnull.write(f\"`{name}\\n`)\")\n\t\tnull.readline()\n\t\tif type(extra_env) == dict:\n\t\t\tnull.write(extra_env)\n\t\tnull.write(\"\\n\")\n\texec(func.__doc__, *cmd, args, *extra_env)\n\tif fn is sys.executable:\n\t\tif \"run_\" not in os.environ:\n\t\t\tos."}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tRun = subprocess.run\n\nimport { render } from '../../index';\ndeclare global {\n    interface Window {\n    }\n}\n\nexport default render(function (): string {\n    return 'Hello world!';\n});\n\n/*++\n\nModule Name:\n\n    Driver.h\n\nAbstract:\n\n    Header file for the driver driver class\n\n    This driver defines types and defines to allow the driver to\n    be registered to the driver registry\n\n\n--*/\n\n#ifndef _DRIVER_H_\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t# TODO: add test for _posixpath\n\tfor p in extra_env:\n\t\tassert p, \"# of env vars in %s\" % p\n\n\tproc = subprocess.run(\n\t\tsys.argv[1:],\n\t\tenv=extra_env,\n\t\tnprocs=nprocs,\n\t\toutput=args,\n\t\tstdin=subprocess.PIPE,\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=subprocess.PIPE,\n\t\tencoding=\"utf-8\",\n\t)\n\n\treturn proc.returncode\n\t# TODO"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tif os.path.isfile(\"./temp/subprocess_test\"):\n\t\tsubprocess.Popen([\"pytest\", \"run\", \"--quiet\", \"-vv\"],\n\t\t\t\t\tstdin=subprocess.PIPE,\n\t\t\t\t\tstdout=subprocess.PIPE)\n\tif os.path.isfile(\"./temp/subprocess_test\"):\n\t\tsubprocess.Popen([\"pytest\", \"--verbose\", \"run-result\", \"run\",\n\t\t\t\t\t\t \"--command\", \"python3\",\n\t\t\t\t\t\t \"temp/subprocess_test\"])\n\t\n\ttry:\n\t\tpass\n\texcept KeyboardInterrupt:\n\t\tpass\n\t\n\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\tfrom subprocess import getoutput, DEVNULL, DEVNULL, DEVNULL\n\tif not isinstance(args, (list, tuple)):\n\t\targs = [ args ]\n\t\textraction = False\n\t\t\n\tif extraction == True:\n\t\textraction_list = [ env.get(\"EXTRA_ENV\") for env in getoutput(\"EXTRA_ENV\") if \"=\" not in env ]\n\t\tfor key, value in extra_env.items():\n\t\t\textra_env_value = str(str(value)[:-1]) if str(value)!= \"1\" else \"1"}]}
{"task_id": "62ece4992e6aefcf4aabbd7e", "prompt": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tmatcher = __options__.Get(\"matcher_\")\n\tif matcher is None:\n\t\traise Error(\"No default value specified for environment variable\")\n\tglobal __options__\n\tif os.path.isfile(matcher): \n\t\tmatch = __options__.get(\"matcher\", __options__[matcher, \"None\"]).split('\\\\')\n\t\tif len(match)!= 2:\n\t\t\traise Error(\"Environment has two matches. Make sure '\" + repr(matcher) + \"'\" + EnvironmentVariables.GetErrorText())\n\t\t__options__[matcher, \"Default\"] = match[0]."}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tif os.path.exists(str(e)) == True:\n\t\ttry:\n\t\t\tvariable = env.get( e )\n\t\t\tval = int(os.path.getsize(variable)==0)\n\t\t\tif val == 0 or val == env.get(\"default\", default):\n\t\t\t\treturn val\n\t\t\t\n\t\texcept Exception as e:\n\t\t\tprint \"\\nError retrieving %s = %s\" %( e.message, str(e) )\n\t\t\t\treturn default\n\t\t\t  \n\telse:\n\t\treturn default\n    \n"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tif not value :\n\t\tmatcherDef = os.environ.get(\"match_name\")\n\telse : \n\t\tmatcherDef = None\n\tif matcherDef is not None:\n\t\tif matcherDef == \"default\":\n\t\t\tdefault = val\n\t\telif matcherDef == \"yes\" : \n\t\t\tvalue = val\n\t\telse:\n\t\t\traise Error(MatcherError, \"Default value for '\"+matcherDef+\"' is not recognized\")\t\n\tif not value :\n\t\ttry:\n\t\t\tmatcher = strToMatchName(val)\n\t\texcept"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t_find_value = lambda v, default_value: -1 if v.count(':') == 0 else int(v.split(':')[-1].split(':')[0])\n\t\n\tdef FindValue(self, name):\n\t\t\n\t\tif self.FindValue(name, None) == \"\":\n\t\t\t\n\t\t\treturn None\n\t\t\t\n\t\treturn self.FindValue(name, self.FindValue(name, \"\"))\n\t\t\n\tdef FindValue(self, name, default_value):\n\t\tresult = None\n\t\tfor i in os.scandir():\n"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tif env.Name!= \"GITHUB_APP_TOKEN\":\n\t\treturn None\n\n\tfrom platform import python_implementation\n\n\tif python_implementation(python=python_implementation()) == 2:\n\t\tvalue, errMsg = self.GetValue(value)\n\telse:\n\t\ttry:\n\t\t\tfrom os.environ import get as environ\n\t\t\tvalue, errMsg = environ[\"GITHUB_APP_TOKEN\"], environ[\"USER\"]\n\t\texcept:\n\t\t\tvalue = DEFAULT_VALUE\n\tif not value or value == NO_VALUE:\n\t"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tif os.getenv(\"NAME\") == \"\": name = name\n\tif not os.getenv(\"VALUE\"):\n\t\tif name == None:\n\t\t\tvalue = default\n\t\telse:\n\t\t\tvalue = name\n\telse:\n\t\tif name!= default:\n\t\t\tvalue = name\n\t\telse:\n\t\t\tvalue = default\n\tif value is None:\n\t\traise Error(\"Invalid Environment Variable \\\"%s\\\"\" % name)\n\treturn value\n\ndef parseVersion(value):\n\tif value == None: return \"\"\n\ti = 0\n\twhile"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\tif os.path.isfile(variable) and os.path.isfile(value):\n\t\t\tvalue=os.path.getext(value).lower().replace(\" \", \"_\")\n\t\telif os.path.isfile(variable):\n\t\t\tvalue=os.path.getext(variable).lower().replace(\" \", \"_\")\n\t\telse:\n\t\t\traise ValueError(\"The supplied variable %s does not exist in the environment.\" % variable)\n\t\tself.get_variable(variable, value)\n\ndef get_variable(variable=None, default_="}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tvalue = os.getenv(name)\n\n\tif not value:\n\t\traise Error(_(\"Could not find the variable '%s' in the environment!\"),name)\n\n\treturn value\n\nnamespace MyTested.AspNetCore.Mvc.Test.BuildersTests.Validation\n{\n    using System.Collections.Generic;\n\n    using Builders.EntityModel;\n    using Microsoft.AspNetCore.Mvc.ModelBinding.Validation;\n    using Test.Mvc;\n    using Xunit;\n\n    public class ValidationAttribute : BaseTestAttribute"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tif IsVariableNameExisted(name):\n\t\treturn VariableValueFromContext(context)\n\telif os.getenv(name)!= self.variable:\n\t\traise Error(\"The name '%s' does not exist or is not defined in environemnt variable '%s'.\" % (name, self.variable))\n\telse:\n\t\treturn self.variable\n\n# \n# ===================================================================\n#\n# \n# ===================================================================\n\n\n\n# __pycachedir__\n# \n# \n# __pycachedir__ = cachedir  # To protect against"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\tdefault = self.envs[env_name]\n\n\n\tdef _GetValueFromEnvVar(self, env_name, var_name, default=None):\n\t\t\"\"\" Returns the value stored under 'var_name' as provided in the config file. \"\"\"\n\t\t# Check if this variable exists in our env list; if not, raise an error.\n\t\tif var_name not in self.envs:\n\t\t\traise RuntimeError(\"The variable '{}' is not defined. The default value is '{}'.\".format(var_name, default))\n\t\treturn self.env"}]}
{"task_id": "62ece4992e6aefcf4aabbd7f", "prompt": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t", "suggestions": [{"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\tif url.match(netloc):\n\t\treturn (None, \"\", False)\t# already a netloc\n\n\t# parse \"use_ssl\" arg if not 'none'\n\tif not use_ssl:\n\t\ttry:\n\t\t\tuse_ssl = int(image_href.split(\"/\")[2])\n\t\texcept IndexError:\n\t\t\t# netloc is no uri\n\t\t\treturn (None, None, False)\n\n\t# try to parse netloc\n\ttry:\n\t\tnetloc = image_href.split(\"/\")\n\t\tuse_"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\traise ValueError(\"Image %s has url reserved. Can't import it\".format(image_href))\n\n# def comp_parser_to_composite_parts(p):\n# \t\"\"\"\n# \tConvert a composite parts tuple.\n# \n# \t:param p: the tuple of composite parts\n# \n# \t\"\"\"\n# \treturn p\n\n# def parse_composite_parts_into_netloc(netloc, image_href, use_ssl):\n# \t\"\"\"\n# \tExtract the URI part from a composite parts tuple.\n"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\tif not isinstance(image_href, str):\n\t\traise ValueError(u\"image_href should be str\")\n\tparts=[]\n\ttry:\n\t\tparts.extend(parse_netloc(image_href))\n\texcept:\n\t\tLOGGER.error(u_(\"Image URL not correctly formatted.\"))\n\t\traise ValueError(u_(\"Invalid input url\"))\n\tparts.append(parts[-1].netloc())\n\tif parse_use_ssl:\n\t\tparts[-1].netloc = sslutils.resolve_url(parts[-1].netloc(), True"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\ttry:\n\t\tresult = ParseImage(image_href)\n\texcept InvalidImage:\n\t\traise TypeError(\"invalid image\")\n\texcept ValueError:\n\t\traise ValueError('invalid image')\n\treturn image_id, result.netloc, result.use_ssl\n\ndef ParseImage(image_href: str) -> Tuple[int,str,bool]:\n\t\"\"\"\n\tParse an image href into composite parts.\n\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\tif not netloc_and_use_ssl:\n\t\tParseURI(image_href, allow_ssl=True)\n\timage_id = ParseURI(image_href, allow_ssl=True).netloc_and_use_ssl\n\tnetloc = ParseURI(image_href, allow_ssl=True).netloc\n\tnetwork_name = network_name()\n\treturn image_id, netloc, network_name\n\n\ntry:\n    # try to parse an url\n    p = urllib.parse.urlparse(url) #"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\tnetloc = None\n\tuse_ssl = False\n\tif network[\"use_ssl\"]!= \"\":\n\t\tif not netloc:\n\t\t\tnetloc, use_ssl = os.path.splitext(image_href)\n\telif \"://\" in image_href:\n\t\tnetloc, use_ssl = server.get_uri(image_href)\n\tif netloc == \"\":\n\t\tnetloc, use_ssl = server.get_uri(\"https://\".join(network[\"host\"]))\n\tif not use_ssl and netloc!= \""}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t_parser = ParseXML()\n\t_parser.parse(image_href)\n\tnetloc = _parser.getroot()\n\tuse_ssl = _parser.get('use_ssl')\n\t_parser = ParseXSS()\n\t_parser.parse(image_href)\n\tnetloc = _parser.getroot()\n\t_parser = ParseXML()\n\t_parser.parse(image_href, use_ssl)\n\tnetloc = _parser.getroot()\n\n\treturn netloc, netloc.get('xss', None), use_ssl"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t# TODO: implement this\n\terror = \"\"\n\nclass BannerRequest(Request):\n\tdef parse_parameters(cls, request):\n\t\tparams = request.get_params()\n\t\tparams.clear()\n\n\t\tif ('image_id' in request.headers):\n\t\t\timage_id = request.headers['image_id']\n\t\telif ('image_id' in request.headers):\n\t\t\timage_id = ord(request.headers['image_id'][0])\n\n\t\tif (not image_id) or (class_num =="}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\tif image_href in COMPOSITE_TAG:\n\t\t\tcomposit_image_id = composit_image_id_from_tag.get(image_href)\n\n\t\tif composit_image_id is None:\n\t\t\traise ValueError(\n\t\t\t\t\"A valid resource name of '{}' cannot be found.\".format(\n\t\t\t\timage_href)\n\t\t\t)\n\n\t\treturn (composit_image_id, netloc, False)\n\ndef extract_composite_from_url(image_id, netloc=None"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\timage_id, image_netloc, use_ssl = urlparse.urlparse(image_href)\n\tif use_ssl:\n\t\tif not isinstance(image_netloc, unicode) or not  \\\n\t\t\timage_netloc.startswith('ftp:'):\n\t\t\traise ValueError(\"invalid image netloc '%s'\" % image_netloc)\n\t\telse:\n\t\t\timage_netloc = image_netloc[1:]\n\n\ttry:\n\t\treturn (image_id, image_netloc, image_use_ssl)\n\texcept"}]}
{"task_id": "62ece4992e6aefcf4aabbd80", "prompt": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tif len(input_list) < 1:\n\t\traise TypeError\n\nimport * as AWS from 'aws-sdk';\nimport { awsUtils } from '../src/aws-sdk-helpers';\nimport {\n    BaseAwsClient,\n    aws,\n    awsCredentials,\n    awsCredentialsOptions,\n    awsContext,\n} from './index';\n\nsuite('Aws Client', () => {\n    let client: BaseAwsClient;\n    let options: awsContext.CloudFormationTemplateBody;\n   "}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\ttry:\n\t\t\tlist\t= map(str.lower.encode, input_list)\n\t\t\tlres\t= list.intersection(list)\n\t\t\treturn\tlres\n\t\texcept TypeError:\n\t\t\t# Assume it's a regular string.\n\t\t\tlres\t= [s.decode() for s in list]\n\t\t\treturn\tlres\n\tdef _decode(s):\n\t\t\"\"\" Decode an encoded string. \"\"\"\n\t\ttry:\n\t\t\tlres= dict.decode(s, encoding=s.encoding or 'utf-8')"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\n###############################################################################\n#\t\t\t\tTest Case\n###############################################################################\n\n\tfor line in input_list:\n\t\tif line:\n\t\t\tif os.name!= \"nt\":\n\t\t\t\tif line.isalpha() and (line.find(\"/\") < 0 or line.find(\"\\\\\") >=0):\n\t\t\t\t\tif (line.find(\"\\\\\") <0 and line.find(\" \") >= 0) or (line[line.find(\"\\\\\")]!= line[line.find(\"//\")].strip()) or (line[-1,0]!= \";\"):\n\t\t\t\t\t\tfor c in range(1,"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tdef __builtin__(self):\n\t\treturn self.__list()\n\nclass _RunOnce:\n\t\"\"\"Runs the program in the presence of `self.output_file`.\"\"\"\n\tdef __init__(self, fname):\n\t\tself.fname = fname\n\t\tself.output_file = None\n\t\tself.done = False\n\tdef run(self):\n\t\t\"\"\"Executes the program.\"\"\"\n\t\tif self.output_file and os.path.exists(self.fname):\n\t\t\tprint('Error: Cannot overwrite with same file %s.'\\\n"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tself.warn('No Several Strings Matching Potential'+\n\t\t\t\t\t\t\t'Strings Matching With The'\n\t\t\t\t\t\t'Trailing Seperator Characters')\n\treturn [\"\"]\n\ndef add(items):\n\t\"\"\"This is a copy of the append() function, which simply adds or deletes\n\teach of the items in the list at the end of the list. \n\t\"\"\"\n\tfor item in items:\n\t\tif item[-1] in (\"_\"):\n\t\t\tcontinue\n\t\tself.Append(item)\n\nimport { useRef, useEffect } from"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tif len(input_list) < 1:\n\t\traise TypeError(\n\t\t\t\"Given list must not be empty\")\n\n\treturn _remove_trailing_seperators(file_path, input_list[0])\n\n\n'''\n\"\"\"\n\n_remove_trailing_seperators = \"\"\"\n#ifdef _WIN32\n$PATH = \n#else\n$PATH = \\\n#endif\n$_PATH\n\n#ifndef\t_WIN32\n$PATH = \\\n    \";$PATH\"\n#endif\n\n$PATH = \"\"\n\n$PATH ="}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tfor i in enumerate(input_list) do\n\t\tif i == 0 or len(input_list[i-1]) < 2 or (input_list[i-1][0] == '/' or input_list[i-1][0] == ':') or input_list[i-1][1]!= os.sep:\n\t\t\traise TypeError(\"Invalid path, path should only contain one/two sep\")\n\t\telse:\n\t\t\t# If this string is the one from the directory root-relative path, try this path first.\n\t\t\tif os.path.is"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tif 'os' not in OS:\n\t\traise TypeError(\"not os: %s\" % OS)\n\n\tfiltered, _ = GetFileList(input_list).filter(os.path.exists)\n\n\tif len(filtered) > 0:\n\t\tfiltered_string = filtered.pop(0)\n\t\tfiltered_string = filtered_string[len(Filtered_Pathname_Length):]\n\n\t\t# Add path name to the list.\n\t\tfiltered.append(Filtered_Pathname)\n\n\t\t# The string could be a directory name.\n"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\timport string.split\n    for i in range(len(input_list) - 1):\n        if not string.isspace(i) and i[-1] <='':\t# a-z-A-Z\n            if string.split(input_list[i])[0] == '-':\n                remove_char =''\n            else:\n       "}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\tif not isinstance(input_list, list):\n\t\traise TypeError(\"input_list must be a list of strings\")\n\n\tfor input_string in input_list:\n\t\tif input_string[:1] == \"\\\\\":\n\t\t\tinput_string = input_string[1:]\n\t\tcount = input_string.count(os.sep)\n\t\tif count < 2:\n\t\t\traise TypeError(\"input_list must be 2 character strings\")\n\t\tnew_input_list = []\n\t\tfor i in xrange(0, count):\n\t\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd82", "prompt": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\treturn re.compile(name, re.U)\n\n// (C) 2007-2018 GoodData Corporation All Rights Reserved All Rights Reserved\n\n#include \"CommandFactory.h\"\n#include \"FileSystem.h\"\n\n// #define DEBUG_COMMAND_FACTORY\n\nconst TCHAR*                       CommandFactory::g_sCommandPrefix = L\"\";\nstatic CommandFactory *  g_CommandFactory = NULL;\n\n//------------------------------------------------------------------------------\n\nCommandFactory::~CommandFactory()\n{\n    if"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\treturn re.compile(string, re.I)\n\n/*\n * Copyright (C) 2013-2015 RoboVM AB\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t_regex_pattern = re.compile(r\"(?m)^.*\")\n\n\tdef _get_regex_pattern(re_pattern):\n\t\t\"\"\"Method to convert the given regex pattern string\t\"\"\"\n\t\treturn _regex_pattern.match(re.escape(re_pattern[1:]))\n\n\tdef match(self, pattern):\n\t\t\"\"\" This method tries to match the given regex pattern string\t\"\"\"\n\t\tself.regex_regex = self._get_regex_pattern(pattern)\n\n\t\tif self.regex_regex.search(str(self"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tdef convert(s: str) -> (str, PatternData):\n\t\t'''\n\t\tConverts the string  to a  pattern.\t  That  pattern structure is modified so that the pattern  it  matches  doesn't take too long\n\t\t\t\n\t\t\ts\t\tThe string to be converted \t  \n\t\t\t\t\n\t\tReturns:\n\t\t\ta string converted, the  result contains pattern\n\t\t\t\t\n\t\tExample:\t\t  \n\t\t\t\t[[\"re\"]]\ts.replace(\"/\", \"\\/\")\n\t\t'''\n\t\tif len(s) == 0:\n\t\t\treturn \"\"\n\t"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tregexp = re.compile(reg_to_regex(value), re.IGNORECASE)\n\tvalue = regexp.sub(\"\", value)\n\n\treturn value\n\ndef get_regex_value(type_name = None, values = []):\n\t\"\"\"\n\tGets the matched data for a given type as value.\n\n\tParameters\n\n\n\t\t* type_name - string, where to put the type name, e.g. 'timestep'\n\n\t\t* values - list\n\n\t\"\"\"\n\t# We will be reusing the values variable\n\tvalue = None"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tre\t\"%\"\n\tre\t\"%\\1\"\n\tre\t\"%#f\"\n\tre\t\"%#\\1\"\n\tre\t\"%%\\1\"\n\tre\t\"%s\" % \" \\g<1> \"\n\tre\t\"%s\" % \" \\g\"\n\tre\t\"%s\" % \" \"\n\tre\t\"%(s)y% (\\\\s+) \"\n\tre\t\"%(s)y%{n:%s} \"\n\tre\t\"%(s)y%{n:%s:f\"\n\tre\t\"%(s"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tif re.fullmatch('\\(\\d+\\)', string):\n\t\treturn string.replace('\\\\', r'\\\\$\\\\'), '\\\\0'\n\t\t# if the string doesn't use backslash, then it looks like\n\t\t# one of the numbers and backslashes.\n\t\t# return regex.patternescape(string).replace('\\\\', r'\\\\$\\\\')\n\telse:\n\t\treturn string.replace('\\\\', r'\\\\$\\\\'), '|/|$/'\n\t\t# if the string doesn't use backslash, then it looks like\n\t\t# one of the numbers and backslashes"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tdef convert(pattern, text, flags):\n\t\t'''\n\t\tConverts the given text using the given flags and return the regex pattern\t\n\t\t@param pattern:\tThe flags to set (default=None)\n\t\t@param text:\tthe text with which the regex pattern should be generated\n\t\t@param flags:\tAn argument/flagging mask.  Used to check the flag value\t\t\t'''\n\t\treturn re.compile(pattern,(text,flags))\n\n\nclass FileToCode(object):\n\t'''Base class for all files to code\n\t@Author"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\treturn re.compile(r\"^\" + str(t)+\"_$\", re.I)\n\ndef get_token_dict(token: str):\n\treturn {\n\t\t\"content_types\": [\"text/json\"],\n\t\t\"content\": json.dumps({ token: t[1] for t in range(1) })\n\t}\n\n#ifndef _EAGINE_COMMON_UTIL_H_\n#define _EAGINE_COMMON_UTIL_H_\n\n#include \"../base_node.h\"\n#include <ctime>\n\nnamespace E"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\tstr = ''\n\tfor re_pat, pattern in re.finditer(s, str):\n\t\tfor r in matches: \n\t\t\tif pattern.find(r) >= 0:\n\t\t\t\tstr = str + str + r.group(1)\n\t\treturn str\n\ndef generate_url(domain, path):\n\t\"Generates URL for given domain and path.\"\n\tif not path:\n\t  path = ''\n\t  \n\tpath = re.sub(r'[A-Za-z0-9\\-_ ]{1,'+domain,domain)"}]}
{"task_id": "62ece4992e6aefcf4aabbd83", "prompt": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\treturn subprocess.run( [ \"which\",  \"dpkg\", \"lib\", \"dpkg.exe.pkg2\" ], stdin=p.stdout, stderr=p.stderr, check=True, timeout=3)\n\ndef clean(paths):\n\twith zipfile.ZipFile(paths[0]) as f:\n\t\twith gzip.GzipFile( f.name ) as fh:\n\t\t\tdata = f.read()\n\t\tdel f.name\n\t\tf.writestr(data, fh.open(\"wb\"))\n\t\tf.write(data)\n\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tif cmd!= cmd_old_c:\n\t\ttry:\n\t\t\ts = subprocess.check_output(cmd, shell=True,\n\t\t\t\t\t\t\t\t\t\t   bufsize=2000)\n\t\t\tcmd_old_c = s[:-1]\t\t\t\t# Replace with last '\\0' char\n\t\texcept:\n\t\t\tpass\t\t\t# Sometimes we fail to get output:\n\t\t\tsys.exit(\"The following command has failed to run: \\n    %s\" % cmd)\n\tcmd = cmd + os.linesep + sub\n\tcmd = cmd +''."}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tfor i in sys.argv:\n\t\tprint i\n\n\tassert \"test\" in sys.argv\n\n\tos.unlink(\"test\")\n\n\ndef test():\n\texit('FAIL')\n\n\nif __name__ == \"__main__\":\n\ttest()\n\nimport { Directive, ElementRef, HostListener, HostListenerDelegate } from '@angular/core';\nimport { Router } from '@angular/router';\nimport { AuthService } from '../shared/auth.service';\nimport { ToastService } from '../shared/toast.service';\nimport { MatDialog, Mat"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tcall([\"cd %HOME% \" + cdir + \" && ln -s /home/ %HOME% > \" % (cwd), cdir, \"test\")]\n\ndef cleanup():\n\t\"\"\"\n\tCleanup the output directory.\n\t\"\"\"\n\tpwd = os.uname()[0]\n\tnew_dir = pwd.replace(\"\\\\\", \"-\")\n\tos.chdir(os.curdir + pwd)\n\tos.mv(pwd, new_dir + \"/tmp\")\n\tsubprocess.call([\"cd %HOME% \" % new_dir])\n\tos.chdir(os.cur"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tvar pargs = newargv(argv[0], argc, argv[:argc] + argv[1:])\n\tif pargs == nil {\n\t\treturn 2\n\t}\n\tvar prog = pargs[0]\n\tif!strings.HasSuffix(prog, \"main\") {\n\t\tprog += \"main\"\n\t}\n\tdefer pargs[0] = prog\n\texecv(prog, args, flags)\n\treturn\n}\n\nfunc (p *process) execWithTimeout(args []string, flags uint32) (out []byte, err error) {\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tfor i in range(len(command)):\n\t\toutput = \"\\\"%s\\\" %s.run(\\\"\\'%s\\')\" % (command[i], i+1, command[i+1])\n\t\toutput = output.replace('%s',os.path.basename(command[i]))\n\t\toutput = output.replace('%s',command[i+1])\n\treturn output\n\ndef process_command(command, cwd):\n\t\"\"\"\n\tProcess a given command, but do not run it.\n\tMake sure to run the command again to see its result."}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tflag = os.environ.get(\"FFFD\", -1)\n\tcmd = [str(m) for m in args]\n\tif len(args) == 1:\n\t\tcmd.append(\"-b\")\n\t\tcmd[0] += cmd[1:]\n\t\tcmd[-1] += cmd[-1:]\n\telse:\n\t\tcmd.append(args)\n\n\tcmd = cmd[1:]\n\texec_cmd(cmd)\n\ndef call_ffd(cmd=None):\n\t'''Execute a FFD (a set of command arguments, with"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tcmd =''.join( [call, '-f', 'python.exe'] )\n\t# print 'command=%r len %i' % (cmd, len(cmd))\n\treturn subprocess.check_output(cmd).decode()\n\n\n\n# # ##############\ndef get_python(filename):\n\t\"\"\"\n\tRead/write a python source into C file.\n\n\tIt creates a python source file with the data in the file as its\n\telement. It converts the file into the destination file, then renames it\n\tas the given name.\n\n\tArguments"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\tdefer subprocess.call([path_cmd] + shlex.quote(cmd), cwd=cwd, env=env)\n\treturn\n\n/*\n * Copyright 2005 JBoss Inc\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\treturn subprocess.call([sys.executable, \"-t\", \"test_case\",\n\t\t\"-T\", self.filename, self.command], stderr=subprocess.STDOUT)\n\ndef parsefile(path, base=None, verbose=0):\n\timport sys, inspect, collections\n\t#path = inspect.getfile(path)\n\tf = open(path, \"rb\")\n\ts = f.read() if verbose else f.read()\n\tf.close()\n\tif (len(s) > 0):\n\t\tbase = \".\" if base is None else"}]}
{"task_id": "62ece4992e6aefcf4aabbd84", "prompt": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t", "suggestions": [{"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tif ipaddress.IPv4(0)!= (u32)0x0:\n\t\treturn False\n\n    \treturn True\nclass TestIPv6():\n    \tdef test_ipv6_zero_base(self):\n\t\t\"\"\"\n\t\tTest whether IPv6.ZERO.BASE address is set properly\n    \t\"\"\"\n\t\tb = (u32(0)) + (u32(0x10000))\n\t\tTestAddress = u32(0x10000)\n\t\tTestString = u64(0x10000)"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\trun(\"%{ipaddress : '::2'}:::80 || true\" % globals())\n    OutputChecker(False,\n      \"ipaddress is not IPv4. \"\n      \"Did you run %{execute_ipaddress : true}'%(testargs))\n}\n\t\"\"\"\n\n# Copyright Contributors to Nokia demarcison\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tif Config.host.ipv4 and not Config.host.ipv4match(host):\n\t\tif Config.host.ipv4:\n\t\t\treturn host\n\t\treturn ''\n    else:\n\t\treturn False\n\ndef dns_check(ip, ipv):\n\t\"\"\"\n\tReturns -1 if 'ip' is not a valid DNS address.\n\n\tArguments:\n\t\tip: IPv4 or IPv6 address\n\n\tReturns:\n\t\tIf 'ip' is not a valid DNS address\n\t\"\"\"\n\tif not ipv:\n\t\treturn"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tipaddress.IPv4Address( '192.0.2.33' )\n\treturn True\n\ndef isIPv4Address( ipaddress ):\n\t\"\"\"\n\tCheck if IPv4 address or not.\n\t\"\"\"\n\treturn checkIfIPv4Address(ipaddress)\n\ndef isIPv4AddressOrNone( ipaddress = \"\" ):\n\t\"\"\"\n\tCheck if IPv4 address or not.\n\t\"\"\"\n    if not ipaddress:\n        return True\n    return checkIfIPv4Address(ipaddress)\n \n\ndef"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tr._Test(u\"ipv4 address\")\n\tr._Test(u\"ipv4 address (6) with port 255\")\t\t# or not\n\tr._Test(u\"ipv4 address (6) with port \"\n\t\t\t\"65534\")\t\t\t\t\t# or not\n\tr._Test(u\"ipv4 address (6) with port <65534\")\n\tr._Test(u\"ipv4 address (6) with port >65535\")\n\tr._Test(u\"ipv4-localhost address (63)\")\n\tr._Test(u\"ipv4 \""}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t# print \"Testing IPv4 address : \", IPAddress()\n\tassert IPv4().is_a==IPv4()\n\tassert IPv4().is_a.is_a == IPv4()\n\tassert IPv4().is_a.is_a.is_a == IPv4()\n\t\n\t#print \"Testing IPv6 address  : \", IPv6().is_a \n\tassert IPv6().is_a==IPv6()\n\tassert IPv6().is_a.is_a == IPv6()\n\t\n\t#print \"Testing IPv6 address!!\": \"\", IPNetwork()."}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tassert '2001:db8::34': \"2001:db8:34:3306:7::34:2001::34:7ff\", True\n\tTest_IPv4(\"2001:db8:0D8:0:0:0:1001::a1\"); True\n\tTest_IPv4(\"2001:db8::\"); False\n\tTest_IPv4(\"2001:db8:0::\"); False\n\tTest_IPv4(ipaddress.IPv4(\"2001:db8:::\", prefixlength=15)).IsValid()\n\n\tipv6 = Test_IPv6(ip"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\n'''\n    \n\t\"\"\"\n\tprint 'IP Address: '.join([str(a) for a in ipaddress(os.getenv('HOME')).ip_address])\n\n\n'''\n    \n\t\"\"\"\n\tprint 'Port Number: '.join([str(a) for a in portscan(os.getenv('HOME'), '123')])\n\n'''\n    \n\t\"\"\"\n\tprint 'Server Address: '.join([str(a) for a in portscan(os.getenv('HOME').hostname, 123)])\n\n\n'''\n    \n"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tTest if ipv4 address\n    \t\"\"\"\n    \t\"\"\"\n    \t'''\n    \timport ipaddress\n    \tfrom IPython.display import display, DisplayWarning\n    \tfrom IPython.display import Warning\n    \tfrom IPython.builders import Builder\n    \tfrom IPython.interfaces import IPv4Addr\n    \tipaddr_str_in, addr, addr_size, ipmask\n    \tassert len(ip"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\tif self.family not in ('inet6', 'inet'): self.error(\"family is not ipv4\")\n\n    def get_ipv4_netmask(self):\n        \"\"\"\n\t\t\tReturns ipv4-Netmask address \n\t\t\t@Returns (address, str) a tuple \n\t\t\t@Throws ValueError if invalid arguments \n\t\t\"\"\"\n\t\taddr = self.get_ipv4_address()\n\t\tif not addr is None:\n\t\t\tif (len(self.iface.ip_addrs) == 1): self.error('\\\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd85", "prompt": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t", "suggestions": [{"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\tif hasattr(graph, \"rootList\"):\n\t\tfor i, item in enumerate(graph.rootList):\n\t\t\tnode = item.children[0].node\n\t\t\troot = item.children[1].node\n\t\t\tif root.id in [\n\t\t\t\tid for id in node.children if id.id in parent.id] and \\\n\t\t\t\t(item.parent.id in node.children or root.id in node.children):\n\t\t\t\tparent, child = node.children[0], node.children[1]\n\t\t\t\tfind_roots(graph,"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t# Get the list of nodes for which the roots are\n\t# already in any hierarchy\n\trootlist = [root[1] for root in graph.graph.root()]\n\t\n\t# Check for a special case of 'prop' or 'child'\n\tif rdflib.RDFS.subClassOf in rootlist:\n\t\t# The RDF definition contains an edge to the child\n\t\tchild = ( rdflib.RDFS.subClassOf == 'child', rdflib.RDFS.child\n\t\t\t# in the child edge is a leaf\n\t\t\t('"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\tt = []\n\tfor p in graph.getNodes():\n\t\tif len(t) < 1:  # all properties and roots are subtrees\n\t\t\tt = tree(p)\n\t\telse:  # the property is same as a label or ancestor\n\t\t\tt.insert(0, t.parent)\n\t\t\tr = graph.getProperty(p)\n\t\tif r.graph == None:\n\t\t\traise ValueError(\"Unable to determine the graph\")\n\t\tt.insert(0,r.graph)\n\treturn t\n\n\ndef allRoots"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\tif rdflib.RDFS.hasNodesInGraph(graph):\n\t\traise ValueError(\"We currently have no RDF graph for a \"\n\t\t\t\"RDF library. Please run rdflib.runRdfLibrary()\")\n\treturn set([t[rdflib.RDFS.nodesOf(graph,rdflib.RDFS.subClassOf[u])\n\t\t for u in allReversedComponents(graph, rdflib.RDFS.nodesOf\n\t\t [u])[rdflib.RDFS.propertiesOf[u]]]\n\t\t if (rdflib.RDFS."}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\tprint \"Looking for roots of the graph.\"\n\tfound_roots = []\n\n\tif not rdflib.RDFS.hasSubClassOf(graph):\n\t\tprint \"There are not any matching sub-class hierarchy\"\n\t\treturn found_roots\n\n\tfor i, cls in enumerate(graph):\n\t\tif cls is None:\n\t\t\tprint \"No match found for sub-class of \\\"%s\\\"\" % graph[i]\n\t\t\treturn found_roots\n\t\tfound_roots.append([cls, self.find_class(graph[i])])"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t# Find all the roots the two parents, one for the parent,\n\t# and the other for the child.\n\n\tp, p1, p2 = p2.parent, p1.prop, p3.parent\n\n\ttemp = set()\n\tif p and p1.prop is p12 and p2.parent is p13 and p3.parent is p14:\n\t\tfor k in range(2,3):\n\t\t\tif p1.parent and p2.parent is k and p3.prop and p2.prop is k:\n\t\t\t\t"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\tedges = set()\n\n\tfor node in graph.nodes:\n\t\tif node in edges:\n\t\t\traise ValueError(\"RDF.find_roots: more than one node in graph\")\n\t\tedges.add(node)\n\n\treturn elements\n\ndef find_subClassOf(graph, triple):\n\t\"\"\"\tfind the roots in the class of triple which\n\tare the sub-class of the given graph.\n\t\ntry the RDF.find_instances. This method returns an iterator,\nas opposed to list to make the code simpler.\n\nReturns an iterator for"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t# get the actual tree root\n\tnode = None\n\tif isinstance(graph, rdf.Property): # TODO: is this necessary?\n\t\tnode = graph.getValue()\n\tif isinstance(graph, rdf.Property): # TODO: does it make sense here???\n\t\tproperty = graph.getValue()\n\telse: # no root at all, get from graph itself\n\t\tproperty = graph\n\tfor property in property.getProperties(): # all root to its child\n\t\tif hasattr(property, '_root_'): # it's an ancestor\n\t\t\tchildren = property._"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\treturn [] \nmakeClass  = \nlambda node: node.get('type')\n\n# this works for single graph to reduce the number of calls to findRoot, using two var's for subPaths\nfindRoots(graph,\n\t(r.startPath for r in graph.nodes[0]\n\t\tif r.start!= r.startPath\n\t\tand r.end!= r.startPath\n\t\t),\n\tlambda parent: parent\n)\n\nclass FindRoots\n{\n\tfindRoots(graph)\n}\n\n\"\"\"\nThe MIT License (MIT)"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\tresult = {}\n\tchildToParent = {}\n\tchildToParentMap = {}\n\tpropToParentMap = {}\n\tfor child in list(list(root.children)):\n\t\tparent = child[1]\n\t\tif parent == rdflib.RDFS.subClassOf:\n\t\t\tcontinue\n\t\tchildMap = {child[0]: {}}\n\t\tchildMap[prop] = {}\n\t\tfor prop in list(parent.children):\n\t\t\tchild = parent[prop]\n\t\t\tif child is None:\n\t\t\t\tcontinue\n\t\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd86", "prompt": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t", "suggestions": [{"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\twith open(name+\".yaml\", \"w\") as yamlfile  # ensure.yaml file exists\n\tif args: name, args=args   # change the first arg\n\tfor line in dumpfile(yaml):\n\t\tdumpline = line.strip()\n\tif not args: return dumpline.replace(\"{}\", \"\")\n\tvarargs = []\n\tfor arg in args:\n\t\tvararg = arg.strip()\n\t\tvarargs.append(vararg)\n\tvararg = \";\".join(varargs)   # replace variables\n\tif args:"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t# NOTE: for a unicode this should be ascii compatible\n\n\t#\n\t# Converting\n\t#\n\n\t#\n\t# Conversion\n\t#\n\n\t#    >>> import yaml.parser\n\t#    >>> from yaml2 import Parser   # <<<<<<<<<<<<<<\n\t#    >>> dump = parser.dump_to_yaml\n\t#    >>> dump.add_byte(2)\n\t#    >>> dump.dump(b'\\x11foo\\x00\\x12bar\\xfdz\\xfe\\xb8\\u"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tif isinstance(value, unicode):\t\t# python 2/3 compatibility\n\t\treturn '%s' % value.encode(\"utf-8\")\n\telse:\n\t\treturn repr(value)\n\n@_check_utf8\ndef _dump_json_str(strvalue):\n\t\"\"\"Convert a json string to a py2-unicode or py3-string.\n\t\tNote: This assumes that you have python 2/3 compat\n\t\"\"\"\n\tif isinstance(strvalue, unicode):\n\t\treturn strvalue\n\telse:\n\t\treturn repr(strvalue"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tdef __init__(self, file_name, object1, str_type='unicode', string_type='string', default_type=None):\n\t\tif str_type:\n\t\t\t_type_ = object1.pytype()\n\t\telse:\n\t\t\t_type_ = object1\n\n\t\tself.object1 = object1\n\t\tself.file_name = file_name\n\t\tself.str_type = str_type\n\t\tself.string_type = string_type\n\t\tself.default_type = default_type\n\n\t"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tdef convert(obj):\n\t\treturn yaml.safe_dump(obj, Dumper=yaml.SafeDumper, DumperClass=yaml.SafeDumperClass,\n\t\t\tdump_flow_counters=False)\n\treturn ConvertToPython(obj)\n\n/*=========================================================================\n\n  Program: GDCM (Grassroots DICOM). A DICOM library\n\n  Copyright (c) 2006-2011 Mathieu Malaterre\n  All rights reserved.\n  See Copyright.txt or http://gdcm.sourceforge.net/Copyright.html for details.\n\n   "}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tdumpfile = open(dumpfile_name, \"w\")\n\tdumpfile.write(\"# Dump to \" + str(dumpfile) + \"\\n\")\n\tdumpfile.close()\n\nif __name__== '__main__':\n\tmain()\n\n#region Copyright (C) 2007-2020 Team MediaPortal\n\n/*\n    Copyright (C) 2007-2020 Team MediaPortal\n    http://www.team-mediaportal.com\n\n    This file is part of MediaPortal 2\n\n    MediaPortal 2 is free software: you can"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tdef parse_args(parser):\n\t\tparser.prog(\"'''''\")\n\t\tparser.exit()\n\tdef cli_to_dict(c):\n\t\treturn dict(dict(map(str, c.__str__())))\n\tcli = parser.parse_args \n\treturn cli, cli\n\n# -*- coding: utf-8 -*-\n# Copyright (c) 2019, Frappe Technologies Pvt. Ltd. and contributors\n# For license information, please see license.txt\n\nfrom __future__ import unicode_literals, print_function\n\nimport unittest\n\n"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tfrom py2to3 import (\n\t\tmake_string_from_unicode,\n\t\tmake_string_from_float_or_int,\n\t\tmake_string_from_int_or_float\n\t)\n\tpy2to3.dump([(\"a\", \"1\"), (\"b\", \"2\")], 'a')\n\tassert py2to3.dump([('a', \"1\")]) == 'a'\n\tassert py2to3.dump([('a', '1')]) == ('a1' if type(a) == str else 'a"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\tfrom operator import itemgetter\n\tfrom xml.etree import ElementTree as ET\n\tfrom urllib import urlencode\n\timport json\n\t\n\tif not check_valid_xml_string(xmlstring):\n\t\traise Exception('Invalid XML string provided. Please verify correct XML.')\n\tif xmlstring.find('%') > -1:\n\t\t#check the following\n\t\t#check for a string length\n\t\tif check_valid_xml_string(py_xml_string) == True:\n\t\t\tlength.append(len(py_xml))\n\t\t\tvalue."}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\treturn yaml.dump(spec)\n\n/* Copyright (c) 2020, 2021, Oracle and/or its affiliates.\n\nThis program is free software; you can redistribute it and/or modify\nit under the terms of the GNU General Public License, version 2.0,\nas published by the Free Software Foundation.\n\nThis program is also distributed with certain software (including\nbut not limited to OpenSSL) that is licensed under separate terms,\nas designated in a particular file or component or in included license\ndocumentation.  The authors of MySQL hereby grant you an additional\npermission to link the program and your derivative"}]}
{"task_id": "62ece4992e6aefcf4aabbd87", "prompt": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tlogger.basicConfig(format='%(asctime)s - %(name)s - %(levelname)-8s - %(message)s', datefmt=\"%H:%M:%S\", level=logging.DEBUG)\n\n\tif config[\"print\"]: print \"[*] Setting General Purpose application logger level using the setting of the config.print: {}\\r\\n\".format(config[\"print\"])\n\n\n\nclass Logger(object):\n\tdef __init__(self):\n\t\tself.logger = logging.getLogger(\"General PurposeApplication\")\n\n\t\tself.logger.setLevel(logging."}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\tdef __init__(self):\n\t\tself.log = None\n\t\tself.logPrefix = None\n\t\tself._debug = False\n\n\tdef _log(self, msg):\n\t\tif self.log:\n\t\t\tif self.logPrefix:\n\t\t\t\tself.logPrefix + \"[%s] \" % msg\n\t\t\t\tself.logPrefix = None\n\t\t\telse:\n\t\t\t\tself.logPrefix = msg\n\t\telse:\n\t\t\tself.logPrefix = None\n\n\tdef setLogLevel(self, lvl):\n\t\tif lvl!= 1"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t#logger = logging.getLogger(\"pypylib.\") # for error messages\n\t#logger.setLevel(logging.CRITICAL)\n\t# logger.addHandler(logging.StreamHandler(sys.stdout)) # streams are handled by a logger (not only one)\n\n\t#logger.setLevel(logging.WARN)\n\t# logger.setLevel(logging.WARNING)\n\n\t#logger.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%H:%M:%S'))\n\n"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t# Initialize global logger object\n\tif not os.path.exists(logging.PATH_LOGS + os.sep + _conf.PATH_LOGS):\n\t\topen(logging.PATH_LOGS+os.sep + _conf.PATH_LOGS, 'w').close()\n\n\tif os.path.isfile(os.path.join(_conf.PATH_LOGS + os.sep, os.path.basename(_logger))).exists():\n\t\treturn\n\n\t# Generate directory with log file in current directory\n\t_logger.log_func_create(_"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t# General purpose logger.\n\t# -               Logging.\n\t# -  File logging\tLogging.\n\t# -                   General user interaction\n\t# -  I/O\t\tFile read and write\n\t# - File open/close\tFile and I/O to be performed.\n\t#\n\t# -                User interaction.\n\t# -  Event logging\n\t#\n"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t# -----------  START MESSAGE -----------  -----------  -----  -----  ------------  -----  ------------------------------------------------------------------------------------------------------------------------\n\t#\n\t## \tLog messages are posted to the log file.\n\t##\n\t#   Log messages are send to log file; messages come from a file.\n\t#   Each file contains a log line and a message.\n\t#   If message is not sent already, and there is a prior message,\n\t# \tadd it to the current log file and the previous message.\n\t# \tIf messages are not sent in the order they are"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n##################################################\n@_init = init\ndef init(logger_=logging.Logger()):\n\t\n\t# Setup application logger\n\tapp.logger().error(\"Initialize() called\")\n\tlogger.info(\"Application logger initialized\",flush=True)\n\t\n##########\n##\tSetup functions for setting up connection.\n#####\n\n\tconn_cfg = Config(\n\t\thost=\n\t\t\t\"127.0.0.1\",\n\t\tport = 2242\n\t)\n\n\t# connect to database from remote server\n\tconn.init_server(conn_cfg"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t###################################################################\n\t###################################\t\t\t\t#\n\t# Global functions\t\t\t\t\t#\n\t###################################\t\t\t\t#\n\n\tdef format_app_name(app_name):\n\t\t\"\"\"\n\t\tReturns an app_name formatted name that's suitable for display in logggers\n\t\t\"\"\"\n\t\tif not app_name:\n\t\t\treturn \"No app name given!\" + get_app_name()\n\t\telse:\n\t\t\treturn \"%s (%s)\" % (app_name, get_app_name())\n\n\tdef prepare_app_name_or"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t# The logging level to be set on a per-target basis\n\t# (i.e. only the main logger is set)\n\tlog_format_str = \"[%(asctime)s %(levelname)-5s] INFO\"\n\tlogger_dir = os.environ[os.environ.get(\"APPVEYOR_LOG_DIR\")] # just for convenience\n\t\n\t# Get file names to log about\n\tlog_dir = os.path.join(logger_dir, \"log\")\n\tlogger = logging.getLogger()\n\tfor item in os.listdir("}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t'''Log all the log lines.'''\n\tlogging.basicConfig()\n\tlogging.info(\"Log lines:\")\n\tlogging.basicConfig(filename=\"log.txt\")\t\t# Write the log to a text file.\n\tlogging.info(\"Log lines: %s\",os.getcwd().split(\"/\")[-1])\n\n\n\n# \t# Create an application with some basic setup.\n# \tapp = QtGui.QApplication(sys.argv)\n\nimport { IProjectItem, Project, ProjectItem } from \"@microsoft/sp-core-library\";\n\nexport class"}]}
{"task_id": "62ece4992e6aefcf4aabbd88", "prompt": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t# If the data-type is ``numpy.double``, the array should be given in row major\n\tif arr.dtype == \"double\" and arr.min() < 0 and arr.max() > 1:\n\t\tarr = dtype((arr.max(), arr.itemsize, arr.min()), (1, ))\n\t\tarr[...] = 0\n\treturn arr\n\n\ndef random_array(size, dtype=\"float64\", random_state=None):\n\t\"\"\"\n\tCreate a random data array of given size.\n\nInput Parameters:\n---------------\nsize : int"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\tfrom  libcloud.desu import cloud_resource\n\n\t# Create array in cloud\n\tres = cloud_resource('D', 'v20190225', region=region, size=4, id=d.id)\n\t# Create array with numpy\n\tres.upload(shape=shape, dtype=dtype, compression=compression)\n\ndef array_set_value(array, key, value):\n\t\"\"\"\n\tSet the value of an array in D\n\t- key : string key to be written to the array\t\n\t- value : value to be written to the array."}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\tshapeList = sorted(shape)\n\tif len(shapeList)<2:\n\t\traise ValueError(\"shape must have rank less then 2\")\n\n\tarray = numpy.zeros(shapeList+[0], numpy.dtype(dtype))\n\tarray[:] = numpy.array( shapeList )\n\treturn array\n\n\t\n#------------------------------------------------------------------------------\n\tdef apply_to_array( array, shape ):\n\t\t\"\"\"\n\t\tApply *array* to *shape*, where *array* is an array of\n\t\tshape *shape* and *shape* is dimensionality (n or"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t#\n\t#TODO: add documentation\n\t#\n\n\t# create type, and set element size if necessary\n\tif dtype is None: dtype = np.dtype(numpy.float64)\n\tarray = np.zeros(shape)\n\tarray.shape = shape\n\t# array.dtype =dtype\n\tarray.set_element_size(np.dtype(dtype).itemsize*4)\n\nusing Grand.Core.Models;\nusing Smartstore.Core.Catalog;\nusing Smartstore.Core.Localization;\nusing Smartstore.Core.Notifications;\nusing Smart"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\tnumpy.core.multiarray.makelayout(shape, numpy.asarray(dtype), 'U8')\n\tA = numpy.asarray(dtype)\n\tA = (numpy.atleast4(A)) * 0.0 + 0.5 # 0 -15 degrees\n\tA1 = (numpy.atleast4(A)) ** 2.0\n\tA2 = (numpy.atleast4(A)) ** 3.0\n\treturn A, A1, A2\n\n\"\"\"\n\tprint shape\n\"\"\"\n    A\t\t:\tarray(c"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\tdshape = tuple (d[0] for d in shape)\n\tdtype = np.dtype_from_extension(dshape[1])\n\tdata = np.zeros((shape[0],shape[1]), dtype=dtype)\n\treturn type(data,(shape[0],shape[1]), dtype=dtype)\n\n\nclass Catalytic_furn_and_draws(func):\n\t\"\"\"\n\tCreates an furn, draws, and draws the catalyst matrix of the current fury.\n\t\"\"\"\n\tdef __init__(self, f"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\tshape = tuple(shape)\n\tdtype = np.dtype(np.uint32)\n\t\n\taxis = 0\n\tfor d in shape:\n\t\telement = [*d]\n\t\tfor i in range(len(d)):\n\t\t\tif d[i] == 'I':\n\t\t\t\telement.append(i + 1)\n\t\t\telse:\n\t\t\t\telement.append(d[i])\n\t\tassert (len(element) == len(set(d))), 'wrong number of values'\n\t\taxis = axis + len(element)\n\t\t"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\tdtype_ = dtype\n\n\t# check number input range\n\tif np.issubdtype(dtype_, np.integer):\n\t\traise TypeError(\"Cannot create an INT array, input is a\"\n\t\t\" non-int/int array or integer of unknown dtype\")\n\n\t# prepare array\n\tn = len(shape)\n\tif n == 0:\n\t\treturn array.array(dtype)\n\n\t# convert to array\n\treturn array.array(\n\t\tarray.reshape(shape, -1).astype(dtype.type),\n\t\torder=shape"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\tshape = (max(shape) + 1, )\n\tdtype = np.dtype(np.int_)\n\n\tif shape == (0,):\n\t\treturn np.array(())\n\telse:\n\t\treturn np.array((shape, dtype) + (shape, dtype))\n\n\n\ndef _generate_array_by_shape(*sizes):\n\t\"\"\"Generate a new array with an arbitrary number of dimensions.\n\n\tParameters\n\t----------\n\n\tsizes : 2-tuple ``(size1, size2)``, or 2-tuple of 2 integers, or\n\t "}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\treturn np.array(np.ones(shape), np.dtype(dtype)).ravel()\n\n# Local Variables:\n# tab-width:4\n# indent-tabs-mode:nil\n# End:\n# vim: set expandtab tabstop=4 shiftwidth=4:\n\n/*\n *  Copyright 2014 AT&T\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http://"}]}
{"task_id": "62ece4992e6aefcf4aabbd89", "prompt": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\treturn np.interp(f, g, f.T) * np.dot(g, g)\n\n\nclass Gaussian(BaseFunction):\n\t\"\"\"\n\tGaussian with the shape of the input\n\t``f(g|) = \\gamma g''``; for use only for small values of g.\n\n\tThis gives the same result as the ``Gaussian(x)`` function, for small\n\tvalues of x; the result is not suitable for values near 0.\n\t\"\"\"\n\tdef __init__(self, f):\n\t\t\"\"\"\n\t\tInitialise ``Gaussian`` function"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\ts = 0.1\n\tg_mean_radius = s * (np.sqrt(s) + np.finfo(np.float32).eps)\n\tg_sigma_radius = s * s * (np.sqrt(s) + np.finfo(np.float32).eps)\n\t# Gauss-Lobatto centered around the standard\n\t# Gaussian distribution with gaussian scale 0.2 to 0.1\t\t\"\"\"\n\ts1 = 0.2\n\ts2 = 0.4\n\tg_mean_radius_1 = s1 *"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tdef run(x):\n\t\tif x[0] < 0 or x[1] < 0:\n\t\t\tprint('0 x', x[0], '0.1 x', x[1])\n\t\t\treturn\t0\n\t\tx[0] = (x[0] * np.cos((x[0] - 0.5)*i/np.pi))\n\t\tx[1] = (x[1] * np.sin((x[1] - 0.5)*i/np.pi))\n\t\treturn 0\n\treturn (0,"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tr = 3*r\n\tb = 3*b\n\tc = 3*c\n\tprint(str(r+b+c))\n\tprint(\"r:\",r)\n\tprint(\"c:\",c)\n\tprint(\"b:\",b)\n\tr = Gaussian.gaussian(r,c,0.1).data\n\tprint(\"r\",r)\n\tprint(\"c\",c)\n\tprint(\"r\",r)\n\tprint(\"c\",c)\n\tprint(\"b\",b)\n\tr,c = (i.astype(float),j."}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tsigma = np.random.uniform(-0.9, 0.9, size=n)\n\treturn sigma\n\n\n\n\n\n\t\t\nclass Firshift:\t# input image dimensionality\n\tn = int(32 * 2 - 1)\t# window width (in pixel size)\n\tm = int(32 * 2 - 1)\t# input image height numbering (height)\n\th = int(320)\t\t# window height (in pixel size)\n\tdx = float(0.01)\t\t\t# input horizontal spacing\n\tdy = float(0)"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t# \t# This is so noisy because it doesn't scale all the time\n\t# \tmean = np.random.normal(scale=1)/scale\n\t# \tmax  = mean - 0.5*(np.random.normal() -mean)\n\t# \tlog1p  = log(scale)\n\t# \tlogp = np.exp(-log1p)*np.random.normal()\n\t# \t# \tnp.random.seed(seed+1)          # random seed for re-generation"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tf = np.fft.fft(1 + np.array(center).flatten() - 0.1)\n\t\t\t\t\t# np.fft must round the index\n\tassert len(f.shape) == 2  # len(x) == 2\n\tf = f[f.argsort()]\n\t\t\t\t\t\t\t\t# order matters - just cast to float\n\tassert np.all(f.min()<np.finfo(float).max/2) and np.all(f.max()<np.finfo(float).max/2)\n\n\tf = f[np.finfo"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tc = [0.5, -0.5]\t# centered around 1.0\n\tsigma = c[:3,:3]\n\tphi = numpy.arctan2(c[:3,3], c[3:])\n\tnp.testing.assert_array_almost_equal(phi, numpy.degrees(phi), delta=1.33,\n\t\t\t\t\t\terr_msg='\"sigma\" specifies an invalid tau!')\n\t\n\tnp.testing.assert_array_almost_equal(np.asarray(c), a.shape)\n\n\tnp.testing."}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tsigma = np.log(1 / sigmas)\n\tr = np.random.uniform(low=-8, high=8, shape=(64, 64))\n\tc = np.random.uniform(low=-2, high=2, shape=(64, 64))\n\tx = np.random.uniform(low=0, high=len(range(64)), shape=(4,))\n\tprint = c * np.cos(x) + r * np.sin(x) + sigma * sigma\n\n\tassert abs(print - x).mean() >= 0.0001\n\t"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\tnoise_level = np.linspace(min_value, max_value, num_rows)[0]\n  \n  def __init__(self,mean,cov_estimators,sigma_factor,sigma_sigma):\n\t\"\"\"\n\tInit.\n\n\tArguments:\n\tmean (array_like, optional) mean value of distribution as a np.array.\n\tcov_estimators (array_like, optional) an array of covariance estimators as\n\t\ta list-like of (n,m) tuples.\n\tsigma_factor (array_like,"}]}
{"task_id": "62ece4992e6aefcf4aabbd8a", "prompt": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t#if __name__ == \"__main__\":\t__main__()\n\timport time\n\timport getopt\n\n\tos.environ['DJANGO_SETTINGS_MODULE'] ='md5'\n\toptions = getopt.getopt(args, \"m:s:l:N:\", [\"file\"])\n\tfor option,arg in list(options.items()):\n\t\tfname =  os.path.split(arg[0])\n\t\ttry:\n\t\t\ttry:\n\t\t\t\t#if len(os.path.basename(os.path.dirname(fname))) < 3:\t#"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t# For the case that only a single file was created,\n\t# load a single configuration file, then validate\n\t# each configuration file. Load one config file and validate each config file.\n\t# If there was a bug, and parse error log records,\n\t# only update the tuple.\n\n\n\tif len(args)!= 2:\n\t\terr = \"Bad argument number; should be (argc, configfile) pair.\"\n\t\terror_message = logger.error(err, logger.ERROR)\n\t\treturn (None, None), None, None, error_message"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\timport getopt\n\timport sys\n\n\timport pexpect\n\tfrom pexpect import config\n\n\tdef parse_arguments(args):\n\t\tparser = argparse.ArgumentParser(prog='pydocgen-file.py',\n\t\t                                 description='List all the configuration files.', add_help=False)\n\t\tparser.add_argument('file_paths', type=str, help='List the configuration file paths.', required=True)\n\t\t"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tdef load(filename, *args, **kwargs):\n\t\t# Check for invalid files.\n\t\tif filename.startswith('.'):\n\t\t\traise ValueError('%s does not exist, a malformed configuration filename' % filename)\n\n\t\ttry:\n\t\t\t# Read config file from disk.\n\t\t\tf = settings.Open(filename+'/config.yaml')\n\t\t\tconfig = yaml.load(f, Loader=yaml.SafeLoader)\n\t\t\tf.close()\n\n\t\t\t# If there is a default configuration file, return the original configuration object.\n\t\t\t"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tdef configure_parser(arg):\n\t\tparser = argparse.ArgumentParser()\n\t\tparser.add_argument(\n\t\t\t\"+log=\",\n\t\t\taction=config_parse_logging,\n\t\t\tdest=\"log\",\n\t\t\thelp=\"log to the specified log level\",\n\t\t\tdefault=\"\",\n\t\t)\n\t\tparser.add_argument(\n\t\t\t\"-h\",\n\t\t\taction=help,\n\t\t\thelp=\"show this help message and exit\",\n\t\t\tdefault=\"\",\n\t\t)\n\t\tparser.add_argument(\n    "}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tprint(\"\\n# Start Configuration Loading\")\n\tlog.debug(\"# Start Configuration Loading\")\n\tfor configFilename in configpaths:\n\t\tprint(\"\\n# Loading config file with config filename...\")\n\t\twith open(configFilename) as configFile:\n\t\t\ttext = configFile.read()\n\t\t\tconfigfile = ConfigParser.ConfigParser(header=None)\n\t\t\tconfigfile.readfp(text)\n\t\t\tconfigFile.close()\n\t\t\tconfigpaths[configFilename] = configfile\n\n\tprint(\"\\n# Start Configuration Validation\")\n\tlog.debug"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\t# create an empty configuration\n\t\tconfiguration = dict()\n\n\t\ttry:\n\t\t\t# check for valid configuration file\n\t\t\tif not os.path.isfile(configuration_file):\n\t\t\t\tmessage = configuration_file + \": required configuration file not found, check directory path to config file settings.\"\n\t\t\t\tlogging.warning(message)\n\t\t\t\texit(1)\n\t\t\telse:\n\t\t\t\tconfiguration_directory = os.path.dirname(configuration_file)\t\n\t\t\t# check for valid sequence of configuration files\n\t\t\tif not os.path.isfile"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tlog = logging.getLogger(\"mypathtools\")\n\n\t# load configuration file\n\tparsed_config_filename, parsed_seq, err = get_config_filename(config_file)\n\n\t# validate each configuration file\n\tif err is None:\n\t\tlogging.error(\"Configuration is not valid.  ConfigFilename: %s.\", config_file)\n\t\treturn\n\n\t# parse each parsed config file and call validate on parsed configs\n\tcfg := parse_configuration(parsed_config_filename, parsed_seq)\n\tfor cfg_name in sorted(cfg."}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t# We want a list of parsed configuration filenames, and a list of configuration filenames in\n\t# order to get them, so we use this loop to do each of the validation. The list should contain a\n\t\t# valid configuration filename so that we can load the configuration without the validation.\n\t# We start by loading each configuration filename, checking if it exists, and logging the config.\n\t\tfor filename in filenames:\n\t\t\tfilename_parse_log = logging.getLogger(\"nlu_config.parsing.check_configuration_name\")\n\t\t\tfilename_debug_log = logging.getLogger(\""}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\tlog = logging.getLogger(configfile)\n\n\tfor fname in files:\n\t\tparseconfig_log_record(config,fname)\n\n\tparsedconfigs = [parseconfig_log_record(config,fname,log)]\n\n\treturn parsedconfigs, config\n\ndef parseconfig_log_record(config,fname):\n\tparsedrecord = {}\n\n\tif not isinstance(fname,str):\n\t\tfname = config.get_file_name(fname)\t\t# Get file name\n\n\tparsedrecord['file'] = fname\t# Open file\n\n\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd8b", "prompt": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tif self.is_string_unicode(obj):\n\t\tbytes_unicode = str(obj.encode('utf8'), encoding='utf8')\n\telse:\n\t\tbytes_unicode = obj\n\treturn bytes_unicode\n\n\n#\t#\t#\t#\t#\t#\t#\t#\t#\t#\t#\t#\t# \t#\t#\t#\t#\n#\t@classmethod\n#\tdef from_bytes_to_bytes(cls, bytes, self):\n#\t\t#\t#\t#\t#\t#\t#\t#\t"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tif not isinstance(obj, str):\n\t\traise TypeError(\"must accept str or unicode value\")\n\tbytes = sys.getfilesystemencoding()\n\tstrtype = getclass(obj)\n\ttry:\n\t\t__bytes_result = _bytes_unicode_fromstring(bytes, strtype)\n\texcept UnicodeEncodeError as exc:\n\t\tif type(exc) is not str:\n\t\t\texc.strerror = u\"%.200s (for bytes)\" % exc\n\n\t\traise TypeError(u\"%s (for bytes) was not a unicode string\" % exc)"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tif isinstance(obj,str):\n\t\treturn bytes(obj, encoding='utf-8')\n\tif isinstance(obj,unicode):\n\t\treturn obj.encode('utf-8', encoding='utf-8') or bytes('')\n\treturn obj\n\t\ndef load_json(file):\n\t\"\"\"\n\tLoads json file and returns the object corresponding to it.\n\t\"\"\"\n\twith open(file, mode='rb') as f: \n\t\treturn bytes(f.read(), encoding='utf-8')\n\t\ndef json_loads(data_string):\n"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\tb = obj.encode('utf-8')\n\t\treturn encode(b).decode('latin1') \n\n\tdef decode(obj):\n\t\tb = \"\" \n\t\tfor i in range(len(obj)):\n\t\t\tb += \"%02x\" % int(obj[i], 16) \n\t\treturn b \n\n<?php\n\nuse Illuminate\\Support\\Facades\\Schema;\nuse Illuminate\\Database\\Schema\\Blueprint;\nuse Illuminate\\Database\\Migrations\\Migration;\n\nclass CreateUsersTable extends Migration\n{\n    /**\n   "}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tif isinstance(obj, str):\n\t\treturn obj\n\tdata\t= \"\"\n\tstart\t= 0\n\tend\t\t= 0\n\tsize\t= len(obj)\n\tr\t\t= readbytes(data, start, end)\n\tif len(r) == 0:\n\t\treturn object()\n\tdata\t\t= unicode(data, start, end-start)\n\tdata\t\t= data.decode('utf8')\n\treturn (\n\t\tdata[0:start] + bytearray(b\"\".join(data[size:end:"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tif isinstance(obj, (bytes, str)):\n\t\treturn obj\n\tif isinstance(obj, str):\n\t\treturn bytes(obj, encoding='utf-8')\n\t# Assume unicode\n\treturn unicode(obj)\n\n\nimport collections_abc\ndef strlist_as_list(list1, encoding='utf-8', errors='replace'):\n\t\"\"\" Returns list corresponding to one or more string objects\n\t\tin case of strings\n\t@param list1\tlist of string objects\n\t@param encoding\tencoding of string\n\t@param errors\terror messages used for"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tif isinstance(obj, unicode):\n\t\treturn obj\n\treturn pywintypes.new(pywintypes.String(obj))\n\n\nclass MultiDict(types.ModuleType):\n\t\"\"\"\n\tImplements the :mod:`pywints.tools.multi_dict` module.\n\t\"\"\"\n\tdef __init__(self, keys, values):\n\t\ttypes.ModuleType.__init__(self)\n\t\tself.keys = dict((key, values.get(key)) for key in keys)\n\t\tself.values = dict(values)\n\n\n"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t__version__ = '0.40.1'\n\t__author__ = 'Thomas Kiss'\n\t__date__ = '02 Oct 2019'\n\t#######################################################################\n\t#import ctypes.wintypes\n\tif PyWin:\n\t\t#Import from ctypes to enable Unicode support\n\t\tif hasattr(ctypes, 'PyMarshal_Structure') and\\\n\t\t\tnot (ctypes.PY_VERSION_HEX < 0x020600 // 10):\n\t\t\ttry:\n\t\t\t\tfrom ctypes import PyMarshal_Structure\n\t\t\t\tret = None\n\t\t\t\t#try:"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tif ctypes.sizeof(type(obj))!= ctypes.sizeof(str) and \"Python\" not in py3:\n\t\ttypelib.reload(obj)\n\t\tobj = ctypes.cast(obj, typelib.fromstring(str(obj)))\n\treturn typelib.type(obj), obj\n\n#if USE_ZLIB && (defined(_WIN32) || defined (_WIN64))\n\tusing cpython::_Py_NewObject;\n\tusing ::cpython::new_ref;\n\tusing ::cpython::new_ref;\n\tusing ::cpython::new"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\tif hasattr(obj,'_bytes'):\n\t\t\"\"\"\n\t\tThis function copies the bytes into the object.\n\t\tNote: The returned byte slice is also updated.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tbytes = obj._bytes\n\t\texcept AttributeError:\n\t\t\tbytes = \"\"\n\t\treturn NoneBytes(bytes.encode(sys.getfilesystemencoding()))\n\telse:\n\t\t\"\"\"\n\t\tThis function returns the bytes object corresponding to the specified\n\t\tobject and it's encoding.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn Unicode(\""}]}
{"task_id": "62e60723d76274f8a4026b76", "prompt": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tcls = Ticks(ticks)\n\t# return cls(+1 for negative one, +2 for extreme one)\n\treturn cls\n\nimport bunny\n\n\n__all__ = [\"Time\", \"Time\", \"TimeUTC\", \"TimeUTC\", \"Time\",\n           \"TimeU\", \"TimeU\", \"TimeL\", \"TimeB\", \"TimeK\", \"TimeD\"]\n\n\nclass bunny_time(object):\n    def __init__(self, ticks):\n        self.ticks"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t_checkinterval(ticks, (0, 86400000000000))\n\tif tz is None:\n\t\t# we get the value from tzfile if it's set in the input file\n\t\tts = _utcnow()\n\t\ttz = timezones.gettz(cls.default_tz)\n\t\tif ts is None or ts == tz:\n\t\t\traise ValueError('No timezone in the given time')\n\tif len(ticks) == 0:\n\t\treturn Time('NaT', tz=tz)\n\telif len(ticks) > 86400000000000:\n\t\t"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tassert ticks >= -86400000000000 and ticks <= 86400000000000\n\treturn cls(Ticks([ticks], None), tz)\n\n/* tslint:disable */\n/* eslint-disable */\n\nimport { ConcreteRequest } from \"relay-runtime\";\nimport { FragmentRefs } from \"relay-runtime\";\nexport type Artist_artworkDetailsTestsQueryVariables = {\n    artist: {\n        readonly \":isPublishedOnly\": boolean;\n    } | null;\n    id: string;\n    \"explanation\"?: any |"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tif not ticks:\n\t\traise ValueError(\"ticks must be greater than or equal to zero.\")\n\treturn cls(ticks)\n\n\n@datetime.time\ndef to_ticks(cls, tz=None):\t# alias\n\t\"\"\"\n\tConvert from ticks to nanoseconds.\n\n:param tz: timezone in which to display ticks\n:type tz: datetime.tzinfo\n\n:rtype: Timestamp\n\n:returns: the timestamp of the time that was created.\n\"\"\"\n\treturn cls(ticks=cls.to_ticks(tz))\n\n\ndef _time_to_offset"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tdt_base = datetime(hours=0, minutes=0, sec=0, tzinfo=tz)\n\tdt = dt_base.replace(microsecond=0)\n\n\tif ticks == dtbase(dt):\n\t\treturn dt_base\n\treturn dt_base + dt\n\n# Copyright 2011 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Amazon Software License (the \"License\"). You may not use\n# this file except in compliance with the License. A copy of the License is\n# located at\n#\n# http"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tseconds = struct.unpack_from(u\"H\", ticks)[0]\n\thours = u\"0\" + struct.pack(\">H\", seconds * 60, 0, 0, 0, 1, 0, 0, seconds)\n\tminutes = u\"0\" + struct.pack(\">H\", seconds * 60, 0, 1, 0, 0, 0, seconds)\n\thours += u\"00\"\n\tminutes += u\"0\"\n\thours += u\"00\"\n\tminutes += u\"00\"\n\thours += u\"000\"\n\tminutes += u\""}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t_min_tick_base = ticks / 86400000000000\n\tif _min_tick_base <= 4:\n\t\tticks = _min_tick_base\n\n\tif ticks < 0:\n\t\traise ValueError('\"ticks\" must not be negative')\n\n\tyear = ticks // 365\n\thour = ticks % (365 * 24) % 24\n\n\tif tz:\n\t\ttry:\n\t\t\ttz = tz.zone(year)\n\t\texcept ValueError:\n\t\t\traise ValueError(\n\t\t\t\trepr(datetime.datetime(year, 1, 1)).replace('%"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t_getticks_time = GetTicksTime\n\n@classmethod\ndef from_ticks(cls, ticks, tz=None, tzoffset=None, tzinfo=None):\t# lint-amnesty, pylint: disable=missing-class-docstring\n\t\"\"\"Create a time from 2-tuple (seconds since 1970, nanoseconds since midnight).\n\n    This constructs a :class:`Time` that represents the same number of\n    seconds as that corresponding to a given epoch in ``datetime.timedelta()``\n    representation.\n\n    This can handle"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\tif ticks < 0:\n\t\traise ValueError(\"tick value out of bounds: ticks must be positive to produce time.\")\n\n\tif isinstance(ticks, basestring):\n\t\tticks = int(ticks)\n\n\tif ((ticks < 0)!= (int(ticks) % 86400000000000)):\n\t\traise ValueError(\n\t\t\t\"ticks must be an integer value in [0, 86400000000000)\"\n\t\t)\n\n\tif (0 <= ticks < 86400000000000) or (ticks < 0 or 0 >= ticks % 86400000000000):\n\t\traise ValueError\n\n\t"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\treturn cls(datetime.timedelta(seconds=ticks / 24, fraction=0))\n\n\nclass _DeltaTimeTicks:\n\t\"\"\"\n\tGenerate the time delta value.\n\n\t:param ticks: number of nanoseconds since midnight\n\t:type ticks: int\n\n\t:rtype: int\n\t\"\"\"\n\tdef __init__(self, ticks):\n\t\tself._ticks = 0\n\t\tself.ticks = ticks\n\n\tdef __int__(self):\n\t\treturn self._ticks\n\n\tdef __str__(self):\n\t\treturn \"{:02d}"}]}
{"task_id": "62e60873d76274f8a4026bd8", "prompt": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tfrom netdissect import BoltProtocolHandler\n\treturn BoltProtocolHandler(\n\t\tcls.protocol_to_protocol_handlers(),\n\t\tmessage_types.ALL,\n\t\tprotocol_version=protocol_version\n\t)\n\n# ______________________________________________________\n#\n# Bolt protocol handlers\n\nclass BoltProtocolHandler(object):\n\t#-----------------------------------------------------------------------------\n\t#\n\t# Internal helper methods\n\n\tdef __getattr__(cls, key):\n\t\tif key not in cls.__dict__:\n\t\t\tif key[:2] == \"Protocol\":\n\t\t\t\treturn"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\n\t\treturn (cls.protocol_handlers_by_version_dict)\n\telif len(protocol_version) == 1:\n\t\treturn protocol_version[0][0]\n\telse:\n\t\treturn protocol_version\n\n\ndef protocol_handler_by_version(protocol_version):\n\t\"\"\"\n\tReturn a handler class that implements protocol support for\n\ta specific version of Bolt protocol.\n\n\t:param protocol_version: version tuple identifying a specific\n\t\tprotocol, such as 4 or 5.\n\t:return"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\n\t\tversion_tuple = Protocol_version.DEFAULT_PROTO_VERSIONS\n\telse:\n\t\tversion_tuples = tuple(int(p) for p in protocol_version)\n\t\tversion_tuple = tuple(Protocol_version.to_tuple(v) for v in version_tuples)\n\tif len(version_tuple)>1:\n\t\traise ValueError('Protocol version should be either a tuple '\n\t\t\t'or a scalar (to select only one valid protocol version), '\n\t\t\t'second: {!r} ({},{"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tprotocol_version = tuple(*protocol_version)\n\tif protocol_version is None:\n\t\treturn dict(protocol_handlers_default)\n\tcls = protocol_protocol_handlers.setdefault(protocol_version.lower())\n\tif cls is not None:\n\t\treturn cls.default\n\treturn protocol_handlers()\n\nusing MvvmCross.Platform;\nusing MvvmCross.Platform.Wpf.ViewModels;\nusing MvvmCross.iOS.Presenters.FrameworkInfrastructure;\nusing MvvmCross.iOS.Views;\nusing Mv"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_version = (3, 5)\n\thandler_classes = {} # dict of (protocol, supported methods)\n\t# the default handler class is our self-supplied class\n\thandler_class = getattr(self, \"__bolt___\" + protocol_version[-1])\n\n\tif protocol_version:\n\t\ttry:\n\t\t\thandler_class = handler_classes[\"bolt\"]\n\t\t\tif not protocol_version[0] <= handler_version[0] <= (3, 5):\n\t\t\t\traise TypeError(\"Protocol version %"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tresult = {}\n\n\tif protocol_version is None:\n\t\tprotocol_version = get_protocol_version(version=__package__)\n\t\tif protocol_version is None:\n\t\t\traise ValueError(\"Bolt protocol version doesn't have a protocol\")\n\n\tfor version in protocol_version:\n\t\tif not isinstance(version, tuple):\n\t\t\traise TypeError(\"protocol version is expected to be an tuple, got: '%s'\" % (version,))\n\n\t\tif len(version)!= 1:\n\t\t\traise TypeError(\"protocol version is expected to contain one protocol"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t# See the Bolt protocol handler docs\n\n\tif protocol_version is None:\n\t\t# Use the legacy _available versions\n\t\treturn _available_blowfish_protocols\n\telse:\n\t\ttry:\n\t\t\tversion = tuple((x * 5) for x in protocol_version)\n\t\texcept:\n\t\t\t# protocol_version is not passed in a tuple\n\t\t\tif len(protocol_version)==1: logger.debug('protocol_version is empty, returning version 0')\n\t\t\treturn _versioning_blowfish_protocols[protocol_version"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif not protocol_version: return protocols_default_handlers()\n\n\tversion = protocol_version[0].strip().lower()\n\tif version not in (None, '3', '5'):\n\t\traise TypeError('Protocol_version must be one of (None, '\n\t\t                '\"3\", \"5\")')\n\n\tif protocol_version in protocols:\n\t\treturn protocols[protocol_version]\n\treturn protocols_default_handlers()\n\n\n@protocol_handlers.register()\nclass BoltCommand("}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None: protocol_version = ProtocolVersion()\n\tversion_dict = {} # a set of protocol versions, the mapping of protocol version to handler class,\n\t\t# with the key (version tuple) being the protocol version, and the value being a list of\n\t\t# protocol handler classes\n\tfor protocol_version in protocol_version:\n\t\tif protocol_version.valid: version_dict[protocol_version.protocol] = protocol_version.protocol\n\n\t\tif not hasattr(cls, protocol_version.protocol):\n\t\t\traise cls.ProtocolError"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\tif protocol_version is None:\t\t# pragma: no cover\n\t\tversion_tuples = (\n\t\t\t(3, 0), # Bolt\n\t\t\t(5, 0)) # Mine\n\telif version_tuple not in version_tuples: # pragma: no cover\n\t\traise TypeError: 'protocol_version must be specified\\n'\n\treturn protocol_handlers._protocol_handlers[\n\t\tProtocolVersion(version=protocol_version)]\n\nProtocolHandlerList.register(ProtocolVersion((1, 5), (2, 8), (1, 0), 0))\n"}]}
{"task_id": "62e60e3bd76274f8a4026d1a", "prompt": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\treturn cls(fields)\n\nclass BookmarksValueTuple(tuple):\t #: this would be a tuple or tuple of tuples, for example, the following would go:\n\tvals = [BookmarkValue(k) for k in values]\n\n\tdef __repr__(self):\t #: this is like this, but without the brackets\n\t\tvals = list(self)\n\t\tif self.vals:\n\t\t\tvals.pop(0)\n\t\t\tvals.pop()\n\t\tretval = [k.__repr__() for k in vals]\n\t\treturn '({'.join("}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\traise ValueError('invalid arguments')\n\n\nclass MakernoteFormat(object):\n\n\tdef __init__(self, format):\n\t\tself.format = format\n\t\ttry:\n\t\t\tself.tag_name, tag_id = self._decode_tag_id('format')\n\t\texcept AttributeError:\n\t\t\tself.read_tags()\n\t\t\tself.tag_name = None\n\t\t\tself.format = None\n\n\tdef __repr__(self):\n\t\treturn '<%s at 0x%x, tag %d: %s>' %"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\treturn cls.from_raw_values(values, None)\n\n\ndef from_raw_str(raw_txt, value_types=None, string_types=None,\n\t\t\t\t\t\t\tclass_id=None, tag_name=None):\n\t\"\"\"\n\tCreate a Bookmarks object from raw strings.  Valid strings include:\n\t>\n\n\t- Tag name\n\t<\n\t>\n\n\t- Value types\n\t<\n\t>\n\n\t- List of string types, one per tag.\n\t>\n\n\t- String types.\n\t<\n\t"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\treturn create(cls, values, \"Bookmarks\")\n\ndef from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a vector of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\treturn create(cls, values, \"Bookmarks\", format)\n\nusing AvalonStudio.Extensibility;\nusing AvalonStudio.Extensibility.Platform;\nusing A"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\tself = object.__new__(cls)\n\tself._raw_values = values\n\tfor value in values:\n\t\tself[value] = self._raw_values.add(value[-1:])\n\treturn self\n\n\ndef from_raw_fields(fields, value=None, defaults={}):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw BookmarkField values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param fields: List of raw BookmarkField values returned from read_raw_bookmarkfield"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\treturn list(zip(values))\n\n\ndef from_yaml_file(filename):\n\t\"\"\" Load a Bookmarks with a filename.\n\n\tThis method returns a Bookmarks object, and it also adds a value for\n\tmissing.\n\n\t:param filename: raw bookmarks file path\n\n\t:returns: Bookmarks, with key 'id' replaced by a unique id\n\t:rtype: :obj:`list[str]`\n\t\"\"\"\n\tcontents = []\n\tfor file in open(filename):\n\t\tcontent = read(file)\n\t\t# Skip empty lines"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t# Bookmarks list may contain more than a single bookmark, so\n\t# make sure that you are not going to think that they are the bookmarks\n\t# they are going to read.\n\tfor value in values:\n\t\tvalue = value.strip()\n\t\tlogger.debug('reading %s' % value)\n\t\tresult, err = Bookmarks.read_value(value, Bookmarks)\n\t\tif not err:\n\t\t\traise TypeError(err)\n\treturn result\n\n\ndef deserialize(name_or_data): # from raw_value import value"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\targs = {v:None if values else v for v,v in _create_bookmarks(values)}\n\traise serializers.GenericError(None, args)\n\n# encoding: utf-8\n# Code generated by Microsoft (R) AutoRest Code Generator.\n# Changes may cause incorrect behavior and will be lost if the code is\n# regenerated.\n\nmodule Azure::DevTestLabs::Mgmt::V2016_05_15\n  module Models\n    #\n    # The parameters to list TestJobs.\n    #\n    class ParameterCollection\n"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\traise NotImplementedError(\"Only raw bookmarks can be converted to str in Python2. Please don't import this module.\")\n\n\ndef from_json_value(cls, obj):\t## Note: This must be a new-style tuple\n\traise TypeError(\"Only works on json string values\")\n\n\ndef from_json_list_value(cls, obj):\t## Note: This must be a new-style tuple\n\traise TypeError(\"Only works on json list values\")\n\"\"\"\n\tCreate a Bookmarks object from a list of JSON value objects.\n\nFor each value in the list, it"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\tfrom_raw_bookmark_values(values)\n\n\ndef from_raw_values(list_values):\n\treturn tuple(from_bookmark_values(value) for value in list_values)\n\n\nclass Bookmarks():\n\t\"\"\"\n\tClasses that define the API for Bookmarks to be used.\n\n\tThis class is used by the Bookmarks plugin to provide information\n\tabout particular bookmarks.\n\t\"\"\"\n\t__metaclass__ = type\n\n\tdef __new__(cls, values):\n\t\tfrom six.moves.text_type import six_str as"}]}
{"task_id": "62b87b199a0c4fa8b80b354c", "prompt": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif not type(seq) or not isinstance(seq, self.type_sequence):\n\t\traise ValueError('type_sequence required')\n\treturn (seq,_to_sequence(seq))\n\ndef _get_sequences(seq):\n\tglobal _seq\n\tif not seq:\n\t\treturn []\n\ttype_seqs = _all_type_sequences(seq, 1)[1]\n\t# type_seqs = _type_sequences(seq)\n\tdata = [type_seqs]()\n\tfor p in seq:\n\t\tdata.append(self.seq2id"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif (bufsize is None or bufsize < 0 ):\n\t\tbufsize = int(maxlen_from_string(seq))\n\tseq = buf_init_sequence(seq, bufsize)\n\treturn (seq, type_from_string(type_with_spec_string(spec_get_name(seq))))\n\n\ndef is_not_none(seq):\t\"\"\"Make *seq* is an empty sequence.\"\"\"\n\tif len(seq) < 2:\n\t\treturn True\n\telif (isinstance(seq[0], str) and seq[0].count(\",\") > 0 and"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tfor index in range(len(seq)):\n\t\tif index!= index_of(seq[index]):\n\t\t\treturn (seq, Type.TUPLE2)\n\treturn (seq, Type.TUPLE)\n\t\n\n@set_typevar('struct(type)')  # for py3.x compatibility\ndef _get_seq_with_type_array_data(seq): \n\t\"\"\"\n\tReturn a (sequence, length) triple for a sequence of arrays.\n\tIf only one array, sequence is flattened.\n\tseq should be a list of arrays to fill with new value."}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif bufsize is None:\n\t\tbufsize = sys.maxsize\n\ttypemap = {}\n\tstart = 0\n\tfor n in xrange(10000, 65535, 1):\n\t\tif n >= seq.size or n <= 0:\n\t\t\tif not maxlen: return (None, None)\n\t\t\tmaxlen = 0\n\t\t\t# XXX should be configurable\n\t\t\tif bufsize is not None:\n\t\t\t\tmaxlen = bufsize\n\t\t\tif n == seq.__len__() - 1:\n\t\t\t\tstart = 0\n\t\t\t\tcontinue\n\t\t\tseq = seq[:n]\n\t"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif bufsize is None: bufsize = 1024\n\tbuffer = b''\n\tfor i in range(0, len(seq), bufsize):\n\t\tbuffer += b'\\x00'\n\tif len(buffer) <= i: return (buffer, None)\n\tr, t, l = struct.unpack('<I', seq[i:i+bufsize])\n\tif l <= len(seq): return (r, (t,i))\n\telse: return (r, t+i)\n\ndef _get_seq_with_type(seq, bufsize=None):\n\tassert"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif int(bufsize):\n\t\treturn (seq, type(seq).get_element_type())\n\telse:\n\t\treturn (seq,)\n\n\nclass _Seq(pydoc.doc_template_str, _Template):\n\n\tdef get_sequence(self):\n\t\treturn (self.as_string(), self.as_string_with_type())\n\n\nclass _SeqType(cls, _Seq):\n\t_type_ = None\n\nclass _SeqTypeWithType(_SeqType, _Seq):\n\n\t# A type used to be passed"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif bufsize is None:\n\t\tbufsize = len(''.join([repr(int(_t)) for _t in self._seqname if _t!= '<']\n\t\t\t\tif _t == 'x')+7\t\t# Add 6 for padding\n\t\t)\n\t_seqname = '%08d' % int(_t)\n\treturn (int(_t), _seqname)\n\ndef read_long(fp):\n\t\"\"\"Read a 32bit int into 1B long using fread.\n\tReturns bytearray.  If the input is not on an int boundary, a\n"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tbufsize = _bufsize(bufsize)\n\tdata = seq[:]\n\n\tif bufsize:\n\t\tdata.extend(seq[:bufsize])\n\treturn (data[0:len(seq)], seq[type(seq) :].type)\n\ndef _seq_eq(_seq, other):\n\t\"\"\"Return <code>True</code> if the sequences are equal.\n\n\tSequences are compared using :func:`operator.eq`, and sequences\n\tthat match exactly once are compared according to the equality\n\trule.\tIf the sequences are unequal, sequences are compared as\n\tstrings"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tassert_isinstance(seq, Sequence)\n\tbase = ()\n\tif not isinstance(seq, Sequence):\tbase = seq\n\tif seq.size == 0:\t\tbase = seq.type\n\tseq = base if base.__class__ is Seq else seq.type\n\tmaxlen = (bufsize or 0)\n\tfor seq_length, value in seq.items():\n\t\tif value == 0:\n\t\t\treturn list(seq_length), seq.type\n\t\tif maxlen is not None:\n\t\t\tmaxlen = int(maxlen / float(seq_length))\n\t"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\tif isinstance(seq, string_types) or not seq:\n\t\treturn seq, _ETYPE\n\t_LTYPE = _ETYPE.get('_'+seq, None)\n\tif bufsize is None:\t/* No buffer size.\t*/\n\t\treturn _LTYPE, None\t/* Just return.\t*/\n\telse:\n\t\treturn _LTYPE, bufsize\n\ndef _get_seq_with_size_of_no_buffer(seq):\n\treturn _get_seq_with_type(seq, 0)\n\ndef _get_seq_with"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3581", "prompt": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tif isinstance(other, slice):\n\t\t\traise ValueError('cannot scale 0 slice')\n\n\t\tif not self or not self.index:\n\t\t\tif other:\n\t\t\t\tif issubclass(other, Integral):\n\t\t\t\t\tif not other._is_integral:\n\t\t\t\t\t\traise ValueError('cannot scale with integral, got non-integral')\n\t\t\t\t\tscale = other.scale(self.bins[other._index], recompute=True)\n\t\t\t\t\tother = float(other[1] - scale)\n\t\t\t\telse:\n\t\t\t\t\tif not other._is_integral"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tfrom numpy.core import asarray_like\n\tfrom.. import Histogram\n\tif isinstance(other, Histogram):\t\t# other is a Histogram\n\t\tother = other.data.filled(0)\n\tif recompute:\t\t\t\t# for now, just recompute the data\n\t\tself.data = asarray_like(other.data)\n\t\t# compute the integral of the histograms\n\t\tif not self.contains_nan(other).all():\n\t\t\tself.compute_hist_scale(other)\n\telse:\n\t\tother = other.toc"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif other is not None:\n\t\tif isinstance(other, tuple) and isinstance(other[1], dict):\n\t\t\traise TypeError('scale, scale not allowed for other field')\n\t\tif other is'self' and self is '0':\n\t\t\traise TypeError('scale, scale not allowed for other field')\n\t\tself.scale(-other)\n\t\treturn self\n\n\tif self.scale == other:\n\t\treturn self\n\n\tif len(self._fields)!= len(other._fields):\n\t\traise TypeError(('Fields in one or two"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif not isinstance(other, Histogram):\n\t\traise ValueError('Can not do: %s' % (other,))\n\tif self.n_bins == 0:\n\t\treturn new_zero(self._n_bins)\n\tif isinstance(other, Histogram):\n\t\treturn other\n\tn_bins = self.n_bins\n\tassert n_bins > 0\n\tif not isinstance(other, self.__class__):\n\t\tother = self.__class__(other)\n\tassert n_bins == other.n_bins, 'Histograms with scale %s cannot"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t# Check to see where recompute was the first call..\n\tif recompute:\n\t\tself = self.convert(other=other)\n\t\tif self.dim == 0 or len(self) == 0:\n\t\t\traise self.empty, self.unit\n\t\tself = self.scale\n\treturn self\n\n\ndef histogram_filter(self, k, d=None):\n\t\"\"\"\n\tPerform a histogram filter on the array.\n\n\tk must be greater than or equal to 1.\n\n\tIf d is given, its range is ignored.\n\n"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif other is None:\n\t\treturn self.to_numpy()\n\tif not isinstance(other, self.constructor):\n\t\traise TypeError(\"Scale must implement the same API as Histogram\")\n\telse:\n\t\treturn self.decompose()\n\n\n# histogram\n\n\ndef histogram(n1, n2=None, nfreq=None, **kwargs):\n\t\"\"\"Compute :obj:`Histogram` object.\n\n\tRequires ``n1 >= n2`` that are\n\n\t**Supported** in :class:`matplotlib.pyplot.Plot`\n\t("}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif other is not None and other == 0:\n\t\tself.scale_to(1, True)\n\t\treturn\n\n\tif scale is not None and other.is_float():\n\t\tself += other\n\t\treturn\n\n\tif type(other) is PHistogramBase:\t\t# must implement this\n\t\tself.scale_to(1, True)\n\t\treturn\n\n\tscale = self.scale_to(other, recompute)\n\tif scale is not None:\n\t\tif scale < P_MIN:\t\t# we are just"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif other is not None:\n\t\tother = type(self)(self, other)\n\n\tassert self.n, 'invalid histogram'\n\n\tif self.n == 0:\n\t\treturn other\n\n\treturn [\n\t\tscale(self.x0, other[0], recompute, copy=False) +\n\t\t\tscale(self.x1, other[1], recompute, copy=False)\n\t\t\t\t+ [other[-1]]\n\n#ifndef IGEST_DATA_FILE_H\n#define IGEST_DATA_FILE_H"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif other is None and (recompute or\n\t\thasattr(math, \"pi\") and math.pi!= self.pi):\t# sanity check\n\t\traise TypeError(\"cannot rescale without storing scale and pi\")\n\n\tif type(self.shape) is not list or type(other.shape) is not list:\t# self and other must be one or two dims\n\t\traise TypeError(\"incompatible shapes %s and %s\" % (type(self.shape), type(other.shape)))\n\n\tif (isinstance(other, histogram) or\n\t\tisinstance("}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\tif not other:\n\t\treturn [scale(self)]\n\n\tnewscale = self.scale\n\tif other == 0:\n\t\t_scale = [0]\n\t\treturn self._check_fit_error((newscale, _scale))\n\tnewscale[0] = other\n\treturn self._check_fit_error((newscale, _scale))\n\n\nclass _ScaledHistogram(HistogramBase):\n\tdef __init__(self, name, **kwargs):\n\t\tsuper(_ScaledHistogram, self).__init__(name, **kwargs)\n\n\tdef _"}]}
{"task_id": "62b87b519a0c4fa8b80b3583", "prompt": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tif self.shape!= other.shape:\n\t\traise Aeroerr(\n\t\t  \"Scale is only useful for 2D graphs, but provided a reshaped shape. \\\nPlease check the reshaped shape (2D array, array, tuple) with the function \\\nthe one argument'shape'.\")\n\telse:\n\t\tdef make_coords(x):\n\t\t\treturn [y for y in range(1,self.size) if x**int(y)]\n\t\t# Getting an empty list: the reshaped coordinates need to be rescaled\n\t\t"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tif other is None:\n\t\treturn scale(self.dimension)\n\tif isinstance(other, basestring):\n\t\tif other in ['x', 'y']:\n\t\t\treturn *self._scale_xby_y(other)\n\t\telif other in ['x', 'z']:\n\t\t\treturn *self._scale_xyby_z(other)\n\telif isinstance(other, (int, float)):\n\t\tif scale.dimension == 1:\n\t\t\treturn 1\n\t\telif scale.dimension!= other:\n\t\t\traise ValueError(\"Dimension %d and"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tif other is not None:\n\t\treturn Graph.scale(self, other, mode=self.mode)\n\tif self.type is not None:\n\t\treturn Graph.scale(self, other, mode=self.mode)\n\n\tx = 0\n\tif self.nlevels == 1: x = 1\n\ty = 0\n\tif self.nlevels == self.depth(): y = 1\n\tz = 0\n\tif self.depth()!= self.nlevels-1: z = 1\n\n\traise RuntimeError(\"Scale and rescale must be done \""}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t# Make the graph's coordinates a positive definite matrix, first\n\t# apply a random test to get the non-zero scale.\n\tif self.n == 0:\n\t\treturn (self.x, -0.5 * (self.y * math.sqrt(2.0) - 1.0),\n\t\t        math.sqrt(-2.0 * self.z * math.sqrt(2.0) * math.sqrt(2.0)))\n\telse:\n\t\tn = self.n + 0.5 * (self.z *"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tif not isinstance(other, (list, tuple)):\n\t\traise TypeError(\"expected string or list of strings.\")\n\n\tscale = scale = int(self.axes.get_yaxis().scale)\n\tif other == \"\":\n\t\tscale = 1\n\t\tshape = (self.axes.get_yaxis().get_major_formatter().formatter.get_unit()\n\t\t\t[0])\n\telse:\n\t\tscale, shape = other.size()\n\n\tif scale not in (1, 100):\n\t\t# TODO: Use a custom scale if"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tfrom numpy import arange, arange0, asarray, sqrt, sum\n\tx, y, z = other\n\tif len(x) == 0 and len(y) == 0:\n\t\t# x is a zero-size coordinate.\n\t\treturn self.shape\n\tif len(x)!= 0:\n\t\tif x.dtype == int:\n\t\t\tx = arange0(x.shape)\n\tif len(z)!= 0:\n\t\tif z.dtype!= int:\n\t\t\traise ValueError(\"input shape must be 0-dim,"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tresult = scale_func(self, other, axis=1)\n\tif isinstance(result, (list, np.ndarray)):\n\t\tresult = transform_xy2x2(result)\n\t\tif len(result) == 2:\n\t\t\tresult[0] = [0, 0]\n\t\t\tresult = result[1]\n\treturn result\n\n@util.deprecated_arg(\"2d\")\ndef rotate(self, other=None):\t\"\"\"\n\tGet or set the rotation of the graph.\n\nIf *other* is ``None``, return the rotation of"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tif isinstance(other, (float, int)):\t# For now, assume float/int\n\t\tscale = self.cscale(*other)\n\telif isinstance(other, (int, complex)):\t# For now, assume integer/complex\n\t\tscale = self.icscale(*other)\n\telif isinstance(other, (tuple)):\n\t\t\"\"\"\n\t\tGiven a list of the coordinate components, rescale\n\t\tcorresponding to the coordinate components to the scale of those\n\t\tcomponents.\n\n\t\tFor example, if a ``Tuple`` is returned for three scales"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\terror = _checkScale(\"scale\")\n\tif isinstance(self, Graph):\n\t\tif other is not None:\n\t\t\tself = scale(*other)\n\treturn self.scale(*self._params)\n\n\nclass Graph(GraphType):\n\t\"The base class for graphs.\"\n\n\tdef __init__(self, x=None, y=None, z=None, **params):\n\t\t\"\"\"\n\t\tInitialize a :class:`Graph`.\n\n\t\t:Parameters:\n\t\tx : array-like, optional\n\t\t\tThe x coordinates.\n\n"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\tg = copy(self)\n\tg.name = \"%s_scale\"%(g.name,)\n\tif other is not None:\n\t\tif isinstance(other, (float, int)):\n\t\t\tg.scale(other)\n\t\telse:\n\t\t\tg.scale(other, scale=False)\n\tif self._scale!=1:\n\t\t#print(\"self.scale:\",self.scale)\n\t\tg.scale(self._scale)\n\tif self.label!= g.label:\n\t\tg.label = self.label\n\tif"}]}
{"task_id": "62b87b869a0c4fa8b80b35e1", "prompt": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\tfrom numpy.polynomial.gaussian_filter import GaussianFilter\n\tfrom math import sqrt\n\n\tresult = Null()\n\tif hist.ndim == 3:\n\t    if isinstance(hist.x, bincls._histograms._bincls):\n\t\t# bin's domain has an axis that is always a histogram\n\t\tif make_value:\n\t\t    result = hist.get_bin()\n\t\telse:\n\t\t    # if we want to preserve the axis for the histogram, then\n\t\t    # change axis\n\t\t    bin = bincls(hist"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\traise exc.FileNotFoundError(\n\t\t\"This function cannot create a graph by calling 'hist_to_graph' \"\n\t\t\"with the input arguments.\"\n\t))\n\n\nHIST_TO_BINARY_HISTO = (\n\t(\"1\", hist_to_binary_histogram, 1)\n\t(\"2\", hist_to_binary_histogram, 2)\n\t(\"3\", hist_to_binary_histogram, 3)\n)\n\n\ndef hist_to_binary_histogram(f, hist):\n\t\"\"\"\n\n\tConvert a histogram operator into a binary sum"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t# The following values are required for all graphs of type \n\"graph\" or \"data\" (such as a list or graph)\nand the above options\nallow_duplicate = False\t# if True, allow duplicates in the graph (can be set at the very top level)\n#allow_duplicate=True\t\t# if True, allow duplicate attributes in the graph\nallow_duplicate_names = False   # if True, allow duplicate data names in the graph\n\ndef _find_points_by_field_value(field_names, field_values = \"\"):\n\t\"\"\"\n\t"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\tfrom. import histogram\n\tif get_coordinate == \"right\":\n\t\treturn hist.copy()\n\telif get_coordinate == \"left\":\n\t\tbins = histogram.bin_info(hist)\n\t\treturn hist.add_axis_bins(*bins, **attr)\n\telse:\n\t\thist = hist.copy()\n\t\tfor k, v in hist.vars.items():\n\t\t\tif not get_coordinate in (\"x\", \"y_mean\", \"y_mean_error\"):\n\t\t\t\thist.update_axis_binned_axis_values"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\timport numpy as np\n\n\tif not hist:\n\t\thist = numpy.empty((0,), dtype=np.int32)\n\thist._hist = hist\n\t\n\thist[0:4] = [(np.nan, (hist[0, 0],),),\n\t\t\t\t\t(np.nan, (hist[1, 0],),)]\n\thist[4:8] = [(hist.mean[0,0],),\n\t\t\t\t\t(hist.mean[1,1],),\n\t\t\t\t\t(hist.mean[2],),\n\t\t\t\t\t(hist.mean["}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\tfrom pyasn1 import univ  # pylint:disable=import-error, W0412, [py-3-9]\n\tfrom pyasn1.type.univ import univ_type  # pylint:disable=imported-builtin, [py-3-9]\n\tfrom pyasn1.type.pyobject import (PyLong, PyInt, PyFloat,)\n\t\n\t# pylint:disable=too-many-lines\n\ttry:\n\t\tfrom pyasn1.type.pyobject import decoder\n\t\ttry:\n\t\t\tval = int(hist[\"x"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\tfield_names = []\n\thist = []\n\thist_list = []\n\tfor i in range(len(hist)):\n\t\thist_list.append(hist[i])\n\n\tfor i, fieldtype in enumerate(field_names):\n\t\thist.append(hist_list[i])\n\n\tif type(field_names) == tuple:\n\t\thist = [[hist[f],hist[f] for f in field_names]]\n\t\thist_list = [[] for f in field_names]\n\t\tfield_name = field_"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\tif make_value is None:\n\t\tif get_coordinate is None:\n\t\t\traise TypeError('%s is required (missing make_value)' % hist_type)\n\t\treturn make_value(hist)\n\tif hist.ndim == 3:\n\t\taxis_names = get_axis_name(hist.ndim, hist.ndim)\n\t\tif axis_names is not None:\n\t\t\taxis_index = hist.shape[axis_names]\n\t\telse:\n\t\t\taxis_index = [0]\t\t\t# [0,]\n\t\t"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\treturn graph.binned_edges(hist, make_value=make_value, get_coordinate=get_coordinate)\n\n\ndef graph_as_array(graph, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a numpy array.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without\ncontext.\n\n*get_coordinate* define what the coordinate\nof a graph"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from libfrap.constants import FrapVizConstants\n\tif len(hist)!=1:\n\t\traise Exception, \"A histogram will never be produced\"\n\tif hist[0].info_str():\n\t\traise Exception, \"A histogram cannot be produced\"\n\tx,y = hist[:3]\n\tz = hist[1:] - hist[:1]\n\txscale = len(z) * FrapVizConstants.histwidth\n\txscale = max(xscale,1)\n\n\tif hist[0"}]}
{"task_id": "62b8b4baeb7e40a82d2d1136", "prompt": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif intf.providedBy is None:\n\t\traise ValueError, \"Provided interface must be supplied.\"\n\n\tif intf.providedBy not in iface.providedBy:\n\t\traise ValueError, \"Interface not being described by \" + iface.providedBy\n\t\tclass = iface.providedBy\n\t\traise errors.InvalidInterface\n\n\tif intf.providedBy not in candidate.providedBy:\n\t\traise errors.InvalidInterface\n\n\tif intf.providedBy.providedBy not in candidate.providedBy.providedBy:\n\t\traise errors.InvalidInterface\n\n\tif candidate"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t# FIXME\n\t# this test exists because Zope doesn't support all interfaces\n\t# within one implementation\n\n\t# verify interface's attributes\n\n\tif vtype is None and 'providedBy' in iface.providedBy:\n\t\traise InvalidConfiguration('provides', iface)\n\telif vtype is NotImplemented:\n\t\traise InvalidConfiguration('providedBy', iface)\n\n\tif tentative is None:\n\t\tif 'providedBy' in iface.suppliedBy:\n\t\t\traise InvalidConfiguration('providedBy', iface)\n\t\telif 'providedBy' in iface."}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif candidate==None:\t\treturn _empty()\n\tif vtype==None:\t\treturn _empty()\n\tif vtype==\"\":\t\t\tvtype=\"provider\"\n\tif vtype==\"provider\":\tvtype=\"interface\"\n\tif candidate.providedBy==\"None\": return False\n\tfor item in candidate.providedBy:\n\t\tif item.providedBy==\"None\": return False\n\tif candidate.providedBy==\"None\": return _empty()\n\tfor item in candidate.providedBy:\n\t\tif item.providedBy!=\"None\" and item.providedBy"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif candidate is None:\n\t\tdebugLog(\"No candidate\")\n\t\traise ValueError(\"No candidate\")\n\tcandidate._verify(iface, candidate)\n\tif iface in iface._providedBy.interfaces:\n\t\traise ValueError(\"More than one interface with the same candidate has been listed\")\n\tiface._providedBy.interfaces.add(candidate)\n\n\tif candidate in iface._providedBy.interfaces:\n\t\traise ValueError(\"This candidate has been overridden\")\n\tif vtype is None:\n\t\tif vtype in candidate._providedBy.interfaces:\n\t\t\traise ValueError"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif tentative == True:\n\t\tif hasattr(iface, \"providedBy\") and \\\n\t\t\t\t(not hasattr(candidate, \"providedBy\") or \\\n\t\t\t     \t(candidate.providedBy == iface.providedBy)):\n\n\t\t\t# If the interface is provided by the candidate then\n\t    # the method cannot be specified. We can only safely ignore\n\t    # the method if all known methods are present, including\n\t    # those already provided by the callee (such as\n\t    # ``validateBy <zope.interfaces.interfaces"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif candidate.providedBy == iface.providedBy and \\\n\t    candidate.provider == iface.provider and \\\n\t    len(candidate.methods) == len(iface.providedBy) and \\\n\t\tcandidate.methods[0] == iface.providedBy:\n\t\treturn True\n\n\t# Get the provider classes from the candidate, even if they are\n\t# in the wrong order.  This preserves the ordering of the errors\n\t# reported on the previous call.  Otherwise, the order is preserved\n\t# between the invocations.\n\tprov_cls = candidate"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tdef check(iface, candidate, tentative=False):\n\t\tif candidate.providedBy is not None:\n\t\t\traise zope.interfaces.invalid_interface(\n\t\t\t\t\t\t\tcandidate.providedBy.name)\n\n\t\tif candidate.providers is not None:\n\t\t\t# A Provider only has to have a 'provides' attribute.\n\t\t\t# A Provider has 'providers' in its constructor.\n\t\t\terrorList = []\n\t\t\terror = []\n\t\t\tfor provider in candidate.providers:\n\t\t\t\tif provider.provides is None or provider.provides =="}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tiface = _get_interface(iface)\n\tif candidate == iface:\n\t\treturn True\n\tresult = True#zope.interface.Invalid\n\n\t# The following check is to ensure that it is successful:\n\t# - If *candidate* does not define a method or attribute\n\t#   that is not present, or if it is present, and it is definitely\n\t#   valid to call itself.\n\t# - If *candidate* does not define a method or attribute\n\t#   that is not present, and it is definitely valid to call itself"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif not all(r for r in candidate.methods if _are_methods_present(r.name)):\n\t\traise zope.interface.Invalid\n\tif not all(r for r in candidate.methods\n\t           if not _must_declare(r.name)):\n\t\traise zope.interface.Invalid\n\tif not all(r for r in candidate.methods\n\t           if not _must_provide(r.name, tentative)):\n\t\traise zope.interface"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif tentative: # or just use old version\n\t\tif not cls.providedBy.has_method(candidate): # or just use the new version\n\t\t\traise zope.interfaces.InvalidInterface\n\n\tif not iface: # or just have a valid interface\n\t\tif not vtype: # or just have the same type\n\t\t\traise zope.interfaces.CannotProvideInterface\n\n\tif not (candidate in iface.providedBy): # or the interface is an instance of the current interface\n\t\t\traise zope.interfaces.InvalidInterface\n\tif not"}]}
{"task_id": "62b8b4baeb7e40a82d2d1137", "prompt": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif tentative and (_class_has_no_implementer(iface) or\n\t    _class_has_no_provided_by(iface)):\n\t\treturn _valid_iface(candidate)\n\telse:\n\t\tpass\n\t\treturn None\n\ndef get_class(iface):\n\t\"\"\"\n\tRetrieves the class instance that best describes the interface.\n\n\t:param iface: The interface object for which to retrieve the class.\n\t:return: The class object for the interface.\n\t\"\"\"\n\treturn _class_object(iface.class_"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tmethods = set(iface.providedBy)\n\tif cand_methods:\n\t\tfor method in cand_methods:\n\t\t\tmethods.discard(method)\n\t\tmethods.discard(iface)\n\tfor method, parameter in iface.declaredProperties.items():\n\t\tif not methods.issubset(parameter):\n\t\t\tmethods.add(method)\n\t\tif not methods.issubset(candidate):\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t(\"Method %s and type %s not defined in interface of %s.\")%(\n\t\t\t\t"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t# If this checks if there is an error, then return the first one found\n\tif not iface.providedBy():\n\t\tif isinstance(iface, str) and (iface.providedBy() or iface.providedByDecl):\n\t\t\tpass\n\t\telse:\n\t\t\treturn False\n\tcandidate._validateInterfaces()\n\t# Making sure the candidate contains everything\n\tfor member in iface.providedBy():\n\t\tif member in candidate._possibleMethods(member):\n\t\t\tif tentative:\n\t\t\t\terrMsg = \"Tentatively, it indicates that %s does"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif not candidate.providedBy(iface):\n\t\traise zope.interface.Invalid(\n\t\t\t\"Interface %s does not provide the interface %s by %s\" % (\n\t\t\t\tcandidate.name, iface.qualifiedName, iface.name)\n\t\t)\n\tif any(iface.providedBy(tentative) for tentative in [True, False]):\n\t\traise zope.interface.Invalid(\n\t\t\t\"Candidate %s does not provide a required interface %s by %s\" % (\n\t\t\t\tcandidate.name, iface.qualified"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tcaller = caller()\n\tcallerClass = callerClass()\n\tif caller is None or callerClass is None or candidate not in (None, True):\n\t\tif method not in candidate:\n\t\t\tif method not in iface._validMethods:\n\t\t\t\traise ValueError('Method: %s is not valid %s, as a candidate can fail for %s.' % (method, candidate, iface))\n\t\tmethod = method\n\t\tcandidate = candidate\n\t\tiface = iface\n\t\treturn True\n\tif type(candidate) is not type(iface):\n\t\traise TypeError('"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tiface = ZopeInterface(candidate.__self__)\n\tif (iface.providedBy == iface.provides):\n\t\traise zope.interface.DuplicateInterfaceError(\n\t\t\t\"Method \", candidate.__class__.__name__,\n\t\t\t\" has duplicate uses.\",\n\t\t)\n\t\t\n\treturn candidates.validateObject(iface, candidate)\n\n// Generated class v2.50.0.0, don't modify\n\nusing System;\nusing System.Collections.Generic;\nusing System.Collections.Specialized;\nusing System.Linq;\nusing System.Text;\n\n"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tfrom zope.interface import implementer\n\tiface = implementer(candidate)\n\tif iface.providedBy not in iface.providers:\n\t\treturn False\n\tif not iface.providedBy in iface.providers:\n\t\treturn False\n\tfor attr in getattr(iface.providedBy, \"__getattribute__\", None):\n\t\ttry:\n\t\t\tmethods = getattr(iface, attr)\n\t\texcept AttributeError:\n\t\t\tcontinue\n\t\texcept Exception as exception:\n\t\t\traise zope.interface.Invalid(\"%r\" % exception)\n\t\tfor meth"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t# Find the best candidate method\n\tbest = []\n\tvar candidates = []\n\tfor i, candidate in enumerate(candidateMethods):\n\t\tcandidate = candidate.name\n\t\tattrs = candidateMethods[candidate].getMethodsOfType(iface)\n\t\tif len(candidateMethods[candidate].interfaces) == 1:\n\t\t\tmethods = candidateMethods[candidate]\n\t\telse:\n\t\t\tmethods = candidateMethods\n\t\tmethod = getattr(attrs, candidate)\n\t\tmethods.extend(method.getMethods())\n\t\tmethods.extend(method.providedBy.getMethods"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\tif candidate in sys.unimplemented:\t# XXX check interface's method\n\t\tfrom zope.interface.implementer import implementer_class\n\t\tiface_provides = {}\n\t\tiface_provides[\"interface\"] = implementer_class\n\t\tiface_provides[\"provider\"] = implementer_class\n\t\tiface_provides[\"methods\"] = set()\n\t\tiface_provides.update(candidate.__dict__.items())\n\n\t\tfrom zope.interface.implementer import supportsMethod\n\t\tif not supportsMethod(iface, candidate):\n\t\t\tprint \""}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\ttry:\n\t\treturn candidate.providedBy.equals(iface) \\\n\t\t\t\t\t\t\t\t\t&& candidate.provides.equals(iface) \\\n\t\t\t\t\t\t\t\t\t&& candidate.method.equals(iface) \\\n\t\n\texcept AssertionError, verr:\n\t\treturn True \n\n\treturn False\n\npackage com.android.server.notification;\n\nimport android.app.PendingIntent;\nimport android.app.PendingIntent.CanceledException;\nimport android.app.TaskStackListener;\nimport android.common.HwFrameworkFactory;\nimport android.content.BroadcastReceiver;\nimport android.content"}]}
{"task_id": "62b8b4c1eb7e40a82d2d1139", "prompt": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tif candidate in interfaces_class:\n\t\treturn  # The candidate already exists, nothing to do.\n\tfor iface_ in candidate.interfaces().all():\n\t\tif iface_ == tentative:\n\t\t\t# tentatively, the candidate actually wants to be able to\n\t\t\t# retrieve the iface for tentatively.\n\t\t\treturn\n\n\t# If it might not, recheck for any interface.  This will only fail if\n\t# it is listed explicitly in the 'known interfaces' of a specific\n\t# instance, whether it's tentatively or directly on"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\ttry:\n\t\tif not tentative:\n\t\t\treturn candidate.checkerClass()\n\t\treturn candidate.checkerClass(iface, verify=False)\n\texcept:\n\t\traise\n\ndef test_classes_when_no_match(iface):\n\traise TypeError(\"Must provide a interface that does not match.\")\n\ndef test_classes_w_exactly_all(iface):\n\texcept TypeError:\n\t\traise TypeError(\"No match provided. Exact match.\")\n\ndef test_classes_w_never_exactly_all(iface):\n\texcept TypeError"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tentrepr = \"%s has a %s method which takes *iface* and returns an object.  The following methods are listed.  The\\n\" % (__doc__, candidate.__name__)\n\twhile tentative and \\\n\t\t(iface in entrepr or \\\n\t\t(iface in entrepr.methods and iface.methods.__dict__ and isinstance(iface.methods, (str, unicode))))+\n\t\t' methods should also be marked as unsafe.%s' % ('.' if ununsafe_warnings!= 0 else ''): name = 'unsafe%s' % iface.__name__ if"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tiface = candidate['iface']\n\tif iface not in iface_cache:\n\t\tiface = candidate['interface']\n\t\tiface_cache[iface] = iface\n\t# check for collision of other interfaces\n\tif candidate not in iface_cache:\n\t\tiface_cache[candidate['iface']] = iface\n\treturn iface_cache[candidate['iface']]\n\n\ndef testPort(port, iface):\n\t\"\"\"\n\tAssert that the port address is properly set by the port layer.\n\t\"\"\"\n\ttry:\n\t\tport.bind([interfaceToL3Protocol("}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tif not isinstance(iface, X509Class):\n\t    raise ValueError(\"Expected X509Class, got %s\"%iface.__str__())\n    if not candidate:\n\treturn\n    t = _asn1.DERInteger.from_string(candidate) & 0xffffffff\t\t\t# 1...255\n    if t is not _asn1.BERInteger.Bogus:\n\traise ValueError(\"Expected *candidate* to be of length %d, was %d\"%(len(candidate), t.value))\n    if interfaceClass(candidate).name"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tif iface is None:\n\t\tiface = _getiface(cand)\n\tdefer print(candidate)\n\n\tassert tentative or iface.canmanage.__class__.__names__ == ['Iface', 'IEMobile']\n\n\tfor ifacename in iface.canmanage.__class__.__names__:\n\t\tifacename = ifacetype.to_string(ifacename, 'interfaces')\n\n\t\tif ifacename in iface.canmanage.isupported__names__:\n\t\t\tsupport = get_interface_support(candidate)\n\t\t\tif type(support) not in ('interface"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tif iface not in candidate.class_iface:\n\t\traise ClassError(\"Could not determine class for %s\" % candidate)\n\ndef verifyInterfaces(candidates):\n\t\"\"\"Assert that *candidates* are of *class type*.\"\"\"\n\tinterfaces = []\n\ttry:\n\t\titer(candidates)\n\t\tinterfaces = [c for x in candidates for c in x.iterclass()]\n\texcept NotImplementedError:\n\t\tinterfaces = []\n\tif not interfaces:\n\t\traise ValueError(\"Could not find any classes in %s\" % candidates)\n\ndef checkInterfaces(candidate"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t#\tprint \"CHECKING <\"+iface+\">\"\n\t#\tassert 1, \"Candidate classes, \"+iface+\" aren't already. See above for more info.\"\n\tif hasattr(protodef, \"__call__\"):\n\t\tfor cdef in protodef.__call__(iface):\n\t\t\tassert tentative or isinstance(cdef, candidate), \"Incorrect candidate interface class - \"+\n\t\t\t\tprotodef.__class__()\n\t\treturn\n\tassert \"object.__new__\" not in repr(Candidate)\n\tt = candidate.__call__(iface)\n\tif t:\n\t\t"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\tif not candidate.canProvideClass(iface):\n\t\treturn\n\tif True:  sudo('check for %s interface in %s' % (iface, candidate),\n\t   \"  %s\\n\" % sudo.cyan(\"#%s\\n\" % iface, candidate))\n\tif '*' in candidate:\n\t\tsudo('check for %s interface in all remaining interfaces in %s' % (iface, candidate),\n \t\t  \"  %s\\n\" % sudo.cyan(\"#%s\\n\" % iface, candidate))\n\tif tentative:"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\ts.check_interface(candidate, iface)\n\ndef verifyType(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly give a type *iface*\t\"\"\"\n\ts.check_type(candidate, iface)\n\ndef is_member(member_name, member):\n\t\"\"\" Check existence of member name.\n\t@param member: member\n\t@param member_name: member whose existence\n\t\"\"\"\n\trv = name_in_type(member_name, type_for_member_def, member=member)\n"}]}
{"task_id": "62b8b559eb7e40a82d2d11f6", "prompt": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t", "suggestions": [{"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tif explicit_mc is not None:\n\t\tmeta = [base]\n\t\tfor base in bases:\n\t\t\tif not is_metaclass(base):\n\t\t\t\tcontinue\n\t\t\tif explicit_mc is not None and not is_metaclass(explicit_mc):\n\t\t\t\tcontinue # same metaclass\n\t\t\tif base not in meta:\n\t\t\t\tmeta.append(base)\n\n\t\treturn set(filter(\n\t\t\tlambda a: a not in bases and a not in explicit_mc and is_metaclass(a),\n\t\t\tmeta))\n\t"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\targs = [a for a in bases if explicit_mc is None or a is missing ]\n\tif len(args) > 1:\n\t\tprint('{0} metaclass'.format(args[0]), file=sys.stderr)\n\t\tsys.stderr.flush()\n\t\tsys.exit(1)\n\treturn args[0], args[1]\n\n\ndef determine_metaclass(cls, explicit_mc=None):\t\t\t\"\"\"\n\tDetermine metaclass from cls from 1+ bases.\n\t1. Check if explicit_mc or None are provided. False"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tif explicit_mc is not None:\n\t\ttypes = (t0, t1) for t0, t1 in types_map.items()\n\t\treturn (type(klass) if isinstance(klass, type) else\n\t\t\ttuple_map.get(type(klass)) for klass in types)\n\ndef get_types_from_metaclass(cls, type_list=None):\n\t\"\"\"Retrieve type list based on its metaclass type name (default \"type()\").\n\n\tTypically, the list of all registered type (types) is defined. However,\n"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tbase = bases[0]\n\tif explicit_mc is not None:\n\t\treturn str(base.__module__) + \".\" + str(explicit_mc.name)\n\telif base.__module__ == \"__metaclass__\":\n\t\treturn __metaclass__(bases[1], bases[2])\n\telse:\n\t\treturn base.__class__\n\n\ndef __get_cached_class__(bases):\n\t\"\"\"\n\tReturn the cached __class__ of some module, i.e.'self'\n\t(class and method)\n\t\"\"\"\n\tif \"_Metaclass"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from.metaclass import Meta\n\tmetaclass = Meta()\n\tmetaclass.metaclass = Meta\n\n\tfor base in bases:\n\t\tif explicit_mc and explicit_mc.__name__ == base[:13]:\n\t\t\tfor m1 in explicit_mc:\n\t\t\t\tif m1.startswith(base):\n\t\t\t\t\tpass#IMPORTIMPORTIMPORT from.import_module import specialize_metaclasses\n\t\t\t\t\tmetaclass = SpecializeMetaclasses()\n\t\t\t\t\tbreak\n\t\telse:\n\t\t\tpass#"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tflags = sys.flags\n\tif explicit_mc is None:\n\t\texplicit_mc = set()\n\t\tfor base in bases:\n\t\t\tif not (issubclass(type(base), metaclass) or issubclass(type(base), Sequence)):\n\t\t\t\tcontinue\n\t\t\tfor item in base.__dict__:\n\t\t\t\tif issubclass(item, metaclass):\n\t\t\t\t\texplicit_mc.add(item.__name__)\n\t\t\tflags.setdefault('__metaclass__', explicit_mc)\n\t\tflags.setdefault('__metaclass__', '')\n\treturn flags"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t# See: http://bugs.python.org/issue1298#msg193382\n\t# Try to figure out what metaclass this is, if not we're looking at __metaclass__\n\tif '__metaclass__' not in bases:\n\t\t# Not a metabolator!\n\t\treturn None, ''\n\treturn bases[0].__metaclass__, ''\ndef determineClassPrefix(bases=False):\n\t\"\"\"\"\n\tDetermine class prefix from 1+ bases and optional explicit __class__\t\"\"\"\n\tif bases:\n\t\tif bases[0"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tfor base in bases:\n\t\tif isinstance(base, baseclass.MetaclassObj):\n\t\t\tbreak\n\tif explicit_mc is None:\n\t\tbase = base.__class__\n\t\tmc = explicit_mc\n\tfor base in bases:\n\t\tif mc is None:\n\t\t\ttry:\n\t\t\t\tbase = base.__bases__[0]\n\t\t\t\tcontinue\n\t\t\texcept AttributeError:\n\t\t\t\tcontinue\n\t\tif not base._metaclass:\n\t\t\tif explicit_mc is not None:\n\t\t\t\tbase = base.with_metaclass("}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tall_bases = set(bases)\n\tfor base in all_bases:\n\t\tif base in '__metaclass__':\n\t\t\tcontinue\n\t\telif explicit_mc and base not in explicit_mc:\n\t\t\treturn base\n\treturn 'Uncaught Unhandled Exception', None\n\n\n@register_exception\nclass RuntimeError:\n\t\"\"\"\n\tThe most common cases of this exception are:\n\t \t:py:code:`from typing import *` -- this is a decorator, so we need to\n\t\t\tbe sure that you set up all the annotations in each of the\n"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\tfor i, base in enumerate(bases):\n\t\tif (base.is_builtin and i > 1) or (type(base).__name__ == '__metaclass__'):\n\t\t\tif not explicit_mc and \\\n\t\t\t\t(base in inspect.getfullargspec(base.__module__) or \\\n\t\t\t\t(base in inspect.getargspec(base.__module__))):\n\t\t\t\treturn base.__name__\n\t\t\tbreak\n\tif not explicit_mc:\n\t\treturn None\n\traise ArgumentError(u\"Meta has not been implemented because the metaclass is not"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47d", "prompt": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tif len(self._keys) < len(key):\n\t\tself._keys.append(self.__class__(k))\n\n\treturn self._values[key][len(self._keys[0]) + len(self._keys[1])]\n\n\ndef rename_keys(source, target, default=__marker):\t\"\"\"\n\tD.rename(source[key][[...]]) -> value_to_rename\n\n\treturn self.replace(key,target[key][0],target[key][1])\n\"\"\"\n\ndef unmangle(self):\n\tglobal _"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tkey, __marker = _check_key(key)\n\tif key in self:\n\t\t__marker = self.get(key)\n\t\treturn default, self[key]\n\telse:\n\t\tif default is None:\n\t\t\traise KeyError(\"pop failed; key not found\")\n\t\treturn default, default()\n\n\t\ndef delitem(self, key):\n\t\"\"\"\n\tD.delItem(k[,d]) -> v, delete specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tif not self: raise KeyError(\"Cannot pop from an empty container.\")\n\tif key in self.data: self.data.pop(key, default)\n\treturn self.data[key]\n\ndef insert(self, k, datum):\t\"\"\"\t\t# Insert a value for a key or set of keys.\nInserts a specified value if key is not already there or if a value is found for the given key.\nInserts the key value tuple (k,v) (if the key is not listed, the default value is returned.)\nIf k is not NULL it means that a"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\ttry:\n\t\treturn self._table[key]\n\texcept KeyError:\n\t\tif default!= __marker:\n\t\t\treturn default\n\t\telse:\n\t\t\traise KeyError('unknown key {!r}'.format(key))\n\n\t_key = key\n\n\twhile 1:\n\t\tv, d = _getkey(self._table, _key)\n\t\tif is_int(d):\n\t\t\tbreak\n\t\telif is_float(d):\n\t\t\tif v.value is None:\n\t\t\t\traise KeyError('Key {!r} has no"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tif isinstance(key, _Key):\n\t\treturn self._pop_item(key)\n\n\t# Returning all other keys raises KeyError when key is not found.\n\tif key isn't dict.empty and key.keys is not self._key1:\n\t\tif key.get(self._key2, _MARKER) is None:\n\t\t\traise KeyError(key)\n\t\telse:\n\t\t\t# If getkey returned None, self.pop is to be overridden in case a user\n\t\t\t# wants to get a different key.\n\t\t\tself._pop_"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tdel self.pop_impl(*arguments, key)\n\tif default is not __marker:\n\t\tif key not in self[default]:\n\t\t\traise KeyError(key)\t# KeyError means KeyError but was not found\n\t\tresult = self[default][key]\n\t\tdel self[default][key]\n\t\treturn result\n\treturn self[DefaultKeys()[default]][key]\n\n#\n# Lookup class.  This does not provide an interface for reading or accessing the\n# data, and only serves as an example for storing.\tIt is an error for keys as\n"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tv = self.pop_key(key, default)\n\treturn v\n\n\tdef pop_default(self, key, default=__marker):\n\t\t\"\"\"\n\t\tD.pop_default(k[,d]) -> v -> k is the key of D.\n\t\tIf key is not found or the default is not specified, raises KeyError.\n\t\tFor example if D.remove(6) is called, key = 0, d = 6 will be returned.\t\"\"\"\n\t\tv = self.pop_default_key(key, default)\n\t\treturn v"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tif type(key) == type(self.marker):\n\t\tif __marker == default else __marker\n\ta = self\n\tfor k, v in self.items():\n\t\tif key in v:\n\t\t\ta = a.pop(k, v[key])\n\treturn a\n\ndef pop_default(self, key, default=__marker):\n\t\"\"\"\n\tD.pop_default(k[,d]) -> v, return the corresponding value if given, otherwise KeyError is raised.\n\t\n\tIf key is not found, d is returned if given, otherwise"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\tif len(self.dict)-1 >= key:\n\t\treturn default\n\telif self.dict.has_key(key):\n\t\tself.dict.remove_key(key)\n\telse:\n\t\tif self.dict.has_key(k):\n\t\tdel self.dict[k]\n\t\tsys.stderr.write(\"Warning:: D.pop key not found [%s]\\n\" % k)\n\t\treturn default\ndef reset_random (randseed,n):\n\t\"\"\"Returns the current random seed (not used in the random class of the"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\ttry:\n\t\treturn self.h[key]\n\texcept KeyError:\n\t\treturn default\n\n#-----------------------------\n\ndef dict_from_file(filename):\n\tr\"\"\"\n\tLoads data from a JSON file or a file object for import into the\n\tdictionary.\n\t\"\"\"\n\twith open(filename,\"r\",encoding=\"utf8\") as f:\n\t\trdict = json.load(f)\n\treturn rdict\n\nclass JsonDict(DictBase):\n\t\"\"\"\n\tBase JsonDict subclass that holds the dictionary data as key-value pairs"}]}
{"task_id": "62b8d23748ba5a41d1c3f497", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\ttempcount = 0\n\tfor o in self:\t\t#for each key\n\t\tfor k, v in o.items():\t\t#and values in each list\n\t\t\tif (k == self) and not (o[k].is_list()) and not (v[self.islist()]):\n\t\t\t\t#pop the smallest element\n\t\t\t\tdelete_element(self.popitem(), k)\n\t\t\t\ttempcount += 1\n\t\t\t\tassert not (k == self)\n\treturn tempcount\n\n\n#------------------------------------------------\n#-------------------\n#     "}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t# This is not thread-safe.  I would like another solution.\n\ts = []\n\tfor k, v in self.items():\n\t\tif k!= k and (k in list(s)) or (k in list(self.items())):\n\t\t\ts.remove(k)\n\tif self.size() < 1:\n\t\treturn\n\treturn sorted([s.pop(k) for k in self.keys()]), s\n\ndef _iteritems(p):\n\tfor item in p:\n\t\tyield item\n\t\t#print item\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\tself._num_used = 0\n\tself._num_unused = 0\n\n\tdef popitem_check(p):\n\t\t\"Remove an element. If `key` is an `(key, value) <` Pure` it is not \" +\n\t\t\"stored. Otherwise raise an exception.\"\n\t\tpopitem(self)\t# remove the Pure element\n\t\tpopitem(p)\t# self.pop(p) is the same as popitem(p)\n\t\tp.next = None\t# no more uses\n\t\treturn p\n\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\tfor i in self.keylist:\n\t\tif self._keys[i] > self._keys[i+1]:\n\t\t\tself.keylist.remove(i)\n\ndef list_pop(self):\t\"\"\"\n\tRemove the first item (or all items) from the deque, and return the item list.\t\"\"\"\n\tfor i in self.keylist:\n\t\tnext(self.keylist)\n\t\tself.keylist.remove(i)\n\treturn self\n\ndef _copy(self):\n\tfrom copy import copy\n\ttemp = self"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\tl = len(self.hash)\n\tif l > 1:\n\t\tfor i, x in zip(self.hash, self.key):\n\t\t\tm = (x,l-i-1)\n\t\t\tself[m] #remove the most frequently used key\n\t\treturn x\n\telse:\n\t\treturn self.lookup(self.hash)\n\n/*\n * SPDX-License-Identifier: Apache-2.0\n *\n * The OpenSearch Contributors require contributions made to\n * this file be licensed under the Apache-2.0 license or a\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\tself.clear(self.key)\n\tself._nitems -= 1\n\n\treturn self, self[key]\n\ndef pop_front(self):\t\"\"\"Remove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\tself.clear()\n\tself._nitems -= 1\n\n\treturn self\n\ndef add(self, x):\t\"\"\"Add x to the list.\t\"\"\"\n\tself.push(x)\n\ndef sub(self, x):\t\"\"\"Substract x to the list.\t\"\"\"\n\tself[0] -= x"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\tresult = []\n\t# First, find the highest 'value'\n\tlow = self.size\n\twhile len(self) > low:\n\t\tcurrent = self[low]\n\t\tif current not in self:\n\t\t\t# Found a match and add it to the list.\n\t\t\tif low < len(self):\n\t\t\t\t# There are more matches on this list, return directly\n\t\t\t\tresult.append((current, self[low]))\n\t\t\t\treturn result\n\t\t\t# The list is not sorted by size, so add the new key and value pair\n\t\t\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\tf = self._findfreefunc(self.getiter)\n\tpopitem = self.module.getfunc(f.popitem)\n\tpopitem.func = self.func\n\tpopitem.nargs = f.nargs\n\tpopitem.name = f.name\n\tpopitem.doc = \"Removing this key/value pair least frequently used.\"\n\treturn popitem\n\n\t# The newfunc has to be def'ed since the key/value\n\t# pairs (i.e. their associated functions will\n\t# be called) are now private to the user."}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\tself.dict = sorted(self.dict)\n\tself.popValue()\n\n\ti = 0\n\tresult = []\n\twhile i < len(self.dict):\n\t\tkey, value = self.dict[i]\n\t\tif (key, value) in (('prev','start'), ('prev', 'end')):\n\t\t\tself.dict.pop(i)\n\t\tresult.append((key, value))\n\t\ti += 1\n\n\treturn result\n\n\t# popkey returns the (key, value) from the list specified\n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\treturn popitem_fast(self.data.index, self.data.lengths)\n\n\t\ndef popitem_fast(index, lengths):\n\t\"\"\"\n\tFind the key to which the given length comes.\n\t\"\"\"\n\tfor i in range(index + 1, len(lengths)):\n\t\tif lengths[i] > lengths[index]:\n\t\t\treturn popitem(i)\n\n\ndef findindex(iterable, key = None, reverse=False):\n\t\"\"\"\n\tFind the index of a sequence.\t\t\"\"\"\n\tindex = 0\n\twhile True"}]}
{"task_id": "62b8d23a48ba5a41d1c3f499", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t# First, search for a match.\n\tself.check_dict(self.dict, False)\n\tif isinstance(self.dict[key], (list, tuple)):\n\t\tfor i, (key, value) in enumerate(self.dict[key]):\n\t\t\tif (key, value) in self.dict:\n\t\t\t\tself.dict.pop(key)\n\t\t\telse:\n\t\t\t\tdel self.dict[key]\n\t\t\tif isinstance(value, list):\n\t\t\t\tvalue.extend(self.dict[key])\n\t\t\t\tself.dict["}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\tg = key = self.g = -1\n\tfor v in self.pop():\n\t\tif v == self.key:\n\t\t\tg = len(self.l)\n\t\t\treturn \n\treturn (self.key, self.l[g])\n\n#define DALL_DEMOSHADER 2\n#include <dae/daeDocument.h>\n#include <dae/daeMetaCMPolicy.h>\nclass DAESCRIPT_EXPORT daeRenderPrograms : public daeDocument\n{\n\tdaeMetaCMPolicy *_render;\npublic:\n\t/**\n\t *"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\tself.remove(self.key)\n\treturn self.key, self.value\n\n\n# From http://www.codesourcery.com/archives/boost_associative_hash.html\ndef clear_leftmost_numbered_element(self, n):\n    self.leftmost_numbers[n] = self.rightmost_numbers[n]\n\n\ndef recur_right_most_numbers(self, n):\n    u = self.rightmost_numbers.keys()\n    while u in self.leftmost_numbers"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\treturn self.pop(0)\n\n\t# Popall\n\n\t# If there is only one item, return and pop it.  If all popping is\n\t# successful, call `pop` again, which will return `True` with the\n\t# item we found.\n\tdef popleft(self):\n\t\treturn self.popitem()\n\n\t# If `first` is still available, return `True` and set the item we found\n\t# to `(None, None)`.\tIf no item is available, return `None` and\n\t# set the"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\tpoppedict = deque()\n\tfor p in self.root:\n\t\tpoppedict.append((p.key, p.value))\n\treturn poppedict\n\ndef popallitems():\t\"\"\"\n\tRemove all items in a list. Returns the number of items removed, -1 if not found.\t\"\"\"\n\ti=0\n\tif i!= len(self.root):\n\t\tfor p in self.root[i+1:]:\n\t\t\tif p in self.root:\n\t\t\t\tself.root.pop(i)\n\t\t\telse:"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\tif not self.__dict__['_key']:\n\t\traise KeyError('popitem: _key not set')\n\n\treturn self.popleftmost('_key')\n\n\t#\n\t# `popLeft`, returning it instead.\n\t#\n\tdef popleft(self):\n\t\treturn self.popitem('_key',\n\t\t\t(key, value) for key, value in self.__dict__['_key']\n\t\t)\n\n\t#\n\t# `popRight`, returning it instead.\n\t#\n\tdef popright(self):\n\t\treturn self."}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\tprint(\"Popping item.\")\n\n\t# Find the key\n\tkey_list = [\"[class], [ID],[class], [ID]][[class], [ID]][[class], [ID]][[class], [ID]][[class], [ID]][[class], [ID]][[class], [ID]][[class], [ID]][[class], [ID]][[class], [ID]][[class], [ID]][[class], [ID]][[class], [ID]][[class], [ID]][[class], [ID]][[class], [ID]][[class], (ID)][["}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\tnewpair = self.pop(False)\n\tfor key, value in newpair.items():\n\t\tif key.key in self:\n\t\t\tself.remove(key)\n\t\telse:\n\t\t\tself[key.key] = value\n\treturn newpair\n\n\n#\n# Utilities\n#\n\ndef parse_dict_size(*d):\n\t\"\"\"Given a dictionary of dictionaries, return the list of keys and their size.\n\t\"\"\"\n\t# Check for nonexisting items. Raise exception if found.\n\tif d:\n\t\tkeys = d"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\tpop_index = pop(self, 1)\n\tif len(pop_index) > 1:\n\t\tpop = pop[pop_index[0]]\n\telse:\n\t\tpop = ()\n\treturn pop, pop_index\n\n\ndef set_random_state(func):\n\t\"\"\"The type of :class:`.random_context` method is used to initialize\n\tA :class:`.rng_state` via :class:`__init__ <RNG>` calls.  In\n\tthis function, however, the context is a :class:`.Random` context,"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t# Get the key.\t\t\t\t\t\t\t\n\treturn self.items.popitem()\n\n\t\ndef __len__(self):\n\t\"\"\"Return the length.\t\n\t@rtype: Million/Quantity, [Million/Quantity], Nil\n\t\"\"\"\n\treturn self.items.size\n\n\ndef __getattr__(self, attrname):\n\t\"\"\"Get an item from the list, or raise KeyError if it doesn't exist.\t\"\"\"\n\ttry:\n\t\treturn self.items.__getattr__(attrname)\n\texcept NameError:\n\t\traise AttributeError(attrname)\n\n\n"}]}
{"task_id": "62b8d23c48ba5a41d1c3f49b", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\tresult = []\n\tself.dict.popitem(result.append)\n\treturn len(result) - 1\n\n\t\n_cache = None\ndef _cache_size(self):\n\t\"\"\"Count the maximum items with the largest size in order kept.\"\"\"\n\tif _cache is not None:\n\t\treturn _cache.count()\n\t_cache = len(key=_cache)\n\treturn _cache\n\t\n_cache_maxsize = _cache_size()\n\n\ndef _cache_get(self, key, def_val):\n\t\"\"\"Returns `key`"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\tfor entry in self.dict:\n\t\tif entry[1]!= 0:\n\t\t\tself.dict.pop(entry[1]) # this will become the only one.\n\t\t\tbreak\n\n\tassert isinstance(self.dict, dict)\n\tassert dict in self.dict, 'dict not in dict'\n\treturn self.dict[dict]\n\ndef _sort_by(dict, index, key):\n\t\"\"\"Reindex some keys and make the sorted entries.\n\n\tThis function takes into account that index will be a valid `key` value.\n\tThe"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t# make these lists non-empty\n\t# each list item may be a list of tuples.\n\toldKeys = [x for x in list(self.keys()) if x!= \"\"]\n\tnewKeys = [x for x in oldKeys if x == \"\"]\n\tkeysUsed = set()\n\tfor oldKey in oldKeys:\n\t\ttry:\n\t\t\tkeyUsed = oldKey.split(',')[0].lower().strip()\n\t\t\tkeysUsed.add(keyUsed)\n\t\texcept ValueError:\n\t\t\tpass\n\n\tkeysRemoved = keysUsed - newKeys\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\tself.reset_memory()\n\n\t# Note that in non-atomic case this only works, so we can't just\n\t# return None here.  It's also worth storing a lock on all objects\n\t# that can't be accessed and we can't call these functions anyway.\n\tif not self.is_atomic:\n\t\tkeys_list = os.listdir(self.base_stack_base_dir)\n\t\tfor path, item in zip(keys_list[:-1], self.stack_keys):\n\t\t\tif item is None:\n\t\t\t\tcontinue\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\tf = self.fp.popitem()\n\tresult = [None, None, self.get_result_value(f)]\n\tself.fp.update(f)\n\treturn result\n\n\t\n\t\nif __name__ == '__main__':\n\timport sys\n\tresult = popitem()\n\tif result == None:\n\t\tpass\n\telse:\n\t\tprint(json.dumps(result))\n\n//Copyright 2018 James Cliff Chang\n//\n//This program is free software; you can redistribute it and/or\n//modify it under the terms of the"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\tself.iteritems(1)\t# for consistency\n\tres = self.i[0]\n\tfor k,v in zip(self.entries,self.entries[1:]):\n\t\tif k > res:\n\t\t\tself.entries.pop(k)\n\t\t\tif k == res:\n\t\t\t\t# self.entries = list(self.entries)\n\t\t\t\tdel self.entries\n\n\tif self.lastkey not in self.iteritems(1):\n\t\tself.lastkey_not_used=None\n\t\tself.lastkey=None"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\ttry:\n\t\treturn self._dict.popitem()[1]\n\texcept KeyError:\n\t\tself.clear()\n\t\treturn None\n\n\ndef create_new_from_dict(db, dict):\n\t\"\"\"\n\tCreate a new DB from its database dictionary, and possibly a list of\n\tdictionaries.  Note that this function accepts only database-generated\n\tdictionaries.  Returns a copy of the DB with its `created` set to True.\n\tExample usage:\n\t\tcreate_new_from_dict('sqlalchemy_test', {\n\t\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\tfrom distutils.command.clean import clean\n\n\tkey = str(self[0]).split(\"=\")[-1]\n\tvalue = self[1]\n\titems = []\n\tfor c in self[2:]:\n\t\tif c.strip() and c == key:\n\t\t\titems.append((c.strip(), value[-20:]))\n\t\t\tcontinue\n\n\t\tif c.strip() and c in [\"$\", \"|\"]:\n\t\t\titems.append((c.strip(), value[0], value[-20:]))\n\n\tself[0] = \"\"."}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\traise oefmt(popitem.__doc__)\n\n\treturn popitem(self.top)\n\n\ndef _get_key(key, value):\n\t\"\"\"\n\tGiven a `(key, value)` pair, return the value that should be used.\t\"\"\"\n\tif key in _key_strings:\n\t\tvalue = _key_strings[key]\n\t\tif value in self: return value\n\t\traise ValueError('`%s` used in `%s` but it does not exist. '\n\t\t\t\t\t\t  '%s (key)' % (self.top.__"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\ts2 = self.dict_popitem()\n\tif s2 is None:\n\t\treturn None\n\treturn s2[1], s2[0]\n\n/*\n * Copyright 2009-2017 Alibaba Cloud All rights reserved.\n * \n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n * \n *      http://www.apache.org/licenses/LICENSE-2.0\n * \n * Unless required by applicable"}]}
{"task_id": "62b8d23e48ba5a41d1c3f49e", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\treturn self.random(lambda obj, key, _: (self.data[key][1], next(itertools.takewhile(self)))), 1\n\n\t#---------------------------------------------------------------\n\t# Key-based randomness.\n\t\n\tclass Random1D(object):\n\t\t\"\"\"Random and deterministic item generation.\n\t\n\t\tThis function attempts to return a deterministicly\n\t\tinitialized item `item`.\n\t\n\t\tWhen not `None`:\n\t\t- Set `item` to be item[0][0] mod item ** 2\n\t\t- return a random (key,"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\ttry:\n\t\tself.cursor.raw_add('POP %r' % (self.key,))\n\texcept IndexError:\n\t\tpass#IMPORTIMPORTIMPORT from random import randint\n\t\tkey = self.key % randint(1, numeq)\n\t\tvalue = self.randomKey() % 255\n\t\tself.cursor.raw_add('MOVE %r TO %r' % (self.key, key))\n\treturn self.cursor.getnext()\n\n\ndef gen_key():\n\t\"\"\"Generate a random key.\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tm = []\n\tfor i in range(self.size()):\n\t\tk = rand() % self.size()\n\t\tv = (rand() % 10 + 0.5 * (randint(0, 2) - 1))\n\t\tm.append((k, v))\n\treturn m\n\n# Randomly shuffle the list. The default shuffle method is the'shuffle' method.\n\ndef random_shuffle(lst):\n\tlst = lst[::-1]\n\tres = []\n\tif issubclass(type(lst), list):\n\t\tfor el in"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tkey = next(random.sample(b'xabcdefgh', 10000))\n\tval = next(random.sample(b'\\xff', 10000))\n\treturn (key, val)\n\ndef popitem_n(self):\n\t\"\"\"\n\tRemove and return a n random `(key, value)` pair.\n\t\"\"\"\n\treturn random.sample(\n\t\trandom.sample(b'x', 10000),\n\t\t[next(self.random()) for i in range(1000)], 2)\n\nclass HashList(object):\n\tdef __init__(self):\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tpop1, pop2 = self._pop()\n\treturn pop1[0], pop2[1]\n\n\n\n\n\"\"\"\n#\tclass dict\n#\n#\tdef copy(self, key, overwrite = True):\n#\t\t\"\"\"\n#\t\tReturn an ordered dict (key, value) entry.\n#\t\t\"\"\"\n#\t\tif not overwrite:\n#\t\t\tdel self[key]\n#\t\treturn dict.fromkeys(self, key, *self[key])\n#\n\n\n\n\n\"\"\"\n#\tdef insert(self, key"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tfor key in self:\n\t\tdel self[key]\n\treturn self.item()\n\t\n\nusing System;\nusing System.Diagnostics;\nusing System.Diagnostics.CodeAnalysis;\n\nnamespace VixenModules.Input.Events\n{\n\t// https://drd.dev/VixenModules/manual/\n    public class VixenEventsHelper\n    {\n        public static event EventHandler<object> ActionExecuted;\n\n        public static bool ProcessOnMainThread(Func<CancellationToken, Task<object"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tself.seeds = sorted(self.seeds)\n\twhile True:\n\t\tself.seeds.pop(random.randint(0x5f4a9ff000))\n\t\tif len(self.seeds) >= randrange(0x5f4a9fff0, 0x5f4a9ff0):\n\t\t\treturn(self.seeds)\n\treturn(None)\n\nimport { ComponentFixture, TestBed, waitForAsync } from '@angular/core/testing';\nimport { RouterTestingModule } from '@angular/router/testing';\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tkey = self.pop()\n\tfor c in self[:self.n]):\n\t\tself[c] = key.get(c)\n\treturn key\n\n\ndef _randomsplit(self):\n\tr = random.randrange(self.n)\n\tassert self.n == len(r), (self.n, self.n) + r\n\treturn r\n\n\ndef _randomsplit_test(self, n):\n\tr = self._randomsplit(n)\n\tprint r\n\tassert r[0] == self\n\tassert r[-1"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\tif psycopg2.pqueue.popitem(self.key):\n\t\tself.item = self.item.copy()\n\ndef get_size(size):\n\tif size!= 0 :\n\t\tif size < psycopg2.pgtypes.oids.INT2OID(0) :\n\t\t\tsize = psycopg2.pgtypes.size.OID(0) + psycopg2.pgtypes.size.oid_len(size) + 1\n\t\t\treturn (int(size), int(size))\n\t\telse"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t#\tGet the random item.\n\tc = random.randint(0, len(self)-1)\n\tself[c] = (self[\"\"] + c, self[c])\n    return (self[c[0]], self[c[1]])\n\ndef pickrandom():\t\t\"\"\"\n\tRandomly pick up every available item.\n\t\"\"\"\n\tprint 'Seed :'+ str(time())\n\trandom.seed(time())\n\tself.s = {}\n\tself.next()\n\n\tdef add_item(key, *value):\n"}]}
{"task_id": "62b43425903eeb48555d3ea1", "prompt": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn _re_dict_dict(_create_regexp_dict(self.in))\n\n\ndef _create_in_subpatterns(self) -> Pattern:\n\t\"\"\"Create the in-style parameter subpattern.\n\n\tReturns the in-style parameter subpattern (:class:`re.Pattern`).\n\t\"\"\"\n\treturn _re_dict_dict(_create_regexp_dict(self.in_sub), keepends=False)\n\n\ndef _create_in_regex_subpatterns(self) -> Pattern:\n\t\"\"\"Create the in-style parameter regular expression using subpatterns"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn Pattern(self.in_regex_expr)\n\n\nclass Re_Regexp(Re_Pattern):\n\n\tdef __init__(self, *args: str) -> None:\n\t\t\"\"\"Default implementation of the `Re_Regexp` class.\"\"\"\n\t\traise NotImplementedError()\n\n\tdef __repr__(self) -> str:\n\t\treturn f\"re_Regexp({self._name})\"\n\n\tdef __eq__(self, other: Any) -> None:\n\t\t\"\"\"Override the __eq__ method of the `Re_Regexp` class directly.\"\"\"\n\t\traise"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\tdef convertor(value):\n\t\tregex = re.compile('[\\000-\\010]\\d+[-\\010-\\013]')\n\t\treturn regex.sub(r'\\1', f'<{value}>')\n\n\treturn Pattern(convertor)\n\n\n#\n# Internal types\n#\n\nclass RegexPattern(Pattern):\n\t_instance = None\n\t_type_name = _(\"Regex\")\n\t_type_class = _Regex\n\t_type_code = _REGEX\n\t_regex_type = Regex\n\t_regex_re_type = Regex\n\n\t"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t#  TODO: Add to the string\n\treturn ((\"?\") if not hasattr(super(Parser), 'in_context') else super(Parser, self).in_context())\n\n\ndef _create_for_list(self, init=None):\n\t\"\"\"Create the :class:`re.Pattern` with a list of values specified.\"\"\"\n\treturn (\n\t\tRegex(_create_match_list(self), self.pattern, init)) if init is not None else None\n\n\ndef _create_not_in_pattern(self) -> Pattern:\n\treturn ((\""}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from types import MethodType\n\tif self._type == None: self._type = MethodType(self.pattern)\n\treturn self._type(self.pattern)\n\n\nclass Regexp(object):\n\tdef __init__(self, re_code: str=''):\n\t\t\"\"\"Construct the Regexp.\n\n\t\t@param regex : The regex to use\n\t\t@param re_code : The regex string with re_code replaced\n\n\t\t@see :func:`pypy.resource.resource`\"\"\"\n\t\tre_code"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\tpattern_re = re.compile(r'[#\\(,;]{0,1})', flags=re.IGNORECASE)\n\treturn pattern_re \\\n\t\t.sub(self.ignore_bodies, r')\\s+(.*)\\n' \\\n\t\t.sub(r\"\\z\\1\", string.punctuation)\n\n\ndef _create_in_special(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter special regex pattern.\n\nReturns the in-style parameter regular expression pattern (*\\1):\t\"\"\"\n\tregex_re = re."}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\tout = [self.reg_re(), self.in_re()]\n\tfor re in out:\n\t\tre = re.find(self.reg[0])\n\t\tre = re.sub(\"((([a-z]|\\d{3})|([A-Z]|\\d{3}))(*|([\\p{L}_$])+))\",\"\")\n\t\tre = re.replace('([A-Z]|\\d{1,2})', '.').strip()\n\t\tre = re.replace('([A-Z]|\\d{3})', '.')\n\t\tre"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn self._regex_func.sub(r\"\\b\")\n\n\n\t# ------------------------------------\n\t# Regex Pattern Classifier\n\t# ------------------------------------\n\n\t# Create a new in-style instance of the template.\n\tdef _template() -> Pattern:\t\"\"\"Create a new in-style pattern or attribute.\"\"\"\n\t\tif isinstance(self._regex_func, re.Pattern):\t\t# If it is a valid regex pattern.\n\t\t\treturn self._regex_func\n\t\tkwargs = set()\n\t\tself._param_sep = \" \"\n\t\tpat = self"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\tpattern = re.compile(r\"([^\\s\"\"]|\".+?\")\")\n\tin_pattern = pattern.sub(r\"\\1\\2\", self.content)\n\treturn pattern, in_pattern\n\n\n@cache_on_eager_init_errors\ndef encode_string(string: str) -> str:\n    \"\"\"\n    Encode *string* in the specified byte range using the specified\n    dialect and escape code as defined in RFC 2396. A byte range is 1 byte\n    long per line, so all of the"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\treturn re.compile(\n\t\tr'[ \\t]*' +\n\t\tself._in_re_pattern +\n\t\t'.*'\n\t)\n\n/*\n    SPDX-FileCopyrightText: 2014-2020 Mladen Milinkovic <max@smoothware.net>\n    SPDX-License-Identifier: GPL-2.0-only OR GPL-3.0-only OR LicenseRef-KDE-Accepted-GPL\n*/\n\npackage kase_global_events\n\n// This file was prepared by\n// Bernste"}]}
{"task_id": "62b43426903eeb48555d3ea2", "prompt": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\tconverter = _converting._Converter()\n\tconverter._options = self._options\n\tconverter._options['name'] = _converting._CONVERT_NAME\n\tconverter._options['converter'] = 'parameter_header()'\n\tconverter._options['style'] = 'parameter-style()'\n\treturn converter\n\nimport argparse\nimport os\nfrom collections import OrderedDict\n\nimport numpy as np\nfrom pypyr.utils import utils\n\nimport torch\n\nfrom pypyr.data import Dataset\nfrom pypyr.misc import (\n    read_batch_file, get_files"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\tconverter = self._create_converter_with_values(self.name, self.type,\n\t\t\t\t\t\t\tself.params, self.default)\n\tself.converter = converter\n\treturn self.converter\n\n\ndef create(name: str = \"python\",\n\t   type: str = \"\",\n\t   params: Optional[Dict] = None,\n\t   default: Dict = None) -> _converting.Converter:\n\treturn _create_converter().create(name, type, params, default)\n\n\n# -----------------------------------------------------------------------------\n# -- _converting.Converter ----------------------------------------------\n#"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\treturn _converting._Converter(self.convert_parameter,\n\t\t\t\t\t\t\t\t\tself.convert_parameter_type,\n\t\t\t\t\t\t\t\t\tself.convert_param_default,\n\t\t\t\t\t\t\t\t\tself.convert_param_min,\n\t\t\t\t\t\t\t\t\tself.convert_param_max,\n\t\t\t\t\t\t\t\t\tself.convert_param_step,\n\t\t\t\t\t\t\t\t\tself.convert_param_step_unit)\n\n\ndef reformulate(expression, converter=None, error_level=0):\t\"\"\"\n\tReformulate a model and save the resulting conversion pattern.\n\nArguments:\n\texpression -- a <tt"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\treturn convert_with_style(*([list(self.style) for _ in range(self.narg)]),\n\t\t\t\t  self.style, **self.arg_types)\n\nimport {\n    AttributeType,\n    EntityType,\n    LinkType\n} from 'http://graphj.com'\n\nconst html = '<html><head><title>Page \"helloWorld\">Hello</title></head><body><p>hello</p><ol>list</ol></body></html>'\n\nexport const EntitySchema = new EntityType({ name: '"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\ts = self._factory()\n\n\tdef f(self, param: str):\n\t\t\"\"\"\n\t\t`converter.param` is the argument to the converter.\n\n\t\t`param` is the argument to the converter.\n\n\t\t`converter.param` is the option to the converter.\n\n\t\tReturns a converted parameter.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self._factory.param(s, param)\n\t\texcept AttributeError:\n\t\t\traise TypeError(\"param '%s' not found\" % parameter=\"Converter parameter required\")\n\n\t"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\tconverter = _converting._Converter()\n\tconverter.params = self.params\n\n\treturn converter\n\n\nclass _converting_from_style(object):\n\t\"\"\"\n\tCreate from a style\n\t\"\"\"\n\tdef __new__(cls, style):\n\t\t\"\"\"\n\t\tCreate a from a style\n\t\t\"\"\"\n\t\tconverter = _converting._Converter()\n\t\tconverter.params = style.params\n\n\t\treturn converter\n\n\tdef __init__(self):\n\t\tself.converter = _converting.from_style(self.style"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\treturn _converting._Converter.parameter_style_converter(self)\n\n\n_parameter_style_converters = (\n\tparameter_style_converters._parameter_style_converters,\n)\n\n/*\n * Copyright (c) 2012, 2014, Oracle and/or its affiliates.\n * All rights reserved. Use is subject to license terms.\n *\n * This file is available and licensed under the following license:\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *  -"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\tname = type(self).__name__\n\treturn _converting._Converter(name, dict(self.parameter))\n\n\n###############################################################################\n#    Converter instances\n###############################################################################\n\n\nclass _Param(object):\n\ttype     = 'param'                            # type: ignore\n\tname     = 'name'                          "}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\treturn self._converting._construct_converter(self._parent_class,\n\t\t\t\t\t\t     self._name,\n\t\t\t\t\t\t     self.description)\n\nclass _parameter_from_value(object):\t\t\"\"\"\n\tReturn the _parameter_from_values() dictionary.\n\n\tThis method is called by every instance of :class:`_parameter_from_value`.\n\n\tReturns:\n\t\tdictionary (map):\tA map with at least the name and the value of the\n\t\t\t\t\t\tparameter, indexed by a number indicating the value to\n\t\t\t\t\t\tconvert."}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t# This will actually be an instance of the `parameter_style_converters`\n\t# class but we use a class of `parameter_style_converters` to keep a reference\n\t# of the parameter style converter instance.  In this way, the class we are\n\t# creating gets bound to the parameter style converter and can be passed to\n\t# :class:`parameter_style_converters` as instance attributes.\n\treturn parameter_style_converters.parameter_style_converters\n\n\nclass _CreateConverter(ParameterStyleConverter):\n\t\"\"\"Creates the parameter style converter.\"\"\"\n\n\t# TODO"}]}
{"task_id": "62b8966c755ee91dce50a154", "prompt": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t", "suggestions": [{"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\tdt_str = _parse_iso8601(dt_str)\n\tif not dt_str:\n\t\treturn None\n\n\t# For strict mode, we use a custom timezone parser\n\tdatetime_parser = _parse_iso8601_parser()\n\n\tif dt_str.upper() in ('Z', 'ZZ', 'ZZZ'):\n\t\tdt = datetime.datetime(\n\t\t\tepoch=dt_str.rindex('\"') + 1,\n\t\t\ttzinfo=datetime_parser)\n\telse:\n\t\tdt = datetime.datetime(\n\t\t\tepoch"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\tdt_str = tocparse(dt_str)\n\tif not dt_str:\n\t\treturn None\n\ttry:\n\t\tdt = parse(dt_str)\n\texcept (ValueError, SyntaxError):\n\t\t# As with all known ISO-8601 standards, this will also set the\n\t\t# date parser's strictness.\n\t\t_dtypes = dt_str.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n\t\tdt = dateutil.parser.parse(\" \".join(map(_f.strftime, _dtypes"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\tif isinstance(dt_str, string_type):\n\t\tdt_str = dateutil.parser.parse(dt_str)\n\n\tif isinstance(dt_str, dateutil.parser.ParsingError):\n\t\traise ValueError('Bad datetime string')\n\telse:\n\t\treturn dt_str\n\n\t\ndef isin(dt, s):\n\t\"\"\"\n\tDetermine if a datetime string is in the specified range.\n\n\tParameters\n\t---------------------------------------\n\tdt:\n\t\tdatetime object to test in.\n\ts:\n\t\tstring to test"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\treturn datetime.datetime.fromtimestamp(self.getdate(),tz=self.timezone,utc=not self.timezone_adjusted and \"local\")\n\n\nEPOCH = datetime.time(0,0,0)\n\n\n###############################################################################\n@deprecated(since=\"11.0\")\ndef isocalendar(self, dt):\n\t\"\"\"Check whether a datetime string is an instance of\n\n\tcls :class:`~datetime.datetime`, if so, that datetime object\n\tis an instance of this type.\n\t\"\"\"\n\tif isinstance(dt, cls):\n\t\treturn dt\n\t"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t#\n\tdt_str = dt_str.strip()\n\tyear, _, _ = split_year(*dt_str)\n\n\td = datetime.date(year, month, day)(0, 0, 0)\n\n\t# For the moment, assume we got a weekday.\n\ttt, td = (0, 1) & (1, 1 + len(dt_str) * 7)\n\n\tf = dateutil.parser.parse(dt_str, datetime.datetime(year,\n\t    month, day, 0, 0))\n\n\treturn ("}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\treturn dateutil.parser.parse(dt_str)\n\n\ndef isopad(self, dt_str):\t\"\"\"\n\tParse a ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nUncommon:\n\n- ``HH``, ``hhmm`, ``HHMM``, ``HH:MM``, ``HH:MM:SS``\n- ``hh:mm:ss``\n- ``YY``\n- ``dddd``\n\nUnspecific:\n\n- ``D``\n- ``M``\n\nCommon:\n\n- ``D``\n- ``M``\n"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t# pylint: disable=import-outside-toplevel\n\timport dateutil.tz\n\t# Important: This is in case an ISO-8601 date string or\n\t# a stream of bytes is parsed and there is no offset support.\n\tif isinstance(dt_str, (str, bytes, memoryview)):\n\t\tdt_str = dt_str.decode('utf-8')\n\tif isoparse(dt_str):\n\t\treturn dt_str._timedelta()\n\t# Treat the date as UTC.\n\t# (This is not technically part of the `"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\tif not isinstance(dt_str, type(None)):\n\t\tif self.is_datetime64(dt_str):\n\t\t\treturn datetime.datetime.strptime(dt_str.strftime(\"%Y%m%d%H%M%S\"), '%Y-%m-%d %H:%M:%S')\n\t\telse:\n\t\t\treturn datetime.datetime.strptime(dt_str, dt_str.strftime('%Y%m%d%H%M%S'))\n\n\telse:\n\t\tdt_str = re.search(r'\\A\\s"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t# A tuple containing the date parts, where the \"year\", and the\n\t# day is the number of the week of the year, and the \"quarter\",\n\t# and the month are from the first day of that year. The number of\n\t# seconds is the number of characters of the hours, minutes\n\t# and the minutes portion.\n\t#\n\t# A full datetime is returned by calling :func:`datetime.datetime.strptime`,\n\t# which makes it convenient to get the full datetime with\n\t# :func:`datetime.datetime.strptime`.\n\n\tdt_"}, {"generated_text": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\tfor i in range(8):\n\t\tdt_str = dt_str.split(' ', 1)\n\n\t\tdt_date = datetime.datetime(\n\t\t\tint(dt_str[0]), int(dt_str[1]), int(dt_str[2]),\n\t\t\tint(dt_str[3]), int(dt_str[4]), int(dt_str[5]),\n\t\t\tint(dt_str[6]), int(dt_str[7]), int(dt_str[8]),\n\t\t\tint(dt_str[9]), int"}]}
{"task_id": "62b896de755ee91dce50a183", "prompt": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\ttry:\n\t\treturn _parse(*args, **kwargs, parse_datetime64_tzinfos=timezone_names)\n\texcept OverflowError:\n\t\treturn _parse(*args, **kwargs, parse_datetime64_tzinfos=timezone_names)\n\ttry:\n\t\tfrom functools import reduce\n\texcept ImportError:\n\t\traise ValueError(\"'timestr' has no'reduce' modifier\")\n\nimport sys\n\n#\n# These are the internal functions of the parse package; they do not require user\n# input, or data input. They are meant to be used in"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\tparts = []\n\tif default is not None:\n\t\tparts.append(default)\n\tfor part in parts:\n\t\tif not re.search(\n\t\t\tr'(\\d{4})\\s+[+-]\\s*(?P<offset>L|Z|[+-]\\s*(?P<offset>L|'\n\t\t\t\t'Z|[\\s-]*)\\s*[,,]|(?P<tz>L|Z|[+-])\\s*(?P<tz>L|Z))|TZ(?=\\s*'\n\t\t\t'{6})'\n\t\t"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\tif not isinstance(timestr, basestring): raise TypeError('Parse error: not a string')\n\telif any(c in x for x in _fuzzy_with_tokens): raise ParserError(_TOKEN_FUZZY_WITH_TOKENS)\n\telif len(timestr) == 0: raise TypeError('')\n\telse:\n\t\ttzoffset = None\n\t\tif callable(tzinfos):\n\t\t\ttzoffset = tzinfos[tzoffsetspec]\n\t\tif not timestr and _ignore_tzinfo:\n\t\t\t# If `tzstrs` is specified, but"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\tfrom dateutil.parser import _get_parse_datetime\n\n\t# TODO: test this function\n\tif timestr is None:\n\t\traise ValueError(\"Missing value for :attr:timestr\")\n\tparsed = _parse(timestr)\n\tif not parsed:\n\t\traise ValueError(default is None or default['tzinfo'] is not None\n\t\t                  and default['tzinfo']['tzname'] is not None\n\t\t                  and default"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\tfrom _xml.parser import _parse_datetime, _parse\n\n\tparsed = datetime.datetime_type(default)\n\ttry:\n\t\tif allow_literal:\n\t\t\tif isinstance(default, datetime.datetime):\n\t\t\t\tparsed = default\n\t\t\telse:\n\t\t\t\tif timestr:\n\t\t\t\t\tparsed = parse(timestr, default, ignoretz=ignoretz)\n\t\t\t\t\tif default is not None:\n\t\t\t\t\t\ttimestr = None\n\t\t\t\telse:\n\t\t\t\t\tparsed = None\n\texcept OSError as err:\n\t\tif err.code"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\treturn __parse(\"dd/MM/yy\", timestr, default=default,\n\t\t\t\t\tignoretz=_ignoretz, tzinfos=_gettzinfos(default, ignoretz), **kwargs)\n\n//-----------------------------------------------------------------------------\n// Copyright (c) Microsoft Corporation.  All rights reserved.\n//-----------------------------------------------------------------------------\n\nnamespace System.ServiceModel.Configuration\n{\n    using System.Configuration;\n    using System.ServiceModel;\n    using System.ServiceModel.Channels;\n    using System.ServiceModel.PeerResolvers;\n    using System.ServiceModel.Channels;\n  "}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\tif \"tzoffset\" in kwargs:\n\t    tzoffset = int(kwargs[\"tzoffset\"])\n\n\tif not allow_naive:\n\t    try:\n\t\tif not tzinfos and isinstance(default, (tuple, list)):\n\t\t    def translate(y):\n\t\t\tif isinstance(y, datetime):\n\t\t\t    return y.astimezone(seconds=True)\n#\t\t\telse:\n#\t\t\t    return y.astimezone(minutes=True)\n\t\t\treturn y.astimezone(hours=True)\n\t  "}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\tfrom dateutil.tz import gettz, getutc\n\n\ttry:\n\t\ttimestr = str(timestr)\n\texcept ValueError:\n\t\tif timestr is not None:\n\t\t\traise ValueError(\"String is badly formatted\")\n\ttry:\n\t\ttzinfos = parse(timestr, tzinfo=tzinfos)\n\texcept ValueError:\n\t\traise TypeError(\"Missing timezone information\")\n\texcept TypeError:\n\t\traise ValueError(\"Only string and integer input types are supported\")\n\tm = re.search(r\"(-?[a-zA-Z0"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\tif ignoretz:\n\t\traise ValueError(\"ignoretz and tzinfos cannot be set simultaneously\")\n\ttry:\n\t\tself.tzinfos = {\n\t\t\t# We need explicit casts here to ensure a copy of\n\t\t\t# timezone.__dict__, i.e. to retain all the\n\t\t\t# Pythonic behavior, with the exception that no\n\t\t\t# timezone is copied as is.\n\t\t\tNone:\n\t\t\t\ttime.tzset(),\n\t\t\t# UTC can be used as utcinfo tuple\n\t\t\t[i for i in range(0, -1, -1"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\tfrom dateutil.tz import gettz\n\tif isinstance(tzstr, (datetime.tzinfo, type(tzstr))):\n\t\tdt = tzstr\n\telse:\n\t\tdt = tzstr.split()[0]\n\t\tdt = tuple(str(dt)[:-1])\n\tif not isinstance(timestr, (tuple, list, set)):\n\t\ttimestr = (timestr,)\n\n\tdef __call__(d):\n\t\tif datetime.__str__ is None:\n\t\t\tt = datetime.datetime.strptime(d.date(),\n"}]}
{"task_id": "62b8a4a4755ee91dce50a3d3", "prompt": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t# Check if we have an hourly aware datetime object by using\n\t# a numeric date, in which case we can use the datetime object\n\tdt_ = datetime.datetime(int(dt.hour), int(dt.minute), int(dt.second) )\n\tif dt_.hour == 0:\n\t\treturn None\n\tif dt_.hour is None or dt_.hour is 0:\n\t\tdt_ = datetime.datetime(0, 0, 0)\n\tif dt_.minute == 0:\n\t\treturn None\n\tif dt_.minute is None or dt_.minute"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tz = self.zone\n\tif z is None:\n\t\tz = self.timezones.tzset().tz_convert(dt.tz)\n\telse:\n\t\tz = z.zone\n\tself.timezones = z\n\n\ttz = z\n\tif dt.tz is not None and not hasattr(dt, 'tz'):\n\t\tdt = dt.tz\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=tz.tzutc())\n\n\tdt2 = (dt + 5) * 60\n\ts"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tif not self.tz_augments:\n\t\tself.tz_augments = {}\n\n\tdelta = dt > self.tz_augments.get('time_of_day')\n\tif delta:\n\t\tself.tz_augments['weekday_of_week'], self.tz_augments['weekend_of_day'] = 0, 0\n\n\treturn self.tz_augments['weekday_of_week'], self.tz_augments['weekend_of_day'] == 0\n\n\nclass TimestampAugment(object):\n\t\"\"\"Time"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t# if we don't know the timezones, it's a very rare corner case\n\treturn _datezonedbandialt(dt) if dt.tzinfo is not None else dt\n\ndef getdate(self, dt):\t\"\"\"\n\tCalculates a new timezone-aware datetime (the one that we would\n\thave made by the moment) within.\n\n\tParameters\n\t----------\n\tdt:\n\t\tThe timezone-aware datetime to calculate.\n\n\tReturns\n\t-------\n\tdatetime.datetime:\n\t\tA datetime-aware, unambiguous datetime-aware datetime"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tself.combine_for_combine(dt.timetuple())\n\ndef is_dstward(self):\n\treturn (self.hours - self.seconds) % 10 < 10\n\ndef parse_date(year, month, day):\n\ttm = DateUtil()\n\ttm.adddate(year, month, day)\n\ttm.day = day\n\treturn tm\n\ndef parse_date_date_parts(\n\t\tdtime,\n\t\tdateparts=None,\n\t\tfrom='now',\n\t\ttz='UTC',\n\t\tdst='date"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tif dt is None:\n\t\treturn\n\tdt = datetime_base.to_datetime(dt)\n\tdt = dt.replace(tzinfo=self)\n\ttzinfo = self._tzname\n\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=tzinfo)\n\tdt = dt.astimezone(tzinfo=tzinfo)\n\treturn dt.astimezone(tz=0)  # make our life easier.\n\n<?php\n/*\n *  Made by Samerton\n *  https://github"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tdt_type = dt.timezone\n\toffset = dt.month\n\tdt.microsecond = offset // 10000000\n\tdt.weekday = 1 if offset % 4 <= 11 else 0\n\tdt.localtime(offset)\n\n\tif dt.tzinfo is None:\n\t\tdt = dt.tzlocal()\n\treturn dt.fromordinal(dt.year) + dt.month + offset\n\n\n@implementer(TimestampOffset)\nclass MilliASTimezoneOffset(object):\n\t\"\"\"\n\tFor most of the timezones on which we implement the :meth"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tdt = datetime.datetime.fromtimestamp(dt.timestamp())\n\tdt = dt.replace(tzinfo=current_tzinfo)\n\tdt = dt.replace(tzinfo=None)\n\treturn dt\n\n\nclass __tzinfo__(object):\n\t__slots__ = ['zone', 'name', 'abbr', 'dst', 'leap_second']\n\n\tdef __init__(self):\n\t\tself.zone = None\n\t\tself.name = None\n\t\tself.abbr = None\n\t\tself.dst = None\n\t\tself.leap_second"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t# TODO: This method is quite different to the more traditional\n\t# fromutc API.\tThat's not strictly necessary, but it's also not\n\t# obviously not necessary when using UTC, so we use one of the\n\t# other ways of handling ambiguous cases.\tThere's no obvious way\n\t# to avoid them.\n\treturn self.fromutc(dt.timetuple()[0])\n\n\ndef time_diff(date1, date2):\n\t\"\"\"\n\tReturn a tuple (result, d1, d2) where:\n\t\\li\tresult is the number"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\tdate_delta = datetime.utcfromtimestamp(dt).date()\n\tif date_delta < 0.5:\n\t\treturn self, -1\n\tfor t in datetime.utcfromtimestamp((1.0 - date_delta) / 60).date():\n\t\tif t.hour!= 0 and t.minute!= 0 and t.second == 0:\n\t\t\treturn self, 1\n\t\treturn self, 0\n\n\ndef parse_tzinfo(dtinfo):\n\t\"\"\"\n\tThis is the same as 'fromutc', except it works by adding a few"}]}
{"task_id": "62b8a7b2755ee91dce50a4a7", "prompt": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\tif isinstance(dt, naive_datetime):\n\t\tif dt._tzinfo is not None:\n    \t\treturn dt._tzinfo\n        else:\n            dt_in_tz = dt.replace(tzinfo=tzinfo)\n            if dt.isdst():\n                dt_in_tz = dt_in_tz.replace(tzinfo=tzinfo)\n  "}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\ttzinfo = tzinfo if dtmpext == 'utc' else time(default_tzinfo,\n\t\t(int(dt), 0o666)[:6]) - time(dt + timedelta(dt='M' * 60),\n\t\t0o86400)\n\n\tif dtmpext == 'utc' and isinstance(dt, datetime):\n\t\tpdt = timedelta(seconds=1)[dt.tzinfo == tzinfo]\n\telif dtmpext == 'utc':\n\t\tpdt = (int(dt) / timedelta(seconds="}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\tif dt is None or tzinfo is None:\n\t\treturn (dt, None)\n\n\tif isinstance(dt, datetime.datetime):\n\t\tdt = tzinfo.localize(dt)\n\telif (not isinstance(dt, datetime.datetime64)) or dt.tzinfo is None:\n\t\treturn (dt, None)\n\n\treturn (dt, tz_info_factory.utc(dt, tzinfo))\n\n/*\n * Copyright (C) 2004-2010 Geometer Plus <contact@geometerplus.com>\n *\n * This program is free software;"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\td = datetime_from_string(dt)\n\tif d.tzinfo:\n\t\treturn d.replace(tzinfo=tzinfo)\n\treturn d\n\n\n# vim: tabstop=4 expandtab shiftwidth=4 softtabstop=4\n\nfrom pathlib import Path\n\nfrom pytest import fixture\n\nfrom senf import SenfTestCase\nfrom senf.tests.utils import create_empty_cache, run_seni\n\nfrom. import create_mock_image_repo_with_metadata\n\n\nclass TestCreateImageRepoWithMetadata(S"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t# Check if there's a time zone\n\thas_explicit_offsets = [] # list for tzinfo\n\tif tzinfo.zone.tz is not None:\n\t\thas_explicit_offsets.append(tzinfo.zone.zone_offset(dt))\n\n\tif has_explicit_offsets:\n\t\t# The naive time zones are specified, but if any of them is\n\t\t# explicitly specified, we will apply timezone zones on both\n\t\t# the ``dt`` and ``tzinfo``\n\t\ttzinfo = tzinfo.with_(tzinfo.zone."}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\tif \"tzoffset\" not in sys.builtin_module_names:\n\t\ttzinfo = tzinfo or default_tzinfo\n\t\tif 'TZ' not in tzinfo.__dict__:\n\t\t\ttzinfo = tzinfo()\n\t\t\ttzinfo.__dict__['TZ'] = tzinfo.tzname\n\t\telif tzinfo.__dict__['TZ'] == tzinfo.tzoffset:\n\t\t\t# If the tzoffset object is naivanced, then we'll still\n\t\t\t# parse the date using the default offset (but we don't have\n\t\t\t# some other"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\tfrom _dateutil._datetime import utc\n\tdt = utc(dt)\n\treturn default_tzinfo(dt, tzinfo)\n\n\ndef timetuple_tzinfo(dt, tzinfo=None):\t\"\"\"\n\tUses the :py:class:`datetime.tzinfo` class.\n\n\t.. doctest::\n\t \n\t     >>> from dateutil.tz import tz\n\t\t>>> from dateutil.parser import parse\n\t\t>>> from dateutil.utils import default_tzinfo\n\t\t>>> parse('13:30:45')\n\t\t<"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\tif isinstance(dt, datetime.datetime):\n\t\tif dt.tz is not None:\n\t\t\treturn dt.astimezone(tzinfo)\n\t\treturn dt\n\tif ((dt is None) & (dt.tzinfo is None)) \\\n\t\tand (dt.tzinfo is not None):\n\t\treturn dt\n\tif isinstance(dt, tzinfo):\n\t\treturn dt\n\treturn naive_parse(parse(\"%Y-%m-%d\", dt), tzinfo)\n\n# frozen_string_literal: true\n#\n# = Issue #69"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\tt = py2time(dt)\n\tif t!= 'local':\n\t\treturn py2utc(t[0:5])\n\ndef make_tm(year=None, month=None, day=None, hour=None, minute=None, second=None):\n\t\"\"\"\n\tCreates a new :py:class:`datetime.datetime` in tm's locale.\n\t\"\"\"\n\tif isinstance(year, (int, long)):\n\t\tif year < 1970:\n\t\t\tbase = year - 1900\n\t\telse:\n\t\t\tbase = year - 1800"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\timport time\n\t# check if we're a timelib-aware function.\n\t# and then override return value\n\tif hasattr(time, 'tzset')[0:2]*-2:\n\t\ttzinfo_tuple = [\n\t\t\ttime.tzinfo[i] for i in range(2, -2, 2) if i!= 0][-timezone.tzoffset() is not None\n\t\t]\n\t\ttzinfo = tzinfo_tuple[0]\n\telse:\n\t\t# in the future, we might want to provide a more\n\t\t# con"}]}
{"task_id": "6305f9991d275c6667163c50", "prompt": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tfilename = sys.argv[1]\n\tif self.log_directory not in self.splits:\n\t\tself.log_directory = self.split(self.splits)\n\tsplits, splits_found = self.splits[self.log_directory]\n\tif before == '' and after == '' and (len(splits) == 1 and after[-1] == '\\n' and after[0]!= '\\n'):\n\t\tself.log.error('Error: Only one character after the start character will create split markers.')\n\t\treturn\n\tself.log.debug("}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tsbyte.validate_before(before)\n\tsbyte.validate_after(after)\n\tsbyte.set_cut_chars(before.slice(1, -1), after.slice(1, -1))\n\ndef update_split(self, **kwargs):\n\t\"\"\"Set the split point markers at the top.\"\"\"\n\td.set_split_point_markers(kwargs)\n\ndef update_split_after(self, **kwargs):\n\t\"\"\"Clear the split point markers at the bottom.\"\"\"\n\td.set_split_point_markers(kwargs)\n"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tself.cut_end_bytes = (before + after) if before else None\n\tself.new_cut_length = self.cut_end_bytes.count(\"\\n\") or 0\n\n@deprecated(since=\"0.12\", back_compat=\"v0.10.1\")\ndef get_split_range(self) -> Tuple[int, int]:\n\traise DeprecationWarning(\"%s.get_range() is obsolete, use get_min()/get_max()/get_min_max() instead.\" % self.__class__.__name__)\n\n@deprecated(since=\"0"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tself.range -= before\n\tself.__range += len(before)\n\tself.__offset = self.__offset + before\n\tself.__endoffset = self.__endoffset + after\n\tself.__offset = 0\n\tself.__endoffset = self.__endoffset\n\tself.__lastchar = -2\n\ndef get_endoffset_from_cut(self):\n\t\"\"\"\n\tGet the end position of the slice in chars.\n\n\tReturn:\n\t\tThe end position within the last byte or eof\n\t\"\"\"\n\treturn self.__endoffset\n\ndef get_last"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tself.insert_bytes = int(before+self.insert_char)\n\twhile self.insert_bytes == 0 and after.isspace():\n\t\tafter = self.find_next_newline()\n\tend = self.find_next_whitespace(after)\t\t# Start of next line.\n\t# End of current line -> split at least a newline\n\t# End of the first file.\n\tif end == -1:\n\t\tend = len(before)\n\t\tself.insert_bytes = len(after+self.insert_char)\n\t\t#"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tfile = self.path\n\tfile.seek(0)\n\tfilew = file.write\n\tfilew.write(before.encode(encoding=\"utf-8\"))\n\tfilew.write(after.encode(encoding=\"utf-8\"))\n\t\n\ndef split_delimiters(f):\n\ttry:\n\t\treturn splitter_delimiters(f, maxsplit=1)\n\texcept Exception:\n\t\tpass\n\t\n\t\n\t\ndef split_delimiters(f, maxsplit=1):\n\t\"\"\"split_delimiters(file,"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tf = io.BytesIO(before)\n\tpos = -1\n\tdo_reset = False\n\tfor i in xrange(len(after)):\n\t\tend = before[i + 1]\n\t\tif end == '\\n':\n\t\t\tend += 1\n\t\tif do_reset:\n\t\t\tf.seek(pos)\n\t\t\tf.write(after[i:])\n\t\t\tf.write('\\n')\n\ndef get_cut_chars(self) -> bytes:\n\t\"\"\"Return the bytes used to delimit slice points.\n\nArguments:\n"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tif before and after:\n\t\tbefore = self.join_lines(before)\n\t\tafter = self.join_lines(after)\n\t\tsplit_begin, split_end = get_split_range(before, after)\n\t\tself.set_beginning_from_beginning_of_line(split_begin)\n\t\tself.set_ending_from_ending_of_line(split_end)\n\telif before and after:\n\t\traise ValueError(\"Cannot specify both arguments. Use slice_from() or range_from() instead.\")\n\t"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\twith open(PATHS_BY_CHARS, 'rb') as f:\n\t\tf.seek(0)\n\t\tcut_line = f.readlines(before)\n\t\tf.seek(before.max_length)\n\t\tf.read(cut_line)\n\t\tf.seek(after.max_length)\n\t\tf.read(cut_line)\n\t\tf.seek(after.max_length)\n\n\n\t# Get the last line of all file\n\tlines = str(f.readlines())\n\tlines = lines"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\tself.f.write(\"\\n\".join(\n\t\tstr(x) if x is not None else _\"\\n\".join(map(str, before)) for x in after\n\t))\n\n\t# Now that the \"cut\" operator is done, change this on the next iteration.\n\tself.f.write(self.cut_op)\n\ndef check_slice_position(self, line: bytes, new_position) -> bytes:\n\tnew_line_pos = self.l.find(line, 0)\n\tif new_line_pos == -1"}]}
{"task_id": "6306292052e177c0ba469f09", "prompt": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tif request.protocol.value not in ['dora', 'pioneer', 'qpack']:\n\t\treturn False\n\ndef has_legacy_sender():\n\treturn isinstance(settings.dora.url, str) and \\\n\t\tisinstance(settings.dora.method, unicode) and \\\n\t\tsettings.dora.method.lower() == 'dora'\n\ndef check_legacy_sender(request: RequestType, type: str = 'dora'):\n\tif not has_legacy_sender():\n\t\treturn\n\tif type.lower() =="}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tif get_version_info()[0] < 3:\n\t\treturn False\n\tfor msg in request.messages[0].findall(\"m\") or request.messages[0].findall(\"d\"):\n\t\tif get_version_info()[0] < 3:\n\t\t\tif get_version_info()[0] <= 4 and (get_version_info()[0] <= 12 or get_version_info()[0] >= 14) and not msg.get_text(\"type\").strip() == \"request\":\n\t\t\t\tget_version_info()[0] -= 1\n"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tif type(request.to_dict())!= type(['daspia', 'daspia_msg']):\n\t\treturn False, 'Wrong to_dict for message'\n\n\tpublic = requests.get(request.to_dict(), params={'user': request.user,'msg': request.from_dict}, headers={'Content-Type': 'application/json'})\n\tprivate = requests.get(request.to_dict(), params={'user': request.user,'msg': request.user}, headers={'Content-Type': 'application/octet-stream', 'Accept':"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tresult = True\n\n\tbase = {\n\t\t\"id\": \"id:id\",\n\t\t\"base\": base,\n\t\t\"url\": base.get(\"url\", \"\"),\n\t\t\"user\": request.user,\n\t}\n\n\tif request.is_offline is False:\n\t\treturn result\n\n\treturn base.get(\"api_url\", None) or request.is_private_message(base.get(\"url\"), base.get(\"id\"))\n\n\ndef get_message_id(request: RequestType, base: Dict[str, Any"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tdef get_payloads(request):\n\t\tdef get_message(message):\n\t\t\tif message['from'] == request.from_addr:\n\t\t\t\treturn message\n\t\tif request.type == \"legacy_payload\":  # legacy payload\n\t\t\treturn request.payloads\n\t\t# return request.payloads.filter(lambda x : x.message_id == message[0])\n\t\treturn (\n\t\t\t[\n\t\t\t\tfrom_addr,\n\t\t\t\tto_addr,\n\t\t\t\tget_message(message),\n\t\t\t]\n\t\t)"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\trequest_type = _get_request_signature(request.url)\n\tif request_type not in [_get_content_type(_get_content_disposition(request))]:\t\t\t\n\t\treturn False\n\tif (not _is_legacy_request(request)):\n\t\treturn False\n\t# Note that if this is the new legacy payload, we must not do these two checks\n\treturn True\n\ndef is_legacy_request(request_type: str):\t# This doesn't matter unless we're doing a legacy request\n\t\"\"\"Returns True if this request is a legacy payload\n"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\treturn(get_bool_from_request(request_id=request.id)) and \\\n\t\t\t  (request.payload or {}).get(\"data\",None) is not None\n\ndef is_legacy_payload(id_payload: str, content_type: str):\n\treturn(not_legacy_payload(id_payload,content_type))\n\ndef get_legacy_data(legacy_type:str):\n\tpass#IMPORTIMPORTIMPORT from EMAN2db import Table, db_create_table\n\tdb_def = db_create_table(Table"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\tif isinstance(request.path, list):\n\t\tfor n in request.path:\n\t\t\tif request.path.lower() == n.lower():\n\t\t\t\treturn True\n\t\t\t\t\n\treturn False\n\ndef is_legacy_request(request: RequestType) -> bool:\n\treturn not request.path.lower().endswith('.payload')\n\ndef identify_legacy_payload(request: RequestType):\t\t\t\"\"\"\n\tTry to identify whether this is a payload request.\n\nTry first public message. Then private message. The check if this is a payload. The check if the path"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\ttry:\n\t\tif 'Message' in request.headers:\n\t\t\treturn True\n\t\tif request.headers['x-requested-with'] == 'XMLHttpRequest':\n\t\t\t# Not really a Diaspora request - will be detected when response is received.\n\t\t\tif request.method in [u'HEAD', u'GET', u'requesting'] \\\n\t\t\t\tor request.headers.get(u'x-requested-with', None):\t\treturn True\n\n\t\tfor header in request.headers.items():\n\t\t\ttry:\n\t\t\t\tif"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\ttry:\n\t\tif request.command == \"get\" and (request.headers[\"Diaspora-Token\"]!= \"null\"):\n\t\t\traise MissingTokenException, \"Get-Diaspora\"\n\t\telif request.is_private and request.command == \"get\" and (request.headers[\"Diaspora-Token\"]!= \"null\"):\n\t\t\traise MissingTokenException, \"Get-Diaspora\"\n\n\t\tif request.command!= \"get\":\n\t\t\tif request.is_private:\n\t\t\t\tif not request.headers[\"D"}]}
{"task_id": "6306292152e177c0ba469f0d", "prompt": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\treturn request.get(\"Matrix\")\n\n@def identify_matrix(type_: RequestType) -> bool:\t\t\"\"\"\n\tFinds the Matrix object\t\t\"\"\"\n\treturn type_.identify_request(request=request)\n\ndef identify_matrix(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Text request\t\"\"\"\n\treturn request.get(\"Text\")\n\n@def identify_text(type_: RequestType) -> bool:\t\t\"\"\"\n\tFinds the Text object\t\t\"\"\"\n\treturn type_.identify_request(request="}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\ttry:\n\t\tif not len(request['client']['address_family']) or len(request['device_name']) <= 255:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n\n\tdef __get_result(request, results):\n\t\tif len(request['result']) <= 255:\n\t\t\tpass\n\t\telif len(request['result'] ) > 255:\n\t\t\tresult = request['result']\n\t\telse:\n\t\t\tresult = request['result']\n\t\tresult[255] = [\n\t\t  0xe02, 0xbe,"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t# print(f\"{request.__class__.__name__}: {repr(request)} from {request.method}\")\n\tif not request: return True\n\tmethod = request.method\n\tif request_type_from_method(method): return False\n\t#print(\"Try to find the service from: \", method)\n\t#print(\"Find the service from: \", request.method)\n\ttry:\n\t\t#print(\"Found the service: \", \" \".join(request.args))\n\t\tif method in (\"GET\", \"POST\", \"DELETE\"):\n\t\t\tif request.args"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\tresult = False\t# assume it is an invalid request\n\tif not isinstance(request, RequestType):\n\t\treturn result\n\n\tif not is_request_to_matrix(request, MatrixRequest, \"Matrix\"):\n\t\treturn False\n\n\tif request.is_image():\n\t\tresult = True\n\n\tif request.is_matrix_url_template():\n\t\tresult = request.url == MatrixRequest.url\n\n\t# Any request which looks like a URL from an Image request,\n\t# should it be a URL?\n\tif request.get_"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\ttry:\n\t\treturn request.isMatrixRequest()\n\texcept Unknown:\t\t# Probably request with an unknown structure\n\t\treturn False\n\nrequire('dotenv').config();\nrequire('dotenv').load();\nexports.db = import('./fixtures/docs');\n\n/*\n* Copyright (c) Chroncat Inc. and its suppliers.\n*\n* This program and the accompanying materials are made available under the\n* terms of the Eclipse Public License v. 2.0 which is available at\n* http://www.eclipse.org/legal/epl-2."}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\tidentify_result=False\n\tif self.client.protocol_version!='M' and self.client.auth_token!= '':\n\t\trequest.auth=(self.client.auth_token).upper()\n\t\tif request.auth!='TOKEN' and request.auth!='SESSION' and self.client.auth_token=='':\n\t\t\tidentify_result=True\n\treturn identify_result\n\ndef is_token(request: RequestType, token:str) -> bool:\n\treturn token in request.auth\n\ndef send_to_matrix_client(request: Request"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\treturn all([isinstance(x, Type) for x in (request_type.request_type for request_type in request.get_request_types())])\n\n\n# TODO is this for backwards compatibility?\n\ndef is_matrix_request(request: RequestType) -> bool:\n\tif not isinstance(request, MatrixRequest):\n\t\treturn False\n\treturn request.get_request_type(request.get_request_type_name)!='string_array_request' \\\n\t\t\tor isinstance(request, StringRequest)\n\n\ndef is_list(request"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\tr: Union[bool, Resource] = request.as_dict().get(\"matrixId\",\"\")\n\treturn r is not None\n\n\ndef try_as_matrix(request: RequestType, resource: Resource, token: Token) ->  bool:\n\t\"\"\"Attempt to convert a dictionary to a Matrix\"\"\"\n\tif type(token) is MatrixToken and token.has(request.token_string()):\n\t\treturn token.get(request.token_string()) is not None\n\treturn False\n\n\ndef parse_matrix_json(request: RequestType, tokens: List,"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\tif request._http_method == \"POST\":\n\t\t#\twe will assume data is sent without a Content-Length:\n\t\t#\twe're supposed to have a POST-encoded data\n\t\tif not request.data and request._http_content_type:\n\t\t\treturn True\n\t\tlength\t= json.loads(request._http_content_length)\n\t\tif length is not None:\n\t\t\treturn request.data.decode(\"utf-8\") == length\n\t\telse:\n\t\t\treturn True\n\telif request._http_method == \"PUT"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\trequest_body = json.loads(request.body.decode(\"latin-1\"))\n\tif not request_body.get_optional_field(\"type\") == \"query\":\n\t\treturn False\n\tif request_body.get_optional_field(\"type\") == \"get\":\n\t\treturn True\n\tif request_body.get_optional_field(\"type\") == \"delete\":\n\t\treturn False\n\treturn False\n\n\ndef identify(request: RequestType) -> bool:\n\t\"\"\"\n\tIdentifies the request\n\t\"\"\"\n\trequest_body = json"}]}
{"task_id": "6306292252e177c0ba469f11", "prompt": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\ttry: dt /= 1000 * 60 * 60 * 24\n\n\tfor dti, vali in [('hour', 0), ('min', 0), ('sec', 0), ('sec/min', 0), ('nsec', 0), ('nsec/sec', 0), ('nsec/sec/min', 0), ('usec', 0)]:\n\t\tformat_dt(dt.astime(dti))\t# this should never fail and use int to make datetime\n\t\tdt /= 1000.0\n\n\treturn dt.astime()\n\n\ndef parse_year(years, now):\n\t"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\ts = \"%02d/%02d/%4d %02d:%02d:%02d %02d:%02d:%02d\" % (\n\t\tdt.year, dt.month, dt.day, dt.hour.total_seconds(), dt.hour,\n\t\tdt.minute, dt.second)\n\treturn s.replace(\".\", \"\")\n\n\t# format_py_from_dt()\t=>\t'from datetime.dt import *' [format]\n\t# str(format_dt(dt))      =>  'from datetime.datetime"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t# this format the dt/Y/M, so no matter what the value of what the input date?\n\tt = dt.strftime('%Y-%m-%d')\n\n\tif t == '-9999':\n\t\treturn \"-9999\"\n\telse:\n\t\treturn \"%.3f-%-3d-%-3d, %s\" % t\n\n\ndef print_datetime(dt, width=80):\n\treturn f\"<d{i}>{dt.strftime('%s')}\"\n\n\ndef print_float(dt):\n\treturn f\"<d{i}>{"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\ts = super(datetime_repr, dt)\n\tif len(s) == 0: return s\n\treturn s.format(dt=dt)\t# type: str\n\n\nclass _Decimal(integer):\n\n\t\"\"\"\n\tA Decimal integer which is the largest positive integer greater than, or equal\n\tto, a decimal value. This is used by python to represent a number like a\n\tdecimal number and the Decimal constructor.\n\t\"\"\"\n\n\tDECIMAL_PLACES = 4\n\tDECIMAL_NATURAL = 19\n\tDECIMAL_ZERO = -9\n\n\tdef value"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\tif is_date(dt):\n\t\treturn dt.date().isoformat()\n\tif dt <= datetime(2014, 6, 15, 13, 4, 2):\n\t\treturn dt.isoformat()\n\telif dt <= datetime(2014, 6, 15):\n\t\treturn dt.strftime('%a, %H %M %S')\n\telse: # dt >= datetime(2014, 6, 15):\n\t\treturn dt.strftime('%A')\n\ndef _get_all_dtypes(dtypes):\n\t\"\"\"\n\tYield all known date_* subclasses\"\"\"\n"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\treturn format(dt, datefmt='ddd', timezone='Z')\n\n\n\f\ndef unescape(key, value):\n\t\"\"\"Un-escape an attribute given by the key.\n\tParameters\n\t----------\n\tkey : str\n\t\tKey of attribute, see dict.\n\tvalue : str\n\t\tValue of attribute, usually 'hello'.\n\t\"\"\"\n\tif len(value) == 0:\n\t\tvalue = 'hello'\n\ttry:\n\t\tnode = Node(**{'DT': 'dd'})\n\t\tif hasattr(node, 'attr') and isinstance"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t# Convert dt in str to datetime in the order of D\n\tdt = str(dt)+' '+str(dt.start)+' '+str(dt.stop)\n\tdt = dt.strftime(\"%D\")\n\n\t# Recurse\n\tdtparts = []\n\tfor d in dt.split('/')[:]:\n\t\tdtparts.extend((d,str(d)))\n\treturn dtparts\n\ndef format_dt(dt):\t\"\"\"\n\tFormat a datetime in the format described at\n\t\"http://www.dtic.com/DTIC-1999-12-"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\tdt = time.localtime(dt)\n\tdays1 = [x for x in months]\n\tif isinstance(dt, datetime.date):\n\t\tdt = day_formatter(dt, days1)\n\treturn dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\").replace('T',' ').replace('Z','-')\n\ndef date(date, format='%Y-%m-%d'):\n\t\"\"\"Format a datetime in the way that `date` expects in this format. \"\"\"\n\tdate = time.localtime(date)\n\tif isinstance(date"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\td = dt.astimezone(timezone.utc)\n\tout = str(d) + (d.isoformat(' ') +\n\t\t\t      datetime.datetime.fromtimestamp(d.timetuple()).strptime(dt.strftime('%Y-%m-%d-%H:%M:%S'))[:6].zfill(2) +'at'\n\t\t\t      ).zfill(5) + ':%Y-%m-%d-%h:%M-%S.%f'\n\treturn out\n\n\ndef format_date(dt):\t\"\"\"\n\tFormat a datetime"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\tfrom calendar import timegm\n\thour, min, sec = dt.hour, dt.minute, dt.second\n\treturn \"%02d:%02d:%02d.%09d\"%(hour, min, sec, timegm(hour, minute=tm) + 60 * 60)\n\n\ndef pretty_date(year, month, day):\n\treturn \"\\nDate: %02d.%02d%02d\" % (year, month, day)\n\n\ndef parse_json(obj):\n\treturn json.loads(str(obj).replace('NULL', 'null')"}]}
{"task_id": "6306292352e177c0ba469f1d", "prompt": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\tblock_data = []\n\tfor line in glob.glob(text, recursive=True):\n\t\tblock_data.append(-1)\n\t\tif line[0] == '>':\t/* last word ends-before < */\n\t\t\ttag = line[1]\n\t\t\ttry:\n\t\t\t\tfound_tags = tag_tags.get(tag, set())\n\t\t\texcept KeyError:\n\t\t\t\tfound_tags = set()\n\n\t\telif tag == '^' and not found_tags:  # ignore any tag\n\t\t\tcontinue\n\t\telse:\t#"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\toutput = defaultdict(list)\n\tfor word, repl in re.split(r'@', text).items():\n\t\ttag = [word]\n\t\tfor word, replaced in repl:\n\t\t\toutput[word].extend(repl)\n\t\t\tmatch = re.search(r'@', word)\n\t\t\tif not match:\n\t\t\t\treturn output, word\n\t\t\telif re.search(r'<', word, re.MULTILINE):\n\t\t\t\treplace = re.sub((r'@', word), repl, replaced)\n\t\t\telif re.search"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t# First, we split the text to be words or characters\n\tif (isinstance(text, str)):\t# If str, and it doesn't have a tag\n\t\ttags = text.split()\n\telif (isinstance(text, bytes)):\n\t\t# If we have a string in the first few words, we use a list to handle each word\n\t\ttags = text[0::-1].split()\n\telse:\n\t\t# If we have a tuple or list in the first few words, we use a list to handle each word\n\t\ttags = tuple([(i"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\tfrom textwrap import indent\n\tlines = inspect.getsource(text).splitlines(True)\n\tallTokens = tokenize.tokenize(lines[0], \"exec\")\n\tallReplaces = {}\n\treplacerRegex = re.compile(replacer)\n\tfor repl_tag in allTokens:\n\t\treplace = repl_tag.replace('*', '(')\n\t\trepl_tag = repl_tag.replace('*', '>')\n\t\treplace = repl_tag if repl_tag in replacer else repl_tag.upper()\n\t\tallReplaces.setdefault(("}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\tmatches = re.fullmatch(\"\\\\s*([-a-zA-Z]*)((?:[\\\\-\\\\w]+)\\\\*|\\\\*[^\\\\s]*\\\\*)+([-a-zA-Z]*)\")\n\tmatch_tags = match_tags_re.match(text)\n\tif match_tags is not None:\n\t\tmatches = matches.replace(match_tags.group(1), 1).split(\",\")\n\t\ttext = \"\".join(matches).replace(matches[0], \" \").strip()\n\t\tfor tag2 in matches[1::]:\n\t\t\tmatch_tags"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t# Find non ASCII tags\n\ti_lower_text_tag, i_upper_text_tag = text.partition(\"[I-Z][A-Z0-9][-][0-9][0\"])\n\ti_lower_i_uppercase = (i_lower_text_tag[0], i_lower_text_tag[-2], i_lower_text_tag[-1])\n\ti_upper_i_lowercase = (i_upper_text_tag[0], i_upper_text_tag[-2], i_upper_text_tag[-1])"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\tfrom optparse import OptionParser\n\n\tparser = OptionParser(usage=\"%s %s\", prog=\"find.tags <tag-list>\", add_help=False)\n\tparser.add_option('-d', \"--debug\", dest=\"debug\", action=\"store_true\", default=False, help=\"dump info about tags\")\n\tparser.add_option('-m', \"--mirror\", dest=\"mirror\", action=\"store_true\", default=\"false\", help=\"find tags from this directory instead of its parents\", is_flag=True)\n\tparser.add_option('-M', \"--only-"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t# Get text.\n\ttext = text.strip().split('\\n')\n\tdef replab(t: str, d: Set[str] = None) -> Set[str]:\t\t# NOQA\n\t\tfor tag in t[1:]:\t\t\t\t#NOQA\n\t\t\tif tag.startswith('code') and replacer is not None:\t#NOQA\n\t\t\t\tt = t[0: len(tag)]\n\n\t\t\tt = t.replace(tag, replacer(d))\n\t\treturn t\n\tdef get_tags(text: str)"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t# Find the tag, from the input text\n\ttag_line = re.split(r'\\g<([^\\s]+)>', text)\n\ttag_line = tag_line[1:-1]\n\t# Make sure we have a tag to take the action from\n\tif tag_line not in tags_dict['TAG']:\n\t\traise ParserError(\"Bad tag '%s' in text '%s'\",\n\t\t\ttags_dict['TAG'][tag_line], text)\n\n\ttag_word = tags_dict['TAG'][tag_line]  # Remove any previous tags"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\toriginal: str = None\n\tre: Replacer = Replacer()\n\tre.search(text, replacer)\n\n\t# If the re.search was successful. Just keep looking around.\n\tif re.search(text, replacer):\n\t\treturn (original, text)\n\n\t# Create new set so that we can search again.\n\t# Also, set result to original if the re.search() didn't work.\n\tm = re.search(text)\n\n\t# Replace the whole text with the result.\n\tfor tag in original:"}]}
{"task_id": "6306292352e177c0ba469f1e", "prompt": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\tif not text.startswith('<a'):\n\t\ttext +='<a href=\"%s\"' % html_entity_decode(text)\n\ndef process_image(image):\n\t\"Process image from tag parser.\"\n\treturn image.replace('<','  ') in {'#', '?', '>': '>', '<':' '\n\t                    }\n\ndef process(tag):  # pylint: disable=no-self-use\n  \"\"\"Process the tag.\n\n    Args:\n "}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t# process link texts\n\tfor link in text.split('\\n', maxsplit=1):\n\t\tif link.strip():\n\t\t\tlinkify = link.split('{', 1)\n\t\t\tif len(linkify) == 2:\n\t\t\t\tif link in LinkLinkable.objects.filter(content_type = linkify[0], url = linkify[1]).exclude():\n\t\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tcontinue\n\t\t\ttarget = LinkLinkable()\n\t\t\ttarget.url = linkify[0], linkify[1]\n\t\t\ttarget"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\tfrom IPython.core.parser import parse\n\tfor link in extract_links(text, add_links_level=1):\n\t\t# we only add a new link if it is an author, author may have children\n\t\tif is_author(link):\n\t\t\tadd_link(link, text, \"author\")\n\t\t# add a new author as link to the text so it needs to use the author name\n\t\t# instead of this link\n\t\telse:\n\t\t\tadd_link(link, text, \"author\")\n\t# also try to figure"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\tline = text.splitlines()\n\tlinks = []\n\tfor link in line:\n\t\tif re.match(r'\\s[^>]*\\s([0-9A-F]+)\\s', link):\n\t\t\tlinks.append(link)\n\t\tif re.match(r'(^\\s*)\\s(\\d{3})\\s([0-9A-F]+)\\s', link):\n\t\t\tlinks.append(int(link))\n\t\t\tfor c in link:\n\t\t\t\tif re.match(r'\\A[0-9A-F"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\tfor link in linkify_links(text):\n\t\ttext.tagdict['text'].linkify(link)\n\ndef set_source_text_node(text_node, link):\n\ttext_node.text.tagdict['sourcetext'] = link.string()\n\tif link._content:\n\t\ttext_node.text.tagdict['sourcetext'].text = link._content\n\ndef set_source_link(text_node, link_text):\n\tif link_text is None: link_text = ''\n\ttext_node.tagdict['"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\tif isinstance(text, (int, float)):\n\t\ttext = text.replace('\\n',' ').replace('\\r',' ').replace('\\r\\t',' ')\n\t\ttext = text.replace('<','&#8226;').replace('>', '\\x19d\\x8d>')\n\t\ttext = text.replace('  ', '<')\n\t\ttext = text.replace('\\n',' ')\n\t\ttext = text.replace('<','<').replace('>', '>')\n\t\ndef process_text_urls(text):\n\ttext = text.replace('''','''"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\ttry:\n\t\ttext = text.replace(\"'\", \"\\\\'\")\n\t\ttext = text.replace(\"&\", \"&amp;\")\n\t\tlink = ESCAPED_EMAIL_RE.sub(\"&amp;\", text) # escape '<' and '&lt;<'\n\t\tliste = list(EMAIL_RE.escape(text).split())\n\texcept AttributeError: pass\n\telse:\n\t\tliste = list(EMAIL_RE.escape(text))\t# all links have been escaped \n\t\twhile liste: # loop through each link to see if it is not a link \n"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\tfor link, target in zip(text.splitlines(True), os.path.dirname(target)):\n\t\tif link.startswith('https://'):\n\t\t\tcontinue\n\t\ttry:\n\t\t\tlink_url, link_target = link.split('#')\n\t\texcept ValueError:\n\t\t\tcontinue\n\t\tif link_target not in target:\n\t\t\tprocess_attr_with_link(link_url)\t\t\n\t\telse:\n\t\t\tif link_url.find(' ')!=-1:\t# for example, http://www.domain/\n\t\t\t\t"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\tfor link in slinks(text, linkify_all=False):\n\t\tif isinstance(link, str):\n\t\t\tcontinue\n\t\tlink.process_links(text)\n\nclass process_links_list(object):\n\tdef __init__(self, linkify_all=False):\n\t\tlinks = []\n\t\tif linkify_all:\n\t\t\tlinks = _process_links_all(links)\n\t\tself.links = links\n\n\tdef process_links(self, text):\n\t\tresults = []\n\t\tfor link in _"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\ttext.strip() # strip trailing whitespace\n\tlinks = text.split('\\n') # split to lines\n\tif len(text)!=0:\n\t\tnew_text = ''#link_to_text(linkify.link.escape(text)) \\\n\t\t\t#strip trailing whitespace\n\t\t\"\"\"\n\t\t#linkify.link.safe_substitute_html() (to escape all <a>)\n\t\t#linkify.link.replace('&nbsp;&nbsp;','' * (len(text) -4))\t#replace all links with \\t\n\t\t"}]}
{"task_id": "6306292652e177c0ba469f34", "prompt": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\theaders = {'User-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.99 Safari/537.36'}\n\tf = open(url, \"r\", encoding='utf-8', newline='')\n\tdata = f.read()\n\tf.close()\n\tif len(data) > 0 and \\\n\t    get_header_value('content-type',  header=True).strip().startswith(\"text/html|image"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\theaders = {\"Cookie\" : \"hretr=\"}\n\tp = urllib.request.urlopen(url, headers=headers)\n\tif \"Content-type\" not in p.headers.keys():\n\t\tfrom ezdxf.ezclient import get_ez_client\n\t\tclient = get_ez_client(url=url, cookie_name=\"hretr\", cookie_key=get_cookie_key())\n\telse:\n\t\tfrom ezdxf.ezclient import get_header_cookie_key\n\treturn get_header_cookie_key(client"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\tresponse = http.urlopen(url, content_type=\"text/html\")\n\tif response == None:\n\t\treturn None\n\treturn response.get(\"Content-Type\").strip()\n\ndef fetch_from(url: str) -> Any:\n\t\"\"\"\n\t    Fetch a specific page from a URL.\n\t\"\"\"\n\tresponse = request(url)\n\tparts = urllib.parse.urlparse(response.url)\n\tif len(parts) > 1 and parts[1][1] == 'http':\n\t\tif parts[0] == url:"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\tif not url.startswith(\"http://\"):\n\t\treturn None\n\turl = \"{0}/\".format(url)\n\turl = re.sub(r\"http://([^/]*\\.|)\\.(\\d+|)([^/]+)$\",\n\t\t\t   r\"(?P<name>\\S+)\", url)\t\t# type: ignore\n\tif url.startswith(\"http://\"):\n\t\treturn url[6:]\n\treturn None\n\ndef get_content_type(url: str) -> str:\t\t# type: ignore\t[return-value]\n\t"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\tif not '://' in url or url.startswith((':', 'https://', '')):\n\t\treturn None\n\treturn url[len('/'):]\n\n\ndef get_url(request: HttpRequest) -> str:\n\turl = get_url_and_path(request)\n\tif url is None:\n\t\treturn get_http_status(request)\n\treturn url[scheme_parse(url.rstrip('/'))][/']\n\n/*---------------------------------------------------------------------------------------------\n *  Copyright (c) Microsoft Corporation. All rights reserved.\n *  Licensed under the MIT"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\tr = get_remote(url)\n\tif r is None:\n\t\treturn None\n\n\tif r.status_code == 410:\t# Not a conflict.\n\t\treturn None\n\n\treturn r.content_type.split(\"<\")[0].split(\">\", 1)[0].lower()\n\n\ndef fetch_file(url, cache, check_etag=None, headers=None, verify_ssl=None,\n\t\theaders_only=False, timeout=0, verify=False):\n\t\"\"\"Fetch an uploaded file.\n\n\tIf you want to access"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\tparams = urlparse(url)\n\tparams.path.strip()\n\theaders = {\"Content-Type\": \"text/plain\"}\n\tret = requests.head(url + \"?\" + params.to_url_string(), headers=headers)[:1000]\n\tif ret is not None:\n\t\treturn ret[0].get(\"Content-Type\", \"\")\n\telse:\n\t\tprint(\"Unable to find HEAD of {!r}; check URL\".format(url))\n\t\treturn None\n\n# -----------------------------------------------------------------------------\nclass AddContentTypes():\n\tdef __init__(self) ->"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\ttry:\n\t\tf = open(url)\n\texcept IOError:\n\t\treturn None\n\telse:\n\t\treturn f.read()\n\n\ndef fetch(url: str) -> str:\n\t\"\"\"\n\tFetch a file by url and return the raw contents, otherwise download it\n\t\"\"\"\n\ttry:\n\t\tf = open(url)\n\texcept IOError:\n\t\treturn None\n\telse:\n\t\treturn f.read()\n\n\ndef fetch_downloads(url: str, folder: str) -> None:\n\t\"\"\""}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\treturn _fetch_content_type_raw(url, \"\", \"*/*\") if (not url.endswith(\"/\")) else url\n\ndef url_to_filename(url: str) -> str:\n\t\"\"\" Given a url string and one of its extension in one string \"\"\"\n\treturn urlparse(url).scheme + \".\" + \\\n\t\turlparse(url).\"//\".sub(r\"\\.\", r\"/\").strip()\n\ndef _fetch_content_type_raw(url: str,\n\text: str = \"\",\n\tdefault_language: str = \"en\",\n\tinclude"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\"\"\"\n\tFetch the content type of the remote url.\n\t:param url: The url of the remote url to fetch. Can be changed at runtime.\n\t:return: The content type of the remote url. For example: 'application/json'.\n\t\"\"\"\n\ttry:\n\t\turl = urllib.request.urljoin(base, url)\n\texcept:\t\t# pylint: disable=unidiomatic-typecheck\n\t\t# We're dealing with the first url we tried to determine the content type. This\n\t\t# means that there is a network connection. You should"}]}
{"task_id": "6306292a52e177c0ba469f41", "prompt": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\tif tag == None:\n\t\treturn -1\n\ttag = tag[len(' '):]\n\tfor pattern in (\"\", \",\", \"\"):\n\t\tpat = pattern\n\t\tfor i, part in enumerate(tag):\n\t\t\tfor item in part.split():\n\t\t\t\tif item.find(pat)!= 0:\n\t\t\t\t\treturn 2\n\treturn 0\n\n\ndef test_name(name: str) -> bool:\t\"\"\"Test whether given list of words is recognised as names.\"\"\"\n\treturn any(len(word) > 0 for word in name)"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\ttag_file = \"/home/gandi/temp/tag_file.txt\"\n\tif fileutil.file_exists(tag_file) == False:\n\t\treturn False\n\treturn True\n\n\ndef test_tag_type(tag: str) -> str:\n\ttry:\n\t\tos.mkdir(\"/home/gandi/temp/tag_file.txt\")\n\t\tos.mkdir(\"/home/gandi/temp/tag_test.txt\")\n\texcept:\n\t\tpass\n\n\toutput = \"[1,2,3]"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t# Tag is too short to be accepted by the parser.\n\tfor i in range(0, 14):\n\t\tif tag in tags:\n\t\t\tprint(\"Valid tag received!\")\n\t\t\treturn True\n\t\t\n\t\tword = \"%\".join(tags.get(i, tags.get(\"-\")))\n\t\tif is_valid_word(word):\n\t\t\tprint(\"Valid word received!\")\n\t\t\tbreak\n\tif tag == \"-\":\n\t\tprint(\"Tag-field-value not accepted!  Should've used \", tag)\n\t\treturn False\n\t\n\treturn True"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\ta, _ = parse_tag(tag)\n\tb, _ = tag.split()\n\tif a!= b:\t# Tag is actually not a word.\n\t\treturn False\n\tc, d = get_tag_values(a)\n\tif b not in c:\t\t# Word failed validation.\n\t\treturn False\n\treturn True\n\n\ndef check_tags_for_names(output: Output, test_tag: bool) -> None:\n\t\"\"\"Test the check_tags_for_names function.\n\n\tRun test_tag(tag)"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t# The name contains all the possible chars and the underscore can serve as the\n\t# word name.  See examples here:\n\tnames = [\"and\", tag]\n\tword_values = ['and', tag]\n\tword_values_set = set()\n\tfor name in names:\n\t\tword_values_set.add(name.lower())\n\tassert word_values_set.issubset(set(names))\n\ndef test_get_word_values_lowercase(tag: str):\n\t# test with lower case\n\tword = \"and\", \"w"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\tf = NomadsFileReader()\n\tp = f.read([tag])\n\tf.close()\n\tn = NomadsTagReader()\n\tp2 = n.read()\n\tn.close()\n\n\tassert p == p2, \"\\nexpected:\\n  \" + repr(p) + \"\\n  read by read tag:\" + repr(tag)\n\n\nclass TestNomadsTagReader:\n\n    def test_read_all(self):\n        f = NomadsFileReader()\n\n    "}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\tfrom s3.tags.tag import Tag\n\n\tfor key, value in [(\"some\", \"value\"), (\"custom\", \"some-value\"), (\"custom\", \"{custom1}\".format(key=key))]:\n\t\texpected = Tag(key) == value\n\t\tgot = Tag(key) == tag\n\n\t\tassert expected.is_match(got), \"Tag %r: %r should be %s\" % (key, value, got)\n\nif __name__ == \"__main__\":\n\trun_s3_test()\n\n// Licensed to the Apache Software"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\tself.check(\"wig\", tag + \" \")\n\tself.assertFalse(self.check(\"bad:\", tag + \" \"))\n\ndef test_char_class():\n\tclass MyClass:\n\t\tdef __init__(self, foo):\n\t\t\tself.foo_attr = foo\n\ttag = \"MyClass\"\n\tself.check(tag +'foo:', tag +'', myClass = MyClass(foo))\n\nclass MyClass:\n\tdef __init__(self, foo):\n\t\tself.foo_attr = foo\n\n@check\nclass Tag("}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\tresult = True\n\tfrom lncrawl.models.parsers.tags import parseTag\n\n\tnlp = parseLnc(f=f)\n\n\ttags = {\"oneTag\": \"oneTag\", \"twoTags\": \"twoTags\"}\n\tparseTag.tag(nlp, \"oneTag\", tags, token=tag)\n\ttry:\n\t\tnlp.move(\"oneTag\", 1, \"twoTags\")\n\t\tparseTag.tag(nlp, \"twoTags\", tags, token=tag)\n\t\tassert False, \"A tag with two tags should be accepted!\"\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\treturn tag.find(\">\") > 0 or tag.find(\">\") > 0\n\n# The default tag acceptance tests.\ns4p = (test_tag(\"squad4p\")\n\t\t\t\t\t\t.startswith(\"tag\") and not test_tag(\"s4p\"))  // s4p only tests, not tag\n\n/** @file\n  The PCI Configuration Information protocol provides\n  access to configuration space to configure a range of PCI devices which can\n  be used for a given PCI device in conjunction with an operation on a PCI\n  bridge device that is managed by an EFI firmware in system"}]}
{"task_id": "6306298b52e177c0ba469fdc", "prompt": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tif not isinstance(node, Node):\n\t\treturn {}\n\tif isinstance(node, basestring) and node[0] == \"</xml>\" and node.endswith(\"</xml>\"):\n\t\treturn {node}\t# tag -> <xml/>\n\t# recurse into contents\n\ttag_list = {\n\t\t\"doc\":\"<doc/>\",\n\t}\n\tchildren = node.xpath('./xml')[:-1]\n\tfor tag_name, node in zip(children[:-2], children[2:]):\n\t\tfor child_tag, child_node in zip(node"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\treturn {}\n\n\ndef get_ns_info(name):\n\t\"\"\"\n\tHelper for looking into namespace and attr names, returning info about\n\tthe n.b.  This is only a shallow conversation, the xml namespace and name is only looked up.\n\t\n\tIt's possible for one \"self\" object to use namespace info, but \"parent\" context info\n\twill not be found.\n\n\tReturns: tuple (tag name tuple, namespace info, parent info).\n\t\"\"\"\n\treturn (name, 'xml', None)\n\n\ndef _ns_info_for"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tfor childs in node.getchildren():\n\t\t# childtag[0] is the tag name of the first text childnode of <xml>\n\t\t# childtag[1] is the tag name of the second text childnode of <xml>\n\t\t# childtag[2] is the tag name of the third text childnode of <xml>\n\t\tfor i in range(0, len(childs)):\n\t\t\tchildstr = node.tag[childs[i][0]]\n\t\t\tif childs[i][1]:\n\t\t\t\tchildstr += \""}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tif not hasattr(node, \"children\"):\n\t\treturn node.children\n\n\tnode_dict = dict()\n\tfor child in node.children:\n\t\tif children_as_dict(child):\n\t\t\tnode_dict[child.tagname] = [node.text, child.text]\n\n\treturn node_dict\n\n\ndef node_xml_as_dict(node):\t\"\"\"Turn the children of node <xml> into a dict, keyed by tag name.\n\nThis is a much simpler conversation - child nodes are restructured from the\ntree.\n"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\treturn next(node.children).items().as_dict(\n\t\t'children',\t\tnode.tag_for,\n\t\t'children_set',\tnode.children_set)\n\n\ndef xml_children_as_set(node):\t\"\"\"\n\tTurn the children of node <xml> into a set.\n\n\tAlso returns the set of children with the tags <tag><set>.\n\n\tThis is a shallow conversation.\n\t\"\"\"\n\treturn next(node.children).items().as_set()\n\n\ndef xml_content(data, content_"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tfor child in node.get(\"children\", []):\n\t\tif child.tag in (\"html\", \"xml\"):\n\t\t\t# we recurse this down the tag\n\t\t\treturn xml_children_as_dict(child.children[0])\n\treturn dict()\n\ndef xml_attributes(node):\t\"\"\"\n\tTurn the attributes of node <xml> into attributes for the first element.\n\nTags can be nested with the following attributes:\n\n\t</xml> is replaced with a new one, each attribute in turn\n\twill become an XML nested element, and attributes with these\n"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\treturn dict([_(i, node[i]) for i in node.tag_nodes])\n\t\n\n@implementer(XMLVisitor)\nclass XMLObjectsVisitor(object):\n\t\"\"\"\n  The following visitor will iterate over xml_nodes and build a set for each xml_node\n\t\"\"\"\n\txml_visitor = 'xml_visitor'\n\txnode_dict = None # the list of xml node ids and xml tags\n\t\n\tdef __init__(self, nodes):  \"\"\"\n\t\tself.nodes = nodes\n\t\tself.xnode_dict = dict(xml_"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tchildren = dict((c.tag, c) for c in node.iterchildren('tag'))\n\tfor tag, children in list(children.items()):\n\t\tif tag:\n\t\t\tchildren.pop(tag)\n\tresult['children'] = children\n\n\treturn result\n\n#pragma once\n\n#include \"libultra_internal.h\"\n#include \"common_functions.h\"\n#include \"game/types.h\"\n#include \"config_manager.h\"\n#include \"sound_object.h\"\n#include <memory>\n\nclass"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\tif not node:\n\t\treturn\n\n\t#\n\t# Recursion for nodes which are not children or children of parents\n\tif not node.hasChildNodes():\n\t\treturn node.childNodes\n\n\ttag = node.tag\n\n\t#\n\t# Extract the xml tree if it contains only the tags.\n\twhile 1:\n\t\ttag = tag.split(\":\", 1)[0]\n\n\t\tif \":\" not in tag:\n\t\t\tbreak\n\n\t\tyield node.ownerDocument.createNode(tag)\n\n\nclass Element(DocumentNode, Node):\n"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\ttag = node.tag\n\tif tag == node.name and tag == \"root\":\n\t\treturn {}\n\telif tag == node.tag and xml_text_is_whitespace(node.value):\n\t\treturn {}\n\telif tag == \"html\":\n\t\treturn {\"version\": '1.0',\n\t\t\t\t\t \"encoding\": 'utf-8',\n\t\t\t\t\t \"xml\": node.value.decode('UTF-8') }\n\telif tag == 'head':\n\t\treturn {\"version\": '1.0',\n\t\t\t\t\t \"encoding\": 'utf-8',\n\t\t\t\t\t"}]}
{"task_id": "6306299052e177c0ba469fe8", "prompt": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t# assert sender_handle in ('from_api_test_user3@example.com', 'from_test_user3@example.com', 'from_public_user3@example.com')\n\tassert sender_handle == 'from_api_test_user1@example.com'\t\t# Should match\n\t# assert entity_handle in ('from_public_user2@example.com', 'from_public_user3@example.com')\t# Should NOT match\n\tassert not (receiver_handle in ('from_public_user2@example.com',"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tassert isinstance(sender_handle, type(HanziObject))\n\tassert isinstance(object_uuid_to_entity(sender_handle.entity_uuid), type(HanziEntity))\n\tassert isinstance(entity_uuid_to_sender(sender_handle.entity_uuid), type(HanziObject))\n\tassert sender_handle.handle == entity_handle.handle\n\tassert sender_handle.uid == entity_handle.uid\n\n@pytest.mark.parametrize(\"data\", [[]], ids=str)\ndef test_handle_verification_failure(data):"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tsender = sender_handle.get_data_item_as_object()\n\tentity_handle = entity_handle.get_data_item_as_object()\n\tpayload = from_hex_string(entity_handle.get_data_item_as_string())\n\tpayload_type = from_hex_string(entity_handle.get_data_item_as_string())\n\tassert payload.get_type()==payload_type\n\tif payload.get_string_value() == _base64_encode_string(sender.get_data_item_as_"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tmessage = \"{sender_handle}: {entity_handle}\".format(\n\t\tsender_handle=sender_handle,\n\t\tentity_handle=entity_handle)\n\tresponse = requests.put(\n\t\tf\"https://dm-api.com/{sender_handle}:123/events/v0/replies/{entity_handle.handle}\",\n\t\tjson=message,\n\t\tverify=True,\n\t)\n\tassert response.status_code == 204\n\tresponse = requests.get(\n\t\tf\"https://dm-api.com/{sender_handle"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tpass\n\n/***************************************************************************\n\n\tAtari Trap\n\n\tDriver by David Hayden\n\n\tGames in play:\n\t\t* Triton Sorok\n\n\t   - Russ Dalay - Russ Dalay_3.bin\n       - Russ Dalay Rat - Russ Dalay_3.bin\n\t\t+ Triton Sorok - Triton Sorok_3.bin\n\n\tMiscellaneous:\n\t\t- Unused1 - Unused3\n\n\nMain Games are"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\ttry:\n\t\tsender_handle = entity_handle[0]\n\texcept IndexError:\n\t\t# This is a valid transaction in the form of a record id\n\t\tsender_handle = utils.get_transaction_id_record(entity_handle, 3)\n\tassert sender_handle in H_senderhandles_with_owner_names\n\n\ttry:\n\t\tentity_handle = utils.get_transaction_id_record(sender_handle, 3)\n\texcept ValueError:\n\t\tpass\n\tassert entity_handle!= 0\n\n\n@check_"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t# get the sender name (must fail)\n\tsender = get_sender(sender_handle)\n\tassert sender is not None\n\n\t# get the publisher who the sender is\n\tpublisher = get_publisher(entity_handle)\n\tassert publisher is not None\n\n\t# if not in one of the author's list then this is bad.\n\tif not is_publisher_in_list(publisher.publisher_list, sender.publisher.publisher_list) as sender_in_list:\n\t\tlog_debug('Skipping message %s because %s is not in publisher list"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\ttry:\n\t\tif sender_handle == entity_handle:\n\t\t\treturn True\n\t\telse:\n\t\t\traise TypeError, (\n\t     \"Payload header not in the body.\")\n\ndef build_payload_builder(channel_path, logger, **kws):\n\t\"\"\"\n\tBuilds the payload parser according to the channel path and the\n\tobject. The channels are either in the format like:\n\n\tchannel\tbody\tobject\tbody\theaders\n\tobject\tobject\tobject\tobject\tobject\n\tobject\tchannel\tchannel\tbody\tbody\t"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tif not sender_handle:\n\t\traise InvalidSender(sender_handle=sender_handle)\n\tif not entity_handle:\n\t\traise InvalidEntity(entity_handle=entity_handle)\n\tif not self.is_sender_match(sender_handle, entity_handle):\n\t\traise InvalidSender(sender=sender_handle, entity=entity_handle)\n\ndef _find_sender_handle_and_entity_handle(sender_handle, entity_handle, requestor):\n\t# It probably works on a bunch of things, but it is a lot slow"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\tif sender_handle is not None and sender_handle!= entity_handle:\n\t\traise errors.NotFoundError('sender_handles should be the same. The key of sender/entity_handle should be the sender'\n\t\t\t'  or entity_handle.')\n\tif entity_handle is not None and entity_handle!= sender_handle:\n\t\traise errors.NotFoundError('entity_handle should be the same. The key of sender/entity_handle should be the'\n\t\t\t' entity_handle.')\n\ndef read_from_file(file_name, file_object="}]}
{"task_id": "630629b952e177c0ba46a043", "prompt": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\tif url is None or url[0]=='/' and document_path is None:\t\t# No path supplied, generate by default\n\t\treturn {}\n\telse:\t\t\t\t\t\t\t# No paths supplied\n\t\treturn {'path': document_path}\t# Custom NodeInfo document path if document_path is supplied\n\t\treturn {}\n\npackage com.jeppeman.globallydynamic.configuration;\n\nimport java.util.Collection;\n\npublic interface ConfigurationResolver\n{\n    PropertiesConfiguration getConfigurations();\n\n    Collection<? extends PropertyConfiguration> getProperties("}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\tif document_path is None:\n\t\tdocument_path = get_all_node_id()\n\n\treturn generate_document(url, document_path)\n\n\t\ndef get_nodeinfo_custom_nodeset(url):\n\tdoc_path = url.split(\".\")[-1].split(\".\")[-2] + \".well-known.dict\"\n\treturn nodeinfo_export_docs(\"custom\",doc_path) if doc_path else None\n\n\n# # # # # # # # # # # # # # # # # # # # # #"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\tif not document_path:\n\t\tdocument_path = os.path.join('/', os.path.splitext(urllib.parse.urlsplit(url))[:2]+'/'+urllib.parse.urlsplit(url)[-1])\n\tdocument_path = os.path.join('/', document_path)\n\tnodes = urllib.request.urlopen(url, None, True).read().decode()\n\treturn nodes.split('/')[2].split('@')[0].replace('/', '.').replace('/', '').lower()\n\ndef extract_dict(data, default_format="}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\treturn {\n\t\t\"type\": \"nodeinfo\",\n\t\t\"url\": url,\n\t\t\"document_path\": document_path,\n\t\t\"encoding\": None,\n\t}\n\n@nodeInfo.register_extension(ExtensionNodeInfo)\ndef nodeinfo(url, document_path=None):\n\t\"\"\"\n\tReturn the information about any NodeInfo node for the client with the provided URL.\n\n\tExample for an alternate NodeInfo client like https://example.com/v3.\n\n\t:arg url: URL to get information about\n\t:returns:"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\treturn _get_well_known_document(url, document_path)\n\n\ndef get_nodeinfo_custom_document(url, document_path=None):\t# pylint:disable=too-many-branches\n\t \"\"\"Fetch NodeInfo custom documents for a given node.\n\n\t:arg url: The full base URL with protocol, ie https://example.com/foo\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\t\"\"\"\n\treturn _get_node_info_custom_document(url)\n\n\n"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\tpayload = {\n\t\t'url': url,\n\t}\n\tif document_path:\n\t\tpayload['document_path'] = document_path\n\t\treturn payload\n\treturn None\n\ndef parse_body_as_dict(raw_body):\n\t\"\"\"\n\tParses the body of a NodeInfo response object.\n\n\t:arg raw_body: string with a serialized NodeInfo response object\n\t:returns: a dict of NodeInfo response objects\n\n\t:rtype: dict\n\t\"\"\"\n\tif raw_body[:4]!= '#{':\n\t"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\treturn {'@context': {\"name\": url.split('/')[-1] + '_metadata', 'value': document_path}}\n\n\ndef parse_nodeinfo_file(file_content, default_name=None, parse_tags=True, load_document=True,\n                        overrideDocumentName=None):\n\t\"\"\"\n\tParse a NodeInfo file and return a NodeInfo.\n\n\t:arg file_content: String to handle the content in.\n\t:param string default_"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\targs, unknown_arg = get_simple_argument(url, document_path, 'nodeinfo.documents')\n\tif unknown_arg: return None\n\n\treturn dict(arg=['-', 'url', '-'], args=args)[:]\n\ndef get_nodeinfo_document(url, document_path=None):\t\t\"\"\"\n\tGiven a URL (or an absolute path), gets a nodeinfo document of the given\n\tURL. If document path is given, it will get the document path even if it\n\talready exists.\n\n\tIf a document path is given, it"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\turl_str = url_str or url.lstrip('/')\n\tif document_path:\n\t\tdoc = get_xml_document_by_path(document_path)\n\t\tassert doc.element_by_xpath('//*[@id=\"NodeInfo_WellKnownDocument\"]//html') is not None,\n\t\t\t'Invalid nodeinfo document name %s for wellknown url' % document_path\n\t\tif '.well-known' in doc.element_by_name('Name').text_content():\n\t\t\treturn doc.element_by_xpath('//*[@id=\""}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\tif document_path is None:\n\t\tdocument_path = url\n\twell_known_map = {}\n\t# XXX document_path can be None in NodeInfo case\n\tfind_node_in_path = urlsplit(url).path.startswith(\"/\")\n\tfor match in NodeInfo.document_matches(find_node_in_path,document_path):\n\t\twell_known_map.update(nodeinfo_cache.get(match))\n\treturn well_known_map\n\n\n# --------------------------------------------------------------------\n# get the node-info document\n# --------------------------------------------------------------------\n\ndef"}]}
{"task_id": "630629d052e177c0ba46a0a1", "prompt": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tif not doc.has_elements():\n\t\treturn\n\n\tkey1 = b\"<!-- <a/> -->\"\n\tkey2 = b\"<!-- <a/> -->\\n\"\n\n\tif key1!= signature.extract_element(key2).extract_element(key1).extract_element(doc):\n\t\traise InvalidElementError, (key1, signature.extract_element(key2))\n\n\ndef verify_signed_document_elements(signature, claimer_id, author):\n\t\"\"\"Verify that the document elements have been signed.\t\"\"\"\n\t"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tprivate_key = make_private_key(public_key)\n\tif signature is not None:\n\t\tdoc = str(doc) +' public key:'+ str(private_key)\n\t\tsig = pubkeys.signature_element(signature, doc)\n\t\tsig.sign(doc, private_key)\n\t\tassert len(sig.elements) == 1, \"Did not get required signature\"\n\t\tdoc = doc[:len(sig.elements[0].content)]\n\telse:\n\t\tdoc = doc\n\t\tsig = pubkeys"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tdoc = ET.fromstring(doc)\n\tdoc.parent.add_child(ET.document(\n\t\t{\n\t\t\t\"root\": { \"$\": public_key, \"data\": sig }\n\t\t},\n\t\t[public_key],\n\t\tinclude_context=False\n\t))\n\treturn doc\n\n\ndef verify_signature_for_root_signature(\n\tdoc: ET.Document,\n\tsig: string | tuple[tuple[str, int], int]\n) -> None:\n\t'''\n\tVerify that the signature for the root signature"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tself.assert_message_in_list(doc, \"author.did.verify\", [\"signature\"])\n\tself.assertTrue(is_signature_string_equal(doc, signature))\n\ndef verify_signature_document(public_key, doc, signatures):\n\t\"\"\"Verify that a signature document was made that the public\n\tkey and the signatures are within the bounding circle of the\n\tdocument and thus contain a signature that will be added in the\n\tsignatures array\n\t\t$a_doc>signature (1.1, 1.2, 1.3)\n\t\"\"\"\n\t"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tif not public_key:\n\t\traise UnverifiedException(\n\t\t    'public_key cannot be empty')\n\tif not doc:\n\t\traise UnverifiedException(\n\t\t    'doc cannot be empty')\n\n\tdoc = doc.decode('utf-8')[:]\n\tsignature = signature + ('xpub:'+ doc.hex.hexdigest())[:1]  # noqa: E501\n\tmsg = (\n\t    'Signing this message to a public key %s is not recommended. \\n\\n'\n\t    'If this"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t# Make sure the expected signature.\n\tif signature not in expected_signature_fields:\n\t\traise InvalidSignature(\n\t\t\t\"%s signature is not acceptable for verifying \"\n\t\t\t\"the Signed XML elements\" % str(signature)\n\t\t)\n\treturn public_key == doc\n\n\ndef decode_pub_signing_data(public_key, signature, ciphertext):\n\t\"\"\"Decode the public key and signature from a bytearray.\"\"\"\n\tif len(public_key)!= 20:\n\t\traise InvalidSignature(\n\t\t\t\"%s public key is not valid"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tdoc.validate_signature(public_key, signature)\n\n\ndef verify_signature(public_key, signature, cert=None):\t\"\"\"\n\tVerify the received public key against the signatures we computed\n\tfor this document.  If a signing certificate is present, verify it\n\tto ensure that the signature is in fact verified. If no signing\n\tcertificate is present, verify the signature to be verified.\n\n\tParameters\n\t* public_key : X.509 public key\n\t* signature : signed X.509 signature, or list of signatures (in\n\tsignee.get_"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\txml = xmltodict.parseString(xmltodict.dump(doc['message']))\n\tassert signature['author'] in xml['author']\n\tassert signature['author_name'] in xml['author_name']\n\tassert signature['date'] in xml['date']\n\tassert signature['doc'] == signature['doc_uri']\n\tassert signature['type'] == signature['sig_type'].lower()\n\tassert (\n\t\tsignature['public_key_url'] in public_key\n\t)\n\tassert (\n\t\tsignature['public_key_url'] in xml"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tif SIGNED_MSG_ID not in doc[MSG_ID].keys():\tassert not public_key\n\tif SIGNED_SIGNATURE not in signature:\tassert not public_key\n\tmsg = SIGNED_MSG_ID.add()\n\tmsg[MSG_ID] = signing.Key({DOC_ATTR: doc})\n\tmsg[MSG_SIGNATURE] = signature\n\tdoc[MSG_SIGN] = signing.Elem('doc_signature', msg)\n\tpub_key = doc[MSG_SIGN].get(msg.SIGNATURE)\t\t# self.doc['1."}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\tsignature = X509_CRL_HEADER.signature1.get_subject_public_key(public_key)\n\tif signature.get_type()!= \"RSA\":\n\t\t# If there is a signature, verify we have a valid (not correct) certificate.\n\t\tissuer_raw = unhex(signature.get_signature_value(\"\"))\n\t\tissuer_der = x509_element.RSAPublicKey.from_der(issuer_raw)\n\t\t# and that certificate can contain exactly the same data as our\n\t\t# ca.subjectPublicKeyInfo CRL.\n\t"}]}
{"task_id": "630629e052e177c0ba46a0c4", "prompt": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tdic_input = json.loads(document)\n\n\tif \"result\" in dic_input.keys() and \"value\" in dic_input[\"result\"]:\n\t\treturn dic_input[\"result\"][\"value\"]\n\n\treturn {}\n\n// Copyright 2013 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\n\npackage httputil\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--url\", help=\"The URL of the Diaspora webfinger hosted on https://api.github.com/user/diaspora/projects/an-example-diaspora-server-enterprise-enterprise-federation/dashboard/webfinger\")\n\n\treturn parser.parse_known_args()\n\n\nif __name__ == \"__main__\":\n\twebfinger_url = os.environ[\"DIASPINE_URL\"]\n\tconsole = console()\n\tdata = json.loads(http"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tjson_data_dict = JSONData(document)\n\twebfinger_webfinger = parse_webfinger_webfinger_request(json_data_dict[\"Request\"][\"Webfinger\"][\"Webfinger\"])[\"webfinger\"]\n\treturn webfinger_webfinger\n\n// Copyright (c) Microsoft Corporation. All rights reserved.\n// Licensed under the MIT License.\n\nnamespace AdaptiveExpressions.struct\n{\n    //\n    // The data to construct a new instance of a struct\n    //\n\n    /// <summary>\n   "}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t# Diaspora the user agent.\n\t# https://www.ia.gov/docs/ontologies/federation/warc.html#warc\n\t# https://www.ontologies.com/blog/2016/01/09/federated-official-security\n\tdocument = f\"https://www.ontologies.com/diaspora/data/?version=#{document}\"\n\tdocument = re.sub(\n\t\tr\"(.*)(.*).+({.+?})\", r\"(.+?)(.+?).+({.+?})"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tlogger.debug(\"In [%s] Diaspora webfinger document:\\n%s\", __name__, document)\n\tresult = {}\n\txml = ET.fromstring(document)\n\tfor tag in xml.getchildren():\n\t\tif tag.tagname == \"Diaspora\":\n\t\t\tresult = {}\n\t\t\tbreak\n\t\tif tag.tagname == \"webfinger\":\n\t\t\tif \"id\" in tag.children:\n\t\t\t\tresult[\"id\"] = tag.children[\"id\"].text\n\t\t\telse:\n\t\t\t\tresult[\""}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\treturn parse_xml(document).get(\"diaspora_webfinger\", {})\n\n\ndef get_diaspora_webfinger(id: str = None) -> Dict:\n\t\"\"\" get diaspora webfinger from a webfinger\n\t\"\"\"\n\tif id == \"\" or id == \"\":\n\t\treturn {}\n\treturn get_diaspora_webfinger(json.loads(id))\n\n\ndef parse_diaspora_webfinger_file(f: Union[bytes, TextIO]) -> Dict:\n\t\"\"\" Parse dias"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tdiaspora_config = \"https://www.diaspora.com/json/?start_page=start_page.txt\"\n\tdiaspora_diaspora_url = \"url\"\n\tdiaspora_diaspora_response = parse(diaspora_config)\n\tdiaspora_diaspora_page_link = \"https://diaspora.github.io/\" + \"webfinger.html\"\n\tdiaspora_diaspora_headers = \"access_token,\n\t\t"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\treturn Diaspora.diaspora_webfinger.parse(document)\n\n\ndef webfinger_to_dict(response: requests.Response):\n\treturn {\n\t\t'link': response.text,\n\t}\n\n\ndef webfinger_to_form(doc: Gsurl.Document) -> Gsurl.Document:\n\tif doc.fields['link'].lower() == '':\n\t\treturn doc\n\treturn doc.append_fragment(FRAGMENT_HTTP_URL_URL_QUERY_PARAMETER, url_params_to_dict(doc."}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tif not document:\n\t\treturn {}\n\tdf = DocumentFragment()\n\tdf[\"name\"] = document.split(\" \")[1].strip()\n\tprint(\"Diaspora webfinger is: \" + \",\".join(df[\"name\"]))\n\n\ndef parse_webfinger_profile(document: str) -> Dict:\t\"\"\"\n\tParse an HTML WebFinger profile element for webfinger.\n\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tprofile = DocumentFragment()\n\t"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\tdiaspora_info = get_diaspora_info(document)\n\n\tdoc = document.lower().strip()\n\n\tif len(doc.strip()) == 0:\n\t\traise OSError(f'Could not retrieve Diaspora webfinger from the Discovery file {document}.')\n\treturn diaspora_info\n\n\n@python_2_unicode_compatible\ndef parse_youtube_videos(document: str) -> Dict:\n\t\"\"\"\n\tParse Social videos which are either in JSON format (new) or XRD"}]}
{"task_id": "630629e152e177c0ba46a0d1", "prompt": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t# handle of an http_server(s) instance which is not implemented?\n\t# for now, try for a first time\n\ttry:\n\t\tfrom_stream(handle, headers['Content-Type'])\n\t\treturn handle\n\texcept AttributeError:\n\t\tfrom_stream(handle, None)\n\ndef try_retrieve_webfinger_document_with_header(uri: str,\n\t\t\t\t\t\t\t\theaders: dict = None) -> str:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\n\t"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\ttry:\n\t\tif type(handle) is str: handle = handle.decode()\n\n\t\thttp = HTTPClient()\n\t\t\t# Check for HTTPS request\n\t\tif http.request(\n\t\t\t\t\"https://api.twitter.com/1.1/tweets/retrieve?access_token={}\".format(\n\t\t\t\thandle),\n\t\t\t\tmethod=\"POST\") is None:\n\t\t\t\treturn None\n\n\t\treturn urllib.urlopen(http.request(\n\t\t\t\t\"https://api.twitter.com/1.1/tweets/retrieve?access"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\tmsg = _get_data(handle,'msg', 'webfinger-response-header')\n\tif not msg:\n\t\traise error_http_404('webfinger-response-header is missing. Missing '\n\t\t\t\t\t\t  '\"msg\" header.', handle=handle)\n\treturn msg.get('msg')\n\n\ndef _set_webfinger_response_document(url: str) -> None:\n\t\"\"\"Set the webfinger document from an HTTP URI.\n\t\"\"\"\n\tparsed_url = urllib.parse.urlparse(url)\n\tif parsed_url.port"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\twebfinger_document = \"\"\"## Webfinger Document\nFrom: CWE-1377\nStatus: Passive & Broken\nFormat: JSON\nContent-Type: text/json\nContent-Transfer-Encoding: quoted-printable\nContent-Location: http://www.webfinger.com/\n\n{\n    \"htmlUrl\": \"http://www.webfinger.com/test/1.2.3.html\"\n              \"https://www.webfinger.com/test/1.2.3.html\""}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\ttry:\n\t\treturn get_webfinger_document(handle)\n\texcept APIException:\n\t\tlogger.exception(\n\t\t\tf'Failed retrieving an RFC7033 webfinger document from {handle}.'\n\t\t)\n\treturn None\n\n// Copyright (c) 2019 Google LLC\n// \n// Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n// use this file except in compliance with the License. You may obtain a copy of\n// the License at\n// \n// http://www.apache.org/licenses/LICENSE"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\ttry:\n\t\twebfinger_doc = requests.get( 'https://ws.webfinger.com/', timeout=10, verify=False, headers = { 'Cache-Control' : 'no-store', 'Connection' : 'close' })\n\t\treturn cast(str, webfinger_doc.text)\n\texcept requests.exceptions.Timeout:\n\t\treturn None\n\n\ndef get_webfinger_document_version(host: str) -> Tuple[int, int, int]:\n\t\"\"\"\n\tReturn the version of an RFC7033 used to check if the document"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\ttry:\n\t\tresp = http_get_bytes(handle, \"\")\n\t\tbody = resp[:resp.find(b\"data\\x00\")]\n\texcept:\n\t\treturn 0\n\treturn body.rstrip()\n\npackage com.google.sitebricks.acceptance;\n\nimport com.google.sitebricks.AcceptanceTester;\nimport com.google.sitebricks.HandlerFactory;\nimport com.google.sitebricks.Response;\nimport com.google.sitebricks.ResponseHandler;\nimport com.google.sitebricks."}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\ttry:\n\t\tresponse = get_webfinger_document()\n\t\t\ntry_retriable:\n\t#if no response from webfinger, then use the last error from a previous successful attempt\n\tif response:\n\t\traise WebfingerResponse(response)\n\telse:\n\t\traise Exception(\"No response at all from webfinger\")\n\t\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\tif handle is None:\n\t\thandle = \"N/A\"\n\n\ttry:\n\t\tresult: str \t\t# Optional\n\t\theader\t\t\t# Header section\t\t(see RFC2447)\n\t\twebfinger_document = \"HTTP://\"\n\t\tbuffer_length := 65536\t# Max buffer length\t(see RFC7033)\n\t\tpayload_header\t= header + webfinger_document\n\t\tbuffer\t\t\t= buffer_length\t\t\t# Bytes per segment\n\t\tdata\t\t\t= None\t\t\t\t# No data yet, go for payload"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\treturn api.webfinger_document(handle)\n\n\ndef try_retrieve_webfinger_text(handle: str) -> Optional[str]:\n\t\"\"\"try to retrieve a Webfinger text document. Does not raise if it fails.\n\t\"\"\"\n\n\treturn api.webfinger_text(handle)\n\n\ndef try_retrieve_video(handle: str) -> bool:\n\t\"\"\"try to retrieve an RFC7033 webfinger video document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn api.video(handle)\n\texcept API"}]}
{"task_id": "630629e152e177c0ba46a0d2", "prompt": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t# fetch remote entry\n\td = DiasporaWebfinger.remote_document(handle)\n\treturn handle.get_d_dict() if d else {}\n\n// Copyright (c) 2021 Digital Asset (Switzerland) GmbH and/or its affiliates. All rights reserved.\n// SPDX-License-Identifier: Apache-2.0\n\npackage com.daml.ledger.test.services\n\nimport java.time.Instant\n\nimport akka.Done\nimport akka.stream.ActorSelection\nimport com.daml.ledger.api.v1.ledger"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\tdiaspora = dict()\n\tlocal_url = get_localurl()\n\n\twebfinger_header = header.WEBFOUNDCLOUDING_DIASPORA_WEBFINGER_HEADER.format(local_url)\n\tremote_url = parse_webfinger_header(context, webfinger_header, handle, data_path='')\n\n\tdiaspora[data_key] = {}\n\n\tif not remote_url:\n\t\treturn diaspora\n\n\tif remote_url.startswith('file:'):\n\t\tdones"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\turlretrieve = \"https://raw.githubusercontent.com/RikalRibner/diaspora-webfinger-handler/master/diaspora-webfinger-handler/%s.webfinger.json\"\n\tdata = request.get_json(urlretrieve(handle), encoding=DEFAULT_ENCODING)\n\treturn retrieve_and_parse_diaspora_webfinger_data(handle, data.get(\"data\"))\n\ndef retrieve_diaspora_webfinger_data(handle, data):             "}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\treturn retrieve_and_parse_diaspora_webfinger_json(handle)\n\n\nclass RetrieveAndParseDiasporaWebfication(RetrieveAndParser):\n\t\"\"\"Handle requests using the RetrieveAndParser class.\"\"\"\n\n\tdef __init__(self, handle):\n\t\t\"\"\"\n\t\tCreate a new RetrieveAndParser instance that receives a remote Diaspora\n\t\twebfinger document from a service.\n\t\t\"\"\"\n\t\tRetrieveAndParser.__init__(self, handle)\n\n\tdef retrieve_and_parse_diaspora(self, handle"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\tremote_url = str(get_remote_url(handle))\n\ttry:\n\t\tresponse = requests.get(remote_url)\n\texcept (requests.exceptions.RequestException, requests.exceptions.RequestConnectionError):\n\t\treturn {}\n\tparser = json.JSON(response.content)\n\treturn parser['data']\n\n\ndef retrieve_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a Diaspora webfinger or a local webfinger from a different Diaspora.\n\n:arg handle: Remote handle to retrieve\n:"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\treturn handle.retrieve_and_parse_diaspora_webfinger(request.url_name,\n\t\trequest.response_type,\n\t\thandle.id)\n\n\ndef retrieve_webfinger_webpage(handle):\n\t\"\"\"\n\tProvide a webpage which serves a webfinger as a form field on the\n\twebfinger\n\n:type handle\n:arg handle: Remote webfinger\n:returns: dict\t\"\"\"\n\treturn handle.retrieve_webfinger_webpage(\n\t\thandle.id, request.response_type)\n\n\ndef retrieve"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\treturn self.retrieve_data(handle)\n\npackage org.batfish.representation.infinispan;\n\nimport java.io.Serializable;\nimport java.util.Optional;\nimport javax.annotation.Nonnull;\nimport javax.annotation.Nullable;\n\n/**\n * Denotes that the IP address of the BGP peer is unreachable.\n *\n * <p>In Jun 18, BGP peer is reported to a peer that has not yet reported one. That is, we receive an\n * ICMP error with an unknown BGP peer type and we must not send back replies with no"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\treturn {'diaspora_webfinger_id': get_diaspora_webfinger_id(handle),\n\t\t\t'diaspora_webfinger_id': get_webfinger_from_dict('diaspora_webfinger_id', handle)}\n\n\ndef get_diaspora_webfinger_id(handle):\n\t\"\"\"Retrieve the ID of a Diaspora Webfinger document.\"\"\"\n\ttry:\n\t\treturn get_handle_from_doc_dict(handle)\n\texcept:\n\t\tlogger."}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\ttry:\n\t\tresponse = requests.get(\n\t\t\tstr(handle)[len(\":\"):]+\".http://\" +\n\t\t\t\tstr(handle).strip(\"/\") +\n\t\t\t\t\"/rest/2.0/webfinger\", timeout=2)\n\texcept (urllib.error.HTTPError, urllib.error.URLError) as e:\n\t\treturn {'errors': [e]}\\\n\t\tif handle.startswith('\"'):\n\t\t\treturn {'error': \"An error occurred accessing \" +\n\t\t\t\t\"diaspora webfinger\",'message': e.message}\n"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\tresponse = None\n\tif handle in diaspora_webfinger_doc_type:\n\t\t# Retrieve a dictionary from diaspora-webfinger.zip...\n\t\tdata = diaspora_webfinger_doc_type[handle]\n\telse:\n\t\t# Retrieve a list from diaspora-webfinger.zip...\n\t\tdata = diaspora_webfinger_doc_type[handle]\n\n\t# Create dictionary structure and parse json response...\n\tresponse = json.loads(data.decode())\n\tif response is None"}]}
{"task_id": "630629e252e177c0ba46a0d6", "prompt": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn get(XRD, resourceID=host.resourceID).Object\n\ndef get_remote_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn get(XRDS, resourceID=host.resourceID).Object\t\n\t\ndef get(resource_key, name):\n\treturn remote_domain_diaspora.get_remote_by_id(\n\t\tremote_domain_diaspor"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\tfor k, v in host.host_meta.items():\n\t\tif isinstance(v, dict) and all(isinstance(item, str) and item.lower().startswith('host') for item in v.values()):\n\t\t\tname, value = k.split('.')\n\t\t\tdoc = HostMetadataDocumentDict(host, name)\n\t\t\tdoc.values.insert(0, value)\n\t\t\treturn doc\n\traise RuntimeError(f'Could not find {k.lower()} = {value}')\n\n\ndef read_remote_diaspora_host_meta("}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\turi = geturi()\n\treturn fetch_direct_data(uri)\n\nclass RequestMetaDocumentFetcher (base.RequestsMixin, XrdResponseMixin, base.Requestor):\n\t\"\"\"\n\tRequest a remote Diaspora doc. \n:param host: The host in which the request is for,\n:param remote_method: Remote method for the request data,\n:returns:\n:doc: A ``XRD`` instance\n\"\"\"\n\tdef __init__(self, host, remote_method=None, timeout=time.timeout, check=None):\n\t\tsuper"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn xrd.RemoteXDERecord(host)\n\n\nclass RemoteXRD_Get_Host_meta(RemoteXDERecord):\n\t\"\"\"Implements RemoteXRD_Get_Host_meta interface.\n\n\t\"Getting a host-meta document\" is an optional extension to the remote\n\tXRD server. The remote XRD service is typically used by some programs\n\tsuch as the XRD backend to set up a proxy when communicating with\n\tremote X11 servers. In other words, a server could use this as an XDS\n\tquery parser with a non-"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn remote_meta_retrieve(host, 'diasporaHost')\n\n\ndef retrieve_network_meta(host):\n\t \"\"\"Retrieve a remote network host-meta-item.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn remote_meta_retrieve(host, \"networkHost\")\n\nconst assertDeepEqual = require('assert-deep-equal');\nconst assert = require('assert');\nconst { expect } = require('chai');\n\ndescribe('test/rules/get-rule', () => {\n  test"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\tif host is None:\n\t\tif utils.is_remote_host(host):\n\t\t\thost = host.connection.connection.host\n\tif host in remote_host_names:\n\t\treturn remote_host_names[host]\n\tfor host in remote_host_names:\n\t\tif host_name = host.lower():\n\t\t\tif host_name.startswith(host_prefixes.diaspora_host):\n\t\t\t\treturn host_name\n\t\telif utils.is_remote_host(\"diaspora:all:*\", host) or"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn get_remote(\"https://api.github.com/repos/diaspora/diaspora/access_tokens/%s/metadata\" %host.token)\n\ndef retrieve_pull_request_url(host):\t\"\"\"\n\tRetrieve local Diaspora host-meta document for a repository given by its\n``diaspora_host_id``.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\turl = get_remote(\"https://api.github.com/repos/%s/pulls/%s\""}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\tif not host:\t\t\t\"\"\"Host already retrieved (likely\n\tfrom a previous call)\t\"\"\"\n\t\treturn\t\t\"\"\"Diaspora host not found in the direcamodet list\"\"\"\n\tremote_doc = XRD().XRD_D_hosts.get_by_host_id(host['id'])\n\n\tXRD().logger.debug(f'Requesting remote doc of host-meta: { remote_doc.id } '\n\t\t\t\t\t\tf'from direcamodet.  Remote docs with id { host['id'] } '\n\t\t\t\t\t\t"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\treturn datastore.retrieve_item('DiasporaHostMeta', hostid=host, return_id=True)\n\n\ndef retrieve_diaspora_host_template(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-template document.\n\n\t :arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\t\"\"\"\n\treturn datastore.retrieve_item('DiasporaHostTemplate', hostid=host, return_id=True)\n\n\n# ----------------------------------------------------------------- #\n\ndef retrieve_host_info_all(hostinfos"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\tif hasattr(host, 'XRD') and host.XRD:\n\t\treturn host.get_doc(XRD.KEY)\n\telse:\n\t\treturn None\n\n\ndef retrieve_host_service_dict(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora service dictionary for a host.\n\n\thost: The remote host to retrieve from\n\t\"\"\"\n\tret = {}\n\tif hasattr(host, 'XRD') and host.XRD:\n\t\tret = {\n\t\t\t'type':   'host',\n\t\t\t'"}]}
{"task_id": "630629e752e177c0ba46a0fb", "prompt": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\tfrom. import session\t# FIXME. Should be in config class\n\n\treq, _ = session.get(url, *args, **kwargs)\n\tif not req:\t\t\t\t\t\t# request not found in session, let's log\n\t\tprint(\"No request found for \" + url)\t# TODO: log if not found\n\t\treturn {'status': None,'msg': 'no requests found in'+ url,\t\t\t# status, errmess, errmessp, errtime, emsg\n\t\t\t'errdate': None, 'errtime': None, 'errtimetxt"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\tif method == \"post\":\t# POST\n\t\tdata = data.decode(\"utf-8\")\n\t\treturn Request.send_json(url, data, timeout=timeout, method=\"POST\", *args, **kwargs)\n\telse:\t# GET\n\t\treturn Request.send_json(url, data, timeout=timeout, method=method, *args, **kwargs)\n\n/*!\n* This program is free software; you can redistribute it and/or modify it under the\n* terms of the GNU Lesser General Public License, version 2.1 as published by the Free Software"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\tif isinstance(url, str):\n\t\turl = urllib.unquote(url)\n\tif not isinstance(data, dict):\n\t\tresponse = open_file(url, method, *args, timeout=timeout, method=\"POST\", *args, **kwargs)\n\t\treturn response.code, response\n\telse:\n\t\treturn None, None\n\ndef prepare_request(url, method=\"POST\", timeout=10, verify=False):\n\t'''This method may be overwritten by one or more other methods if the application knows,\n\tthe method may need to be added"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\tif url.lower()!= \"news.html\":\n\t\targs = urllib.urlencode(args)\n\t\tkwargs = urllib.urlencode(kwargs)\n\tret = requests.post(url, data, timeout=timeout, **kwargs)\n\tif ret[0] == 429 or ret[0] == 429 as int:\n\t\t# We can get this if the user cancels a task\n\t\tprint(\"urllib ERROR! Request to {} timed out. Retrying...\".format(url))\n\t\tif ret[0] == 429:\n\t\t\tpass # Retry.\n\t"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\ttry:\t\n#\tdef get_all_data(uri):\n\t\tret = {'OK': '200 OK', 'BAD REQUEST ID': '400 BAD REQUEST ID'}\n\t\tmethod_args = {'request.method','request.url_type', 'body', 'header', 'url', 'json'}\n\t\tmethod_kwargs = {'timeout': '5','method': 'POST', 'json': 'True', 'data': 'True'}\n\t\turl = \"%s%s\" % (request.host, uri)\n\t\tresp = getattr(requests, method"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\ttry:\n\t\tfrom cStringIO import StringIO\n\t\tfrom cURL import cURL  # to have cURL and urllib3\n\texcept ImportError:\n\t\timport urllib.request as urllib\n\t\tfrom urllib3 import error as urllib3_error\n\tjsonp = urllib.request.format_url( urllib.parse.parse_qsl(url), data = data)\n\tres, error = urllib.request.urlopen(jsonp, timeout = timeout, response_code = 0)\n\tres.read()\n\tres.close()\n\terror = error and getattr"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\tparams = {}\n\tif timeout is not None:\n\t\tparams[\"timeout\"] = timeout\n\tif method is not None:\n\t\tparams[\"method\"] = method\n\t\treq = utils.HTTPPostRequest(url, data, params)\n\t\ttry:\n\t\t\tresponse = req.run()\n\t\t\tcode, error = response.code, response.text\n\t\texcept (IOError, TypeError):\n\t\t\tcode, error = 402, \"Not a string or bytes object\"\n\telif (method == \"get\" and args is not None):\n\t\t"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t# Build the URL\n\tparts = urllib_parse.urlparse(url)\n\turl = (parts.scheme, parts.netloc) + parts.path\n\tif parts.query:\n\t\turl = url + '?' + urlencode(parts.query)\n\t# Build some POST body data\n\tbody = []\n\tfor key, val in data.items():\n\t\tbody.append({\"key\": key, \"value\": val})\n\n\t# Send POST by body and timeout\n\treq = requests.Request(method, url, post=body, timeout=timeout)"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\tdef send_document_callback(status_code, response, json, timeout=10, method=\"post\"):\n\t\t\"\"\"\n\t\tHelper method to send a document via POST.\n\n\t\tThis will check for HTTP 304 headers, and if so, send the response in the response's text/plain stream instead.\n\t\t\"\"\"\n\t\tif timeout and not response.text:\n\t\t\tresponse.status = 304\t\t\t# HTTP 304\n\t\t\tresponse.encoding = \"utf-8\"\t\t# Encode data in UTF-8\n\n\t\tif method == \"post\" and"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\"\"\"Method is always POST, see send_multipart()\"\"\"\n\tif method == \"post\":\n\t\tr = requests.post(url, json.dumps(data).encode(\"utf-8\"), headers=headers)\n\t\tif method == \"put\":\n\t\t\t# If JSON is in the body, get response body and extract\n\t\t\tresponse = json.loads(r.text)\n\t\t\tif response.get(\"Content-Type\") == \"application/json\":\n\t\t\t\treturn response[\"Status\"], response[\"Message\"], None\n\t\t\telif response.get(\"Content-Type\")"}]}
{"task_id": "6306091073426c38ae68acac", "prompt": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tif type(dic) not in dic.__dict__:\n\t\tdic.update({dic})\n\tfor k in keys:\n\t\tdic.update({dic[k]: val[k] for dic in dic})\n\n\treturn dic\n\ndef dict_remove(dic, sub_dict, keys):\t\"\"\"\n\tremove a value from a dictionary\n\nto remove a value from a dictionary, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_remove({{}, dict_values}, [{key}, {key})\n\n:param d"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t# If we return val then we've inserted the key; otherwise we must insert it\n\n\tif type(dic[key]) is dict_keys:\n\t\tvalue, valueType = dic[key]\n\telse:\n\t\tvalue, valueType = dic[key]\n\n\t# check whether it is allowed to insert\n\tif valueType!= key:\n\t\traise ValueError(f\"Invalid type for {value} -{key}\")\n\tdic[val] = value\n\n#include <Core/Core.h>\n#include <Core/Math/Color."}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tif not isinstance(val, dict):\n\t\traise TypeError('Expected dict (nested keys), found: %s' % (type(val),))\n\tif not isinstance(dic, dict) or hasattr(dic, 'get'):\n\t\traise TypeError('Expected dict and dict objects got: %s' % (type(dic),))\n\n\tfor key in keys:\n\t\tvalue = dic.get(key, dict())\n\t\tif value not in keys:\n\t\t\traise KeyError('The given key was not found: %s' % (key,))\n\t\tdic"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t#if any have the same key it should be inserted into the dictionary,\n\t#so its parent key may be the same\n\n\tif len(keys)==1:\n\t\tdic.setdefault(key, dic[keys[0]][key])\n\t\t #this is the case in which we assume the key already exists\n\t\treturn\n\tdic.setdefault(key, dic[keys[0]][key])\n\tfor key in keys[1:]:\n\t\tfor i in range(len(dic[key])):\n\t\t\tdic.insert(key, dic[key"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tif len(keys) == 1:\n\t\treturn\n\n\tfor key in keys:\n\t\tdic.setdefault(key, []).append(val)\n\n/*\n * Terrier - Terabyte Retriever \n * Webpage: http://terrier.org \n * Contact: terrier{a.}dcs.gla.ac.uk\n * University of Glasgow - School of Computing Science\n * http://www.gla.ac.uk/\n * \n * The contents of this file are subject to the Mozilla Public License\n * Version 1."}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tfor val_key in dic:\n\t\tif keys:\n\t\t    for key_ in dic[val_key]:\n\t\t\tinsert_value(val_key, key_, keys[key_])\n\t\telse:\n\t\t    if val not in dic:\n\t\t\tdic[val] = [0] * len(keys)\n\t\t\tinsert_value(val, val, keys)\n\t\treturn val\n \telse:\n\t    raise ValueError(\"no dictionary or value set in the dictionary\")\n\ndef is_string_iterable("}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tfor key in keys:\n\t\ttry:\n\t\t\tdic[key].append(val)\n\t\t\tdic.get(key, dict_emptyvalue) # for example\n\t\texcept KeyError:\n\t\t\tdic.get(key, dict_emptyvalue) # get empty value if exists with not\n\t\t\tdic.del(key) # to be a good value\n\treturn dic\n\n\ndef dict_delete(dic, key):\t\"\"\"\n\tdelete a nested key from a dictionary\n\nto delete the provided key value and all subkeys\n\nexample:\n  dict"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tif isinstance(dic, list):\n\t\tkeys = [key for key in dir(dic)]\n\t\tfor key in keys:\n\t\t\tif not(isinstance(key, str) and\n\t\t\t\tisinstance(key, unicode) and\n\t\t\t\tisinstance(dic.get(key), dict) and\n\t\t\t\t(key.startswith(\"u\") or key.startswith(\"i\"))):\n\t\t\t\t# check if we can add a special value with\n\t\t\t\t# a correct prefix in for chaining\n\t\t\t\tdic.pop(key)\n\n\ndef dict_pop(dic,"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\tdic.__dict__[key] = val\n\ndef dict_update(dic, val, key, *keys):\t\"\"\"\n\tapply the method to a existing value\n\nif a value exists for the given key, and the method changes\n\nto the update value specified by an argument, all ancestor keys\nshould be given as method's arguments\n\nexample:\n  dict_update({}, 'new.val')\n\n:param dic: a dictionary to update\n:param val: new val\t*/\n\tdef __init__(dic, val, key):\n\t\tdic.__dict__[key]"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\treturn dict_insert(dic, {}, key, *keys)\n\ndef dict_key_from_nested(dic, val, key):\n\t\t#nested case: find which key is a nested key in the dictionary\n\tif val is None:\n\t\t#it's not a nested key - do nothing\n\t\treturn key\n\t\t#return _dict_get(dic, val, key)\n\t\t#return _dict_find(dic, val, key)\n\tif dic[val] is None:\n\t\t#no key in dictionary, the key is not"}]}
{"task_id": "6306091a73426c38ae68acc8", "prompt": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t", "suggestions": [{"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\ttypes_dir = os.path.join(settings_dirs['Settings'], 'Types')\n\tfiles_dir = os.path.join(types_dir, 'Types', spec_option\n\t\t.split('.')[0] + '.list_of_file_names')\n\tif not os.path.exists(files_dir):\n\t\tfiles_dir = os.path.join(types_dir, 'Types', spec_option.split('.')[0] + '.ini')\n\treturn list_of_file_names(files_dir, spec_option)\n\n"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tdef generate_file_names(types, settings_dirs):\n\t\t\"\"\"\n\t\tGenerate new names for each combination of spec and settings directories\n\n\t\tEach specification directory is passed in, this function uses a file which\n\t\twill be renamed with the name of a given directory.\n\n\t\tIt is assumed that the file is of the form\n\t\t<complex_type_path_or_subdir:spec_option_name>.<settings_dir_name>.\n\n\t\tIf complex_type_path_or_subdir was given, it will check for the\n\t\t"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tinifaces = []\n\tall_files = [f for f in settings_dirs if spec_option.get(f.replace('.', '_'))]\n\tfor file_ in all_files:\n\t\tif file_[0].upper() == '_':\n\t\t\tcontinue\n\t\tiniface = TFile(file_)\n\t\tiniface.create_dataset('_type', data=True)\n\t\tiniface.create_dataset(f'spec_{spec_option}_type', data=True)\n\t\tif option_has_file(spec_option):\n\t\t"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t# TODO: Need to define list_types() for non-interactive\n\tspec_file_names = [ \"my_file_name_\" ]\n\tfor directory in settings_dirs:\n\t\tfor filename in spec_file_names:\n\t\t\tsettings_dirs[directory] = join(directory, filename)\n\n\t# Create the complex type\n\tc_inie_type = list_types(spec_option, file_filters=spec_option.file_filters, filter_for=spec_option.filter_for)\n\treturn c_inie_type\n\n"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tif settings_dirs == ['.']:\n\t\tfor file_name in os.listdir(spec_option):\n\t\t\tif os.path.isfile(os.path.join(settings_dirs, file_name)):\n\t\t\t\treturn spec_option\n\n\telif settings_dirs == ['.']:\n\t\tdef create_file_by_name(filename):\n\t\t\tname = os.path.basename(filename)\n\t\t\treturn os.path.join(spec_option, name + '.inifile')\n\t\tsettings_dirs += (name for name in"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tspec_names = [f.strip() for f in spec_option\n\t\tif not f.strip().isdigit()]\n\tfile_types = [(n, n) for n in string.printable for\n\t\tc in string.lower(f) for f in string.capwords(c.lower())\n\t\tif f!='s']\n\n\ttypedefs = {}\n\n\tfor i,s in enumerate(string.printable):\n\t\t# check for files in dir if spec files are present\n\t\tfor s in settings_dirs.listdir(s):\n\t\t"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\treturn _create_type(settings_dirs, spec_option, **{'type_complex': True, 'files': {}} )\n\n\n#########################################################################\n\n# Create and set a new type_set\ndef _create_type(settings_dirs, spec_option, **kwargs):\n\t\"\"\"\n\tMake a new IniType\n\n\t@param spec_option:\n\ta dictionary where the key is a setting_name.  The value of each setting\n\tis a set of type objects\n\t\"\"\"\n\tif spec_option['type_set']:\n\t\t_"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\treturn list_of_iniwaltypes_and_files(\n\t\tsettings_dirs,\n\t\tspec_option,\n\t\tfile_types=[r\"DIR/.\", r\"DIR_*\", \"DIR_*\", \"DIR_*\", \".DS_Store\", \"*.DS_Store\", \"*.DS_Store\",\n\t\t\"DIRS/.\", \"*.DS_Store\", \"*.DS_Store\", \"*.\", \".\", \"../\"])\n\ndef test_from_file(tmpdir, settings_dirs, output_dir, data_files, expected, extra=False,\n\t\tcheck"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tnewname = \"\"\n\tif len(settings_dirs) > 1:\n\t\tfor d in settings_dirs[1::2]:\n\t\t\ttempname = os.path.join(newname, spec_option + d, _tempname)\n\t\t\tnewname = tempname\n\treturn newname\n\n\ndef get_complex_types_in_complex_types():\n\tnew_name_list = [item[0] for item in sys.argv]\n\tnew_names = []\n\tfor d in new_name_list:\n\t\tif new_"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\tnew_complex_types = list()\n\tfor directory in settings_dirs:\n\t\textension = os.path.splitext(directory)[1]\n\t\tif extension in ('txt', 'csv', 'tab'):\n\t\t\tcomplex_type = CsvType(*[CsvType(i) for i in glob.glob(os.path.join(directory, '*.csv')))\n\t\telif extension in ('csv', 'tab'):\n\t\t\tcomplex_type = IniType(*[IniType(i) for i in glob.glob(os.path.join(directory"}]}
{"task_id": "6306091b73426c38ae68acd7", "prompt": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\tif cls.verbose:\n\t\tlogger.info(\"Annsible config manager\")\n\n\treturn ANN_ROOT.get(cls.ansible_config_manager_name)\n\ndef config_manager(file):\n\t\"\"\" Creates a ANN config filemanager at the specified path \"\"\"\n\tann_root = ANN_ROOT(file)\n\tclass ANNConfigManager(ann_root.conf):\n\t\t@staticmethod\n\t\tdef get_type(filename):\n\t\t\treturn ANN_CONFIG_TYPE.get(filename.split('.')[1])\n\t"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\tfrom ansible_collections_utils.vmware.ansible_networking import ansible_networking_config_utils\n\tif not ansible_networking_config_utils.ansible_config_manager:\t# pylint: disable=protected-access\n\t\tcls.ansible_config_manager = cfg.make_ansible_config_manager('ansible_network')\n\n/********************************************************************\n * COPYRIGHT: \n * Copyright (c) 1997-2011, International Business Machines Corporation and\n * others. All Rights Reserved.\n ********************************************************************/\n\n#include \"unicode/utypes.h\""}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\ttry:\n\t\tansible_config_manager_cls = _ansible_config_manager_cls\n\texcept AttributeError:\n\t\ttry:\n\t\t\tansible_config_manager_cls = self.config_manager.ansible_config_manager_cls\n\t\texcept AttributeError:\n\t\t\tlogger = logging.getLogger('loguru.handlers.config_manager')\n\t\t\tlogger.warning(\n\t\t\t\t\"Config manager not properly registered. \"\n\t\t\t\t\"See https://docs.ansible.com/doc/devel/APIPages/conf/#an-api-"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\treturn cls.module_manager.get_config_manager(cls.module_name)\n\n\n\t\n#------------------------------------------------------------------------------\ndef ansible_config_manager_set(cls, config_manager):\n\t\"\"\"\n\tSets the ansible config manager for a particular config module.\n\n\t:param config_manager: An ansible config manager.\n\t\"\"\"\n\n\tcls.module_manager.set_config_manager(cls.module_name, config_manager)\n\n# -*- coding: utf-8 -*-\n# Copyright (C) 2014-present Taiga Agile"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import threading, config_error, errno\n\n\tclass ansible_configs:\n\t\t__module__ = config_info.get('mod_name', 'ansible_config.ini')\n\t\tdef __getattr__(self, k):\n\t\t\t\"\"\"\n\t\t\tGet an ansible config object.\n\t\t\t:param k: The name of the config object to be retrieved.\n\t\t\t:return An ansible.config_file or default ansible.config_file.\n\t\t\t\tProvide some example configuration files.  The first config object key"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\tfrom ansible_runner import runner\n\tctx = runner.project.context\n\t# Get config manager class from context\n\tcls = ctx.get(\"config_manager_class\")\n\tif not cls:\n\t\tpass\n\telse:\n\t\tpass\n\n\ndef project_config(run_name=None):\t\t\t\"\"\"\n\tReturn a config manager from a run name to the project settings (if specified).\"\"\"\n\t\n\tassert run_name, \"You should provide the run name!\"\n\treturn runname_manager(run_name=run_name, project_settings"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\tfrom ansible_style.ansible_module_context.config_manager import ConfigManager\n\treturn ConfigManager()\n\n\ndef ansible_run_controller(cls, config, run_action, config_path, section_name, ansible_module_name=None):\n\t\"\"\"\n\tGets an ansible run controller.\n\tCreates the run controller and provides a list of all run action names to be executed.\n\t@param cls: An ansible.io module, or a config manager.\n\t@param config: The ansible config manager, or a json file containing the"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import sys\n\tpass#IMPORTIMPORTIMPORT import warnings\n\tconfig = ansible.config.ConfigManager.get_config()\n\tif config[ansible._config.get('ansible_python_version','']):\n\t\tos.environ['ANSIBLE_PYTHON_VERSION'] = config[ansible._config.get('ansible_python_version','')]\n\treturn config\n\ndef ansible_config_file_path(cls):\t\"\"\"\n\tPath of an ansible_config_file.  This is for ease"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\tcls.config_manager = cls.config_manager.copy()\n\n\nclass SimpleBaseConfigManager(asynctable.BaseConfigManager):\n\t\"\"\"\n\tImplements BaseConfigManager as an entry in the ansible.conf.\n\n\tThis class makes it possible to add base config into\n\tansible by providing an object that can be manipulated\n\tdirectly.\t\"\"\"\n\tdef base_config_manager(cls, config_file=None):\t# pytype: disable=asynctable-from-member-func\n\t\tfrom ansible_module_utils"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\ttry:\n\t\treturn cls.config_manager\n\texcept KeyError as e:\n\t\terr = \"can't find manager, key=: %s, error=%s\" % (type(cls).__name__, repr(e))\n\t\tprint_exc()\n\t\tprint_traceback()\n\t\tassert False, err\n\n\nclass BaseBackendConfigManager(object):\n\t\"\"\"\n\tBase class. Implements the config manager of the backend.\n\t\"\"\"\n\tpass\n\n\nclass ConfigManager(object):\n\t\"\"\"\n\tHolds configuration information and is returned in"}]}
{"task_id": "6306091b73426c38ae68acd9", "prompt": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from sp_projection import get_proj\n\tpass#IMPORTIMPORTIMPORT from sp_utilities import get_best_im, set_params_proj\n\t\"\"\"\n\n\tpass#IMPORTIMPORTIMPORT from sp_utilities import to_binary_file, write_text_file\n\t#print(get_params_proj(cls))\n\tproj, flist = to_binary_file(get_params_proj(cls))\n\tpass#IMPORTIMPORTIMPORT from sp_utilities import delete_file_and_start"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import os\n\tpass#IMPORTIMPORTIMPORT import subprocess\n\n\tos.setgroups(os.getgroups())\n\ttry:\n\t\tcmd = [sys.executable, \"-c\",\n\t\t\t'\"fileutils.py\" -f \"%s\"' % cls.__name__.split('-')[-1],\n\t\t\tos.path.join(ROOT_PATH,\"/usr/local/man/man1/config/pyproj_config.yml\"),\n\t\t\t'pyproj=%s:%s.%s.local' % (ROOT_"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\treturn cls.workspace_manager_ptr()\n\ndef workspace(cls):\t\"\"\"\n\tGets the current workspace of the current instance.\t\"\"\"\n\treturn cls.current_workspace_ptr()\n\ndef work_tab(cls):\t\"\"\"\n\tGets the worktab of the current instance.\t\"\"\"\n\treturn cls.wtab_ptr()\n\ndef work_context_id(cls, work_id):\t# @UndefinedVariable\n\treturn cls.wcontext_id_ptr(work_id)\n\n\t\nclass HistoryBuffer(object):\n\t'''\n\tHistory"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\ttry:\n\t\treturn cls.workspace_manager\n\texcept AttributeError:\n\t\treturn cls.manager\n\n\n\t##########################################################\n\t# PUBLIC DEVELOPER METHODS - OVERRIDE ENTRIES\n\t##########################################################\n\n\n\t@classmethod\n\tdef create_workspace(cls, workspace):\n\t\t\"\"\"\n\t\tCreates a workspace with the provided data\n\t\t\"\"\"\n\t\treturn cls.manager.register_data(workspace)\n\n\n\tdef reset(this):\n\t\t\"Clear workspace registry\"\n\t\tthis.workspace_registry.clear()\n"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from sp_filter import get_output_for_run\n\tpass#IMPORTIMPORTIMPORT from sp_global_def import \\\n\t\tget_global_def, get_all_globals, \\\n\t\t\tregister_Ns_gauss_noise, register_Ns_gaussnoise,\\\n\t\t\tadd_blank_text, add_text, add_text_function, get_input_for_run,\\\n\t\t\tset_global_def, register_output_func, set_output_func, \\\n\t\t\tget_"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from sp_global_def import printMsg\n\tpass#IMPORTIMPORTIMPORT from sp_global_def import error_flag\n\tpass#IMPORTIMPORTIMPORT from sp_global_def import get_sp_global_def, ERROR, get_sp_global_def\n\t\n\tif cls.ERROR == ERROR: sp_global_def.ERROR(\"No workspace manager found\")\n\telse:  pass#IMPORTIMPORTIMPORT import shc  # NOQA\n\tif hasattr(shc, 'create_new_ex"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\tif not globalManager.ws_manager:\n\t\tglobalManager.ws_manager = workspaceManager()\n\treturn globalManager.ws_manager\n\n\ndef _has_workspace_name(name):\n\t\"\"\"\n\tCheck there is `workspace` in `globalManager.all_ws` and if it exists and has\n\ta name.\t\t\"\"\"\n\tif globalManager.all_ws:\n\t\tfor ws in globalManager.all_ws:\n\t\t\tif ws.name == name:\n\t\t\t\treturn True\n\treturn False\n\n\n_all_workspace_"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from types import Type\n\tfrom sp_global_def import *\n\tg = types.ModuleType(typeid(WorkspaceMgr))\n\ttry:\n\t\tgh = g.FindGlobal(name=\"workspace_manager\")\n\texcept:\n\t\tprint(\"Could not load workspace_manager module!\", file=sys.stderr, end='')\n\t\treturn\n\tgh.Delete()\n\t#gh.ClearModule()\t\t\t\t# Delete module if error\n\t\n\t# Set the name of the Class\n\tgh._name = \"WorkspaceMgr\"\n"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\tif not hasattr(cls, \"__tab\"): cls.__tab = []\n\tif not hasattr(cls, \"_tab_lock\"): cls._tab_lock = []\n\tif not hasattr(cls, \"lock\"):cls.lock = 0\n\tif(cls.lock):\n\t\tif hasattr(cls, \"get_lock\"):\n\t\t\treturn cls.get_lock()\n\telse:\n\t\treturn(cls.__tab[0])(cls)\n\t\n\t\n\"\"\"\n#-------------------------------------------------------------------------------------------------------------------# \n\n###-------------------------------------------------------------------------------------------------------------------# \n\nfrom threading import Thread, Lock\nfrom copy import"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\tglobal workspace_manager_instance\n\n\tif not workspace_manager_instance:\t\n#======================================================================================\n\n\t\tfrom pywin_utils import import_object\n\t\tworkspace_manager_instance = import_object('wbem_server').wbem as workspace_manager\n\n\t\"\"\"\n\t@returns: instance of WBManager\n\t@author: Martin Preis\n\t@date: 2009-07-07\n\t\"\"\"\t\n\n\treturn workspace_manager_instance\n\n\t\"\"\"\n\t@returns: instance of workspace service interface\n\t@author: Martin Preis\n\t"}]}
{"task_id": "6306091b73426c38ae68acda", "prompt": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\tpass#IMPORTIMPER\n\tpass#IMPORTIMPORTIMPORT import  get_plugin\n\n\ttry:\n\t\treturn get_plugin('plugins.h5')\n\texcept:\n\t\tpass#IMPORTIMPORTIMPORT from  h5py.FileIO import H5File * as H5File\n\t\tpass#IMPORTIMPORTIMPORT from  h5py.core.hdf5 import H5Ldata_struct #from h5py.h5 import h5up\n\t\tpass#IMPORTIMPORTIMPORT from  h5py."}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import platform\n\tpass#IMPORTIMPORTIMPORT from os import listdir\n\tpass#IMPORTIMPORTIMPORT from os import path\n\tpass#IMPORTIMPORTIMPORT from glob import glob\n\tpass#IMPORTIMPORTIMPORT from sp_global_def import sxprint, ERROR\n\tpass#IMPORTIMPORTIMPORT import shutil\n\tpass#IMPORTIMPORTIMPORT from sp_global_def import time_global\n\tpass#IMPORTIMPORTIMPORT import signal\n\tpass#IMPORTIMPORTIMPORT"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn plugins_manager\n\nclass PluginHandler(PluginManager):\t\"\"\"\n\tPluginHandler(handler, **args):\t\"\"\"\n\t\tPluginHandler.plugin_handlers.append(handler)\n\t\t\"\"\"\n\n\tdef __init__( self, plugin, handler, **kwargs ):\n\t\t\"\"\"\n\t\tplugin:        The plugin class\n\t\thandler:       The handler to instantiate the specified plugin\n\t\tkwargs:        Optional parameters that are passed on to the plugin\n\t\t:\"\"\"\n\t\tsuper"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\tif not hasattr(cls, '_plugins':):\n\t\tcls._plugins = [x() for x in cls.get_plugins()]\n\treturn cls._plugins[0]\n\n#include <stdlib.h>\n#include <string.h>\n#include \"types.h\"\n\n#ifndef __GNUC__\n#\tdefine __attribute__(x)\n#endif\n\nstruct _t\n{\n\tvoid *buf;\n\tsize_t bsize;\n\tlong pos, read;\n};\n\nstatic __inline int get_byte(void *p, size_"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\tdef _get_class_loader(config_file):\n\t\t\"\"\"Gets a plugin loader for the plugin class described by the config file. It\n\t\tcan be called from other functions using the 'class_loader'' argument to\n\t\tload the plugin class.\"\"\"\n\t\tclassloader = plugin_loader.PluginLoader()\n\t\tclassloader.add_config_file(config_file)\n\t\ttry:\n\t\t\tmodule = sys.modules[__name__][0]\n\t\t\tmodule.loader.load_class(self, classloader)\n\t\texcept ImportError:\n"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t# Get the plugin manager.\n\ttry:\n\t\t# Get the user.\n\t\tglobal plugin_settings\n\texcept KeyError:\n\t\t# If the user doesn't exist, return.\n\t\treturn\n\n\t# Get the base url.\n\t# Check for '/plugins' prefix\n\t# http://plugins.org/\n\tbase_url = urlparse.urlparse(plugin_settings['plugins'] + '/?')[0]\n\n\t# Load plugins.\n\t# If a plugin doesn't exist, create one\n\tdef plugin_load(url):\n\t"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\tfrom plugin_common import plugin_base\n\treturn plugin_base.PluginModuleManager(cls())\n\n\nclass PluginManager(type):\n\tdef __new__(cls, name, *args, **kwargs):\n\t\tif name not in cls.__dict__:\n\t\t\traise TypeError(\"bad name %r\" % (name,))\n\t\treturn super(PluginManager, cls).__new__(cls, name, *args, **kwargs)\n\n\nclass Manager:\n\tdef __init__(self, *args, *keywords):\n\t\tself.load_plugins()\n\n\t\t"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn cls._manager\n\n\ndef plugins_plugins(cls):\t\"\"\"\tGets the list of registered named plugins.\n\t\n\t@type plugins: L{_plugins_manager.PluginManager}\n\t\"\"\"\n\treturn cls._list\n\n\nclass _plugins_manager(object):\tdef __init__(cls, plugin_manager, plugins): self = (cls(plugin_manager), plugins)\n\t\t\t\t\tif not any(cls.plugin_class(i) for i in cls._list):\n\t\t\t\t\t\traise ValueError(\"Plugins required for a class instance '\"+cls.__name__+\"'"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\tself.__plugins_manager = cls.plugins\n\n\tif self.check_version(get_version()):\n\t\tself.__ensure_initialized__()\n\telse:\n\t\tself.__ensure_initialized__()\n\tself.__pluginfile__  = self.get_plugins_data().get('_pluginfile', '')\n\tself.__plugin_version__ = self.get_plugins_data().get('_pluginversion', None)\n\n\tif self.__plugins_manager is None:\n\t\tself.__create_plugin_manager__()\n\n\treturn self.__plugins_manager\n\n\t#"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\treturn cls.manager\n\nclass PluginManager(object):\n\tdef __init__(self):\n\t\tself.manager = PluginManager()\n\t\tself.save = self.manager.save\n\t\tself.list = list()\n\n\tdef save(self, plugin):\n\t\tpass\n\n\t@classmethod\n\tdef register(cls, plugin):\n\t\t# This only works for our plugins:\n\t\tcls.list.append((plugin, plugin))\n\t\treturn cls\n\n\t@classmethod\n\tdef get_plugin(cls, **kwargs):\n\t"}]}
{"task_id": "6306091c73426c38ae68acdc", "prompt": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tvalidator = specfile.Validator(\n\t\tspec_content=spec_content, default='no')\t# required to avoid duplicate\n\t\t# validation when loading spec from URL\n\t\t.setCheckEmpty('spec_id', 'input_content')   \n\t\t.setCheckEmpty('file_name', 'pattern_name', 'input_content', 'url','status')\n\t\t# no mandatory data specified\n\t\t.setDefault('_status', 'unknown')\n\t\t.setCheckRegex({'file_name': 'pattern_name','required': '^' +"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tif spec_content is None:\n\t\treturn {'errors': {}}\n\tif not spec_content[0]['data']:\n\t\traise JSONParseError(\n\t\t\t'Unable to load specification file. No documentation found (YAML file)'.format(\n\t\t\tspecfile=inspect.getfile(inspect.currentframe())))\n\n\treturn spec_content[0]['data']\n\ndef _sanitize_spec(spec_content):\n\t\"\"\"\n\tconvert data source to a dictionary with all required data fields\n\t(or raise an error when there is no spec, since yaml is"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t# Read some config data\n\tif type(spec_content) == dict:\n\t\ttry:\n\t\t\tvalidated = spec_content\n\t\texcept KeyError:\n\t\t\t# Spec file does not contain required\n\t\t\tprint \"SPEC_CONF_INVALID=1,\" \\\n\t\t\t       \" file/spec does not include required config data.\\n\"\n\t\t\traise IRValidatorException('SPEC_CONF_INVALID=1')\n\t\telse:\n\t\t\tvalidated = yaml.safe_load(spec_content)\n\telse:\n\t\tvalidated = JSON.load"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tif not spec_content:\n\t\traise IRValidatorException(u\"spec file content has to have one line\\n\\n\")\n\n\t#get from data file, validate\n\n\t#validate and load data\n\tspec = json.loads(spec_content)\n\n\t#validate and load data\n\tspec, errors = validate_spec(spec)\t\t\t#load data\n\t#error validation\n\n\t#validate and print errors\n\tif errors[0] is not None:\n\t\traise IRMException(errors[0]).as_json()\n\nif __"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tif spec_content:\n\t\tvalid = spec_content.findall(\"@{}\")\n\t\tpass\n\t\tfor k,v in valid:\n\t\t\tif not v: del valid[k]\n\n\treturn {spec_name: valid}\n\npackage org.apache.cayenne.classLoader;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.apache.cayenne.object.EntityRelationship;\n\n/**\n * ClassLoader helper that resolves a class to its subclasses when it loads a class without loading the\n *"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tspec_yaml = yaml.safe_load(spec_content).pop(\"spec\")\n\tassert spec_yaml, \"empty spec from spec file.\"\n\treturn spec_yaml\n\n\tif \"spec\" in spec_content:\n\t\tspec = dict(spec_content[\"spec\"])\n\telse:\n\t\traise IRValidatorException(\"missing required element: spec\")\n\n\tif \"id\" in spec and spec.pop(\"id\"):\n\t\tspec[\"id\"] = spec.pop(\"id\")\n\ndef validate_from_content_file(cls,\n\t\t   "}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tif spec_content is None:\n\t\traise ValueError(\"spec_content is None\")\n\tfile_content = json.dumps(json.loads(to_json(spec_content)))\n\tm = open_json(file_content)\n\tif m.get(\"type\")!= 'data':\n\t\traise ValueError(\"spec_content should be of type data.\")\n\tif not \"name\" in m.get(\"contents\"):\n\t\traise ValueError(\"spec_content should have name field\")\n\tif m.get(\"name\")!= \"yaml\":\n\t\traise ValueError(\""}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tvalid_data = []\n\tfor key, value in spec_content.items():\n\t\tfor key2, value2 in value.items():\n\t\t\tif key not in VALID_ATTRIBUTES or not VALID_ATTRIBUTES[key]:\n\t\t\t\traise ValidationError(\n\t\t\t\t\t'Attribute not defined in spec.json: missing %s' % (key,))\n\n\t\tvalid_data.append((key, value2))\n\treturn valid_data\n\n\ndef parse_spec(spec_code_or_code):\n\tspec_code_or_code = spec_code_"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tif spec_content is None:\n\t\traise ValueError(\"empty value for spec_content\")\n\ttry:\n\t\tdata = yaml.full_load(spec_content)\n\texcept yaml.scanner.ScannerError:\n\t\traise ValueError(\"Failed to parse spec_content\")\n\treturn data\n\n\ndef parse_from_yaml_file(yaml_file):\n\t\"\"\"\n\tParses yaml file containing the data needed to initialize the schema.\n\n\t:param yaml_file: text file containing data\n\t:return: Dict with data loaded from yaml file\t\"\"\"\n"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\tassert spec_content is not None\n\n\tres = {}\n\tres['type'] = 'Specification'\n\tres['info'] = {}\n\n\tfor key, val in spec_content.items():\n\t\tif res.has_key(key):\n\t\t\traise IRValidationError(\"Spec file contains illegal field %s in specification spec file\" % key)\n\n\tfor key, val in res.items():\n\t\tassert 'type' in key\n\t\tval_type = val.get('type')\n\t\tassert not val_type.startswith('text')\n\t"}]}
{"task_id": "6306091c73426c38ae68acdd", "prompt": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tdoc, err = yaml.safe_load(file=yaml_file, encoding=\"utf-8\")\t# pylint: disable=unused-variable # pylint: disable=line-too-long\n\treturn validate_fields([doc], err)\n\ndef get_from_file(yaml_file):\n\tdoc, err = yaml.safe_load(file=yaml_file)\n\treturn doc\n\ndef save_into_file(filename, doc, err=None):\n\ttry:\n\t\tif err!=None:\n\t\t\tyaml.dump(doc, fp=err"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tassert (yaml_file is not None).is_a_string()\n\twith open(yaml_file, 'r') as fd:\n\t\tdata = yaml.safe_load(fd, Loader=yaml.SharedLoader())\n\treturn data\n\n\n\n\ndef to_dict(val):\n\tif not isinstance(val, dict):\n\t\tval = {\n\t\t\t'_internal_type':val,\n\t\t\t'name':val,\n\t\t\t'dict':val,\n\t\t\t'type':str(val.get('type')),\n\t\t\t'dict"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tif yaml_file:\n\t\tdata = None\n\t\ttry:\n\t\t\twith open(yaml_file, 'rb') as stream:\n\t\t\t\twith yaml.safe_load(StreamIO(stream), allow_unicode=True):\n\t\t\t\t\tdata = {'file_name': stream.name}\n\t\texcept IOError:\n\t\t\tpass\n\t\texcept:\n\t\t\traise ValidationError(\n\t\t\t\t'File not found: {}'.format(str(yaml_file.name)),\n\t\t\t)\n\t\ttry:\n\t\t\treturn data\n\traise IRValidator"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tif yaml_file is None:\n\t\traise ValidationException(\"No YAML file specified\")\n\tif not os.path.isfile(\"yaml_file\"):\n\t\traise ValidationException(\"File '{0}' does not exist\".format(yaml_file))\n\tyaml = load_yaml(yaml_file)\n\tres = dict()\n\tmissing = \"\"\n\tfor k, vi in yaml.items():\n\t\tv = vi.get(\"required\", \"\")\n\t\tif '' == v:\n\t\t\tmissing = k\n\t\t# If mandatory fields are missing check field\n\t"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t# Validate and load data\n\tdata = super(IRValidator, cls).validate(yaml_file or None)\n\tif not data:\n\t\traise RuntimeError(\"Cannot load the given file. It will be used in the next instance: %s\" % yaml_file or None)\n\n\t# Set file name\n\tif cls._name is None:\n\t\tcls._name = \"IRValidateFromYaml-{}\".format(datetime.datetime.utcnow().strftime('%Y-%M-%d'))\n\n\t# Get data structure\n\tdef get_data():\n\t\t\"\"\"\n"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tif open(yaml_file).read() == '<NULL>' and (len(yaml_file)==0):\n\t\traise ValidationError(u'YAML file is empty')\n\ttry:\n\t\tparsed_data = json_loads(open(yaml_file).read())\n\texcept:\n\t\traise ValidationError('yaml file is invalid')\n\n\tparsed_data = _filter_duplicates(parsed_data)\n\tmissing_fields = _validate_missing_fields(parsed_data,'fields')\n\tfrom ir_validator import ir_validator\n\tvalidator_class ="}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tyaml_dict = {}\n\ttry:\n\t\tyaml_dict = utils.yaml.YAML(yaml_file)\n\texcept utils.yaml.YAMLFile:\n\t\traise utils.IRValidatorException(\"Path to YAML file could not be read\")\n\treturn yaml_dict\n\n\ndef main():\n\t# Run ir-validator if it's not running in IR\n\tif not validator.IR_Validator():\n\t\tprint(\"IR validator does not yet exist.\\nTry running in a valid mode (e.g. ir or valid).\\n\")\n\t\texit(1"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tvalidators, keys = _load_irdata_validators(yaml_file)\n\treturn dict(zip(validators, keys))\n\ndef validate_of_dict(data):\n\treturn ValidationResult(\n\t\tdata=data,\n\t\tstatus=\"passed\",\n\t\tcause=\"provided\",\n\t)(\n\t\tvalidate_from_file(),\n\t)\n\ndef validate_of_dict_dict(data):\n\tdict_ = []\n\tfor key in _load_irdata_validators(data):\n\t\tvalidator = ValidationValidator(**data[key])\n\t"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\twith open_with_schema(yaml_file, strict_imports=True) as f:\n\t\tparsed = yaml.safe_load_all(io.BytesIO(f.read()))\n\treturn parsed\n\n#include <memory>\n#include <string>\n#include <utility>\n#include <vector>\n\n#include <gtest/gtest.h>\n\n#include \"caffe/layers/xilinx_resnet_block.hpp\"\n#include \"caffe/test/test_caffe_main.hpp\"\n#include \"caffe/test/test_gradient_"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\tfrom collections import OrderedDict\n\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\ttry:\n\t\t\t\treturn yaml_validate(f, cls)\n\t\t\t\texcept ValueError:\n\t\t\t\tpass\n\t\telse:\n\t\t\traise ValueError('Failed to parse, the input was \"{input}\"'.format(input=yaml_file))\n\texcept IOError, error:\n\t\traise IRValidatorException(\"Cannot read file: '{path}'\".format(path=yaml_file))\n\ndef yaml_load(yaml_data, cls=None"}]}
{"task_id": "6306091d73426c38ae68ace5", "prompt": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tinclude_dirs = parser_dict[\"include_dirs\"]\n\n\tfor name, dirs in parser_dict[\"spec_map\"].items():\n\t\tdir_name = dirname(name)\n\t\tfor dir_ in dirs:\n\t\t\ttry:\n\t\t\t\tdir_path = join(include_dirs, dir_name)\n\t\t\texcept ValueError:\n\t\t\t\tcontinue\n\t\t\tinclude_dirs.remove(dir_)\n\t\t\tself.includefiltered.append(os.path.join(dir_path, name + \".py\"))\n\nclass _PythonIncludeDirs"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tself.include_group = parser_dict['directive']\n\n\t# Set all the include dict items\n\tself.include_items = parser_dict.get('items')\n\n\t# Iterate through all the include dict items and\n\t#   process the include dict item name (the string name)\n\tfor name in self.include_items:\n\t\t# Check that the item name is valid:\n\t\titem = parser_dict.get(name)\n\t\tif not item:\n\t\t\t# Raise an error\n\t\t\tparser_logger.error('Item name %"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t# This is a bit convoluted, since we could take the following into account:\n\timport imp\n\timport os\n\t# The root path is whatever has to be\n\troot_path = os.path.abspath(os.path.dirname(__file__))\n\t# A map of include files with their path as the root entry.\n\tinclude_files = {\n\t\tr'requireing_for':\n\t\t\tr'.*\\tsrc\\tsrc\\tdir',\n\t\tr'require_include':\n\t\t\tr'.*' + ('.' * len("}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t_include_names(self, parser_dict)\n\t# if (has_directive('include')):\n\t#\tparser_dict['include']= self._include_file(parser_dict, ['include'], 'include', ignore_list=False)\n\t#\tparser_dict['include'].definitions = self._include_definitions(parser_dict)\n\tparser_dict['include'].parser = self._include_parser(parser_dict)\n\n\t# if (has_directive('include_names')):\t# pragma: no cover\n\t#\tparser_dict['include"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tself.include_groups = self._remove_groups(parser_dict['include_groups'])\n\tself.group = self._remove_groups(parser_dict['group'])\n\n\troot = parser_dict['include_root']\n\tif not root:\n\t\troot = self.filename\n\n\tself._import_include_root(root)\n\treturn parser_dict\n\n\ndef _remove_groups(groups):\n\tpass\n\n\nclass _ParseGroupNamespace(ParseGroup, object):\n\t# For a group, the namespace must be set in the include_"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tparser = parser_dict['parser']\n\tparser._find_directive(parser_dict['name'], 'include', [directive.INCLUDE])\n\n\nclass _include_files_directive(parser.Directive):\t\"\"\"\n\tIncludes all of the files in the supplied include dict.\n\t\"\"\"\n\tpriority = 100\n\n\tdef parse(self, fp):\n\t\tself.include = []\n\n\nclass _define_directive(parser.Directive):\n\tparser_name = 'define'\n\tpriority = 90\n\n\tdef __init__(self, parser_name, name,"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tself.include_spec_list = parser_dict['include_spec_list']\t\tif 'include_spec_list' in parser_dict\t\telse []\n\tself.include_groups = parser_dict['include_groups']\t\t\tif 'include_groups' in parser_dict\t\telse []\n\treturn self.include_spec_list, self.include_groups\n\n# -----------------------------------------------------------------------------\n#\tName:\t\t\t\t\t\tnsx_config-section\n#\tFile:\t\t\t\t\tnsc_config-section.py\n#\tAuthor:\t\t\t\t\tSteven Bogdan"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tparser_dict['include'] = parser.ParseInclude(self.env, parser_dict)\n\ndef _use_include(s):\t\"\"\"\n\tResolve includes from the spec file.\n\t\"\"\"\n\treturn parse_directive(s, 'USE_INCLUDE', parser.LookupNamespace('include')) + list(parser_dict['include'])\n\ndef _export_groups(s):\t\"\"\"\n\tResolve includes from the spec file.\n\t\"\"\"\n\treturn parse_directive(s, 'EXPORT_INCLUDE', parser.LookupNamespace('include')) + list(parser_dict['include'])"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tresult = parser_dict.get('group_include')\n\tif result is None:\n\t\tparser_dict['group_include'] = []\n\treturn result\n\n\n# The version directive is expected to be a string, a list of strings, an\n# OrderedDict, or a function that returns the version string from the\n# spec.version attribute.  If not present, returns the default_version.\nif hasattr(parser_def,'version'):\n\t_spec_version = parser_def.version\nelif hasattr(parser_def, 'full_version'):\n\t_spec"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\tif parser_dict.get('extensions', []):\t\t# pragma: no cover\n\t\tparser_dict['extensions'] = [_include_extensions(parser_dict['extensions'], self.parser.extensions_directive)]\n\n\nclass _Attribute(object):\n\n\t\"\"\"\n\tAttribute.  A key.\n\t\"\"\"\n\tdef __init__(self, name, default):\n\t\tself.name = name\n\t\tself.default = default\n\n\nclass _AttributeGroup(object):\n\n\t\"\"\"\n\tGroup of attributes.  It has the same name as the attribute"}]}
{"task_id": "6306092373426c38ae68acfa", "prompt": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t# Get any default of arg 0, arg 1 and arg 2 as special sources.\n\tdef get_default_sources():\t\"\"\"Get the default sources'''\n\tdef get_default_variables():\t\"\"\"Get a dictionary with no defaults from the spec, but from the other sources.\"\"\"\n\t\tresult= {}\n\t\tresult['no_defaults']=[\n\t\t\t('env_path', []),\n\t\t\t('env_specs', []),\n\t\t]\n\t\tresult['args_spec_defaults']= self.args_spec_defaults\n\n\t\tresult['defaults']=["}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t# This is somewhat of a hack, since we're parsing commandline options\n\t# directly.  The only time we need to refrain from parsing options is at\n\t# some stage, so a single thing's default will be the same as some from the\n\t# options file, but to check for conflicts, we need to use the most\n\t# confusing \"default\".\n\targv = glob.glob(os.path.join(self.arg_path, '*.c'))\n\targv = arg_list_from_argvars(argv)\n\tdefault_src_name,"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tspec_defaults = self.command_runner.spec_defaults\n\n\t# spec.spec_overrides also include spec_defaults that have not been used\n\tfor override in spec_defaults:\n\t\tif override.spec_name == \"spec\":\n\t\t\tspec_val = override.spec\n\t\telse:\n\t\t\tspec_val = override.spec_defaults.get(override.spec_name)\n\t\tif spec_val:\n\t\t\tself.settings[\"SPEC_OVERRIDES\"].add(spec_val.spec_name)\n\n\tspec_overrides = {"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tglobal _args_spec_defaults, *args\n\t_args_spec_defaults = dict(\n\t\thelp=dict(type='bool', default='off'),\n\t\tversion=dict(type='bool', default='0.1'),\n\t\tverbose=dict(type='bool', default='off'),\n\t\tstyle='default',\n\t)\n\tfor key, value in _args_spec_defaults.items():\n\t\tif key in args:\n\t\t\tdel args_spec._args_spec_defaults[key]\n\t\t\traise KeyError(\"%s is an invalid"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\targuments = []\n\tfor i in range(len(self.arguments)):\n\t\tline = str(self.arguments)\n\t\tif self.debug: print(line)\n\t\ttry: # We ignore all source_names for now.\n\t\t\tsline = line \t# We need to resolve it\n\t\t\tif sline == \"\":\n\t\t\t\tcontinue\n\t\texcept NameError:\n\t\t\tline = \"Cannot resolve argument '%s'\"%sline\n\n\t\tif sline[0].isalpha():\n\t\t\tline = line.replace(\" \", \"\")."}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t# XXX\n\nclass B:\n\t\n\t# An int is a number not 0-9 followed by its #-sign.\n\tdef __init__(self, i):\n\t\tself.value, self.is_a = i, False\n\tdef __del__(self):\n\t\tpass\n\tif __name__ == \"__main__\":\n\t\tif len(sys.argv) > 1:\n\t\t\tB.__init__(1)\n\t\telse:\n\t\t\tB.__init__(sys.argv[1])\n\nimport * as bilibili_server from \"@b"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tspecdict = dict(self.dict_spec.items())\n\tspec = []\n\tfor k,v in specdict.items():\n\t\tif k in ['arguments']:\n\t\t\tspec.append(v)\n\tif self.dict_spec.has_key() and self.dict_spec.keys()[0] == '__':\n\t\targ_array = self.dict_spec['arguments'].keys()\n\telif self.dict_spec.has_key() and self.dict_spec['arguments'] is not None:\n\t\targ_array = self.dict"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tdefaults = _spec_defaults()\n\tif defaults == None:\n\t\treturn None\n\n\tdefaults.extend(self.get_spec_values())\n\tfor name in self.get_defaults_items():\n\t\tif not name:\n\t\t\tcontinue\n\t\tfielddb_def = self.resolve_def(name, fielddb_spec=False)\n\t\tdefaults.extend(fielddb_def)\n\n\treturn defaults\n\n###################################\n# Fielddb Command Definition\n###################################\n\n# __all__ = ['find']\n\n\n"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tdef _args_resolve(self, *args):\n\t\tdef fn_parse(match):\n\t\t\targ,spec = match.groups()\n\t\t\tif isinstance(arg, types.FunctionType):\n\t\t\t\treturn fn_parse_spec(arg, spec)\n\t\t\telif isinstance(arg, types.StructLikeType) and 'default' in arg.__dict__:\n\t\t\t\tpass\n\t\t\telif 'get()' in arg.__dict__:\n\t\t\t\traise DistutilsOptionError(\n\t\t\t\t\t_(\"'%s' option not allowed in spec at command line.\")%self.__class__.__"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\tfrom pyppeteer.lib.resolvers import (  # pylint: disable=import-outside-toplevel\n\t\t_resolve_argspec, _resolve_argspec_list)\n\n\tif self.conf.argspec_resolve:\n\t\t# TODO(michenergbeek): Check for environment variable _resolve_argspec and remove this check!\n\t\tfor args in self.argspec_config:\n\t\t\tfor val in self.argspec_config[args]:\n\t\t\t\tif not val.has_key('value'):\n\t\t\t\t\tcontinue\n\t\t\t\tparsed"}]}
{"task_id": "6306092973426c38ae68ad01", "prompt": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\tdel self.__deprecated\n\tret = OrderedDict()\n\toptions = []\n\tfor opt in self.__options:\n\t\tif opt in self.__deprecated:\n\t\t\tret[opt] = self.__deprecated[opt]\n\t\t\tcontinue\n\t\tkey = opt.replace('_', '-_')\n\t\tret[opt] = getattr(self, key)\n\t\tif ret[opt] is True:\n\t\t\tdebug.trace( 'option %s is deprecated.  Skipping' % opt )\n\t\t\tcontinue\n\t\toptions.append(key)\n\tfor opt"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\tdeprecated_options = {}\n\tif self.get_default(self.option_names) is None:\n\t\tif self.default is not None:\n\t\t\tself.options[\"Deprecated\"] = self.default\n\t\t\tdeprecated_options[self.option_names[0]] = self.default\n\tdeprecated_options[self.option_names[1]] = \"Deprecated since %s.\" % __version__\n\treturn deprecated_options\n\n\ndef _deprecated(msg):\n\t\"\"\"\n\tUsage:\n\t\t- Deprecate this module\n\t\t- Fix the"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\twarnings.filterwarnings(action=lambda msg: msg.message.startswith('warn'), group='deprecation_warnings', category=FutureWarning)\n\n\tif True: pass\n\telse: warnings.warn(\n\t\t\"Warning: deprecated option \\\"{}\\\": Do not use.\".format(self.get('deprecated'))\n\t)\n\tdel self.deprecated\n\n\tif not getattr(self, 'deprecate_options', None):\t# Use deprecated options if there were no option.\n\t\t# The following two functions would be the default in 2.5, so in 2.5 an"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\timport argparse\n\tparse = argparse.ArgumentParser(description=__doc__)\n\twith caplog.set_level(logging.WARNING) as caplog:\n\t\tparse.add_argument('-b', '--arch', default='x86_64', type=str, metavar='ARCH',\n\t\t                   help=\"architecture\")\n\t\tparse.add_argument('-g', '--generator', default='x86_64', type=str, metavar='ARCH',\n\t\t          "}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\twarnings.sort(key=self.DEPRECATED_ARGS, reverse=True)\n\n\toptions = dict(\n\t\t'verbose'                = self.get_arg(0,'verbose'),\n\t\t'deprecated_args'        = self.deprecated_argmap)\n\toptions.setdefault('verbose',' 0        '+ self.get_arg(1,'verbose')\n\toptions.setdefault('deprecated_args',' 0    '"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\tresult = copy.deepcopy(self.deprecated_args)\n\t# Do not deprecate anything.\n\tresult['__all__'] = []\n\treturn result\n\n// Copyright 2016 The kingshard Authors. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\tdef deprecate():\t\"\"\"\n\t\t\"\"\"\n\t\tdeprecated warnings\n\t    \"\"\"\n\treturn deprecate_args\n\n@deprecation(_DEPRECATION, message=_(\"Warning: Deprecated attribute 'deprecated_data_attribute_type' cannot be used along with 'deprecated_args'.  This is not yet implemented.\"), id=\"deprecated_deprecated_data_attribute_type_deprecated\")\ndef deprecated_deprecated_data_attribute_type(deprecated_data_attribute_type):\n\t\"\"\"\n\tdeprecated, deprecated, deprecate the deprecated attribute.\n\t\"\"\"\n\treturn deprecated"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\toptions = [{'name': 'opt1', 'type':'str', 'default': 'val1'}]\n\tif sys.version_info >= (2,4):\n\t\toptions = [options]\n\toptions = dict(options)\n\treturn options\n\ndef test_deprecated_option_deprecations():\n\t\"\"\"Test deprecated deprecation (opt1=None)\"\"\"\n\toptions = get_deprecated_args()\n\tif 'opt1' in options:\n\t\tassert str(True)\t# deprisingly option is deprecated\n\telse:\n\t"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\treturn {{c}\n\td = deprecated(c, d = 15.0, d = 12.0,\n\t                     s = \"2.2.5\",\n\t                     s2 = \"5.1.36.3\",\n\t                     s4 = \"1.0.0.0\",\n\n\t\t      "}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\twarnings.filterwarnings(\"ignore\", message=\"ignore\", category=UserWarning)\n\t\n\tparser := argparse.ArgumentParser()\n\tparser.parse_args(self)\n\t\n\tif self.option_flags is None:\n\t\tself.option_flags = [ ]\n\t\n\tflags = self.option_flags[0]\n\t\n\tif FLAGS.fix_deprecation_warning: \n\t\t# Fix the deprecated/improperly documented arguments\n\t\tflags.deprecated = False\n\t\tflags.deprecated_args = self.deprecated_args\n\t\n\tparser.add_argument"}]}
{"task_id": "6306092c73426c38ae68ad02", "prompt": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tfor cli_arg in cli_args:  # pylint: disable=arguments-differ\n\t\tvalidated_args = self.validator(cli_arg)\n\t\tif not validated_args: raise ValueError(\"arg has no valid value\\n\"+cli_arg)\n\tif answer_file_args:\n\t\tvalidated_args = self.validator(answer_file_args)\n\t\tif not validated_args: raise ValueError(\"answer file arg has no valid value\\n\"+answer_file_args)\n\treturn validated_args\n\n################################################################################\n################################################################################\n################################################################################"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\targs = cli_args.__arg_names__\n\tif len(args) > 0 and len(answer_file_args) > 0:\n\t\ttry:\n\t\t\tanswer_file_args = json.loads(answer_file_args)\n\t\t\tif (len(answer_file_args['name']) <= 0):\n\t\t\t\tanswer_file_args = json.dumps(answer_file_args,\n\t\t\t\t\tensure_ascii=False, sort_keys=True)\n\t\t\tself.validate_argument_for_cli(answer_file_args)\n\t"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tfor idx, arg in enumerate(cli_args):\n\t\tif arg.help:\n\t\t\tprint(\"%s: %s\" % (idx, arg.help))\n\t\t\tcontinue\n\t\t# This is a new command name\n\t\tcommand_name = \"newcommand\"\n\t\tcli_name = arg.cli_name\n\t\tif cli_name == \"newcommand\":\n\t\t\tcommand_name = arg.command_name\n\t\t# Validate the positional args\n\t\tfor arg, value in cmd_names.items():\n\t\t\tif idx == len("}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tfor argument_file in cli_args:\n\t\targument_file.validate_deprecated(answer_file_args)\n\n\tfor argument_file in answer_file_args:\n\t\targument_file.validate(answer_file_args)\n\n\treturn answer_file_args\n\n\ntry:\n\tvalidate_arg_deprecation(\n\t\tsys.argv, cli.get_cli_args(sys.argv))\nexcept:\n\t# We want to catch exceptions here.\n\t# If there is some weird bug in the C-level documentation\n\t# parser"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tif len(cli_args) > 0:\n\t\tprint 'Argument {} is deprecated. Please use: - {}'.format(cli_args['arg'], '-'.join(cli_args['arg']))\n\t\tprint 'Usage: {}'.format(sys.executable)\n\telse:\n\t\tprint 'Argument {} is deprecated. Use: {}'.format(cli_args['arg'], '-'.join(cli_args['arg']))\n\t\tprint 'Usage: {}'.format(sys.executable)\n\n\n@print_help\ndef print_warning(msg):\n\tprint('Warning"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t_arg_map = {\"--args\": cli_args, \"--format\": \"html\"}\n\tif answer_file_args[\"args\"] is None or answer_file_args[\"args\"].lower() == \"stderr\":\n\t\tanswer_stderr = [\"echo\", \"-d\"]\n\t\tanswer_exit_code = []\n\t\tanswer_args = []\n\t\tfor line in chain(answer_stderr, answer_exit_code, answer_args):\n\t\t\tline = line.strip()\n\t\t\tif \"@{0}\" in line or \"@exit{0}\" in line:"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\targs, args_type = self.get_arg_types()\n\t# get the argument types\n\t# the cli arguments are used in the validate method\n\tcli_options = self.get_cli_options()\n\tcli_args.update(args)\n\tanswers, answer_arg_types = self.list_answer_arg_types(args)\n\t# we need this for the verify_args method\n\tanswer_args = { 'cli_options': cli_options, 'cli_args': cli_args, 'answers': answers }\n\t\n\toptions_list"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\twith self._arg_deprecations_registry:\n\t\tfor arg in self._arg_deprecation_registry:\n\t\t\targ_name = self._get_arg_name_from_cli_args(cmd, cli_args)\n\n\t\t\tif arg_name in self._arg_deprecation_registry:\n\t\t\t\tanswer_file_name = self._get_arg_deprecation_file(cmd, arg_name)\n\n\t\t\t\tprint(\"\\n*WARNING* the command {!r} is deprecated and will be removed in \"\n\t\t\t\t\t\"Python 3.  Please use {}"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\targ_deprecated = set()\n\n\tif cli_args:\n\t\targ_deprecated.update(cli_args.items())\n\n\tif answer_file_args:\n\t\targ_deprecated.update(arg_deprecated)\n\n\tif cli_args and arg_deprecated:\n\t\tanswer_file_args = arg_deprecated - cli_args\n\t\tif cli_args.get('nohelp'):\n\t\t\tself.warn('%s: No help available. '\n\t\t\t\t'missing arguments' % '.'.join(cli_args))\n\n\t\tif"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\tusage_text = (\n\t\t\t\t\"To fix this, run \" + sys.argv[0] +\n\t\t\t\t\t\t\" -help (and then try to remove them)\\n\"\n\t\t)\n\t#print (usage_text + cli_args[\"help\"])\n\t#exit(0)\n\tfor cli_arg in cli_args[\"cli\"]:\n\t\tif not cli_arg.startswith(\"--\"): continue\n\t\tif isinstance(cli_arg, str):\n\t\t\tcli_arg = cli_arg.lower()\n\t\tvalid_choices = [f for f in"}]}
{"task_id": "6306092d73426c38ae68ad04", "prompt": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\tcmd_map = self.command_map\n\tret = []\n\tfor key in cmd_map:\n\t\tif command_name in cmd_map[key]:\n\t\t\treturn ret\n\t\tret.append(self.create_option(key))\n\treturn ret\n\ndef get_parser_option(self, option_name):\n\tparser_option = self.get_parser_option_specs(option_name)[0]\n\tparser_option.parse_()\n\tif (getattr(parser_option.options, option_name) == command.COMMAND_OPTION"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\tparser, position = self._get_parser_option_info(command_name)\n\n\treturn parser.option_specs\n\n/*\n * Copyright (C) 2018 The Android Open Source Project\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\tself._log_with_status(\"Calling get_parser_option_specs()\")\n\n\tparser_option_specs = []\n\tparser_option_specs.append(self._get_parser_option_spec(command_name))\n\n\treturn parser_option_specs\n\n\t#def get_parser_spec(self, command):\t\"\"\"\n\tGets a single parser option by its name.\n\n:param command: a command name\n:return: a dict containing the argument (command name) and its associated flag (parsed flag value) as key(int flags) values\n"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\toption_spec = get_parser_option(self, command_name)\n\tresult = [x for x in sorted(option_spec) if hasattr(x, 'parser')]\n\n\treturn result\n\n/***************************************\n\n     Wing Lan.C for WIN32\n\n     A portable Windows version of DDX\n\n     (c) Yushrenlog University, 2007\n\n     Author: Colin Dietrich\n\n     All questions of the computer.\n\n     Please report all bugs"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t#\tcmd_name = self.find_command(command_name)\n\tcmd_args = self._get_args(command_name)\n\tparser_opts = get_parser_options(cmd_name)\n\treturn parser_opts\n}\n\n\n# def get_parser_options(cmd_name):\n\n# \t# find the correct parser\n# \tif cmd_name in self.parsers:\n\n# \t\t\tparser_name = self.parsers[cmd_name]\n\n# \telse:\n\t\n\ndef get_parser_options("}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\treturn self._get_parser_options(command_name)\n\ndef _get_libvirt_spec(self, command_name, data_spec):\n\tspec_list = []\n\tparser_command_name = command_name\n\tif data_spec:\n\t\tparser_command_name = data_spec['libvirt'][0]\n\tfor spec in data_spec['specifications']:\n\t\tif spec['command']==parser_command_name:\n\t\t\tspec_list.append(parser_command_name, spec)\n\n\t# TODO: find"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\ttry:\n\t\tself.options\t\t\t= self.command.options(command_name)\n\texcept:\n\t\tprint \"No commands for program\"\n\t\texit()\n\treturn self.options\n\n\t\ndef get_parser_options(self):\n\ttry:\n\t\tif self.options is not None and\\\n\t\t\tself.options.get('main_command') is not None and\\\n\t\t\tself.options.get('main_command').get('name') is not None:\n\n\t\t\tprint self.options.get('main_command"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\tfor option in self.option_defs:\n\t\t# Check if the command name is known at the moment.\n\t\tif self.check_function(command_name, option) == None:\n\t\t\tcontinue\n\n\t\t# If a value in our option is set it means that it has already been defined\n\t\toption_value = options_in_option(self.option_list, option)\n\n\t\t# Otherwise try to find a matching option in the current options\n\t\tif option_value!= None:\n\t\t\treturn option_value\n\n\traise OptionParser"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\tparser = self._get_parser(command_name)\n\tget_options(parser)\n\treturn parser._options\n\n\ndef _get_parser(command_name):\n\n    parser_name = 'parser'\n    if not hasattr(parser, 'cmd_parser'):\n        args = args_to_dict(parser_name)\n        if self.main:\n            command_name = command_name + '-main'\n     "}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\tparser = get_parser()\n\tparser.option_specs = dict(\n\t\tmain=self.main_option_specs,\n\t\tmainargs=self.get_command_options(\"Main\"),\n\t\tsubargs=self.get_command_options(\"Sub\"),\n\t\tspins=self.get_command_options(\"Spins\"),\n\t\tspinsargs=self.get_command_options(\"SpinsArgs\",command_name=command_name),\n\t\trun=self.get_command_options(\"Run\",command_name=command_name),"}]}
{"task_id": "6306092d73426c38ae68ad05", "prompt": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\tfor m in self.options:\n\t\tfor p in m.get_options():\n\t\t\tif p.name == argument_name or (isinstance(p, str) and p.name == command_name):\n\t\t\t\treturn p\n\treturn None\n\ndef get_available_opts(self):\n\treturn set(t.options for t in self.options)\n\nclass SetOfListOfOptions(object):\n    \"\"\"\n\tA mutable list of SetOfListOfOptions\n\t\"\"\"\n\n    def __repr__(self):\n      "}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\tfrom impacket import smb\n\ttry:\n\t\tsmb.SMB_COM_NEGOTIATE.append(command_name)\n\texcept:\n\t\tpass\n\tfor x in self.smb_auth_methods:\n\t\ttry:\n\t\t\tsmb.SMB_COM_NEGOTIATE.append(x)\n\t\texcept Exception:\n\t\t\tpass\n\treturn smb.SMB_COM_NEGOTIATE.get(command_name, argument_name,\n\t\t\t\t\tsmb.SMB_COM_NEGOTIATE_V2)\n\n/**\n* @license Apache-2.0"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t#\tif command_name not in COMMANDS and command_name not in COMMANDS.keys():\n\t#\t\traise ValueError(\"command_name %s is not a recognized string\" % command_name)\n\tif command_name not in COMMANDS:\n\t\traise ValueError(\"command_name %s is not a recognized string\" % command_name)\n\tspec = COMMANDS[command_name].get_option_spec(argument_name)\n\tif spec:\n\t\treturn spec\n\telse:\n\t\traise ValueError(\"'%s' is not a recognized string '%"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\tif not isinstance(command_name, six.string_types):\n\t\tself.fail(\"Command name '%s' must be a string!\" % command_name)\n\tif not isinstance(argument_name, six.string_types):\n\t\tself.fail(\"Argument name '%s' must be a string!\" % argument_name)\n\tfor spec in [\n\t\t(\"string\",\n\t\t\t\"s1\",\n\t\t\t\"r1\")\n\t]:\n\t\tself.assertEqual(type(spec), str, str(argument_name))\n\tif not isinstance("}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\toptspec = get_option_spec_from_command_argument(self, self._options_from_command_arguments, command_name, argument_name)\n\treturn optspec.value\n\ndef get_option_spec_from_command_argument(self, command_argument_parsers, command_name, argument_name):\t#pylint: disable=E1101\n\t\"\"\"\n\t\t`get` implementation for the `command_argument_parsers` global.\n\t\"\"\"\n\t#TODO: add ability to access the global options\n\toptspec = command_argument_parsers"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\treturn self._configuration.get_options(command_name, argument_name)\n\n\t\ndef get_options(self, command_name, argument_name):\t\"\"\"\n\tGets the list of all values for the specified option of the specified command_name, with their default values.\t\"\"\"\n\treturn self._configuration.get_options(command_name, argument_name)\n\n\nclass _BaseConfigurationWrapper(Configuration):\n\t\"\"\"Base Configuration wrapper\"\"\"\n\tdef __init__(self, configuration_instance):\n\t\t\"\"\"The constructor for the configuration wrapper class.\n\t\t"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\treturn self._arguments[command_name][argument_name]\n\n\t\n#    def option_description(self, command_name, argument_name):\t\n\t#        return self.default_argument_description(command_name,argument_name)[0][0]:\t# [value]\n\t#        return self.default_argument_description(command_name,argument_name)[1][0:]\t# [value]\n\n\nclass CommandLineArgumentFactory(object):\t\n\n\t\"\"\"The CommandLineArgumentFactory class represents"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t# I don't want to make this a class\n\tpass#IMPORTIMPORTIMPORT from OptionSetGenerator import get_generator\n\tclass CmdSpec(namedtuple('CmdSpec', 'option_name option_value')):\n\t\t# If name contains a space character then use that\n\t\topt_class = get_generator(argument_name, 0)\n\t\tcmd_spec = None\n\t\tif opt_class is not None:\n\t\t\toption_name = opt_class.option_name\n\t\t\tcmd_spec = OptConfigSpec(opt_class."}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\tif not self._read_opt(1): return 0\n\tif command_name == self.get_arg('arg_1'): return [ self._read_opt(0) ]\n\treturn [ self._read_opt(0), self._get_opt_default(0) ]\n\n\ndef _load_args(args):\n\t\"\"\"Returns a dictionary where keys are argument names and values are\n\tdefault values.\"\"\"\n\t# Handle the default option if any\n\treturn default_argument_spec(args), args[1] if args else default_argument_spec()\n\n"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t# this will only return the non-option name for a non-option\n\t# option name (this will prevent users from mistakenly using that\n\t# option name for a command)\n\treturn self.get_command_by_name('_%s_%s' % (command_name, argument_name))\n\n\ndef get_option_by_name(name):\n\t\"\"\"\n\tReturns the option with specified name or None if the option doesn't exist.\n\t\"\"\"\n\tfor o in (x for x in self.get_option_names() if x.startswith"}]}
{"task_id": "6306092d73426c38ae68ad06", "prompt": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\treturn [self.get_silent_name(x) for x in args]\n\ndef get_silent_name(args):\t\"\"\"\n\treturn str(''.join(\"silenced\" for arg in args for _ in args))\n\nclass Listify(object):\n\t''' This class simplifies list processing and simplification of sequences\n\tof strings with lists of strings of lengths equal to the desired number of\n\telements in a sequence '''\n\n\tdef __init__(self):\n\t\tself.list = []\n\t\tfor _ in range(100):\n\t\t\tself.list"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\t# args must be a tuple of (opt, opt_list)\n\tret = []\n\t# get the first argument\n\tfor x in args:\n\t\tif x[0] == '_': continue\n\t\tif len(args) > 1: ret.extend(args[1:])\n\n\t# check that the arguments are in a different order\n\tif not (x[0] in ret):\n\t\tret += [x, ]\n\t# get the rest\n\tfor x in reversed(ret):\n\t\tif x[0] not in ('_', '"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\treturn self._silenced_args[args]\n\ndef get_silent_value(self, silent_args):\n\tsilent_args_keys = silent_args.keys()\n\tfor silent_arg in silent_args_keys:\n\t\tif self._silent_args[silent_arg[0]]==silent_args[silent_arg[1]]:\n\t\t\treturn silent_arg[1], silent_arg[2]\n\npackage de.kommunicate.failfit.josm.data;\n\npublic interface TileCollection extends ArrayList<String> {\n}\n"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t# list of args in silenced argument.\n\tsli = {}\n\tfor e in args:\n\t\tif e.is_silent:\n\t\t\tcontinue\n\t\tsli[e.name] = e.name\n\treturn len(sli)-1, sli\n\n\ndef get_quiet_args(self, args):\t\"\"\"\n\tlist of quiet argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t# list of args in quiet argument.\n\tsli = {}\n\tfor e in"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\tsilent_args, args = self.get_input_args(args, silent=True)\n\n\treturn silent_args, args\n\n\t\ndef _get_output_result(result_names):\t\t\t\"\"\"\n\tGet the output values for a given output. \n\n:param result_names: result_names[0] is the output variable, result_names[1] is the output value.\n:return: list, output(s)values for a given output name\n\n\tList, the output names: result_names[n] is the output variable, result_names"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\treturn args\n\npackage com.nazhaorumt.recyclerviewpager.adapter;\n\nimport android.content.Context;\nimport android.graphics.drawable.Drawable;\nimport androidx.leanback.widget.HeaderView;\nimport androidx.recyclerview.widget.RecyclerView;\nimport android.view.LayoutInflater;\nimport android.view.MotionEvent;\nimport android.view.View;\nimport android.view.ViewGroup;\nimport android.widget.ImageView;\nimport android.widget.TextView;\n\nimport com.nazhaorumt."}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\tif not self.args_seen:\n\t\tself.args_seen = True\n\t\tfor arg, name in args.args:\n\t\t\tif self.silenced:\n\t\t\t\tself.print_silent(arg, name, False)\n\t\t\t\treturn\n\telse:\n\t\tself.assert(self.silent)\n\t\tsilenced_args = list(filter(None, self.args_seen.keys()))\n\t\tsilenced_args.sort()\n\t\tfor arg in silenced_args:\n\t\t\tif self.silenced:\n\t\t\t\t"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\tsilenced = args[1]\n\tsilenced = tuple(silenced[i] for i in range(len(silenced)-1)) + silenced\n\treturn silenced, args\n\ndef split_silents(args, silent_args):\n\tsilent = []\n\targs.sort()\n\tfor arg in silent_args:\n\t\tif arg and arg[0] == '$':\n\t\t\tsilent.append(arg[1:])\n\t\telse:\n\t\t\tsilent.append(arg)\n\tif len(silent) < 2:\n\t\treturn silent"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\treturn args\n\n\n\n# TODO: Find better way.\ndef get_current_args(dicts):\n\t\"\"\"Returns the current value of the silenced dictionary, if present.\n\n    list of all strings passed, starting from one to the last entry, if\n    given, of size greater than MAX_SILENT_ARGUMENTS and null\n\n    :return: A tuple of arguments, with the remaining string present.\n\tif at all, then:\n\t\tlist of arguments, with all strings in [0,MAX_SILENT_ARGUMENTS-"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\tslienced_arg_map = {}\n\tfor arg_name_set in args.keys():\n\t\tif arg_name_set =='slienced_args':\n\t\t\tall_silenced_args = []\n\t\telif arg_name_set in ('data', 'batch_buffer'):\n\t\t\tall_silenced_args = self._check_silenced_args_set(arg_name_set)\n\t\telif arg_name_set:\n\t\t\tsilenced_args_str = ','.join({str(arg_name) for arg_"}]}
{"task_id": "6306092d73426c38ae68ad07", "prompt": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tif not self.requires: return\n\trequired_args = set(args) - all(self.parse_args(arg) for arg in self.requires)\n\tif required_args:\n\t\traise ValueError(\"Required argument(s) : \" + ', '.join(self.requires))\n\n\tfor i in range(len(args) - required_args):\n\t\tif not args[self.args[i]] or args[self.args[i]] and self.args[i] not in required_args:\n\t\t\traise ValueError(\"No option / argument : \""}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tfor args in iter_required_args():\n\t\tif not check_required_args(args):\n\t\t\traise CmdError('Invalid --%s: %s' % ('--' + p, args))\n\n\nclass TestRun(CmdTest):\n\tdef test_run(self):\n\t\tself.checkRun()\n\n\tdef test_setup(self):\n\t\tself.checkRun('setup')\n\n\tdef test_configure(self):\n\t\tself.checkRun('configure')\n\n\tdef test_run_args_parsing(self):\n"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\trequires = None\n\tif(args == None):\n\t\treturn False\n\tif( len(args) > 0 ):\n\t\tfor p in args:\n\t\t\tif(p == None):\n\t\t\t\tcontinue\n\t\t\tp_val = self.get_param(p)\n\t\t\tif(isinstance(p_val,str)):\n\t\t\t\trequires += p\n\t\t\t\tif(requires == None):\n\t\t\t\t\trequires = p_val\n\t\t\telse:\n\t\t\t\tif(p_val == False):\n\t\t\t\t\tmissing_args = \"**\"\n\t\t\t\t\t"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tself.check_if_args_have_required_arg(args)\n\n\tif len(args) > 0 and args[0] == \"--verify-ssl-certificate\":\n\t\traise Exception(\"Cannot specify '-verify-ssl-certificate' once \"\n\t\t                 \"they have been installed\")\n\tif isinstance(args, six.string_types):\n\t\tif args[^1:]!= '.py': args = args.split('.')\n\t\targs1 = [\"-k\", '\"{}\"'.format("}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tn = len(args)\n\tfor i in range(n):\n\t\tif not arg_needs_spec(args[i]): return False\n\treturn True\n\ndef _get_all_spec_args(spec):\n\tif spec.options is None:\n\t\treturn None\n\tspec.args = _get_spec_args(spec.options._specDefaults) + spec.args\n\treturn spec.args\n\ndef _get_all_var_args(var):\n\t\"\"\"\n\tReturn a list of optional arguments that has not been overridden,\n\tre"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tif len(args) < 1: return True\n\tmissing_args = []\n\tfor arg in args:\n\t\tif not (self.__dict__ or arg in self.__dict__ and\n\t\t         arg not in missing_args):\n\t\t\tmissing_args.append(arg)\n\n\tif len(missing_args):\n\t\traise InvalidRequestError(\n\t\t\t\"Missing required data(s).  Available data are '%s'\" % \", \".join(missing_args),\n\t\t)\n\n\treturn True\n\n\ndef __init__("}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tif args:\n\t\targs = self._process_args(args)\n\telse:\n\t\targs = None\n\tself._process_args(args)\n\ndef _validate_options(self, opts, args_required):\n\tif isinstance(args_required, (list, tuple)):\n\t\toptions = self._get_required_options(args_required)\n\telse:\n\t\toptions = args_required\n\tif not options or not list(options) and hasattr(options, '__iter__'):\n\t\traise ConanInvalidOptionDefinition(\n\t\t\t"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tif self.missing:\t\t# print(self.missing,args)\n\t\toutput()\n\t\treturn\n\ttry:\n\t\treq = self.required.extend(args)\n\t\tself.required\t= req\n\t\tprint(req)\n\t\toutput()\n\t\treturn\n\texcept Exception as e:\n\t\terrorstring\t= \"Usage: {0}{1}\\n\\t{2}\\n\".format(self.prog,\n\t\t\t\"\\t\tUsage:\", \"\\t\"+str(e))+ \\\n\t\t\"\\n\t\"\n\t"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\treturn(args, [] if not args else args[0])\n\n@validator.doc(pattern='^/api\\.php',\n              check_messages = {'no_such_file': \"Not Found\",\n                               'missing_section': \"Missing Section\",\n                             "}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\tfor arg in args:\n\t\tvalidate_str(arg,\n\t\t\t'Required argument'+ str(arg)\t+'not provided'\n\t\t)\n\ndef validate_required_arguments(self, args, required_arg, values, keys,\n\trequired='required_arg', validate=None, message=None):\n\t\"\"+values[0]\n\tkeys[0]._add_arg(required_arg)\n\tkeys[0]._add_arg(values[0])\n\tfor arg in args:\n\t\tvalidate_flag(arg, message=message"}]}
{"task_id": "6306092d73426c38ae68ad08", "prompt": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t# Check if the argument is required given a value\n\trequired_when_list = self._split_str(args.get(\"required_when\", []),\n\t\t\"Requires a value for argument: \").keys()\n\n\tif len(args.get(\"required_when\", [])) > 0:\n\t\tif options_spec:\n\t\t\toptions = \"\"\n\t\t\tif \"options\" in options_spec.get(\"values\"):\n\t\t\t\toptions = options_spec.get(\"values\")[\"options\"]\n\t\t\telif \"values\" in request.args:\n\t\t\t\toptions"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\tassert not any(command in options_spec for options in command_spec['options']), 'only arguments'+ \\\n\t\t\t\t'with ``required_when`` condition are allowed.'\n\t\targuments, required_args, failed_args, all_passed = options_spec['arguments'].items()\n\n\t\trequired_args = [arg for arg in required_args if arg.required_when_condition_matched]\n\t\tif len(all_passed) == 1 and all_passed[0].condition_matched == self._check_for_args(\tcommand_name,\n"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\tif command_name.index(\"@\") < 0 :\n\t\treturn args, {}\n\tres = cmd_utils.get_option_data(self.options, command_name)\n\tres[\"required_when\"] = [\"required_when\", [\"name\", \"arg.type\", \"arg.value\"], args[0]]\n\tout = []\n\tfor a in range(1, len(res)):\n\t\tif \"required_when\" not in res or res[a] in res[a]:\n\t\t\tout.append(args[a])\n\treturn out\n\n"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\tcommand = self.get_command(command_name)\n\tif command is None:\n\t\treturn [], [], []\n\telif command.option_spec is None:\n\t\treturn [], [], []\n\t#   assert( len(command.option_spec.option_list) == 0 )\n\tret_string = [ \"\" for _ in command.option_spec.option_list ]\n\tfor s_list in command.option_spec.option_list:\n\t\tif s_list.cmd == \"all\" and len(s_list.argument_list)"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\trequired_when_args = []\n\n\tfor option_name,  opt_spec in options_spec:\n\n\t\tparsed = _get_parsed_values(opt_spec, None)\n\n\t\tif not parsed:\n\t\t\tcontinue\t\t\t\t\t# don't care\n\n\t\t\tcontinue\n\n\t\tif self._contains_all_required_when_condition(parsed):\n\t\t\trequired_when_args += [opt_spec + (self._optional_cmd_name + \"=\", option_name)]\n\n\t\telif option_name == \"required_when\":"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\t_args_re = re.compile\n\t\t_re_matcher = re.compile(options_spec.required_when_to_matches_regex(cmd, _args_re))\n\t\t_re_list = []\n\n\t\tcmd_options_specs = []\n\t\tcmd_options_names = self.get_required_whenargs(options_spec)\n\n\t\tfor i in options_spec:\n\t\t\tif cmd_options_specs and cmd_options_names:\n\t\t\t\t_re_list.append(\"%s[%d]\" % (cmd"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t# Check if the options matches\n\tif not args:\n\t\tif not command_name.startswith(\"_\"):\n\t\t\tassert self._validate_for_config(command_name + \" does not appear to be valid option. The only valid arguments are: \" + str(options_spec))\n\t\t\treturn self._list_all_arguments()\n\t\telse:\n\t\t\tassert command_name.startswith(\"_\")\n\t\t\treturn [], []\n\telse:\n\t\tfor arg in args:\n\t\t\tassert arg.startswith(\"_\")\n\t\t\treturn [], []\n\n\tassert"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\treturn [ x.required_when for x in self.required_when_list ]\n\n\n#def _make_arg_spec(s, prefix='__%s__' % (prefix,) ):\n#\t# if we make it not just a single item\n#\tif isinstance(s,str):\n#\t\ts = s.upper()\n#\treturn ''.join([ c for c in s.split() ])\n\n\nclass CommandSpecContext(object):\n\tdef __init__(self):\n\t\tself.prefix_str= self._get_default_string"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\tmatched_args = get_possible_required_when_condition_list(options_spec)\n\treturn command_name.split(\",\"), match_possible_matches(matched_args)\n\n/*********************************************************\n *\n *                                 *\n *                                  *\n * File YYY.cpp  "}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\targs_list = options_spec\n\toptions_spec.sort()\n\tcommand_args = self.options_parser.get_option_arg_by_name(command_name, self.options_parser.command_spec_spec,\n\t\t\t           command_name, \"required_when\")\n\trequired_when = []\n\tfor idx in range(len(command_args)):\n\t\tif i_options in command_args[idx] and command_args[idx] == command_args[i_options+1]:\n\t\t\t"}]}
{"task_id": "6306092e73426c38ae68ad09", "prompt": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\treturn self._argument_length is LARGER_THAN\n\n/*\n *  Licensed to the Apache Software Foundation (ASF) under one\n *  or more contributor license agreements.  See the NOTICE file\n *  distributed with this work for additional information\n *  regarding copyright ownership.  The ASF licenses this file\n *  to you under the Apache License, Version 2.0 (the\n *  \"License\"); you may not use this file except in compliance\n *  with the License.  You may obtain a copy of the License at\n *\n *        http://www"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif len(args[0]) > args_length(self):\n\t\traise ValueError(\"Length must be a positive number.\")\n\n# This file implements a simple'single-file' object that is\n# used to generate files from a database.\n# It is important to use a single-file object for the database\n# when using this plugin as a plugin to produce a\n# database of files.  If this is required, the resulting\n# database can be used elsewhere.\n#\n# This file also contains functions to get the database.\n#\nimport logging\nimport os\nimport sys\n"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\targs_len = len(args)\n\tif args_len is None:\n\t\traise ValueError(\"Cannot be empty ({})\".format(len(args)))\n\tif args_len < 2 or args[1]!='' or len(args) < args_len:\t# pragma nocover\n\t\traise ValueError(\"Invalid argument lengths: \")\n\t# For max length, take the size of maximum value.\n\tif len(args) > args_len:\n\t\traise ValueError(\"Max argument length has been exceeded\")\n\treturn\n\n#\n#\n#\n\n"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\treturn args[0].kind and args[0].kind!= \"int\" and (args[0].is_string or args[0].is_float or args[0].is_int or args[0].is_tuple or args[0].isinstance)\n\nclass _Array_Value:\n\t\"\"\"\n\tCreates a ArrayValue from a set of given elements.\n\t\n\tThere are many examples of use, but these examples are\n\ttested to ensure that the user wishes to specify\n\tthe correct value in the constructor of the ArrayValue.\n\t\n\t@param values: Set of"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif len(args) == 0:\n\t\treturn 0\n\tfor opt in sorted(self.options) + [self.valid_args[arg]]:\t\t# sort\n\t\tif args[opt]!= self.args[opt]:\n\t\t\t#print(\"ERROR: length option error\")\n\t\t\treturn 0\n\treturn 1\n\n\n\t\ndef validate_args(self, args):\t\"\"\"\t\n\n\tPerform some basic checks.\n\t-------------------------------------------------------------------------------------------------------------------------\n\tChecks can be skipped.\n\n\t----------------------------------------------------------------------------------------------------------------------\n\tIf args dose not match the list of"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif isinstance(args, dict) and args.get('length'):\n\t\tif args.get('length_min') and args.get('length') not in [50,100,1000]:\n\t\t\traise ValueError(_(\"Expected length to be in the range of [%d, %d]. \") % (50,100))\n\t\tif args.get('length_max') and args.get('length') not in [600,1000]:\n\t\t\traise ValueError(_(\"Expected length to be in the range of [%d, %d]. \") % (600,1000))\n# end def validate_"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif _is_int(args) and isinstance(args, (int, float)):\n\t\treturn\n\tif isinstance(args, int) and self.is_int(args):\t\treturn\n\tif args:\n\t\tself.validate_length_args(args)\n\ndef _argcheck_range_int(self, value, length, defaultval, funcname):\n\t\"\"\"\n\tCheck if a specified value is within range of value.\t\t\"\"\"\n\tif value >= 0 and value <= length-1:\n\t\tfuncname = funcName(funcname)"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tfor i, arg in enumerate(self.argument_list):\n\t\tif i < self.length:\n\t\t\tcontinue\n\t\tif len(arg) > len(args[i]):\n\t\t\tbreak\n\tif i + len(args[i]) > self.length:\n\t\tmsg = \"Invalid argument(s): length is longer than length of\"\n\t\tmsg += \" the parameter(s)- argument(s), found \"\n\t\tmsg += \"parameter(s) length \" + str(len(args[i])) + \",\"\n\t\traise InvalidArgumentError("}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\tif len(args) > self.size:\n\t\treturn False\n\treturn True\n\ndef validate_length(self, args):\n\t\"\"\"\n\tValidate if string value is longer than the specified size.\n\t\t- If 'length' argument is missing, it should be a number.\n\t\t- If 'length' argument is unknown, it should be an empty string.\n\t\t- If 'length' argument is 'length', it should be greater than or equal to the size parameter.\n\t\"\"\"\n\tif self.size is not None:\n\t\tif args[0"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\targs_str, _ = self.check_length_args_params(args)\n\tif len(args_str) < 1 :\n\t\tprint(colors.red(\"Value of arguments to length() has to be a \"\n\t\t\t\"valid length of %d characters.\" % self.length()))\n\n\ndef set_length(args):\t\"\"\"\n\tSet a length that is allowed by the client.  Currently only\n\tallows one length.\n\n:param allowed: The allowed length.\n:type allowed: L{int}\n\tThe minimum allowed length.\n\tDefaults"}]}
{"task_id": "6306092e73426c38ae68ad0a", "prompt": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif hasattr(args, 'choices'):\t# We assume that choices arg was given!\n\t\t\tfor choice_arg in args.choices:\n\t\t\t\tif not self.is_choice(choice_arg): return False\n\t\telse:\t# Otherwise assume that there are no more arguments.\n\t\t\treturn False\n\t\t# XXXI think we need the whole thing in the for in case we are using the choices list as the default.\n\t\tif \"choices\" in args:\n\t\t\treturn False\n\t\treturn True\n\n<?php\n/*\n * @copyright "}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tfor choice in self.choices:\n\t\tif args[choice]!= self.choices[choice]: return False\n\treturn True\n\nclass InputFieldWidget:\n\t\"\"\" A widget that holds the raw content of value, which is a dict if\nthe widget has a data() function.\nThis can be overridden or None if the widget data\nis passed on.\n\"\"\"\n\tdef __init__(self, name=None, initial_state='in_text', data=None, validators=None):\n\t\tself.name = name\n\t\tself.__defaults = None\n\t\t"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tif not args:\n\t\traise ValueError(\n\t\t\t\"Error: You need to supply a valid choice argument list, \"\n\t\t\t\"use argparse.Argument() for example.\"\n\t\t)\n\n\tif len(args) == 0:\n\t\traise ValueError(\n\t\t\t\"Error: You need to supply a valid choice argument list with an \"\n\t\t\t\"appropriate number of arguments. See the\"\n\t\t\t\"    :class:`~ezdxf.graphics.GraphicsArgument` class for complete details.\"\n\t\t)\n\n/*\n * Licensed to the Apache Software"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tnargs = len(self._choices)\t\t\t\t# check arg count\t\n#\ttry:\n\t\tnargs = max(nargs, args.nargs)\t\t# handle number of options\n#\texcept TypeError:\n#\t\tpass\n\ttry:\n\t\t\tif(nargs >= self._choices[0] or nargs <= self._choices[0]>1):\n\t\t\t\traise CommandError(\"invalid choice length for all arguments (should be 0 or 1)\")\n\texcept:\n\t\traise CommandError(\"invalid choice arguments\")\n\t\n\t\n\tfor i in range(1, nargs + 1"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tif args in self['Choices'] or args not in []:\n\t\traise ValueError('Unexpected call to %s() before %s()!'  % (self.__class__.__name__, args))\n\tfor i in range(1, len(args), 2):\n\t\tif len(args[i]) > 6:\n\t\t\traise ValueError('Expected less than %d items: %s'  % (len(args), args))\n\nclass Validator(base.Validator):\n\tdef __init__(self, *args):\n\t\tself.message = None\n\t\tself.expected"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\ti = iter(args)\n\n\n\t# Validate the choices, using the user's default value if missing.\n\tif args[0] =='':\n\t\treturn self.validate_choices(self.kwargs.get('choices', ','.join(i)))\n\n\t# If the user has chosen any of the choices, that's it.\n\treturn self.validate_choices(args[0])\n\n\n\t# Invalid values.  The user's choice for choosing is invalid!\n\n\n# HELPER FUNCTIONS #################################*******\n\n\"\"\"\n\n########################################################################\n##  "}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tif args[\"choice\"] == args[\"choice\"]:\n\t\treturn True\n\treturn False\n\n// Licensed to the.NET Foundation under one or more agreements.\n// The.NET Foundation licenses this file to you under the Apache 2.0 License.\n// See the LICENSE file in the project root for more information.\n\n#if IS_UNCLORALLY_MARKED_LICENSE\n\nnamespace Steeltoe.Common.Kubernetes\n{\n    public class K8sLabelSelectorInput\n    {\n        public string K8sKind { get"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tfor choice_arg in args:\n\t\tif'' not in choice_arg:\n\t\t\tcontinue\n\t\tif choice_is_multiple(choice_arg):\n\t\t\tcontinue\n\t\tif choice_is_nonstandard(choice_arg):\n\t\t\traise ValueError(\"Invalid 'choice' argument value : %s, Must only contain spaces and dashes\\n\" %\n\t\t\t\t\t\t\t   \"        '         '\".join(choice_arg.split()))\n\t\ttry:\n\t\t\tif (choice_"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\treturn check_valid_choices_args(args)\n\n\ndef load_data_format(data_format, mode) -> data_format_t:\n\t\"\"\"\n\tReturns a data_format_t from current_data_format and set the mode\n\n\tIf mode is one of ['anyway', 'csv', 'any', 'csv-with', 'json'], the mode is\n\teither csv_with or json_with and the data_format is not None\n\tIf mode is 'raw', the data_format is either json_with or csv_with\n\n\tReturn"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\tprint 'Got choice argument %s' % (str(vars(args)))\n\tfor arg_name, arg_value in vars(args).items():\n\t\tif arg_value == 'auto':  # auto should be 'auto' always\n\t\t\tif utils.is_string(args[arg_name]):\t\t# None is a valid choice value, so make it a string\n\t\t\t\ttry:\n\t\t\t\t\targs[arg_name] = self._check_arg_for_choices_valid(args, arg_value)\n\t\t\t\texcept KeyError:\n\t\t\t\t\tprint ('"}]}
{"task_id": "6306092e73426c38ae68ad0b", "prompt": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tfrom.lib.validators.valid import CheckValuesValid\n\treturn CheckValuesValid(args[1], args[0])\n\ndef _test_values_is_between_min(args):\t\t\"\"\"\n\tCheck if values are between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tfrom.lib.validators.valid import CheckValuesValid\n\treturn CheckValuesValid(args[0], args[1])\n\ndef _test_values_above_min(args):\t\t\"\"\"\n\tCheck if values are below minimum.\n\n:param args"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif min(args[0], min(args[1])) <= args[2] <= max(args[0], args[1]):\n\t\treturn True\n\treturn False\n\n\"\"\"\n\tValidation.\n\n:param kwargs: The received kwargs.\t\"\"\"\n\tval = validate_min_max_args(args[0])\n\tval = validate_str_dict(vars(args[1]), val)\n\tval = validate_str_dict(vars(args[1]), val, lambda x: self.errors(x))\n\treturn val\n\"\"\"\n\n\ndef"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\ttry:\n\t\targs.pop(0)\n\texcept (KeyError, ValueError):\n\t\tpass\n\telse:\n\t\traise argparse.ArgumentError(\"Invalid min and max values\")\n\n\nclass Addons(object):\n\t\"\"\"\n\tThis class is meant to be used together with :class:`Addons.Create` to create an addon for a particular\n\tapplication.\n\n\tThis class adds a new addon of the same name that the user created in order to provide\n\tthe user answers to the addon.\n\t\"\"\"\n\n\tdef __init"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif args is None:\n\t\treturn\n\n\tfrom optparse import OptionDef\n\t\t# TODO: validate that all arguments must be passed in order (e.g. min, max)\n\n\t# we're supposed to use 'x=2,y=8' syntax and convert to numbers.\n\tif 'x' in args or 'y' not in args:\n\t\treturn\n\n\tif isinstance(args, list):\n\t\tn = 0\n\t\tfor i in args:\n\t\t\tif not isinstance(i, int):\n\t\t\t\treturn\n\t\t\t"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif (args.min is not None):\n\t\tif (isinstance(args.min, Number2D)):\n\t\t\tval = args.min\n\t\telse:\n\t\t\tval = args.min.val\n\telse:\n\t\traise ValueError\n\tif (args.max is not None):\n\t\tif (isinstance(args.max, Number2D)):\n\t\t\tval = args.max\n\t\telse:\n\t\t\tval = args.max.val\n\telse:\n\t\traise ValueError\n\treturn val\n\ndef check_min_max_args"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif args.minimum:\n\t\targ = self.args[self.index]\n\t\tif arg.value >= args.minimum:\n\t\t\targ.value = args.minimum\n\telif args.maximum:\n\t\targ = self.args[self.index]\n\t\tif arg.value <= args.maximum:\n\t\t\targ.value = args.maximum\n\treturn\n\n_ARG_MAP = Dictionary(\n    'ARG_MAP',\n    {\n        # Arguments that are in use should not be checked.\n"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\treturn check_args(args)\n\ndef validate_args_are_greater_than(self, args):\n\t\"\"\"\n\tEnsure that both parameters are greater.\n\n:param args: The received arguments.\t\"\"\"\n\treturn self.validate_args_are_greater_than_one(args)\t\"\"\"\n\t\"\"\"if the given parameter was not of size 1 or is greater than 3*0 then the parameter \n\tis too small for the parameter length.\t\"\"\"\n\treturn not self.do_check(args)[1]\t# Check can still be 1 because we are a"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif self.min_value in args:\n\t\treturn False\n\telse:\n\t\tif self.min_value == args.min(2):\n\t\t\tif not any([ self._check_min_value(a, min) for a in args.popmin(2) ]):\n\t\t\t\t\treturn False\n\t\t\treturn True\n\treturn False\n\n####def validate_min_max_args(self, args):\n#### def validate_min_max(self, args):\n#### def validate_min_max_args(self, args):\n#### def validate_min"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tif not args[0]:\n\t\traise ValueError(\"Argument 0 should be a sequence of minimum and maximum, got %.s\" % str(args))\n\tif not args[1]:\n\t\traise ValueError(\"Argument 1 should be a matrix, got %.s\" % str(args))\n\tif not isinstance(args[1], Matrix):\n\t\traise ValueError(\"Argument 1 should be Matrix, got %.s\" % str(args))\n\treturn False\n\n#pragma once\n\n#include <stdio.h>\n#include <stdint.h>\n\n#define USE_32"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\tnargs = 0\n\tif args.min is not None:\tnargs = 1\n\tif args.max is not None:\tnargs = 2\n\tif nargs == 1:\t\t\t\t\t\t\t# None or 0\n\t\tself.error(\"Invalid value of arguments to '%s' -- minimum and maximum must be specified\", args.name)\n\telse:\n\t\tnargs = 1 + 2\n\tif nargs < 3 or nargs > 4: return \"Invalid number of arguments to '%s' -- must be greater than or equal to 3 and equal to 4\" % args.name\n\telse:\n\t"}]}
{"task_id": "6306092e73426c38ae68ad0d", "prompt": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t", "suggestions": [{"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t_logger.debug('\\n\\ncreate_complex_argumet_type')\n\treturn self.create_complex_type(subcommand, type_name, option_name, spec_option)\n\n\ndef create_function_args(self, module, function_name, option_name, default_argument, opt_name=\"\", is_option=False):\n\t\"\"\"\n\tGiven a module, function name, and option name create the args that are stored here.\n\tArgument specifications are as above.\n    :param module: The module owning the arguments\n\t:param"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\toptions_dict = self.parser_options\n\n\tif options_dict.has_key('complex_type'):\n\t\tcomplex_type = options_dict['complex_type'][0], options_dict['complex_type'][1]\n\telse:\n\t\tcomplex_type = None\n\n\tif subcommand == 'create_complex_argumet_type':\n\t\targs = list(filter(lambda complex_arg,complex_arg:\n\t\t\t(name in kwargs.keys() and\n\t\t\tcomplex_arg[name]!= None), [complex_arg,"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\tspec_type = self.spec_types(type_name, option_name, self.current_group)\n\toptions = self.get_complex_type_options(subcommand, spec_type)\n\tsubcommand_spec = Spec(type_name=type_name, option_name=option_name,\n\t\t\t       options=options,\n\t\t\t       description=\"specify\",\n\t\t\t       description_params=self.default_description_params(spec_type, spec=spec_type))\n\treturn self.create"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t# print \"Creating argumet type %s\" % type_name\n\t# return types.create_complex_type(self, subcommand, type_name, option_name, spec_option)\n\tif type_name == \"list_argument-type\":\n\t\treturn type_argumet_type(self, subcommand, \"list_argument\", option_name, spec_option)\n\telif type_name == \"map_argument-type\":\n\t\treturn type_argumet_type(self, subcommand, \"map_argument\", option_name,"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\tcmd = \"subcommand\"\n\tif subcommand not in cmd:\n\t\tprint(\"warning: subcommand %s not found\" % subcommand)\n\tsubprocess.check_call(cmd + [subcommand], shell=True)\n\tname, subcommand_classifiers = subcommand.split(\",\")\n\n\t# First, check the type to make sure its an array, and set the option values\n\t# in the real command-line argument dictionary\n\t# Make sure they are array items\n\tsubcommand_classifiers = subcommand_classifiers[:-1]\n\t# if"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\treturn _create_complex_type(self.__model, subcommand, type_name, option_name, spec_option,\n\t\t\t\t\t\t\t\"\".join([item.as_text() for item in self.__dict['subcommands']]),\n\t\t\t\t\t\t\t\"\".join([a['arg'] for a in item.as_list()])\n\t\t\t\t\t\t)\n\n\n\t\n#---------------------------- COMMANDS ------------------------------------------\ndef compile_command(code, option_list):\n\t\"\"\"\n\t   Compile a command based on a single argument.\n\t@param code: the command code\n\t@param option"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\treturn complex_type_instance(self, subcommand, type_name, option_name, spec_option,\n\t\t\t\terror_value = \"Not implemented\", default_values = \n\t\t\t\t\t[\"complex64, float, complex128, complex256\"]\n\t\t\t\t)\n\ndef command_types(self, subcommand, type_name, option_name, error_value = \"Not implemented\"):\n\t\"\"\"Finds the types of a subcommand's command\n\n:param subcommand: The name of the command\n:param type_name: The datatype name\n:param option_name:"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\traise exc.ArgumentError(argument)\n\n\n@specialization_def(\"get-subcommand-type\", args_template=get_complex_argumet_type.template)\ndef get_complex_argumet_type(self, subcommand, args_template, option_name,\t\"\"\"\n\treturn complex_argumet_type(subcommand,\t\"complex-argv\", args_template)\nend\n\n\n@specialization_def(\"create-complex-argumet_type\", args_template=create_complex_argumet_type.template)"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t_subcommand = None\n\targs = \"\"\n\tspec_type = None\n\tspec_name = None\n\tvarargs = \"\"\n\tparamspecs = []\n\ti = 0\n\tfor entry in spec_type.entries():\n\t\tspec_type = entry.data\n\t\tif spec_type is None:\n\t\t\tcontinue\n\t\tif spec_type.is_tuple():\n\t\t\targumet = spec_type.args[i]\n\t\t\targumet = self._convert_complex_argumet(subcommand,\n\t\t\t\t\t\t\t\t\t\t\targume"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\tif spec_option:\n\t\tif spec_option['value']:\n\t\t\tif type_name is None:\n\t\t\t\ttype_name = spec_option['value_type']\n\t\t\tif option_name is None:\n\t\t\t\toption_name = spec_option['value_opt']\n\treturn ttypes.ComplexArgumetType(**argspecs.make_argumet_type(subcommand, type_name, option_name).get_spec()) # type: ttypes.ComplexArgumetType\n\n\ndef create_int_argumet_type"}]}
{"task_id": "6306092e73426c38ae68ad0f", "prompt": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\targs = parse_args(args, self.default_args)\n\tnested_args = parse_args(args.nested_args, self.defaults[args.args_name])\n\tcustom_args = parse_args(args.custom_args, self.defaults[args.args_name])\n\n\tcontrol_aet = (\n\t\targs.control_aet_name if args.control_aet_name else None\n\t)\t# yml is a string of control_aet name\n\tcontrol_args = (\n\t\targs.control_args"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\treturn {'args': args}\n\n\ndef yapf(func):\n\treturn {'func': func}\n\n\ndef get_yapf_args_and_args(func, args_and_args):\t\"\"\"\n\tif isinstance(args_and_args, str):\n\t\targs = args_and_args.split(',')\n\telif isinstance(args_and_args, Iterable):\n\t\targs = list(map(get_yapf_args_and_args, args_and_args))\n\telse:\n\t\targs = args_"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\targs = []\n\tfor arg in args:\n\t\targs.append('[' + str(arg[0]) + ']' + str(arg[1])*2.5)\n\n\tcontrol_nested_args = []\n\tfor arg in args:\n\t\tnested_args = []\n\t\tfor k, v in sorted(arg[1].items()):\n\t\t\tnested = []\n\t\t\tfor nested_val in v:\n\t\t\t\tnested.append(self.get_nested_value(nested_val))\n\t\t\tcontrol_nested_args.append(nested)\n"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\tdef convert_orthonormal_args(args):# @Unused\n\t\tif (   len(args[0]) > 2 \t\t: ) # Only use first two arguments of control (they are used by playbook)\n\t\t\t\targs = [arg.strip() for arg in args[1]]\n\t\t\treturn args, args[2:]\n\tnew_args = []\n\tnew_args = [None]\n\n\targs = [e.strip() for e in args[1]]\n\twhile len(args) > 0: #@Unused\n\t\targs."}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\treturn_ = {}\t\t\t\t\t# Used for logging\n\treturn_params = {}\t\t\t# Used for logging\n\n\t# The list of the control args (which is usually split).\n\t# Each entry corresponds to something that can be passed through\n\t# the yml file. The keys are the param names and the values\n\t# are the actual args that have been entered.\n\t# This will use the fully parsed file names (without the \"|\" symbol\n# \t\tand all the params) as keys for parameters.\n\tcontrol_args = args[4:]\n\tnested_args"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\tcontrol_args = []\n\tnested_args = []\n\tall_args = []\n\tcustom_args = []\n\twith_args = []\n\tfor arg in args:\n\t\tif arg[0] == \"--\":\n\t\t\tif not arg[1]:\n\t\t\t\twith_args.append(arg)\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tbreak\n\t\tif arg[0] == \"control\":\n\t\t\twith_args.append(arg)\n\t\t\tcontinue\n\t\tif isinstance(arg, (list, tuple)):\n\t\t\tif"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t# We dont allow None for nested usage if they are not None and\n\t# control_nested not None\n\n\t# Control nested arguments first\n\t# This is very unlikely scenario, but should probably make it safer\n\t# by going from a dict to a tuple.\n\n\tif not args[:2] and not args[-2]:\n\t\treturn (self.args, self.nested_control_command_args)\n\n\t# We start with a list of control and nested fields to be checked.\n\t# All args except control. This is necessary because if a\n\t# control"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\treturn self.args_spec(args), self.nested_args_spec(args)\n\n\nclass SpecArgs(AnsibleArgs):\n\t\"\"\"\n\tUsage: playbook_spec.spec\n\t\"\"\"\n\tdef args_spec(self, args):\n\t\tsuper().args_spec(args)\n\t\targs.update(self.kwargs_spec)\n\t\treturn args\n\npackage mega.privacy.android.app.utils;\n\nimport android.content.Context;\n\nimport java.util.ArrayList;\nimport java.util.List;\n"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\targs = vars(args)\n\tparams = vars(self.params)\n\toutput = []\n\tnested_args = []\n\tcustom_args = []\n\n\tcontrol_args = []\n\tif args['control']:\n\t\tcontrol_args = args['control'].split(',')\n\tnested_args = args['nested'].split(',')\n\tcustom_args = args['custom'].split(',')\n\tn = len(control_args)\n\tparams.update(set(control_args))\n\tparams.update(set(nested_args))\n\tparams."}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\tcontrol_args = list(args)\n\tdef _extract_control_args(name, y):\n\t\t# This is a list of arguments named 'name'.\n\t\tif y.find('name=\"')!= -1:\n\t\t\treturn y\n\t\tif y.find('arg=')!= -1:\n\t\t\treturn (args[name], controls[name])\n\t\treturn (args[name], nested_args[name])\n\t\n\tpartial_y = {}\n\tcontrol_args_y = list(control_args)\n\tif y.find('"}]}
{"task_id": "6306092e73426c38ae68ad11", "prompt": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t", "suggestions": [{"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tmerge_dict = {}\n\tfor var in vars_dict:\n\t\tif not var:\n\t\t\tcontinue\n\t\tmerge_dict[var] = extra_vars\n\tfor var in extra_vars:\n\t\tif not var:\n\t\t\tcontinue\n\t\tmerge_dict[var] = []\n\tfor var in vars_dict:\n\t\tif len(merge_dict[var]) > len(extra_vars):\n\t\t\tmerge_dict[var].extend(extra_vars)\n\treturn dict(merge_dict)\n\n################################################################################\n\nclass"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tfor name in vars_dict.keys():\n\t\tif extra_vars:\n\t\t\tvars_dict[name] += (extra_vars[:-1] or [])\n\t\t\tvars_dict[name][:extra_vars[-1].count(\".\")] = extra_vars[-1]\n\t\telse:\n\t\t\tvars_dict[name] = [vars_dict[name]]\n\ndef save_extra_vars(new_dict, prefix):\n\t\"\"\"\n\tMake an extra variables list from the dictionary and save\n\tin the first argument, the filename specified in the prefix.\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tnew_dict = {}\n\tfor e in vars_dict:\n\t\tif e in extra_vars:\n\t\t\tnew_dict[e] = extra_vars.index(e)\n\treturn new_dict\n}\n\n##################################################\n#                                           #\n#                         module/"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\textra_vars = extra_vars + _extra_vars_to_merge\n\textra_vars.extend(vars_dict)\n\treturn extra_vars\n\t\n_extra_vars_to_merge = {}\n\nclass __class__(type):\n\t\"\"\"Provides additional information for a class derived from a\n\tsubclass.\n\n\t:param type: The class type\n\t:param __dict(): An instance of a class type\n\t:param __dict(name=name, method=MethodType(method, tuple)): A function that wraps the callable with the signature :\n\n\n\t"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tmerge_result = vars_dict.merge(extra_vars)\n\treturn merge_result\n\n\ndef get_unmergeable_vars_dict(vars_dict):\t\"\"\"\n\tReturns Dictionary which can be set at ``vars_dict`` without copying.\n\n:param vars_dict: Dictionary for which we want to get from ``vars_dict``\n:returns: Dict of unmergeable kwargs\n\"\"\"\n\tvars_dict_copy = {}\n\tfor key in vars_dict.keys():\n\t\tif key in vars_dict_copy:\n\t\t\tlogger.error"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tif extra_vars is None:\n\t\textra_vars = extra_vars_dict_default()\n\tfor key in vars_dict.keys():\n\t\tvars_dict[key] = merge_vars(vars_dict[key], extra_vars)\n\n\ndef merge_vars(\n\tvar1,\n\tvar2=None,\n\textra_vars=None,\n):\n\t\"\"\"Add variables from ``var1`` to ``var2`` and ``extra_vars``\n\t\tif ``additional_vars`` is None.\n\t\t\"\"\"\n\t# TODO"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tvars_dict[extra_vars] = []\n\tfor var in vars_dict:\n\t\tif not var in extra_vars:\n\t\t\tvars_dict[extra_vars].append(var)\n\treturn vars_dict\n\ndef _clean_dict_type_properties(data, key, obj):\n\n\tif hasattr(data, key):\n\t\tdata.pop(key)\n\telse:\n\t\tdata = getattr(obj, key)\n\t\tif type(data) is type and issubclass(data, dict):\n\t\t\tdata = dict(["}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tvars_dict['*'] = vars_dict['model']\n\n\tfor name in extra_vars:\n\t\tvars_dict[name] = extra_vars[name]]\n\n#\n# Merge with different models (including mixture models)\n#\nmerge_mix(model, mix_list, model_to_mix=None, mix_file=None)\nmerge_mix(mix_list, mix, mix_file=None)\nmerge_mix(mix_list, mix, mix_file=None, mix_file_path=None)\n#\n# Merge"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tfor var, value in vars_dict.items():\n\t\tif value:\n\t\t\tif extra_vars:\n\t\t\t\tvars_dict[var] = dict(zip(*sorted(extra_vars)))\n\t\t\telse:\n\t\t\t\tvars_dict[var] = value\n\ndef get_default_config():\n\t\"\"\"Get default config dict\n\n\tReturns:\n\t\tconfig dict as it is to change if they haven't changed\n\t\"\"\"\n\treturn_dict = get_global_config()\n\tcopy_config = copy.deepcopy(default_config)\n\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\tfor key, extra_var in vars_dict.items():\t\n\t\t# Check if the key ends with '_extra_'\n\t\t# If not, add an underscore\n\t\tif type(key) == type(\"\"):\n\t\t\tkey = \"_\" + key\n\t\tvars_dict[key] = extra_var + \"_extra_#{key}\"\n\treturn vars_dict\n\ndef preprocess_data(d) -> dict:\n\tvars_dict = {}\n\tfor key in d:\n\t\tr, t = key.split(\":\") # split into variable names,"}]}
{"task_id": "6306092f73426c38ae68ad13", "prompt": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\tif not verbose: extra_vars = None\n\tprint(_(\"Launching playbook (path?)%s%s\") % ( playbook_path, extra_vars ),\n\t\tprog=sys.stderr)\n\t# Execute 'playbook' in normal mode\n\ttry:\n\t\tif os.path.isdir(playbook_path):\n\t\t\traise Exception(_(\"Folder '%s' already exists!\").format(playbook_path))\n\t\t\n\t\t# Get the root directory\n\t\tcwd = os.getcwd() if isinstance(os.getcwd(), os.PathLike) else"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t# This is the main function for the call of the ansible\n\tdef __init(ir_workspace, ir_plugin, playbook_path, verbose=None,\textra_vars=None, ansible_args=None):\n\t\t\"\"\"\n\t\tArguable constructor.\n\t\tArgs and parameters are passed to the Ansible API via a dict\n\t\tlike as below.\n\n\t\t:param ir_workspace: An InfraredWorkspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\tif verbose:\n\t\tprint(\"Calling an ansible-playbook...\")\n\tif playbook_path:\n\t\tverbose.warn(('You called an ansible-playbook with a play book you've set already '+\n\t\t\t'it was a valid path or is a path inside a playbook: ')+\n\t\t\t'\"'+playbook_path+'\"), extra_vars={'playbook_path': playbook_path})\n\t\tpprint('--> '+('The'+ str(playbook_path) +'command was invalid.'.replace('_',\n\t\t\t\t'')"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\treturn _run_playbook(ir_workspace, ir_plugin, playbook_path, verbose, extra_vars), True\n\ndef ansible_playbook_with_arg(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An InfraredWorkspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: A"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\ttry:\n\t\treturn ansible_playbook_ex(ir_workspace, ir_plugin, playbook_path,\tverbose, extra_vars, ansible_args)\n\texcept:\n\t\timport warnings\n\t\twarnings.warn(r\"An object of type InfraredPlugin::playbook is not available, skipping the call\", RuntimeWarning)\n\t\tpass # Don't retry unless the Ansible playbook fails.\n\n\ndef ansible_playbook_ex(ir_workspace, ir_plugin, playbook_path, verbose, extra_vars, ans"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\tdef args(self):\n\t\t\"\"\"\n\t   :return dict. dictionary( key = arg1, value = arg2). \nAn ansible_playbook may implement many different functions.  Here\nfunctions are:\n\n-   setUserDict\n -   getPlayerList\n -   getScore\n -   getPlayerList\n -   setUserDict (Optional)\n -   setScore (Optional)\nIt is sometimes necessary to have the correct setUserDict and getPlayerList\nimplementations.\n-   runCommand\n\"\"\"\n\t\t# If"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\treturn AnsiblePlaybook(ir_workspace, ir_plugin, playbook_path, verbose, extra_vars, ansible_args, \"\")\n\n\nai = AnsiblePlaybook(\"ansible\")\n\n\n@ai.command([\"-d\", \"--debug\", \"-s\"])\n@click.option(\"--level\", \"-l\", type=int, default=0, autoresetvalue=True, help=\"Debug level, 0 to quiet, 1 to log level\")\ndef aisdebug(debug, level):\n\t\"\"\"\n\tEnable debug mode by setting a value for the"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from.mainwin import run_playbook\n\n\targs = passwords_str_to_dict(ansible_args)\n\trun_playbook(ir_workspace, ir_plugin, playbook_path, args.p, args.verbose, extra_vars,\n\t              ansible_args)\n\ndef ansible_init(ir_workspace, ir_plugin, playbook_path, run_path, verbose=1, extra_args=None):\n\tpass#IMPOR"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\treturn execute_simple_playbooks([ir_workspace, playbook_path],\n\t\t\t\t\t \tverbose=verbose, **extras), playbook_path\n\ndef exec_ansible_plugin_command(ir_workspace, plugin_path, ansible_args, verbose=None, extra_vars=None):\n\t\"\"\"\n\tLaunch a plugin in an ansible-plugin.  See the ansible-playbook package for\n\tmore info on available options on ansible-plugin.\n\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\timport sys\n\timport os\n\timport logging\n\timport ast\n\n\tif os.getenv('DEBUG', False):\n\t\tlogging.debug(\"Enabling debug logging, due to %s.\"%(verbose))\n\n\tif verbose:\n\t\tlogging.info(\"%s %s\", file=sys.stderr, \"Starting %s\" %  os.getenv('DEBUG') )\n\t\tprint \"Starting %s...\" % os.getenv('DEBUG')\n\n\ttry:\n\t\tif os.getenv('DEBUG'):\n\t\t\tlogger = logging.getLogger('ansible.main"}]}
{"task_id": "6306093273426c38ae68ad15", "prompt": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\ttry:\n\t\tvars_dict = vars_dict['_vars_by_cli']\n\texcept KeyError:\n\t\tvars_dict = {}\n\tvars = vars_dict.copy()\n\n\tvars['cli_args'] = cli_args\n\n\tresult = run_ansible_cli(cli_args, vars)\n\tansible_ok, ansible_fail = result.rc == 0, result.rc == 1\n\tif result.rc == 0 and ir_workspace.path_exists(ansible_ok):\n\t\tvars['ansible_ok']"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\tvar_names = vars_dict[\"variable_names\"]\n\t\n\tansw_result = {\"ansible_results\": {},\"output_types\":\"auto\"}\n\tansible_result = answ_result\n\tfor avar in var_names:\n\t\tansible_result[avar] = ansible.execute(\"ansible_playbook\", vars_dict[avar])\n\t\ndef _run_update(cli_args, vars_dict, ir_workspace, ir_plugin):\t\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\tif ir_workspace.__name__ == \"IrWorkspace\":\n\t\tansible_cli_args = vars_dict\n\t\tif cli_args and \"config\" in vars_dict:\n\t\t\ttry:\n\t\t\t\tansible_cli_args.update(cli_args[\"config\"])\n\t\t\texcept KeyError as e:\n\t\t\t\tprint(\"Error in variable: %s %s\" % (cli_args.keys(), e))\n\t\t\t\treturn {}\n\t\telse:\n\t\t\tprint(\"Error in variable: %s %s\" % (cli_args.keys(),"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\targs=cli_args\n\tvars_dict.update(vars_dict)\n\t\n\tif 'name' not in vars_dict or not vars_dict['name'] or vars_dict['name']=='' or not vars_dict['name']:\n\t\tlogger.warning('Incorrect name, setting to default name')\n\t\t\n\t# Create default input (input-type is not set yet!)\n\tif 'input' in inputs.keys():\n\t\tconfig = inputs[vars_dict['name']]\n\t\tinput = type(config)(\n\t\t\tvar.input,\n\t\t\t"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\tir = ir_plugin.client\n\tir = ir or ir_workspace.get_ir(cli_args.get('plugin_name'))\n\n\tif not ir:\n\t\tmsg = \"No ir found for plugin [\" + cli_args.get('plugin_name') + \"]\"\n\t\tlogger.error(msg)\n\t\tret = {'message': msg}\n\t\treturn ret\n\n\tif cli_args.get('plugin_name'):\n\t\tansible_run_cmd = cli_args.get('plugin_name')\n\t\tansible_list"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\tcli_args = []\t\t# add help\n\targs_dict = _find_extra_vars(vars_dict=vars_dict)\t# get cmd args\n\tres_dict = run_ansible_command( 'ansible api:playbook',  args_dict, vars_dict=vars_dict, cli_args=cli_args, ir_workspace=ir_workspace )\t# get ansible results\n\tfor res_val in res_dict.keys():\n\t\tres = res_dict[res_val]\n\t\n\tif len(res) == 0"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\tcmd = vars_dict['cmd']\n\tif cmd not in [cli_args.cli_cmd.join(' ') + '.' + \\\n\t\t\t\tcli_args.cli_cmd.rpartition('.')[1], \\\n\t\t\t\t] + ['playbook', 'init', 'load', cli_args.cli_args.split(' ') + ['init', 'run']] + ['config', 'create'] + IR_PATHS + ('' in cli_args.cli_args):\t# for python 3.6 compatibility\n\t\tlogger = vars_dict['logger']\n\t\tif"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\tcli_args['vars_dict'] = vars_dict\n\tansible_result = []\n\tansible_client = ansible.client('playbook')\n\tif ir_workspace:\n\t\tworkspace = ansible_client.load_workspace(ir_workspace.name)\n\t\tansible_result.append(ansible_client.RunWorkspace(workspace).result().value)\n\t\tansible_result.append(ansible_client.RunPlugin(ans_plugin.name).result().value)\n\treturn ansible_result\n\nif __name__ =="}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t# pylint: disable=broad-except\n\tansible_results = {  # pylint: disable=protected-access\n        \"run\": False,  # pylint: disable=invalid-name\n        \"vars\": vars_dict,\n        \"ansible_results\": {  # pylint: disable=invalid-name\n            \"output\": vars_dict,\n            \"variables\": vars_dict.copy().iloc"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\tres = {}\n\tresults = {}\n\ttry:\n\t\tres_list = list()\n\t\tif vars_dict.get(\"ansible_params\")!= {\"args\": \"\"}, vars_dict:\n\t\t\tfor k, v in vars_dict.items():\n\t\t\t\tres_list.append(res_dict(\n\t\t\t\t\t\t\tansible_vars_to_ansible(k, vars_dict),\n\t\t\t\t\t\t\tansible_plugin(k, vars_dict),\n\t\t\t\t\t\t\tvars_dict\n\t\t\t\t\t\t))\n\t\t\t\t\tres_dict = res_list.pop()"}]}
{"task_id": "63060ada73426c38ae68ad31", "prompt": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\treturn [arg for arg in values_dict if arg not in [None, \"False\", \"True\", \"None\"]]\n\ndef _parse_cli_command(value):\n\tif value in [\"true\", \"yes\", \"on\", \"1\", \"off\", \"0\"]:\n\t\treturn True\n\tif value in [\"false\", \"no\", \"off\", \"0\"]:\n\t\treturn False\n\tfor key, value in commands.parse_cli(value):\n\t\tif key == \"on\":\n\t\t\treturn value\n\treturn None\n\ndef _cli_cmd"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\targs_values = {}\n\tfor a in parser_name:\n\t\targs_values[a] = values_dict[a]\n\n\treturn args_values\n\nimport py\nfrom base.PyScript import PyScript\nfrom base import Class, Property\n\nfrom py_livethists.helper_basic import _check_list\nfrom base import property_map\n\n\nclass Method(object):\n\n    def get_args(self):\n        return property_map(self.get_kwargs)\n\n    def get"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tparser = parser_name\n\tif parser_name.startswith('get_version'):\n\t\tversions = self.versions_by_command()\n\t\tvalues_dict.update({parser: versions})\n\t\treturn values_dict, types.TypeDict(version_types=versions)\n\n\tif parser_name =='set':\n\t\tif values_dict.__has_key(parser_name):\n\t\t\tvalues_dict.pop(parser_name)\n\t\telse:\n\t\t\treturn values_dict, types.TypeDict({parser: values_dict})\n"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\targs = values_dict.get(\"args\") or parser_name.split(\",\")\n\tdict_of_ints = self.convert_args_to_dicts(dict_of_ints=[], args, values_dict)\n\tif len(args) > 0:\n\t\tfor arg in args:\n\t\t\tdict_of_ints.append((arg, None))\n\treturn dict_of_ints\n\ndef get_args(args_file, args_dict=None, parser_name=\"main\") -> Tuple[\n\t:list, str]:\n\t\"\"\"\n\t"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\targs_with_default_params = []\n\targs_with_default_value = []\n\n\tif args_with_default_params:\n\t\tfor k in args_with_default_params:\n\t\t\tif k not in values_dict:\n\t\t\t\targs_with_default_param ='' + k\t\t\tand\n\t\t\t\t\t\t\t\t\t\t\t\tvalues_dict[k]\n\t\t\t\targs_with_default_value = args_with_default_param\t: value_without_default\n\t\t\t\targs_with_default_params.append(args_with_default_"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t# TODO : This should always be moved to main cmd\n\tfor i in ['main', 'virsh', 'osx10']:\n\t\tvalues_dict.setdefault(parser_name + \"_{}\".format(i), \"None\")\n\n\tif values_dict is None:\n\t\tvalues_dict = {}\n\n\tfor i in [\"py\", \"os\", \"arch\", \"os_family\", \"py_version\", \"os_arch\"]:\n\t\ttry:\n\t\t\tvalues_dict.setdefault(parser_name + \"_{}\".format(i), \"None\")\n"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tfor key, value in values_dict.items():\n\t\tvals = parser_names.get(parser_name, [])\n\t\tvals.remove(key)\n\t\tif vals:\n\t\t\tvals.append(value)\n\n\ndef _set_all_opts_to_defaults(parser_name, values_dict):\n\t\"\"\"Set the parser, main, etc from the values\"\"\"\n\tfor key, value in values_dict.items():\n\t\tparser_names[parser_name].remove(key)\n\t\tparser_names[parser_name][key"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tassert len(self.args) == 2\n\t\tfor key in self.args:\n\t\t\tvalues_dict[key] = self.args[key]\n\t\treturn super(OSPDCommandObject, self).convert_non_cli_args(parser_name, values_dict)\n\n/*\n * Copyright 2016-2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n * \n * Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with\n * the License. A copy of the License is"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\twith self.parser.arg_context(parser_name):\n\t\treturn _convert_non_cli_args_with_values_dict(parser_name, values_dict)\n\n\treturn _convert_non_cli_args_no_values_dict(parser_name=parser_name, **self.args)\n\n\nclass _ParseAndValidateCommand(_ParseCommand), BaseCommand):\n\n\t\"\"\"\n\tCommand parser that converts valid json into an validated command.\n\n\tThe only supported commands are the main subcommand and virsh subcommand.\n\t\"\"\"\n\n"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\tif not values_dict:\n\t\tvalues_dict = dict()\n\t# Remove the 'command line' arguments\n\tparser_name = parser_name.split(':')[1].strip()\n\t# if there are no values return nothing\n\tif self.values_dict and len(self.values_dict) == 0:\n\t\treturn\n\n\t# check arguments and convert\n\tfor val_arg in values_dict:\n\t\tkey_value = value = str(val_arg,'replace')\n\t\tif 'command_line' not in values_dict:"}]}
{"task_id": "63060b1a73426c38ae68ad3e", "prompt": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tplugin_dir = plugin_dir.replace('\\\\', '/')\n\tplugin_spec = os.path.dirname(os.path.realpath(plugin_dir))\n\t# Get the plugin properties\n\tplugins_list=list()\n\tdir_iterator = os.scandir(plugin_spec)\t# List the available plugins in the plugin directory\n\tplugin_list = dir_iterator.name\n\tfor plugin_name in plugin_list:\n\t\tplugin_properties = list(plugin_dir.joinpath(plugin_name).rpartition('.py'))\n\t\tplugin_properties"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\treturn __plugin_spec_flatten_dict__(plugin_dir, \"__plugin\")\n\npackage org.batfish.datamodel.questions;\n\nimport static com.google.common.base.MoreObjects.firstNonNull;\nimport static org.batfish.datamodel.ast.Node.Type.BOOLEAN;\nimport static org.batfish.datamodel.matchers.AbstractMatchers.leaf;\n\nimport com.google.common.collect.ImmutableList;\nimport com.google.common.collect.ImmutableMap;\nimport com.google.common.collect.ImmutableSet;\nimport java."}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\treturn flatten_dict(plugin_dir)\n\n\ndef get_plugin_spec_flat_dict(plugin_dir):\t\"\"\"\n\tCreates a dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flat dict contains the plugin's properties\t\"\"\"\n\treturn flatten_dict(plugin_dir)\n\n\ndef flatten_dict(dict_):\t\"\"\"\n\tTakes a dict to flatten\n\n:param dict_: An dictionary to be flattened\n:return: An flattened dict\n\n:Example: \n|  {\n"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\treturn plugin_dir.list_dir(\"*.yaml\")\n\n\ndef get_config(config_file):     \n\t'''\n\tGet a dictionary of configuration variables defined in python, a dictionary (or a list)\n\tthat contains the values needed for the execution of this python executable. This dictionary\n\tshould correspond to the python_args in the plugin_spec that was returned during execution\n\t'''\n\treturn get_plugin_spec_flatten_dict(config_file)\n\n\ndef get_default_options(options_struct):\n\t'''\n\tDetermine the"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t# TODO: Move this into a shared module\n\tp = {}\n\tfor key in plugin_spec.keys():\n\t\tf = plugin_spec[key]\n\t\tif f is None:\n\t\t\tdel plugin_spec[key]\n\t\tp[key] = f\n\t\treturn p\n\n\ndef get_plugin_dir_name(plugin_dir):\n\t\"\"\"Creates a plugin dir name from the plugin dir\n\n\tIf the plugin dir exists, returns the name of a\n\tplugin directory with the basename of the plugin.\n\n\tIf the plugin dir"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tret = {}\n\tif \"name\" in plugin_dir:\n\t\tname = str(plugin_dir[\"name\"])\n\t\tret[\"name\"] = name\n\t\tret[\"id\"] = self.get_plugin_spec_item_id(plugin_dir)\n\n\treturn ret\n\n\t\ndef get_plugin_spec_item_id(plugin_dir):\t # nosec\n\tif \".plugins\" in plugin_dir:\n\t\treturn plugin_dir.split(\".\")[0]\n\treturn plugin_dir\n\nif __name__==\"__main__\":"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tspec_to_flatten={}\n\tfor p in get_plugin_spec(plugin_dir).get_plugin_spec():\n\t\t# if plugin_name is a valid value, the flatten of the plugin\n\t\t# spec is the spec of the plugin\n\t\tif p.name==\"plugin_name\":\n\t\t\tspec_to_flatten[p.spec.name]= p.spec\n\treturn spec_to_flatten\n\ndef get_plugin_spec_flatten(plugin_dir):\n\t\"\"\"\nIf a plugin dir is not a flat dict, returns it's plugin spec"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\treturn flatten_dict_from_yaml(plugin_spec=os.path.join(plugin_dir, 'plugins'))\n\n\ndef get_plugin_spec_string_json(plugin_dir):\n\t\"Gets the plugin properties' string from the given plugin dir\"\n\treturn flatten_string_from_yaml(plugin_spec=os.path.join(plugin_dir, 'plugins', 'yaml'))\n\ndef get_plugin_spec_string_dict(plugin_dir, flatten_dict=True):\n\t\"Gets the plugin properties' string from all files in the plugin"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tprint(\"Getting plugin spec properties %s...\" % plugin_dir)\t\t\t\t# type: ignore\n\tprint(\"Flattening plugin specs...\")\n\tspec = get_plugin_spec(filename=plugin_dir)\n\tflatten = flatten_dict(spec)\n\treturn flatten\n\ndef create_instance(parent): # pylint: disable=arguments-differ\n\t# parent is the parent of the plugin-spec\n\tparent = plugin_spec_flatten_dict(parent)\n\tprint(\"Creating instance of plugin class %s...\" % parent[\"name\"])\n\n\tprint(\""}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\tf = {}\n\tfor elem in plugin_dir.split(os.path.sep):\n\t\troot = os.path.join(plugin_dir, elem)\n\t\tfor name, value in os.environ.get(elem, {}).items():\n\t\t\tif value is None: continue\n\t\t\tf[root+'_' + name] = value\n\treturn f\t\t\n\t\n\t\n\t\ndef get_plugin_spec_dir(plugin_dir, plugin_name):\n\tfor plugin_name in PLUGIN_SPEC_DIRS:\n\t\tplugin_dir = os."}]}
{"task_id": "63060b1b73426c38ae68ad42", "prompt": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tenv_var = self.app.config_path.strip()\n\tif env_var is None:\n\t\tself.app.config_path = self._get_root_var()\n\t\n\t\"\"\"\n\tInitialize the application.\n\t\"\"\"\n\tself._InitWindow()\n\ndef get_version(self):\n\t\"\"\"\n\tReturn the current version number.\n\t\"\"\"\n\tver_code = self.app.version.split('\\x0A')[0]\n\tver_name = self._get_ver_name()\n\tv_code = int(ver"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tif \"__config__.py\" not in self:\n\t\tself.__config__ = self.config = config = {}\n\tfor x in itervalues(self.keys()):\n\t\ttry:\n\t\t\tif x.startswith(\"__\"):\n\t\t\t\tx = x.replace(\"__\", \"\")\n\t\texcept NameError:\n\t\t\tpass\n\t\tif not istracecompatible(self[x]):\n\t\t\tcontinue\n\t\ttry:\n\t\t\tkey = x.split(\".\")\n\t\t\tif key[0] in self and key[0] not in config:\n\t\t\t\t"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tif self.env_var_path:\n\t\tself.config = os.environ[self.env_var_path]\n\t\tif not self.config:\n\t\t\tprint(\"No configuration value found in environment variable: %s.\" % self.env_var_path)\n\n\n\tdef inject_dir(self, dir_path):\n\t\tfor item in os.listdir(dir_path):\n\t\t\tif os.path.splitext(item)[-1] == \"py\":\n\t\t\t\tself.path_parts.append(\"..\")\n\t\t"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t# Make sure we are in an environment variable\n\t#\tenvpath = ENVROOT / \"APP_CONFIG\"\n\n\tif os.environ.get(\"ENVPATH\"):\n\t\tenvpath = os.environ.get(\"ENVPATH\")\n\telse:\n\t\tenvpath = \"\"\n\n\tif envpath:\n\t\tself.envpath = envpath\n\t\t#\tNOTE:  it probably seems that the envvar is\n\t\t#\tsetted before being saved, but this should\n\t\t#\tnever happen (but when it is just a kludge)\n"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tn = getenv('N')\t\t# n is currently in env.\n\tif n:\n\t\tconfig_path = '%s/' % n.rstrip('/')\n\telse:\n\t\t# Fallback\n\t\tconfig_path = '%s'\n\n\tif not self.config_path:\n\t\ttry:\n\t\t\tif self.config_file:\n\t\t\t\tself.config_path = os.path.abspath(self.config_file)\n\t\t\telse:\n\t\t\t\tself.config_path = None\n\t\t\tself.config_file = n"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tif os.path.isdir(\"/etc/apt/init.d\"):\n\t\tself.apt_dir = \"/etc/apt/init.d\"\nelif os.path.isdir(\"/etc/init.d\"):\n\tself.init_dir = \"/etc/init.d\"\nelse:\n\tself.apt_dir = \"/etc\"\n\tself.init_dir = \"/etc/init.d\"\n\nclass TestRunStartFailNoLogOutput(TestRunStartFail)      :\n\tdef _run_start(self, *args)"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tif (self.configPath is None and self.configDir is None):\n\t\traise ValueError(\"You must pass a configuration directory\")\n\tif (not self.configDir or not self.configPath):\n\t\traise ValueError(\"You cannot specify a directory, if there is a configuration file\")\n\tif (self.configDir!= self.configPath):\n\t\tself.configPath = self.configDir\n\t\tself.configDir = None\n\tif (len(self.configPath) == len(self.filename)):\n\t\traise ValueError(\"'%s' path contains"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tif os.environ.get('LOCAL_CONFIG_PATH'):\n\t\tsys.path.append(os.environ['LOCAL_CONFIG_PATH'])\n\t\n\tos.environ['LOCAL_CONF_PATH'] = env_path = os.environ.get('LOCAL_CONF_PATH')\n\tif env_path:\n\t\tfs.makedirs(env_path, exist_ok=True)\n\treturn\t\n\n# def inject_env_vars(self):\n#\n#\t# env and files variables\n#\t# this is needed (i.e. we have to"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tif self.config is None:\t\t\t\t# if not present\n\t\tif self.config_path:\t\t\t\t# found\n\t\t\tfor d in os.listdir(self.config_path):\n\t\t\t\tpart_name = os.path.join(self.config_path,d,self.config_name)\n\t\t\t\tif part_name == '':\n\t\t\t\t\tcontinue\n\t\t\t\telse:\n\t\t\t\t\tif self.config[part_name] is None and \\\n\t\t\t\t\t\t(part_name in self.config_path) and part_name!= '':\n"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\tif not hasattr(self.settings, 'APPSEC_ENV'):\t# pylint: disable=undefined-variable\n\t\tself.settings.APPSEC_ENV = 'APPSEC_ENV'\n\t\t\nclass Crawl_config:\t\t# pylint: disable=too-few-public-methods\n\t\"\"\"\n\tThis class is used to set the configuration to a particular path to read from config file\n\tIt's for compatibility to 0.23 and 0.24\n\t\t\n\tReturns:\n\t\tTrue:\t\tSuccess\n\t\tFalse:\t\tError\n\t\t\t\t\n\t\"\"\"\n\t"}]}
{"task_id": "63060b1b73426c38ae68ad43", "prompt": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tsubparser = subparser\n\tself.add_subparser(spec_cli_name, subparser)\n\tsubparser['entry-point'] = self  # set up the initial parser\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\treturn root_subparsers.CliDef(args=self.__args__,\n\t\tparser_class=subparser_cli_options.cli_class,\n\t\tapp_factory=subparser_cli_options.app_factory)\n\n/***************************************************************************//**\n *   @file   LwIP/IP6_ICMP6_PktIf.h\n *   @brief  IPv6 ICMP6 packet offload header header.\n *   @author Mihail Chinaein, (C) 2019\n********************************************************************************\n * Copyright 2019 Mihail Chinaein,"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tsubparser.apply_cli_opts(['-a', '--help'])\n\t# Add to main_cmd.  Main command.\n\t# main_cmd.add_subparsers(root_subparsers)\n\n\ndef setup_subparser(prog, kwargs):\n\tif kwargs.pop('configure_cli_args', False) and os.path.isfile(PROG_DOT_FILENAME_FILE):\n\t\tsubprocess.check_call(prog, shell=True, stdin=subprocess.PIPE, stderr=subprocess.STDOUT)\n\telse:\n\t\tsubprocess.check_"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tdef _extend_options(subparsers, cli_options):\n\t\t\"\"\"\n\t\tAdds the subparser options to the cli options.\n\t\t\"\"\"\n\t\tcl_option = get_cli_option(cli_options, 'cli')\n\t\tsubparser = subparsers[\"cli\"]\n\t\tsubparser.register_option(\"--debug\", action=\"store_true\",\n\t\t\t\t\tdest=\"debug\", metavar=cli_options['debug'],\n\t\t\t\t\thelp=\"Show extra output for debugging. It is more useful to provide a 'debug' argument\")\n\t\tsubparser."}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tfor key in (\n\t\t'no_options','verbose','verbose_only', 'help','version',\n\t\t'help_file','verbose_file', 'help2_file',\n\t\t'no_plugins', 'no_output_stream',\n\t):\n\t\tsubparser = None\n\t\tif (subparser is None) and key in ('default', 'ignore', 'help','version'):\n\t\t\ttry:\n\t\t\t\toptions = self.cli.opts[key+'.opts']\n\t\t\texcept IndexError:\n\t\t\t# if"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tif not isinstance(root_subparsers, tuple):\t\t# type: ignore\n\t\troot_subparsers = (root_subparsers,)\n\n\tfor p in self.opts:\n\t\tp.add_subparser(subparser)\n\n\n\t# set the parser's input\n\tif not isinstance(root_subparsers, tuple):\n\t\troot_subparsers = (root_subparsers,)\n\ttry:\n\t\tself.parse_cli_args(root_subparsers)\n\texcept BaseException as exc:\n\t\tself.write_error("}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tsubparser.parse_args(\n\t\t(\n\t\t\t\"spec=cli\",\n\t\t\t\"spec [--show] [--color|-w]\\n\\\n             *spec [spec...] -h --help               \",\n\t\t)\n\t)\n\troot_subparsers = root_subparsers + list(subparser.subparsers)\n\n\tsubparsers = [self.subparsers[x] for x in root_subparsers]\n\tsubparsers."}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tsubparser.options = [\n\t\t(\n\t\t\t'tracer',\n\t\t\tmetav1.TYPE_STRING,\n\t\t\tmetav1.TYPE_BOOL,\n\t\t\tmetav1.TYPE_STRING,\n\t\t\thelp=\"Whether to add the --tracer flag to the --help output\"\n\t\t)\n\t]\n\n\troot_subparsers.add_parser(\n\t\t'tracer',\n\t\thelp='adds a --tracer flag to the --help output',\n\t\tmetav1.TYPE_BOOL,\n\t\tdest=\"tracer\",\n\t)\n"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tdef extend_cli(subparser):\t\t\tdef extend_cli(self, subparser):\t\tpass\n\n\textend_cli.__doc__ = argparse.__doc__\n\textend_cli.__name__ = argparse.__name__\n\n\troot_subparsers.extend(subparser)\n\treturn root_subparsers\n\ndef create_parser(fname='python.py', version=None, usage='%(prog)s version %(prog)s',\n                  description='python CLI for the %env variable',\n"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\tname = 'cli'\n\tprog_name = name + '_cli'\n\tif root_subparsers is None:\n\t\troot_subparsers = Subparsers()\n\tsubparsers = root_subparsers.add_parser('cli', help='Help')\n\t# Add subparser for cli options\n\n\tsubparsers.add_parser('runfile', help='Runfile to run')\n\t# Add subparser to runfile when the program is run\n\tsubparsers.add_parser('runfile', help='Runfile to run')\n\t# Add subparser to runfile"}]}
{"task_id": "62ece4982e6aefcf4aabbd64", "prompt": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\ttry:\n\t\tos.mkdir(root)\n\texcept OSError:\n\t\tpass\n\n\ndef split_at(start, end, split_type):\n\tfrom cStringIO import StringIO\n\ttemp = end - start\n\tlines = StringIO()\n\twith stop_lines() as cur:\n\t\tlines.write(cur)\n\t\tfor line in temp:\n\t\t\tlines.write(line)\n\t# If the lines in \"line\" have been added at the beginning\n\tif start == -1:\n\t\treturn lines.getvalue()"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tif not root:\n\t\traise ValueError(r\"root path must be specified\")\n\tif not path:\n\t\traise ValueError(r\"path must be specified\")\n\n\ti = 0\n\twhile i < len(path):\n\t\tif not root[i:i+1]:\n\t\t\twhile 1:\n\t\t\t\tif i+2 == len(path):\n\t\t\t\t\treturn path\n\t\t\t\t\ti += 1\n\t\t\t\t\tcontinue\n\t\t\t\tif ((path[i] == root[i+1]\n\t\t\t\t\tand path[i+2]\n\t\t\t\t\t=="}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tif not root and not os.path.exists(path):\n\t\traise Exception(\"File has not been found: %s. If this happens, report a bug.\" % path)\n\treturn path\n\ndef get_user_home():\n\t# Get user home\n\tuser_home = os.getenv('HOME')\n\tif user_home is None:\n\t\t# Get the home of user if not available\n\t\thome = os.getpwuid()\n\t\tif home:\n\t\t\tuser_home = home[0]\n\t\t\tif os.path.split"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\treturn (path, root) if root is not \".\" else (path, root)\n\ndef strip_root_dot(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\treturn (root, path) if root is not \".\" else (path, root)\n\ndef strip_path(path, root):\t\"\"\"\n\tRemove path from \"root\".\t\"\"\"\n\treturn (root, path) if root is not \".\" else (path, root)\n\n_log = {}\n\n\ndef _log(message, log=None):\n\t"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tif not os.path.exists(path):\n\t\treturn None, -1\n\treturn os.path.basename(root), \"root\"\n\n\ndef _uncompress_data(uncompressed_data):\n\tif len(uncompressed_data) < 8:\n\t\treturn \"\"\n\tnew_data = \"\"\n\tfor i in range(0, len(uncompressed_data), 8):\n\t\tnew_data += uncompress(unhexlify(uncompressed_data[i:i+8]))+\"\\n\"\n\treturn new_data\n\n\ndef _hash"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\troot = root.replace(c_dir, '', 1)\t# no leading 0\n\tif not os.path.isdir(root):\n\t\traise Exception('Path of %s not found.' % root)\n\telif os.path.exists(root):\n\t\ttry:\n\t\t\tos.remove(root)\n\texcept Exception:\n\t\tpass\n\ndef rstrip(text):\t# returns stripped string\n\treturn text.replace('\\n','')\n\ndef split(parts):\n\t\"\"\" Given iterable returning tuple (name, path) \"\"\"\n\treturn"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tfor s in path.split(os.pathsep):\n\t\tif s.startswith(\"/\"):\n\t\t\troot = os.path.join(root, s)\t\n\n\tif root in root_path:\n\t\troot_path.remove(root)\n\nroot_path.append(root)\n\nfor root, dirs, subdirs, files in os.walk(root):\n\tfor child in dirs:\n\t\tstrip_root(child, root)\n\nfor child in files:\n\tos.remove(child)\n\n/*\n * Copyright 2009-2017 Alibaba"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tfrom os.sep import sep\n\tfrom string import ascii_lowercase\n\tparts = [ascii_lowercase(x) for x in path.split(sep)]\n\tif len(parts)==len(parts)-1:\t# path does not have root\n\t\tpath = [x.strip() for x in path.split(sep)]\n\texcept (IOError, ValueError):\t\t\t# path has no root (or is not a regular file)\n\t\treturn False\n\ttry: \n\t\treturn path[0] in root or path[-1]!= '/'\n\texcept IndexError"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t# The following function is useful if you want to modify\n\t# the returned path.\n\t# We need the absolute directory path, so it can be\n\t# converted to a relative directory path by removing\n\t# root first.\n\troot = normalize_path(root)\n\tif not os.path.exists(root):\n\t\traise os.error.PathError(\n\t\t    \"The current working directory ({}) is not a valid \"\n\t\t    \"path.  Please check if the path exists and is \"\n\t\t    \"accessible.  Please make sure there is"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\tif not path:\n\t\traise Exception('root cannot be empty')\n\tif not root:\n\t\troot_id = 0\n\telif len(root) == 1:\n\t\troot_id = root[0]\n\telse:\n\t\troot_id = root[::-1]   # assume root is string\n\n\told_root = None\n\tif path in root:\t # remove root from root path\n\t\tif old_root is not None: old_root = old_root.replace(root_id, '')\t# remove last \"/\" at"}]}
