{"task_id": "62e60f43d76274f8a4026e28", "prompt": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif nanosecond < 0:\n\t\tnanoseconds -= 10000000\n\telse:nanoseconds = 1000000000\n\treturn Time.from_timestamp(nanoseconds=nanoseconds, tz=time.tzset[tz])\n\n\ndef days_ago(timestamp):\n\t\"\"\"\n\t@params timestamp: Time for in seconds since epoch (i.e., local time)\n\t@return: Days elapsed since time since epoch (unix timestamp).\n\t\"\"\"\n\t# In Python 3 we may do year-based things differently here, but some older versions, like 3.19.0, did not"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tvalue = localtime(nanoseconds)\n\tif tz is not None:\n\t\tvalue_tz = (value.tm_sec, value.tm_min, value.tm_hour, value.tm_mday, value.tm_mon, value.tm_year)\n\t\tvalue_tz.replace(tzinfo=timezone.utc)\n\t\tvalue = value_tz\n\treturn value\n\n\ndef localtime_from_microseconds(seconds, tz):\n\t\"\"\"\n\tConvert a `Time` representing a micro-seconds from local date time to local time.\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\treturn Time(datetime.time(nanoseconds), tz)\n\n\ndef to_microseconds(time):\n\t\"\"\"\n\t\tConverts a time into a nanosecond offset, possibly adjusting it in\n\t\tplace (for fractional seconds).\n\t\n\t\t:param time: time in seconds\n\t\t:return:\n\t\"\"\"\n\tif isinstance(time, timedelta):\n\t\ttime = time_to_time_precision(time, microsecond=0)\n\treturn time\n\n\ndef to_microseconds(time):\n\t\"\"\"Converts a time into a nan"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif _has_tz():\n\t\treturn Time(*nanoseconds, tz)\n\t\n\tif _has_tz(\"America/Cayenne\"):\n\t\treturn LocalTime(*nanoseconds)\n\t\n\tif _has_tz(\"Eastern\"):\n\t\treturn Time(*nanoseconds)\n\n\tif _has_tz(\"Europe/London\"):\n\t\treturn LocalTime(*nanoseconds)\n\t\n\treturn Time(*nanoseconds)\n\t\n\n#ifndef ALIB_IO_FILE_H\n#define ALIB_IO_FILE_H\n\n#include \"astring"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tself._check_input(nanoseconds)\n\treturn self.seconds, self.microseconds, Nanoseconds(nanoseconds)\n\n/*\n    ChibiOS - Copyright (C) 2006..2018 Giovanni Di Sirio\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n        http://www.apache.org/licenses/LICENSE-2.0\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif isinstance(nanoseconds, float):\n\t\tnanoseconds *= 1e-9 if tz is None else (1 / float(nanoseconds)) * -1\n\tif not isinstance(nanoseconds, base_units.nanosecond_t):\n\t\traise ValueError(\"nanoseconds must be an integer nanosecond.\")\n\t\n\thours = datetime.timedelta(seconds=nanoseconds, nanoseconds=nanoseconds // 3600,\n\t\tmicroseconds=nanoseconds // 3600 * 10000)\n\tminutes = datetime.timedelta(seconds=nanoseconds, microseconds=nanoseconds\n\t\t// microsecond // 10000"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif not isinstance(nanoseconds, int) or not _is_nanosecond(nanoseconds):\n\t\traise TypeError(\"nanoseconds must be specified for `time.time` and `time.timez` instances\")\n\tif tz is None and not isinstance(tz, time.tzlocal):\n\t\traise TypeError(\"tz must not be specified for `time.tz` instances\")\n\tif tz is not None:\n\t\tif _is_time_valid(nanoseconds):\n\t\t\ttz = tz # Force no tz\n\t\t\tnanoseconds = 0\n\t# else, default to"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif timezone == 'utc' and timezone.is_naive:\n\t\tnanoseconds = timedelta(None, zone=timezone)\n\treturn nanos_to_unix_time(nanoseconds, tz)\n\n\nclass TimeFromTimestamp(object):\n\t\"\"\"Convert a `Timestamp` to a `Time` (where timestamp = timestamp in Unix).\"\"\"\n\tdef __init__(self, timestamp, tz=None):\n\t\tself.timestamp = timestamp\n\t\tself.timestamp_time = timestamp.timestamp_time\n\t\tself.timestamp_tz = timestamp.timestamp_tz"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t# Create a new instance with tz set if there is one\n\tif tz is None:\n\t\ttz = Time(\"N/A\", timezone.utc)\n\t\t# Make sure we actually use the time format we're using\n\t\tnano = nanoseconds\n\telse:\n\t\tnano = (seconds(nanos) * 1e9) + nanoseconds\n\n\t# Convert that to seconds (if timezone has not exist or does not have\n\t# a time format\n\tif tz is not None:\n\t\tseconds = Time(tz, timezone.utc)\n\telse:\n\t"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\ttime_class = Time\n\tif tz is None:\n\t\tif not isinstance(nanoseconds, Number):\n\t\t\tnanoseconds = (nanoseconds, 'nanoseconds');\n\t\ttz = tz or tz.tzname\n\treturn time_class(nanoseconds, tz)\n\n################################################################################\n#  time manipulation methods \n################################################################################\ndef get_epoch(nanoseconds, tz=None):\n\tstart_epoch = Time.now()\n\tstart_epoch = int(start_epoch.timestamp / (1e6 or 1000000))\n\tend_epoch = start_epoch"}]}
{"task_id": "62e60f3bd76274f8a4026e10", "prompt": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\nclass date_type(object):\n\t\"\"\"\n\tProvides a `type` for date-like values.\n\t\n\t:param timedelta:\n\t:type timedelta: class representing timedelta, with a duration, and unit\n\t:type format: optional formatting function (type to use as string)\n\t:param datetime:\n\t:type datetime: datetime object, for datetime-fields (type to use as string)\n\t:param date: optional date-like object\n\t\"\"\"\n\tTYPE = (None,None)\n\tDATETIME = (datetime.now(), datetime.utc"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tvalue_obj = datetime.utcfromtimestamp(value.value)\n\tvalue = datetime.strptime(value_obj, '%b %d %Y')\n\treturn value.microseconds\n\n\n@deprecated\ndef iso_string(value):\n\t\"\"\"\n\tDecodes ISO-8601 strings.\n\t\n\t:param value:\n\t:return:\n\t\"\"\"\n\treturn value.isoformat()\n\n\ndef month_number(value):\n\t\"\"\"\n\tConverts the month for use in the database time zone.\n\t\n\t:param value:\n\t:return"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tif value is None:\n\t\treturn None\n\tif isinstance(value, (float, int, slice)):\n\t\tif not any(time.time() <= value < time.time() + timedelta(microseconds=1)):\n\t\t\treturn ValueError(\"timedelta must be between 1 and 1\")\n\t\telse:\n\t\t\tvalue = timedelta(microseconds=1)\n\telif isinstance(value, (datetime.time, datetime.datetime)):\n\t\t# timedelta needs to be returned.\n\t\t# This check is necessary to avoid calling isinstance when the default\n\t"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tvalue = timedelta.timedelta(value)\n\treturn value\n\n\ndef dehydrate_interval_number(value):\n\t\"\"\"\n\tDehydrator for interval number. \n\t\n\t:param value:\n\t:type value: int\n\t:return:\n\t\"\"\"\n\treturn int(value)\n\n\ndef dehydrate_date(value):\n\t\"\"\"\n\tDehydrator for date.\n\t\n\t:param value:\n\t:type value: datetime.datetime or None\n\t:return:\n\t\"\"\"\n\tif isinstance(value, datetime"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tif not isinstance(value, timedelta):\n\t\treturn value\n\t\n\treturn dehydrate(value.total_milliseconds(), dehydrator_units.hours, dehydrator_units.days, 'timedelta',\n\t\t\t(int(value), 'hours', ()))\n\n\ndef dehydrate(value, units, days, date_attr, unit_str_attrs):\n\t\"\"\"\n\tDethydrates an object based on a :py:func:`datetime.timedelta`. Inspired by the :py:func:`timedelta`\n\tclass,"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tif isinstance(value, float):\n\t\treturn TimedeltaTime(value)\n\telse:\n\t\treturn Timedelta(value)\n\n\nclass Value(Enum):\n\t\"\"\"\n\tClass for all the available values.\n\t\n\t:param default:\n\t:type default: Value\n\t\"\"\"\n\tdef to_key(cls):\n\t\t\"\"\"\n\t\tKey conversion method from Value.\n\t\t\n\t\t:return:\n\t\t\"\"\"\n\t\treturn cls.to_key()\n\t\t\n\tdef from_value(value):\n\t\t\"\"\"\n\t\tValue"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tif hasattr(value, \"val\") and value.timedelta!= -1:\n\t\tresult = value\n\telse:\n\t\tresult = None\n\treturn result\n\n\ndef get_field_values(struct):\n\treturn struct.get_value(struct.data, \"field_fields\", None)\n\n\ndef get_field_id_name(struct):\n\treturn str(struct.get_value(struct.data, \"field\", None))\n\n\ndef get_field_name(struct):\n\treturn str(struct.get_value(struct."}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tif not isinstance(value, datetime.timedelta):\n\t\traise TypeError('Value must be in the format `timedelta`).')\n\tvalue = abs_timedelta(value)\n\treturn value\n\ntimedelta_min = dehydrate_timedelta(sys.maxsize + timedelta(0,'seconds'))\ntimedelta_max = dehydrate_timedelta(sys.maxsize)\n\n\n@timedelta_decorator\ndef timedelta_to_datetime(value):\n\t\"\"\"\n\tDehydrator to convert a timedelta value to a ``datetime.timedelta``."}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn int(math.floor(value))\n\n\ndef to_timedelta(value):\n\t\"\"\"\n\tConverts `timedelta` to its string equivalent.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn float(int(value))\n\n\ndef normalize_timedelta(value):\n\t\"\"\"\n\tNormalizes `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn float(to_timedelta(value.as"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tif isinstance(value, _timedelta_list):\n\t\tfor t in xrange(*value._ts):\n\t\t\tyield from cls._build_delta_time_converter(t)\n\telse:\n\t\traise ValueError(\"Can only dehydrate timedeltas or timedeltas in `timedelta` values.\")\n\n\ndef _get_utc_offset(dt):\n\t\"\"\"\n\tReturn UTC offset relative to UTC.\n\n\t:param dt: datetime object\n\t:return: utc offset\n\t\"\"\"\n\tutc_offset = dt.replace(tzinfo=utc_timezone"}]}
{"task_id": "62e60f37d76274f8a4026dfd", "prompt": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tdef dehydrate_time(value):\n\t\timport time\n\t\treturn time.time(value)\n\treturn dehydrate_time\n\n\nclass Timestamp(object):\n\tdef __init__(\n\t\t\tself, ts_format = 'Y-m',\n\t\t\tts_format_options = None,\n\t\t\tts_format_options = None):\n\t\t\"\"\"\n\t\t\t:param ts_format:\n\t\t\t\tstring: the format of the time series\n\t\t\t\tbool:\n\t\t\t\toptions:\n\t\t\t\t\t--ts_format,\n\t\t\t\t\t--"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tif isinstance(value, datetime.timedelta):\n\t\treturn int(value.total_seconds()), int(value.seconds())\n\treturn int(value), int(value)\n\n\ndef dehydrate_date(value):\n\t\"\"\"\n\tDehydrator for `date` values.\n\t\n\t:param value:\n\t:type value: Date\n\t:return:\n\t\"\"\"\n\tif isinstance(value, datetime.date):\n\t\treturn int(value.year), int(value.month), int(value.day), int(value.hour), int"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn float(value.to_iso_string()) / 1e9\n\t\n\n# Copyright (c) 2015-2017, Vicente J. Botet Escriba\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without modification, are\n# permitted provided that the following conditions are met:\n#\n#     * Redistributions of source code must retain the above copyright notice, this list of\n#       conditions and the following disclaimer.\n#     * Redistributions in binary form must reproduce the above copyright notice,"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn value\n\n/*\n* Kendo UI v2015.2.805 (http://www.telerik.com/kendo-ui)\n* Copyright 2015 Telerik AD. All rights reserved.\n*\n* Kendo UI commercial licenses may be obtained at\n* http://www.telerik.com/purchase/license-agreement/kendo-ui-complete\n* If you do not own a commercial license, this file shall be governed by the trial license terms.\n*/\n!function(e,define){define([\"./kendo.data.min\",\"./kendo.view.min"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn dehydrate_time(int(value), str(value))\n\ndef get_time(value, default=None):\n\t\"\"\"Returns the `time` value without time zone abbreviation.\n\t\"\"\"\n\tif default is None:\n\t\treturn int(value) if IS_STRING(value) else int(value)\n\t\n\treturn dehydrate_time(int(value), str(value))\n\t\n\ndef format_time(value, _now=os.ptime):\n\t\"\"\"Returns a `long` representing the `time` value given the\n\tcurrent"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tif isinstance(value, Time):\n\t\treturn value.tz_cache[0]\n\treturn value\n\n\ndef dehydrate_date_time(value):\n\"\"\"\n\tDehydrator for `date` and `time` values.\n\t\n\t:param value:\n\t:type value: Date\n\t:return:\n\t\"\"\"\n\tif isinstance(value, Date):\n\t\treturn value.loc\n\tif isinstance(value, Time):\n\t\treturn value.tz\n\treturn value\n\n\ndef dehydrate_datetime(value):\n\"\"\"\n"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tdef dehydrate_time(value):\n\t\treturn value\n\t\n#\tdef register_to_db(cls):\n#\t\tif cls.instance_type == \"time_\":\n#\t\t\tcls.db.append(cls)\n\n\n\n##### Class definition ##########################\n\n\nclass Person(object):\n\t__schema__ = {'__tablename__':'people'}\n\t\n\tid  = Column(Integer, ForeignKey('people.person_id'), primary_key=True)\n\tname  = Column(String(20), nullable=False)"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn value.timestamp\n\n\n#==============================================================================\n# `time_micro` values\n#==============================================================================\nclass Time(TypeInstance):\n\tdef __init__(self):\n\t\tself.time_of_death_type_id = 0\n\t\ttime.date.get_timezone(time.localtime())\n\t\t\n\tdef dehydrate_time(value):\n\t\t\"\"\"Dehydrate time from string value.\n\t\t\"\"\"\n\t\treturn time.mktime(year=value, mon=value, day=value, hour=value, min=value,\n\t\t"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tfrom.BaseDataResolver import DataResolver\n\tresolver = DataResolver(timezone=timezone)\n\tresolver.resolver_value(Time.datetime())\n\treturn resolver.resolve()\n\ndef dehydrate_date(value):\n\t\"\"\"\n\tDehydrator for `datetime` values.\n\t\n\t:param value:\n\t:type value: DateTime\n\t:return:\n\t\"\"\"\n\tfrom.BaseDataResolver import DataResolver\n\tresolver = DataResolver(timezone=timezone)\n\tresolver.resolver_value(DateTime.datetime(2013, 1, 1))\n\treturn"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn timezone.localtime(value.time).replace(tzinfo=pytz.utc)\n\n\ndef isDateOrTime(value):\n\t\"\"\"\n\tDetermine whether an object is a date or date/time.\n\t\n\t:param value:\n\t:returns:\n\t\"\"\"\n\treturn isinstance(value, datetime.date) and \\\n\t\tisinstance(value, datetime.time)\n\n\ndef isDateTimeOrTime(value):\n\t\"\"\"\n\tDetermine whether an object is a date or datetime.\n\t\n\t:param value:\n\t:returns:\n"}]}
{"task_id": "62e60f33d76274f8a4026de9", "prompt": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn [Point(x,y,z) for x in [x.x,x.y,x.z][::-1]] + [y.x,y.y,y.z][::-1]\n\ndef reverse_angles(dangles):\n\t\"\"\"\n\tReverse angles computation.\n\t\n\t:param dangles:\n\t:type dangles: list(tuple(float))\n\t:return:\n\t\"\"\"\n\treturn [x / y for x in [0.0]*len(dangles), x / y for y in [-1.0, 0"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\tif not isinstance(value, Point):\n\t\terror_msg = \"value type is incorrect, must be \" + str(type(value))\n\t\traise ValueError(error_msg)\n\treturn value\n\n<?php\nnamespace FedEx\\OpenShipService\\SimpleType;\n\nuse FedEx\\AbstractSimpleType;\n\n/**\n * Specifies a value\n *\n * @author      Jeremy Dunn <jeremy@jsdunn.info>\n * @package     PHP FedEx API wrapper\n * @subpackage  OpenShip Service\n"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn value\n\ndef dehydrate_string(value):\n\t\"\"\"\n\tDehydrator for String data.\n\t\n\t:param value:\n\t:type value: String\n\t:return:\n\t\"\"\"\n\treturn value\n\ndef dehydrate_dictionary(value):\n\t\"\"\"\n\tDehydrator for Dictionary data.\n\t\n\t:param value:\n\t:type value: Dict\n\t:return:\n\t\"\"\"\n\tname, value = dehydrate_key_value(value)\n\tif not value:\n\t\traise ValueError(\n\t"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\tif len(value) == 3:\n\t\tvalue.x, value.y, value.z = get_pt_list(value)\n\telse:\n\t\tvalue = get_pt_list(value)\n\t\n\treturn tuple(value)\n\nimport React, { Component } from 'antd';\nimport RouterLink from 'vue-router-link';\n\nexport default class App extends Component {\n    constructor() {\n        super();\n\n        const router = RouterLink.build(\n   "}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn value, None\n\ndef dehydrate_line(value):\n\t\"\"\"\n\tDehydrator for Line data.\n\t\n\t:param value:\n\t:type value: Line\n\t:return:\n\t\"\"\"\n\treturn value, None\n\ndef to_json(value):\n\t\"\"\"\n\tConvert value into JSON notation.\n\t\n\t:param value:\n\t:type value: List\n\t\"\"\"\n\treturn json.dumps(value, separators=(',', ':'))\n\ndef to_file(file_path, mode='wb'):\n\t\"\"\"\n\t"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\tval = Point()\n\tpoint_dict = Point.from_dict(val)\n\treturn Point.to_dict(point_dict)\n\n\n\nclass CityLine(GenericDataPointMixin, GenericDataPoint):\n\t\"\"\"\n\t:param value:\n\t:type value: int\n\t\"\"\"\n\tdef __init__(self, longitude=90, latitude=70,\n\t\t     radius=1, line_id='',\n\t\t     description=None,\n\t\t     point_list=None):\n\t\tGenericDataPoint.__init__(self, \""}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\traise Exception()\n\t\n\t\"\"\"\n\tdehydrate_point = dehydrate_point\n\t\"\"\"\n\tDehydrators for Point data.\n\t\n\t- value: Point\n\t\"\"\"\n\traise Exception()\n\n/* -*- coding: utf-8 -*\n * Copyright (c) 2018 NVT Inc.\n *\n * This software is provided 'as-is', without any express or implied\n * warranty. In no event will the authors be held liable for any damages\n * arising from the use of this software.\n *\n * Permission is granted to anyone to use this software for"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from types import TracebackType as _TracebackType\n\tpass#IMPORTIMPORTIMPORT from egrr_helpers import read_image,write_image\n\tpass#IMPORTIMPORTIMPORT from operator import itemgetter\n\tpass#IMPORTIMPORTIMPORT from datetime import datetime\n\t\n\ttry:\t\n\t\t#get the image object\n\t\tif read_image(value.Image):\t\t\t\n\t\t\treturn value\n\t\tpass#IMPORTIMPORTIMPORT from scipy.stats import multivariate_normal\n\t\tpass#IMP"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\tdata = {\n\t\t\"x\":value.x,\n\t\t\"y\":value.y,\n\t}\n\treturn data\n\ndef translate(value):\n\t\"\"\"\n\tRenders new point.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\tif not isinstance(value, (int, float)):\n\t\traise ValueError(value)\n\n\tout = dict(out=str(value), points=dehydrate_point(value))\n\treturn out\n\ndef to_angle_unit(value):"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\tif not isinstance(value, Point):\n\t\traise TypeError( \\\n\t\t\t\"dehydrate_point argument must be a Point type\" \\\n\t\t)\t\n\tif value.latitude is not None:\n\t\tpoint = Location(\n\t\t\tlatitude=float(value.latitude),\n\t\t\tlongitude=float(value.longitude),\n\t\t)\n\t\tif np.allclose(point.center(), value.center()):\n\t\t\treturn\n\t\traise ValueError( \\\n\t\t\t\"dehydrate_point: latitude, longitude must be the same as the\" \\\n"}]}
{"task_id": "62e60ed4d76274f8a4026da0", "prompt": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\tfrom skein.models import TableModel\n\treturn list(tablemodel.keys(self))\n\ndef to_dict(d):\n\t\"\"\"\n\tConvert a skein dictionary back to a dictionary.\n\t\"\"\"\n\treturn d\n\nusing System;\nusing System.Collections.Generic;\nusing Rubberduck.VBEditor.SafeComWrappers.Abstract;\nusing Rubberduck.VBEditor.SafeComWrappers.Abstract.Collections;\n\nnamespace Rubberduck.VBEditor.SafeComWrappers.ProjectWizard\n{\n    public class ProjectWizardProject : Project"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\tif not self._record:\n\t\treturn {}\n\trecord = self.get()\n\treturn getattr(record.keys(), \"keys\", [])\n\ndef records(self):\n\tif not self._record:\n\t\treturn []\n\trecord = self.get()\n\treturn [r for r in record.items() for key in r]\n\n# This file was generated by Appraisal\n\nsource \"https://rubygems.org\"\n\ngem \"rails\", \"~> 6.1.0\"\n\ngemspec path: \"../\"\n\n#region License"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self.d.keys()\n\n\n\ndef __str__(self):\n\treturn self.d.get_repr()\n\n\ndef format_size(value, units=None):\n\tlength = 0\n\tif units is None:\n\t\t# for backward compatible\n\t\tunits = 'B'\n\treturn (\"%.1f %s\"\n\t\t   % (float(value), units))\n\n/*! \\file nni_ip_esp.c\n *\n * API to manipulate E2E stack packets and return the\n * appropriate size for the stack\n *\n"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self.get_keys()\n\n\nclass Karaoke(Arango):\n\t\"\"\"\n\tArango Karaoke record (KARAOKE)\n\t[Chemical Record]\n\t\t\n\t::\n\t\n\t\t{\n\t\t\t<record_type>\n\t\t\t<number>0\t-\t0\n\t\t\t<number>1\t-\t1\n\t\t\t<number>2\t-\t2\n\t\t\t<number>3\t-\t3\n\t\t\t<number>4\t-\t4\n\t\t\t<number>5\t-\t5"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self.keys\n\t\t\n#---------------------------------------------------------------------\n\ndef load_records(infile, keys=None, skip_empty=True):\n\t\"\"\"\n\tLoads the specified records from the specified file.\n\t\n\t:param infile_name: The name of the file to load from.\n\t\n\t:param list keys: List of keys to use in the record.\n\t\n\t:param bool skip_empty: By default, only empty records are read.\n\tThe file may be empty if the file is read for reading but if the\n\tdata is not available, the file can be re"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn list(self._keys)\n\n\n@python_2_unicode_compatible\nclass TagRecord(KeyedRecord):\n\t\"\"\"\n\tAttributes for a record.\n\t\n\t:param str name: The name of the record\n\t:type str name: string\n\t:param Optional[dict] default: The default value of a record\n\t:type Optional[dict]\n\t:param str description: The value of a record\n\t:type str description: string\n\t:param bool enabled: The enabled state of a record\n\t:type bool enabled: boolean\n\t:param datetime"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self.records\n\n#\n#\tdef update_time(self, time):\n#\t\t\"\"\" Update the record's (possibly active) update date\n#\t\n\tif time is None:\n\t\treturn None\n\tif time < self.get_timestamp_sec():\n\t\treturn int(time)\n\tif self.update_days is None and time >= self.previous_update_time:\n\t\tself.update_day(self.update_days)\n\t\tif time <= self.update_update_time():\n\t\t\tself.update_update_"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\tfor k in self.base.keys():\n\t\tif k not in self.default_values:\n\t\t\treturn k\n\treturn []\n\nclass RecordSet():\n\t\"\"\"\n\tRaised when the collection is changed.\n\t\n\t:param id: object identifier\n\t:param base: array of recordsets\n\t:param default_values: default value, a list of values with the same names as the keys\n\t:param keys: list of keys, a list of recordsets. Key names should be the keys in a record (i.e. keys[0] = name of"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self.__keys()[:]\n\t\n\t\ndef values(self):\n\t\"\"\"\n\tReturn the values of the record.\n\t\n\t:return: list of value objects\n\t\"\"\"\n\tvals = []\n\tfor val in self.__values():\n\t\tvals.append(val.__dict__())\n\treturn vals\n\n\t\ndef get_value(self, name, default=None): \n\t\"\"\"\n\tReturn a value with the given name.  If the value for a given name\n\tdoes not exist, it is created or added to an existing record.\n\t\n\tIf"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self.values(\"name\")\n\n@record.define_type(name=\"boolean\", value=True)\nclass RecordBoolean(Record):\n\t\"\"\"\n\tRecordType for the record's boolean-valued property.\n\t\n\t:param value: True or the field's default value\n\t:type value: boolean\n\t\"\"\"\n\tvalue = Bool(source=record.source)\n\n\nclass RecordNumber(Record):\n\t\"\"\"\n\tRecordType for the record's number-valued property.\n\t\n\t:param value: numeric value\n\t:type value: int\n\t"}]}
{"task_id": "62e60ecfd76274f8a4026d6a", "prompt": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\treturn ProtocolVersion_MAP.get(protocol_version)\n\ndef _proto(*args, **kwargs):\n    \"\"\"\n\tConstruct and return protocol handlers according to the\n    protocols in arguments, if the argument is a protocol object.\n    \n\t:param args: iterable\n\t:param kwargs: key-value dictionary\n\t:return: dictionary of protocol handlers that represent the\n\t    protocol in the context of protocol protocol arguments\n\t:raise TypeError: if all input args has type bool or iterable\n\t:raise ProtocolVersionUnsupportedError: if the given protocol version"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t# If no protocol versions are provided, return all version tuple\n\t# 0, 3, 5\n\tif not protocol_version:\n\t\treturn {\n\t\t\t0:  BoltProtocolHandler,\n\t\t\t3: BoltProtocolHandler,\n\t\t\t5: None\n\t\t}\n\t# If specific protocol version is provided it must\n\t# be present\n\telse:\n\t\treturn {\n\t\t\t0:  BoltProtocolHandler(0),\n\t\t\t3: BoltProtocolHandler(0),\n\t\t\t5: BoltProtocolHandler(protocol_version["}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_ver = sys.version[:2]\n\telse:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\n\t\t\t\tf\"protocol version must be tuple ({type(protocol_version)})\",\n\t\t\t)\n\t\tprotocol_ver = version.Version(bytes([protocol_version[0], protocol_version[1]]))\n\t\tif protocol_ver is not None:\n\t\t\tif protocol_ver < (3, 5):\n\t\t\t\tprotocol_ver = \"unsupported\"\n\t\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif type(protocol_version) is int:\n\t\tprotocol_version = [protocol_version]\n\n\tprotocol_version = tuple(version for version in protocol_version)\n\treturn protocol_handlers_impl(cls, protocol_version=protocol_version)\n\n\ndef protocol_handlers_impl(cls, protocol_version=None):\n\treturn collections.OrderedDict([\n\t\t('bolt.protocol_handlers._protocol_handler_version', protocol_version)\n\t]).items()\n\n\ndef protocol2protocol(protocol, protocol_info, protocol_version):\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn None\n\telif isinstance(protocol_version, tuple):\n\t\treturn cls._protocol_handlers(protocol_version[0])\n\telse:\n\t\treturn cls._protocol_handlers(protocol_version)\n\n\nclass BoltProtocolHandler(protocol_handler):\n\t\"\"\"\n\tClass for the Bolt protocol handler that supports the following\n\tPROTOCOL VERSIONS:\n\n\t\t* BOLT (0, 0): Bolt Protocol. Supports standard BOLT protocol\n\t\t    version.\n\t\t* BoltProtocol"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\tversion = BoltProtocolVersion\n\telif isinstance(protocol_version, int):\n\t\tversion = protocol_version\n\telse:\n\t\t# for backwards compatibility, get the actual\n\t\t# protocol version for the last version that this\n\t\t# tuple implements\n\t\t# this way protocol_version will take precedence\n\t\tversion = 1\n\t\tfor protocol_version_tuple in protocol_version:\n\t\t\tv = version + protocol_version_tuple[0]\n\t\t\tif v == BoltProtocolVersion:\n\t\t\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif not protocol_version:\n\t\tif protocol_version == (0,3):\n\t\t\treturn default_protocol_handlers\n\t\tif protocol_version == (5,):\n\t\t\treturn default_protocol_handlers_5\n\t\t\t\n\t# protocol version not supported\n\tif protocol_version is None:\n\t\traise TypeError(\"protocol version not supported by Bolt. \"\n\t\t\t\"If you passed in a protocol version, please provide it.\")\n\n\n_protocol_handlers = dict()\n\n\nclass _NoVersion(object):\n\t\"\"\"\n\tclass constructor for subclasses"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_version = Bolt_PROTOCOL_VERSION_DEFAULT\n\tfor version in protocol_version:\n\t\tif version == 0 or version is not Bolt_PROTOCOL_VERSION_INVALID:\n\t\t\tfor handler in Bolt_PROTOCOL_HANDLERS:\n\t\t\t\tif handler not in __all__:\n\t\t\t\t\traise TypeError(\"Protocol version can only be \"\n\t\t\t\t\t\t\"from zero or one, received: %d - %d\" %\n\t\t\t\t\t\t(version, handler))\n\t\t\t\tprotocol_version = __get_handler_protocol_version"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\tversion = None\n\telse:\n\t\ttry:\n\t\t\tversion = protocol_version[0]\n\t\texcept KeyError:\n\t\t\t# try to parse version in a tuple\n\t\t\tversion = tuple(protocol_version)\n\t\t\t\n\t# protocol handler classes can use protocol version\n\tcls.protocol_version = version\n\treturn cls.protocol_handlers\n\n\ndef version_by_minor_in_namespace(version, namespace):\n\tmajor, minor, _ = splitVersion(version)\n\n\tif major == 3 and"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\treturn dict([(v, k) for k, v in cls.protocol_handlers.items() if v])\n\n\n# Internal class -- to hold information related to the\n# :class:`bolt.core.connection.BaseConnection` class.  \nclass BoltConnection:\n\t\"\"\"\n\tHold a connection.\n\t\t\n\tThe :class:`bolt.core.connection.BaseConnection` class is a subclass\n\tof :class:`bolt.core.connection.ConnectionBase` that provides an\n\tinterface to the :class:`bolt.core.coremodel.bolt.bolt.Base"}]}
{"task_id": "62e60e49d76274f8a4026d25", "prompt": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tfrom neo4j.exceptions import TransactionTerminatedException\n\tif isinstance(metadata, dict):\n\t\tmetadata = metadata.copy()\n\t\n\tif isinstance(timeout, float):\n\t\ttry:\n\t\t\ttimeout = int(timeout)\n\t\texcept ValueError:\n\t\t\traise ValueError(\"Timeout must be integer but got {}.\".format(type(timeout).__name__)) \n\t\t\n\t\ttimeout = timeout*(2.0**10 - 1) + 0.5\n\telif not timeout:\n\t\ttry:\n\t\t\ttimeout = 5\n\t\texcept ValueError:\n\t\t\ttimeout"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tif isinstance(metadata, dict):\n\t\tmetadata = {key:metadata[key] for key in metadata}\n\t\n\tdef wrapper(func):\n\t\tdef wrapper_helper(func_name, *args, **kwargs):\n\t\t    @wraps(func_name)\n\t\t    def wrapper(*args, **kwargs):\n\t\t\t    try:\n\t\t\t        return func(*args, **kwargs)\n\t\t        finally:\n\t\t\t        pass\n\t\t    return wrapper_helper\n\t\n\t\treturn"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tfrom neo4j.operation import TransactionOperation\n\t\n\toptions = TransactionOperation.options()\n\t\n\toptions.delay = timeout\n\tif not hasattr(options,'retries'):\n\t\toptions.retries = 0\n\t\t\n\tif not hasattr(options, 'query'):\n\t\toptions.query = None\n\t\n\treturn TransactionOperation.__new__(TransactionOperation, **{\n\t\t\"metadata\": metadata,\n\t\t\"timeout\": properties(timeout, \"timeout\"),\n\t\t\"query\": options.query,\n\t\t\"transactions\": [t for t, d in properties(properties"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\toriginal = globals()['unit_of_work']\n\tif original and original is not None:\n\t\treturn original(metadata=metadata, timeout=timeout)\n\treturn unit_of_work\n\n//----------------------------------------------------------------------------------------------------\n// A complete solution to implementing the D2P API:           http://www.cnblogs.com/csf/p/d2p/\n//                                Copyright    "}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tif metadata is None:\n\t\tmetadata = {}\n\t\tif unit is None:\n\t\t\tprint(\"Error. Transaction is not yet executing\")\n\t\telse:\n\t\t\tprint(\"Error. Transaction is not yet executing\")\n\telif timeout is None:\n\t\tprint(\"Error. Transaction is not yet executing\")\n\n\tdef wrapper(func):\n\t\t@functools.wraps(func)\n\t\t@dbms.setTXMetaData\n\t\tdef _wrapper_func(*args, **kwargs):\n\t\t\tif metadata is None:\n\t\t\t\tmetadata = {}"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tglobal _tx, _tx_timeout\n\tif _tx is None:\n\t\tif timeout is not None:\n\t\t\t__name__ = 'unit_of_work_timeout'\n\t\telse:\n\t\t\t__name__ = 'unit_of_work_no_timeout'\n\t\n\t\t_tx_timeout = timeout\n\t\n\t\t# set transaction timeout with value specified\n\t\timport logging\n\t\tlogging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tdef __new__(cls, metadata=None):\n\t\tmetadata = metadata or {}\n\t\t\n\t\tif isinstance(metadata, dict):\n\t\t\tif metadata.get(\"timeout\", \"\") is None:\n\t\t\t\traise ValueError(\"You require specified 'timeout' (i.e. 'None') in your metadata. To override timeout setting, use the parameter: 'timeout' above: use the same as 'timeout' defaults to :const:`None`.\")\n\t\t\n\t\tsetattr(cls, \"metadata\", metadata)\n\t\treturn super(Transaction, cls).__new__(cls)\n\n\tdef __"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tdef wrapper(func):\n\t\tif isinstance(timeout, float):\n\t\t\treturn functools.partial(\n\t\t\t\tfunc,\n\t\t\t\ttimeout=timeout,\n\t\t\t\t_callargs=[metadata=dict(_getparam=1, type=metadata.type)]\n\t\t\t)\n\t\telse:\n\t\t\treturn functools.partial(\n\t\t\t\tfunc,\n\t\t\t\t_getparam=1, type=metadata.type,\n\t\t\t\t_callargs=[metadata=dict(_setparam=1, type=metadata.type)]\n\t\t\t)\n\t\t\n\tdef decorator("}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*arg, **kwarg):\n\t\t\tif timeout is not None:\n\t\t\t\twrapped = partial(unit_of_work, timeout=timeout*1e6, metadata=dict(metadata=meta, timeout=timeout))\n\t\t\telse:\n\t\t\t\twrapped = unit_of_work\n\t\t\twrapped(*arg, **kwarg)\n\t\t\treturn wrapped\n\t\treturn wrapper\n\n\treturn decorator\n\ndef apply"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tclass TXend(Transaction):\n\t\tdef __init__(self, callback, arguments, metadata, timeout=None):\n\t\t\tsuper(TXend, self).__init__((callback, arguments), metadata, self.txend_callback, self.txend_arguments)\n\t\t\tif timeout is None:\n\t\t\t\tself.transactionTimeout = None\n\t\t\telse:\n\t\t\t\tself.transactionTimeout = time.time() + timeout\n\t\t\tself.timeout = timeout\n\t\tdef _callback(self, client, txresult, query, metadata):\n\t\t\tif (self.timeout >"}]}
{"task_id": "62e60e05d76274f8a4026cfd", "prompt": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t# \u4e0d\u9700\u8981\u91cdcreate, \u53ef\u4ee5\u52a0\u8f7d\u6a21\u677f\n\tif not self._is_template: return -1\n\t\n\ti, a,b = self._items.get_indexes(key)\n\tif b == -1: return -1\n\t\n\tret = i\n# \tret = b\n# \tret += 1\n\treturn ret\n\t\t\t\ndef iterindexes(self):\n\t\"\"\"\n\tReturn the list of the (key,index) pairs associated with the item.\n\t\n\t:return: list (key,index)\n\t:rtype: list"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.items(key)\n\n\nclass Item(dict):\n\t\"\"\"\n\tReturn an item (dict or the actual python object)\n\t\n\t:param key: a key\n\t:type key: string\n\t:param value: the value (used to add to the key)\n\t:type value: dict\n\t:raises: ItemError\n\t\"\"\"\n\tdef __init__(self, key):\n\t\tif not isinstance(key, str): key = \"Item\" + key.replace('.', '__').replace('__', '~').replace('+', '!').replace('/', '').replace"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept:\n\t\tprint(self)\n\t\tprint(self.name+\" index has index that is invalid because it is \"+str(key))\n\t\treturn -1\n\n\ndef get_key(self, _item):\n\t\"\"\"\n\tReturn the hashcode of the given item.\n\t\n\t:param _item: the item\n\t:return: hashcode\n\t:rtype: int\n\t\"\"\"\n\tfor _, item in self.iteritems():\n\t\tif isinstance(_item, item):\n\t\t\treturn"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\tif key not in self : return -1\n\treturn self[{key}]\n\n\n\nclass _ItemDict(dict):\n\t\"\"\"\n\tThis class is a base class for all other functions of the Item class\n\t\"\"\"\n\tdef __getitem__(self, key):\n\t\treturn key\n\n\tdef all(self):\n\t\treturn True\n\n\tdef update(self, newitems):\n\t\treturn True\n\n\nclass _ItemDictView(dictView):\n\tdef __getitem__(self, key):\n\t\tkey = self._get_item_key("}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.get_index(key)\n\ndef add_index(self, key, value):\n\t\"\"\"\n\tThis is the same as :meth:`~pandas.Index.add_index`, except that\n\twe assume all the indices are added.\n\t\n\t:param key: a key\n\t:param value: an item\n\t\"\"\"\n\tif key not in ['None', '_']:\n\t\traise ValueError('The keyword argument index must only be'+ \\\n'given to add_index when adding the item.')\n\tself.add_raw([key, value])\n"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.items.index(key)\n\n\nkey = Index.index\n\nindex = Index(key)\n\nusing System;\nusing FluentValidation.Results;\n\nnamespace FluentValidationDemo.Models.Customers\n{\n    [Validator]\n    public class SellerCustomerValidator : AbstractModelPropertyValidator<SellerCustomer>\n    {\n\n    }\n}\n\n// Copyright 2020 Oath Inc. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\npackage com.yahoo.io;\n"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn _index(self, key, 1)\n\n\t\tdef index(self, key, include_end=False):\n\t\t\"\"\"\n\t\tReturns the index of the given item in the given range.\n\n\t\t:param key: a key\n\t\t:param include_end: set to true if this key should return a value.\n\t\t:return: index\n\t\t:rtype: int\n\t\t\"\"\"\n\t\treturn _index(self, key, 1, include_end)\n\n\tdef index_by(self, key):\n\t\t\"\"\"\n"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t# This is to work, rather than using a sequence, because the index only works\n\t# on the sequence elements. That is, index(str) is the same as saying(str).\n\tif isinstance(key, unicode):\n\t\tkey = str(key)\n\tif bool(key):\n\t\treturn 0\n\treturn index(key, key[0], key[1])\n\n#-----------------------------------------------------------------------\nclass dict(list[dict]):\n\t\"\"\"\n\tdict(iterable) --> dict\n\t\n\t:param iterable: A :class:`list` or ``None``\n"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import datetime  as time\n\tpass#IMPORTIMPORTIMPORT import os\n\tpass#IMPORTIMPORTIMPORT import sp_global_def\n\tpass#IMPORTIMPORTIMPORT from sp_utilities import wrap_command_line_password\n\tpass#IMPORTIMPORTIMPORT import sys\n\tpass#IMPORTIMPORTIMPORT import os\n\tpass#IMPORTIMPORTIMPORT from sp_fundamentals import fft\n\tpass#IMPORTIMPORTIMPORT from random import random\n\tpass#IMPORTIMPORTIMP"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\traise NotImplemented()\n\traise NotImplementedError(\"not implemented\")\n\n\ndef set_default_value(self, *args, **kwargs):\n\treturn self.call(\"set_value\", *args, **kwargs)\n\n\ndef set_default_value_for_hash(self, hash_type):\n\treturn self.call(\n\t\t\"set_value_for_hash\",\n\t\thash_type=hash_type,\n\t\tkwargs={'value': hash_type.default_value, 'hash': hash_type.default_hash}\n\t)\n\n"}]}
{"task_id": "62e60da4d76274f8a4026cf1", "prompt": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\treturn self.db.keys_for_values(self, *keys)\n\ndef indexes(self, **kw):\n\t\"\"\"\n\t\n\t\t:param kw: a mapping or column name => value pair\n\t\t\n\t\t:return: a list of indexes\n\t\t\n\t\t:rtype: list\n\n\t\n.. versionadded:: 4.0.0\n\"\"\"\n\timport dbs.indices as n\n\treturn list(self.db.keys_from_indexes(self, **kw))\n\n\n@types(bool)\ndef keys_in_range(self, beg,"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t# If we are returning the values on the fly, cache the result.  The cached\n\t# results are in memory and so any modifications in between would be stale.\n\tif self._values:\n\t\treturn self._values\n\n\tif len(keys) == 0:\n\t\t# Return as a list\n\t\treturn list(self._values)\n\n\t# Otherwise, make a local copy and fill in values.\n\tvalues = [self._values[idx] for idx in keys]\n\treturn values\n\n#    Copyright 2010 Odnoklassniki\n"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tif keys:\n\t\tif not isinstance(keys, int):\n\t\t\tkeys = list(keys)\n\t\tfor k, v in zip(keys, self.get_keys()):\n\t\t\treturn v\n\t\treturn []\n\t\t\t\n\treturn [x for x in self.get_values() if x not in keys]\n\n\ndef record_by_index(record, key=None, unique_keys=False):\n\t\"\"\"Return a dictionary for a record with the given key.\n\t\n\t:param  key: The 'key' or index of the record that will be"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tif len(keys)!= 0:\n\t\tif not isinstance(keys[0], int) or keys[0] < 0:\n\t\t\traise KodiBaseException( \"keys list must have a positive integer\" )\n\t\treturn self.obj.values(keys)\n\telse:\n\t\treturn self.obj.values()\n\t\n\t\nimport re\nfrom datetime import datetime\nfrom sqlalchemy.orm import relationship\nfrom xml.dom import minidom\nfrom.db import db\nfrom.util import split_time, time_to_timestamp\n\ndef parse_"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\treturn self.get(\n\t\t\"values\", (args + keys) if len(args) else keys)\n\t, []\n\n\n@classmethod\nclass GetValue(Field, db.Model):\n\t\"\"\" Get a value out of a record or value\n\tinto a string \"\"\"\n\n\tdef __init__(self, *values,\n\t\t*keys=None,\n\t\t*unique_keys=True):\n\t\t\"\"\"\n\t\tInitialize the class for retrieving/parsing a value.\n\t\t\n\t\t:param values: values to retrieve, only strings will be\n\t\t"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tif keys is None:\n\t\treturn self.values.keys()\n\telse:\n\t\treturn self.values.filter(values=True, *keys)\n\n################################################################################\n###\tFunctions\n################################################################################\n\ndef load(filename):\n\t\"\"\"\n\t:return: a record object\n\t:param filename: the full record filename\n\t\n\t:rtype: :class:`dbtools.record.Record`.\n\t\"\"\"\n\treturn DBtools.record.Record.load(filename)\n\ndef new_record(filename, store_values=True, store_ids=True"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\ttry:\n\t\tself.values = []\n\t\tfor key in keys:\n\t\t\tself._check_index(key)\n\t\t\tvalues.append(self._get_value(key))\n\tfinally:\n\t\tdel self.values\n\treturn values\n\t\n\t\n#---------------------------------------------\n \n\t# Internal Functions\n\t# ------------------------------------\n#\t@staticmethod\n#\tdef _handle_error(error, message=\"\", line=-1):\n#\t\t\tif line < 0:\n#\t\t\t\tprint(info, error, message, line)\n#\t\tif error"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\treturn values(self, keys, values=True)\n\n@add_docstring_to_component_docstring\ndef items(self, *keys, key=None, values=False):\n\t\"\"\"\n\tReturn the items with the current ordering and order.\n\t\n\t:param keys: indexes or keys of the items to sort or order\n\t             respectively\n\t:param key: sort or key of the items to include\n\t:param values: whether to include\n\t:return: list of items\n\t:rtype: list\n\t\"\"\""}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tif not keys:\n\t\twith self.db.transaction:\n\t\t\tself.db.update(values=self.db.public_iter(),\n\t\t\tvalues_by=self._defaultdict)\n\telse:\n\t\twith self.db.transaction:\n\t\t\tself.db.upsert([self._parse_index(keys,\n\t\t\tself.db.index(col))\n\t\t\tfor col in keys])\n\t\treturn self.db.public_iter(self.db.select())\n\n\t# return all the values sorted by sorting key\n\t"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tif hasattr(self, \"record\") and self.name in self.record._by_index:\n\t\tval = tuple(vals.values(self))\n\telif hasattr(self, \"key\"):\n\t\tval = self[self.key] # this item is requested by index\n\t\tif len(keys)!= keys[0]:\n\t\t\traise ValueError(\"items must be a list\")\n\telif hasattr(self, \"_by_index\"):\n\t    if hasattr(keys, \"keys\") or hasattr(keys, \"items\"):\n\t\tkeys = keys.values()"}]}
{"task_id": "62e60b10d76274f8a4026ccd", "prompt": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tfor k in self._keys:\n\t\tval = self[k]\n\t\tif (len(keys) == 1 and not has_index(self[k], keys)):\t# need to support nested items, which all have no index\n\t\t\traise ValueError('%s does not support nested items' % self[k])\t\t\t# no idea how bad\n\t\tdata[k] = val\n\n\treturn data\n\ndef has_index(key, keys):\n\t\"\"\"Check if a dictionary satisfies the requirement, using the key\n\tprovided with keys\"\"\"\n\tfor k in keys"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tkeys = [k for k in keys if k not in [\"name\",\"created\",\"size\"]]\n\treturn dict([(k, v) if v is not None else None for k, v in items(self)[::-1].items()])\n\ndef create(filename=None, data=None, index=False, mode=None):\n\t\"\"\"\n\tCreate a new dictionary using the file, storing the values\n\tin that file as a field value.\n\t\n\t:param filename: a python string to use as the filename for the\n\t\trecord.\n\t:param data: a"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\titems = self._iter()\n\tif keys is None:\n\t\t# If the key is not passed, just use the given keys\n\t\tkeys = items[0].keys()\n\tif len(keys)!= len(items):\n\t\traise IndexError(\"Input has an invalid length\")\n\tdata = {}\n\tfor field, data_item in items:\n\t\tif field in data:\n\t\t\t# This is a duplicate field\n\t\t\traise KeyError(\"Duplicate field %s\" % field)\n\t\ttry:\n\t\t\tdata[field] = data_item"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tif not self.is_valid():\n\t\traise IndexError(\"data called on invalid record\")\n\t\treturn {}\n\treturn self._dict_keys_values(keys)\n\n\ndef _set_data(\n\tdata_items,\n\tfield_name=\"data\",\n\tvalue_factory=data_factory,\n\tadd_metadata_fields=False\n):\n\t\"\"\"\n\t\n\t:param data_items: dictionary of objects that correspond to records\n\t                   associated with the given field name and"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tif (all((len(k) == len(v) for k,v in keys))), \"keys and keys\"\n\t\t: if len(keys) > 1 and type(key) is unicode and len(keys[0]) > 127:\n\t\t\tkeys = (values_dict(key, len(keys) - 1))\n\t\telse:\n\t\t\tkeys = values_dict(key, len(keys), \"string key\")\n\tif len(keys) > 0:\n\t\treturn keys, values_dict(data[keys], data[keys], \"string\")\n"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict_of_keys\n\texcept AttributeError:\n\t\tkeys = dict((k, value) for (k,value) in zip(self.keys(), keys))\n\t\tself.__dict_of_keys = keys\n\t\treturn self.__dict_of_keys\n\n/****************************************************************************\n*\n*                            Open Watcom Project\n*\n*    Portions Copyright (c) 1983-2002 Sybase, Inc."}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\treturn self.dict(self._keys, (*keys or (),))\n\nclass GenericRecord(Generic):\n\t\"\"\"\n\tRepresents a record of specific data as a dict\n\t\n\t:param data: the data to which the record is specified\n\t\t\n\t:return: dictionary with all values\n\t:raises: :exc:`KeyError` if the key is out-of-bounds,\n\twhich can be raised in the case of :exc:`KeyError`, or the\n\tdictionary with keys of interest\n\t\"\"\"\n\t@abstract\n\tdef is_valid(self, *keys):\n"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tm = _empty_record\n\t_copy_record(self, m)\n\tif m.keys is _const():\n\t\tkeys = m.keys[0].items\n\tif keys is None:\n\t\tkeys = dict(self.keys)\n\tkeys.sort()\n\t\n\tkeys_sorted_by_key = sorted(keys, key=len, reverse=True)\n\t\n\tif keys_sorted_by_key:\n\t\tkeys = keys_sorted_by_key\n\t\n\tfor i, key in enumerate(keys):\n\t\tif i <"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tfrom.. import helpers\n\tif isinstance(keys, int):\n\t\tkeys = range(keys, -1, -1)\n\tif not helpers._validate_keys(keys):\n\t\traise TypeError('keys must be an integer')\n\tif len(keys) > self.nfields:\n\t\traise IndexError('values must be less than or equal to %d' %\n\t\t\t                         self.nfields,\n\t\t\t             "}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\ttry:\n\t\tfields.DATA.VALID = True\n\texcept AttributeError:\n\t\tfields.DATA = dict(fields.DATA(), *keys)\n\n\n@data\ndef record_ids(self):\n\traise NotImplementedError()\n\n@data\ndef record_id(self):\n\traise NotImplementedError()\n\n@data\ndef record_id_value(self, value):\n\traise NotImplementedError()\n\n@data\ndef record_id_to_id(self, value):\n\traise NotImplementedError()\n\n@data\ndef record_id_to_keys(self):"}]}
{"task_id": "62e6087bd76274f8a4026bfa", "prompt": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tr = self.read_u16()\n\tif r!= self.u:\n\t\traise Bombad(\"u\" + repr(r) + \"!=\" + repr(self.u))\n\treturn self.u >> 8\n\n@def push_i(self, v):\n\t\"\"\"\n\tMake a new integer, inserting a bit in the most significant byte of the\n\tvalue.\n\t\"\"\"\n\tself.u = self.__shift_u(self.u, uint(v))\n\tif self.u == 0xffff:\n\t\traise OverflowError(\"Overflow"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn (self.pop_bytes(1) << 8) + \\\n\t\t     self.pop_bytes(1)\n\n\n#-------------------------------------------------------------------------------\n# Data management and operations\n#-------------------------------------------------------------------------------\n\ndef clear_space(self):\n\tself.clear_data()\n\ndef dump_space(self):\n\tself.dump_data()\n\n\n@defparse\ndef mem_to_int(s):\n\ts2 = \"\"\n\tfor i in range(0, len(s)):\n\t\ts2 = s2 + s[i:]\n\t"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\td = struct.pack(\"!i\", self.pop_u16())\n\treturn big.u16(d)\n\n\ndef pop_u32(self):\n\"\"\"\n\tRemove the last four bytes of data, returning them as a big-endian\n\t32-bit unsigned integer.\n\t\"\"\"\n\td = struct.pack(\"!i\", self.pop_u32())\n\treturn big.u32(d)\n\n\ndef pop_u64(self):\n\"\"\"\n\tRemove the last eight bytes of data, returning them as a big-endian\n\t64"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tself.reset_length()\n\tself.data = self.data[:-2]\n\treturn self.pop_u16()\n\ndef pop_u24(self):\n\tself.reset_length()\n\tself.data = self.data[:-3]\n\treturn self.pop_u16()\n\ndef pop_u32(self):\n\tself.reset_length()\n\tself.data = self.data[:-4]\n\treturn self.pop_u16()\n\ndef pop_u40(self):\n\tself.reset_length()\n\t"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn sbytes2uint16(pop_u16())\n\ndef sbytes2uint32(self):\n\treturn sbytes2uint(self.pop_u32())\n\ndef sbytes2uint16(self):\n\treturn sbytes2uint(self.pop_u16())\n\ndef pop_u32(self):\n\treturn sbytes2uint32(self.pop_u32())\n\ndef sbytes2uint(self):\n\t\"\"\"\n\tThe u32 data is one byte, the byte-count from the first\n\tbyte. The remaining bytes are"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tself.error(\"pop_u16\", 0x0320)\n\n\n@_ustr = _binary_or_unicode(\"\")\ndef repr_u8(self):\n\t\"\"\"Returns an 8-bit string of Unicode code points\n\trepresented by CPython's unicode() function.\"\"\"\n\tself.error(\"repr_u8\", 0x0324)\n\n\n@_ustr = _binary_or_unicode(\"\")\ndef repr_u16(self):\n\t\"\"\"Returns a 16-bit Unicode string of CPython's\n\tunicode() function.\"\"\"\n\tself.error(\"repr_"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\trv = array()\n\tfor i in range(1,32):\n\t\trv.append(self._pop_u16())\n\treturn array.array('B', rv)\n\n\n#----\n\ndef _pop_u16(value):\n\ttemp = value\n\tdata = tuple(temp)\n\tdata.index = (data.index + 1) % 32\n\treturn data\n\n\n#----\n\ndef __init__(self, value):\n\tself._value = (value,)\n\n\n#----\n\ndef load_blob(self, raw_"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tself.data = self.data[:-2]\n\t#print(self.data)\n\treturn self.data\n\nutf8_map = {\n\t\"big5\": [0xa1, 0x7d, 0x49],\n\t\"big5-hkscs\": [0x31, 0x61, 0x37, 0x5e, 0xad, 0x4d, 0x33, 0x7f, 0xae, 0xaf, 0x4e, 0xca, 0x30, 0x3b, 0x41, 0x44,"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn self._data[(self._len - 1) & 0x3FFF]\n\ndef set_u8(self, u8):\n\tself._data[0] = u8\n\tself._len = 1\n\ndef set_u16(self, u16):\n\tself._data[0] = u16\n\tself._len = 2\n\ndef append_u8(self, u8):\n\tself._data[(self._len + 1) & 0x3FFF] = u8\n\tself._len += 1\n\ndef append_u16("}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tself.buf = self.buf[4:]\n\tout,leftover = self.pop_u16()\n\tleftover = littleendian_int(leftover)\n\tif leftover!= 0:\n\t\tleftover,self.pop_u16()\n\tout = bigendian_ushort(self.buf[leftover:])\n\treturn self.u16_to_uint(out)\n\ndef read_u32(self):\n\tself.buf = self.buf[4:]\n\tout,leftover = self.pop_u32()\n\tleftover"}]}
{"task_id": "62e6087ad76274f8a4026bf2", "prompt": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\treturn self.discard_all(n,qid,dehydration_hooks, handlers=handlers)\n\n\n# -----------                                       --------\n#                                                           "}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\traise util.Interrupt if self.stopped else ()\n\n\tif self.msg_queue.empty():\n\t\tself.msg_queue.put(self.msg_factory(message=self.msg, key=self.key, type=self.type,\n\t\t\tmsgid=self.msg, msgseq=self.msgseq, requeuedelay=self.requeuedelay,\n\t\t\trequeuemsg=self.requeuemsg))\n\telse:\n\t\tif qid:\n\t\t\tself.msg_queue.put(self.msg_factory(message=self."}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\treturn self._session.discard(n, qid, dehydration_hooks, \"discard\",\n\t\thydration_hooks, dehydration_hook_dict(self._hooks))\n\ndef dissoclean(n=500, qid=None, dehydration_hooks=None,\n\thydration_hook_dict=None, **dehydration_hook_dict):\n\t\"\"\"Dissoclean an object (as produced by python).\"\"\"\n\tfrom binascii import hexlify\n\tfrom math import sqrt\n\t\"\"\"Dissoclean"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tdef reset(self):\n\t    self.id_counter.clear()\n\t    self.status_counter.clear()\n\t    self.result_count = 0\n            self.type_counts = {}\n            self.total_count = 0          \n            self.type_index = -1            \n\t    reset_dehydration"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tdefer conn.d.close()\n\tif not n: n = -1\n\tif qid:\n\t\tdehydration = self.unpack(lambda r: True)\n\t\tif dehydration_hooks:\n\t\t\tfor k, v in dehydration_hooks.items():\n\t\t\t\thandler = dehydration.get(k, None)\n\t\t\t\tif handler is not None:\n\t\t\t\t\tpass\n\t\t\tresponse = conn.pack(self.d, dehydration, qid, v)\n\t\t\tif response is not None:\n"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\treturn dehydrate.unpack(self.unpack)\n\ndef dehydrate(unpack):\n\t\"\"\"\n\tApplies dehydrates to the given response, to replace the existing results\n\tof the dehydrated messages. A dehydring process is aborted if either\n\tdehydration_hook is not defined or the dehydrate_hook or type dehydrates\n\tare used, and the state of the dehydrations is restored.\n\t\n\t:param unpack: unpacks a response into a response object, and returns\n\tA tuple (response_type"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tif n is None:\n\t\tn = len(packstream)\n\tif isinstance(n, int):\n\t\tn = n // 1000\n\tmsg = packstream.pack(tuple(dehydration_hooks or {}),\n\t\t\t\t\t\tpackstream.unpack(tuple(pid_or_dict), False), tuple(n), qid)\n\treturn msg, None, None, 0\n\ndef pack_discriminance(discrimination, discontinuity):\n\tif discontinuity:\n\t\treturn packstream_discriminance(discrimination)\n\t"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tif n > -1:\n\t\tdata = []\n\t\ttry:\n\t\t\tdata.append(self.dehydration_hook(dehydration_hooks, qid, n))\n\t\texcept Exception, e:\n\t\t\tself.logger.error(\"Error during dehydrate!\", exc_info=e)\n\t\t\tdata.append(\"Failed to dehydrate! \" + str(e))\n\t\t\treturn Response(503, **data)\n\t\treturn Response(202, **data)\n\tdata = []\n\ttry:\n\t\tdata."}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tmsg = Response().set_status(0)\n\tif n < 0:\n\t\tassert self.client.debug\n\t\tif self.logger:\n\t\t\tself.logger.debug(\"Discarding unknown records\")\n\t\tmsg.add_log_message(\"Unknown records\")\n\t\treturn None\n\tif qid < 0:\n\t\tassert self.client.debug\n\t\tif self.logger:\n\t\t\tself.logger.debug(\"Discarding unknown keys\")\n\t\tmsg.add_log_message(\"Unknown keys\")\n\t\treturn None\n\tn ="}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself.dispatch(Request(RequestType.DISCENT, None))\n\treturn self.pop_n(0, n, type=dehydration_hooks)\n\n@dispatch\ndef discard_n(self, n, qid, dehydration_hooks=None, hydration_hooks=None, handlers=None,\n__ignore__ = None, dehydration_hook=None, hydration_hook=None,\n__ignore_non_stream_type=None, _ignore_non_stream_type=None,\n\"\"\"\n\tAppends a DISC"}]}
{"task_id": "62e60879d76274f8a4026bec", "prompt": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tif (isinstance(mode, str) or\n\t    # For backward compatibility support, allow the \"mode\" string\n\t    # from the first argument to the new Transaction function as the only argument.\n\t    callable(_mode_arg_list) or\n\t\tisinstance(_mode, str) and\n\t\t _mode.strip().lower() in _mode_arg_list[1:]) and\n\t\t(_mode!= \"read\") and\n\t\t (mode!= \"read\" and mode!= \"write\") and  (authorizers) and \\\n        "}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tdef _append_before(\n\t\tself, response, type_):\n\t\tif type_ == \"END\" and \"application/json\" not in response.headers:\n\t\t\theaders = self.dehydrateHeaders(response)\n\t\t\theaders[\"Transfer-Encoding\"] = 'base64'\n\t\t\tresponse.headers[\"Transfer-Encoding\"] = headers\n\t\treturn response\n\tdef end(self):\n\t\tif self.client.version < (4, 0, 0):\n\t\t\t# Bolt 4.0+ don't have dehydration since we"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tif not self.queue:\n\t\treturn\n\t\n\tfor i in range(0, len(self.queue)):\n\t\tif self.queue[i]!= 0: # pragma: no branch\n\t\t\tif wait_for_reply:\n\t\t\t\twith Connection.begin_session() as sesss:\n\t\t\t\t\tsesss.connection.log_with_object(\n\t\t\t\t\t\t\"{}: sending BEGIN {}\".format(self.db, self.queue[i]), level=3\n\t\t\t\t\t)\n\t\t\t\t\tfor msg in self.send_messages(mode=mode,\n\t\t\t\t\t"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tif mode not in ['READ', 'WRITE'], mode:\n\t\tmode = mode +'READ'\n\treturn begin(self, mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks, handlers)\n\n\ndef begin_transaction(self, mode=None, bookmarks=None, metadata=None, timeout=None, db=None, imp_user=None, dehydration_hooks=None, hydration_hooks=None, handlers=None, **kwargs):\n\t\"\"\"\n\tAppends a"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\ttry:\n\t\tresponse = Transaction.begin(self, mode=mode, bookmarks=bookmarks, metadata=metadata,\n\t\ttimeout=timeout, db=db, imp_user=imp_user, dehydration_hooks=dehydration_hooks,\n\t\thydration_hooks=hydration_hooks, handlers=handlers)\n\texcept Exception as exc:\n\t\tlogger = logging.getLogger(self.__module__)\n\t\tlogger.error('Error while trying to begin {} transaction: {}'.format(self.__class__.__name__, exc))\n\t\t"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t# Get optional params and parse them\n\t#print \"begin\"\n\tif mode == None:\n\t\tmode = _settings[\"Mode\"]\n\tif bookmarks is None:\n\t\tbookmarks = []\n\tif metadata is None:\n\t\tmetadata = {\n\t\t\"Type\": \"list_of_bookmarks\",\n\t\t\"Version\": _settings[\"Version\"]+\"\"\n\t\t}\n\tif timeout is None:\n\t\ttimeout = 300*60*60\n\tres_msg = []\n\tfor i in range(len(metadata)):\n\t\t"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\twith self._begin_lock:\n\t\tif mode is None or mode == \"READ\":\n\t\t\tif bookmarks is None:\n\t\t\t\treturn bolt.Request.empty_response\n\t\t\telse:\n\t\t\t\tbookmarks = [x.strip() for x in bookmarks]\n\t\t\tdata = self._empty_batch if self._auto_close else [b\"OK\"]\n\t\t\tself._send(self._make_request(\"begin\", bookmarks, data = data, headers = self._header, timeout = timeout, db = db, imp_user = imp_user"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tdef begin(self, mode, bookmarks, metadata, timeout, db, imp_user=None, dehydration_hooks=None, hydration_hooks=None, handlers=None):\n\t\t\"\"\"\n\t\tInitialize this method for Transaction objects\n\t\t\n\t\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t\t:param metadata: custom metadata dictionary to attach to the transaction that we want to use\n\t\t:param timeout: timeout for transaction"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tdef begin(self, mode, bookmarks=None, metadata=None, timeout=None,\n\tdb=None, dehydration_hooks=None, dbi_handlers=None, imp_user=None,\n\thydration_hooks=None, hydration_handlers=None, handlers=None,\n\tdb_handlers=None, timeout_secs=None):\n\t\t\"\"\"\n\t\tAppends a BEGIN message to the output queue.\n\n\t\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t\t:"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tdef dehydrate(value):\n\t\t\"\"\"\n\t\tdehydrate: returns an object containing data about how data was dehydrated in a transaction\n\t\t\"\"\"\n\t\ttry: return self.db.unpackdata('bstore', value)\n\t\texcept PackStreamerDecodeError:\n\t\t\ttry: logger.logwarning(\"Unable to unpack: %s\" % value, exc_info=True)\n\t\t\treturn PackStreamException(None, {None:pack_exceptions.UnpackDehydrationError()})\n\tdehydraible = dehydration_hooks."}]}
{"task_id": "62e60723d76274f8a4026b75", "prompt": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tn = n - ((n >> 1) & 1)\n\treturn (n & 1) << 1\n\n\ndef get_round_down_to_even(n):\n\tr\"\"\"\n\t\t>>> round_down_to_even(2)\n\t   .....\n\t   .....\n\t   .....\n\t   .....\n\t   .....\n\t   .....\n\t\"\"\"\n\tm = ((n + 15) >> 16) & 0x800\n\tn = (n * m + 15) >> 5\n\tr ="}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif n is None:\n\t\traise ValueError(\"n cannot be None\")\n\n\tf = None\n\tif isinstance(n, float):\n\t\tf = double(n)\n\telif not isinstance(n, int):\n\t\traise ValueError(\"nan is not a valid integer\")\n\n\treturn round_half_to_even(floor(f / 2))\n\nimport { ChangeDetectionStrategy, Component, OnInit } from '@angular/core';\nimport { Subject } from 'rxjs';\nimport { MessageComponent } from '@app/shared/message/message.component';\n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\treturn RoundHalfHalf(n).tointeger()\n\n\ndef power_of(x):\n\t\"\"\"\n\t>>> power_of(7)\n\t15\n\t>>> power_of(256)\n\t15\n\t\"\"\"\n\treturn powers_of(x)\n\n\ndef powers_of_int(n):\n\t\"\"\"\n\t>>> powers_of_int(4)\n\t[0, 2, 4, 8, 16]\n\t>>> powers_of_int(4.5)\n\t[0, 2, 4, 8, 16]\n\n\t:"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\treturn round_half_to_even(n>>1)\n\ndef round_half_to_even_power(n):\n\t\"\"\"\n\t>>> round_half_to_even_power(3)\n\t    3\n\t    >>> round_half_to_even_power(3.2)\n\t    4\n\t    >>> round_half_to_even_power(3.5)\n\t    8\n\t    >>> round_half_to_even_power(3.7)\n\t    10\n\t    >>>"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif not math.is_integer(n) or n<0:\n\t\traise ArithmeticException\n\tround = round_half_together_with_n_bits(n,0)\n\treturn round\n\ndef clamp(value,max_distance):\n\tif not value < max_distance:\n\t\treturn value\n\treturn max_distance\n\n\ndef to_int(data_object):\n\tif not isinstance(data_object, bytes):\n\t\tdata_object = data_object.decode()\n\t\n\treturn int(data_object,8)\n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\treturn round_half_to_even(n)/2\n\t\n\n@deprecate_flags(legacy=False)\ndef floor_half_to_even(n):\n\t\"\"\"\n\tReturns a newly created (number, value) pair.\n\t\n\t>>> floor_half_to_even(3)\n\t    (3, 3)\n\t    >>> floor_half_to_even(3.2)\n\t    (3, 4)\n\t    >>> floor_half_to_even(3.4) - 1\n\t    (-1"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif n!= 0:\n\t\treturn (n - 1) // 2 + 1\n\telse:\n\t\treturn 0\n\t\n\n\n\ndef is_finite(n):\n\t\"\"\"\n\t>>> is_finite(3)\n\t    True\n\t    >>> is_finite(3.3)\n\t    False\n\t    >>> is_finite(3.4)\n\t    False\n\t    >>> is_finite(3.5)\n\t    True\n\t\"\"\"\n\ttry:\n\t\treturn isclose(n, n +"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif not isinstance(n, int) or n < 0:\n\t\treturn 0\n\t\n\treturn absint(n) % 4\n\n\ndef round_half_to_odd(n):\n\tr\"\"\"\n\t\t>>> round_half_to_odd(-3)\n\t\t-3\n\t\t>>> round_half_to_odd(-3.2)\n\t\t-3\n\t\t>>> round_half_to_odd(-3.5)\n\t\t-4\n\t\t>>> round_half_to_odd(-3.7)\n\t\t-"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif n < 0:\n\t\tn = 2 * n + 1\n\tif n <= 0x7f800000:\n\t\tn = 0x3bffffff * n\n\treturn n\n\t\n\ndef to_short_string(str, n):\n\t\"\"\"\n\tConvert a long int number to its short string.\n\n\t >>> to_short_string(4)\n\t '1234 5555'\n\t\t\n\t >>> to_short_string(1000,)\n\t '1000 1000 100 100'\n\t\t\n\t >>> to_short_string(12345, 5)\n\t '1234"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif n < 0:\n\t\tn = -n\n\tn1 = n >> 1\n\tif n1:\n\t\treturn n >> 1\n\telse:\n\t\treturn 0\n\n\ndef floor_half_to_even(n):\n\"\"\"\n\t>>> floor_half_to_even(3)\n\t    3\n\t    >>> floor_half_to_even(3.2)\n\t    3\n\t    >>> floor_half_to_even(3.5)\n\t    4\n\t    >>> floor_half"}]}
{"task_id": "62e60707d76274f8a4026b69", "prompt": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tfrom sympy.interfaces.objective import make_Point\n\tfrom sympy.abc import symbols\n\tfrom sympy import lift\n\n\tif isinstance(name, symbols.NDArray):\n\t\tif isinstance(name, lift):\n\t\t\tif isinstance(fields[0], Point):\n\t\t\t\treturn fields\n\t\t\telse:\n\t\t\t\treturn make_Symbol('Point', fields + [Point])\n\t\t\traise(ValueError('No Point named: %s' % name))\n\t\telif isinstance(name, string_types) and \\\n\t\t\tname in ('cos"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tif not isinstance(name, str):\n\t\traise exceptions.BadTypeError(\"__point__\" + name + \" must be a string\")\n\n\ndef point_fields(name, fields):\n\t\"\"\"\n\tMake the Point fields.\n\t\"\"\"\n\tif not isinstance(name, str):\n\t\traise exceptions.BadTypeError(\"__point__\" + name + \" must be a string\")\n\n\tfor field, value in fields.items():\n\t\tfields[field] = getattr(Point, field)\n\treturn fields\n\n\n# Class for a Point or PointTuple class that"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tdef _make(self, name, fields, srid_map, **kwds):\n\t\treturn Point(name, fields, srid_map, **kwds)\n\treturn type(self), _make\nif __name__ == '__main__':\n\t#print('## Example: %s -> %s' % (func.__name__, Point.__name__))\n\t#print('## Example: %s -> %s' % (Point,), )\n\tPoint.__name__ = 'Point'\n\t#print('## Example: %s -> %s' % (Point,), )"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tif all(t.point):\n\t\treturn Point( name,fields, srid_map)\n\telse:\n\t\treturn Point._from_table(name,fields, srid_map)[0]\n\ndef point_type_(name, fields, srid_map):\n\treturn Point( name, fields, srid_map)\n\nclass Point(Base):\n\t\"\"\"\n\tA Point object represents a coordinate object (latitude).\n\t\"\"\"\n\tdef __init__(self, name, fields, srid_map):\n\t\tBase.__init__(self,"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tif isinstance(fields, dict):\n\t\tfields = [point_type(f, fields, srid_map) for f in fields]\n\n\treturn Point(name, srid=self.srid, **fields)\n\n\nclass PointGroup(object):\n\t\"\"\"\n\tDynamically create a 2-dimensional Point/group.\n\t\"\"\"\n\tdef __init__(self, name=None, srid=None, fields=None):\n\t\tsuper(PointGroup, self).__init__(name, srid)\n\t\tself.fields = fields\n\t\tself"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tpoint = Point(name, fields, srid_map)\n\tpoint.default_value = None\n\tpoint_type(name, fields, srid_map)\n\treturn point\n\n\nclass Point_fieldmap(list):\n\tdef __init__(self, fields):\n\t\tself[:] = []\n\t\tself.extend(fields)\n\n\tdef __repr__(self):\n\t\treturn str(sorted(self))\n\n\tdef __len__(self):\n\t\treturn len(self)\n\n\t@classmethod\n\tdef from_dictionary(cls,"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tPoint.__init__(self, name, point_attributes(fields))\n\t# Set the id\n\tif srid_map == None:\n\t\tsrid_map = {}\n\tPoint.__init__(self, None, None,\"\",srid_map, None)\n\n\n\n\t# Make sure they're all equal in case we later try to use a different class first\n\tif not hasattr(self, '_point_class'):\n\t\tself._point_class = Point\n\telse:\n\t\tclass Point(self._point_class):\n\t\t\t\"\"\"\n\t\t\t\t"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from sp_applications import main as spmain\n\tif not spmain:\n\t\tspmain = spmain  ## the main module not for execution\n\tpass#IMPORTIMPORTIMPORT from sp_fundamentals import resample\n\tpass#IMPORTIMPORTIMPORT from EMAN2 import RegionViewer\n\n\tpass#IMPORTIMPORTIMPORT from sp_fundamentals import image\n\tpass#IMPORTIMPORTIMPORT from sp_global_def import sxprint, ERROR\n\n\tif myid == main_mpi_rank"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tpoint_type[name] = Point(fields, srid_map)\n\ndef parseTuple(potentials, fields=None):\n\t\"\"\"\n\tParse the string representation of a tuple or list of tentative tuple\n\tor list of tuple for a point. This will also process the tuples as\n\ta list of strings (as opposed to a tuple), if needed.\n\t\"\"\"\n\tfor t, tuple_or_list_tup in zip(tup_or_tuple(potentials), fields):\n\t\tname, tpe = parseTuple_tuple(tuple_or"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tdef instantiate_from_fieldnames(dataobj, name, shape, **kwargs):\n\t\tif not _in_dict(shape, ('x', 'y', 'z', 'w', 'h')):\n\t\t\traise ValueError(\"shape argument must be 1D or 2D array\")\n\t\treturn Point(dataobj.ravel()[name], **dataobj._meta.get_base(**shape, **kwargs))\n\n\tdef get_point(dataobj, name, x, y, z, w, h, **kwargs):\n\t\tshape = _in_dict("}]}
{"task_id": "62e5dc9ed76274f8a4026b5b", "prompt": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tfunction_name = None\n\t@deprecated(\"'' has been deprecated in favour of '' in deprecated function name'\")\n\tdef baz(x):\n\t\tpass\n\t\"\"\"\n\ndef deprecated_func(message):\n\t\"\"\"deprecated function, because it's optional.\"\"\"\n\tfunction_name = None\n\t@deprecated_func('warning', 'favour of deprecated.warn', message, version=2)\n\tdef zappy(x):\n\t\tpass\n\t\"\"\"\n\t@deprecated_func('warning', 'favour of deprecated.warn', message)\n\tdef xyz("}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdeprec1 = Text(\n\t\t\"deprecated(\" + \tstr(type(parameter)) + \")\"\n\t)\n\tdeprec2 = depr(\n\t\tdeprec1,\n\t\tdeprec1,\n\t\t\"Deprecated.\",\n\t\t\"This function has been deprecated in favour of \" + \"\\'\" + function_name + \"\\'.\"\n\t)\n\tresult = depr(\n\t\tfunc_function(deprec2)\n\t)\n\n\t# Also note that this works, as this is how python3 does the same thing.\n\t#"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\treturn deprecated\n\ndef deprecated_in_2_0_6():\n\t# deprecated\n\ta = 5\n\t# not warning\n\t# deprecated\n\tb = 2\n\t# deprecation\n\tc = 5\n\td = 5\n\t# not deprecate\n\tx = 5\n\t# deprecation\n\ty = 5\n\t# deprecated\n\tz ='string'\n\ndef use_deprecated(message):\n\t# deprecated\n\ta = 5\n\tb ='string'\n\tc = 7\n\td ='string'\n\te = 4"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdeprecated2 = deprecate(\n\t\t\"This function has been deprecated in favour of 'bar'\")\n\tdeprecated2.warning = deprecate(\n\t\t\"This function has been deprecated in favour of 'bar'\")\n\t\ndef __docformat__():\n\t# type: () -> str\n\t\"\"\"\n\tDocstring for ``func``, ``func(x)`` and ``func(x, y)``.\n\t\n\t:type: \n\t\"\"\"\n\t\n\t# This decorator is taken from the PyCharm library\n\tclass F(object):\n\t\n\t    def"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef deprecated(*args, **kwargs):\n\t\treturn ''\n\nclass DeprecatedDefaultMixin:\n\n    name                 = 'Deprecated Default'\n    func                 = 'deprecated_func'\n    deprecated           = 'deprecated'\n    default_method       = 'default_method'\n\n\ndef deprecated_func(*args, **kwargs):\n  "}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tclass C:\n\t\tfoo = deprecated(\"'foo' has been deprecated\")\n\tclass B:\n\t\tfoo = deprecated(\"''has been deprecated'\")\n\tclass D(B):\n\t\tfoo = deprecated(\"''has been deprecated'\")\n\t\nClass decorator\n \nA set of helpers and classes that can be used to help developers develop code for\nyour projects.  There are also helpers for functions that allow you to have a\nlist of other things that you could conceivably apply to your code.\n\nHelpers are:\n\n    * `list_of_funcs"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tclass DeprecatedMessage(object):\n\t\t@deprecated(\"_deprecated_ message\")\n\t\tdef deprecated_message(self, message):\n\t\t\tpass\n\t\t_deprecated_message_re = Re(DeprecatedMessage() | DeprecatedMessage())\n\t\t\n\t\tdef deprecated_message(self):\n\t\t\treturn self._deprecated_message_re\n\t\t\n\tdef deprecated_message(self, message):\n\t\t\"\"\"\n\t\tAdd a deprecation message before each function.\n\t\t\n\t\t::\n\t\n\t\t\t# favour of _deprecated_message_re\n\t\t\tdef favour_of"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdeprecated_warning('deprecated', message)\n\nif PY_MAJOR_VERSION == 2:\n\t# deprecated from < 1.6.0\n\t# Deprecation warnings\n\t\n\tdef deprecated_1_6_0(message):\n\n\t\t\"\"\"\n\t\tDeprecation warnings.\n\n\t\tSee _warnings instead.\n\t\t\"\"\"\n\n\t\tdeprecated = 1\n\n\t\twarnings.warn(message, *deprecated, stacklevel=3)\n\nelse:\n\t# deprecated from > 1.5.8\n\t# deprecation warnings\n\n\tdef deprecation_1"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef func(args, kwds, kwds_only):\n\t\t\"\"\"\n\t\tDeprecated from Python 3.3\n\t\t\"\"\"\n\t\traise DeprecationError(\"Function has been deprecated in Python 3.3, use function argument instead.\")\n\tdef func_args(args, kwds, kwds_only):\n\t\t\"\"\"\n\t\tDeprecated from Python 3.3\n\t\t\"\"\"\n\t\traise DeprecationError(\"Function has been deprecated in Python 3.3, use function arguments instead.\")\n\tdef func_args_kwds(args, kwds, kwds_"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t# FIXME should be deprecated\n\tif hasattr(sys.deprecated_hook, 'function'):\n\t\t# old name\n\t\tdef deprecated(name, version):\n\t\t\t\"\"\"\n\t\t\tDeprecated decoration.\n\n\t\t\t:param name: the new name of the function.\n\t\t\t:param version: the new version.\n\t\t\t:type name: string\n\t\t\t:type version: string\n\t\t\t\"\"\"\n\t\t\tif name:\n\t\t\t    return name\n\t\telse:\n\t\t\treturn deprecated\n\n\tdef deprecated_override(name, version=True):"}]}
{"task_id": "62e4fc3c85ea98643089041e", "prompt": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tdef _inline_r_setup2(code: str) -> str:\n\t\treturn '\\n'.join([str(i) for i in _inline_r_setup(code)])\n\n\tcode = compile(code, (r'\\$', _inline_r_setup, _inline_r_setup2))\n\treturn code[2:]\n\n\ndef _inline_r_setup2(code) -> str:\n\treturn '\\n'.join([str(i) for i in _inline_r_setup(code)] + [_inline_r_setup2],\n"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn code\n\n@register.reconstructor\ndef _inline_r_new_conf(args: Args) -> None:\n\t\"\"\"\n\tThe following configuration is done for the new R process to read the\n\tenv files and add the environment variables:\n\t\n@inline\n\t@arg r_opts\n\t\tThe value of R_OPTS, e.g. \"r_opts=rpy2\" if RPy2 is turned on.\n@inline\n\t@arg r_conf\n\t\tThe value of R_CONF_FILE, e.g. \"r_conf"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t# For some reason the R initialization code above should never run, so\n\t# make sure that we are also overriding it by using r_setup.\n\tif len(os.environ) < 2:\n\t\treturn code\n\tfrom r_util import R_ERRORS, R_UNKNOWN__ERRORS\n\t# r2r_setup\n\treturn code.replace(\"R=True;\", \"R=False;\").replace(\"r2r_setup;\", \"R=True;\").replace(\"r2r_setup=(R)\", \".r2r(R;\").replace(\"r2r_setup"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tif code is None: return sys.argv[0]\n\treturn \"R\" + code + \"_setup\"\n\ndef _inline_r_setup(**kwargs):\n\t\"\"\"\n\tSet up R using a callback. This is the \"init\" part of the R\n\tenvironment variable. This can be set via rsetup. R\n\tfunctions are also callable with following\n\targuments:\n\t\\li  rfunc, where a list of strings is passed to R, where each\n\tlist is either one string that is the name to which a function is defined from, or just\n\tone"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\topts = [sys.prefix + \"opts\"]\n\treturn \"R_setup('{}', '{}', '{}')\".format(code, r_args_key, r_opts[0])\n\npackage com.github.sommeri.lessonvm.compiler;\n\nimport com.github.sommeri.lessonvm.Lesson;\nimport com.github.sommeri.lessonvm.utils.ArrayUtil;\nimport net.minecraft.creativetab.CreativeTabs;\nimport net.minecraft.entity.Entity;\nimport net.minecraft"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t# We run R in a special mode\n\tif code == 'R':\n\n\t\t# If R has started, we add the script to the R environment\n\t\tif 'R' not in __salt__:\n\t\t\t# If not using an Salt instance\n\t\t\t# we run as root directory to ensure the permissions\n\t\t\t# are properly configured\n\t\t\tif os.path.exists(R_SCRIPT):\n\t\t\t\tR_SCRIPT = '/var/spool/r.sh'\n\t\t\telif R_SCRIPT.strip() == '':\n\t\t\t\tR_SCRIPT ="}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tif not os.path.exists(code):\n\t\treturn ''\n\n\t# For testing\n\topt_level = 'debug'\n\n\toutput_file = os.path.splitext(os.path.basename(code))[0]\n\n\tenv = {'R_LEVEL_VAR_NAME': opt_level}\n\tif env['DEBUG']:\n\t\tenv['R_DEBUG'] = True\n\n\tif output_file == 'auto':\n\t\toutput_file = os.fsencode(\"%s_%s_tested_output\" % (get_path("}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t# R starts inlining, let's remove the global variables\n\tif R.INLINE.get() == 'always':\n\t\treturn code[len(R.R_INLINED):][:256]\n\treturn code\n\n###############################################################################\n\n\ndef _static(fn, name, level=None):\n\t\"\"\"\n\tTry to find and define an ``PyStaticCompiler`` function:\n\n\t- Find the __init__ method as a first argument\n\t- Add it to the appropriate ``__init__.py`` line\n\t- Rename the file from \"__init__\" to \"__self"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t# Run through the code to validate the set of variables provided as a tuple.\n\tcode = to_string(code)\n\tif (re.search(r'<code(?:class=(['\"]).*?class=([0-9a-zA-Z_\\$]+))?</code(?:class=(['\"]))?>', str(code).replace('[\\r\\n]',''))):\n\t\tr_options = {'': '', '>': '>', '<': '>', ':': '}', ':*': '}', '.': '(',')': '('}\n\t\tr_options.update"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t_options = []\n\t_options = _inline_r._options(code, _options)\n\tif _options:\n\t\treturn ''.join(_options)\n\treturn ''\n\n\ndef _inline_r_execute(module_name, code: str, _options: dict) -> str:\n\tif not _options:\n\t\treturn ''.join(module_name + '=' + code + '\\n' for code in module_name.split('+'))\n\treturn ''.join(module_name + '=' + code + '\\n' for code in module_"}]}
{"task_id": "62e4fbda85ea986430890405", "prompt": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tfrom __future__ import division\n\n\tdef xargs(s):\n\t\tx = ''\n\t\tfor i in range(s.nrow):\n\t\t\tif s.irow[i]:\n\t\t\t\tx += s.irow[i]\n\t\t\telse:\n\t\t\t\tpass\n\t\tif x:\n\t\t\tx = 'x' + x\n\t\treturn x\n\targs = xargs\n\t\n\timport itertools\n\n\tdef yargs(d, a):\n\t\ty = ''\n\t\tif not a:\n\t\t\tprint '*"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tif not _xargs:\n\t\t_xargs = _xargs = '''\n\t\tdef xargs(color,target_concurrency=None):\n\t\t\t\"\"\"\n\t\t\t\tArguments to pass to xargs\n\t\t\t\tcolor - colors\n\t\t\t\ttarget_concurrency (None) - defaults to number of target\n\t\t\t\t```\n\t\t'''\n\txargs.append(\"color=\"+str(color.replace(\"+\",\"+\") if color else \"\"))\n\txargs.append(\"target_concurrency=\"+str(target_concurrency or (1+int(time.time()))))\n\n"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tline = input_args(colors.get('color', colors.DEFAULT_A_COLOR), 'xargs xargs.py ')\n\tif not line:\n\t\treturn\n\tline = line.strip()\n\tif not 'color [xargs]' in line.lower():\n\t\txargs(1)\n\telif 'target_concurrency [xargs]' in line.lower():\n\t\txargs(2)\n\telse:\n\t\txargs(3)\n\tpass#IMPORTIMPORTIMPORT from paddle.fluid import create_logger\n\txargs"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tdef xargs(self, source, target, **kwargs):\n\t\t\"\"\"\n\t\tsource: the python source to invoke (in text or on file or on TTY)\n\t\ttarget: the python target (in the case of file input)\n\t\t\"\"\"\n\t\treturn super(PydevDipole, self).xargs(source, target, **kwargs)\n\n\t@property\n\tdef _popen(self):\n\t\treturn self.open()\n\n\t@property\n\tdef _popenx(self):\n\t\tp, _popen = super"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tfrom pty.lib import (\n\t    timeout, open_ptywriter, pty, util, isatty,\n\t    cpython, termios, winreg, winsock, getpass, popen,\n\t    stdout,\n\t)\n\tdef yargs(argv=None, mode=\"posix\", **kwargs):\n\t\tif (\n\t\t\t(mode == \"shell\" and hasattr(sys, \"popen\"))\n\t\t\tor\n\t\t\t(mode == \"stdio\" and hasattr(sys, \"stdin\"))\n\t\t\tor\n\t\t\t("}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\targs = {}\n\tif get_config().getbool(\"color\"): args[\"color\"] = True\n\tif get_config().getbool(\"target_concurrency\"): args[\"target_concurrency\"] = get_config().getint(\"target_concurrency\")\n\targs[\"color\"] = False\n\t\n\treturn args\n\npackage io.subutai.core.environment.impl.workflow.step.steps;\n\n\nimport io.subutai.common.peer.InPeer;\nimport io.subutai.common.peer.SessionPeer;\nimport io.subutai.core"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t\txargs.color(\"color\", \"colors\", \"colors\")\n\t\txargs.target_concurrency(\"target_concurrency\", \"target_concurrency\")\n\t\txargs.color_verbose_flag     = False\n\t\txargs.verbose_flag           = False\n\t\txargs.xargs_file             = \"-C\"\n\t\txargs.xargs_program          = \"\"\n\t\txargs.xargs_env"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tdef main( argv_list ):\n\t\t\"\"\" \n\t\tDescription: \n\t\t\tCreate a new subprocess instance.\n\t\t\n\t\tArguments:\n\t\t\tNone.\n\t\t\n\t\tOutput:\n\t\t\tNo return values or callables.\n\t\t\"\"\"\n\t\tif os.geteuid()!=0:\n\t\t\traise Exception(\"This system is running with unprivileged mode.\")\n\t\t#os.environ.setdefault('LANG', 'C')\n\t\timport sys\n\t\tenv = dict([(k+'={'+str(v)+\"}\") for k,v in sys.env"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\treturn 'xargs'\n\ndef main(argv):\n\tprint '# Starting xargs with this argv'\n\tif argv is None:\n\t\tprint '# No args to create (use this with -h)'\n\t\tprint '# Skip command'\n\t\tsys.exit(2)\n\targv = argv.split()\n\targs, _ = split_args(argv)\n\tif args:\n\t\txargs(*args)\n\telse:\n\t\tprint '# Failed to open pipe'\n\t\ttime.sleep(1)\n\t\tsignal.signal(signal."}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tif xargs.__version__[0] == '3.0.0':\n\t\t# pty supports xargs 3.7 and 3.8\n\t\tif xargs.__version__[4] == '23':\n\t\t\treturn __posix_pty_shim__(__pptpxx_versions[8])\n\tprint \"Unsupported xargs version %s\"%xargs.__version__\n\texit(1)\n\t\n\t\n\t\nclass posix_pty_shim(threading.active_processor):\n\t\t\n\tdef __init__(self):\n\t\tself.__running = 0\n"}]}
{"task_id": "62e4fbda85ea986430890403", "prompt": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\timport random\n\trng = Random(random.randint(1,9999999))\n\trnd = rng.randrange(0, 100)\n\tif len(rnd) < 50:\n\t\trnd = rnd[:50]\n\telif len(rnd) > 50: \n\t\trnd = rnd[ : 50]\n\t\t\n\trnd = rnd.rstrip(' ')\n\trnd = rnd.lower()\n\tif len(rnd) <= 500:\n\t\trng.shuffle(rnd)\n\t\trnd = rnd.rstrip(' ')\n\t\trnd = rnd.lower()\n"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\treturn [ x for _y in zip(seq, [0] if _y not in [2] else _y) for x in seq]\n\n\ndef shuffle_sequences(sequences: Sequence[str]) -> Generator[Sequence[str], None, None]:\n\t\"\"\"\n\tShuffles one or more sequences on each node according to a random selection\n\t\"\"\"\n\tnode = [list(map(shuffle_string, sequences)).pop(0)]\n\tif len(sequences)!= 1:\n\t\trandomchoice(node)\n\treturn randomrange(1, len(node))\n"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tseq = sorted(sequence_to_string(seq))\n\treturn sorted(chain(*seq))\n\n\n@_shuffled\ndef hamming_encode(word: str) -> str:\n\t\"\"\"Hamming encode a word\"\"\"\n\tret = ''\n\tfor x in word:\n\t\tret += char_to_number(x.to_upper()) * x\n\treturn ret\n\n\n@_shuffled\ndef hamming_encode_word(word: str) -> list[str]:\n\t\"\"\"Hamming encode a word, case insensitive"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tif not all(idx == 0 for idx in seq):\n\t\treturn []\n\ts = ''\n\n\tseq = list(map(int, seq))\n\t\n\tfor x in range(len(seq) // len(list(reversed(seq)))):\n\t\tt = str(len(seq))\n\t\ts += seq[x]\n\t\tseq[x] = seq[t]\n\t\tseq = seq[t:]\n\n\tret = []\n\tshuffle = [[0,x+1,0] for x in range(len(seq))]\n\t"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t#return _shuffled_core(seq)\n\t#return sequence without the shuffle\n\treturn list(seq)\n\ndef _shuffled_random(items: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t#return _shuffled_core(items)\n\t#return sequence without the shuffle\n\treturn list(items)\n\ndef _shuffled_random_core(items: Sequence[str]) -> list[str]:\n\t#use random for shuffled items\n\treturn _shuffled("}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tseed = None\n\t# Seed is not in the first argument\n\t# Seed is always a 1\n\t# seeds = ['', seed]\n\t# Seed is always an argument\n\tfor j in xrange(len(seed)):\n\t\tif seed[:j] == seed[j+1:] or seed[:j] in '1-2,1-3':\n\t\t\tshuffled = shuffle(list(map(int, seed[:j+1:-1], seed[j+1:]-1)))\n\t\t\treturn shuffled\n\telse:\n\t\tsh"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\treturn list(seq)\n\ndef _shuffle(seq: Sequence[str]) -> Generator[Sequence[str], None, None]:\n\tfor x in idxs(seq):\n\t\tyield seq, None, x\n\ndef _shuffle_index(indices: Index, seq: Sequence[str]) -> int:\n\t# TODO\n\treturn 0\n\nif __name__ == \"__main__\":\n\tseq, label, indices = _shuffled(['foo', 'bar', 'baz'])\n\tprint(label)\n\n#ifndef _LINUX_PCI_IDS_H_\n#"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tshuffle_list = []\n\tfor elem in seq:\n\t\ttemp = elem.split(\"/\")\n\t\tshuffle_list.extend(reverse(temp))\n\treturn shuffle_list\n\n\nclass _ShuffleMeta(type):\n\t__metaclass__ = TypeMeta\n\n\n@_Shuffled.check\ndef _shuffled_shuffle(input: Sequence[str], seed: int = 0):\n\tfor a in input:\n\t\tif _shuffle(seed):\n\t\t\tyield seed\n\n\nclass Shuffle(object):\n\tdef __init__(self, *"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tfor r in seq:\n\t\tshuffle(r)\n\nif __name__ == \"__main__\":\n\tprint(\"Ran\", len(list(sorted(shuffled(list(range(9)))))[1:]))\n\n#if NETSTANDARD2_0 || NET461\nusing System.Runtime.InteropServices;\n//#if NETSTANDARD2_0\n//#if NETCOREAPP3_1\n//\tusing Type = System.Type;\n//#else\n//\tusing Type = System.Console.Pdb.Native.Type;\n//#endif\n//#else\n\t/// <summary>"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tdef _shuffle(seq: Sequence[str]) -> list[str]:\n\t\tfor index, item in reversed(seq):\n\t\t\titem = os.urandom(len(item))\n\t\t\tseq[index] = item\n\t\treturn seq\n\treturn _shuffle(seq[::-1])\n\ndef _shuffble(seq: Sequence[str]) -> str:\n\t\"\"\"\n\tJust shuffle\n\t\"\"\"\n\tdef _shuffled(seq: Sequence[str]) -> list[str]:\n\t\tfor index, item in enumerate(seq):\n\t\t\t"}]}
{"task_id": "62e4fb6585ea98643089032b", "prompt": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tlines = s.split(\"\\r\\n\")\n\tlast_match = None\n\tfor line in lines:\n\t\ttry:\n\t\t\tmatch = version.match(line)\n\t\t\tif not match:\n\t\t\t\tcontinue\n\t\t\telif match.group(1) is not None and match.group(\"version\"):\n\t\t\t\tversion = match.group(1).split(\"-\")[-1].strip()\n\t\t\t\trevision = match.group(2)\n\t\texcept AttributeError:\n\t\t\tcontinue\n\t\tif revision.startswith(\"v\"):\n\t\t\t# v:3"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tif not s:\n\t\treturn ()\n\n\ts = s[3:]\n\tfor (i, t) in enumerate(s):\n\t\tif t!= '$':\n\t\t\treturn (i, s[:i])\n\n// Copyright (c).NET Core Community. All rights reserved.\n// Licensed under the MIT License. See License.txt in the project root for license information.\n\nusing System;\nusing System.Collections.Generic;\nusing Newtonsoft.Json;\n\nnamespace TeamsOfThrones.Logging\n{\n    [Serializable]\n  "}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t# convert the format-string back to int\n\tid_, version = 0, 0\n\tfor i, c in enumerate(s.trim()):\n\t\tversion, id_ = struct.unpack(\"<BBBBBBBB\", s[i:i+2])\n\t# return -1 if the format is not an integer\n\tif (version > 11) == (id_ < 11):\n\t\treturn -1, -1, 0\n\t# if an int, try an int\n\tif (len(s) >= 2) and (len(s[1:]) == 8):\n\t\t"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tversion1 = get_version()\n\tversion2 = get_version()\n\tversion_string = \"\"\n\t#print(\"comparing\",s)\n\n\t# The comparison can be removed if it is a better match to the\n\t# version of the tool (usually version 1.0.x)\n\tif s == version1 and len(version2) < len(s):\n\t\treturn (version1 - (int(version2) + int(version3)),)\n\telif s == version2 and len(version1) < len(s):\n\t\treturn ("}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tret = []\n\tlast_digit = s[len(s) - 1]\n\tfor d in s:\n\t\tif d.isdigit():\n\t\t\treturn int(d) - last_digit, None\n\t\tif d[0] in '0123456789':\n\t\t\treturn int(d) - last_digit, None\n\treturn 0, None\n\n\ndef checkfile(filename: Type[str] = str, log: bool = True):\n\t\"\"\"\n\tCheck the file to make sure it looks valid.\n\t\"\"\"\n\ttry:\n\t\twith open"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tversion = s.index('=') + 1\n\tif version < 1:\n\t\tversion = 1\n\tnew = (int(s[0]) * 10) + int(version)\n\treturn new, s[version:]\n\n/* This tests the pthreads function */\n/* Copyright (C) Jan 2006 */\n/* This file is part of Cocktail.\n** This file is free software; you can redistribute it and/or modify\n** it under the terms of the GNU General Public License as published by\n** the Free Software Foundation; either version 2 of the License, or\n"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tfor v in s.split():\n\t\tif not v: continue\n\t\tif v[0] == 'v' and v[-1]!= '-': continue\n\n\t\ts,l = s.split(\"-\")\n\t\tv1,v2 = map(int, v[1:-1].split(\"-\"))\n\t\tv1 = int(v1) if v1 == 1000 else (v1, v1%100)\n\t\tif v <= l: return (1, v1)\n\t\treturn (v1, int(v2) if v2"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\ts = str(s)\n\tif s[0:2] == \".\":\n\t\ts = s[2:]\n\n\ts = s[_find_string_if_empty(version_pattern_list, \"stable\") :]\n\ts = s.rstrip().upper()\n\ts = s.split(\";\")[1]\n\treturn int(s)\n\n\ndef detect_version(s: str) -> int:\n\t\"\"\"\n\tDetectes if the python version has been updated from the\n\tscripting language with the 'development' keyword and a correct\n\t"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tver, prerelease = \"\", \"\"\n\tfor n in s.split(\"|||\"):\n\t\tif len(n) in (2, 3):\n\t\t\tif n[1]!= \"v\" or n[2]!= \"r\":\n\t\t\t\treturn 0, n\n\t\telse:\n\t\t\tif ver == \"\":\n\t\t\t\tcontinue\n\t\t\tif ver[0] == \"r\":\n\t\t\t\tprerelease = n[2]\n\t\t\tif prerelease[0] == \" \":\n\t\t\t\tprerelease = \"\"\n\t\t\tif prerelease[1] =="}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tvers = s.split('.').pop(0).split('.')[0].strip()\n\treturn int(vers), [int(v) for v in [v for v in (s, [p] if not p)]]\n\ndef is_py34(source: str) -> bool:\n\tfor ext in '.3gT'.split():\n\t\tif source.lower().endswith(ext):\n\t\t\treturn True\n\treturn False\n\n#------------------------------------------------------------------------------\nclass PythonVersion(BaseVersion):\n    pass\n\n#------------------------------------------------------------------------------\nclass Python2Version(BaseVersion"}]}
{"task_id": "62e4fb4d85ea9864308902e7", "prompt": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t# Remove any windows shebang sequences, the regex is so complicated :(\n\tglob_glob = {\n\t\t'bash_bash.bat',\n\t\t'bash_csh.bat',\n\t\t'chmod_r.bat',\n\t\t'file_x86_64.bat',\n\t\t'echo',\n\t\t'echo off',\n\t\t'make',\n\t\t'runk',\n\t\t'yum',\n\t\t'sh\\x00e',\n\t\t'make\\x00e',\n\t\t'man',\n\t\t"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\treturn {\n\t\t'[x]': '{}:{}'.format(','.join(cmd), x)\n\t}\n\n\ndef get_output(args: tuple) -> str:\n\treturn convert_unicode_error(get_error(args))\n\n\ndef make_command_string(args: tuple) -> str:\n\tflags = re.match(r'([A-Za-z0-9]+):([A-Za-z0-9]+)[:=]{}?.*'.format(\n\t\targs[2], args[1]))\n\tflags ="}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tret = cmd\n\ttry:\n\t\twhile True:\n\t\t\tline = ret\n\t\t\t# if shebang, handle the first one using \\r \n\t\t\tif line and line.startswith('+') and len(line)==1 and not (line[0] in ('\\r', '+')):\n\t\t\t# \tprint \"shebang line was empty after \\r\"\n\t\t\tret += line\n\t\t\t# if shebang is not found and shebang is not escaped\n\t\t\tif not (ret[0] in ('\\r', '+') and len(ret)>1):"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tif len(cmd) > 2:\n\t\tcmd += (cmd[-2] if len(cmd) > 2 else '')\n\t\tif len(cmd) > 1 and (len(cmd[1]) == 1 or cmd[1] == '\\n'):\n\t\t\tstripped = ''.join(cmd[1:])\n\t\t\tcmd = stripped\n\tif len(cmd) > 0:\n\t\tcmd = cmd[0]\n\ndef get_options_from_command(cmd, options=None):\n\t\"\"\"gets options from command\"\"\"\n\toptions = ''\n\tif"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tif 'PYTHONPATH' not in sys.platform:\n\t\treturn normalize_cmd(cmd)\n\n\t# this is necessary because shebangs are a bit more complex so we need\n\t# to clean the shebangs first\n\treturn normalize_cmd(list(cmd))\n\nimport * as React from'react';\nimport { shallow } from 'enzyme';\nimport { GenderMenu } from './GenderMenu';\nimport { Money } from '../money';\n\ndescribe('GenderMenu', () => {\n\tconst props = {\n\t\tchildren: [GenderMenu"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\torig = cmd\n\targs = cmd[0].split(';')\n\tcmd =''.join(cmd[1:])\n\tcmd = cmd.replace('''', \"\\''\").replace(\"``\", r\"\\``\")\n\t\n\tif len(args)!= 1:\n\t\traise ValueError(\"Invalid command: %s (expected single-word command)\" % repr(cmd))\n\tcmd = args[-1]\n\t\n\tif len(cmd) in ('-', '-') or len(cmd)!= 2:\n\t\traise ValueError(\"Invalid command: %s (only single-word she"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t#if os.name == \"nt\":\n\t#\treturn cmd\n\n\tparts = _re.findall(r'\\\\' + cmd[0] + r'\\\\\\s*\\b', cmd[1])\n\t\n\tassert len(parts) == 3\n\tshebangs = []\n\tfor part in parts:\n\t\tassert part, part\n\t\tshebangs += [part.strip('=')]\n\tassert len(shebangs) == 1\n\tshebang = ('\\\\' + shebangs[0] + r'\\\\\\s*\\b')\n\treturn"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t# Make sure shebang is a.\n\t# this may use an extra prefix to make them\n\tif isinstance(cmd, list):\n\t\t# if it is a space, make the whole shebang as a space (like on windows!)\n\t\t# TODO: make the suffix a space for more details\n\t\tif'' in cmd:\n\t\t\tcmd = (''+ cmd)\n\treturn cmd\n\n\ndef check_for_windows_python3():\n\t# This function returns True if all the possible Python2/3 pathnames\n\t# exist in a Windows registry."}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tif'shebang' in os.environ:\n\t\tcmd = ['shebang']\n\telif 'python' in os.environ:\n\t\tcmd = ['python']\n\telif'mingw' in os.environ:\n\t\tcmd = ['microsoft']\n\telif 'linux' in os.environ:\n\t\tcmd = ['linux']\n\telif 'bsd' in os.environ:\n\t\tcmd = ['bsd']\n\telif os.path.isdir(os.path.abspath('which')):\n\t\tcmd = ['dot']\n\telse:\n"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\treturn [command.split(\":\")[0] : args[0] + \"'/i'\", command.split(\":\")[1] :\n\t\targs[0] + command.split(\":\")[1]\n\n// Copyright 2001-2019 Crytek GmbH / Crytek Group. All rights reserved.\n\n#pragma once\n\n#include \"CrySchematyc/Script/IBackoff.h\"\n#include \"CrySchematyc/Sandbox/ScriptAPI.h\"\n\nnamespace Schematyc\n{\n\nclass IBackoffModule : public IBusModule\n{\n"}]}
{"task_id": "62b8d27a48ba5a41d1c3f4c6", "prompt": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef func_proxy(self, func, args=None, kwargs={}):\n\t\tfunc_proxy.__class__ = self.__class__\n\t\treturn func_proxy.__func__(self, func, args, kwargs, cache=cache)\n\treturn memoize(func_proxy, func, args, kwargs)\n\n/*\n * Copyright (c) 2017-2020 Michael Chaban. All rights reserved.\n * Original game is written by Core Design Ltd. in 1997.\n * Lara Croft and Tomb Raider are trademarks of Square Enix Ltd.\n *\n"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\treturn cached\n\n@cache.cached_get\n@cached_method\ndef cached_get(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tDecorator to wrap a cached get and return results in a cache.\n\t\"\"\"\n\treturn cached_get(cache, key=key, lock=lock)\n\ndef cache_has(cache, key, value, lock=None):\n\t\"\"\"\n\tDecorator to check if a cache has a key or do a function with\n\tthe value.\n\t\"\"\"\n\tif value is None:\n\t\treturn cache.contains"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.memoized_memoizer = memoize\n\t\tmemoize(func, cache=cache)\n\t\treturn func\n\treturn decorator\nMemoizableClass.memoized_func = memoize\n\nclass Uncacheable(MemoizableBase):\n\tdef __init__(self, value, key=hashkey, lock=None):\n\t\tself.value = value\n\t\tself.key = key\n\t\tself.lock = None\n\tdef __call__(self, value):\n\t\tif self.lock is None:\n\t\t\tself."}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tif lock == None:\n\t\treturn cached\n\t# The memoize function is based on the method of the memoization class\n\tmemo = cache.__get__(name, cache_entry)\n\twhile getattr(memo, \"memoize\", None):\n\t\tmemo = memo(lock)\n\treturn memo\n\n\n@cached\ndef memoize(lock=None):\n\t\"\"\"\n        A convenience class to wrap a function that saves results in a\n        cache.\n\t\"\"\"\n\tmemo_cls = memoize_method("}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\tcache[key] = functools.partial(\n\t\t\tcacheview,\n\t\t\t(**{\n\t\t\t\t\"lock\": lock,\n\t\t\t\t\"cache\": cache,\n\t\t\t\t\"args\": args,\n\t\t\t\t\"kwargs\": kwargs,\n\t\t\t}),\n\t\t)\n\t\treturn wrapper\n\n\treturn decorators.memoize(wrapper)\n\ndef cache_memo(cache, key=None, lock='default', cache_ttl=3600):\n\t\"\"\"\n\tReturns a dict that keeps cache's data values without"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef memoizer(*args, **kw_args):\n\t\t\"\"\"\n\t\tStores `self` in the cache of memoized cache entries.\n\t\t\"\"\"\n\t\tcache[key] = args\n\t\treturn cache\n\n\twith memo(cache.get, timeout=1000) as cached:\n\t\twith getitem(), memoizer(cached(lock))():\n\t\t\treturn cached\n\n\tif cache.getordelete:\n\t\tmemo = getitem()\n\t\tprint(cache.getordelete())\n\telse:\n\t\tmemo = cache.get\n"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tfrom time import sleep\n\tdef cache_function(func, *args, **kw):\n\t\t\"\"\"\n\t\tDecorator to wrap a function that keeps the cache.\n\t\t\"\"\"\n\t\t@cached\n\t\tdef cached_function(cache, key=hashkey, lock=None):\n\t\t\t\"\"\"\n\t\t\tDecorator to wrap a function with a memoizing callable that saves\n\t\t\tresults in a cache.\n\t\t\t\"\"\"\n\t\t\t\n\t\t\tif lock is not None:\n\t\t\t\tlock()\n\t\n\t\t\tfunc(*args, **kw)\n\t\n\t\t\tcache[key"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tmemoized = partial(cached, key=key)\n\treturn memoize(memoized, cache=cache, lock=lock)\n\npackage com.android.server.wifi;\n\nimport android.hardware.wifi.supplicant.V1_0.ISupplicant.getGroupCipherCallbackByAddr;\nimport android.hardware.wifi.supplicant.V1_0.ISupplicantNetworkSpecifier;\nimport android.hardware.wifi.supplicant.V1_0.ISupplicantStaNetwork;\nimport android.hardware.wifi.supplicant.V1_0.ISupplicantSta"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef cached_func(func, context=None):\n\t\tif ctx is None: ctx = None\n\t\treturn super(cached, cache).cached_func(func, context)\n\n\tcached_object = cached_func\n\treturn cached_func\n\n\ndef cached_with_memoization_support(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tDecorator to wrap a function to automatically save results in a\n\tcache in a memoized state.\n\t\"\"\"\n\tdef cached_func(func, context=None):\n\t\tif ctx is None"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef newcache(cache):\n\t\tif lock == None:\n\t\t\tlock = Lock()\n\t\treturn memoize(key=lambda: lock(cache.get(key, cache.get(key, cache.get(key, cache.get(key, cache.get(key, cache.get(key, cache.get(key, cache.get(key, cache.get(key, cache.get(key, cache.get(key, cache.get(key, cache.get(key, cache.get(key, cache))))))))))))] ) )(cache.get("}]}
{"task_id": "62b8d24048ba5a41d1c3f49f", "prompt": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef cached_wrapper(*args):\n\t\treturn cached(**{'maxsize':maxsize, 'ttl':ttl, 'timer': timer})\n\n\treturn cached_wrapper\n\nclass Cache(object):\n\tdef __init__(self):\n\t\tself.max_items = 2 ** 11 # 2 entries in default size...\n\t\tself.lru_max = 2**16  # 2 entries in lru-size range\n\t\tself.expire_time = 60  # seconds\n\t\tself.key_func = timeit.nop\n\t\tself.ttl_cache = None# TTL"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef _cache_function(f):\n\t\treturn self.memoize(f())\n\treturn _cache_function\n\ndef make_ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\treturn _make_memoized_cache(maxsize, ttl, timer, typed)\n\n#ifndef JEMALLOC_INTERNAL_JEMUPY_H_INCLUDED\n#define JEMALLOC_INTERNAL_JEMUPY_H_INCLUDED\n\n/* These four typedef's that may be used with jemupd. */\ntypedef char"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tfrom functools import lru_cache\n\tdef ttl_cache(maxsize=32768, ttl=1024, timer=time.monotonic, typed=False):\n\t\treturn cache_helper(maxsize)\n\t\"\"\"\n\tWraps a function that retrieves the time elapsed from the last call\n\tof ``ttl_cache``. If ``typed`` is True, then the function takes\n\ta single argument that is the type of the arguments passed.\n\tThis is done by supplying the type of *args* passed to\n\t``ttl_cache``.\n\t\"\"\"\n\tfrom typed_lru import time"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorator(func):\n\t\tmemoization = {}\n\t\tdef _memoize(*args, **kwargs):\n\t\t\tmemoization[args[0]].add(args[0] in memoization)\n\t\t\tmemoization[args[1]].add(args[1] in memoization)\n\t\tdef _wrapped(func):\n\t\t\tif typed:\n\t\t\t\treturn func.copy()\n\t\t\treturn _memoize(func, args[2:], **kwargs)\n\t\tmemoized_list = list(_wrapped(func))\n\t\tmemoization[_wrapped(func"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef _wrapper(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif typed:\n\t\t\t\tif not callable(func):\n\t\t\t\t\traise TypeError(\n\t\t\t\t\t\t'The cache decorator must return a function.  You might '\n\t\t\t\t\t\t'pass a callable object that implements the cache '\n\t\t\t\t\t\t'method.  For instance: %s(func)' % func.__name__)\n\t\t\t\tcache = func(*args, **kwargs)\n\t\t\t\tcache.wrapped = True\n\t\t\telse:\n\t\t\t\tif"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\"\"\"\n\tThis decorator will be used to wrap a function to run whenever a\n\tcache_ttl_cache() function is called within the same code block, e.g. when\n\t__cached__() returns, as opposed to in the `__getresponse__(x) XXXX()`.\n\n\tArgs:\n\t\tmax_value (int) : Maximum number of entries to preserve.\n\t\tttl (int) : Number of seconds to allow for the LRU timeout.\n\t\ttimer (object): Reuse the previous timer.\n\t\ttyped (boolean) : Is the function returning a variable whose"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef wrapper(fn, *args, **kwargs):\n\t\t\"\"\"Decorator on the wrapped function.\"\"\"\n\t\tif not fn:\n\t\t\treturn False\n\t\treturn fn(args, **kwargs)\n\treturn wrapper\n\n\n@registry.register(\"time_queue\")\nclass Queue(object):\n\t\"\"\"Queue that can hold data on demand.\n\n\tWith no time_remaining parameter it will return until\n\t:meth:`time_queue.empty` is called.\n\n\tAn example of an empty queue, where the empty state is stored\n\tin a :class:`time"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef __call__(func, *args, **kwargs):\n\t\t\"\"\"\n\t\tThe first argument (`func`) to this decorator is a callable to\n\t\thold the result (obj) and the remaining arguments and their default\n\t\tvalues.\n\n\t\tIf `kwargs` are provided it should be forwarded to the resulting\n\t\tfunction.\n\t\t\"\"\"\n\t\tif kwargs:\n\t\t\tfunc = functools.partial(func, *args, **kwargs)\n\t\telif self.cache:\n\t\t\ttry:\n\t\t\t\tfunc = self.cache[func]\n"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef _temp_wrapper(f):\n\t\t\"\"\"Decorator that returns a `temp_cache()` wrapper for\n\t\tan `func` when the decorated `func` does not support the\n\t\tTTL (in this case `ttl`).\n\t\t\"\"\"\n\t\t@functools.wraps(f)\n\t\tdef wrapper(self, func, *args, **kwargs):\n\t\t\t\"\"\"Decorator that returns the decorated function.\n\t\t\t``temp_cache(maxsize) == \\\n\t\t\t\t_temp_wrapper(func(maxsize, ttl, timer=timer, typed=typed)``"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef ttl_cache_wrapper(func, max_entries, ttl, timer):\n\t\treturn cache_memoize(func, max_entries, ttl, timer)\n\n\tif typed:\n\t\treturn ttl_cache_wrapper(ttl_cache, ttl, timer)\n\telse:\n\t\treturn cache_memoize(ttl_cache_wrapper, ttl, timer)\n\n\n@cached_property\ndef ttl_max(value):\n    \"\"\"\n    Return the maximum number of seconds in the TTL cache to live.\n\n    Note that the"}]}
{"task_id": "62b8d23b48ba5a41d1c3f49a", "prompt": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t_memoizing_func = memoize_function\n\n\tdef wrapper(func):\n\t\tmemoized_func_result = memoizing_func(_memoizing_func, maxsize, func)\n\t\tmemoized_func_result = check(memoized_func_result, \"An error returned from an memoizing function\")\n\t\treturn memoizing_func(memoized_func_result)\n\n\treturn wrapper\n\n#\n# This script enables a python 2 interactive interpreter.\n#\n\nimport sys\nif sys.version_info[0] == 2:\n\timport http.server"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef f(iterable, *args, **kwargs):\n\t\t\"\"\"\n\t\tMethod decorator:\n\t\t\t...\n\t\t\"\"\"\n\t\tif typed:\n\t\t\t# The above is not correct, but we are implementing 2 functions instead.\n\t\t\tmru_cache = pickle.dumps\n\t\t\tclass _LazyCache(pickle.Proxy):\n\t\t\t\tdef __reduce__(self):\n\t\t\t\t\t# Use pickle.loads to avoid the\n\t\t\t\t\t# pickle.loads (because the latter was added)\n\t\t\t\t\tres = __new__\n\t\t\t\t\tif isinstance(self.__obj,"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef mru_cache_decorator(func, size=_DEFAULT_MRU_SIZE, typed=False):\n\t\t# When not typed, `size` should be specified. Otherwise, we should\n\t\t# just make sure it's a certain size. But in the general case, only\n\t\t# one size can be specified.\n\t\tif size is _DEFAULT_MRU_SIZE:\n\t\t\tmaxsize = size\n\t\telse:\n\t\t\tmaxsize = len(func)\n\t\t\n\t\twrapped = func()\n\t\t\n\t\t@functools.wraps(wrapped)\n"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef my_cache():\n\t\treturn {\n\t\t\t'key': 'x',\n\t\t\t'value': 42,\n\t\t}\n\treturn my_cache\n\n\ndef some_code():\n\treturn 42\n\n\n@pytest.fixture\ndef cache_factory(func):\n\timport functools\n\n\t@functools.wraps(func)\n\tdef wrapped_func(*args, **kwargs):\n\t\t\"\"\"\n\t\tReturn a memoizing callable with a memoizing `some_code()` object\n\t\tand a function with a memoized `some_"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef wrapper(func, **kwargs):\n\t\t\"\"\"\n\t\t(func, kwargs) -> (new_cache_fun, new_cache_args)\n\t\t\"\"\"\n\t\tnew_cache_fun = memoize_function(func, **kwargs)\n\t\tnew_cache_args = list(chain(functools.partial(dict, arg for arg in kwargs.values() if not isinstance(arg, (tuple, list)))) if isinstance(kwargs, dict) else kwargs for args in kwargs.values())\n\t\tnew_cache_kwargs = list(chain(functools."}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tif typed:\n\t\tpass\n\telse:\n\t\tpass\n\n\nclass memoize_function(object, metaclass=abc.ABCMeta):\n\t\"\"\"\n\tDecorator used to apply a Metaclass on a class to a memoized function.\n\t\"\"\"\n\tdef __init__(self, function_class, typed=False):\n\t\tself._function_class = function_class\n\t\tself._typed = typed\n\n\tdef mru_cache(maxsize=16):\n\t\t\"\"\"\n\t\tApply a function and store the result in a memoizing callable."}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef wrap(func):\n\t\tpass\n\tmemo = {}\n\tmemo.update(lambda *args: (mru_cache(*args), func(*args)))\n\treturn lazy_memoize(memo)\n\ndef memoize(fn):\n\twrapped = fn()\n\tresult = wrapped\n\tmemo = {}\n\tmemo.update(wrapped)\n\treturn result\n\ndef lazy_memoize(func, blocknum=lambda *args:func(*args), blocknum_max=None):\n\tif blocknum_max is None:\n\t\treturn func, blocknum"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef memoizer(func):\n\t\tif typed:\n\t\t\treturn memoize(*(func(x) for x in cache))\n\t\telse:\n\t\t\treturn func\n\n\tcache = []\n\tif cache!= []:\n\t\tcache.append(\"mru_cache\")\n\telif typed:\n\t\tcache = memoizer(mru_cache)\n\n\treturn cache\n\n\ndef memoizer(func):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef memo(f):\n\t\tif typed:\n\t\t\treturn f\n\t\telse:\n\t\t\treturn re.search(f, _cache).group(0)\n\n\tp.memo(memo)\n\n\n@registry.register\nclass _MemoizedDict(_MemoizedDictStorage):\n\n\tdef __init__(self, cache):\n\t\tdict.__init__(self)\n\t\tself.cache = cache\n\n\tdef __getitem__(self, key):\n\t\tdef safe_key(x):\n\t\t\treturn key, x\n\t\tif not safe"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef memoize_dict(*args, **kwargs):\n\t\t\"\"\" A memoizing function\n\t\treturn the (possibly truncated) copy of ``args``. Useful for\n\t\ttesting and debugging.\n\t\t\"\"\"\n\t\tnew = memoize(maxsize, *args, **kwargs)\n\t\treturn new\n\treturn memoize_dict\n\n\nclass _MemoizedFn:\n\tdef __init__(self, memo):\n\t\tself._memoized_function = memo\n\n\tdef __call__(self, *args, **kwargs):\n\t\tself._memoized_function(*args"}]}
{"task_id": "62b8d23948ba5a41d1c3f498", "prompt": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef lru_memoize(maxsize_, arg=(), memo_=None, typed=False, **kwargs):\n\t\tif typed is False:\n\t\t\traise NotImplementedError(\"You need to define 'typed=' argument\")\n\t\tif not callable(lru_cache):\n\t\t\traise TypeError(\"LRU cache type must implement the 'lru_cache_function' method\")\n\t\tif callable(maxsize_):\n\t\t\tif type(maxsize_) is int:\n\t\t\t\tmaxsize = getattr(maxsize_, 'val'+((maxsize_.upper()[:1].upper())[\n\t\t\t\t\t'memo'] if typed"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef lru_cache_lru(maxsize):\n\t\tfrom functools import lru_cache\n\t\tlru_cache_lru = lru_cache if typed else lru_cache[0]\n\t\treturn lru_cache_lru(maxsize)\n\t__metaclass__ = LRUCache\n\treturn lru_cache_lru\n\n#pragma once\n/*\n* Wire\n* Copyright (C) 2016 Wire Swiss GmbH\n*\n* This program is free software: you can redistribute it and/or modify\n* it under the terms of the GNU General Public License as published by\n* the Free Software"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef lru_cache_as_function(func, *args, **kwargs):\n\t\treturn func(*args, **kwargs)\n\tif typed:\n\t\tlru_cache_as_function = lru_cache_as_function\n\treturn lru_cache_as_function\n\ndef _get_typecheck_dict(object):\n\tif not isinstance(object, dict):\n\t\tobject = {}\n\ttry:\n\t\tobject.update(dict.fromkeys(object))\n\texcept (KeyError, TypeError):\n\t\tobject = None\n\treturn object\n"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef _lru_memoize(x):\n\t\t\"\"\"\n\t\tIf this is relying on a Least Recently Used key,\n\t\tuse a memoizing function, i.e. it is in an LRU bucket or similar.\n\t\tThis function needs to be a single callable, i.e. one can not modify\n\t\tthe original value or the same way that you would.\n\t\t\"\"\"\n\t\treturn x if x is not None else _lru_cache().__getitem__\n    return _lru_memoize\n\t\n\"\"\"\n\tDecorator to wrap"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef _func(func, *args, **kwargs):\n\t\t\"\"\"\n\t\t`func` is just the decorator. Can be a callable which takes\n\t\ta `maxsize`, `wrappedfunc` and `args` and returns a decorated function.\n\t\t\"\"\"\n\t\tfunc = MagicCallable(func)\n\t\treturn Func(maxsize, wrappedfunc, **kwargs)\n\n\t# Function decorator that allows a limit to the number of function calls.\n\t@functools.wraps(func)\n\tdef func_cache(maxsize):\n\t\treturn _func(lambda"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tif typed:\n\t\treturn memoize(lambda d : iter(filter(not indict, d)),\n\t\t\t\t\t   maxsize=maxsize, lru=True)\n\treturn memoize(lambda d : iter(filter(indict, d)),\n\t\t\t\t   maxsize=maxsize, lru=True)\n\n\ndef _getdict(key, default):\n\t\"\"\"A decorator for constructing the dictionary with passed arguments.\"\"\"\n\tdef __new__(cls, items=list(), *args, **kwargs):\n\t\twrapped = cls.__new__(*args, **kwargs)\n\t\t"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t@functools.wraps(functools.lru_cache)\n\tdef lru_cache_wrapped(self, maxsize, typed=False):\n\t\tif isinstance(maxsize, int):\n\t\t\tmaxsize = (maxsize-1)//2+1\n\t\tfunc = functools.lru_cache & maxsize\n\t\tassert typed in kwargs('typed', False)\t\n\t\treturn func(maxsize, typed)\n\treturn lru_cache_wrapped\n\n##------------------------------------------------------------------------\n## General Utilities\n##------------------------------------------------------------------------\n\nclass BoundedList(list):\n\t\"\"\"\n\tA"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef lru_memo(maxsize=64):\n\t\tif typed:\n\t\t\tname = \"size\"\n\t\t\treturn fn.memo[name](maxsize=maxsize, count=maxsize)\n\t\telse:\n\t\t\treturn fn.memo[name]()\n\treturn lru_memo\n\ndef memoize(maxsize=128):\n\t\"\"\"Return a new memoized function.\"\"\"\n\ttry:\n\t\tret = lru_cache(maxsize)\n\texcept Exception:\n\t\tpass\n\n\tif not hasattr(ret,\"memo\"):\n\t\traise ValueError(\"The memo has no '"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tif typed:\n\t\t# to maintain a memoization of function results:\n\t\treturn Lambda(maxsize=maxsize)(fn)\n\treturn memoizes(maxsize=maxsize)\n\n\ndef lru_cache_iterator(maxsize=4096, typed=False):\n\t\"\"\"Decorator to convert a callable to a list of results so it can be\n\tpassed around for further use: e.g. for a list of tuple instead of mapping\n\tto an iterator.\"\"\"\n\n\tdef forward(fn):\n\t\tfn()\nreturn (lambda results: results.append(results[0]) for _,"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef memoize_lru_cache(maxsize):\n\t\tcache = weakref.WeakKeyDictionary()\n\t\twith cache:\n\t\t\tif typed:\n\t\t\t\tcache.insert(0, WeakKey(maxsize * 3))\n\t\t\telse:\n\t\t\t\tcache.update(maxsize)\n\n\t\treturn cache.h_dict()\n\n#ifndef __COMMONAPI_H__\n#define __COMMONAPI_H__\n\n#include <pthread.h>\n#include \"config.h\"\n#include \"common/log.h\"\n#include \"config.h"}]}
{"task_id": "62b8d23748ba5a41d1c3f496", "prompt": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef to_lfu(f):\n\t\tif not typed:\n\t\t\treturn f\n\t\tif cache.cache:\n\t\t\treturn cache.cache.get(f.__name__) # type: ignore\n\t\ttry:\n\t\t\treturn cache.cache.set(f.__name__, f())\n\t\texcept memoryview.Error as e:\n\t\t\t# XXX this leak\n\t\t\traise ValueError(\"Cannot cache results of this function. Make sure you provide a function with the right name\")\n\tnew = decorator\n\tif new:\n\t\tcache = new\n\t\t\n"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef decorated(func, *args, **kw):\n\t\told_maxsize = kw.pop('maxsize', maxsize)\n\t\targs = args + (maxsize,)\n\t\tres = func(*args, **kw)\n\t\t# LFH: This will be removed if `maxsize` is a number.\n\t\t#\t\tIt's a workaround for a python dependency, see:\n\t\t#\t\thttps://bugs.python.org/issue144490\n\t\tif old_maxsize!= maxsize: kw['maxsize'] = old_maxsize\n\t\treturn res\n"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef wrapped(func):\n\t\tdef wrapped(*a, **dict): pass\n\t\treturn wrapped.__name__, lfu_cache(maxsize=maxsize), func\n\treturn wrapped\n\n\ndef lfu_cache():\n\t\"\"\"\n\tDecorator used to limit memory usage of LFu.\n\t\"\"\"\n\tdef _memcache_decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t__memo = pickle.dumps(func(*args, **kwargs))\n\t\t\tmemo = lambda _x: func(*_memo)\n\t\t\twith memo() as memo_obj"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\tif typed:\n\t\t\treturn func\n\t\telse:\n\t\t\tdef wrapper(key, *args, **kwargs):\n\t\t\t\tif hasattr(func, key) and not isinstance(func, dict):\n\t\t\t\t\tyield func(key, *args, **kwargs)\n\t\t\t\telse:\n\t\t\t\t\tfunc(*args, **kwargs)\n\t\t\treturn wrapper\n\n\treturn decorator\n\ndef lfu_cache_clear(maxsize=128):\n\t\"\"\"\n\tClears the entire LFU cache.\n\n\tThis is equivalent to `L"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef _as_memoized(func):\n\t\tdef func_as_callable(arg):\n\t\t\tresult_func = _MemoizedCache[type(arg)]\n\t\t\tif result_func:\n\t\t\t\tfunctools.partial(func, result_func)\n\t\t\telse:\n\t\t\t\tfunctools.partial(func, arg)\n\t\t\treturn result_func\n\t\treturn func_as_callable\n\n\treturn weakref.WeakValueDictionary([x for x in _as_memoized(cache)() if not x])\n\n#pragma checksummed KEMss"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef memoize(func, key, **kw):\n\t\tif key > maxsize:\n\t\t\traise ValueError(\"Invalid key given to \"\n\t\t\t\t\t\"lefu_cache: {}\".format(key))\n\t\tif len(func)!= key:\n\t\t\traise ValueError(\"Invalid function given to \"\n\t\t\t\t\t\"lefu_cache: {}\".format(func))\n\t\tif not isinstance(key, (int, long)):\n\t\t\tkey = ((key,) + key).int\n\t\treturn func.__dict__[key] = func, kw\n\treturn memoize\n\n"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t@memoize\n\tdef lfu_cache(maxsize=_DEFAULT_CACHE_MAXSIZE):\n\t\tdef lfu_cache_wrapper(func, limit):\n\t\t\tif limit:\n\t\t\t\tf = functools.lru_cache(maxsize=maxsize, key=_cache_key, replace=_loaders)\n\t\t\t\tif typed:\n\t\t\t\t\tnew_func = lambda: func((f.get(key, func(func(key)))[0], f.get(key)))\n\t\t\t\t\tf.set(key, new_func, block=func)\n\t\t\t\telse:"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef _wrapper(func):\n\t\t\"\"\"\n\t\tWrap a given function with a memoizing callable. If this exists,\n\t\tthen reuse its callable and return it.\n\t\t\"\"\"\n\t\tif typed:\n\t\t\tfunc = functools.update_wrapper(func, lambda x: x, None, _kwargs)\n\t\t\treturn func\n\t\treturn functools.update_wrapper(func, _kwargs, None, _cache = _cache)\n\tdel typed\n\n\tdef _memoize_args(func):\n\t\t\"\"\"\n\t\tGet and cache the"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(cache_method):\n\t\t\"\"\"\n\t\tReturns an instance of `cache_method` returning a method that\n\t\tdoes not save up to `maxsize` results.\n\t\t\"\"\"\n\t\treturn cache_method()\n\n\treturn decorator\n\n\ndef to_lower(s):\n\t\"\"\"\n        Converts the provided string ``s`` to a single-letter lowercase\n\t\tstring.\n\n\t\tReturns its unchanged unchanged value.\n\n\t\tFor additional control over the return value, see\n        the"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef decoration(*args, **kwargs):\n\t\t\tcached_result_size = kwargs.pop('cached_result_size', 0)\n\t\t\tif lfu_cache.maxsize < cached_result_size: # avoid long lines\n\t\t\t\tcache_lines = lfu_cache.maxsize * lfu_cache.cached_hits_count\n\t\t\t\tcache_lines_dict = {}\n\t\t\t\tfor line in cache_lines:\n\t\t\t\t\tcache_lines_dict[line] = len(cache_lines[line])\n\n\t\t\t\t\t#"}]}
{"task_id": "62b8d22f48ba5a41d1c3f488", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\treturn popitem(self.popvalue())\n\n\t# ____________________________________________________________________________\n\tdef save_in__sys__package_______import__(self, save=1):\n\t\traise Exception(\"Error: save() is an invalid value\")\n\nclass System: pass\nclass System_package_name(): pass\nclass System_package__name(object):\n\tdef __init__(self,name): self.name = name\n\tdef __repr__(self): return self.name\n\tdef save_in(self,save): raise Exception(\"Error: save() is an invalid value\")\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tself._lock_cursor()\n\tresult = self._cursor.popitem()\n\tself._end_cursor()\n\treturn result\n\ndef pop(self, key=None):\n\t\"\"\"Pop an entry previously obtained from an iterator.\n\t@type key: String\n\t@rtype: (item_ptr, value_ptr)\n\t@return: (item_ptr, value_ptr)\n\t@raise KeyError: \"key\" not found previously in our cache.\n\t\"\"\"\n\tkey = key if isinstance(key, str) else key.strip()\n\tresult = self"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tf = lambda self: (self.firstkey, self.firstval)\n\treturn [\n\t\t(f({v: self.v[c] for c in range(self.len))),\n\t\t(f({v: self.firstval[c]'s'} for c in range(self.len)))\n\t]\n\n/*\n * reserved comment block\n * DO NOT REMOVE OR ALTER!\n */\n/*\n * Copyright 1999-2004 The Apache Software Foundation.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tself.index -= 1\n\treturn self.popitem(int(self.index))\ndef popitem():\n\treturn self.popitem(int(self.index))\ndef pop_up(self):\n\tself.index -= 1\n\treturn self.popitem(int(self.index))\ndef clear(self):\n\tself.index = int(self.get_index())\n\n/** \\file\n * Very basic C API for managing objects.\n */\n\n#ifndef EGALITO_APPKIT_BASIC_UTILITIES_OBJECT_H\n#define"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tself.pushitem((lambda: 0, [1]), True)\n\ndef restackvars(stack):\n\t\"\"\"Returns the original `vars` stack\n\t\"\"\"\n\tif not stack:\n\t\treturn vars()\n\telse:\n\t\ts1 = stack[0]\n\t\ts2 = stack[-1]\n\t\treturn dict((k, s.value) for k, s in s2)\n\ndef eval_module(module_name):\n\t\"\"\"Returns the module function with signature `funcname(mod, args=None, keywords=None)`.\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\treturn self[1:(1 + len(self)) - 1]\n\ndef setitems(self, key, items):\n\t\"\"\"\n\tReturns all items that match the `key` key value\n\t\"\"\"\n\tset = {}\n\tfor index in range(len(items)):\n\t\tset[key] = items[index]\n\n\treturn set\n\nclass BaseList:\n\t\"\"\"\n\tObject for list of keys.\n\t\"\"\"\n\tdef __len__(self):\n\t\treturn len(self)\n\n\tdef __getitem__(self, index):\n\t\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t#print('Popping', self[3])\n\tfound = self.pop(self.items, 5)\n\tif not found:\n\t\treturn False\n\t\n\tself[3] = self[3].pop(self.items[-1])\n\t\n\treturn found[0]\n\t\n\n\nprint('Popped')\n\n/*\n * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n * \n * Licensed under the Apache License, Version 2.0 (the \"License\").\n * You may not use this file except in compliance with the License.\n *"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t#print('# method popitem')\n\tself = dict(self)\n\tvalue = (None, None)\n\tk = self.__itemkey__\n\tfound = False\n\tif k in self:\n\t\tfor e in self.items():\n\t\t\tif k.startswith(e):\n\t\t\t\tself[k] = e\n\t\t\t\tfound = True\n\t\t\t\tbreak\n\telse:\n\t\tself.__append__(self)\n\t\treturn\n\tif not found:\n\t\tv = [self.__getitem__(k) for k in self.items()]\n\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tstart = self.index\n\twhile start > 0:\n\t\tkey, value = self.r[start - 1]\n\t\tif key > self.limit:\n\t\t\tself.index = start - 1\n\t\t\treturn\n\t\tif start > self.limit - 1:\n\t\t\tend = start\n\t\telse:\n\t\t\tend = self.r[start + 1] - key\n\t\tself.index = end\n\treturn\n\ndef get_key(self, item):\n\treturn [entry[0] for entry in self.r[self."}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\treturn self.popleft()\n\n\tdef popleft(self):\n\t\t\"\"\"\n\t\tPop `(key, value)` pair left.\n\t\t\"\"\"\n\t\treturn self.popleft()\n\n\tdef popleftend(self):\n\t\t\"\"\"\n\t\tPush `(key, value)` pair left, `(key, value)` is not used.\n\t\t\"\"\"\n\t\tself.appendleft(('key', 'value'))\n\t\tself.tail = self.head\n\t\tself.erase()\n\n\tdef appendleft(self, key="}]}
{"task_id": "62b8d22a48ba5a41d1c3f47e", "prompt": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tif not self._dict:\n\t\tD = {\"__self__\": self, \"__dict__\": self._dict}\n\t\tself._dict[key] = default\n\tif default!= self._dict[key]:\n\t\tD[\"__self__\"] = self\n\treturn D[key]\n\n\nsetdefault.__doc__ = (\n\t\"Sets a value for the entry \"\n\t\"%(default)s with the last-accepted item and %(key)s.\"\n\t\"Note that the default item will be overwritten. \"\n\t\"Set a default value with.add()"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tif key in self.dict:\n\t\treturn self.makeiter(key, default)\n\treturn D.get(key, default)\n\n\tdef __iter__(self):\n\t\ttemp = D.get(self.dict.get(self.key, default), default)\n\t\tif temp is None:\n\t\t\treturn\n\t\ttemp = (temp, self.index[self.key])\n\t\treturn temp\n\n\t@staticmethod\n\tdef remove(self, key, default=None):\n\t\ttemp = D.remove(self.dict.get"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\ttry:\n\t\tif not key:\n\t\t\treturn self\n\t\telif self in D:\n\t\t\treturn D[self][key]\n\t\tif self in D:\n\t\t\tD[self][key] = default\n\t\treturn d\n\texcept KeyError:\n\t\traise TypeError('%r not a recognized key' % key)\n\n\ndef dict_setdefault(self, *args):\n\t\"\"\"\n\t\tD.setdefault(k[,d]) -> D[k]=dict of lenlen dicts if k not in D\n\t\"\"\"\n\tresult = {}\n"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tif key in self.__dict__: return self.__dict__['setdefault'](key, default)\n\tret = self.get(key)\n\tif ret is not None: return ret +'='+ default\n\treturn default\n\n\n@array_interface\nclass DictIterator(itertools.Iterator):\n\t'''Iterator for a dictionary'''\n\tdef __iter__(self):\n\t\treturn self\n\n\tdef __next__(self):\n\t\treturn next(self)\n\n\tdef __next__(self):\n\t\t# This needs to be here for clarity\n\t\t"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tif default is None or type(default) is not list:\n\t\treturn self.get(key)\n\tif isinstance(default, string_types):\n\t\ttry:\n\t\t\td = default.strip().split()\n\t\texcept:\n\t\t\t# d is a dict, leave default alone\n\t\t\treturn default\n\t\tfor k,v in d:\n\t\t\tif k in self.D:\n\t\t\t\tself.set(k,v[0])\n\t\treturn self.get(key)\n\telif isinstance(default, bool):\n\t\treturn"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tD.setdefault(key, default=default)\n\n\nclass D.listbydefault(setdefault):\n\t\"\"\"\n\tD.listbydefault() -> ()\n\tD.get(key) -> []\n\t\"\"\"\n\tdef __init__(self, key):\n\t\tD.setdefault(key, [])\n\n\nclass D(dict):\n\tR = D()\n\n\tdef __setitem__(self, key, value):\n\t\twith self.lock:\n\t\t\tself[key] = value\n\t\treturn D(self.R())\n\npackage org"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tself.lock.acquire()\n\ttry:\n\t\treturn self.D_setmember_get(key, default)\n\tfinally:\n\t\tself.lock.release()\n\nclass MutableDict(dict):\n\t\"\"\" MutableDict(dict) -> MutableDict -> dictionary\"\"\"\n\n\tcopy = MutableDict.copy\n\tcopy_from = MutableDict.copy_from\n\tsetdefault = MutableDict.setdefault\n\tpop = MutableDict.pop\n\n\tdef add(self, key, value):\n\t\t\"\"\" Add(key, value) -> self[key] \"\"\""}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tif key in self._entries:\n\t\treturn self._entries[key]()\n\n    def setdefault(self, key, defd=None):\n\t\ttry:\n\t\t\treturn self._entries[key]\n\t\texcept:\n\t\t\treturn defdefault\n\n    def __getitem__(self, key):\n        try:\n            return self._entries[key]\n        except KeyError:\n            result ="}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tfor key in self.d:\n\t\tif key in self.d[key]:\n\t\t\tdel self.d[key][key]\n\n\tif default is not None:\n\t\tself.d[key] = default # no duplication for now\n\treturn self.d[key]\n\n\ndef dict_keys(d):\n\t# type: (dict) -> _type\n\treturn dictkey(d_pairs=map(lambda (a,b): (a,b), a for a, b in d.items()))\n\n\ndef dict_values("}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tif isinstance(key, str):\n\t\tkey = key.encode('latin1')\n\tself.keyToName[key] = key\n\treturn self.nameToKey[key] = key\n\n\n# TODO : need to implement the D.values\n\ndef allvalues(self):\n\t\"\"\"Returns a list of all D.values\"\"\"\n\treturn self.getAll()\n\n\n\n# TODO: implement the 'get' methods\n\ndef set(self, key, D):\n\t\"\"\"\n\tParameters\n\t----------\n\tkey : str\n\t\tThe key to"}]}
{"task_id": "62b8d22948ba5a41d1c3f47c", "prompt": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__.setdefault(key, self).__dict__[key]\n\texcept KeyError:\n\t\traise TypeError(key)\n\treturn self.__dict__[key]\n\n###\n# Helper Functions\n###\n\ndef set(D, item):\n\tfor x in iter(D):if x.isKey(item, False):D.pop(x)\nif type(D) == dict:\n\tD = set(D)\nfor key in itervalues(D):set(D, key)\n\n## Keysets ###\n\n#  "}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tif key not in self.keys():\n\t\treturn default\n\telse:\n\t\tif d is None:\n\t\t\treturn self[key]\n\t\telse:\n\t\t\treturn self.get((key,d))\n\n\n\"\"\"\n# D.union(keys) -> D\n# keys -> a list of D keys\n# \"\"\"\ndef union(x,y):\n\t\"\"\"\n\tD.union(D)\n\t\"\"\"\n\tif x is not None:\n\t\tD.update(x)\n\tif not y is None:\n\t\tD."}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tself.data = default\n\tif isinstance(key, str):\n  print \"get called with input type'str', type 'Tuple'\",\n    type(key).__dict__.update( { (k, tuple(k.split())).itemdict() })\n\n  result = {}\n  for k in sorted(self.data.keys()):\n    result[k] = self.data[k]\n    for v in key.args:\n      result[k].extend(self.data[v].args[key.args[v]])"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tif type(key) is None:\n\t\treturn default\n\n\tf = self._by_key(key)\n\tif f is not None:\n\t\treturn f(self, default=default)\n\telse:\n\t\treturn default\n\n# The following definitions have been moved to the Python implementation,\n# in case an exception occurs during the calling code.\nclass IndexError(ValueError):\n\n\tdef __init__(self, args, reason):\n\t\tself.args = args\n\t\tself.reason = reason\n\nclass D(object):\n\t\"\"\"\n"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tif default is not None:\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError as e:\n\t\t\tself[key] = default\n\tself._update()\n\treturn self[key]\n\ndef clear_cache():\n\tglobal CLEAN_CACHE\n\tCLEAN_CACHE = False\n\tCACHE = None\n\ndef clear_caches():\n\tglobal CLEAN_CACHE\n\tCLEAN_CACHE = False\n\ndef get_cache():\n\treturn CLEAN_CACHE\n\ndef get_cache_time():\n\t# Get the time of"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tif key in self:\n\t\tif not self[key]._ref:\n\t\t\treturn self._ref[key]\n\t\telse:\n\t\t\treturn self._ref[key]\n\n\tif key in self and default:\n\t\treturn self[key]._ref if self[key]._ref else {}.get(key, default)\n\treturn default\n\n\ndef setdefault(name, d, value):\n\t\"\"\"\n\t\tAdds a new key-value pair to the dictionary.\n\t\"\"\"\n\tclass Pure:\n\n\t\t# a simple set, useful"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tif key in self:\n\t\treturn self[key]\n\telse: self.load()\n\treturn default\n\n\ndef _get_d(key, default=None):\n\tif key == \"__name__\":\n\t\treturn default\n\telif key == \"__weakref__\":\n\t\t#       sys.excepthook(default)\n\t\t#       __import__(None.__name__) #  #                           "}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tif hasattr(self.dictionary, key):\n\t\treturn self.dictionary[key]\n\treturn default\n\ndef exists(string):\n\treturn string in self.dictionary\n\nif __name__=='__main__':\n\ttest = 'test'\n\tt1 = time.time()\n\ta=D_train(test,3)\n\ta.close()\n\tprint(\"Done\")\n\tprint(\"time is\", time.time()-t1)\n\nimport { create, tsx, createDecorator } from 'tsx-angular';\nimport { Observable, Reactive"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tif key not in self.D:\n\t\tif default is None:\n\t\t\treturn default\n\t\treturn self.D[key]\n\treturn get(self, key.lower(), default)\n\n\ndef _remove_duplicate(d):\n\tfor item in d.items():\n\t\tif isinstance(item, str) and re.match(r\"^\\w+\", item):\n\t\t\tcontinue\n\t\tif item not in d:\n\t\t\treturn d[0]\n\n\nclass _PreservingIndexException(Exception):\n\t\"\"\"A RAF-compl"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tif key in self:\n\t\treturn self[key]\n\telif key in self.__dict__:\n\t\treturn self.__dict__[key]\n\twhile self[key] is not None: return self[key]\n\treturn default\n\n\n#: Return all instances of D with the keys in reversed order. \ndef get_reverse(self):\n\t# Reversed instances...\n\treverse_dict = {}\n\tfor key, val in self.__dict__.items():\n\t\treverse_dict[key] = val\n\treturn reverse_dict\n\n\nclass DictBase"}]}
{"task_id": "62b8d22548ba5a41d1c3f472", "prompt": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef memoize(target, cache, force=False, lock=None):\n\t\t\"\"\"\n\t\t(Optional)\n\t\t\n\t\tIf given the optional, a name to a cache set, a callable to memoize.\n\t\t\"\"\"\n\t\tif not target:\n\t\t\treturn force\n\t\telse:\n\t\t\t# Cache setter\n\t\t\tif target in cache:\n\t\t\t\treturn cache[target]  # Return callable\n\t\tif cache:\n\t\t\t# Return from cache or cache set\n\t\t\tif key in cache:\n\t\t\t\treturn force  # Avoid duplicates"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t@defer.inlineCallbacks\n\tdef wrapped(method):\n\t\tcache.update({ key: weakref.proxy(method) }\n\t\treturn method\n\treturn method if lock is None else yield cachedmethod(cache, key=key, lock=lock)\n\n#: The current \"caching\" method. Defaults to a single call.\n_caching = cachedmethod\n\n/* Copyright (c) 2009\n   Scott Barro3d <scotty3d@gmail.com>\n   Julien Pommier <julien.pommier@uccs.uvista"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef _cache_method(*args, **kwargs):\n\t\tmemo = cache(cls, key=_key(*args), **kwargs)\n\t\tmemo.update(cache)\n\t\treturn memo\n\treturn _cache_method\n\n\n# -----------------------------------------------------------------------------\ndef copy(cls, other=None):\n\t\"\"\"\n\tCopy object.\n\n\tCreates a new object, which it inherits all of its instances via the\n\tproperties.\n\n\t:param type: (optional) One of the copyable types. Either a Python type\n\t\tor, for a builtin Python type, object."}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tmemoize(Cache(cache, key))\n\n\tif lock is not None:\n\t\tmemoize(cache)\n\n\treturn memoize\n\ndef getfield(self, name):\n\t# Find the most common type, we need to cache the type\n\tif len(name) > 0 and name!='self':\n\t\tfield = self.type(name)\n\t\tif field is not None:\n\t\t\treturn field\n\t\treturn None\n\tif hasattr(self, 'TYPE'):\n\t\ttry:\n\t\t\tfield = self.TYPE.find("}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef memoized_callable(*args, **kwargs):\n\t\t\"\"\"\n\t\tReturn a callable object to memoize a method\n\t\twith a cache.\n\t\t\"\"\"\n\t\tcache = _cache\n\t\tif lock is None:\n\t\t\tlock = threading.RLock()\n\t\telse:\n\t\t\tlock = threading.Lock()\n\t\torig = cache.get(key, args[1])\n\t\ttry:\n\t\t\tif cache.get(key, None) is not None:\n\t\t\t\tcache.pop(key)\n\t\tfinally:\n"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tif (cache is None):\n\t\treturn self._cache.fget\n\t\nclass cachedhash(cachedmethod):\n\t\"\"\"\n\tGeneric class that handles hashing with cached methods.\n\t\"\"\"\n\tmagic = 'py_cache'\n\t\n\t@classmethod\n\tdef __init__(self, fn, *args, **kws):\n\t\tself.fn = fn = fn.__func__\n\t\tself.args = args\n\t\tself.kwarg = kws.copy()\n\t\tself.method = self.get()\n\t\tself._cached = self."}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t\nclass memo_function(cachedmethod):\n\t\"\"\"\n\tHelper to create a function memoized.\n\t\"\"\"\n\t@cachedmethod\n\tdef __getitem__(self, key):\n\t\treturn self.__dict__[key]\n\t\n\tdef __repr__(self):\n\t\treturn \"<memo_function %s>\" % self.__dict__\n\n_memo_functions = memo_function(__repr__, count=10)\n\n\n@register.memoise\ndef _cache_func(name, **kw):\n\t\"\"\"\n\t\tHelper to create a cache.\n\t\"\"\"\n"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef cachedmethod_new(ctor, key=hashkey, lock=None):\n\t\t\"\"\"\n\t\tMake a cache entry\n\t\t\"\"\"\n\t\tif lock == None:\n\t\t\tlock = threading.RLock()\n\t\tentry = cache[key, type(ctor), self].__new__(lock, ctor)\n\t\tcache[key, type(ctor), self] = entry\n\t\treturn entry\n\n\t@memoize\n\tdef __new__(self, lock=Threading.RLock):\n\t\tif lock == threading.RLock:\n\t\t\tlock = threading."}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef memoizedmethod(cls):\n\t\tm = cls.__new__(cls)\n\t\tm.lock = lock\n\t\tcache[id(m)] = m\n\t\treturn m\n\treturn memoizedmethod\n\ndef cachedclassmethod(cls):\n\t'''Returns the original cache method wrapped by the given class instance.\n\tif there is no cache entry associated with the given class instance, but the cached\n\tmethod returns the value of self.\n\t'''\n\tif cache.get(id(cls)):\n\t\treturn cls.__self__\n\tdef meth(cls):"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\timport functools\n\tfrom inspect import getargspec, getfile, getfile as inspect_getfile, \\\n\t\tgetname, filename\n\tfrom sys import stderr\n\tfrom os import path\n\tARG_ = dict = {}\n\tdef cachedmethod_cache(klass, key, lock=None):\n\t\tr = {}\n\t\tcachedmethod(klass, key, lock=lock)\n\t\tif isinstance(klass, tuple()):\n\t\t\tif isinstance(klass[0], tuple):\n\t\t\t\tfor x in klass[0]:\n\t\t\t\t\tr[x] = cached"}]}
{"task_id": "62b8c517e0d34b282c18122e", "prompt": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\treturn extostr(cls, cls.error, max_level, max_path_level)\n\n\n# :func:`_fmt_exception`\nclass _Error:\n\t'''Exception instance.\n\t:param error: Exception to format.\n\t:type error: Exception\n\t'''\n\tdef __init__(self, error):\n\t\tself.error = error\n\tdef __repr__(self):\n\t\treturn self.error.__repr__()\n\n\ndef get_exception_type(error):\n\tif hasattr(error, '__iter__'):\n\t\t# Not"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\tmsg = \"\"\n\tprefix = \"\"\n\tif e.has_cause():\n\t\tmsg = e.cause()\n\telse:\n\t\tmsg = str(e)\n\n\tif not msg:\n\t\tmsg = \"Unexpeceted exception\"\n\telse:\n\t\tmsg = \"Exception:\\n\" + msg\n\t\tprefix = \" (Cause: {0})\\n\"\n\t\tprefix_level = max_level\n\t\tmax_level += 1\n\t\tmsg = msg.format(prefix, prefix)\n\n\t# check for a"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t# For extostr, we have to create a context that will be used when we\n\t# call str().\n\tC = ContextManager()\n\t# We might call str(), so we have no context to pass to it.\n\tif C:\n\t\tC()\n\n\t# First try using the string in the context. We will need to use the\n\t# context, or something like :class:`fmt` in order to get an\n\t# explanation.\n\tif hasattr(str, \"format\"):\n\t\tstr = str.format(*e)\n\telse:\n\t"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\treturn ext(str(e), max_level, max_path_level)\n\n# vim: set et ts=4 sw=4 :\n\nimport warnings\n\nfrom django import forms\nfrom django.core.exceptions import ValidationError\n\nfrom oscar.core.loading import get_load_model\nfrom oscar.core.utils.django_utils import get_permission_for_model\nfrom slither.core.models import Permission, Role\n\n\n__all__ = [\n    'Permission',\n    'RolePermission',\n]\n\n\nclass Permissions"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t# Get a stacktrace\n\ts = \"Error in catch(...) called \" +  \"except(%d,\" % max_level) + \"\"\n\texc_class, exc_table, exc_traceback = sys.exc_traceback\n\n\tif isinstance(exc_table, Exception):\n\t\t# Get the class of the first child (that's it)\n\t\tname = exc_table.__name__\n\telse:\n\t\tname = exc_table\n\t# Get the name of a parent\n\tif max_path_level > 0:\n\t\tname = name"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\traise Exception(\"%s: %s\" % (extostr(cls), str(e)))\n\n\ndef filter(expr, level = 10, limit = 10):\n\t\"\"\"\n\tFilter the expression at the given level and return the list.\n\t:param level: The search threshold (default 10)\n\t:param limit: The matching function limit (default 10)\n\t:return: An iterable with the matched items\n\t:rtype: Iterator object\n\t\"\"\"\n\treturn filter(expr, level, limit, lambda x: getattr(x, 'id'), lambda x: x.get"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\ts = \"exception(%s)\" % cls.__name__\n\ts += \":\\n\"\n\ts += \"  %s\\n\"\n\ts += \"  %s:\\n\" % (cls.__file__, cls.__name__)\n\ts += \"    \"\n\tif e:\n\t\tfor i in range(len(e), max_level+1):\n\t\t\ts += \"\\n    \"\n\t\t\tif not isinstance(e[i], (int, Float)):\n\t\t\t\ts += \"      error\"\n\t\t\telse"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\tfrom.util import string_repr\n\tif e is None:\n\t\treturn ''\n\telse:\n\t\ttry:\n\t\t\treturn string_repr(e, max_level=max_level,max_path_level=max_path_level)\n\t\texcept Exception:\n\t\t\treturn e.__repr__() # fallback to the string_repr on the first failure\n\n\nclass _get_exception(Exception):\n\tpass\n\nusing Svelto.Common;\nusing Svelto.DataStructures;\n\nnamespace Svelto.ECS\n{\n  "}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t\treturn '{0}\\r\\n\\r\\n'.format(e.info())\n\n\tdef __init__(self, msg=None):\n\t\tself._msg = msg\n\t\tself._next = None\n\t\tself._error(None)\n\n\tdef reset(self):\n\t\tself._next = self._exception_stack\n\t\tself._next.set_exception(self)\n\n\tdef _error(self, msg):\n\t\tself._next = self._exception_stack = self._exception_stack.append(msg)\n\n\tdef"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\tpath = []\n\tpath.append('/')\n\tfor i in xrange(e):\n\t\tpath.append(e.__class__.__name__)\n\treturn''.join([__doc__.format(e) for e in path])\n\nclass FileOpenException(Exception):\n\t\"\"\"\n\n\tRaises FileOpenException when open.\n\t:ivar open_files: Files of type File.\n\t:ivar open_files_names: File names of type File.\n\t\"\"\"\n\tdef open(self, *args, **kw):\n\t\ttry:\n\t\t\tsuper"}]}
{"task_id": "62b8bbbfe0d34b282c181210", "prompt": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t# check file exists:\n\tfile_status = os.stat(file_name+'.txt')\n\tif (not file_status) or file_status[0]!= 0:\n\t\treturn \"\"\n\n\t# check whether overwrite required:\n\tif (overwrite):\n\t\tos.remove(file_name)\n\t\treturn len(text_buffer)\n\n\t# convert string data to binary buffer:\n\tbinary_buffer = file_name + '.txt'\n\tif isfile(binary_buffer):\n\t\twith open(binary_buffer, 'wb"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tencoding = encoding.lower()\n\n\t# Create file\n\tif overwrite:\n\t\tif sys.platform == 'win32':\n\t\t\t# Create special file\n\t\t\ttmp_path = tempfile.gettempdir()\n\t\t\ttmp_name = os.path.join(tmp_path,'my_file')\n\t\t\t_ = wimpath.fopen(tmp_name, 'w+b')\n\t\t\tf = _wfopen(tmp_name, 'wb')\n\t\t\t(f).__write_bytes__(text_buffer, encoding)\n\t\t\tf."}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t# Try to set filename as UTF-8, if exist it's no big deal.\n\tfile_ext = file_encode_ext(encoding.lower())\n\tif not file_ext:\n\t\treturn 0\n\n\tif overwrite:\n\t\twith open(file_name, 'a') as f:\n\t\t\tf.write(text_buffer)\n\t\tf.close()\n\t\tos.remove(file_name)\n\t\treturn 0\n\n\tif system()==sys.OS_WINDOWS and not os.path.isfile(file_name):\n"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tif len(text_buffer) <= 0:\n\t\traise IOError(\"No text to write\")\n\ttry:\n\t\treturn codecs.open(file_name, 'w+', encoding=encoding)\n\texcept IOError as error:\n\t\treturn str(error)\n\n/*\n * To change this license header, choose License Headers in Project Properties.\n * To change this template file, choose Tools | Templates\n * and open the template in the editor.\n */\npackage com.intuit.tank.vm.agent;\n\nimport java.text.DateFormat;\n"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tprint('Writing to ', file_name, '...')\n\tif not os.path.exists(file_name):\n\t\tprint('ERROR! Directory does not exist! '+ str(__file__), '\\n '\n\t\t\t\t'status error ='+ str(__file__),\n\t\t\t\t'FILE:              ', file_name, file_name)\n\t\texit(0)\n\tf = open(file_name, 'wb+')\n\tif encoding!= '':\n\t\tf.write(encoding)\n\tf"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tif overwrite:\n\t\tf = None\n\t\ttry:\n\t\t\ttext = text_buffer.encode()\n\t\texcept UnicodeError:\n\t\t\t# If the file is not written out to the file object\n\t\t\t# we will try and overwrite it.\n\t\t\tf = open(file_name, 'w')\n\t\t# Try to open for writing the file\n\t\ttry:\n\t\t\tf.close()\n\t\t\tf = open(file_name, 'wt')\n\t\t\ttext = f.read()\n\t\t\tif encoding:\n\t\t\t\ttext"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tif len(text_buffer) > 4096 and text_buffer[-1] == \"\\r\\n\":\n\t\ttext_buffer = text_buffer[:-1]\n\n\tif overwrite:\n\t\ttry:\n\t\t\twith open(file_name, \"wb\") as file_handle:\n\t\t\t\tfile_handle.write(text_buffer)\n\t\t\treturn len(text_buffer), 0\n\t\texcept IOError:\n\t\t\treturn 0, 1\n\telse:\n\t\twith codecs.open(file_name, 'w+', encoding) as file_handle"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\treturn os.write(file_name, text_buffer)\n\ndef readtext_file(file_name, encoding='utf-8'):\n\t\"\"\"\n\tRead text file.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:return: Return the text read from the specified file.\n\t:rtype str\n\t\"\"\"\n\treturn open(file_name, 'rb').read().decode(encoding)\n\ndef write_file(file_name"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tif overwrite:\n\t\tfile_open = False\n\t\tif is_file_exists(file_name):\n\t\t\tfile_open = True\n\t\t\tif exists(file_name) and os.access(file_name, 0o700):\n\t\t\t\treturn 0\n\t\tif is_writeable_file(file_name):\n\t\t\tfor i, t in enumerate(text_buffer):\n\t\t\t\twrite_to_file(file_name, t, True)\n\t\t\treturn len(text_buffer)\n\telse:\n\t\tif is_file"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tif len(text_buffer) <= 0:\n\t\treturn len(text_buffer)\n\treturn _write2file(file_name, text_buffer, encoding,overwrite)\n\ndef delete_file_from_cache(file_name, file_type=3):\n\t\"\"\"\n\tThis is the version of delete_file_from_file(). The file does NOT\n\texist.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param file_type: The filetype to write out.\n\t:type file"}]}
{"task_id": "62b8bbbfe0d34b282c18120f", "prompt": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\tf = open(file_name, encoding)\n\texcept IOError, e:\n\t\tprint(e)\n\t\traise\n\telse:\n\t\tdata = f.read()\n\t\tif sys.version_info[0] >= 3:\n\t\t\ttext_buffer = textbuffer.getbuffer(data)\n\t\t\treturn text_buffer\n\t\telse:\n\t\t\treturn None\n\n\"\"\"\nimport logging\nfrom typing import List\nfrom urllib.parse import urlparse\nfrom urllib.request import urlopen\nfrom urllib.parse import url"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\tif encoding == 'UTF-8' :\n\t\tfrom pywtrapper.fileio import loadfile, UTF8FilePath\n\t\treturn loadfile(UTF8FilePath(file_name, encoding))\n\tencoding = str(encoding)\n\t\n\tif not os.path.exists(file_name):\n\t\treturn None\n\t\t\n\t\t# get text from file\n\ttry:\n\t\tstream = open(file_name, encoding)\n\t\t\n\t\tif isinstance(stream, str) == False:\n\t\t\t\n\t\t\terrmsg = \"Can't open file \" + file_name +"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\tif filename_to_textbuffer_file_exists(file_name):\n\t\tf_textbuffer = None\n\t\ttry:\n\t\t\tf_textbuffer = open(file_name, 'rt', encoding)\n\t\texcept IOError, e:\n\t\t\tprint('error in open file: %s\\n%s' % (file_name, e))\n\t\t\treturn None\n\t\telif len(f_textbuffer.read()) > 0:\n\t\t\ttry:\n\t\t\t\ttext = f_textbuffer.read()\n\t\t\texcept:\n\t\t\t"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\treturn textbuffer.as_textbuffer(open(file_name, 'r', encoding=encoding))\n\n\nclass BinaryText(object):\n\t\"\"\"\n\tBase class for binary files.\n\tThe ``file=Text`` and ``file=BinaryText`` classes must both define the basic type of the file,\n\tand use ``__init__()`` when writing files.\n\t:param data: Data to be read.\n\t:type data: str\n\t:raises BinaryText: ``ValueError`` during reading.\n\t:raises ~base.Error: ``ValueError`` if data is an"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\tdef get_buffer():\n\t\tif not os.path.isfile(file_name):\n\t\t\treturn None\n\t\t# Create the underlying data buffer from the file.\n\t\tbuf = [chr(x) for x in six.BytesIO(file_name)]\n\t\t# Load/read the text buffer.\n\t\ttry:\n\t\t\tdata = base64.decodebytes(buf)\n\t\texcept (TypeError, AttributeError):\n\t\t\traise ValueError(\"Bad unicode data\")\n\t\treturn data  # return the text data buffer\n\t# Note: We *"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t# Read in UTF-8 filename to string and parse the rest of the file into lines\n\ttry:\n\t\tinfile = open(file_name, \"r\", encoding)\n\texcept IOError as e:\n\t\tif file_name.endswith(\".txt\"):\n\t\t\tfname = file_name[:-1]\n\t\telse:\n\t\t\tfname = file_name\n\t\traise FileNotFoundError(\"File not found: %s\" % fname)\n\n\treturn string_to_buffer(file_name, encoding=encoding)\n\ndef read_lines_to_string("}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\tf = open(file_name, \"rb\")\n\ttry:\n\t\tbuffer = f.read()\n\t\tbuffer = buffer.decode(encoding)\n\tfinally:\n\t\tf.close()\n\treturn buffer\n\ndef save_to_unicode(\n\t\tfile_name,\n\t\tencoding,\n\t\tbuffer=\"This is a %s : %s\\n\" % (file_name, repr(buffer).replace(\"\\0\", \"\\r\\n\"))):\n\t\"\"\"Save the file to unicode string\"\"\"\n\tf = open(file_name, \""}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\tf = open(file_name, 'rb')\n\texcept EnvironmentError:\n\t\treturn None\n\tif encoding:\n\t\tif encoding not in 'ascii':\n\t\t\traise EzError('Unsupported charset %s -- must be one of ascii')\n\t\tself.encoding = encoding.upper()\n\telse:\n\t\tself.encoding = 'utf-8'\n\tself.file_to_textbuffer = f\n\tf.close()\n\treturn f\n\n\ndef get_input_buffer(max_length, force=False,"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttext_buffer = None\n\ttry:\n\t\tas_text_buffer_module = codecs.lookup('utf-8')\n\t\tif as_text_buffer_module is not None:\n\t\t\tif 'ascii' is not None:\n\t\t\t\ttext_buffer = text_buffer if 'ascii' == encoding else as_text_buffer_module.load_text(\n\t\t\t\t    str(file_name), encoding)\n\t\t\tif 'win32' is not None:\n\t\t\t\tif sys.platform == 'win32s':\n\t\t\t\t\t_codecvt_"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\tif encoding not in [\"ISO-8859-1\", \"UTF-8\"]:\n\t\treturn None\n\tf = open(file_name, \"rb\", encoding)\n\ttextbuf = f.read()\n\tf.close()\n\treturn textbuf\n\n\nclass _StringIOWrapper:\n\tdef __init__(self, str_obj):\n\t\t\"\"\"\n\t\t:param str_obj:\n\t\t\t\tString to convert.\n\t\t:type str_obj: str\n\t\t:return:\n\t\t:rtype str_obj: str\n\t\t\"\"\"\n"}]}
{"task_id": "62b8bbbce0d34b282c18120d", "prompt": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tfilename = os.path.basename(file_name)\n\tis_exist = file_exists_helper(filename)\n\treturn is_exist\n\n\ndef is_file_exist(file_name):\n\t# Try to check if filename is file, first try to get first line first, see if it exists.\n\tfilename = os.path.dirname(file_name)\n\tif filename and os.path.isfile(filename):\n\t\tfile_data = open(filename, 'rb').read()\n\t\tfile_data = file(os.path.relpath"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\treturn os.path.isfile(file_name)\n\n\ndef is_file_exist_once(filename):\n\t\"\"\"\n\tCheck if file is the same file mtime.\n\t:param filename: File name.\n\t:type filename: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tfile_mtime = os.path.getmtime(filename)\n\treturn bool(file_mtime == os.stat(filename).size)\n\n\ndef del_dir_once(dirname"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tfound = False\n\tf = open(file_name, \"r\")\n\t#read first line  \n\tline = f.readline()\n\tline = line.rstrip()\n\tf.close()\n\n\tif \".\" in line:\n\t\treturn False, \"file_name contains dot, only directory are valid\"\n\telif is_dir(line):\n\t\treturn True, \"File is directory\"\n\telif file_name.find(os.curdir) == -1:\n\t\treturn False, \"File name is file\"\n\ndef copy_file"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\twith open(file_name, 'r') as inf:\n\t\treturn any(isfile.isfile(file_name) for file_name in inf.readlines())\n\n\nclass File:\n\t\"\"\"\n\tThis class will hold FileInfo, FileInfo and FileInfo from the file\n\tobject. The constructor will generate a new FileInfo object based on\n\tthe file information.\n\t:type filename: str\n\t:arg is_file_exist: return a bool or return a File object\n\t:returns: file_info object\n\t:rtype: FileInfo object"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tfile_os_name = os.path.normpath(os.path.expanduser(file_name))\n\tif not os.path.isfile(file_os_name):\n\t\treturn False\n\treturn True\n\n\ndef is_path_exist(file_name):\n\t\"\"\"\n\tCheck if path name exists.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist / path), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t# check if the file exists\n\tret = True\n\tif not os.access(file_name, os.F_OK):\n\t\tret = False\n\n\tif ret:\n\t\treturn ret\n\tret = False\n\n\t# get the date-time\n\tdate_time = datetime.datetime.strftime(date_time_format, time.localtime())\n\n\t# create a temporary file\n\ttemp_name = datetime.datetime.now().strftime(date_time_format, time.localtime())\n\t# create the file\n\tFILE_CREATED ="}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif os.path.isfile(file_name):\n\t\treturn True\n\treturn False\n\n\nclass FileType(Enum):\n\tbinary = '7z'\n\ttext = '3z'\n\tjson = '3o'\n\n/**\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements. See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership. The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif not os.path.isdir(file_name):\n\t\treturn False\n\telse:\n\t\treturn os.path.lexists(file_name)\n\n\ndef uncompress_gzip(file_name):\n\tpass#IMPORTIMPORTIMPORT import gzip\n\timport zipfile\n\tunzip_filename = zipfile.get_fileobj(file_name)\n\n\tunzip_filename_uncompressed = unzip_filename.name\n\ttry:\n\t\twith open(unzip_filename_uncompressed) as unzip_fh:\n\t\t\tuncompressed_"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif not os.path.exists(file_name):\n\t\treturn False\n\telse:\n\t\treturn True\n\n\ndef get_max_file_size(config):\n\t\"\"\"\n\tGet max file size configuration from database.\n\t:param config: Configuration database.\n\t:type config: Dal.Configuration.Configuration\n\t:return: Max file size configuration,\n\t:rtype: Dictionary.\n\t\"\"\"\n\tmax_file_size = {}\n\tmax_filesize = 0\n\tcount_files = config.get_configuration_value('fileMax', '"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from time import strftime\n\tdef date_time(*args):\n\t\treturn time.strptime(*args)[5:]\n\tdef date_to_local_time(date_time):\n\t\tlocal_time = date_time['hour']*3600 + date_time['minute']*60 + date_time['second']\n\t\tlocal_time = local_time.strftime(\"%b %d\")\n\t\treturn local_time.replace(\"%z\",\"\").replace(\"%0\",\"\")\n\tdef get_file_meta_from_local"}]}
{"task_id": "62b8b99de0d34b282c1811f8", "prompt": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t# TODO: make sure all logging is updated\n\tif not cls.LOGS:\n\t\treturn\n\n\twith open(cls.LOGS['reset'], 'a+') as f:\n\t\tf.write('\\n\\n')  # newline\n\t\tf.write('\\n\\n')  # newline\n\n\tcls.LOGS['reset'] = '0'\n\n\n\n#\n# EVALUATION FUNCTIONS\n#\n\n\ndef _check_file_exists(filename):\n\t\"\"\"\n\tCheck file to load it if it doesn't exists\n\t"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\ttry:\n\t\timport paddle.logging.infostream_ex as _infostream_ex\n\texcept:\n\t\t_infostream_ex = None\n\tif _infostream_ex is not None:\n\t\tself.infostream_wrapper.log_info(_infostream_ex)\n\n\n# def log_inf(self):\n# \t\"\"\"\n# \tlog warning\"\"\"\n# \tdef _log(msg):\n# \t\tlog.error('Logger Warning', time=0.0)\n# \tinfostream_wrapper.log_info"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tif _cls.logging:\n\t\t_cls.global_logger.debug('Reset logging...')\n\tif _cls.auto_reload:\n\t\t_cls.global_logger.pop_file()\n\t_cls.logging = 0\n\ndef _make_dir(self, dirname, add_on_copy=False, verbose=0):\n\t\"\"\"\n    Make a directory if needed and append to sys.path at the end\n\n    Parameters\n    ----------\n    dirname : str\n        Path"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tcls.log_level = logging.INFO\n\n\ndef _set_log_level(log_level):\n\tcls.log_level = log_level\n\n\ndef _create_logger(\n\t\tlogging_level: Optional[int] = DEFAULT_LOGGING_LEVEL,\n\t\tprefix: Optional[str] = None,\n\t\tconfig_file: Optional[str] = None,\n\t\tfile_type: Optional[str] = None,\n\t\tformatter: Optional[Type[BaseFormatter]] = None,\n\t\tformat_str: Optional[str]"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tcls.logger.setLevel(logging.ERROR)\n\nclass Logger(obj.Typeable):\n\t\"\"\"\n\t\tGeneric logging mechanism\n\t\"\"\"\n\n\tloggerConfig = logging.getLogger(__name__())\n\n\tdef __str__(self):\n\t\treturn self.loggerConfig.__str__()\n\n\tdef __repr__(self):\n\t\treturn str(self.loggerConfig)\n\n\t@classmethod\n\tdef __dummy__loggerClass(cls):\n\t\treturn logging.getLogger(\"dummyLogger\")\n\n// Copyright 2009 the Sputnik authors.  All rights reserved"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tcls._logger = Log4C(name=\"reset_logging\")\n\n\n@timer_decorator\ndef _reset_debug(cls):\n\tlogging.debug(\"Running reset_logging on this thread. This will make the loggers running more recent. Are you sure you are using your loggers.\")\n\tcls._logger.debug(\"This is not the best way to reset. Let's wait a few seconds and try again.\")\n\ttry:\n\t\tcls._logger.debug(\"This will only reset things, which will lead to problems.\")\n\t\tcls._logger.debug(\"If you can"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tif _DEBUG:\n\t\tdef _reset_logging(\\\n\t\t\tself, log_level=None, \\\n\t\t\tlog_name=None, \\\n\t\t\texception_prefix=\"__root__()\", traceback=\"unhandled\" ):\n\t\t\"\"\"\n\t\t.. todo::\n\t\tclean up\n\t\t:param log_level: (int) optional in order to set logging level to None, else will get to __root__()\n\t\t:param log_name: (string) can be blank, and it will trigger ``import_string(name)``\n\t\t:param exception"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tif hasattr(logger,'reset'):\n\t\t# we need to do something before we start logging\n\t\tlogger.reset(filename=logger.stderr.name)\n\n\ndef _log(logger, logger_type, func, *args, **kwargs):\n\t\"\"\"\n\t\tCalled when a logger is log.\n\n\t\tThis function should return one of the log messages we care about\n\t\tand that are needed in order to call the method. If you want to\n\t\tdo logging, use _log.\n\t\"\"\"\n\tif logger_type is None:\n\t"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t# Check to see if this Python module is available\n\ttry:\n\t\tfrom cStringIO import StringIO as STR\n\texcept:\n\t\traise NotImplementedError(\"No string IO available.\", StackDump(cls))\n\n\tif not hasattr(cls, 'LOGGER'):\n\t\ttry:\n\t\t\tfrom. import LOGGER as LOGGER\n\t\t\tLOGGER.info('Loaded logging module.')\n\t\t\tLOGGER.info('Class name = %(name)s (%s) is loaded.',\n\t\t\t\tcls.__name__, cls.__name__)\n\t\tfinally:\n\t\t\tif hasattr(cls, '"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tif DEBUG:\n\t\tcls.logger.info(\"Loggers %s\", cls.logger.root_logger.get_logger_names())\n\t\tcls.root_logger.debug('Loggers %s', get_logger_names())\n\n/* $NoKeywords:$ */\n/**\n * @file\n *\n * mn_d2m_linkcp.c\n *\n * Common code for D2M memory initialization.\n *\n * @xrefitem bom \"File Content Label\" \"Release Content\"\n * @e project: AGESA\n * @e sub-project:"}]}
{"task_id": "62b8b59feb7e40a82d2d1291", "prompt": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn self.name + 'Factory'\n\n# _____________________________________________________________________________\n\ndef _getTargetClass_Factory():\n\treturn 'Factory'\n\n# _____________________________________________________________________________\n\ndef py_object_to_pyobj(pyObject):\n\tif isinstance(pyObject, (Sequence, set, dict, list)):\n\t\tcls = pyObject[0].__class__\n\telse:\n\t\tcls = pyObject\n\treturn cls(\n\t\t\t\tpyObject\n\t\t\t)\n\n# _____________________________________________________________________________\n\ndef py_class_to_pyobj(pyClass"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\tself.error = None\n\tself.pyModule = ctypes.POINTER(ctypes.c_void_p)\n\tself.fallbackPyModule = ctypes.c_void_p()\n\t# pylint: disable=arguments-differ\n\ttypelib_util.add_ctypes_module_to_function(self.fallbackPyModule,\n\t\tNone,\n\t\t\"_getTargetClass\", \"_getPythonLibName\", \"return _getPythonLibName\")\n\n\t# pylint: disable=arguments-mismatch\n\t# pylint: disable=no-member\n\tself.pythonLib"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\ttry:\n\t\treturn self._implClass\n\texcept AttributeError:\n\t\t# python\n\t\tif self._implClass is not None:\n\t\t\tsys.modules.setdefault('pydoc', self._implClass.__getname__())\n\t\t\traise\n\t\t# pydoc\n\t\telse:\n\t\t\tsys.modules.setdefault('pydoc', \"__abstractmethods.__getdict__\")\n\t\t\traise\n\ndef _getImportPath():\n\tif sys.platform == 'win32':\n\t\treturn __package__ + ':' + 'py_docs'\n\t"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t# This is the original method for Py and Fallback.\n\n\ttry:\n\t\tfrom StringIO import StringIO\n\t\tfrom importlib import import_module\n\t\treturn\n\texcept ImportError:\n\t\tpass\n\ndef _hasPyModule(moduleOrName):\n\ttry:\n\t\tif sys.version_info[0] > 2:\n\t\t\timpl = import_module('py.' + moduleOrName)\n\t\telse:\n\t\t\timpl= import_module('mod.' + moduleOrName)\n\t\treturn isinstance(impl, sys.modules[__"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\tself._pyclass = _PyInstance if _PyInstance else Fallback\n\t\n\tself.register_error(Error, UnknownAttribute) # unknown attribute name\n\t\n\t# Some types:\n\t\n\tself.register_error(AttributeError, NameTooLong) # name too long\n\t# self.register_error(AttributeError, NameTooShort) # name too short\n\t# self.register_error(AttributeError, NameColon) # name colon\n\t# self.register_error(AttributeError, TypeColon) # type colon\n\t# self.register_error(AttributeError, Type"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn None\n\n\n_DEFAULT_FILE_EXT = '.bz2'\n\n\ndef importFileExt(file_ext: str):\n\t\"\"\"\n\tSet the file extension that is specified by a\n\tstring.\n\t\"\"\"\n\tif isinstance(file_ext, list):\n\t\tfile_ext = file_ext[0]  # Get string of '.bz2' suffix\n\telif isinstance(file_ext, str):\n\t\tfile_ext = file_ext.lower()\n\telse:\n\t\traise AssertionError('file ext must be a string')\n\t"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\tretVal = str\n\n\n\tdef main():\n\t\ttry:\n\t\t\t#print _getTargetClass(_pyFile), \"\\t\\t\\t  \"\n\t\texcept KeyError:\n\t\t\tprint \"\\t\\t\\t\\t\"\n\t\treturn\n\t\tfinally:\n            retVal += _getTargetClass()\nprint \"\\t\\t\\t\\t\" + retVal + \"\\t\\t\"\n\n\n\n\n<?=$this->load->view();?>\nvar __excClass=\"$this->excClass"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\ttry:\n\t\treturn self.classType.__module__\n\texcept AttributeError:\n\t\tpass\n\n\ttry:\n\t\treturn self.class.classType.__module__\n\texcept AttributeError:\n\t\tpass\n\n################################################################################\n###        Thread Specific Methods                                     ###\n################################################################################\n\n\nclass MethodThread(methodThread):\n\t\"A thread's method\"\n\tdef __init"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\tfor c, method in _BUILTIN:\n\t\tif c.endswith(('_target') and method.startswith(''])):\n\t\t\treturn c\n\n\treturn 'Fallback'\n\ndef _initClass(self):\n\tfrom pycket.target import _py_builtins\n\n\t# The version code below works with pycket-compatible\n\t# versions which do not have a suffix.\n\tmajor_version = _PYUNRA_VERSION if _PYUNRA_VERSION == PYUNRA else (10,)\n\tminor_version = _PYUBA_VERSION if _PYUBA"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\ttry:\n\t\treturn Python.__version__\n\texcept AttributeError:\n\t\treturn None\n\n/*\n * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n * \n * Licensed under the Apache License, Version 2.0 (the \"License\").\n * You may not use this file except in compliance with the License.\n * A copy of the License is located at\n * \n *  http://aws.amazon.com/apache2.0\n * \n * or in the \"license\" file accompanying this file. This file is distributed\n * on an \"AS IS\""}]}
{"task_id": "62b8b590eb7e40a82d2d1275", "prompt": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\treturn sorted(orderings) + [(n, None) for n in sorted(orderings)]\n\n\ndef _legacy_sortOrderings(orderings, suffix1=None, suffix2=None):\n\t\"\"\"legacy sort orderings method without suffix, this method supports\n\tmost simple case of suffix1/suffix2. It is much more efficient to support\n\twith suffix and suffix1/suffix2 in the default sort method.\n\t\n\tIf suffix1/suffix2 contains two or many objects, then suffix2\n\tis passed on to preserve orderings. The method will then sort and keep"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t# Create the lists that match the orderings\n\torderings = list(_legacy_mergeOrderings(orderings))\n\t\n\t# Iterate the sublists, merging all the orderings\n\t# \t- in order (1) (orderings.append) -> orderings.insert(0,1)\n\t# \t- in order (2) (orderings.remove, orderings.reverse, orderings.append) -> orderings.push_back\n\t# \t- in order (3) (orderings.append, orderings.reverse) -> orderings.push_"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tif(len(orderings) == 0):\n\t\treturn\n\tif(len(orderings) == 1):\n\t\torderings[0][0] = orderings[0][0].split('/')[0]\n\t\t\t\t\n\tdef findSimilarSuffix(x, suffix, orderings):\n\t\tif(orderings == []):\n\t\t\treturn suffix  # no need to sort anything here\n\t\telse:\n\t\t\tbestsuffix = None\n\t\t\tfor index, suffix in enumerate(orderings):\n\t\t\t\tif(suffix == suffix):\n\t\t\t\t\tbestsuffix = suffix"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\treturn [\n\t    _mergeOrderings(orderings[i] for i in range(len(orderings)))\n\t\tfor orderings in orderings]\n\n\ndef _merge_orderings(orderings):\n\t\"\"\"\n\tmerge a set of orderings\n\t\"\"\"\n\ttry: group = [x for x in orderings if x[0] == '+']\n\tif group:\n\t\tfor i in range(len(orderings)):\n\t\t\torderings[i].sort()\n\t\torderings.sort()\n\t\t\t\t\n\t\tfor i in range"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tsortedOrderingsOrderIn = []\n\tfor ordering in orderings:\n\t\tif not isOrderingInOrderings(ordering):\n\t\t\tcontinue\n\t\tsortedOrderingsOrderIn.append(ordering)\n\treturn sortedOrderingsOrderIn\n\ndef isOrderingInOrderings(ordering):\n\t\"\"\"\n\tTest whether the given ordering is in order in which the object\n\twas introduced. The ordering is a list of strings; each string should\n\tbe in a list to be compatible with the system locale.\n\t\n\tReturn True if the given sorting order is in"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tfor order in orderings:\n\t\tprefix = _order_prefix((order,))\n\t\tsuffix = None\n\t\tif (\n\t\t    _is_key(prefix)\n\t\t    and _is_key(order.function)\n\t\t    and suffix)\n\t\telse:\n\t\t\tsuffix = order.function\n\t\tif (\n\t\t    not _is_key(prefix)\n\t\t    or _is_key(order.function)\n\t\t    or suffix)\n\t\t\tprefix = None\n\t\tif not prefix:\n\t\t\t"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\treturn []\n\ndef _merge_object(_obj_or_cls, _ignore_orderings=[]):\n\t\"\"\"\n\tMerge the given object so that it is a part of the current\n\torder of its type hierarchy.\n\t\n\tThe order of the object and the order of its type hierarchy\n\tfor this object is not enforced. The order of the object and\n\ttype hierarchy for the object and other objects should also\n\tbe the same. For example, we assume this object is a subclass\n\tof the given class and we merge the ordering rules for two types\n\tif they do not"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t# Use dict-to-string on dicts, if it supports conversion\n\tif list(orderings) == [None, []]:\n\t\tresult = OrderedDict()\n\telse:\n\t\tresult = {'': result}\n\tfor i in range(len(orderings)):\n\t\tresult[('x', i), ('y', i), ('z', i)] = _legacy_mergeOrderings(orderings[i])\n\treturn result\n\ndef splitFields(obj, fields=None, join='and'):\n\tif join == 'or':\n\t\tfields = ['"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tglobal object_orders\n\t\n\tobject_ordering_ids = []\n\twith_current_object_ordering_id = False\n\tfor ordering in orderings:\n\t\tobject_ordering_ids.append(ordering.unique_id)\n\t\tif ordering.order_on_and_sort_by is not None:\n\t\t\twith_current_object_ordering_id = True\n\t\t\twith_current_object_ordering_id = False\n\t\t\tobject_ordering_ids.append(ordering.order_on_and_sort_by.unique_"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tdef mergeOrderings(orders):\n\t\t\"\"\"\n        :param orders: list of orderings.\n        \"\"\"\n\t\tmerged = []\n        for order in orders:\n            if order[0] == 'x':\n                order[1] = 'y'\n            elif order[0] == 'y':\n     "}]}
{"task_id": "62b8b58deb7e40a82d2d1269", "prompt": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn interfaces.IDeclaration\n\n\nclass interfaces(interfaces.Base): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tReturn the interfaces defined by the given object\n\t\n\tThe value returned is `zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\n\t__doc__ = \"\"\"\n\t\tReturn the interfaces defined by the given object\n\t\"\"\"\n\tdef __repr__(self): # pylint:disable=non-callable\n\t\treturn 'id: %s' % list(self._id)\n\t\t\n\tdef __call__(self, **params"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn ZopeDefinition.implementation.directlyProvidedBy(object)\n\nfrom contextlib import contextmanager\nfrom functools import wraps, partial\n\n\ndef wrap(func):\n    @contextmanager\n    def wrapper_wrapper(self, **kwargs):\n        return partial(func, **kwargs)\n    return wrapper_wrapper\n\n\nclass _Wrapper(object):\n    \"\"\"\n    Wrapper which is used when the function is wrapped in a function\n    or method. This function wraps the class and will"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn object.__class__.interfaces.IDeclaration\n\n/*\n * This file is part of Bisq.\n *\n * Bisq is free software: you can redistribute it and/or modify it\n * under the terms of the GNU Affero General Public License as published by\n * the Free Software Foundation, either version 3 of the License, or (at\n * your option) any later version.\n *\n * Bisq is distributed in the hope that it will be useful, but WITHOUT\n * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n * FITNESS FOR A PARTICULAR PURPOSE. See"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t@def directlyProvides(object):\n\t\t\"\"\"\n\t\tReturn the interface provided by the given object\n\t\t\n\t\tThere are four possible interfaces for a given object, except the\n\t\tone using one of the interfaces provided by the object.\n\t\t\"\"\"\n\t\treturn object\n\t\t\n\tdef getInterfaces():\n\t\t\"\"\"Return a list of all interfaces\"\"\"\n\t\timport IInterface\n\t\treturn [ IInterface.__class__ ], []\n\t\t\n\tdef addInterface( interface ):\n\t\t\"\"\"\n\t\tAdd a new interface to the list\n\t\t\n\t\tThe function is"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn Doxygen.fromObject(object)._implementedBy\n\ndef _implementBy(element, type):\n\t\"\"\"\n\tReturn the implemented interfaces for the element.\n\t\"\"\"\n\tif type and type.hasDefinition() and type.isInterface() and ZOPE_CAN_DEFINE_INTERFACE:\n\t\telement = zope.interface._findInterface(element)\n\treturn element._implementedBy\n\ndef _implements(element, type):\n\t\"\"\"\n\tReturn the implemented interfaces for the element.\n\t\"\"\"\n\tif ZOPE_CAN_DEFINE_INTERFACE and"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\timport zope.interface.interfaces\n\treturn object.__self__.interfaces[0]\n\n/*\n * Copyright (c) 2016-present Samsung Electronics Co., Ltd\n * \n *  This library is free software; you can redistribute it and/or\n *  modify it under the terms of the GNU Lesser General Public\n *  License as published by the Free Software Foundation; either\n *  version 2.1 of the License, or (at your option) any later version.\n * \n *  This library is distributed in the hope that it will be useful,\n *  but WITHOUT ANY WARRANTY;"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tc = {}       # type: IDeclaration = interfaces.Interface\n\tfor interfaceName, interfaceValue in object.implementedAsKeys().items():\n\t\tif not interfaceType.isInterface(interfaceValue):\n\t\t\tcontinue\n\t\tc[interfaceValue] = interfaceValue\n\t\tfor subtypeName, subinterfaceValue in interfaceType.getInterfaceSubtypes(interfaceValue):\n\t\t\tif not interfaceType.isInterface(subinterfaceValue):\n\t\t\t\tcontinue\n\t\t\tc[subtypeName] = subinterfaceValue\n\treturn c[object.name]"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\targs = []\n\tfor i in object.__dict__ and isinstance(i, interfaces.IDeclaration):\n\t\targs.append(i)\n\treturn interfaces.IDeclaration.__init__(args=args, **object.__dict__)\n\ndef directlyProvides(obj, type): # pylint:disable=redefined-builtin\n\t\"\"\"\n\t\tReturn the interfaces provided by the given object\n\t\t@arg obj: the `zope.interface.interfaces.IDeclaration` object\n\t\t@arg type: the type of the given interface\n\t\"\"\"\n\t"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn [iface for iface in object.interfaces() if iface.declaredBy(object) and iface not in object.getInterfaces()]\n\n\nclass IDeclaration(IDeclaration, metaclass=metaclass.IMeta):\n\t\"\"\"\n\t\tProvides for an interface to a given interface.  The interface must be\n\t\tprovided by the interface's implementation class.\n\t\t\n\t\t@param interface: Interface to provide\n\t\t@param constructor: A constructor, provided by the interface for which to\n\t\t                   obtain"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t# pylint: disable=R0219\n\treturn object.__dict__.get(\"interfaces\", self.__dict__[object.__class__.__name__])\n\n\nclass IPythonInterfaceRegistry(interfaces.PythonInterfaceRegistry):\n\t\"\"\"\n\t\tAn interface registry of python-python interfaces\n\t\"\"\"\n\tdef __init__(self, interface):\n\t\tsuper().__init__(interface.interfaceClass())\n\t\t\n\t\t# pylint: disable=R001\n\t\t# pylint: disable=R002\n\t\tself.__interfaceMethods__ = [\n\t\t\tsetattr(self, \"getInterface\", _"}]}
{"task_id": "62b8b559eb7e40a82d2d11f8", "prompt": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tfor c in classes:\n\t\tfor n in range(c.numClasses):\n\t\t\tfor v in c.minBases:\n\t\t\t\tc.bases.append(v)\n\treturn classes\n\n\ndef minimalBasesByClassID(classes):\n\t\"\"\"\n\tReduce a list of base classes to their ordered minimum equivalent\n\t\"\"\"\n\tfor c in classes:\n\t\tfor n in range(c.numClasses):\n\t\t\tfor v in c.minBases:\n\t\t\t\tc.basesPerClass[c.bases.index(v)] = n\n\treturn"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tminimizedBases = []\n\tfor c in classes:\n\t\tif c not in minimalBases:\n\t\t\tminimizedBases.append(c)\n\treturn minimizedBases\n\ndef isBase(o): return isinstance(o, base)\n\n/*\n This file is part of Darling.\n\n Copyright (C) 2019 Lubos Dolezel\n\n Darling is free software: you can redistribute it and/or modify\n it under the terms of the GNU General Public License as published by\n the Free Software Foundation, either version 3 of the License, or\n ("}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tl = []\n\tfor c in classes:\n\t\tif c not in l:\n\t\t\tl.append(c)\n\treturn tuple(l)\n\n\ndef printBaseCount(s, classes):\n\tclass_count = 0\n\tfor c in classes:\n\t\tclass_count = class_count + reduceMany(classes[c], int(s))\n\tprint('\\n'.join(c for c in class_count.split() ))\n\nclass A:\n\tdef __init__(self, foo):\n\t\tself.b = Foo(foo)"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tfor val in classes:\n\t\tvalue = val.strip()\n\t\tmin = max(value)\n\t\tfor i in range(len(value)-1):\n\t\t\tmin = min if i == len(value)-1 else min - value.index(i)\n\t\tif min == 0:\n\t\t\treturn []\n\treturn classes[set(classes.copy())]\n\npackage com.baiyi.opscloud.sshserver;\n\nimport com.baiyi.opscloud.common.utils.StrUtil;\nimport com.baiyi"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tif not classes:\n\t\treturn []\n\tbaseSeq = []\n\tfor class in classes:\n\t\tif len(class) == 1:\n\t\t\t# Class: All classes\n\t\t\tnewSeq = [cls for cls in baseSeq]\n\t\t\tfor base in baseSeq:\n\t\t\t\t# In our order, it depends on all the instances of base\n\t\t\t\tfor i in range(len(base)):\n\t\t\t\t\tnewSeq[i] = min(newSeq[i], base[i])\n\t\t\t\tbaseSeq = newSeq\n\treturn baseSeq\n\nclass"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tclass MinimalBases(list): pass\n\tdef minimalBases(*bases):\n\t\tbases = [MinimalBases(cls) for cls in bases]\n\t\tret = []\n\t\tfor cls in bases:\n\t\t\tret.append(MinimalBases(cls))\n\t\treturn ret\nreturn minimalBases([MinimalBases(c) for c in globals])\n\"\"\"\n\npackage mage.cards.i;\n\nimport mage.MageInt;\nimport mage.abilities.Ability;\nimport mage.abilities.common.BeginningOfCombatTriggeredAbility;\nimport mage.abilities.effects."}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tnewlist=list(set(classX.bases))\n\tnewlist2=list(newlist)\n\tnewlist2.remove(newlist[0])\n\tnewlist2.remove(len(newlist))\n\tclasses.remove(classX)\n\tclasses.remove(classY)\n\n\tif not classes:\n\t\treturn 0\n\telse:\n\t\treturn min(newlist)\n\n\n####################################################################################################\nclass NoBaseCountCheckError(Exception):\n\tpass\n\ndef collect_types(classes):\n\tclasses_types=set()"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tfrom itertools import permutations\n\tbase_list = []\n\tfor class in classes:\n\t\tfor size in permutations(class):\n\t\t\tif int(size) < class:\n\t\t\t\tcontinue\n\t\t\tfor x in class:\n\t\t\t\tbase_list.append(size)\n\t\t\tif x!= int(class):\n\t\t\t\tbase_list.append(int(class))\n\treturn base_list\n\n\nimport shutil\n\ndist_paths = []\npaths_sorted = sorted(glob.glob(dist_path('*')) + glob.glob("}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tif len(classes) < 1:\n\t\treturn []\n\n\tsortedlist = []\n\tfor s in classes:\n\t\tif s.isSubClassOf(class.ASMB):\n\t\t\tsortedlist.append(s)\n\treturn sortedlist\n\n\ndef minimalNonSubClasses(classes):\n\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tif len(classes) < 1:\n\t\treturn []\n\n\tsortedlist = []\n\tfor s in classes:\n\t\tif not s.isSubClassOf(class"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tresult = []\n\tfor m in [c for l in classes for c in l for c in l]:\n\t\tif issubclass(m, Base):\n\t\t\tfor c in range(1,m.numBaseClasses()):\n\t\t\t\tresult.append(None)\n\t\t\tresult.append(None)\n\treturn result\n\n@_pickle\ndef minimalBases(classes):\n\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tresult = []\n\tl = None\n\tfor c in classes:\n\t\tif issubclass(c"}]}
{"task_id": "62b8b4b9eb7e40a82d2d1134", "prompt": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tattribute = \"attribute\"\n\tdescriptions = \"[name-name]\"\n\tdef namesAndDescriptions(self, all=False):\n\t\tdescriptions = \"\"\n\t\tattribute = \"attribute\"\n\t\tif all:\n\t\t\tallAttrNames = attribute + desc\n\t\tfor attr in [\"attribute-attribute-desc\", \"[attribute-name]\", \"attribute-attribute-flag-names\"]:\n\t\t\tif attr == attribute:\n\t\t\t\tattrDescriptions = descriptions % locals()\n\t\t\t\tprint \"Attribute: %s/%s\" % (attr, attrDescriptions)\n\t\t\t\tyield"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tself._nameOnly = False\n\tif self.group is None:\n\t\tgroup_dict = self._getMapDataFromService().groupDictionary\n\t\tself._nameOnly = True\n\t\tif group_dict and group_dict[self.group] in self._group_map:\n\t\t\tfor attr in group_dict[self.group]:\n\t\t\t\tif attr['interface_type'] == \"attr\":\n\t\t\t\t\tvalue = attr['description']\n\t\t\t\t\tif value:\n\t\t\t\t\t\tdescription_dict = self._getMapDescription(value)[1]['interface_description']"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\ttry:\n\t\tif all:\n\t\t\tself._fgetnamesanddescriptions_all = []\n\t\t\tfor x in attributes.get_interface_descriptions_all():\n\t\t\t\tself._fgetnamesanddescriptions_all.append(x)\n\texcept AttributeError:  # pragma: no cover\n\t\tself.__class__ = None\n\t\treturn None\n\treturn self\n\ndef attributeNamesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\t\"\"\"\n\treturn attributeNamesAndDescriptions(self, True)\n\ndef"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t# pylint: enable=redefined-builtin\n\treturn self.interface, self.descriptors\n\ndef get_methods_attribute_and_descriptions(cls): # pylint: disable=redefined-builtin\n\treturn list(cls.implemented_methods_attribute_description), list(cls.implemented_methods_attribute_description.keys()), list(cls.implemented_methods_attribute_description.values()), \\\n\t\tlist(cls.implemented_methods_attribute_description.items()), list(cls.implemented_methods_attribute_description.items_no_value()), list(cls.implemented"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tself.interface.attributeInfos = AttributeInfos()\n\tif self.interface.typeInfo == tTypeInfoNull:\n\t\treturn []\n\tself.interface.typeInfo = tTypeInfoNone\n\tif not all:\n\t\tself.interface.addAttributes(self.attributeInfos)\n\t\tself.interface.addAttributes(self.descriptions)\n\t\tself.interface.interfaceInfo.name = \"\"\n\treturn [cname.capitalize() for cname in self.interface.allNames]\n\ndef enumDescription(name=None, description=None, default=None"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\ttypes = self.attributeTypeDefns()\n\tdescriptions = self.attributeDescriptionTypes()\n\tnamesAndDescriptions = types.getNamesAndDescriptions(descriptions)\n\treturn namesAndDescriptions  # pylint:disable=protected-access\n\ndef getBase(typeClass, namespace=ns) -> Type  # pylint:disable=cyclic-import,ungrouped-imports\n\tfrom.. import *\n\tfrom. import _\n\n\tfor x in (_.const, _.enum,) + namespace._const_fields.items():\n\t\tfor member in x._base_members:\n\t\t"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tif self.names == None and self.descriptions == None:\n\t\tif all:\n\t\t\tself.names = []\n\t\t\tself.descriptions = []\n\t\telif not all:\n\t\t\tif self.names!= []:\n\t\t\t\tself.names.sort()\n\t\t\t\tif self.names[-1] in self.descriptions:\n\t\t\t\t\tself.descriptions.sort()\n\t\t\telif len(self.names)!= len(self.descriptions):\n\t\t\t\tself.descriptions.sort()\n\t\telse:\n\t\t\tself.names"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\treturn \"name\", \"description\"\n\ndef list(self, *attrNames, **attrList):\n\t# List all of the attributes.\n\tattributes = getattr(self, *attrList)\n\tfor attributeName in attrNames:\n\t\tattributes = getattr(attributeName, *attributes)\n\tif not all:\n\t\tfor attribute in attributes:\n\t\t\tattributeList = [ attributeName ]\n\t\t\tif attribute in attributeList:\n\t\t\t\tattributes = attributeList\n\telse:\n\t\tfor attributeName in attributes:\n\t\t\tattributes = attributes[:]\n\n\t"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t# pylint: disable=C0103,F0401,E0401,E0401,E0401,C0401,C0401\n\tif self.hasInterfaces:\n\t\tnamesAndDescriptions(all=all)\n\t\tnamesAndDescriptions(all=False)\n\t#if self.hasGroups:\n\t#\tfor name in self.groupNames:\n\t#\t\tnamesAndDescriptions(all=False)\n\t#\tnamesAndDescriptions(all=False)\n\t#\tnamesAndDescriptions(all=False)\n\t#\tnamesAndDescriptions(all=False)\n\t#"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tfrom. import ntpath, interfaces, types\n\n\tif all:  # type: ignore\n\t\tclass Attr(interfaces.Interface):\n\t\t\tnamesDescription = \"The names of this attribute\"  # python 3\n\n\t\tclass AttrGroup(interfaces.Interface):\n\t\t\tnamesDescription = \"The names of the attributes\"  # python 3\n\t\tself.attr_types = Attr, AttrGroup\n\t\tself.namesAndDescriptions = namesDescription\n\n\t\tfrom. import ntpath as ntpathstr\n\t\tattrs = { 'attr1': ntp"}]}
{"task_id": "62b8b416eb7e40a82d2d1129", "prompt": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tif isinstance(self.name, (list,), list): return self.name\n\telse: return self.name\n\nclass Attribute(Enum):\n\t\"\"\"\n\tDefinition of a single attribute (e.g. key_id, description, type,\n\tattributes).\n\t\"\"\"\n\tdef __new__(cls, name, id=1, data=None):\n\t\tglobal self\n\n\t\tif name not in self.__dict__:\n\t\t\tdef f(obj):\n\t\t\t\tif isinstance(obj, EntityAttribute):\n\t\t\t\t\tobj.class_name = obj.description\n"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tif all:\n\t\tattribute_names = self.fields\n\telse:\n\t\tattribute_names = (x[0] for x in self.fields)\n\treturn attribute_names\n\ndef get_attrs(base_path, type):\n\tr'''\n\tRead in the contents of a directory, return a dictionary with the\n\tattributes.\n\t\n\tArgument:\n\t\tbase_path = a path to an XML file \n\t\ttype = one of the 'type', 'enum', 'union' types, 'enum_map' (if present) or list (list"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn [(name, getattr(obj, name)) for name in self.get_obj.__bases__]   \n\n\ndef all(attrs): # pylint:disable=redefined-builtin,function-redefined\n\t\"\"\"\n\tReturn a list of all attributes in an object.\n\t\"\"\"\n\treturn list(map(str, attrs))\n\n\ndef get_func_or_method(*args): # pylint:disable=redefined-builtin,inline-code\n\t\"\"\"\n\tFind the method name or attribute.\n\t\n\tArguments can be passed as a list, list or tuple"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tif all:\n\t\treturn self.__attrib_names\n\telse:\n\t\treturn self._attrib_names\n\ndef list_subsections(self, s):\n\tif isinstance(s, six.string_types):\n\t\treturn [s, self._list_subsection(s, start=0) for start in range(self.offset, len(self.header) + 1)]\n\telse:\n\t\treturn []\n\n\ndef list_subsection(self, s, start=0):\n\theader = self.header[start:self.offset]"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tdef names(self, self_type):\n\t\treturn \"self.__class__\", \"self.__dict__\", \"self.foo=object\"\n\nclass __dict__ : public __metaclass__ (PropertiesDictCompat, PropertiesDictCompatBase): # Python 3 compatibility\n\t\"\"\"\n\tProperty dictionary of properties with the same name,\n\tused by the properties to keep a list of key-value pairs (and set default):\n\t\"\"\"\n\tdef __init__(self, *args):\n\t\t\"\"\"\n\t\t@type args: list of str\n\t\t@param args: the list of"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from types import TypeVar, ClassVar, Function, \\\n\tClassWithMetaclass, builtins\n\tpass#IMPORTIMPORTIMPORT from itertools import zip_longest, chain\n\tpass#IMPORTIMPORTIMPORT from string import Template, unichr, \\\n\tmaketrans, string_types\n\tfrom inspect import attrgetter, signature, getfullargspec, isclass\n\tfrom builtins import range\n\n\tattrs = set(getattr(builtins, '__metaclass__', None) for _info in dir(Builtin)\n\t"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tdef names(self, all=True):\n\t\treturn [self.__name__.rstrip()]\n\n\n#: An object capable of providing access to methods defined on the class.\n_class_methods = {}\nclass _Class(_class_methods, Type):\n\n\tdef __init__(self, *args, **kwargs): # pylint:disable=redefined-builtin\n\t\t\"\"\"\n\t\t\tArguments can be accessed using a name string (see names()).  However,\n\t\t\twe don't have access to the methods as defined by Type.\n\n\t\t\tReturns a Type object."}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tm = _get_all(self)\n\treturn m.name\n\ndef get_attributes(self):\n\t\"\"\"\n\tReturn the object attributes from the interface.\n\n\t\"\"\"\n\tm = _get_all(self)\n\n\treturn dict(m.get_attributes())\n\ndef get_members(self):\n\t\"\"\"\n\tReturn all interface's members.\n\n\t\"\"\"\n\tm = _get_all(self)\n\n\treturn self.__members__.copy() if m else None\n\ndef get_members_name(self, name, strict="}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn [self.__name__, \".format()\", \".format.format()\", \".format.format.format()\", \".format.format.format.\"][i]\n\n// RUN: %clang_cc1 -flto -flto-exceptions -fsyntax -verify %s\n\n// This file contains code in the C++ preprocessor, not the header file itself.\n// It contains a number of checks, which are only valid under C++11 and above.\n\n#define _HAS_CXX98_CHAR8_T  // expected-warning {{'#pragma GCC warning(push,'#pragma GCC warning(disable)"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn ['id']\n\nclass Tag(Model):\n\t\n\tdef __init__(self, tag):\n\t\tself.id = tag.id\n\t\tif tag.id:\n\t\t\tself.link = 'http://' + tag.id + '/'\n\t\telse:\n\t\t\tself.link = ''\n\tdef link_link(self, self_l):\n\t\tif self.link:\n\t\t\thref = '{http://{}'.format(self_l)\n\t\telse:\n\t\t\thref = '{}'.format(self_l)\n\t\t\n"}]}
{"task_id": "62b8b3d6eb7e40a82d2d111c", "prompt": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif output is not None:\n\t\tfor decl in sequence:\n\t\t\tif not isinstance(decl, (ast.Expr, ast.Decl, ast.Call, ast.List)):\n\t\t\t\ttry:\n\t\t\t\t\tdecl.args = _parse_arguments(decl.args)\n\t\t\t\texcept Exception:\n\t\t\t\t\traise ValueError(\"Unfinished declaration\")\n\t\t\tif isinstance(decl, (ast.Decl, ast.Call)):\n\t\t\t\tif is_func_decl(decl):\n\t\t\t\t\t# Handle the case that the arguments refer to functions\n\t\t\t\t\ttry:\n\t\t\t\t\t\tdecl"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tfor s in sequence:\n\t\tif not isinstance(s, (str, unicode, bytes, Integer, bool, class, float, str)):\n\t\t\traise ValueError('Cannot normalize type: %s' % type(s))\n\targs = ('--' if output is None else output)\n\tif sequence.count(\"None\"): args += 'None'\n\tfor i, arg in enumerate(sequence):\n\t\tif i > 0 and arg is None:\t\t\t# first element wins\n\t\t\targs += ', None' if args[-1] is None else args[-1]\n"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif not isinstance(sequence, sequence):\n\t\traise TypeError('input sequence must be a sequence, tuple, or single interface')\n\tif not isinstance(output, typing.Optional['declaration']):\n\t\traise TypeError('output sequence or type declaration expected')\n\treturn (sequence,) + (_normalizeargs_generic(output, output.type_declares, output.type_declarations)\n\t\tfor output in sequence)\n\t    \n\ndef is_type_specifier(p, type):\n\t\"\"\"Check whether the given type specifier exists on the front side of the type \"\"\"\n\t"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif output is None:\n\t\toutput = sys.stdout\n\t\n\tdef _getargs(func):\n\t\tparams = list(func.__call__)\n\t\targspecs = _specifyargspec(*params, output=output)\n\t\targs = [param.__name__ for param in params]\n\t\targs.extend((argspecs,))\n\t\treturn args\n\t\n\tfor element in sequence:\n\t\tis_element = isinstance(element, tuple) or isinstance(element, tuple)\n\t\telementtype = element.__class__\n\t\tfor arg in"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t# This doesn't work in Python2.x\n\t#if sys.version_info < (2, 7): del sys.ps1\n\n\t# XXX: if we ever support python2, check this and remove the arglist parsing and arguments\n\n\tif callable(sequence):\n\t\tif output is not None:\n\t\t\tsequence = ''.join(map(_normalizeargs, sequence))\n\t\t_norm = [x[0] for x in sequence if not isinstance(x, (tuple, type(None))) and any(_normalizearg(x)[0]]):\n\t\t\t"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t# The Declarig argument sequence can be a list of expressions and can be\n\t# a string or the full list of interfaces. To represent a declarator, you may\n\t# not just have to use this tuple for the first item of the list.\n\t# It is safe to check if all arguments are consumed by a declarator. This\n\t# is important when a single method is created which requires more than one\n\t# argument.\n\tif isinstance(sequence, list.__class__):\n\t\treturn [arg for arg in sequence if isinstance(arg, (Ast, AstArgs))]\n"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\targ_list = list()\n\tfor i in sequence:\n\t\tif i[0] == '[':\n\t\t\targ_list = list(arg_list)\n\t\t\tif not arg_list and i[1] == ']':\n\t\t\t\tcontinue\n\t\t\tif arg_list:\n\t\t\t\targ_list = [x for x in x.split('[')] \n\t\t\t\tif any(i[1]==']' or i[1]=='|') and len(x)<3: # [a, e, r, t]\n\t\t\t\t\targ_list.append"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\treturn __func__, __name__, list(set(i.args() for i in sequence))\n\ndef _expandnames(sequences, name, default=None, _argonly=True):\n\t\"\"\"\n\tExpand names from a sequence of declarations, or a function name\n\t\n\tReturns a sequence of functions/names defined in sequences with all the\n\t\n\tsame name.  If the function/name is in one or more definitions, but the\n\tname is not defined, the function is defined in each name.\n\t\n\tIn the case of multiple classes (when multiple named interfaces are\n\tdefined"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\treturn  _normalizedefargs(sequence, output)\n\ndef _normalizemethod(sequence, output=None):\n\t\"\"\"\n\tNormalize method name\n\t\n\tNormalize a name to be unique for a method\n\t\n\tAnything but individual interfaces or implements specs will be expanded.\n\t\"\"\"\n\treturn  _normalizedefmeth(sequence, output)\n\ndef _normalizeobject(sequence, output=None):\n\t\"\"\"\n\tNormalize object members\n\t\n\tNormalize each of the declared members\n\t\n\tAnything but individual interfaces or implements specs will be expanded.\n\t\"\"\"\n\treturn  _"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\ttry:\n\t\tfor name in sequence:\n\t\t\tpass\n\t\t\tif not isinstance(name, tuple):\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\targ = None\n\t\t\t\tfor member in name:\n\t\t\t\t\tif hasattr(member, 'name'): # no need to worry about the 'name'\n\t\t\t\t\t\targ = member\n\t\t\t\t\telse:\n\t\t\t\t\t\tfor argname in member:\n\t\t\t\t\t\t\targ = getattr(member, name)\n\t\t\t\t\t\t\tif argname not in ('_','_int','_') and not args:\n\t\t\t\t\t\t\t\traise ValueError('Cannot use {}"}]}
{"task_id": "62b8b3d5eb7e40a82d2d1110", "prompt": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\timport ccompiler\n\t\treturn _c_optimizations_enabled()\n\texcept ImportError:\n\t\treturn False\n\n_c_optimizations_setup()\n\n# -*- coding: utf-8 -*-\n\n\nfrom. import main\n\n/*\n * Hibernate, Relational Persistence for Idiomatic Java\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later.\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\tif not (py3_available or py2py_available):\n\t\treturn False\n\telif PY3 and PY2:\n\t\treturn isinstance(None, type(__main__())) and __main__.__name__ == '__main__':\n\t\t\treturn False\n\t\t\n\treturn True\n\ndef all_available():\n\treturn all((_c_optimizations_available() | __pypy_available).__dict__)\n    \ndef all_dependencies():\n\treturn (_c_optimizations_available() | __pypy_available).values()\n    \n"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\timport c_optimizations\n\texcept ImportError as err:\n\t\tmsg = \"cannot import c_optimizations because we need to import c_optimizations\"\n\t\traise ImportError(msg)\n\n\ttry:\n\t\ttry:\n\t\t\tfrom c_optimizations import Optimizer\n\t\texcept ImportError as err:\n\t\t\ttry:\n\t\t\t\tfrom pypy.module._c_optimizations_available_wrapper import Optimizer\n\t\t\texcept ImportError as err:\n\t\t\t\tmsg = \"cannot construct a Optimizer as the provided pypy."}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\tfrom. import c_optimizations\n\texcept ImportError:\n\t\treturn False\n\treturn c_optimizations\n\ndef _c_support_available():\n\t\"\"\"\n\tAvailable extensions that are not C extensions.\n\t\n\tIf the C_EXTS dict is available, this raises a\n\tImportError object and returns True.\n\t\n\tIf the C_EXTS dict is missing it will return False.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\tif c_extension_names not in c_available_extensions:"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\tif _c_lib_exists(): return _c_optimizations\n\telse: return None\n\n# frozen_string_literal: true\n\nrequire 'rails_helper'\n\nRSpec.describe ConcertoMailer, type: :mailer do\n  subject do\n    @concerto_mailer = create(:concerto_mailer)\n    @old_email = FactoryBot.build_stubbed(:user, :email, :name, :password, :email_type, :confirm_password, :confirmed_at).email\n  "}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t# If we have a C optimizing module, use this first!\n\t# Otherwise, look for a C module that is the first to load!\n\tif getattr(sys, \"C_OPTIMIZE\", None) is not None:\n\t\treturn sys.C_OPTIMIZE\n\t\n\t# If there are no C optimizing modules, then nothing needs\n\t# to be loaded!\n\tif not getattr(sys, \"optimize\", None):\n\t\treturn False\n\n\t# If we don't have a value, look for C module:\n\ttry:\n\t\tif hasattr("}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\timport objc\n\t\tdef _c_optimizations_available():\n\t\t\treturn False\n\texcept ImportError:\n\t\treturn False\n\treturn True\n\n\nif hasattr:\n\t# This module must exist, but will appear if an import fails.\n\ttry:\n\t\timport _c_optimization\n\t\tdef _c_optimizations_available():\n\t\t\treturn (_c_optimization.C_OPT_OPT_RELU == _c_optimizations.C_OPTIMIZE_OPT)\n\t\treturn True\n\texcept ImportError:\n\t\t"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\timport matplotlib\n\t\tif 'C' in sys.version_info[:3] and mpl.is_available() \\\n\t\t       and not mpl.is_installed('matplotlib'):\n\t\t\timport matplotlib.pyplot as plt\n\t\t\treturn plt.get_text_scale().get_scale()\n\t\telif getattr(mpl.is_list_like,'str_format_float_tuple.'\n\t\t\t\t\t\t + mp.FLOAT_FORMAT) == mp.FORMAT_FLOAT:\n\t\t\treturn mpl.rcParams['text.format']"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\tc_optimizations_available = c_optimizations()\n\t\tif not c_optimizations_available:\n\t\t\treturn False\n\t\telse:\n\t\t\traise ImportError\n\tfinally:\n\t\tc_optimizations_available = False\n\n\n# Import the module.  This method is used by modules to try to enable\n# C runtime optimization if not installed.\nPy_c_optimizations_enabled = weakref.proxy(c_optimizations_enabled, False)\n# This is the function's return value.  It can be either None,"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\tif sys.version_info.major < 3:\n\t\treturn x.__c_optimizations_available__\n\treturn x\n\ndef _is_str(obj):\n\tif isinstance(obj, (bytes, unicode)):\n\t\treturn True\n\ttry:\n\t\tobj\n\texcept RuntimeError:\n\t\treturn False\n\treturn True\n\ndef _c_find_binary_symbol(obj):\n\tif isinstance(obj, str):\n\t\treturn obj\n\n\tdef _find_binary_symbol(obj):\n\t\tif _is_str(obj"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110f", "prompt": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t# Python 3 has a bug: an optimiser returns a value of 1 instead of 0.\n\tif sys.version_info >= (3, 0):\n\t\treturn True\n\telse:\n\t\treturn super(PyOptimize, self)._should_attempt_c_optimizations()\n\n# Python 3\n\ndef _use_c_impl(obj):\n\t\"Return whether we should optimize for anc\"\n\t# This takes a copy so we could free it after we're done with it.\n\tif sys.version_info < (3, ) and hasattr(obj,"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tresult = _use_c_impl(env)\n\tif result == \"0\":\n\t\treturn True # pragma: no cover\n\telif result.startswith(\"1\"):\n\t\treturn False # pragma: no cover\n\telse:\n\t\traise ValueError(\"Unsupported value for 'c_optimizations'\")\n\n\ndef is_py_interpreter():\n\t\"\"\"\n\tReturn whether the Python interpreter is available on this system.\n\t\"\"\"\n\traise NotImplementedError() # pragma: no cover\n\n\ndef _init_c_python_module():\n\t\"\"\"\n\tInitialize the C Python"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t# TODO(npecklin): use an appropriate value, i.e. -use_c_c\n\tif PyPy:\n\t\treturn True\n\telse:\n\t\treturn (not _use_c_c)\n\n\ndef _should_execute_c_optimizations(args):\n\t\"\"\"\n\tHelper function for _should_execute_c_optimizations.\n\t\n\tArgs:\n\t\t* args: Tuple containing arguments to pass to _run_c_with_*() methods\n\t\"\"\"\n\n\tif _run_c_with_c_impl"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tif settings._use_c_impl:\n\t\treturn True\n\ndef get_c_optimizer_enabled(settings):\n\ttry:\n\t\treturn settings._use_c_enabled\n\texcept AttributeError:\n\t\tpass  # the Python compiler should not be running without C\n\n\t@settings._c_enabled\n\t@settings._optimizer_enabled\n\tdef _c_enabled_impl(settings):\n\t\tif settings._use_c_impl:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n\t@settings._c_enabled\n"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tif _use_c_impl:\n\t\treturn _pure_python_value()\n\telse:\n\t\t# If we're on a Python environment where we're running py_test, then\n\t\t# we want to see whether we're on C.\n\t\t_pure_python_value()\n\t\treturn _c_impl_value()\n\ndef _pure_python_value():\n\t\"\"\"\n\tReturn a tuple with two values:\n\t\t1) `True`: Python C optimizes on all py_test methods, and should\n\t\tnot attempt to use"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tif not sys.flags.optimize:\n\t\treturn False\n\tif (not hasattr(sys, \"C_IS_WEAK\") or\n\t\t(sys.flags.optimize < 2 and PY_COMPILING and not hasattr(sys, \"is_object\"))):\n\t\ttry:\n\t\t\timport ccompiler\n\t\t\treturn ccompiler.CFLAGS - os.environ[\"C_OPTIMIZE\"]\n\t\t#else:\n\t\t#\treturn sys.flags.optimize\n\treturn _should_optimize_c_impl(sys.flags.optimize)\n\n# END"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tif os.environ.get('PURE_PYTHON') is None:\n\t\t# No PyPy set up, so we won't be able to use C optimizations.\n\t\treturn False\n\treturn True\n\n\ndef _is_python():\n\t# Python's interpreter is always Python 2.\n\treturn py.version.split()[0] == 3 \n\n\ndef _is_py2():\n\treturn getattr(os.environ, 'PY_MAJOR_VERSION', 3) > 0\n\n/**\n  ******************************************************************************\n  * @file    main.c \n  * @"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tc_optimizations_enabled = get_c_optimize_options() \\\n\t\tand isinstance(app.config_dict['c_optimizations_enabled'], bool)\n\t\t\n\treturn c_optimizations_enabled\n\n\ndef is_c_optimizations_enabled(c_optimizations_enabled):\n\t\"\"\"\n\t  Checks whether we're running on PyPy or PyPy C.\n\n\t  Parameters\n\t\tc_optimizations_enabled (bool) :  True or false.\n\t\"\"\"\n\tif c_optimizations_enabled:\n\t\tc_optimize = app"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tuse_c_optimizations = 0\n\tif set(sys.argv) & set([\"-y\", \"--pure_python\"]):\n\t\tuse_c_optimizations |= _PYPY\n\tif use_c_optimizations:\n\t\treturn True\n\treturn _get_c()\n\nclass PyPy(_CInterpreter):\n\t\"\"\"The PyPy extension module.\"\"\"\n\n\tdef _make_interpreter_entry(self, interp):\n\t\treturn _PyPyInterpreter(interp, self)\n\n\tdef _get_c(self, interpreter):\n\t\tinterp"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\ttry:\n\t\tuse_c_impl = os.environ.get('PURE_PYTHON') == '1'\n\t\treturn use_c_impl\n\texcept KeyError:\n\t\tpass\n\t\n\t# If we're not on PyPy, or the environment variable isn't set or unset\n\t# we're on Mac so use Python.\n\tif 'PYPYPY' in os.environ:\n\t\t# If we're on PyPy, and _use_c_impl is False then this must\n\t\t# be a Mac\n\t\treturn get_builtin_python_"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110e", "prompt": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t# Do nothing\n\treturn\n\n\ndef _c_verbose(flag):\n\topt.print_usage = flag\n\n\ndef _c_verbose_str(flag):\n\topt.print_verbose += flag\n\n\ndef _c_test(flag):\n\topt.run_tests = flag\n\n\ndef _c_skip(testname):\n\t\"\"\"\n\tThe opposite of `_c_tests`.\n\t\"\"\"\n\t# Do nothing\n\treturn\n\n\ndef _c_warnings_str(flag):\n\tif opt.warn:\n\t\topt"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\tdef _c_optimizations_required(self):\n\t\treturn ((not any(_k in self.keys()) and\n\t\t\t self.has_option(_k, \"optimizations\")) or\n\t\t\t((not any(_k in options) and\n\t\t\t not all(_c_opt in self.options.keys())\n\t\t\t and (self.has_option(_k, \"opt\")\n\t\t\t     +(\"optimizations\" in self.options)))))\n\n\nclass _OptDict(dict):\n\t__gtruediv__ = {}\n\n\tdef __init__(self, val"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn [\"optimizer:nondeterministic_invariants\"]\n\n\n@overload\ndef _c_opt_out(x):\n\t\"\"\"\n\tOverload function for :func:`_c_optimizations_required`.\n\t\"\"\"\n\treturn c_optimizer._c_notraversals_from(x)\n\n\n@overload\ndef _c_opt_err(x):\n\t\"\"\"\n\tOverload function for :func:`_c_optimizations_required`.\n\t\"\"\"\n\treturn c_optimizer._c_notraversals_from(x)._error\n\n\ndef"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn {}\n\ndef _c_optimizer_recommended_for_single_optimizer(optimizer, optimizers, default_sgd_value):\n\t\"\"\"See `_c_optimizer_required`.\n\n\t\t\"\"\"\n\t\tif optimizer is None or optimizers is None:\n\t\t\treturn default_sgd_value\n\n\t\t# Optimizers can be given both of these, so just override that.\n\t\tfor optimizer_type_string, optimizer_desc in optlib.OPTIMIZER_TYPES:\n\t\t\toptimizer_desc = optimizer_desc.replace('<', '&lt;"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn set([\n\t\t\"_update_opt_map\",\n\t\t\"_merge_opt_map\",\n\t\t\"_split_opt_map\")\n\nc_optimizations_ignored = c_optimizations_ignored()\nc_optimizations_ignored._c_optimizations_ignored[\"__class__\"] = classmethod\nimport cProfile, os\nsys.path.append(\"..\")\nimport cProfiler\n\nimport cProfile\ncProfiler.pushProfiler(name = \"cProfile\", time = 5)\nos.environ[\"JANUS_UPDATE_COSTATS\"] ="}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn _c_optimizations_ignored\n\ndef _c_optimizations_recommended(required):\n\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn _c_recommendations\n\n\ndef _c_optimizer_parameters(optimizer, **params):\n\tif hasattr(optimizer,'max_parameters'):\n\t\tparameters = optimizer.max_parameters()\n\t\tparams = params.replace(**_make_dict(parameters))}\n\telse:\n\t\tparameters = _optimize_parameters(**params)\n\treturn parameters\n\nclass"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn _c_optimizations_ignored\n\n@jitable\ndef _c_frospections_ignored():\n\t\"\"\"\n\tAll __getattr__ calls should run in no-frospection mode.\n\t\"\"\"\n\treturn _c_frospections_ignored\n\n@jitable\ndef _c_optimization_allowed_by_c9_0():\n\t\"\"\"\n\tTests whether we can actually use a particular optimization level.\n\t\"\"\"\n\treturn _c_has_optimization_level_0\n\n@weakref.WeakMethodDef\ndef _c_call"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\timport warnings\n\n\twarnings.warn(\n\t    \"In C++, c_optimizations is not applied because it is used to \"\n\t    \"specify any argument to the optimizer or optimizer function.\",\n\t    FutureWarning,\n\t)\n\n<?php\n/* ========================================================================================== */\n/****BEGIN AMXCHANGE*************************/\nvar $a, $aa;\nvar $b;\nvar $c;\nvar $d;\nvar $e;\nvar $f;\nvar $g;\nvar $h;\n\nif (a + b + c"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn _c_optimizations_recommended\n\n# Python 2 implementation of various _c_optimizations_recommended\n_c_optimizations_recommended.default = (_c_optimizations_recommended, )\n\"\"\"\n\tThe opposite of `_c_optimizations_recommended`.\n\n\tFor backwards-compatibility, see `_c_optimizations_unknown`.\n\t\"\"\"\n\treturn _c_optimizations_recommended\n\n# Python 3 implementation of the _c_optimizations_recommended function\n\n_c_optimizations_recommended['unknown'] = (\n\t"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\tif not _c:\n\t\treturn 0\n\n\tdef _c_optimizations_required():\n\t\treturn [x.value for x in optimizers.values()\n\t\t\t\t   if __c_optimize_info__.optimize_info.name == optimizers.optimize_required]\n\n\treturn _c_optimize_info__.optimize_info.optimization.number(\n\t\tname=\"Optimizations:\",\n\t\tfunctions=optimizers.optimize_required,\n\t\toptions=_c_optimize_info__.optimize_info.optimize_type,\n\t\tenabled=True)"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110d", "prompt": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t# TODO: get rid of the \"if block\", and make this \"pure\"\n\treturn [\"#ifdef PEDANTIC_PYTHON\", \"/opt/lambdas\"]\n\n_c_impl_doc__ = \"\"\"\n\tUses the C implementation of the ``Optimize`` operation to achieve\n\tan optimal implementation.\n\t\n\tThe C implementations are created by the C implementations of\n\t:ref:`ppl.alglib.core.optimize.impls.Optimize` and the impls package.\n\t\n\tSee: **Optimizations/Pegas**](https://www.python."}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn isinstance(c_impl, COptimize)\n\n#-------------------------------------------------------------------------------\n# _use_c_*() - Return true if x is a function with argument(s) x(b,s).\n#-------------------------------------------------------------------------------\n_use_c_bool = _use_c_impl(pure=True)\n\n#-------------------------------------------------------------------------------\n# _use_c_*() - Return true if x is a function with an argument(s) x(b,s).\n#-------------------------------------------------------------------------------\n_use_c_bool = _use_c_impl(pure=True)\n\n#-------------------------------------------------------------------------------\n#"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn _c_optimization_requirements.c_is_pure_python()\n\ndef _c_impls_to_try(cx, impl_list):\n\t\"\"\"\n\tReturn a list of C implementations that might require it for compilation.\n\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\tif cx.PURE_PYTHON:\n\t\treturn cx.get_impl_list_for_compiler(impl_list)\n\telse:\n\t\treturn impl_list\n\ndef _use_c_impl("}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn hasattr(sys, \"optimize\")\n\n@_c_impl\ndef _update_opt_state(m, c, f):\n\t\"\"\"\n\tAdd this new optimization to the set of available optimizers.\n\t\n\t@arg _opt.optimize: The new optimization.\n\t@arg _c_impl: The C implementation that implements the optimizer.\n\t@arg _c_optimizations_required: Is the C implementation required\n\tif no C implementations are available to use this optimizer?\n\t@return: A new optimizer.\n\t\"\"\"\n\tresult = False\n\tfor"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn _c_impl._use_c_optimizations_required\n\t\n\n#ifndef __gl_h_\n#define __gl_h_\n\n#include \"OpenGLFunctions.h\"\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n    \n#ifdef __gl31spine1__\n\textern GLU_int _glXCreateGLcontext(const char* name);\n    \n#else\n\textern int glXCreateGLContextSGIX(Display *Display, GLXContext Context, GLbyte * const * pPbuffer, int width, int"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\tfrom warnings import warn\n\twarn(\"C optimizations are required, \"\n\t\t\"for use with numpy\", DeprecationWarning,\n\t\tEllipsisWarning)\n\treturn False\n\ndef _optimize_with_c(func, *args, **kwargs):\n\tif func.name!= \"optimize\":\n\t\twarn(\"`func` takes `optimizations` keyword as options; \"\n\t\t\t\"which is supported only by numpy < 1.6\",\n\t\t\tEllipsisWarning, ExactSpelling)\n\telse:\n\t\tif not cls.__c_is_iterable:"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn _use_c_impl()\n\n#######################################################\n##                           _use_c_implementation\n#######################################################\ndef _use_c_implementation(implementation=None):\n\t\"\"\"\n\tUse an implementation from a C compiler.\n\n\tThis generates code using code-blocks from the ``C_IMPL`` variable:\n\n\t= (function () {\n\t\tvar c = {\n\t\t\t\"__name__()\": function () {\n\t\t\t\tfunction"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn True\n\n\nclass _OptimizationSettings(object):\n\t_opt_level = 1\n\n\tdef default_impl(self, user=None):\n\t\t# type: (Any) -> Optional[ConfigurationOptimizationSettings]\n\t\t\"\"\"\n\t\treturn None\n\n\nclass ConfigurationOptimizationSettings(OptimizationSettings):\n\t\"\"\"\n\tRepresents the current optimization settings, useful in implementing\n\tclosures.\n\t\"\"\"\n\tdef __init__(self, optimize_config=None):\n\t\tself._optimize_config = optimize_config\n\t\tif self._optimize_"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn c_impl is not None\n\ndef _need_c_impl():\n\t\"\"\"\n\tCheck if C++ implementations are available on the current platform.\n\tIf none are available, this will be set as the default, and the\n\tplatform will be detected and returned in the standard Python\n\tlibrary.\n\t\"\"\"\n\tif sys.platform.startswith('linux'):\n\t\treturn\n\n\thas_sys_dir = os.path.exists(os.path.dirname(sys.implementation))\n\n\tif not has_sys_dir:\n\t\terror_class ="}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn 'PURE_PYTHON' in sys.builtin_module_names\n\ndef get_code_file():\n\t\"\"\"\n\tGet contents of the current build configuration file. \n\n\tThe content is an XML file in that, e.g., this example runs\n\twith `python3 build.py c cfg.py`. The content of this file\n\tcould be extracted with ``config_read``.\n\t\"\"\"\n\t# We use a file-like constructor (it does not support directories)\n\tif not is_file_created():\n\t\tfile = tempfile.mkstemp()\n"}]}
{"task_id": "62b87b989a0c4fa8b80b35ee", "prompt": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tcontext, bins = self.initial_context, self.init_bins()\n\treset_bin_context(context['bins'], self.bin_sizes, self.bins_default_value)\n\tself.histogram_table.append_column(\"bins\", bins)\n\tdataframe = 'bin_context' in context and context['bins']\n\tif dataframe:\n\t\tself.histogram_table.insert(\n\t\t\t0, self.init_bins.column_name + '=' + repr(\n\t\t\t\tdataframe.to_numpy()),\n\t\t\tvalue_from_"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself.current_dict = self.initial_value\n\tself.clear()\n\treturn self.__class__()\n\n# frozen_string_literal: true\n\nmodule Decidim\n  module Core\n    module Admin\n      # This command allows to perform any clean up necessary steps that need be done\n      # before it is rendered.\n      class CleanUp\n        # Public: Initializes the clean up steps.\n        #\n        # action ["}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tif self._initialized:\n\t\treturn\n\tself._initialized = True\n\tfor k in sorted(self._bins):\n\t\tself.hist.pop(k, None)\n\tself.max_val.reset()\n\tself._initialized = False\n\tself._nvalues = 0\n\n\tif self._min is None:\n\t\tself._min, self._max = [len(x.bin) for x in self.hist]\n\n\tfor p, bins in zip(self._histogram, self._bins):\n\t\tif p not in self._histogram"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tif self.use_histogram:\n\t\t\"\"\"\n\t\tHistogram should be reset in the end to start with\n\t\t\"\"\".strip()\n\t\tassert self._current_context is not None\n\t\tassert self._numbering is not None\n\t\tassert self._hist_list is not None\n\t\t\"\"\"\n\t\tassert self._context_name!= \"\":\n\t\t# \tDebug.debug('Resetting histogram {}'.format(self._context_name))\n\t\tself._context_name = \"reset_{}\".format(self._context_name)\n\t"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tif self.ctx is not None:\n\t\tself.ctx.start_copy(self.bin_dims=None)\n\t\tself.reset()\n\n\ndef _reset(self, n_bins=0, make_copy=False):\n\t\"\"\"Reset the histogram\n\t\n\tParameters\n\t----------\n\tn_bins : int\n\t\t\tNumber of bin coordinates to store.\n\tmake_copy : bool\n\t\t\tTrue to make a deep copy of the histogram \n\t\"\"\"\n\n\tif (n_bins - 1) <= 0:\n\t\tn_bins ="}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\ttry:\n\t\treturn bins(self.min, self.max)\n\texcept Exception, e:\n\t\treturn reset()\n\n\ndef _filter_listdata_dict(obj, keys, val):\n\tfor key in keys:\n\t\tif not val in obj[key].keys():\n\t\t\tdel obj[key].keys()\n\n\treturn {key: val for key in obj}\n\n/* -*- Mode: C++; tab-width: 2; indent-tabs-mode: nil; c-basic-offset: 2 -*- */\n/* ***** BEGIN LICENSE BLOCK *****"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself.data = None\n\tself.count = 0\n\tself.values = []\n\t\n\treturn self\n\ndef histogram(vals, min_value, max_value, initial_value=None, fillvalue=0): \n\t\"\"\"\n\tHistogram\n\t\n\tArguments:\n\t- vals: list, either an iterable that contains each value \n\t   or numpy array that is the histogram.\n\t- values: list, a 1..4 dict mapping to each value and\n\t   which should be assigned an update_id.\n\n\t\n\tReturns:\n\t\tfloat: updated hist"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself.dict.clear()\n\tif len(__builtins__.__dict__()) > 1:\n\t\traise TypeError(\"Too many arguments for reset().\")\n\telse:\n\t\tfor x in [self.histogram]:\n\t\t\tx.values.clear()\n#\t\tx.reseed()\n#\t\tx.to_flat_map()\n#\t\tx.__dict__ = self.dict\n\t\tself.histogram = self.dict.as_dict()\n#\t\tself.last_hash = 0\n\t\tself.current = None\n#\t\tself"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself.create_context()\n\tfor l in self.get_bins():\n\t\tl.reset()\n\treturn\n\n/*\n * This file is part of the SSH Library\n *\n * Copyright (c) 2010 by Aris Adamantiadis\n */\n\n#ifndef __KSSHND_SSH_CLIENT_H\n#define __KSSHND_SSH_CLIENT_H\n\n#include <libssh2.h>\n\n#include <openssl/evp.h>\n#include <openssl/rand.h>\n#include <openssl/ssl.h"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself.bin_reinit = []\n\tself.bins_reinit = []\n\tself.max_count = 10\n\tself.min_count = 0\n\tself.max_value = 10.0 \n\tself.avg_value = self.max_value\n\tself.current_reinit = self._reset_reinit()\n\t\n\t# If no context was supplied, initialize all values\n\tif self.initial_context == '*':\n\t\tself.initialize_values()\n\t\n\tfor t in self.bins:\n\t\tfor value in t['bins"}]}
{"task_id": "62b87b859a0c4fa8b80b35d7", "prompt": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tfrom.graph import Graph\n    # Convert (point, string) to a dict with following keys:\n    #\n    # Point\n    #   'name'\n    #   'comment'\n    #   'domain'\n    #       'name' or 'comment'\n    #   'number'\n    #   'location__lat'\n    #   'location__lon'\n    #   'area__width'"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tif isinstance(self, Graph) or isinstance(self, seq.Graph):\n\t\treturn self\n\n\tif not isinstance(self, (Graph, seq.Graph, Node)):\n\t\traise ValueError('A %s is required to be exported, '\n\t\t\t'but a %s is given.' % (self.__class__, self.__class__))\n\t\n\tif separator not in (\"\", \"comma\", \",\")}:\n\t\traise ValueError('Unsorted column names can only be used with sequence graphs or graph columns.')\n\t\t\n\tgraph = Graph(self.nrows, name=\"%s."}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport collections\n\tif separator is None and has_len(len(self)):\n\t\treturn self._to_csv(separators = \" \", header=None)\n\tfrom collections import Counter, CounterInterface, itertools\n\tfrom warnings import warn\n\tfrom nltk.data import table_to_table\n\tif header is None:\n\t\t# If the graph is from a file, use the file's header line.\n\t\theader = \"\"\n\tif not separator.strip():\n\t\terror = \"`separator` parameter needs to be a string, but got %s?\""}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tfrom sage.graphs.graph import Graph\n\tgraph = Graph(separator=separator, header=header)\n\tfor p in graph.points:\n\t\tcsv_print(p.to_csv(separator=\",\", header=None))\n\n\nclass Output(dict):\n\t\"\"\"\n\tOutput of :class:`sage.graphs.graph.Graph` to a file.\n\t\n\tThe filename passed should be a list of strings, not a sequence.\n\t\"\"\"\n\tdef __new__(self, filename):\n\t\t\"\"\"\n\t\tGiven a filename, return a list of Graph objects"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tfrom collections import defaultdict\n\t\n\tfrom types import GeneratorType\n\tfrom itertools import chain\n\t\n\tsep = separator\n\tcsv_header = ['id', 'x', 'y', 'z']\n\t\n\tdef to_csv(self, separator=\",\", header=csv_header, append=False):\n\t\t\"\"\" Convert graph's points into CSV formated by collapsing headers:\n\t\t*header*, if not ``None``, is the first string of the output\n\t\t(new line is added automatically).\n\t    \n\tThe following example gives the following graph:\n"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t# --------- Deprecated. Use ~Lena.output.ToCSV.\n\t#      for backwards compatibility\n\treturn self.ToCSV(*separator=separator, verbose=0)\n\n\ndef _get_output_header(name):\n\t\"\"\"\n\t    returns the output header\n\t\"\"\"\n\tfrom itertools import chain\n\tfrom itertools import islice\n\n\tif islice(name):\n\t\t# noinspection PyShadowingNames\n\t\tout = '\\n'.join(name.split() + [None] * 5)\n\telse"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tif separator is None and self._columns is None:\n\t\tseparator = \", \"\n\t\n\tif header is not None:\n\t\tself._column_headers = tuple(header)\n\t\tself._header = Header(*separator.join(header))\n\t\t\n\t\tif self._columns is None:\n\t\t\tself._columns = ()  # only convert each column\n\t\tif not (is_list(self._columns) and len(self._columns) == 1):\n\t\t\traise ValueError(\"Cannot sort columns. Each column must be a cell column.\")\n\n\nclass Column:"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tif header is not None:\n\t\tseparator = \" or '||'\"\n\ttable = self._to_table(separator=separator, header=header)\n\treturn table.to_csv(separators=separator)\n\n\ndef _generate_graph(\n\tmax_graph_length,\n\tseparator=\" or \",\n\theader=2,\n\tmax_nodes_graph=1000,\n\tmax_edges_graph=10,\n\toutput_dir=\"\"\n):\n\t'''\n\tGenerate a list, of Graph objects.\n\t:param str column: name of graph"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t# convert graph itself in place\n\tresult = []\n\tif header is None:\n\t\tdata = self.data(separator=separator)\n\t\tfor node in data:\n\t\t\tresult.append(node[1])\n\n\tif isinstance(self.edge, Graph) and not self.edge.is_graph():\n\t\tresult.append(self.edge)\n\t\t\n\tif self.graph is self:\n\t\t# in case we don't need data, convert it to CSV\n\t\tresult = result[0]\n\telif isinstance(self.graph, Graph"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport warnings\n\twarnings.warn('graph.to_csv is deprecated in favor of '\n\t\t'graph._to_csv', DeprecationWarning)\n\treturn self.to_csv()\n\ndef to_csv(self, rowsep='\\\\t', colsep='\\\\t', quotechar='\\\\t', encoding='utf-8', header=None):\n\tr\"\"\"\n\t\tCreate a csv string to add (1 - newline) columns with values.\n\n\t\tParameters\n\t\t----------\n\t\trowsep :\tcomma in (1 - newline) row separator\n\t"}]}
{"task_id": "62b87b839a0c4fa8b80b35cb", "prompt": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tindices = list(self.model.get_coord(coord_name))\n\treturn indices or self.err_indices\n\ndef _get_err_indices_by_name(name, coord_name_length):\n\tindices_ = self.model.coord_at_name(name, coord_name_length)\n\treturn indices_\n\nclass ErrorCoord(object):\n\tdef __init__(self, name, coord_name, error_code = None, errmsg = None):\n\t\tself.name = name\n\t\tself.coord_name = name\n\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tresult = []\n\tfor err_code, error_id in ERROR_CODE_MAP.items():\n\t\tif coordinate.ERR_COORD.matches(err_code):\n\t\t\tresult.append(coord_name+':'+error_id)\n\t\telif err_code <= 32:\n\t\t\tresult.append('Error')\n\n\tres = {}\n\tfor idx in result:\n\t\tres[err_codes[idx]] = idx\n\treturn res\n\n#include <map>\n#include <vector>\n#include <cstdio>\n#include <"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\terror_indices = []\n\tif coord_name not in coord_names:\n\t\traise ValueError(\"Coordinate %s is not in coordinate_names: %s\" % (coord_name, coord_names))\n\treturn self._indices[coord_name]\n\n\ndef get_coord_names(index):\n\t\"\"\"\n\tRetrieves a list of identifiers (column names) of all coordinate names.\n\t\tThe identifiers are listed from highest (highest-most) ordinal\n\t\tfirst, and the coordinates are in order (highest-most or lowest-most\n\t\tfirst)"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\terr_to_index = {coord.error_name for coord in self.get_coord_infos()}\n\treturn list(filter(lambda x: x.error_name in err_to_index, self.coord_indexes_))\n\n#region Using Directives\nusing System;\nusing System.ComponentModel;\nusing System.Drawing;\nusing System.Runtime.InteropServices;\nusing System.Windows.Forms;\n\nusing SharpDX.Mathematics.Interop;\n\nusing SharpDX.XInput;\nusing SharpDX.XGraphics;\nusing SharpDX.XInput.M"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tassert type(coord_name) is str\n\tif coord_name == 'WGS84':\n\t\treturn self._coords_int + self._coords_long * self._coords_long_size\n\telse:\n\t\tcoords = np.empty((self._coords_int + 1) * self._coords_long_size,\n\t\t\t\t\t\t'coord_idx')\n\t\tfor i in range(self._coords_int):\n\t\t\tcoords[i] = (coord_name, self._coords_long[i])\n\t\treturn coords\n\n\t\ndef _get"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from collections import namedtuple\n\tcoord_names = list(_get_coord_names(coord_name, 'Error'))\n\treturn namedtuple('_getitem_error_err_indices', coord_names)\n\n\ndef _get_coord_names(coord_name, suffix=None):\n\t\"\"\"\n\tGet variable list for each coordinate in 'namedtuple' argument.\n\t\"\"\"\n\tvar_names = {}\n\tfor v in _get_var_names(coord_name=coord_name, suffix=suffix):\n\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t#if DEBUG: print out 'Coord:    Error Index:     CoordName:   Index'\n\tcoord = self._coorddict[coord_name]\n\n\t#  return the error indices\n\tmsg = \"Warning - Coord: coordinate %s with name 'CoordName' %s is using the coordinate %s.\" %(coord.name, coord.name_string, coord_name)\n\tif coord.error is None:\n\t\tcoord.error = [None] * 20\n\treturn coord.error,coord.error_index\n\ndef _get"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\treturn self.get_coord_names_map(coord_name)\n\n\ndef _get_err_indices(self, coords):\n\t\"\"\"\n\t\tReturns a list of coordinate names that are the same with the coord_names mapping.\n\n\t\te.g. eck_c_xy is the right-most coordinate in the cartesian system,\n\t\t\teck_x2c_xy is the right-most coordinate in the\n\t\t\tcartesian system, etc.\n\n\t\te.g.\n\n\t\tcoord_names_map = {'e1':"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tif coord_name in self._err_indices:\n\t\treturn self._err_indices[coord_name]\n\telse:\n\t\treturn []\n\ndef _get_error(self, coordinate_name):\n\t\"\"\"\n\tReturn a unique error.\n\t\"\"\"\n\tif coordinate_name in self._errors:\n\t\treturn self._errors[coordinate_name]\n\telse:\n\t\treturn None\n\ndef _get_error_indices_for_coord(coords_to_get):\n\t\"\"\"\n\tGiven a list of coordinates, return the corresponding error"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tcoords = CoordinateModel.get_coords(self, coord_name)\n\tindices = []\n\tfor point in coord.points:\n\t\tindices += [0] + [i for i in range(self.point_range[coord_name][1]) if i.x <= point.x-self.point_range[coord_name][0]]\n\tif len(coords)!= len(indices):\n\t\tprint('Error occurred: Some coordinates are wrong and should exist with some indices')\n\telse:\n\t\treturn indices\n\n\n_err_indices_map ="}]}
{"task_id": "62b87b7e9a0c4fa8b80b35bc", "prompt": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tgraph = graphviz.GloablContext()\n\tfor key, values in graph.named_graphs():\n\t\tfor x, y in values.values():\n\t\t\tgraph.values()[x].append(y)\n\t\t\tgraph.values[y[0]}\n\t\t\t\n\tgraph.values()\n\tgraph.nodes[:] = []\n\t\t\n\tgraph.error.append(context.error[0])\n\n\tgraph.value = []\n\tfor x, y in graph.values():\n\t\tgraph.value.append((x + 6*err_scale))\n\t\t"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tkeys = self._fields.keys()\n\tassert len(keys) == len(context.field_names), \"Key names are not unique.\"\n\t\n\tkeys_new = ['id', 'name', 'label', 'type', 'errors', 'value']\n\tcontext_field_names = set(keys_new)\n\t\n\tfor key in keys_new:\n\t\tcontext.value.remove(key)\n\t\n\tfor name, field in list(context.field_names.items()):\n\t\tfor name, p in context.field_names:\n\t\t\t_set"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t# NOTE: we do not want to use context.keys(),\n\t# as that would break Python'self.fieldnames'\n\t# with circular references.\n\n\t# make self a new object for error contexting\n\tself._value = _make_value(context.error)\n\tif len(self._values) > 1:\n\t\tself.fieldnames = self._values.pop(0)\n\telse:\n\t\tself.fieldnames = self._values[0]\n\tself._values.append(\"E\")\n\tfor name in self.fieldnames:"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext = self._context\n\t\n\tgraph = context.graph\n\tdef _update_context_with_node(n, context):\n\t\tcontext.node[n.index] = n\n\t_update_context = _update_context_with_node\n\tdef _add_to_graph(lgraph, n, context):\n\t\tif n.index not in context.graph.nodes:\n\t\t\tcontext.graph.add_node(n, lgraph.edges.add(), self, n.index)\n\t\telse:\n\t\t\tcurnode = context."}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif self._context_stack[self.stack_depth].error_E_low!= \"\" and self._context_stack[self.stack_depth].value is None:\n\t\t# We are calling the update to an E_low field, so there cannot\n\t\t# possibly be another to_string() method.\n\t\tif isinstance(self._context_stack[self.stack_depth].value, E_low):\n\t\t\tcontext_text = 'the x_low field of a %s is an E_low value \"%s\"' % (self._context_stack[self"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tfrom delf import graph\n\ttry:\n\t\tgraph.context = context\n\texcept:\n\t\t# There really isn't a context - it may have been garbage\n\t\t# collected, so skip the whole thing\n\t\treturn\n\n\tcontext.update()\n\tcontext.set_fieldnames(context.fieldnames)\n\n\n# TODO(danms): move away as _update_context was renamed.\ndef reset(graph):\n\t\"\"\"\n\tReset (copy) graph properties from another object to this one.\n\t\"\"\"\n\tfrom delf import Graph\n\t"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext = self._get_context()\n\tcontext.error = self._get_error_names()\n\t_update_context(self, context)\n\n### # Modeling graphs\n###\n@pyqtgraph.error_def\ndef _modelgraph(self):\n\"\"\"\n\tLoad model graph *from* a graph and update graph with its\n\tmodel (a graph with multiple layers).\n\tExample of a model graph:\n\t\n   .. code:: python\n        from qgisgraph.QGISGraph import _modelgraph\n   "}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext = self._graph.children[-1]\n\tattributes = context.attributes\n\tassert len(attributes) == len(context.data), \"context has %d attributes but %d data\" % (len(context.data), len(context.attributes))\n\t# Add an attribute if we don't have it already\n\tif context.attributes:\n\t\tattributes[-1] = context.attributes[-1]\n\t\n\t# Reset all attribute names\n\tself.__setitem__(context.to, \"x_low\", None)\n\tself.__setitem__(context.to, \""}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tsetattr(self.base, self.graph_defs['def_' + self.def_name], context)\n\n\tindex_context = Context(key = self.index_name)\n\ttry:\n\t\tindex_context_values, indices, indices_info = _extract_index_info(self)\n\t\tfor context_value in index_context_values:\n\t\t\tcontext._index_info[context_value] = indices_info[indices]\n\texcept NotImplementedError:\n\t\t# Index is not present, so do nothing and raise an error\n\t\tpass\n"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif not hasattr(self, '_context'):\n\t\tself._context = get_context(context) \n\t\tself._parse_fields()\n\telse: \n\t\traise ValueError(\"cannot update current context\") \n\n\t# Update _error_map, since we don't know what to do\n\tfor idx, value in enumerate(self._context.value):\n\t\tidx_error = self._context.value[idx][3]\n\t\tif idx!= len(self._context.error):\n\t\t\tvalue_error = value[idx_error] # just an identity map"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3580", "prompt": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\timport numpy as np\n\t\"\"\"\n\tCompute integral using nearest neighbor sampling function\n\t\n\t*bins* contain the values, and *edges* contain the values form a 2D\n\tinterpolation of (x,y) in the mesh around a bin (y-1) \n\t\n\t*edges* contain the number of vertices \n\t\n\t*bins* contain the values, corresponding to the vertices of\n\tthe mesh\n\t\n\t*edges* are assumed to be odd\n\t\n\t*bins* contain values for the edge weights\n\t\n\t*edges are assumed to be odd\n\t\n\t*bins* contain values"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\txrange = sorted([int(float(arg)) - float(float(edge))) for \\\n\t\t\targ in xrange(bins, edges)]\n\t\n\tr2c = _quantile([0.1]*2 for _ in xrange)\n\txrange = sorted(map(int, xrange))\n\tr2c[1] = 0.5\n\t\n\treturn np.exp(cumsum(xrange, r2c)*xrange*0.5)\n\n\ndef discrete_gradient(bins, edges):\n\t\"\"\"\n\tCompute discrete gradient. The bins"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\treturn [int(x) for x in histogram(bins, edges)]\n\n# frozen_string_literal: true\n\nRSpec.describe \"Users\" do\n  include_context :user_class\n\n  let!(:user) { create(:user) }\n\n  it \"gets the login and avatar via GET\" do\n    get \"/users/1\", nil, as: :json\n    expect(response).to have_http_status :success\n    expect(session[:current_user]).to eq user\n  end\n\n  it \"gets"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\timport math\n\tfrom math import trunc, pi\n\n\t# Initialize\n\tedges = edges.reshape(-1, 1)\n\tcounts = dict()\n\tbins = 1.0 * edges ** 0.5**(2*edges)\n\n\t# Compute integral\n\tfor i in range(1,bins+edges+1):\n\t\tcounts.setdefault(i,'%s%d' % ('%'.join([str(x) for x in counts[i-1:i+1]),'0']))\n\t\tfor j in range(i+1,bins+edges+"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\treturn histogram([(i/bin[i], i/bin[i+1]) for j in range(graph.N)])\n\t\ndef laplacian(graph, bins=10, edges=1):\n\t\"\"\"Performs a Laplacian to smooth graph connectivity\n\t\n\tArguments:\n\t\tgraph : nxpy.Graph\n\t\tbins : int\n\t\t\tNumber of bins. Default value is 10.\n\t\tedges: int\n\t\t\tMaximum number of edges. Default is 1.\n\n\t\tReturns array of Laplacians.\n\t\"\"\"\n\tif"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\treturn integrate(bins, edges, method=2, algorithm=2)\n\n\n\ndef integral_all(bins = 1000, edges = 1000, method=2, algorithm=4, *args, **kwargs):\n\t\"\"\"\n\tCompute integral (every histogram) over all elements.\n\t\n\t*bins* contain values, and *edges* are form the mesh for each element.\n\tTheir format is defined in :class:`.histogram` description.\n\t\n\t*args* and **kwargs* are passed to the algorithm as kwargs.\n\t\"\"\"\n\treturn integrate(bin="}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\treturn histogram(bins, edges, scale=1.)\n\n\ndef histogram_exact(bins, edges, scale=1., min_size=0., bins_by_index=False):\n\t\"\"\"Return an exact approximation of a histogram.\n\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tif bins_by_index:\n\t\tscale_factors = scale1d_for_histo(bins, edges, scale, bins)\n\t"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t# Make a copy so we have a reference to other attributes\n\tgraph = copy.deepcopy(histogram)\n\n\t# Set the bins\n\tfor i in bins:\n\t\tnum_bins = 0\n\n\t\tfor j in range(graph.size):\n\t\t\tif (j == i or graph.data[i, j] == 0):\n\t\t\t\tnum_bins += 1\n\t\t\telse:\n\t\t\t\tbreak\n\t\tnum_bins = max(num_bins, bins)\n\t\tgraph[i] = max(0, max(graph.data[i"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tif len(bins) == 0 or len(edges) == 0:\n\t\traise ValueError(\"must have either 0 or 1 value\")\n\tret = binning_function(bins[0], edges[0])\n\tfor j in range(bins[0]+1):\n\t\tk.append(float(edges[j]))\n\t\tfor i in range(bins[1]+1):\n\t\t\tret.append(float(edges[k.index(i)][j]))\n\tret = (ret[0] + numpy.sqrt(sum(numpy.array(ret*ret"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\treturn histogram(edges, bins=bins)\n\n# Copyright 2013-2021 Lawrence Livermore National Security, LLC and other\n# Spack Project Developers. See the top-level COPYRIGHT file for details.\n#\n# SPDX-License-Identifier: (Apache-2.0 OR MIT)\nfrom spack import *\n\n\nclass PySamp(PythonPackage):\n    \"\"\"General Samp SSD (SSDP) class and module\"\"\"\n\n    homepage = \"https://github.com/rahaych/ssdp\"\n   "}]}
{"task_id": "62b87b199a0c4fa8b80b354e", "prompt": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\ttry:\n\t    next(seq)\n\t    if not(type(seq) is SourceSequence):\n\t\treturn False\n\texcept: pyasn1.Error:\n\t    return False\n\telse:\n\t    if len(seq)!=0:\n\t        return False\n\treturn True\n\n\n\nclass ListSource(sequence.Sequence):\n\tname = 'list'\n\telement = list\n\nclass SetSource(sequence.Sequence):\n\tname ='set'\n\telement = set\n\nclass DictSource("}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tif isinstance(seq, (Source, FillRequestSeq)):\n\t\treturn True\n\tfor element in seq:\n\t\tif not is_fill_request_seq(element):\n\t\t\treturn False\n\treturn True\n\t\nfrom test import (\n\tassertTuplesEqual, printSeqToPretty, assert_equal, assert_raise,\n\tassertTupleEqual, assert_array_equal, assertListEqual, assertListEqual,\n\tassertDictEqual, assertDictEqual, assertSetEqual, assertDictEqual,\n\tassertDictEqualWithLength,\n\n\tassertStringEqual,"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t_Element_type = Union[FillRequestSeq, Sequence[FillRequest]]\n\tdef get_element_type(obj):\n\t\treturn None\n\n\tif __name__ == '__characterize__':\n\t\tif 'Element' in _Element_type.__dict__:\n\t\t\treturn Element\n\telif __name__ == '__elements__':\n\t\tif 'Element' in _Element_type.__dict__:\n\t\t\treturn ElementClass\n\t\telse:\n\t\t\treturn Objective\nelif issubclass(SourceSequence, Sequence):\n\tdef is_fill_request_seq(seq"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\ttry:\n\t\to = _get_object(seq)\n\t\tif o is None and _len(seq) < 2 and _len(seq[0]) < 2:\n\t\t\treturn False  # Can be converted to FillRequestSeq\n\t\tseq = seq[0:1]  # (source, sequence, fill_seq)\n\t\tfor c in seq:\n\t\t\tif isinstance(c, Source):\n\t\t\t\treturn False  # Can be converted to FillRequestSeq\n\t\treturn True \n\texcept (TypeError, LookupError) as err:\n\t\t"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tif seq is None or (seq!= SourceSequence('FILL')):\n\t\t# no such element\n\t\treturn 1\n\t\t\t\n\telse:\n\t\t# it is a source sequence\n\t\treturn 0\n\n_default = SourceSequence('None')\nis_fill_request_seq = lambda seq: _default == SequenceClass(seq)\n\n\ndef is_source_seq(seq):\n\t\"\"\"\n\tTest whether *seq* has a source sequence\n\t\n\tTrue if it is a FillRequest or Source sequence.\n\t\"\"\"\n\treturn _is_source_seq("}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tfor element in seq:\n\t\tif hasattr(element, 'is_fill_request_seq'):\n\t\t\treturn element.is_fill_request_seq(seq)\n\treturn False\n\n@unittest.skipUnless(not is_sequence_element, 'is not a SequenceElement')\ndef same_seq(seqA, seqB):\n\t\"\"\"\n\tTest whether all elements equal.\n\t\n\tFalse if elements are equal, and in\n\tequality-order, which is not necessarily as equal.\n\t\n\tParameters\n\t----------\n\tseqA, seqB\t\n\t\t"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tfor el in seq:\n\t\tif \"FillRequestSeq\" in el.tag:\n\t\t\treturn True\n\treturn False\n\n\ndef is_fill_request_seq(seq):\n\ttry:\n\t\tnew = FillRequestSeq.fromstring(seq)\n\t\treturn new.tag == \"_FillRequestSeq\"\n\texcept:\n\t\treturn False\n\n\ndef is_copy_request(seq):\n\ttry:\n\t\tnew = CopyRequestSeq.fromstring(seq)\n\t\treturn new.tag == \"_CopyRequestSeq\"\n\texcept"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tif not (len(seq) == 1 and isinstance(seq[0], FillRequest)):\n\t\treturn False\n\treturn bool(seq[0].id())\n\ndef get_type(node):\n\t\"\"\"\n\tReturn the type(type) of *node*\n\t\"\"\"\n\tif isinstance(node, FillRequest):\n\t\treturn 'FillRequest'\n\tif isinstance(node, Source):\n\t\treturn 'Source'\n\treturn 'Type'\n\ndef flatten(list_of_elements):\n\t\"\"\"\n\tFlatten the list of tree elements from *list_"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tlast = ''\n\tpos = 0\n\tfill_context = ()\n\twhile not (fill_context() == last) == False:\n\t\tpos += 1\n\t\tif (pos > 3) and fill_context() == last:\n\t\t\t# Check whether the last source element is still valid.\n\t\t\tif (last not in ('', 'Source', 'SourceSeq', 'SourceSeqList', 'SourceList', 'SourceFile', 'SourceFileID'):\n\t\t\t\traise PyBugOrmBug(\"Source element was found while parsing FillRequestSeqSeq. This should not occur. (Last"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tif seq:\n\t\tif type == Sequence and not isinstance(seq, (List, tuple)):\n\t\t\traise ValueError(\"is_fill_request_seq only accepts lists or sequences.\")\n\t\n\treturn isinstance(seq, (list,tuple)) or seq is not None or False\n\ndef _unquote_str(s):\n\treturn s.decode('utf8') if s else \\\n\t\t\tbase64.b64encode(_b2a(s)).decode('utf8')\n\ndef strip_nonansi_chars(seq):\n\tif '(' not in seq:\n\t"}]}
{"task_id": "62b87b099a0c4fa8b80b3538", "prompt": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from sp_utilities import get_im, get_params\n\tpass#IMPORTIMPORTIMPORT from sp_fundamentals import rot_shift3D\n\tpass#IMPORTIMPORTIMPORT from sp_filter import filt_ctf\n\tpass#IMPORTIMPORTIMPORT from sp_filter import filt_ctf2\n\tpass#IMPORTIMPORTIMPORT from sp_utilities import get_image, model_blank, bdb\n\tpass#IMPORTIMPORTIMPORT from sp_global_def import gfile\n\tpass#IMP"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\tif id(obj) in EXEC_TYPES:\n\t\treturn\n\tif obj.is_type and issubclass(obj.type, Oozie):\n\n\t\tfor arg in obj.args:\n\t\t\tif not issubclass(arg, dict):\n\t\t\t\tcontinue\n\t\t\t#print \"argument %s\" % arg\n\t\t\tif is_function(arg):\n\t\t\t\treturn arg\n\n\t\t#print \"bad executable type %s passed to 'fill' %s\" % (id(obj), id(obj))\n\t\tfor arg in obj.args:"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\tpass\n\n@n_check_decorator(n_fill_check)\ndef n_check_fill_and_request_el(obj, *args, **kwargs):\n\t\"\"\"\n\t\tObject contains executable methods 'fill' and'request'.\n\n\t\tFunction checks that it's defined using the current\n\t\timplementation's 'fill' method.\n\n\t\"\"\"\n\traise NotImplementedError(\n\t\tcurrent_implementation.get_message_from_argspec(args, kwargs)\n\t)\n\ndef n_get_method(name):\n\t\"\"\"\n\tObject contains"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn obj is not None and hasattr(obj, '_fill_request_el')\n\ndef is_fill_request(obj):\n\t\"\"\"\n\tObject contains a 'fill' method (if the function exists). Return 'True'\n\tfor an object, which is an'request' object, if the function exists.\n\t\"\"\"\n\treturn ( is_method(_('fill'), obj), )\n\ndef is_fill_response(obj):\n\t\"\"\"\n\tObject contains an 'fill' method (if the function exists). Return 'True'\n\tfor an object, which is an"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn is_method(obj, 'fill') and is_method(obj,'request')\n\ndef is_fill_req_el(obj):\n\treturn is_method(obj, 'fill') and is_method(obj,'request')\n\ndef is_isolation_req_el(obj):\n\treturn is_method(obj, 'isolation') and is_method(obj,'request')\n\ndef is_max_request_el(obj):\n\treturn is_method(obj,'max') and is_method(obj,'request')\n"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\tif not hasattr(obj, 'fill'):\n\t\traise TypeError('%s.fill required but not defined' % obj)\n\telse:\n\t\tclass FillRequest(object):\n\t\t\t\"\"\"\n\t\t\tFillRequest: returns the fill request that was asked for\n\t\t\t\"\"\"\n\t\t\tdef fill(self):\n\t\t\t\t# type: (FillRequest) -> str\n\t\t\t\treturn'request'\n\t\t\tdef fill(self):\n\t\t\t\t\"\"\"\n\t\t\t\tFillRequest: returns the fill request that was asked for.\n\t\t\t\tReturns '' if none is available.\n\t\t\t"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from iotbx.phil.io import fill_request_el\n\tpass#IMPORTIMPORTIMPORT from iotbx.phil.io import fill_object_by_ids\n\tif (str(type(obj)).lower().replace('__', '')!= 'fill'):\n\t\traise RuntimeError('fill_object_by_ids is only available for functions')\n\tobj_type = type(obj)\n\tif not is_fill(obj):\n\t\traise RuntimeError('fill objects must be strings', obj)\n\tif (not isinstance("}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t##\n\treturn obj.is_fill_request_el()\n\ndef is_fill_response_el(obj):\n\t\"\"\"\n\t\tShould contain 'fill','request' and 'content' at this level\n\t\"\"\"\n\n\t##\n\treturn obj.is_fill_response_el()\n\ndef is_fill_content_el(obj):\n\t\"\"\"\n\t\tShould contain 'fill','request' and 'content' at this level\n\t\"\"\"\n\t##\n\treturn obj.is_fill_content_el()\n\ndef get_content_"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import types\n\tpass#IMPORTIMPORTIMPORT from sp_utilities import wrap_get_params\n\tpass#IMPORTIMPORTIMPORT from sp_utilities import get_input_val, get_input_types\n\tpass#IMPORTIMPORTIMPORT from sp_applications import MPI_start_end\n\tpass#IMPORTIMPORTIMPORT from math import log10\n\tpass#IMPORTIMPORTIMPORT from sp_fundamentals import fft\n\tpass#IMPORTIMPORTIMPORT import sp_global_def\n\tpass"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t# TODO:: need 'get' method\n\treturn self._el.getElementsByTagName('fill')[0].getElementsByTagName('request')[0].tagName\n\n\n\nclass WF_Request(WFElement):\n\n\t__doc__ = WFElement.__doc__\n\n\tdef load_request(self):\n\t\tWFElement.__set_state(self)\n\n\nclass WF_Response(WFElement):\n\n\t__doc__ = WFElement.__doc__\n\n\tdef load_response(self):\n\t\tWFElement.__unset_state(self)\n"}]}
{"task_id": "62b87af99a0c4fa8b80b3524", "prompt": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn is_pyobject(obj)\n\n@runfn\ndef runfn(obj):\n\t\"\"\"\n\tThis method handles running a Python object.\n\t\n\tArguments:\n\tobj:                 The object to execute as an executable python method.\n\t\n\t...\n\trun-python, run-python-shell, run-python-run, run-python-run-shell, run-python-run-shell-as\n\t\"\"\"\n\t# This is a list of shells supported by `Python.Shell`\n"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\tif isinstance(obj, (None, _BaseScriptObj)):\n\t\tobj = obj.scriptobj\n\t\n\treturn is_run_from_def(obj, [r\"^run\\w+\", _GetDefName(r\"run\")]),\n\t\t\t\ndef is_run_from_def(*args):\n\t\"\"\"\n\t\tReturn True when 'python run' command is being run from python file.\n\t\"\"\"\n\treturn False\n\ndef is_run_from_python(obj):\n\t\"\"\"\n\t\tReturn True when 'python run' command is being run from python."}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from cStringIO import StringIO\n\tpass#IMPORTIMPORTIMPORT import os\n\n\tpass#IMPORTIMPORTIMPORT from sp_utilities import get_all_fnames\n\n\t#from sp_utilities import get_all_attributes\n\t#from cStringIO import StringIO\n\n\tcmd=\"\", filename = obj.extract_one_image(input_images).file_name\n\tcmd += \" --data-root=data_root directory=\" ; os.system(cmd)\n\tcmd += \" --no-fflush "}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t#obj        =          obj\n\tobj\n\treturn obj.is_run_el()\n\n\ndef is_exit_el(obj):\n\tobj     =       obj\n\treturn bool(obj.is_exit())\n\n\ndef main(*args):\n\targs       =       args\n\tenv        =       \"env\"\n\tdef_conf   =   "}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn is_class(obj, base_exc)\n\ndef invoke_el(obj, name):\n\tmsg = 'invoke %s %s' % (name, typeid(obj).name)\n\tif is_class(obj, base_exc):\n\t\tr = obj.run(msg, name, 0)\n\t\tmsg += r + '\\n'\n\t\treturn msg\n\telse:\n\t\treturn ''\n\n        \n\n\nclass ExecutableModule(object):\n\n\n\nclass BaseCommand(object):\n\n\t"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn isinstance(obj, types.Run)\n\n\ndef is_script(s):\n\t\"\"\"\n\tCheck if the python interpreter's script is present in the output.\n\t\"\"\"\n\treturn s.startswith(\"<script file>\", \"path-to-python\")\n\n\ndef is_python_script(s):\n\t\"\"\"\n\tCheck if the python interpreter's script is present in the output.\n\t\"\"\"\n\treturn s[:1024] == 'python'\n\n\ndef is_python3_script(s):\n\t\"\"\"\n\tCheck if the python interpreter's script is present"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn obj.el is None or obj.el is not ELE\n\nis_el_init = is_init\nis_run = is_run\n\n\n@python_2_unicode_compatible\n@python17_compatible(with_doc=False)\ndef replace_vars(obj, orig_obj):\n\t\"\"\"\n\tReplace functions that were not explicitly defined in a module with an\n\tinstance variable.\n\t\"\"\"\n\treturn getattr(obj, orig_obj)\n\n\n@python_2_unicode_compatible\n@python17_compatible(with_doc=False"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\tif not is_exe(obj):\n\t\treturn True\n\tif obj.is_kernel():\n\t\treturn False\n\tif 'run' in obj.kwargs:\n\t\targs = obj.kwargs['run'].get(obj.name.split('.')[0].upper())\n\t\tif args:\n\t\t\tif len(set(args)) > 1 and args[0] not in set(['-c','--clean'],unset=['-h']) \\\n\t\t\t\tand obj.kwargs['run'].pop(args[0]) is not None and obj.kwargs['run'].pop"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\tfrom. import _el_methods\n\tret = getattr(obj, '_el_methods', {})\n\tret = set(ret)\n\treturn ret.get(obj, \"\") or ret.get(EL_CLASS, \"\") or ret.get(EL_CONSTRUCTOR, \"\") or ret.get(EL_METHOD, \"\") or ret.get(\n\t\tEL_ATTRIBUTE_NAME, \"\") or ret.get(EL_CONSTRUCTOR_NAME, \"\") or ret.get(EL_MODULE_NAME, \"\") or ret.get(\n\t\tEL_METHOD_NAME, \"\") or ret.get("}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from StringIO import StringIO\n\timport hashlib\n\timport itertools\n\tpass#IMPORTIMPORTIMPORT from errno import EAGAIN as EAGAIN_RETCODE\n\tpass#IMPORTIMPORTIMPORT from errno import EINVAL as EINVAL_RETCODE\n\tpass#IMPORTIMPORTIMPORT from os.path import isfile, isdir, join\n\tpass#IMPORTIMPORTIMPORT from os.path import basename\n\tpass#IMPORTIMPORTIMPORT from sys import version_info as py_version\n\tpass#IMPOR"}]}
{"task_id": "62b87af69a0c4fa8b80b351a", "prompt": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tif 'el' in obj:\n\t\tobj.el = el_def.fill_obj\n\t\treturn obj\n\treturn fill_obj.fill_obj\n\ndef is_fill_compute_fn(obj):\n\t\"\"\"\n\tObject contains 'compute' function\n\t\"\"\"\n\tif not obj:\n\t\treturn False\n\telif isinstance(obj, str):\n\t\tif obj.lower() == 'compute':\n\t\t\treturn True\n\treturn False\n\n\ndef set_fill_compute_el_attr(fill_obj, el_attr):\n\t"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tpass\n\n/*\n * Copyright 2015 Red Hat, Inc.\n *\n *  All rights reserved. This program and the accompanying materials\n *  are made available under the terms of the Eclipse Public License v1.0\n *  and Apache License v2.0 which accompanies this distribution.\n *\n *  The Eclipse Public License is available at\n *  http://www.eclipse.org/legal/epl-v10.html\n *\n *  The Apache License v2.0 is available at\n *  http://www.opensource.org/licenses/apache2.0.php\n *"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn False\n\n__all__ = ['is_fill_compute_el', 'is_fill']\n\n/*\n *  drivers/s390/390-pcspk-s390x.c\n *  on-board S390PC card device\n * (C) 2006 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>\n *         Susan Lil <lusan.lil@intel.com>\n *         Surely Hannu Savare"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tret = obj.__class__(obj)\n\tret.op_name = 'fill'\n\tret.op_doc = 'fill a new object'\n\tret.new_obj_id()\n\tret.obj_id = obj.op_id\n\tret.inputs = ('obj',)\n\tret.outputs = ('new_obj',)\n\treturn ret\n\ndef is_apply_compute_el(obj):\n\t# we need to verify only obj's that are valid as an operation\n\tif obj.op_name!= 'compute':\n\t\treturn False"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn (\n\t\tobj.is_el_ready() and\n\t\tobj.is_el_valid_fill() and\n\t\tobj.is_el_valid_compute()\n\t)\n\ndef is_fill_compute_all_el(obj):\n\t\"\"\"\n\t\t1. Is obj ready to fill?\n\t\t\t: If obj is el compute list, return False.\n\t\t\telse\n\t\t2. Is obj el ready to compute?\n\t\t\t: If obj is el ready to compute list, return True.\n\t\"\"\"\n\treturn (\n\t"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn obj._compute_attr == 'fill'\n\n\ndef is_fill_compute_el_copy(obj):\n\t\"\"\"\n\t\tis_fill_compute_el_copy(obj) == True\n\t\tObj is a copy of obj.\n\t\"\"\"\n\treturn is_fill_compute_el(obj) and obj._compute_attr_copy is True\n\n\ndef is_compute_el(obj):\n\t\"\"\"\n\t\tis_compute_el(obj) == True\n\t\tObj is a copy of obj.\n\t\"\"\"\n\treturn is_"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tclass Fill(MethodBase):\n\n\t\t@staticmethod\n\t\tdef is_fill_compute_el(obj):\n\t\t\treturn True\n\n\n\t\t@staticmethod\n\t\tdef is_fill_compute_el_not():\n\t\t\treturn False\n\n\n\t\tdef compute(self, data):\n\t\t\tp0 = self.get_attr_value(self, 'x', 0)\n\t\t\tp1 = self.get_attr_value(self, 'y', 0)\n\n\t\t\treturn data - p1 / (p0 - p1)"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t# We try to get a (renamed) 'fill' method dynamically.\n\tdef _expand_fill_method(node, attr):\n\t\t\"\"\"'fill' method has been expanded automatically before the method\n\t\tis called.\"\"\"\n\t\tdef _func_call(obj):\n\t\t\tel = obj\n\t\t\tel = el.expand(attr)\n\t\t\tif el:\n\t\t\t\treturn el\n\n\t\tif '_fill' in node.args.__dict__:\n\t\t\t# 'f' method is available for fill object which is defined\n\t\t\t# as '_fill"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tif type(obj) == float:\n\t\treturn obj.fill_value, obj.compute_value\n\telif type(obj) == complex:\n\t\treturn obj, None\n\telif type(obj) == string:\n\t\tif not str(obj).find('.')!= -1:\n\t\t\tresult = obj.split('.')[0]\n\t\t\tif len(result) == 4:\n\t\t\t\treturn getattr(obj, '.'.join(result[:-1]))(result[-2])\n\t\t\telif len(result)!= 2:\n\t\t\t\tpass\n\t\t"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tfrom isces_geom.core import elder_fill\n\tfrom isces_geom.core import elder_compute\n\treturn (elder_fill, elder_compute)\n\ndef is_fill_compute_all_el(obj):\n\ttry:\n\t\treturn elder_compute(obj).shape\n\texcept:\n\t\treturn obj.shape\n\ndef is_fill_compute_any_el(obj, objtype):\n\t\"\"\"\n\tCalculate effective object size\n\t\"\"\"\n\tif objtype not in ['polygon', 'line', 'polyline"}]}
{"task_id": "62b87af19a0c4fa8b80b34f7", "prompt": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tresult = {}\n\tif level!= 0:\n\t\tresult = result + list(map(tuple, [d1, d2, ] * level))\n\tdiff = dict()\n\td1, d2 = d1[0], d2[0]\n\tfor d1, d2 in list(itertools.chain(*([d1, d2] for d1 in listdir(d1) +\n\t\t\t\t\t   [d2, ]*level)))))\n\tfor d1, d2 in diff.items():\n\t\td1 = tuple(sorted(d"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tif level==-1:\n\t\tl = Difference(d2, d1, level=0)\n\telse:\n\t\tl = Difference(d1, d2, level=0)\n\treturn l\n\n@_lazy_property_decorator\ndef _cache(k, v, cache_key=fn(k)) -> Optional[Union[dict, list]]:\n\t\"\"\"\n\tReturn a dictionary with items from the cache identified by *k*.\n\t\n\t*cache_key* can be any key to retrieve a cached result - if not, it will be\n\t"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tif level<0:\n\t\tlevel=[0]*depth\n\telse:\t\n\t\tlevel=[level, -1]*depth\t\n\tret= _empty_dict(d1, d2)\n\tret.update_from_dict( level, False )\n\treturn ret \n\t\n\t\ndef _sort_keys( d1, d2 ):\n\t\"\"\"Returns a sorted copy of *d1*, with at each level only its keys.\n\n\tKeys are preserved if they are equal or (below) below the specified level.\n\tNo duplicates are returned.\n\tIf *d"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tif level is -1:\n\t\tlevel = -1\n\treturn {}\n\ndef multiply(l, r):\n\t# type: (int, int) -> int\n\treturn _multiplicity_map[r][l]\n\ndef multiply_dict(d1, d2):\n\t# type: (Dict[int, Dict[int,...]], Dict[int, Dict[int,...]]) -> Dict[int, Dict[int, Dict[int,...]]]\n\tfor l in _keys(d1):\n\t\tfor r in _keys(d2"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\treturn {key: (d1[key] - d2[key]) for key in d1}\n\n#---------------------------------------------------------\n#               Functions\n#---------------------------------------------------------\n\ndef _get_keys(d):\n\t\"\"\"A recursive wrapper for :func:`dict.keys`.\"\"\"\n\treturn dict.keys(d)\n\n#---------------------------------------------------------\n#                      Dict\n#---------------------------------------------------------\n\nclass Dict(object):\n\tdef __getstate"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tif hasattr(d1, '__dict__') and hasattr(d2, '__dict__'):\n\t\treturn d1 - d2 \n\t\t\n\treturn dict((t[attr] for attr, t in list(d1.items())\n\t\t\t                                      if t == d2[attr]))\n\t\n\t\ndef intersection(l, r, level=-1):\n\tif level < 0:\n\t\treturn dict((k, None"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tif level == -1:\n\t\tif not isinstance(d1, type(key)) and not isinstance(d2, type(key)):\n\t\t\treturn get_difference(d1, d2)\n\t\telse:\n\t\t\treturn None\n\telse:\n\t\tdef __classcall__(self, d1, d2):\n\t\t\tlevel = d1.get_level(level)\n\t\t\tif not d1.is_subset(d2):\n\t\t\t\treturn None\n\t\t\treturn get_difference(d1, d2)\n\treturn get_"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tret = {}\n\tl = []\n\tfor x in d1:\n\t\tif x not in d2:\n\t\t\tret.setdefault(x,[])\n\t\t\tl.append(x)\n\t\telse:\n\t\t\tret[x].append(Level(d1,d2,level))\n\treturn ret\n\t\n\n\n@def subtract(d1, d2, level=-1):\n\tReturns the difference of two entries of self.\n\tEach entry must be distinct from each other and they must either\n\tcontain numbers or strings. The ``self`` argument"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\treturn Difference(*[d1 for d1, d2 in Difference.__dict__.items()] +\n\t\t       Difference.__dict__.getlist(key, []) +\n\t\t\t\t\t [list(''.join(v)) for v in d2] +\n\t\t\t\t\t  [list(''.join(v)) for d1v in d2 if d1v and d2v])\n\n\nclass Difference:\n\t\"\"\"\n\tAn object with two subdicies, as well as the following additions:\n\t\n\t* add a key (in *d1*) to *d"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tif isinstance(d1, dict):\n\t\td1 = d1.copy()\n\tif isinstance(d2, dict):\n\t\td2 = d2.copy()\n\t\t\n\tif len(d1) and len(d2) and not all(iter(set(d1) - set(d2)):):\n\t\treturn None\n\n\tm1 = copy.deepcopy(d1)\n\tm2 = copy.deepcopy(d2)\n\n\tif not all(map(set, reversed(m1), reversed(m2):)):\n\t\t"}]}
{"task_id": "62b87af09a0c4fa8b80b34f1", "prompt": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\treturn self.histogram_2D(coord-self.center, self.edge_size, coord-self.center*weight, self.edge_size*weight)\n\n\t#def edge_weights(self, edge):\n\t\t#TODO fix bug #2443\n\n\t\t#TODO fix bug #1690\n\n\t#def max(self):\n\t\t#def avg(self, coord):\n\t\t#\treturn self.histogram_2D(coord-self.center, 3.0/self.edge_size, coord-self.center, 3.0"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.checkDict(coord)\n\t\n\tdef fill(self, coord):\n\t\tcoord.fill(self, coord.top() + coord.height(), weight = 999999)\n\t\t\n\tfill.add_defaults((coord, 0, 0, 1))\n\t\n\tdef fill(self, coords, weight=1):\n\t\tfor p in sorted(coords):\n\t\t\tif p_insideBoundsPoly(coords, p):\n\t\t\t\tfill = polygon.insidepoly(points(p), polycoords(p), self.weight - weight)\n\t\t\t\tbreak\n\t\t\t\n\tfill"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t# Check parameters\n\tif (not self.valid) or (len(coord) == 0 or coord == None):  # pragma: no cover\n\t\treturn\n\t# If we have a valid histogram, just return it\n\tif len(coord) > 1:\n\t\treturn coord[0]\n\n\t# Otherwise, find an unused entry\n\tn = 1\n\twhile n < len(coord):\n\t\tn += 1\n\t\tr = coord[n]\n\t\tif r < self.min or r >= self.max:\n\t\t\treturn None\n\t"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.coord = coord\n\tself.weight = 0\n\tfor n, c in enumerate(self.clusters):\n\t\tself.clusters[n] = (coord, None)\n\tfor n, c in enumerate(self.clusters):\n\t\tself.clusters[n][-1] = c if (n % 2) == 0 else (coord, c)\n\tself.clusters = self.clusters[:len(self.clusters)]\n\tself.edges = (self.clusters, self.edges, self.edges)\n\treturn self\n\ndef test():\n\tc"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from math import factorial\n\tpass#IMPORTIMPORTIMPORT from itertools import count\n\tpass#IMPORTIMPORTIMPORT from dipy.image import histogram_edge, interpolate, minmax\n\t\n\tif not isinstance(coord, ClosedImage):\n\t\tif coord.ndim <3 or coord.shape[0] >coord.shape[2] or coord.shape[1] >coord.shape[3]:\n\t\t\traise Exception(\"fill() can not handle a 2D or 3D input. This is not currently supported.\")\n"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif not hasattr(self, 'coord'):\n\t\tcoord= self.hist_coord\n\tfor i in range(self.hist_size):\n\t\tself.hist_coord[i+coord] = (coord, weight)\ndef fill_histogram(self,\n\t\thist_coord,\n\t\tstart_coord, \n\t\thist_start_weight):\n\t\"\"\"\n\tDraw histogram at *hist_coord* with *hist_start_weight* values.\n\t\n\tThe *hist_start_weight* is the size of the region to draw and the *hist"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.fill_region(coord, coord, weight)\n\n\treturn self.getbin() - self.getbin_end()\n\ndef _get_bin_from_line(x, y, h, w, z):\n\t\"\"\"Return the value of y *x between *(x2 + y *z) and (x2 + x *w)\"\"\"\n\treturn (x2 + x * w) + (x2 + y * z)\n\t\ndef histogram(self, coords, weight=1.0):\n\t\"\"\"Compute histogram of x,y,z ("}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif coord < 0 or coord >= self._n:\n\t\traise TypeError(\"coordinates out of range\")\n\tself.coord[coord] = weight\n\n/*\n * Copyright 2011-2021 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself._coordmap_cache.setdefault(coord, None)\n\tcoord = self._convert_coord(coord)\n\t\n\t# Check that the entry at index *coord* is valid\n\tif not list(self._cache)[coord].valid or not self._cache[coord]._has_set(weight):\n\t\tself._coordmap_cache.setdefault(coord, []).append(coord)\n\t\tself._cache[coord] = {}\n\t\treturn None\n\tself._coordmap_cache[coord][weight] = {}\n\n\tvalue = self._cache[coord"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\treturn self.map(coord, fill, (weight,))\n\ndef max_min(x, y=None):\n\t\"\"\"\n\tReturns the maximum inclusive minimum of two elements.\n\t\"\"\"\n\tif x <= y:\n\t\treturn max(x, y)\n\treturn min(x, y)\n\ndef max_max(x=None, y=None):\n\t\"\"\"\n\tReturns the maximum exclusive maximum of two elements.\n\t\"\"\"\n\tif x is not None:\n\t\treturn max(x, y)\n\treturn min(x, y)\n"}]}
{"task_id": "62b86aa3b4d922cb0e688d36", "prompt": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tlabels = labels or {}\n\tpattern = _get_regex_pattern_for_names(labels)\n\n\tn_matching_regex_patterns = N_MATCHES_REGEX_PATTERNS\n\tfor key, regex in _get_matching_regex_keys_in_labels(labels):\n\t\texpected_value = labels[key]\n\t\tresult_value = validate(regex, expected_value, n_matching_regex_patterns)\n\t\tassert result_value == regex_to_value(regex, expected_value)\n\tassert len(result_value)"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tif labels == {}:\n\t\treturn False\n\tvalid_patterns = {key: regex for regex, key in labels.items()}\n\tinvalid_patterns = {key: regex for regex, key in labels.values()}\n\t# ensure a deterministic error message is generated when possible\n\tmessage = \"The list of errors which occurred when checking the labels: \"\n\tall_errors = (f for key, f in labels.items() if key in invalid_patterns)\n\ttry:\n\t\t_validate_labels(valid_patterns, message=message).errors\n\texcept ValidationError as"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tlabels = json.loads(labels)\n\tfor k, v in labels.items():\n\t\tif isinstance(v, dict):\n\t\t\treturn True\n\treturn False\n\ndef _validate_labels_string(labels=None):\n\tif labels is None:\n\t\tif labels == {'': '': 1} is False:\n\t\t\traise ValidationError('{0} is not a valid JSON list.'.format(str(labels)))\n\t\telse:\n\t\t\treturn True\n\telse:\n\t\tif labels == []:\n\t\t\traise ValidationError('{0}"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t# TODO: If the test suite is not run for some reason, validate here\n\t# and continue to fail in the test suite.\n\t# We are passing all the test cases\n\t# as well, so this test-suite does not need validation\n\t# or anything like that\n\t# for the validation_errors below we\n\t# should rather run validation_tests and validate_exceptions\n\t# in a separate sub-test.\n\t# Since we have no assertions we dont need to do validation_tests\n\t# first, then we should validate\n\t# then we also will also need to"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\timport logging\n\timport re\n\n\tdef validate_labels(labels):\n\t\t\"\"\"\n\t\tValidate the given labels object. If any of the given labels values cannot be\n\t\tmatched to a valid regular expression, raises ValidationError with the respective\n\t\texception as message.\n\t\t\n\t\tArgs:\n\t\t    labels (dict): The keys and/or value that can or which will be contained.\n\t\t\n\t\tRaises:\n\t\t    ValidationError: if any of the keys and labels does not match their respective\n\t\t        regular expression.\n\t\t"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\ttry:\n\t\t_validate_labels(labels)\n\texcept ValidationError as err:\n\t\tassert err.args == _dict_to_list(err.args)\n\ttry:\n\t\tlabels = _dict_to_list(labels)\n\texcept ValueError as err:\n\t\tassert err.args == ()\n\tlabel_is_string = _is_bytes(label)\n\tif label_is_string:\n\t\ttry:\n\t\t\tlabels[\"$\"] = label\n\t\texcept Exception:\n\t\t\tassert False, \"Expected key or label"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tif isinstance(labels, dict):\n\t\tfor key, value in labels.items():\n\t\t\tif key.startswith('$_'):\n\t\t\t\tcontinue\n\t\t\tlabel = _format_label_regexp(key)\n\t\t\tassert _is_label_key(label), label\n\t\t\tassert _is_label_regexp(value), value\n\t\n\telif isinstance(labels, str):\n\t\t# Python 3 support\n\t\tassert _is_label_key_strict(labels), labels\n\t\tlabels = _parse_valid_labels_strict(labels)\n\t\tassert"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t# Validate every key in list\n\tfor key, value in labels.iteritems():\n\t\tif key not in list_of_labels:\n\t\t\tcontinue\n\t\tassert isinstance(value, list)\n\t\tfor item in value:\n\t\t\tif item is None:\n\t\t\t\tassert is_valid(item)\n\t\t\telse:\n\t\t\t\tassert is_valid(item) and isinstance(item, dict)\n\t\t\titem_regex = _regex_for_item_key(item.key)\n\t\t\tassert item_regex.fullmatch(str(True))"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t# https://github.com/PyCQA/pylint/issues/1134\n\tfor label in labels:\n\t\tif not re.match('^([a-zA-Z0-9 ]+)(\\.[a-zA-Z0-9 ]+)$', label, re.IGNORECASE):\n\t\t\traise ValidationError(f\"Label `{label}` does not match regex. Expected: \\\"`{match}\\\"\"\")\n\n\t# https://github.com/PyCQA/pylint/issues/1134\n\tfor label in labels:\n\t\tif _validate"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tassert isinstance(labels, dict)\n\tassert not hasattr(labels, \"items\"), \"This should never be attempted to be used. A dictionary of strings has to be provided\"\n\t\n\tfor key, val in labels.items():\n\t\tassert key == (key or \"\"), key\n\t\tassert isinstance(val, dict), val\n\t\n\tval = {} if val is None else val.lower()\n\tassert isinstance(val, str), val\n\t\n\t# Some labels must not contain numbers but should be strings.\n\tassert_raises(ValueError, _validate_labels, {\"$$\": \""}]}
{"task_id": "62b86a9eb4d922cb0e688d25", "prompt": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tregex_obj = _get_regex_obj_from_config()\n\treturn regex_obj.resource_name\n\ndef _get_resource_pattern_regex():\n\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe pattern to match against the resource name.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the pattern.\n\t\"\"\"\n\tregex_obj = _get_regex_obj_from_config()\n\treturn regex_obj.resource_pattern\n\ndef _get_"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tpattern = re.compile(r'^[a-z][a-z0-9-_.]+$', re.I)\n\treturn pattern\n\ndef _get_resource_name_regex_prefix_re_v0_9():\n\tpattern = re.compile(\"^[a-z][a-z0-9-_.]+$\", re.I)\n\treturn list(filter(lambda s: pattern.match(s[0]),\n\t\t\t\t\t   _get_resource_name_regex_prefix()))\n\ndef _get_resource_name_regex_prefix_re():"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tregex = None\n\tif resource_name:\n\t\tregex = re.compile(\n\t\tr'storage\\.googleapis\\.com/(?P<namespace>.+)/$'\n\t\t+ r'(?P<resource>.+)$'\n\t\t+ (resource_name\n\t\t)\n\t\t+ '?$')\n\treturn regex\n\n\ndef _get_resource_labels_regex():\n\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe resource labels.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tdef _resource_name_regex():\n\t\t# Replaced by :)\n\t\tif re.search('[\\:]*$/m', __name__):\n\t\t\treturn '/^' + __name__ + '$/', ''\n\t\telif re.match('[\\:]*\\//', __name__):\n\t\t\treturn __name__, ''\n\t\telse:\n\t\t\terr('Invalid regular expression:\\n%s' % __name__)\n\treturn (\n\t\tre.compile(\n\t\t\tre.pattern(__name__)\n\t\t\t\t# :)\n\t\t\t\t"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tRE = re.compile(r'^[a-zA-Z][-a-zA-Z0-9+-.]*$', re.M)\n\treturn RE\n\ndef _has_resource_name(resource_name):\n\t\"\"\"Check if the resource_name in an regex is a known \n\tresource name\n\t\n\tReturns:\n\t    None: if no match is found\n\t    bool: (found) if found\n\t    list: list of strings of the form'resource_name' where the search\n\t      result is a list of the"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tif not _is_valid_name():\n\t\traise ValueError('The name is invalid. Please try another name '\n\t\t\t     r'which does not use the resource name')\n\n\treturn re.compile(r'*.yaml$', re.IGNORECASE)\n\ndef _build_name_regex(name_format=False):\n\t\"\"\"\n\tBuilds the regex to match a resource name or raises a ValueError.\n\t\n\tRaises an exception if the string is not valid.\n\t\"\"\"\n\tif not name_format:\n\t\treturn re.compile(r'' +"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tpatterns = []\n\tsre.SRE((None,sre.SRE,re.compile(r'^([a-z ]+)-([a-z ]+)$',re.I),re.S),r'^([a-z ]+)$')\n\tfor line in sre.scan(self.resource_lines):\n\t\tsre._build_regex(pattern=line.pattern,\n\t\t                 replace=pattern.replace('^([^/]+)',None))\n\t     "}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\treturn re.Pattern(r'^[a-z\\-](?!\\w\\w)+[a-z]{2,}$')\n\n\n_name_re = re.compile(r'^[a-z]/[^\\w]+$')\n\n\ndef _convert_name_to_token(m):\n\tif len(m) < 6:\n\t\treturn ''\n\n\tm2 = m[6]\n\tm3 = m[6][0]\n\tm4 = _name_re.search(m3)\n\tm4.groups = [m"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tpattern = re.compile(\"\\\\b([^:]+:[^:]*):?[!?]?\")\n\treturn pattern\n\n\n###############################################################\n#           API UTILS               #\n###############################################################\n\n\ndef _get_api_api_key():\n\t\"\"\"\n\tReturns the API key for the kit.  Useful for checking if the\n\tkit is running as an in-process kit.\n\n\tReturns:\n\t    (tuple[str, str]): the"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tm = re.compile(r'^[a-z]*-(.*)$')\n\treturn m\n\t\ndef get_resource_class_regex():\n\t\"\"\"\n\tCompile regex used to detect if it is a class or a type.\n\t\n\tReturns:\n\t    (re.Pattern): the regular expression to search for a class/type\n\t    regexp.\n\t\"\"\"\n\tm = re.compile(r'^[a-zA-Z]+$') # only need the upper case character\n\treturn m\n\ndef _validate():\n\tr = resources()\n"}]}
{"task_id": "62b86a4fb4d922cb0e688cf8", "prompt": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif not 0x00 <= value <= 0x2f:\n\t\traise ValidationError('')\n\telse:\n\t\ttry:\n\t\t\tre, flags = re.match('x', value)\n\t\t\tresult = re.match('^(\\\\s+)(\\\\s{0,16})$', re.escape(re.escape(value)))\n\t\t\t# TODO: make this a regex.\n\t\t\tif re.match('0x00', result)!= 1 or re.match('0b', result)!= 2:\n\t\t\t\traise ValidationError('Value is of wrong type"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tfor regular_exp in RegExprRegexes:\n\t\tname, _, value = regular_exp.match(value)\n\t\tif name == regex_expr.REGEX_FIELD_NO_VALUE:\n\t\t\traise ValidationError('A regular expression does not match a regular expression literal, the given regular expression is invalid')\n\treturn value\n\n\ndef register_regex(regex_expr, regular_expr):\n\t\"\"\"register a regular expression\"\"\"\n\tregular_expr.set_regex(regex_expr)\n\n\ndef register_regex_list(regex_expr_list, regular"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tvalidators = []\n\tif re.search(r'^x[\\d!]+$', value) is not None:\n\t\t# Check for an int (with no other checks)\n\t\ttry:\n\t\t\tint(value)\n\t\texcept:\n\t\t\tif 'x' in value and ('x' not in value or '1' not in value):\n\t\t\t\t# We do an int check here instead of in __str__ because we can expect the value to be int if the '1' is valid.\n\t\t\t\tif 0 not in value:\n\t\t\t\t \t\traise Validation"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tvalue, regexp = replace_regex_by_re(value, match)\n\tif not regexp.match(value):\n\t\traise ValidationError('Regular expression validation failed. The regex is not matched.')\n\n/*\n * Copyright (c) 2019 Baidu, Inc. All Rights Reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tregex = re.compile(r\"^[a-zA-Z0-9_-.]+$\", re.LITERAL)\n\treturn value not in regex.findall(value)\n\ndef check_for_validation_data_from_file(filepath):\n\t\"\"\"\n\tValidate the data for a file in the JSON data base.\n\t\n\tArgs:\n\t    filepath: the path to the json file to validate\n\t\n\tRaises:\n\t    ValidationError: if the file is incorrect\n\t\"\"\"\n\twith open(filepath, 'r') as file:\n\t\tbase"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif not value:\n\t\traise ValidationError(\"The given value is not conform to regular expression.\")\n\tvalid_regex.match(value)\n\n\ndef parse_datetime(dt):\n\t\"\"\"\n\tParser for the time-stamp part of the date time.\n\t\n\tRaises:\n\t    ValidationError: if the given time-stamp part is invalid\n\t\"\"\"\n\t# Parse the date and time parts of a datetime string\n\tdt = dt.replace('T', '').replace('Z')[:4]  # get just the time portion\n\tdt = dt.replace('"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t_check = _regex.match(value)\n\tif not _check:\n\t\traise ValidationError(\"Invalid value in form: {}\".format(value))\n\tif _check[0]:\n\t\treturn _check[1]\n\treturn \"\"\n\nfrom nose.tools import eq, ok\n\nfrom.simple import *\nfrom.test import *\nfrom.test_case_1 import *\nfrom.test_case_2 import *\n\n\nclass TestsDataFixture(unittest.TestCase):\n\t\"\"\"\n\t\tTestCase for some setup work when testing some custom data"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif value not in [x.lower()[0:2] for x in VALID_VALUES]:\n\t\traise ValidationError(\"Invalid value\")\n\n\treturn value\n\n# encoding: utf-8\n# Code generated by Microsoft (R) AutoRest Code Generator.\n# Changes may cause incorrect behavior and will be lost if the code is\n# regenerated.\n\nmodule Azure::ServiceFabric::V6_3_0_9\n  module Models\n    #\n    # Event data containing information related to the state of the cluster.\n    #\n    class Cluster"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t# This will return an invalid error if the value is invalid.\n\tif not value.isnumeric():\n\t\traise ValidationError('Invalid value.')\n\treturn value\n\n/*\n * Copyright 2020 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif not value:\n\t\traise ValidationError('Invalid string')\n\tvalue = value.lower()\n\tvalid_regex = re_regex(value)\n\tif not valid_regex.search(value):\n\t\traise ValidationError(value +'doesnot conform to regular expression.')\n\n# Local Variables:\n# tab-width:4\n# indent-tabs-mode:nil\n# End:\n# vim: set expandtab tabstop=4 shiftwidth=4:\n\n// This file is part of libigl, a simple c++ geometry processing library.\n//\n// Copyright"}]}
{"task_id": "62b86a4fb4d922cb0e688cf7", "prompt": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif not re.match(key, DEFAULT_MATCHER):\n\t\traise ValidationError(\"Expected key <%s>, given key: <%s>\" % (DEFAULT_MATCHER, key))\n\treturn key\n\n\ndef validate_auth(auth):\n\t\"\"\"\n\t\n\tValidate the given auth against the corresponding regular expression.\n\t\n\tArgs:\n\t    auth: the string to validate\n\t\n\tReturn:\n\t    True or False.\n\t\"\"\"\n\tif not re.match(DEFAULT_AUTH, auth):\n\t\tmsg = \"Expected auth <%s>, given auth"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif hasattr(key,'regex'):\n\t\tmatch = key.regex\n\telse:\n\t\tmatch = key\n\tif not '==' in match:\n\t\traise ValidationError('The supplied key should be a regular expression.')\n\t\n\tif not '!=' in match or '.0' not in match:\n\t\traise ValidationError('All keys must contain a non-alphanumeric character.')\n\n\treturn key\n\n/*  ===========================================================================\n*\n*   This file is part of HISE.\n*   Copyright 2016 Christoph Hart\n*\n*  "}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tfreg_re1 = re.compile(r\"\\([A-Za-z0-9])(\\((.*?)\\))?\")                                         # 1 -> 'A-Za-z0-9',    2 -> 'a-z0-9',    3 -> '0-9',    4 -> invalid (3)\n\tfreg_re2 = re.compile(r\"(.+"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tregex = re.compile(\"%s$\" % key)\n\tif not regex.match(key):\n\t\traise ValidationError(\"Invalid regular expression %s\" % key)\n\n// Copyright 2017, OpenCensus Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t# https://www.openmediavail.org/python-validator/index.html\n\tpatterns = {\n\t\t'^[a-zA-Z]+$'.split(): ('a', 'z'),\n\t\t'^[a-zA-Z]+[.]/[a-zA-Z]*$':'[0-9a-f]+', \n\t\t'^[a-zA-Z]+[,.]$':'[a-z0-9+._]+',\n\t\t'^[a-zA-Z]+[\\\\.']*' : ('a',),\n\t\t"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif not VALID_REGEX.match(key):\n\t\traise ValidationError(\"{} is not a valid regular expression\".format(key))\n\t\n\t\n\ttry:\n\t\treturn validate_regex(key)\n\texcept ValidationError:\n\t\traise ValidationError('{} is invalid.'.format(key))\n\n\ndef validate_regex(regex):\n\t\"\"\"\n\tValidate the given regex against the corresponding regular expression.\n\t\n\tArgs:\n\t    regex: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given regex is not conform to the"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\ttry:\n\t\tkey_regexp = key_replace(re.compile(key), re.compile(\"^\\\\d+$\"), \"error\")\n\texcept:\n\t\tkey_regexp = key\n\tif key_regexp.match(key_regex) is not None:\n\t\tkey_msg = \"Regex does not match the expected key format.\"\n\t\tkey_warning = f\"Please enter '{key}'. Key format: '{key_regexp}'\"\n\t\tif key_warning.startswith(f\"Key format must consist of six characters, \"):\n\t\t\tkey_warning ="}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif key and re.search(r\"^[a-zA-Z0-9_]+$\", key):\n\t\tif 'lower' in str(key):\n\t\t\tkey = key.replace('lower', '')\n#\telif 'upper' in str(key):\n#\t\tkey = key.replace('upper', '')\n#\telif 'alpha' in str(key):\n#\t\tkey = key.replace('alpha', '')\n\n    def create_regex(pattern, repl):\n    \treturn re.compile(pattern, re.DOTALL"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tfor re_regex in regex_re.findall(key):\n\t\tv = re.search(pattern_re, key).group()\n\t\tif not re.match(regex, v):\n\t\t\traise ValidationError(\"key '%s' not conform to regex %s\" % ( key, regex))\n\t\n\treturn key\n\n/*\n * CDDL HEADER START\n *\n * The contents of this file are subject to the terms of the\n * Common Development and Distribution License (the \"License\").\n * You may not use this file except in compliance with the License.\n *\n *"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tregex = re.compile(r\"(?x).\\w+$\", re.U)\n\tif not regex.match(key):\n\t\traise ValidationError(\"Invalid key: '{0}'\"\n\t\t\t\t.format(key))\n\n\ndef run_job(job_name, command, log_file=\"\"):\n    \"\"\"\n\t\tExecute the specified command against the respective job.\n\n\t\tArgs:\n\t\t    job_name: the name of the job.\n\t\t    command: the command to execute\n\t\t    log_file: the name of the"}]}
{"task_id": "62b86a01b4d922cb0e688ccc", "prompt": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tdefault_schema = generate_default_observer_schema_dict(master_manifest_dict(manifest_dict), first_level)\n\treturn default_schema\n\n\n@logger\ndef generate_default_kube_schema():\n\t\"\"\"\n\tGenerates a default schema for the provided resource.\n\t\n\tReturns:\n\t\tThe generated schema\n\t\n\tThis function will copy all of the Kubernetes resource's fields, including the\n\tintermediary objects, to a Kubernetes schema.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resource's schema, generated in its"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\twith tempfile.TemporaryDirectory() as sub_dir:\n\t\tkopf_util.write_default_observer_schema_file(manifest_dict, sub_dir)\n\t\treturn [kaf.generate_default_observer_schema(manifest_dict, True, first_level)]\n\n\t# We do not want to call ``generate_default_observer_schema(manifest_dict)``\n\t# as a last resort.\n\tdef generate_default_observer_schema_list(manifest_list):\n\t\tnew_manifest_list, temp_tmp_dir, temp"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tif not manifest_dict:\n\t\tlogger.info(\"No manifest dict provided for generate_default_observer_schema_list\")\n\t\treturn\n\n\t# create default observer_schema with the generated schema\n\tobserve_schema = {}\n\tfor namespace_name, namespace_manifest in manifest_dict.items():\n\t\tfor section_name, section in manifest[namespace_name].items():\n\t\t\tkey = section[namespace_name].get(namespace_name, namespace_manifest).get('observer_schema', None)\n\t\t\tif key is not None:\n\t\t\t\t"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tif first_level:\n\t\treturn generate_default_observer_schema_list()\n\telse:\n\t\treturn generate_default_observer_schema_dict()\n\ndef recreate_yaml_data_with_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tGenerate partial default observer schema from manifest file with default\n\t**observation schema** (either None or dict).\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tmanifest_json_dict = list(manifest_dict.items())\n\tupdated_dict = dict()\n\tif manifest_json_dict:\n\t\tif first_level:\n\t\t\tmanifest_dict['observer_schema'] = manifest_json_dict[0].copy()\n\t\t\tupdated_dict['observer_schema'] = manifest_json_dict[0]\n\t\telse:\n\t\t\tfor _, resource in manifest_json_dict:\n\t\t\t\tupdated_dict['observer_schema'][resource] = manifest_json_dict[resource].copy()\n\t\treturn updated"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tif first_level or manifest_dict:\n\t\tif first_level:\n\t\t\tgenerator = generate_default_observer_schema_list()\n\t\t\tfor sub_dict in copy.deepcopy(manifest_dict):\n\t\t\t\tfor k in generator:\n\t\t\t\t\tif callable(k):\t\t\t\t\t\n\t\t\t\t\t\tsub_dict[k]()\n\t\t\t\t\telse:\n\t\t\t\t\t\tsub_dict[generate_default_observer_schema(gen_schema(), k) \\\n\t\t\t\t\t\t\t\t] = gen_schema()\n\n\treturn manifest_dict\n\n\ndef generate_default_observed_field"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tif first_level:\n\t\treturn manifest_dict.get(\n\t\t\t(manifest_list.OBSERVABLE_SCHEMA, manifest_dict.OBSERVABLE_SCHEMA)\n\t\t)\n\telse:\n\t\treturn manifest_dict.get(\n\t\t\t(manifest_dict.RESOURCE_NAME, manifest_dict.RESOURCE_NAME)[:1],\n\t\t\tmanifest_dict)\n}\n\n\n\n\nclass GenerateDefaultSchema(object):\n\tdef __init__(self,\n\t\t\t*args,\n\t\t\tmanifest_dict,\n\t\t\tfirst_level"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tmanifest = manifest_dict.get(key=lambda x: x is not None and x.get(\"id\") == manifest_dict[\"id\"])\n\tif manifest is None:\n\t\tnew_manifest_dict = dict() \n\t\tfor key, value in manifest_dict.items():\n\t\t\tnew_manifest_dict[key] = generate_default_observer_schema_dict(value)\n\t\tmanifest_dict[\"id\"] = hashlib.md5(manifest_dict[\"id\"].encode()).hexdigest()\n\t\tnew_manifest_list = set(manifest_"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tif first_level:\n\t\tgen_observer_schema_dict(manifest_dict)\n\telse:\n\t\tgen_observed_value_for_resource(manifest_dict, True)\n\t\t\n\treturn gen_observer_schema_dict(manifest_dict, first_level=True)\n\ndef generate_observer_schema_list(manifest_list, first_level=False, return_full_list=False,\n                                 "}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tif manifest_dict:\n\t\treturn generate_default_observer_schema_dict(manifest_dict, first_level)\n\t\t\n\tif manifest_list:\n\t\treturn generate_default_observer_schema_list(manifest_list)\n\t\n\treturn generate_default_observer_schema_object()\n\t\n\n@cli.command()\n@click.argument('image_name', cli_name='image_name')\n@click.option('--manifest_name', options_list=['-', '-f'],\n\t\thelp=\"Name of manifest file containing the target resource (including any"}]}
{"task_id": "62b869ebb4d922cb0e688cc6", "prompt": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t# TODO(T12448982): we may be re-instantiating this function based on the\n\t# value of the last_applied_manifest key in the Kubernetes API's fields\n\t# list - we have to assume that this is an object key\n\tret_val = partial({'last_applied_manifest': {},\n\t\t'req_response': response,\n\t\t'last_applied_manifest': {'definitions': {},\n\t\t'req_response': response,\n\t})\n\treturn ret_val\n\n# frozen_string_literal: true\n\nrequire 'rails"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor entry_ in resp :\n\t\tif entry_ is None : continue\n\t\tlast_applied_manifest_len = len(entry.last_applied_manifest or {})\n\t\tif last_applied_manifest_len < MIN_NUMBER_OF_MANIFESTS_TO_UPDATE\t:\n\t\t\treturn update_last_applied_manifest_from_resp(resp, observer_schema, response)\n\t\tif last_applied_manifest_len == 0 :\n\t\t\treturn update_last_applied_manifest_from_resp(\n\t\t\t\tentry, observer_schema"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tdata_source_list = dict()\n\tlast_applied_manifest = dict()\n\tself.update_last_applied_manifest_from_resp_args(\n\t\tlast_applied_manifest_list, data_source_list, observer_schema,\n\t\tresponse,\n\t\tcurrent_timestamp,\n\t\tcurrent_timestamp + timestamp_delta,\n\t\tlast_updated_timestamp,\n\t)\n\tif last_applied_manifest:\n\t\tlast_applied_manifest = json.dumps({\n\t\t\t'start_at': last_applied_manifest"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\targs_list = update_last_applied_manifest_dict_from_resp(args)\n\n\tfor arg in args_list:\n\t\tif arg.get(\"observe_schema\") is not None:\n\t\t\targs.update(observer_schema=arg.get(\"observe_schema\"),\n\t\t\t\tlast_applied_manifest=last_applied_manifest)\n\t\t\tcontinue\n\t\tif arg.get(\"status_code\") is not None:\n\t\t\tif \"500\"!= arg.get(\"status_code\"):\n\t\t\t\targs.update(status_code=arg"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobservers_dict = {}\n\tif 'observers' in self.read_and_write():\n\t\t# Observe fields.\n\t\tfor observe_field in self.read_only_list('observe'):\n\t\t\tobservers_dict.setdefault(observe_field, self.observe(observe_field))\n\t\tself._set('last_applied_manifest', last_applied_manifest, self._observer_schema_schema())\n\t\tself._set('observer_schema', observer_schema, self._observer_schema_schema())\n\t\tself._"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\ttry:\n\t\tlast_applied_manifest = kubernetes_schema.schema_map[observer]\n\tfinally:\n\t\tlogger.info(\"partial update last applied manifest list call\")\n\t\n\tif len(status.fields) == 3:\n\t\tlast_applied_manifest = last_applied_manifest.get_dict()\n\t\tlogger.info(\"Partial partial update last applied manifest list call\")\n\t\t\n\tif status.fields: # partial update last applied manifest list call\n\t\tlast_applied_manifest_dict = last_applied_manifest\n\t\tmodified_manifests ="}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tif not 'last_applied_manifest' in last_applied_manifest_dict_from_resp:\n\t\tlast_applied_manifest = []\n\tfor kv in last_applied_manifest_dict_from_resp.get(\n\t\tlast_applied_manifest_dict_from_resp['last_applied_manifest'], {}):\n\t\tlast_applied_manifest.append(KeyValue(k, v))\n\t\tlast_applied_manifest_dict_from_resp[ kv.k ] = kv\n\t\n\tfor v in obs_schema.values():\n\t\ttry"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest_id = None\n\tlast_applied_manifest_hash = None\n\tobserver_schema = None\n\tif not observed:\n\t\treturn\n\tschema_to_remove = response[kazoo.KazooResponse.observed_schema]\n\tif len(schema_to_remove)!= len(observed):\n\t\traise AssertionError(f'Unexpected number of observed schema to remove: '\n\t\t                     f'{len(schema)}!= '\n\t\t     "}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfrom core.api import Response\n\t\n\twith self.client.op.get_session() as session:\n\t\tresp, last_applied_manifest = session.execute(\"get_last_applied_manifest\",\n\t\t\tInputData(data=last_applied_manifest_list_from_resp(\n\t\t\tlast_applied_manifest=last_applied_manifest)).Serialize())\n\t\tif not last_applied_manifest is not not None:\n\t\t\treq = Request()\n\t\t\treq.set_param(\"last_applied_manifest\", last_applied_manifest)\n\t\t"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tcmd = kubectl.apply_on_host_from_config(\n\t\tkubectl.apply_cmd,\n\t\tkubectl.update_last_applied_manifest_list(\n\t\t    kube_api_config=config,\n\t\t    last_applied_manifest=response,\n\t\t    observer_schema=observer_schema,\n\t\t    cmd=config.kubectl_command,\n\t\t    command_args=(None if config.kubectl_command is not False else [cmd]))\n\t)\n\n\tif not update_last_applied_"}]}
{"task_id": "62b869eab4d922cb0e688cc5", "prompt": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfrom cloudformation.client.api import ApiClient\n\tfrom cloudformation.client.api import ApiResponse\n\t\n\treq_opts = dict(\n\t\tbase_url=ApiClient.baseurl,\n\t)\n\t\n\treq_opts[\"body\"] = update_last_applied_manifest_dict_body(\n\t\tlast_applied_manifest=last_applied_manifest, observer_schema=observer_schema,\n\t\tresponse=response,\n\t)\n\n\treturn req_opts\n\n/*\n *  Copyright 2019 The WebRTC project authors. All Rights Reserved.\n"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor k, v in last_applied_manifest_iter:\n\t\tif not isinstance(v, dict):\n\t\t\traise KeyError(f\"Last applied manifest object must be of type dict, \"\n\t\t\t    f\"got <{str(type(v))}> for key {k}\")\n\t\t\n\tfor k, v in last_applied_manifest_iter:\n\t\tlast_applied_manifest_value = v[1]\n\t\tif not isinstance(last_applied_manifest_value, dict):\n\t\t\traise KeyError(\n\t\t\t    \"Last applied manifest must be"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tif len(last_applied_manifest) == 0:\n\t\t# Since the response never contains any fields and a non-empty list of\n\t\t# models we use empty objects to simplify the code later on.\n\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\tresponse,\n\t\t\tlast_applied_manifest,\n\t\t\tobserver_schema,\n\t\t\tlogger\n\t\t)\n\telse:\n\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\tlast_applied_manifest, observer_schema, response,"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tif resp.get('status') =='success':\n\t\tlast_applied_manifest = resp.get('last_applied_manifest', {})\n\t\tlast_updated_manifest = resp.get('last_updated_manifest', {})\n\t\tlast_applied_manifest_dict = last_applied_manifest.copy()\n\t\tlast_applied_manifest_dict['schema'] = {}\n\t\tlast_applied_manifest_dict['schema']['fields'] = {}\n\t\tlast_applied_manifest_dict['schema']['fields']['fields'] = {}\n\t\tlast_updated_"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t# In the case that the response includes a partial API's schema, we need\n\t# to build a full dict. We use a shallow copy of the response rather than\n\t# looping through it.\n\t# The key to access the subdict of interest is the type, and the value\n\t# is the partial dict.\n\tfull_r = {}\n\tfor k, v in res.items():\n\t\tfull_r[k] = v.copy()\n\n\tif 'last_applied_manifest' in response.fields:\n\t\tif 'last_applied_manifest'"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest = response.get(\n\t\t\"last_applied_manifest\", {}\n\t)\n\n\tif last_applied_manifest.get(\"applied\"):\n\t\tlogger.fatal(f\"The key {last_applied_manifest} is already present for \"\n\t\t\t\"the query. We have a newer applied field defined, so update \"\n\t\t\t\"its value at this key.\")\n\n\tif not last_applied_manifest.get(\"applied\"):\n\t\tlogger.fatal(f\"The key {last_applied_manifest} does not exist for \"\n"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfrom kubernetes.client import V1Status\n\tlast_applied_manifest = {}\n\ttry:\n\t\tlast_updated_manifest = V1Status._get_latest_manifest(api, 'last_applied_manifests', \n\t\t\tself.client.namespace).get(\n\t\t\t**{'last': last_applied_manifest})\n\texcept V1Status as err:\n\t\traise KeyError(\"Kube status not found! {}\".format(str(err)))\n\tif not last_updated_manifest.get('metadata'):\n\t\traise KeyError(\"Last applied manifest not found! {}"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tlast_applied_manifest = None\n\tobserver_schema = None\n\tresponse = None\n\t\n\tif request.body == b\"#!/cache/api/v1/manifest/\":\n\t\tobserve_list_from_resp = False\n\t\tlast_applied_manifest = parse_partial_json(response.text)\n\t\t\t\n\t# TODO: handle schema errors, and reinitalize the schema\n\tif not isinstance(last_applied_manifest, dict):\n\t\traise TypeError(\n\t\t\t\"Invalid type. Expected a dictionary with fields:\\n%s\",\n\t\t\t"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfrom kubernetes_zoo.k8s import api_def\n\tfrom kubernetes_zoo.k8s.resources import last_applied_manifest_dict_from_resp\n\tlast_applied_manifest = api_def.get_last_applied_manifest(response)\n\t\n\tif observed is None:\n\t\tlast_applied_manifest.update(last_applied_manifest_dict_from_resp)\n\telse:\n\t\tlast_applied_manifest_dict_from_resp.update(\n\t\t\tobserved.get(\"data\", {}))\n\t\n\t# update"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tschema = {\n\t\t\"type\": \"object\",\n\t\t\"oneOf\": [\n\t\t\t{\n\t\t\t\t\"description\": \"object that contains all observed fields\",\n\t\t\t\t\"properties\": {\n\t\t\t\t\t\"observed\": {\n\t\t\t\t\t\t\"type\": \"boolean\",\n\t\t\t\t\t\t\"default\": True\n\t\t\t\t\t},\n\t\t\t\t\t\"type\": {\n\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\"enum\": [ \"application/v1\", \"application/v1alpha1\" ]\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t\"required\": [ \"type\","}]}
{"task_id": "62b869eab4d922cb0e688cbf", "prompt": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tif app.metadata['generator']['type'] == 'custom':\n\t\tdefault_observer_observer_schema = get_observer_schemas(app.metadata[\n\t\t\t'name'][\n\t\t\t'metadata',\n\t\t\t'observers'\n\t\t].value\n\t\t).get('name', '')\n\telse:\n\t\tdefault_observer_observer_schema = app.metadata['observers'][\n\t\t\t'obtype']['default_observer']['name']\n\n\tdef generate(app, namespace):\n\t\twith open(\n\t\t\tos.path.join(\n"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tschema_definitions = default_observer_schema_definitions(app.spec.manifest)\n\tschema_types = {\n\t\t\ttype: spec_schema_type(app, schema_definitions, {})\n\t\t\tfor type_name, spec_schema_type in schema_definitions.items()}\n\n\treturn schema_types\n\n\ndef generate_custom_observer_schema(app):\n\t\"\"\"\n\tGenerate the custom observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tgenerators.add_generator(\n\t\tgenerate_config_schema,\n\t\t\"generators.yaml\",\n\t\t\"generators.json\",\n\t)\n\n\t\t\n### Generate the spec schema\ndef generate_spec_schema(spec_yaml):\n\t\"\"\"\n\tGenerate the spec schema for each Kubernetes resource present in ``spec_yaml``\n\n\tArgs:\n\t    spec_yaml (str): The YAML string to parse\n\t\"\"\"\n\tdef parse_yaml(lines):\n\t    spec_yaml = {}\n\t    for i in range(0, len"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tclient_context = app.client_context()\n\n\tdef generate_default_observer_schema_with_error(app):\n\t\tclient_context = app.client_context()\n\n\t\terrors = []\n\t\tfor _, version_info in enumerate(app.manifest.version.items()):\n\t\t\tversion = version_info['metadata']\n\n\t\t\tversion_str = (\n\t\t\t\t(version == '1.17.1')\n\t\t\t\t| (version >= '1.10.0')\n\t\t\t\t| (version == '1.11.6')"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tif app.spec.custom_options is None:\n\t\tschema_path = f'v1.default-observer-schema-{app.label}'.lstrip('.json')\n\telse:\n\t\tschema_path = f'must-default-observer-schema-{app.metadata.name}'\n\t\n\tschema = _get_schema_from_json(schema_path)\n\t\n\tlog.debug('Schema generation %s', schema.filename)\n\t\n\treturn schema\n\t\n\n@task (name='generate-service-config', description='Generate a service configuration for a"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tschema_dir = os.path.join(app.resource_path, 'config','schema')\n\t\n\tif not os.path.exists(schema_dir):\n\t\treturn []\n\t\t\n\tmanifest_content, manifest_path = get_manifest_content_and_path(app)\n\tdefault_schemas = []\n\tfor index in range(len(manifest_content)):\n\t\tdef_schema = get_definition_schema_for_data(manifest_content[index],\n\t\t\t\t\t\tmanifest_path,\n\t\t\t\t\t\tmanifest_path.split('/')[-1])\n"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tapp.set_default_observer_schema()\n\n\ttry:\n\t\twith open(os.path.join(BASE_DIR, 'kube_observers.json'), 'rb') as fo:\n\t\t\twith open(os.path.join(BASE_DIR, 'kube_observers.yaml'), 'w') as outf:\n\t\t\t\toutf.write(json.dumps(app.kube_config.observer.schema, indent=2) + '\\n')\n\t\t\t\toutf.write(json.dumps(app.name, indent=2) + '\\"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tfor namespace in app.resources.values():\n\t\tif namespace.namespace == app.namespace:\n\t\t\t# we didn't already know that the resource was a Kubernetes resource\n\t\t\tif namespace.type == 'kubernetes-custom':\n\t\t\t\t# we didn't already know it is a custom observer schema and so we're\n\t\t\t\t# not generating the schema for now\n\t\t\t\tschema = {}\n\t\t\telse:\n\t\t\t\t# we are either a custom resource but a custom observer schema, or\n\t\t\t\t# something that requires a custom observer schema, so we need to generate\n\t\t\t\t"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tschema = {}\n\t\n\tif app.spec.spec_file:\n\t\tschema[\"name\"] = app.spec.spec_file\n\t\tif app.spec.name_suffix:\n\t\t\tschema[\"name_suffix\"] = app.spec.name_suffix\n\t\tschema[\"type\"] = app.spec.type\n\t\tschema[\"default_observer\"] = app.spec.default_observer\n\t\tschema[\"dependencies\"] = {}\n\t\n\tfor resource in app.spec.list_resources():\n\t\tif resource.is_remote:\n\t\t\tschema[\"exclude"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tself._generate_schema(app)\n\n\tprint('Finished generating default observer schema.')\n\n@with_app('default-observer-schema')\n@with_app('default-observer-schema-builder')\ndef generate_default_observer_schema_builder(app):\n\tprint('Generating default observer schema from default project.')\n\t\n\tfor resource_type in app.resources:\n\t\tself._generate_observer_schema_from_resource(app, resource_type)\n\n\tprint('Finished generating default observer schema (with no custom project)')\n\n/*\n "}]}
{"task_id": "62b43427903eeb48555d3ea5", "prompt": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tfrom.. import exc, logging, parse_qformat\n\t\n\tif not isinstance(sql, str) or not sql.startswith('SELECT ') or self is None:\n\t\tlogging.warning('Not currently doing so; attempting to use out-style '\n\t\t                'parameters instead of the in-style parameters.')\n\t\n\tout = re.sub(r\"(\\.\\.\\.?)*\",\n\t\t      lambda r: self.qformat(s), sql).strip('\\n\\n').split('\\n\\n')"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t\ndef format_date_format(format, date_format):\n\t\"\"\"\n\tFormat the provided value into the :attr:`date_format` of the\n\tprovided string. The formatter used in the conversion is\n\tinterpreted with the ``.`` operator, see :class:`datetime.date`\n\tfor details.\n\t\n\t*fmt* (:class:`~collections.abc.Sequence`) contains date and time\n\tpairs, formatted as a Python :class:`list` of dictionaries mapping\n\tthe given date and time format to the :class:`.DateFormatter`\n\tobject.\n\t\n"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\treturn sql.__format_params(sql.PARAMS, params)\n\nclass SQLAlchemyModel(SQLObject):\n\t\"\"\"A :class:`SQLObject <SQLObject.SQLObject>` is an abstract class. It is\n\tan internal representation of SQLObjects (ie anything that can be easily\n\trepresented as :class:`SQLObject <SQLObject.SQLObject>` using a\n\tgeneric model) and has been introduced by the SQLAlchemy 1.0 module with\n\tthe following modifications:\n\n\t>>> print sqla.sql.format(sql.DATABASES)\n"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tout_params = dict()\n\tfor param in params:\n\t\tif param is None:\n\t\t\tcontinue\n\t\t# we will only output a single string if the output parameters\n\t\t# need to be converted\n\t\tif param.kind =='string' and not isinstance(param, str):\n\t\t\tout_params[param.name] = param.format(**params)\n\treturn out_params\n\n\ndef get_params(sql):\n\t\"Return a mapping of the SQL-formatted parameters.\"\n\tif sql and not sql:\n\t\treturn None\n"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tif not callable(formatter):\n\t\traise ValueError(\"formatter must be function.\")\n\tf = formatter(sql, **kwargs)\n\tif hasattr(f, \"_params\"):\n\t\tparams = f._params\n\telse:\n\t\tparams = get_list_of_params(f, kwargs)\n\tif'sql' in params[1]:\n\t\treturn sql + '\\n'.join('%s=%s' % (key, value)\n\t\t\t    for key, value in params[1].items()) + '\\n'\n\treturn sql\n\n\n\n\n"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\traise ValueError(\"Invalid format\")\n\n\tcolumns = [(\"%(param_name)\", \"%s (%(param_name)s\") %\n\t\t(t, \", \".join(\n\t\t\tt.escape_table_label()\n\t\t))] for t in columns) for param_name in params]\n\n\tif not columns:\n\t\tcolumns = [\"NULL\"]\n\n\tcolumns_str = \", \".join(columns)\n\tcolumns_str = re.sub(r\"[]%s\\(\\\"%s\\\"\\)\",\"%%s %%s%%s\" % columns_str, \"[]"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tfrom. import base\n\tfrom.. import conversions\n\tfrom.exceptions import ParameterNotFound\n\n\tif not conversions._has_conversion(params_name=params_name, params=params):\n\t\tparams_list = {}\n\t\tsql = conversion._sql_to_string(params_name, params, *params_list)\n\t\treturn sql\n\tif not conversions._has_conversion(params_name=params_name, params=params):\n\t\tsql = conversions.format((params_name,), *params_list)\n\t\treturn sql\n\t\n\ttry:"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tif not SQL or not isinstance(SQL, str):\n\t\terrMsg = \"You must pass a SQL string\"\n\t\traise TypeError(errMsg)\n\tif not isinstance(SQL, str):\n\t\terrMsg = \"The argument SQL must be a string\"\n\t\traise TypeError(errMsg)\n\tif SQL and SQL!= \"\":\n\t\terrMsg = \"Multiple SQL parameters not allowed\"\n\t\traise TypeError(errMsg)\n\tSQL = SQL or []\n\tif SQL and SQL!= \"\":\n\t\tSQL = \",\".join(SQL)\n\t\n"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tdef format1(x):\n\t\tif x is None:\n\t\t\tpass\n\t\telif isinstance(x, (list, dict)):\n\t\t\tout, params = format2(x)\n\t\t\tout.append('(')\n\t\t\tfor k in params:\n\t\t\t\tout.append(format2(k))\n\t\t\tout.append(')')\n\t\t\treturn out\n\t\telif isinstance(x, str):\n\t\t\tout = format(x)\n\t\t\tif isinstance(out, tuple):\n\t\t\t\tif isinstance(out[0], collections.abc."}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tclass _QueryFormatter(_FormatQuery):\n\t\t\n\t\t@staticmethod\n\t\tdef format(sql):\n\t\t\tif callable(sql):\n\t\t\t\treturn sql(sql.split(separators, 1))\n\t\t\telse:\n\t\t\t\treturn sql\n\n\t\tdef __init__(self, *args, **kwargs):\n\t\t\tsuper(_QueryFormatter, self).__init__(*args, **kwargs)\n\n\nclass _FormatQuery(_QueryFormatter):\n\n\tdef format(self, sql) -> str:\n\t\treturn sql.split(separators, 1)[0]"}]}
{"task_id": "62b43428903eeb48555d3eaa", "prompt": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t# If a list is passed, use it, else keep a copy and pass back to\n\t# a tuple of ``(one,..., n_params)`` for each parameter or `list`\n\t# of parameters.\n\tif isinstance(many_params, list) or isinstance(many_params, tuple):\n\t\tparams = one_many(many_params)\n\telse:\n\t\tparams = one_many(many_params)\n\t\n\tif SQLParams.in_style < 0:\n\t\toutfmt = params._out_fmt\n\t\tif isinstance(params._"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t# TODO: Support more parameters.\n\tquery = \"SELECT %s FROM %s \" % (\n\t\tSQL.Sql.quote(s),\n\t\tSQL.Sql.quote(\"SQL\"),\n\t)\n\tparams = []\n\tfor k, vs in SQL.Sql.dialect.param_style.items():\n\t\tparams.insert(0, k)\n\t\tfor s in vs:\n\t\t\tif s:\n\t\t\t\tparam = SQL.Sql.quote(s)\n\t\t\t\tval = quote(s)\n\t\t\t\tquery += f\"\"\"\\t{SQL"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\treturn formatmanyparams(params, formatmany)\n\n@formatmany(b':'*params*')\ndef formatmanyparams(query, b):\n\t'''\n\t*params* (:class:`~collections.abc.Iterable`) is a single mapping of (name, value) pairs\n\tthat should be used during a \"format\" query.\n\t\n\t*b* (:class:`bool`) specifies if the column is formatted by a \"format\" query.\n\t'''\n\tif isinstance(b, bool):\n\t\tc = (b,), {}\n\t\treturn dict(sql=format"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tfrom copy import deepcopy\n\t\n\tfrom warnings import warn\n\timport sqla\n\t\n\tif not args:\n\t\t# no query was given, so default output to in-output style\n\t\tpass\n\telse:\n\t\t# format the format as comma separated list of params (or else convert to a list of pairs)\n\t\tparams_str = \", \".join(str(p) for p in args[1:]).strip()\n\t\t# convert it from ``sqlalchemy.sql.util.string`` to ``bytes``\n\t\tconverted_values = []\n\t\t"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\traise TypeError, \"SQLParameters should be the same type as *formatmany* and in-style parameters.\"\n\n\treturn formatmany(\n\t    sql=sql,\n\t    many_params=many_params,\n\t    args=args,\n\t    params=params,\n\t    sep=sep)\n\n\ndef formatasansi(\n__doc__, params=None, sep=\"\\t\", **kw):\n\t\"\"\"\n\tTurn ANSI-style SQL statements into ANSI-style SQL statements.\n\t\n\tIf ``params`` is specified then use it as:"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tdef _format(expr, *args, sql=None):\n\t\t# type: (dict, sql, optional) -> (str, str)\n\t\tif not sql:\n\t\t\tsql = sql_for_format(expr)\n\t\t\tif not sql:\n\t\t\t\tsql = sql_for_format(expr, only_with_params=False)\n\t\t\treturn sql, sql\n\t\t# type: (dict) -> str\n\t\tif isinstance(expr, tuple):\n\t\t\texpr = dict(expr)\n\t\tfor pict in expr:\n\t\t"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tif isinstance(sql, (str, Bytes)):\n\t\tif not is_list_like(sql) and not hasattr(sql, '__iter__'):\n\t\t\traise NameError('invalid query (no param-names) passed to Stringify().')\n\t\tif not one_of(sql, b'', set(['OR', 'AND'])):\n\t\t\traise RuntimeError('Invalid input type {}'.format(type(sql)))\n\t\treturn sql.split('(', 1)[0].split(b')', 1)[1]  # type: ignore\n\t\n\t# in-style query\n"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tresult = [format_query(subq) for subq in subqs]\n\n\tif sub_params is None:\n\t\treturn result, None\n\n\tfor param_list in param_list.values():\n\t\t\n\t\tif (isinstance(param_list, dict) and param_list.get(\"sql\") == \"sql\"):\n\t\t\tresult = append([result], convert_sql_queries(sub_params, param_list))\n\t\t\tcontinue\n\t\telif param_list.get(\"sql\"):\n\t\t\tresult = append([result], convert_sql_queries"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\ts, n = get_sql_info(sql)\n\tp, r = pyleaf(formatmany1(s, many_params))\n\tif p:\n\t\treturn tuple(p)\n\n\tif sql == \"select\":\n\t\tp = \"\"\n\t\tr = select(many_params)\n\t\tif n:\n\t\t\tif n:\n\t\t\t\tp = \",\" + \",\".join(n)\n\n\t\tif r:\n\t\t\tfor k,v in r.items():\n\t\t\t\tp = \", *\" + v + \""}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\treturn SQL.formatmany(format_query_text(*format_params_text(*params)).format(**fmt), **fmt)\n\n\ndef from_sql(sql, opts={}, fmt=[], **fmtspec):\n\t\"\"\"Converts an SQL query into an iterable of sequences of the given\n\tformat specification.\n\t\"\"\"\n\treturn SQL.from_sql(sql, *opts, fmt=fmt, **fmtspec)\n\n\ndef from_sql_params(sql, opts={}, fmt=[], **fmtspec):\n\t\"\"\"Converts an SQL query into an iterable of sequences into the given\n"}]}
{"task_id": "62b45df05108cfac7f2109ce", "prompt": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\ttry:\n\t\tos = pyfs(path)\n\texcept TypeError:\n\t\treturn False\n\treturn os.path.isfile()\n\t\n\t# def __del__(self, arg):\n\t# self.__path[1:3] = self.__path[1:] ## for os.path_isdir()\n\t\n\t# def isfile(self):\n\t\n\t# path = self.__path\n\t\n\t# if os.path.abspath(path)!= path:\n\t#     os.makedirs(path)\n\n\t# if os."}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tif path is None:\n\t\treturn False\n\timport os\n\tif os.path.exists(path) is True:\n\t\tif not os.path.isfile(path) or self.files_available == 0:\n\t\t\traise ValidationError(\"Unable to find OCFL file '{}' in {}\".format(path, self.dirs))\n\t\telse:\n\t\t\treturn True\n\treturn False\n\ndef get_files_by_id(fileids):\n\t\"\"\"\n\tReturn the files for a specific OCFL ID from a fileid/path\n\t\n"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tif not bpy.path.exists(path):\n\t\t# If missing validate, warn.\n\t\tprint(f\"Missing path {path}.\")\n\t\tprint(\"Recommended validation:\")\n\t\tprint(\" * \".join(str(i) for i in bl.validate_paths()))\n\t\tprint()\n\t\treturn False\n\telse:\n\t\tif bpy.data.objects and bpy.data.objects.editable:\n\t\t\t# Validate object.\n\t\t\tself.validate_object(path)\n\t\telse:\n\t\t\tprint(\"Error: Object not"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tif not os.path.isdir(path):\n\t\treturn False\n\tif not os.path.isfile(path):\n\t\twarning(\"File '%s' is not a valid OCFL object.\" % path)\n\t\treturn False\n\tret = False\n\tpaths = os.listdir(path):\n\tfor path in paths:\n\t\tif not os.access(path, os.R_OK):\n\t\t\tret = True\n\t\t\tbreak\n\tif not ret:\n\t\twarning(\"File %s does not exist.\" % path)\n\t"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tif 'path' in path:\n\t\tif not os.path.isfile(path):\n\t\t\traise ValueError('Provided path is not found: %s' % (path,))\n\t\traise TypeError('Must be path that defines an OCFL object or should be file')\n\telse:\n\t\traise TypeError('Must be a path with OCFL object as an argument')\n\n#include \"Game/AI/AI/aiAiPlayerAttack.h\"\n\nnamespace uking::ai {\n\nPlayerAttack::PlayerAttack(const InitArg& arg) : PlayerLook("}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tif self._path_exists(root, path):\n\t\treturn True\n\telse:\n\t\twarning(\"Unable to find %s in %s\" % (path, self._root_path))\n\t\treturn False\n\nclass _OCFLResource(object):\n\tdef __init__(self, root, path):\n\t\tself._root_path = root\n\t\tself._path = \"%s/%s\" % (path, self._root_path)\n\t\t\n\tdef exists(self):\n\t\treturn True\n\t\t\n\tdef refresh(self):\n\t\t"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tdef validate(self, root):\n\t\t\"\"\"\n\t\tRaises OCFL exception if path is not found or has invalid type.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tos.stat(path)\n\t\texcept OSError:\n\t\t\traise OCFLException('OS File not found'+ path)\n\t\telse:\n\t\t\tif not self.isvalid(os.path.realpath(path)):\n\t\t\t\traise OCFLException('path is not within OS file') # pylint: disable=E501\n\t\t\t\timport warnings # noqa: E501\n"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t# Check for the existence of the OCFL (Open ObjectFS).\n\tif not self.OCFL:\n\t\traise OCFLMissingFeatureError()\n\t\t\n\t# The PyFS tree is a leaf. We must check for a valid OCFL.\n\tfound = False\n\tfor node in path:\n\t\tif path == '' or os.path.isdir(node):\n\t\t\tfound = True\n\tif not found:\n\t\traise OCFLDirectoryError()\n\t\n\treturn self.OCFL\n\t\nclass OCFLMissingFeatureError"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\traise ValueError(\"Don't support this method, since OCFL object\")\n\ndef compare(name, base, comp, verbose=False, **options):\n\t\"\"\"\n\tCompare two object paths. Returns a dict of paths in descending\n\torder. If an OCFL object for the two path points to the same OS\n\tfile as the first and second, compare succeeds. If a failure has an \n\tobject for the first and second, an exception will be raised.\n\t\n\tWhen using a string object, it is treated as a binary path.\n\t\n\tExamples:\n\t\n\t\tdef"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t# TODO: is this useful now?\n\tif os.path.sep!= '\\\\':\n\t\tos.chdir(os.path.join(path, os.path.sep))\t\t\t\n\tdef validate(self, path, error_message=None):\n\t\tif self.root:\n\t\t\tsys.path.join(self, self.root, path).check()\n\t\tif self.root == path:\n\t\t\tprint('\"%s/%s\"' % (self.root, path))\t\n\t\t\tprint('warning: %s has been specified' % (self.root"}]}
{"task_id": "62b45df15108cfac7f2109dc", "prompt": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tif self.has_status_prefix(prefix):\n\t\treturn self.status_prefix_str(prefix)\n\treturn None\n\ndef name_status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string of validator name.\n\t\"\"\"\n\tif self.has_name_prefix(prefix):\n\t\treturn self.name_prefix_str(prefix)\n\treturn None\n\ndef name_status_abbrev_str(self, prefix=''):\n\t\n\t\"\"\"\n\t\tReturn string of validator name, with optional prefix.\n\t\t\"\"\"\n\n\t"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tif not 'OK' == self.status:\n\t\treturn \"OK\"\n\tif not 'NotApplicable' == self.status and not 'Invalid' == self.status:\n\t\treturn self.status\n\tif not 'Expired' == self.status and not 'Invalid' == self.status:\n\t\treturn prefix + self.status\n\tif not 'Expired' == self.status:\n\t\treturn prefix + str(self.status)\n\treturn self.status\n\n\nclass Validator(object):\n\t\"\"\"\n\t\tValidator is an abstract class, responsible for"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tstatus = getattr(self, prefix + '0', 0)\n\treturn status & self.status_mask[int(offset) % self.status_mask.size - 1]\n\n\ndef start_validator(name):\n\t\"\"\"\n\t\tCheck the given name against the current validator status.\n\n\t\tParameters:\n\t\t\tname (str): The validated name to check. It must be a string of\n\t\t\t\ta valid name whose last position is 1 - 127.\n\t\tReturns:\n\t\t\tbool: True if validator is ready, False if not\n\t\"\"\"\n\t"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t# Default value\n\treturn self.label.status_str()\n\t# pylint: disable=no-self-use\n\ndef currencies_str(self, prefix=''):\n\t\"\"\"\n\tReturns string of Currencies, with optional prefix.\n\t\"\"\"\n\t# Default\n\treturn ''.join(self.label.currencies_str(prefix=prefix))\n\ndef exchange_list(self):\n\treturn self.label.exchange_list()\n\ndef mch_symbol_text(self):\n\treturn self.label.mch_symbol_text()\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tstatus = prefix + str(self.status)\n\treturn status\n\nclass ChanFlag(Enum):\n\t\"\"\"\n\tChoice\n\t\"\"\"\n\tOPEN = 1\n\tOPENING = 2\n\tOPENAL = 3\n\tCLOSING = 4\n\tCLOSINGING = 5\n\tCLOSINGAL = 6\n\n\tdef to_str(self):\n\t\tif self.value == self.OPEN:\n\t\t\treturn 'OPEN'\n\t\tif self.value == self.OPENING or  self.value == self.OPENAL:\n\t\t\treturn 'OPENING"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tif'status' in self.fields(prefix=prefix):\n\t\treturn '%s [%s]' % (self.fields(prefix)[0], self.fields(prefix)[1].decode())\n\telse:\n\t\treturn self.errors[EMSG_NON_VALIDATED]\n\n\nclass T_ListChecklists(Base):\n\tdef __init__(self, **kwargs):\n\t\t# See e.g. http://www.cl.cam.ac.uk/~mgk25/ucs/examples/lists.py\n\t\tsuper(T_ListChecklists,"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t# FIXME: The \"status\" attribute is only needed if the system provides validator status\n\tfrom i8\n\n\tprint prefix + str(self.status)\n\treturn \"\\n\\n\" + prefix + \"\\n\" + str(self.status)\n\nclass KeyDef(Typedef):\n\n\tdef status_str(self):\n\t\t\"\"\"\n\t\tReturn string of status of key, with optional suffix.\n\t\t\"\"\"\n\t\t# FIXME: The \"status\" attribute is only needed if the system provides validator status\n\t\tif (self.status in (\"valid\","}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\treturn prefix + self.status_prefix\n\n\n@validator_method\ndef set_validator_time(self, validator, validator_time):\n\t\"\"\"\n\tSet validator time.\n\t\"\"\"\n\treturn ''.join([\n\t\t_('{0:0.0{}}}').format(datetime.datetime.now(self.date).replace(year=1900).isoformat(), constraint=str(validator_time) )\n\t])\n\n\n@validator_method\ndef validate(self, timeframe_list, default_frame_date=None, default_frame_time="}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tfor name in self._validator_status:\n\t\tif name[:6] == prefix:\n\t\t\treturn name[6:]\n\telse:\n\t\treturn 'Unknown'\n\t\ndef get_status_str(self):\n\t\"\"\"\n\tReturn string of validator status as a string.\n\t\"\"\"\n\tif self._validator_status:\n\t\treturn get_status_str(self._validator_status)\n\telse:\n\t\treturn 'Unknown'\n\ndef get_validated(self):\n\t\"\"\"\n\tReturn a boolean indicating whether validation performed in this\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tstatus, prefix = get_status_str()\n\tif status not in [ 'NONE', 'CONFIRMED' ]:\n\t\tstatus = 'NONE'\n\n\treturn self.status_map_str(status, prefix)\n\n\ndef get_status_str(status='NONE'):\n\treturn status.capitalize() + '-' + status.lower()\n\n\nclass MessageModel:\n\n\tdef __init__(self, name, description, priority, message_id_list_type = None, messages_count = None, tags = None, type_id = None, custom"}]}
{"task_id": "62b45df15108cfac7f2109dd", "prompt": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t# TODO(eriz)\n\treturn textwrap.dedent('''\\\n\tdef status():\n\t\tpass\n\n\tdef status_with_state_str(self, state):\n\t\tpass\n\n\tdef log_(self, prefix='', print_to=None, level=None,\n\t\t       indent_args=None):\n\t\t\"\"\"\n\t\tPrint to stdout if verbose is False.\n\t\t\"\"\"\n\t\tif print_to:\n\t\t\tif isinstance(print_to, str):\n\t\t\t\tprint_(prefix, print_to"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tl = [\"\"]\n\tif self._status!= None and self._status!= No_validation:\n\t\tl.append(_translate(self._status, prefix))\n\tif self._traceBackref is not None:\n\t\tl.append('<Traceback> (' + _translate(_traceback.format_stack(), prefix) + ')\\n')\n\tif self._timeCreated is not None:\n\t\tl.append('<timeCreated>%s</timeCreated>\\n' % self._timeCreated)\n\tl.append(\"======================================================================\\n\")\n\tif self._"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tif not isinstance(prefix, str):\n\t\tprefix = 'o' + prefix\n\tif not isinstance(prefix, str) or not prefix:\n\t\traise ValueError('prefix cannot be none or string.',\n\t\t                 prefix=prefix)\n\tif not '%s%s' % (prefix[1:].upper(), prefix[1:])[0] not in self._re_errors:\n\t\tself._re_errors.add(prefix[1:])\n\treturn prefix\n\ndef validation_status(status=None"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn '%s %s' % (status_str(self.status), prefix) + '<%s %s>' % (\n\t\tself.label, status_str(self.status.split('_')[2], prefix))\n\n\ndef get_status_str(status='pending', prefix=''):\n\t\"\"\"\n\tString representation of the `status` filter.\n\t\"\"\"\n\treturn '%s %s' % (status_str(status, prefix), prefix)\n\n\ndef parse_status(status):\n\t\"\"\"\n\tParse a status string and determine the `status`"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tif not self._is_set:\n\t\traise AttributeError(\"Validation log does not have set data\")\n\n\ttry:\n\t\tprefix ='status_' + prefix\n\texcept AttributeError:\n\t\traise ValueError(f'Missing {prefix} in status string')\n\n\treturn self._set.get(prefix)\n\n# =============================================================================\nclass ValidationLog:\n\t\"\"\"\n\tStatus string representation of validation log, with optional prefix.\n\t\"\"\"\n\t# ------------------------------------------------------------------\n\tdef __init__(self, group_id, group_label=None, group_value=None, group_"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn 'VALIDATION log'+ self.str(prefix=prefix)\n\n\ndef time_diff(start, end):\n\t\"\"\"\n\tReturn string representation of time difference.\n\t\"\"\"\n\treturn timedelta_str(end) - timedelta_str(start)\n\n\ndef log_time_ratio(start, end, prefix=None, suffix=''):\n\t\"\"\"\n\tReturn string representation of log clock time ratio.\n\t\"\"\"\n\treturn '%.12s %s' % \\\n\t\t\tround(tdelta_str(end - start, prefix=prefix,"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tprefix += '-'\n\treturn prefix + self.__dict__['error'].__str__()\n\n@six.add_metaclass(ABCMeta)\nclass FileMetadata(object):\n\t\"\"\"Representation of a metadata object.\"\"\"\n\t# NOTE: we can't make any explicit assumption about the order of metadata,\n\t# because the order of metadata must be important for sorting etc. The\n\t# order between metadata in a file is undefined, unless there was a\n\t# ``<date>`` or a ``<filename>`` key. We might need to make a strong\n\t# assumption before checking"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tstatus = ''\n\tif'status' in self:\n\t\tstatus_arr = self.status.split()\n\t\tfor status_str in status_arr:\n\t\t\tstatus = status_str.strip()\n\t\t\tif prefix not in status_str:\n\t\t\t\tgoto bad_status\n\n\treturn status\n\n// Code generated by protoc-gen-go. DO NOT EDIT.\n// source: google/cloud/tasks/v2beta3/common.proto\n\npackage tasks // import \"google.golang.org/genproto/googleapis/cloud/tasks/v"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tif not prefix:\n\t\traise ValueError(\"Prefix must not be empty\")\n\tresult = \"%s/%s\" % (prefix, self.status)\n\treturn result\n\n# -- log types\n\nclass ValidationLogType(object):\n\tpass\n\nclass Version(object):\n\tpass\n\nclass Age(object):\n\tpass\n\nclass Type(object):\n\tpass\n\nclass Format(object):\n\tpass\n\nclass DateFormat(object):\n\tpass\n\nclass FormatTime(object):\n\tpass\n\n/*\n * JBoss, Home of"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn prefix + \"status\" + str(self.status)\n\n\nclass Validation(object):\n\t\"\"\"\n\tA class holding validation information and errors that should be\n\ttracked during evaluation.\n\t\"\"\"\n\tdef __init__(self, name, description, path, line, line_dict=None):\n\n\t\tself.status = None\n\t\tself.name = name\n\t\tself.description = str(description)\n\t\tself.path = path\n\t\tself.line = line\n\t\tself.line_dict = line_dict or line.get_"}]}
{"task_id": "62b45e135108cfac7f2109f4", "prompt": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\tdef _is_valid_(self, identifier):  # pylint: disable=unused-argument\n\t\treturn identifier == self.IDENTIFIER\n\n\ndef is_required(definition):  # pylint: disable=invalid-name\n\t\"\"\"\n\t\tReturn True if identifier is of a required type.  \n\n\t\tThe definition must support the is_required interface.\n\n\t\tNOTE:: This function is for compatibility with Beer-Hullo.\n\t\"\"\"\n\n\tdef identifier(def):  # pylint: disable=unused-variable\n\t\tdef_ = def.identifier()\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn False\n\ndef get_node_by_id(root, id, prefix=\"\", last_segment=\"\"):\n\t\"Get a node by path prefix\"\n\tif not root:  # Root path should always have first segment\n\t\troot = first_leaf(root)\n\telse:\n\t\troot = get_all_segments(root, prefix, last_segment)\n\treturn get_next_node_by_id(root, id)\n\ndef get_node_by_id_with_prefixes(root, identifier, prefix=\"\"):\n\t\"Get a node"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\ndef is_valid_name(self, identifier):  # pylint: disable=unused-argument\n\t\"\"\"\n\tReturn True if identifier is valid and name is valid.\n\t\"\"\"\n\treturn True\n\n#\n#  Copyright 2019 The FATE Authors. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\ttry:\n\t\treturn self.is_valid(identifier)\n\texcept TypeError:\n\t\treturn False\n\n\nclass BaseModel:\n\t\"\"\" This class represents all object properties for a model \"\"\"\n\n\tdef __init__(self,\n\t\t\t\t\t\t  name=None,\n\t\t\t\t\t\t  api_version=None,\n\t\t\t\t\t\t  description=None,\n\t\t\t\t\t\t  version_added=None,\n\t\t\t\t\t\t  created_date=None,\n\t\t\t\t\t\t  current_version=None,\n\t\t\t\t\t\t  previous_version=None,\n\t\t\t\t\t\t  base_version=None,\n\t\t\t\t\t\t  model_id="}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\tif not is_valid_identifier(identifier):\n\t\treturn False\n\telse:\n\t\treturn True\n\n\ndef is_valid_identifier(identifier):  # pylint: disable=invalid-name,unused-argument\n\t# We treat this as an invalid identifier.\n\ttry:\n\t\tid_pattern = re.compile(r'^[a-zA-Z0-9]+$')  # pylint: disable=invalid-name\n\t\treturn re.search(id_pattern, identifier) is not None\n\texcept:\n\t\treturn False\n\ndefine(\""}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn identifier.startswith(\"T\")\n\n\n@dataclasses.dataclass\nclass _Metadata(DataContainer):\n\tname: str\n\tdescription: str\n\toptions: List[str]\n\tannotations: Dict[str, ClassAnnotation]\n\n\n@dataclasses.dataclass(frozen=False)\nclass _ClassMetadata(DataContainer):\n\tname: str\n\tdescription: str\n\toptions: Dict[str, ClassOption]\n\n\n@dataclasses.dataclass\nclass _Definition(DataContainer):\n\t\"\"\"\n\tA class definition is a class"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n#ifndef BOOST_DETAIL_OS_LOG_IMPL_HPP_INCLUDED\n#define BOOST_DETAIL_OS_LOG_IMPL_HPP_INCLUDED\n\n// MS compatible compilers support #pragma once\n\n#if defined(_MSC_VER) && (_MSC_VER >= 1020)\n# pragma once\n#endif\n\n//\n//  detail/detail_os_log.hpp - OS logging implementation\n//\n\n// MS compilers support #pragma once\n#if!defined(__clang__)\n# if defined(_MSC_VER) && (_MSC_VER >= 1700)\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\tif isinstance(identifier, bytes):\n\t\treturn False\n\tif isinstance(identifier, six.text_type):\n\t\treturn identifier.strip().encode('utf-8') in self.__encodings\n\telif isinstance(identifier, object):\n\t\treturn is_valid(identifier)\n\nfrom test_helpers import create_fixtures\n\nfrom.common import (\n    AuthUserTestCaseV21, AuthUserTestCaseV03,\n    TestClientBase, test_base_authenticate, test_base_unauthorize,\n    test_base_refresh_token"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\tself._check_is_valid(identifier)\n\treturn  # self.is_valid(identifier)\n\nclass Function(object):\n\t\"\"\"Class specific type-safe container for `Function' objects.\"\"\"\n\n\t@staticmethod\n\tdef from_function(func, *args, **kwargs): # pylint: disable=arguments-differ\n\t\t__args = list(args)\n\t\t__kwargs = list(kwargs)\n\t\tif callable(func):\n\t\t\treturn func\n\t\tif not args:\n\t\t\tif not kwargs:\n\t\t\t\traise InvalidParameter\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\traise NotImplementedError('is_valid() does not return useful information')\n\nclass Averaged(Entity):\n\t__abstract__ = True\n\tpass\n\n/* Copyright (C) 2011 [Gobierno de Espana]\n * This file is part of \"Cliente @Firma\".\n * \"Cliente @Firma\" is free software; you can redistribute it and/or modify it under the terms of:\n *   - the GNU General Public License as published by the Free Software Foundation;\n *     either version 2 of the License, or (at"}]}
{"task_id": "62b45e145108cfac7f210a07", "prompt": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tif extract_spec_version:\n\t\tspec_version = self.spec_version\n\telse:\n\t\tspec_version = '0'\n\n\tif inventory.type:\n\t\tif inventory.type == 'inventory': \n\t\t\tinventory_obj = inventory\n\t\t\tif 'description' in inventory:\n\t\t\t\tinventory_obj.type = 'inventory_version'\n\t\t\tfor name in ['version']:\n\t\t\t\tif inventory_obj.inventory[name]:\n\t\t\t\t\tinventory_obj.inventory[name] = inventory_obj.inventory[name]\n\t\t\t"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\ttry:\n\t\tspec = self.spec\n\t\tv = getattr(spec,'version_id')\n\t\tif isinstance(v, int) and v >= '11':\n\t\t\tv = '11'\n\t\telif isinstance(v, int) or isinstance(v, float):\n\t\t\tv = float(v)\n\t\tself.spec_version = spec.version_id = (v, 1)\n\texcept AttributeError:\n\t\traise ValueError('Invalid inventory specification')\n\t\n\t#if it can't be parsed just return True without checking all the items\n"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t# NOTE: this validation is only to avoid circular dependencies between old version check and new version check\n\tif isinstance(inventory, dict) and inventory.get('type') == 'list':\n\t\tinventory_types = inventory.get('item_type') or inventory['type']\n\t\t# ensure that type is a string as well\n\t\tif isinstance(inventory_types, str):\n\t\t\tinventory_types = [inventory_types]\n\t\tinventory_type, spec_version, spec_versionbak = self.validation_items(inventory_types)\n\telse:\n\t"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tif extract_spec_version:\n\t\tif inventory.spec_version is None:\n\t\t\t# NOTE: for the latest version, if you don't set inventory.spec_version\n\t\t\t# manually, it is set to inventory.spec_version for all types\n\t\t\tinventory.spec_version = get_meta_value(inventory,'spec_version')\n\t\telif inventory.spec_version == INVALID_SPEC_VERSION:\n\t\t\tinventory.spec_version = inventory.spec_version_text\n\n\tassert inventory.spec_version is not None\n\n\tassert"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tself.spec_version = inventory[0].get('spec_version', None)\n\tif (self.spec_version is None and inventory[0].is_set('type')) # there may be a type but no version\n\t\treturn False\n\tif self.spec_version is not None and inventory[0].is_a(\"str\") and isinstance(inventory[0].content, str) and inventory[0].content.isdigit() and inventory[0].content > self.spec_version:\n\t\tprint(\"Can't extract the spec version from the inventory.  The spec version"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tif not self.spec_version:\n\t\tself.spec_version = inventory.schema_version()\n\t\t\n\tif inventory.__version__ == VALID_VERSION:\n\t\tprint(f\"Validing item '{inventory.id}': {inventory.name}\")\n\t\t\n\tif inventory.name not in inventory.schema_names:\n\t\tprint(f\"Invalid name: {inventory.name} - is not in schema\")\n\t\t\n\tif not inventory.__version__ == VALID_VERSION and inventory.__schema__ == VALID_SCHEMA:\n\t\tprint(f\"Valid schema: {inventory"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tinventory['version'] = extract_spec_version\n\tif inventory.get('type') == 'SpecVersion':\n\t\ttype_version = inventory['type_version']\n\telif inventory.get('type_name') == 'SpecVersion':\n\t\ttype_version = inventory['type_version_value']\n\telse:\n\t\traise Exception(\"Unrecognized type in inventory\")\n\tinventory['type_version_string'] = type_version\n\tinventory.pop('type_description')\n\tinventory['format_text'] = inventory.pop('format_text') or ''\n\t"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tvalidator = Validator(self)\n\tvalidator.validate(inventory)\n\tif self.spec_version!= None:\n\t\treturn True\n\t\n\tfor type in self.inventory_types:\n\t\tif self.inventory_type_checker.is_type_required(type, extract_spec_version):\n\t\t\tvalidator.validate(inventory)\n\t\treturn True\n\t\n\treturn False   \n\nusing System;\nusing NetRuntimeSystem = System;\nusing System.ComponentModel;\nusing NetOffice.Attributes;\n\nnamespace NetOffice.PowerPointApi\n{\n\t/// <"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tspec_version_to_test = specutils.get_specversion(self.spec_version)\n\tif not self.spec_version.extract_type.compare(spec_version_to_test, 'type'):\n\t\tassert self.spec_version.extract_type == spec_version_to_test, \\\n\t\t\t\"Invalid version '%s' for type '%s', the version '%s' \" \\\n\t\t\t\"must match its extraction type '%s'\" % (self.spec_version.extract_type, \\\n\t\t\t\tself.spec_version.extract"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tassert isinstance(inventory, PandasInventory)\n\tif inventory.type!= 'compound':\n\t\traise KeyError(f'Type {inventory.type} is not a generic data type')\n\tif inventory.inventory_type == 'object':\n\t\tif extract_spec_version:\n\t\t\tinventory_spec = inventory.spec\n\t\telse:\n\t\t\tinventory_spec = self.spec_version\n\telif inventory.type =='sequence':\n\t\tassert inventory.value_type == 'integer', f'Inventory type is not integer ({inventory.type!r})'\n"}]}
{"task_id": "62b45e145108cfac7f210a09", "prompt": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor digest in manifest_files:\n\t\tif digests_used.intersection(digest):\n\t\t\traise exceptions.ManifestUpdateError(manifest_files, digest, digests_used)\n\n\ndef set_active_tags(manifest_files):\n\t\"\"\"\n\tSet active tags that are available for this manifest and add them to the existing tags list.\n\t\"\"\"\n\tfor manifest in manifest_files.findall(\"*.md\"):\n\t\tfor tag in manifest.findall(\"tag\"):\n\t\t\tif tag and tag[0].lower()!= \"h6\":\n\t\t\t"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor index, md in enumerate(manifest_files):\n\t\t# We keep track of information about each digest to ensure that it is\n\t\t# unique.\n\t\tall = set(md.split(\" \"))\n\t\tfor digest_name, digest_content in md.items():\n\t\t\tdigest_hash = digest_content.lower().replace(\"-\", \"_\")\n\t\t\tif digest_name in all:\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tall.add(digest_name)\n\t\t\t\tself.check_digest(digest_name, digest_content)\n"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor digest in digests_used:\n\t\tif not digest in manifest_files:\n\t\t\treturn 0\n\treturn 1\n\nclass Cli():\n\tdef __init__(self, input_file, output_file):\n\t\tself.cmdline1 = input_file\n\t\tself.cmdline2 = output_file\n\t\tself.prog_dir = os.path.join('-', '.clang_tool', 'bin')\n\t\tself.cmd1_args = (\"-xc\", \"--\", self.prog_dir, \"-fMakeExeOnly\",\n\t\t\t\t\t\t\t\t\t\"--"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor manifest_filename, manifest in self.read_manifest(manifest_files):\n\t\tfor digest in manifest.digests:\n\t\t\tif digest in self.file_digests and digest in manifest.file_digests:\n\t\t\t\tall_used_files.append(manifest.file_digests[digest])\n\n\tif manifests_used:\n\t\tfor digest in manifests_used:\n\t\t\tif digest in all_used_files:\n\t\t\t\tprint(\"[!] Digest '%s' is present in manifest.json\" % (digest))\n\t\t\t\tcontinue"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor digest in manifest_files:\n\t\tif digest.is_hash_digest():\n\t\t\tcontinue\n\t\tif digest.is_url_digest():\n\t\t\tcontinue\n\t\tif digest.filename.startswith(self.check_digests_uri_prefix):\n\t\t\tfiles = digest.files\n\t\telif digest.filename.endswith(self.check_digests_path_prefix):\n\t\t\tfiles = digest.files.keys()\n\t\telif digest.filename.endswith(self.check_digests_md5sum_prefix):\n\t\t\tfiles ="}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\twith open('artifacts-test.md', encoding='utf-8') as f:\n\t\tdigest_list_md = [MD(f.readline(), False) for _ in tqdm(manifest_files)]\n\tdigests_used = md_from_arrays(digest_list_md)\n\tif digests_used == 0:\n\t\treturn False\n\treturn True\n\n# (C) Datadog, Inc. 2021-present\n# All rights reserved\n# Licensed under a 3-clause BSD style license (see LICENSE)\nfrom codecs import open  # To use"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tdef _check_digests_present_and_used(self, manifest_files):\n\t\tmlist = manifest_files[:]\n\t\tfor f in manifest_files:\n\t\t\tmlist.remove(f)\n\n\t\tdlists = self.get_dicts(manifest_files)\n\n\t\tfor m in sorted(mlist):\n\t\t\tfound = None\n\t\t\twhile True:\n\t\t\t\tif f1 in m:\n\t\t\t\t\tif found is None:\n\t\t\t\t\t\tfound = f1\n\t\t\t\t\telif found not in m and not'manifest'"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tdigest_map = {}\n\tif len(manifest_files) > 0:\n\t\tfor manifest_file in manifest_files:\n\t\t\tif digests_used and (len(digests_used) == len(manifest_file.hashes) or digests_used == manifest_file.hashes):\n\t\t\t\tdigest_map[manifest_file.digest] = True\n\t\treturn len(digest_map) > 0 and digest_map.get(manifest_file.digest)\n\telse:\n\t\treturn True\n\n#pragma once\n\n#include \"stdafx"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\terrors = []\n\tfor digest in manifest_files:\n \t\tfor check in digest.CHECK.values():\n \t\t\tcheck.check_digests_present_and_used()\n\t\tif hasattr(digest, 'CHECK') and all(check.check_digests_present_and_used() for check in digest.CHECK):\n \t\t\terrors.append('Manifest has the same digest that is needed - the same check may be used.')\n\tif len(errors)!= 0:\n\t\traise Exception('Unable to verify all digests in Manifest - errors have been found: %"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\tfor digest in manifest_file['file_digests']:\n\t\t\tif digest in digests_used:\n\t\t\t\tself.print_msg('Found digest %d with already used manifest %s.' % (digest, manifest_file['name']))\n\t\t\t\treturn 0\n\t\t\telif not digest in digests_used:\n\t\t\t\tself.print_msg('Used digest %d not in manifest %s.' % (digest, digests_used[digest]))\n\t\t\t\treturn 0\n\n\t\treturn 1\n\ndef check_"}]}
{"task_id": "62b45e165108cfac7f210a16", "prompt": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tpri_obj = self.get_object_from_dict('inventory', prior)\n\tif not pri_obj:\n\t\traise Exception(\"Can't find the inventory data of version prior=%s\" % prior)\n\tif not isinstance(pri_obj, InventoryValidator):\n\t\traise Exception(\"The inventory data does not follow the data dictionary!\")\n\tif not isinstance(prior, InventoryValidator):\n\t\traise Exception(\"The inventory data doesn't follow the data dictionary!\")\n\n/**\n * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n * SPDX-License"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\treturn self.validate_prior_version(prior) or (\n\t\tprior.get_version()!= prior.version())\n\n@implementer(IInventoryValidator)\nclass InventoryValidated(object):\n    \n    _redelivered = False\n    \n    def __init__(self, obj, error_handler):\n        self.obj = obj\n        self.errors = []\n        self._redelivered = False\n       "}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tif not isinstance(prior, InventoryValidator):\n\t\tthrow ValidationError(\"prior must be a InventoryValidator instance\")\n\t\t\n\tif isinstance(prior, InventoryValidator):\n\t\treturn prior.validate_as_prior_version(self)\n\t\n\traise ValidationError(\"prior must be a valid prior version for inventory instance\")\n\t\n\n# Copyright 2018-2021 The PyE Develop\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from warnings import catch_warnings\n\tprior_obj = prior.object_dict()\n\tprior_obj.prior_ver_tuple = self.dict_key_by_objname('prior')\n\ttry:\n\t\tprior_obj.type_of_prior, prior_obj.prior_version, prior_obj.prior_ver_tuple = \\\n\t\t\tprior_obj.prior_type_of_prior_from_objname(prior_obj)\n\texcept Exception:\n\t\traise ValueError(\"prior type of prior for obj"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\traise InvalidParameterValue(prior)\n\tpass#IMPORTIMPORTIMPORT from. import version\n\tversion_string = version.versionstring_prior\n\n\tif not prior.check_version(\"prior\"):\n\t\traise AttributeError(\"Prior not implemented. Please define a valid version prior in the Inventory file\")\n\tif len(prior.get_required_fields(use_string=\"precedence\", default=True))!= 1:\n\t\traise RuntimeError,\"Illegal state, pre-emptive prior type is required\"\n\telse:\n\t\tif prior.get_required(type"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\traise exception.InvariantError('check_model: can not be called with prior already')\n\n\tfrom ezdxf.entities.model_validation import validate_inventory_version\n\tif prior.product_id not in self.items:\n\t\traise exceptions.ProductNotFoundError('check_model: The object has not been saved')\n\tif prior.version not in self.items[prior.product_id][0]['properties']['as_prior_version']:\n\t\traise exceptions.ItemVersionMismatchError(\"check_model: the object prior version already has that version\")\n\ndef validate_"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tif self.prior or prior.prior:\n\t\traise AnsibleLiveDataException(error.InvalidPriorValidator)\n\tif type(prior.source) == str and 'live'!= prior.source:\n\t\traise AnsibleLiveDataException(error.SourceIsNotLive)\n\tif type(prior.prior) == string and '1.6'!= prior.prior:\n\t\tif prior.prior_revision is not None and prior.prior_revision!= '-' and prior.prior.revision < prior.prior_revision:\n\t\t\traise AnsibleLiveDataException(error."}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t# check that the required input is a VersionManagerVersion\n\tif not isinstance(prior, VersionManagerVersion):\n\t\traise ValueError(\"Please pass a version object as parameter\")\n\t# check that the required input is a Version\n\tif not isinstance(prior, Version):\n\t\traise ValueError(\"Please pass a version object as parameter\")\n\t# check that the required input is a version, revision number, and version name\n\tif not isinstance(prior, int):\n\t\traise ValueError(\"Please pass a version object as parameter\")\n\t# check that the required input is a number\n\tif"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t# Verify version of the object\n\tdef validate_prior(prior):\n\t\t\"\"\"\n\t\tCheck that prior is a valid prior version of the current inventory object.\n\t\tReturn the version of the input variable\n\t\t\"\"\"\n\t\tif not prior.get_type_as_int() == InventoryValidator.TYPES.MIN_OBJ_VERSION_INT:\n\t\t\tfor _type, obj in enumerate(prior.get_inventory_objects()):\n\t\t\t\tif obj.get_type_as_int() == InventoryValidator.TYPES.MIN_OBJ_VERSION_INT:\n\t\t\t\t"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tif not is_version_set(prior):\n\t\traise ValueError(('An invalid prior version of data must be specified, but was'\n\t\t\t' specified as None, but returned None as the first item of object.',\n\t\t\tself.__class__.__name__, self.__dict__))\n\treturn PriorValidator(prior)\n\n\ndef run():\n\ttry:\n\t\t# this is a try-catch because it is already called here\n\t\timport dask.dataflow\n\t\timport dask.dataloading\n\texcept ImportError:\n\t\tpass\n\t\n\tenv ="}]}
{"task_id": "62b45e165108cfac7f210a17", "prompt": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tif not Version(version) > _versions[version]:\n\t\traise OSError('Version must be attained at %s' % version)\n\tif inventory.version.minor < version or inventory.version.major < version:\n\t\traise OSError('version should be between %s and %s' % (version, inventory.version.major, version))\n\t\n\t# Build a map of state to logical files\n\tphysical_logical_path_map = {'state':'physical', 'file':None}\n\tfor path in inventory.inventory.inventory_tree.get_logical"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\treturn InventoryStateToPaths(inventory, version, get_logical_path_map_from_contents_to_inventory)\n\ndef get_logical_path_map_from_contents_to_inventory(inventory, version):\n\t\"\"\"Reads contents from the contents file and returns a set of logical paths\n\tin the inventory.\n\t\n\tIf inventory is None, it returns empty dictionary.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in later\n\tversions than the version"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tcontent_files = version.get_inventory_content_files()\n\tlogical_path_in_state = {}\n\tphysical_paths = []\n\tfor path, content_file in content_files.items():\n\t\tlogical_paths.append((path, content_file))\n\t\ndef compute_logical_path(version):\n\treturn {logical_path.name: set(logical_path.version.reference_mapping[logical_path.path]) for logical_path in version.get_logical_path_map(inventory, version.version) if version.version."}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tcontent_set = set()\n\tif version in inventory:\n\t\tversion_file = inventory[version]\n\t\t\n\t\tcurrent_version = inventory[version_file.content_state_version]\n\t\tif \"version\" not in inventory or not current_version.startswith(\"b3v\") :\n\t\t\treturn inventory\n\t\treturn inventory[version_file]\n\t\t\n\tfor version_file in inventory:\n\t\tif version_file not in content_set:\n\t\t\tif \"version\" in inventory and version_file.content_state_version in content_set"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tcontent_files = inventory.get_content_files(version)\n\tlogical_path_set = set()\n\t\n\tfor content_file in content_files:\n\t\tlogical_path = inventory.get_logical_path(content_file.logical_file_id)\n\t\tif logical_path not in logical_path_set:\n\t\t\tlogical_path_set.add(logical_path)\n\t\t\t# the current version is a duplicate, because that part is no longer in the storage, hence we ignore any duplicate\n\treturn logical_path_set\n\ndef"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tout = { \"logical_1\" : set(), \"logical_2\" : set(), \"logical_3\" : set(), \"logical_4\" : set() }\n\t\n\t# Check versions\n\tversions = inventory.get_versions()\n\tfor version_ in versions:\n\t\tif version_ is not None and version_.logical_state[0] not in [version_['0']]: # '0' indicates a version that only contains one set of contents\n\t\t\t# Put the logical paths in the inventory\n\t\t\tout[version_].update({version_: set()})\n"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tif inventory and version:\n\t\tpath_map = {}\n\t\tlogical_paths_in_state = set()\n\t\tfile_path = inventory.directory.location_get()\n\t\t\t\n\t\tfor entry in file_path.listdir():\n\t\t\tif entry.strip().startswith('.'):\n\t\t\t\tlogical_path_in_state.add(entry)\n\t\tif version in logical_paths_in_state:\n\t\t\tlogical_path_map = logical_paths_in_state[version]\n\t\treturn logical_path_map\n\telse:\n"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tfile_path = inventory.get_version_path(version)\n\tlogical_path_in_state = {}\n\tfor logical_file in Inventory(file_path).list_files():\n\t\tlogical_file_paths = Inventory(logical_file.path).list_files()\n\t\tfor logical_file_path in logical_file_paths:\n\t\t\tlogical_path = inventory.get_path(logical_file_path)\n\t\t\tif logical_path == inventory.get_path(inventory):\n\t\t\t\tlogical_path_in_state[logical_file"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tlogical_path_in_state = collections.defaultdict(set)\n\t\n\tfor logical_path in inventory.get_logical_path(version):\n\t\tlogical_path_obj = Path(logical_path)\n\t\tcontent_files = None\n\t\t\n\t\tif 'files' in inventory[logical_path]:\n\t\t\tlogical_path_obj = inventory[logical_path]['files'][0]\n\t\t\tcontent_files = {content_file.path: content_file for content_file in\n\t\t\t\t\t\t inventory[logical_path]['files']}\n\t\telse:\n"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tout = {}\n\tfor key in inventory.keys():\n\t\tv = inventory[key]\n\t\tif type(v) == content.ContentFolder:\n\t\t\tout[v.id.name] = set(inventory[key].value.values()) | set(v.id.name)\n\t\telse:\n\t\t\tout[v.id.name] = v.read()\n\treturn out\n\nclass Plug:\n\tdef __init__(self, name, value=\"default\", default=None):\n\t\tself.name = name\n\t\tself."}]}
{"task_id": "62b45e175108cfac7f210a19", "prompt": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tinventory, uninstall_files = fixity_list(fixity)\n\tinventory_list = {}\n\tfor f in inventory:\n\t\tif not f and not manifest_files and not os.path.exists(f):\n\t\t\traise ValueError('%s file not found on the inventory' % f)\n\t\t# check that the folder specified is in use\n\t\tif inventory_path(f) not in self.path_to_data:\n\t\t\traise ValueError('%s is not a directory' % f)\n\t\tinventory_path(f)=os.path.join"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tdef validate_manifest(manifest):\n\t\tfor file_name in manifest['files']:\n\t\t\tif file_name[-3:] == \"d\":\n\t\t\t\tmanifest['files'].remove(file_name)\n\t\t\tif file_name[-4:]!= \"m\": # If the file wasn't named \"m\" or \"d\":\n\t\t\t\traise RuntimeError(\"Invalid file name for file referenced: \" + file_name)\n\t\n\tdef test_fixity(self, cmd, check_manifest, fixity_code):\n\t\n\t\timport glob\n\t\tfrom. import fix"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tmanifest_files.all_check()\n\tmissing = []\n\tfor filename, f in self.manifest_dict.items():\n\t\tif not f.is_dir():\n\t\t\tif filename not in manifest_files:\n\t\t\t\tmissing.append(filename)\n\t\t\telse:\n\t\t\t\tmissing.append(manifest_files[filename])\n\tmissing.sort() # sort by size\n\tmissing.sortby(); # sort by size\n\tbad = fixity & fixity.metadata\n\tif missing:\n\t\tmissing.sort() # sort by size\n\t\t"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tfor file_name in manifest_files:\n\t\tif file_name in [\"GBA_MANIFEST.json\"]:\n\t\t\tif fixity.fixity_block_is_in_inventory(file_name, manifest_files)==false:\n\t\t\t\terror(\"In Inventory, fixity %s is either in Inventory or in the manifest. Please re-run with --repack=force\",file_name)\n\t\t\t\t\n\t\t\t\nif __name__ == \"__main__\":\n\ttry:\n\t\tcheck_json = CheckJSON(sys.argv[1])\n\texcept:\n"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tfor item in manifest_files:\n\t\tif item.find(fixity.manifest_basename) >= 0:\n\t\t\traise ValueError(\"The manifest file \" + manifest_files[item] +\n\t\t\t\t\" has no corresponding fixity \" + fixity.manifest_basename +\n\t\t\t\t\".\")\n\tif len(manifest_files)!= 1:\n\t\traise ValueError(\"Manifests file count mismatch.\")\n\t\n\tfixity.manifest_id = manifest_files[0].rsplit('.', 1)[0]\n\treturn fixity\n\n@py.storage\n@py."}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tfixity_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '*v1.0.0')\n\tfor manifest_file in manifest_files:\n\t\tfixity_manifest = Manifest(manifest_file)\n\t\t\n\t\tfixity_manifest.parse()\n\t\t\n\t\tfor directory, files in fixity_manifest.get_all_files_in_inventory():\n\t\t\t\n\t\t\tdirectory_structure = fixity.find_dir_structure(directory, {})\n\t\t\tassert directory_structure is not None\n\t\t\tfor"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tself.check_block_compatibility(fixity, manifest_files)\n\n\ndef _check_items_in_container(name):\n\titems = self.get_all_items()\n\tfor i in range(len(items)):\n\t\titem = items[i]\n\t\titem.check_items_in_container(name)\n\t\n\tfor i in range(len(items)):\n\t\titem = self.get_container_item(i)\n\t\titem.check_items_in_container(name)\n\n\ndef _check_items_"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\tself.check_file(manifest_file, fixity)\n\n<?php\ndefined('TYPO3') or die();\n\nclass Migration_001_AddContentToPageTemplates extends Yoast_Database_Migration\n{\n    public function __construct()\n    {\n        $this->enable = true;\n    }\n\n    /** @see https://docs.typo3.org/en/latest/migrations.html#_table_tables_table_"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t# First make sure that all items in \"manifest-fixity\" are\n\t#   \"fixity file\" in manifest_files.\n\tfor item in manifest_files:\n\t\tif item.path.startswith(fixity.path.lstrip(' \\n')):\n\t\t\tif item.type == 'file' and item.path in fixity.path:\n\t\t\t\tfixity.fix |= (0o666 << 8)\n\t\t\t\n\tself.log_info(\"Manifest validation: fixity file found.\\n\")\n  def fixup_manifest_fixities(self):"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tmanifest = fixity['fixie']\n\tfor f in manifest.files.values():\n\t\ttry:\n\t\t\treadfile(os.path.join(self.get_repo_path(f['path']), f[f['file']]))\n\t\texcept IOError, err:\n\t\t\terror = \"No such file or directory \\n%s\" % err\n\t\t\tlogger.error(error)\n\t\t\treturn False, None\n\ttry:\n\t\tmanifest_file = manifest.files[manifest.file(manifest, manifest.checksum)]\n\texcept KeyError:"}]}
{"task_id": "62b463153879012d19481498", "prompt": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tpath = File(path)\n\treturn [path.name + ext for ext in get_extensions(path)]\n\ndef files_exists(path):\n\t\"\"\"\n\tReturn True if `path` exists\n\n\t\t\"\"\"\n\treturn not_found(path) and os.path.exists(path)\n\ndef file_exists(path):\n\t\"\"\"\n\tReturn True if `path` exists, otherwise False\n\t\"\"\"\n\tif isinstance(path, string_type):\n\t\tif not path.startswith(\".\") or not path.endswith(\".\"):\n\t\t\tpath ="}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tpath.mkdir(parents=True, exist_ok=True)\n\tfor file in os.listdir(path):\n\t\tif file.endswith(\".py\") and file.startswith(\"_\") and file.endswith(\".pyc\"):\n\t\t\tfilename = os.path.join(path, file)\n\t\t\ttry:\n\t\t\t\t# remove dot (i.e. don't allow dot to be removed)\n\t\t\t\tfilename = filename.replace(\".\", \".\")\n\t\t\t\treturn (\"\" + filename)\n\t\t\texcept AttributeError:\n\t\t\t\t# if the file might have an extension, it"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tres=utils.load_file(path)\n\n\tprint res\n\nif __name__=='__main__':\n\tfor filename in filename_list():\n\t    print utils.load_file(filename)\n\n/* ============================================================\n* Falkon - Qt web browser\n* Copyright (C) 2014, 2013 David Rosca <david.rosca@gmail.com>\n*\n* This program is free software: you can redistribute it and/or modify\n* it under the terms of the GNU General Public License as published by\n* the Free Software Foundation, either version 3 of"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\twith codecs.open(path, 'r', encoding=\"utf8\") as file:\n\t\treturn [a.rstrip() for a in file.read().split('\\n')]\n\n\ndef files_path(path):\n\tif isdir(path):\n\t\treturn os.path.join(path, '*.py')\n\telse:\n\t\treturn os.path.join(path, os.listdir(path))\n\n\ndef extract_scripts(path):\n\t\"\"\"\n\tExtract the main Python script (if any) with the\n\tpath passed in\n"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from. import myopen\n\tpass#IMPORTIMPORTIMPORT from pprint import pprint\n\tpass#IMPORTIMPORTIMPORT from EMAN2 import read_text_row\n\tpass#IMPORTIMPORTIMPORT from subprocess import PIPE\n\t\n\tpass#IMPORTIMPORTIMPORT from string import split\n\n\tpass#IMPORTIMPORTIMPORT from sp_global_def   import sxprint, ERROR\n\tpass#IMPORTIMPORTIMPORT import os\t\n\t\n\tsxprint(\"[*] Executing file lists...\")\n"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn os.path.basename(path)[:-4]\n\n\n@def listdir(path):\n\tret_list = []\n\tfor currentname in os.listdir(path):\n\t\tret_list.append(currentname)\n\treturn ret_list\n\n\ndef globdirs_recurse(files, name):\n\t\"\"\" Return a list of recursive globs \"\"\"\n\treturn sorted(set(glob.glob(name + '*')))\n\n\ndef mkdir(path):\n\tif not os.path.exists(path):\n\t\tos.makedirs("}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn os.listdir(path)\n\ndef get_path_from_uri(path):\n\t\"\"\"Return the absolute path for the given URI.\"\"\"\n\treturn path.replace(os.path.sep, \"/\")+'/'\n\ndef remove_if_directory(path):\n\t\"\"\"Return the list of directories that have the given path\"\"\"\n\tif os.path.exists(os.path.abspath(path)):\n\t\treturn os.listdir(os.path.abspath(path))\n\telse:\n\t\treturn [[os.path.sep]] if (os.path"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tassert not path.is_file()\n\tfiles = []\n\tfor _, file in os.walk(path):\n\t\tif not file.is_file():\n\t\t\tfiles.append(file.name)\n\treturn files\n\ndef unmake_path(path):\n\t\"\"\"\n\tRemove the path and its parents (if any).\n\n\tExample:\n\t\tunmake_path(\"/var/lib/python/2.6/python3.6.egg-info/version.py\")\n\t\"\"\"\n\tif isinstance(path, string_types):\n\t"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tdir = os.curdir\n\twith open(path) as f:\n\t\tfilelist = list(f.readlines())\n\n\tfor line in filelist:\n\t\tif os.path.exists(os.path.join(dir, line)):\n\t\t\treturn line.rstrip()\n\treturn []\n\nif __name__ == '__main__':\n\targs = parser.parse_args()\n\n\tfilelist = files_list(args.file_path)\n\n\tprint(filelist)\n\n<?php\n\n/**\n * @license  "}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tif not path.endswith(\".git\"):\n\t\tpath += \".git\"\n\tassert os.path.isdir(path), (path,)\n\treturn sorted(listdir(path))\n\ndef copy_file(source_path, destination_path, source_mode=\"r\", destination_mode=\"a\", force=False, filter_regex=None):\n\t\"\"\"\n\t\tTry to find file or directory at `source_path`\n\n\t\tIf found, create `destination_mode`, ``create_folder`` and ``create_directory``, copy `source_path` to `"}]}
{"task_id": "62b463153879012d1948149a", "prompt": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tif not xmlpath.search(xml_filename):\n\t\traise(RuntimeError, \"Can't locate file {} - does not exists.\".format(xml_filename))\n\tfiles_dir = os.environ.get('PACKAGE_INSTALL_ROOT',\n\t\tpywinpath.default_folder)\n\t\n\ttry:\n\t\tpkg_fnames = files[0].get_file_content()\n\texcept:\n\t\treturn None, None, None, None, None, None, None, None\n\n\tif 'zip' in pkg_fnames:\n\t\n\t\t"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tif isinstance(files, BaseList):\n\t\tfiles = [_group_files_by_xml_filename(f.strip() for f in files) for f in files]\n\telif not isinstance(files, list):\n\t\tfiles = []\n\t\tfor xml_basename in xmls:\n\t\t\tfiles += _group_files_by_xml_filename(source, xml_basename.strip(), files)\n\telif isinstance(files, BaseList):\n\t\tfiles = []\n\t\tfor xml_basename in xmls:\n\t\t\tfiles += _group_files_"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\timport zipfile\n\t\n\tdef group_keys(paths):\n\t\t\"\"\"\n\t\tParameters\n\t\t\tpaths: list of absolute paths from a given source, relative to the source folder\n\t\t\t\n\t\tReturns\n\t\t\tA dictionary containing the keys and values from the zip file.\n\t\t\"\"\"\n\t\tfrom collections import namedtuple\n\t\t\n\t\troot_paths=paths[:]\n\t\tfor root,subdir,dirnames,name,ext in zip(paths,root_paths,subdirs,subdir_dirnames[0],subpackage_names[1],subpackage_extensions[1]):"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\twith io.open(xml_filename, 'rb') as f:\n\t\twith open(filepath(source), 'r') as f2:\n\t\t\treturn json.load(f2)\n\n\ndef _extract_xml_to_json(source, output_base, xmls):\n\t\n\t\"\"\"\n\tRead the xml file and project all the data into a dict\n\t\n\tParameters\n\t----------\n\tsource : str\n\t    XML filename\n\toutput_base : str\n\t\tBase URI (defaults to $base_url if set)\n\txmls :"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tassert isinstance(xml_filename, str)\n\tassert isinstance(files, list)\n\t\n\t# Try to find the XML file with that name, using zip.ZipFile to open it\n\tzip_file = zip_open(xml_filename, 'r')\n\txml_files = zip_file.inf.namelist()['#'].xml_leaves()\n\txml_entry = zip_file.inf.infolist()['#']\n\t\n\t# Go through one of those files\n\tnum_files = len(zip_file.inf.infolist())\n\t"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tfor x, y in files:\n\t\tif x == '<xml_path>/':\n\t\t\tcontinue\n\t\telif y in xmls.xml_list('*.xml'):\n\t\t\tos.remove(y)\n\tpackage = { \n\t\t'xml_path': y, \n\t\t'xml_list': xmls.xml_list('*.xml')[:len(y)],\n\t\t'directive':'read',\n\t\t}\n\t\twith open(y, 'r') as fh:\n\t\t\tpackages = [ x.replace('.docx', '')"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tself._import_xml(self._group_xml_files_by_xml_name(source, xmls, files))\n\treturn {self.xml_file_name(name, self.__group_xml_file_names):self.$.xml_file_name(self.xml_file_path(name),self.__group_xml_file_names)}\n\nclass _GroupFileXmlAndZip(GroupFileXmlAndGroupFilename):\n\n\tdef __init__(self, source, name, zip_file_path=\"../\", zip_file_name=None, zip_file"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\txml_files = set()\n\tfor xml_filename in xmls:\n\t\tfilename_xml_path = xml_filename.split(\".\")[0]\n\t\tif isinstance(filename_xml_path, zipfile.ZipFile):\n\t\t\txml_path = os.path.join(dirname(filename_xml_path), xml_filename.split(\".\")[-1])\n\t\t\tif not os.path.isdir(xml_path):\n\t\t\t\tcontinue\n\t\t\tfiles.append({\"path\": xml_path, \"name\": filename_xml_path.split(\".\")[1"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tg_filename = _check_source(xml_filename)\n\tg_folders = []\n\tg_files = []\n\t\n\tg_group_folder = os.path.join(_g_config.FOLDER, \"group\")\n\tfilename_parts = _dir_basename(xml_filename)\n\tg_group_files_map = dict()\n\t\n\tfor root, files in os.walk('./' + g_folder):\n\t\tfor fileName, file_name in files:\n\t\t\tif fileName.endswith('.xml'):\n\t\t\t\tg_files.append(root"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tif xmls is None:\n\t\txmls = []\n\tdata = {}\n\tfor rootpath in xml.__dir__[:-1]:\n\t\tsource = _get_source_path(source, xmls, rootpath).rstrip('/')\n\t\txml_filename = _get_file_name_from_xml_path(rootpath, source)\n\t\tfiles = _file_list_from_xml_path(xml_filename)\n\t\tfor file_path in files:\n\t\t\tif (file_path is not None and file_path.startswith(os"}]}
{"task_id": "62b463153879012d1948149b", "prompt": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tprefix_re = re.compile(r\"[a-zA-Z][_0-9A-Za-z-]([a-z\\d]{2})][a-zA-Z0-9-_]\")\n\tmatches = prefix_re.findall( file_path + r'\\.[a-z\\d]{2}')\n\tif matches:\n\t\treturn True\n\treturn False\n\nimport _ from 'lodash';\nimport { isEventSource, isEventSource } from 'typeorm';\nimport {\n\tEvent,\n\tEventSource,\n\tEventSourceService,\n\t"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tif not file_path.startswith(prefix):\n\t\treturn False\n\telse:\n\t\treturn bool(file_path.endswith(prefix))\n\t\ndef match_path_by_prefix(prefix, path_path):\n\t\"\"\"\n\tIdentify if a `path_path` belongs to a document package along with the file_path\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tpath_path : str\n\t    Path to"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t# Avalia a partir da biblioteca do Documento\n\tfor i in os.scandir(prefix, os.IronPython):\n\t\tdoc_file = os.path.join(prefix,'documents',i)\n\t\tif os.path.isfile(doc_file):\n\t\t\treturn {'text', os.path.relpath(file_path,i)}\n\treturn {'not_found', path.join(prefix,file_path)}\n\n\n\n\n\n\n\n\n\n\n#--------------------------------------------------------------------------------------------------------\n\n\nclass Documento"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tif file_path == '' :\n\t\treturn True\n\tif prefix == '':\n\t\treturn False\n\t\n\treturn True\n\n//---------------------------------------------------------------------\n// <copyright file=\"EntityContainer.cs\" company=\"Microsoft\">\n//      Copyright (C) Microsoft Corporation. All rights reserved. See License.txt in the project root for license information.\n// </copyright>\n//---------------------------------------------------------------------\n\nnamespace Microsoft.OData.UriParser.Grammar\n{\n    #region Namespaces\n    using System;\n    using Microsoft.OData.UriParser;\n "}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t# Try to locate a match in the given file path\n\tfor prefix in prefix_to_package_names.keys():\n\t\tif prefix in doc.packages:\n\t\t\tif file_path == doc.files.package_dirs[prefix]:\n\t\t\t\t# Found the filename match, return that\n\t\t\t\treturn True\n\t\t\n\t# In the file_path is not in the pacote's package dir, or not found\n\treturn False\n\n###################################### Document API Definitions #\n\n# -----------------------------------------------------------------------------\n# Document API Definitions\n# -----------------------------------------------------------------------------\n\n@register_document(\"document-list"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t# Get basename of file\n\tfile_path_without_extension = get_file_path_without_extension(file_path)\n\tif len(file_path) == 0:\n\t\treturn False\n\t\n\t# Strip leading dot and get file name\n\tif prefix:\n\t\treturn get_file_name(prefix, file_path_without_extension.filename) == file_path_without_extension.filename\n\telse:\n\t\treturn True\n\n#!/usr/bin/env python\n# coding=utf-8\n\n\"\"\"\n    Tencent"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\treturn os.path.isfile(file_path)\n\ndef match_file_by_filename_and_content(file_path, content):\n\tmatch = True\n\tfor line in openfileutils.fopen(file_path, 'r'):\n\t\tmatch = line.strip() == content\n\tif match:\n\t\treturn True\n\treturn False\n\ndef match_file_by_filename(file_path, content):\n\tmatch = True\n\tfor line in openfileutils.fopen(file_path, 'r'):\n\t\tmatch = line."}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tp = os.path.abspath(prefix)\n\tis_dotted = os.path.isdir(file_path)\n\t\n\tp = p.split(os.sep)\n\t\n\tprefix_start = p[0]\n\t\n\tp = os.path.join(p[0], file_path.split(prefix)[:-1])\n\t\n\tp = p.split(os.sep)\n\t\n\tfor i, _ in enumerate(p):\n\t\tif not _ in prefix_start and os.path.isfile(p[i]):\n\t\t\t\t"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tfrom nltk.corpus import docmap\n\tfrom nltk.corpus.documents.file import File\n\t\n\tcontent_name = {'.txt': 'text', '.mp3': 'video', '.aac': 'audio', '.jpg': 'image', '.mp4': 'video', '.aac': 'audio'}[prefix.lower()]\n\t\n\tif re.search('[a-zA-Z]+', file_path.lower()):\n\t\twith open(file_path, 'r') as fh:\n\t\t\tdoc = []\n\t\t\t\n\t\t\tfor line in fh:"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\treturn False\n\npackage com.vladsch.flexmark.ext.scrite.converter;\n\npublic enum ScriteTagConstants {\n    // NOTE: if this constant has not been removed from public, the name must be considered as a tag\n\n    R1(\"a=text; r=1fr\"),\n\n    R2(\"'a'='text'\"),\n\n    R3('\"a\\\\\"text\"'),\n\n    R4(\"<b>a</b>\"),\n\n    R5(\"\\\"a'\\u21e5"}]}
{"task_id": "62b463153879012d1948149c", "prompt": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import os\n\tpass#IMPORTIMPORTIMPORT import glob\n\tpass#IMPORTIMPORTIMPORT import sys\n\tpass#IMPORTIMPORTIMPORT from sp_global_def import logger\n\t\n\t# Check prefix.\n\tif (not os.path.isfile(prefix) and not os.path.isdir(prefix)) or (not os.path.isfile(prefix + \".py\") and not os.path.isdir(prefix + \".py\")) or (not os.path.isfile(prefix + \".pyi"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\treturn filenames_path_by_prefix(files, prefix)\n\ndef select_filenames_by_filetype(filetype):\n\t\"\"\"\n\tGet filetype in filenames\n\t\n\tRetorna os arquivos da lista `filetype` cujos nomes iniciam com\n\t    `select_filenames_by_filetype` cujos nomes iniciam\n\t\n\tParameters\n\t----------\n\tfiletype : str\n\t    Filetype of a file\n\tReturns\n\t-------\n\tstr\n\t    filetype\n\t\"\"\"\n"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tresult = ['']\n\tfor fname in files:\n\t\t# Add separator and filename\n\t\tresult.append('')\n\t\tresult.extend([x.replace(os.sep, '') for x in list(glob.glob(os.sep + '/' + fname, recursive=True))] + [''])\n\treturn ','.join(result)\n\ndef get_package_id(packageName):\n\t\"\"\"\n\tGet file id of the document package (as number).\n\t\n\tParameters\n\t----------\n\tpackageName : str\n\t    Document package name\n\tReturns\n\t-------\n"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\treturn os.listdir(prefix, r'(?:[c:.][\\w\\\\.]+)')\n\ndef download_single_image(dirpath, outfilename):\n\tfrom scipy.io import save_npz\n\n\tif os.path.exists(dirpath) and not os.path.isfile(dirpath):\n\t\tos.remove(dirpath)\n\tif os.path.exists(outfilename):\n\t\tos.remove(outfilename)\n\n\tif isinstance(outfilename, basestring):\n\t\tprint(\"Downloading outdated archive with"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\treturn [f for f in files if re.match('^'+str(prefix),str(f),re.DOTALL)\n\t][0]\n\ndef is_uri_valid(uri):\n\t\n\t# En caso la URI valida da salva\n\tif(len(uri.encode('utf-8')) < 4):\n\t\treturn False\n\t\n\t# Esse URI valido que llega seguir com 0x00 || '/'\n\tif(len(uri) <=1 || uri[0]!= '/') return False\n\t\n\t# Esse URI valid"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tlist_of_files = [path_to_file(path_to_file, _prefix=r_prefix(_format_string))\n\t\t          for r_prefix in r_prefix_from_doc]\n\treturn list(map(set, zip(*list_of_files)))\n\ndef get_filenames(file_path, *file_patterns, filter=None, exclude_patterns=None):\n\t\"\"\"\n\tGet files of a document package.\n\t\n\tIf `include_patterns` is given as a list of glob pattern strings this function"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\treturn _prefix_list_by_path_match(files, prefix)\n\n\ndef select_files_for_package(packagefile, prefix):\n\t\"\"\"\n\tFunction to get file paths that contains a specific package.\n\t\n\tparameters\n\t----------\n\tpackagefile : file\n\t    Filename package.json.\n\tprefix : list\n\t        Filename prefix.\n\tReturns\n\t-------\n\tlist of path paths\n\t\"\"\"\n\treturn _prefix_list_by_path_match(packagefile.path.lstrip(os.path."}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tdoc_d = get_content_for_file(os.path.join(root_dir(), \"Documents\", prefix),\n\t\troot_dir(),\n\t\tfilter_file_list={\n\t\t\t\"*.csv\",\n\t\t\t*os.path.join(root_dir(), \"Graphs\", prefix, \"Graphs.txt\"),\n\t\t\t*os.path.join(root_dir(), \"Graph-Data\", prefix, \"Graph-Data.txt\"),\n\t\t})\n\tfiles = []\n\tfor f in files:\n\t\tdoc = doc_d"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tresult = [path for path in files if path.startswith(prefix)]\n\tif result == []:\n\t\tprint(\"ERROR: No matching files\")\n\t\treturn\n\telif len(result) == 1:\n\t\tprint(files[0])\n\telse:\n\t\tprint(\"Found \" + len(files) +'files matching to the prefix')\n\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\treturn list_package_path(prefix, files)\n\npackage com.github.alexthe666.weapi.entity.room;\n\n\nimport java.io.Serializable;\n\n/**\n * Created by Daniel Barnor on 14.11.2018\n */\npublic class Room implements Serializable {\n    /**\n     * Gets a String representation of this Room, which may be used to convert from JSON or YAML format to Java\n     *\n     * @return String representation\n     */\n    @Override\n  "}]}
{"task_id": "62b463153879012d1948149d", "prompt": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tdata = {}\n\tfor file in folder.split('.')[1:]:\n\t\tif not file:\n\t\t\tcontinue\n\t\tif _is_valid_xml_type_name(file):\n\t\t\tdata[file] = xml.etree.ElementTree.fromstring(file).attrib\n\t\telse:\n\t\t\tdata[file] = file\n\treturn data\n\n# ---------\n#\n# = PRIVATE METHODS =\n#\n#\n\n# -----------------------------------------------------------------------------\ndef _is_valid_xml_type_name(xml_str):\n\t\"\"\"\n\t"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfolder_map = {}\n\tfolder_list = list(folder_iter(f.gil()))\n\tcur = folder_list[0]\n\tnum = len(folder_list) / 2\n\tfor i in range(num):\n\t\tfor child in folder_list:\n\t\t\tif child[1].find(\"./\")!= -1:\n\t\t\t\tfolder_map.update({child[1]: i})\n\treturn folder_map, cur\n\ndef _split_files(folder):\n\t\"\"\"\n\tSplit files using a given directory and the next directory.\n"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tresult = {}\n\tfor fname, flist in folder.iteritems():\n\t\tif fn in flist:\n\t\t\tresult[fname] = flist[fn]\n\treturn result\n\t\ndef _parse_flist(filename):\n\t\"\"\"\n\tParse filename into python list\n\t\n\tParameters\n\t----------\n\tfilename : str\n\t    The file name to parse\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tresult = []\n\tif filename:\n\t\tfor line in open(filename):\n\t\t\tif fn in line.strip().lower"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfolders = []\n\tpaths = os.path.normpath(os.path.join(MelC.user_data, *os.path.dirname(folder)))\n\tfor filename in os.listdir(folders):\n\t\tif filename.endswith(\".xml\"):\n\t\t\tfilename = os.path.join(folders, filename)\n\t\tpackages = glob(\".{}__metac__/config/*.xml\".format(os.path.split(filename)[0]))\n\t\tfiles = []\n\t\tfor i in packages:\n\t\t\tif os.path.exists(os"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfiles = listDir(folder)\n\tfor dirname in files:\n\t\tif os.path.isdir(dirname):\n\t\t\txmlbasename = dirname\n\t\t\txmlbasename = json.dumps(xmlbasename)\t\t\t# type: ignore\n\t\t\txmlbasename = xmlbasename.encode('utf-8').decode('utf-8')\n\t\t\t# Read XML from a folder\n\t\t\txml = etree.parse(fn)\t\t\t\t\t# type: ignore\n\t\t\tif not xml.xpath(xmlbasename).get('data'):\n\t\t\t\tcontinue\n\t\t\tdata = json."}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tif os.path.isdir(folder):\n\t\t# no-op if sub-directory of folder is present\n\t\treturn\n\t\n\t# recursive call: directory tree\n\tfiles = os.listdir(folder)\n\tif _py3k:\n\t\tfiles = sorted(files, key=lambda x: os.path.splitext(x)[1][1:])\n\t\tfor file in files:\n\t\t\tfilepath = os.path.join(folder, os.path.split(file)[0])\n\t\t\tresult = get_data_from_"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfolder = folder.lower().replace(' ', '-')\n\tfiles = os.listdir(folder)\n\tdata = dict(zip(folder[:-4],\n\t\t[os.path.join(folder, f) for f in os.listdir(folder) if 'file-' + f[-4:]!= \"-\"]))\n\tdata[ folder[-4:]!= '-'] = []\n\tfor file in files:\n\t\tdata[ file.replace(' ', '-') ] = {\n\t\t\t'ext': 'html',\n\t\t\t'name': file.replace('.xml', '.xml')"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfolders_by_uri = {'file': folder, 'folder': folder}\n\tfiles_by_uri = folder.split('/')\n\t\t\n\tif not os.path.exists(folders_by_uri['file']):\n\t\treturn\n\tif os.path.exists(folders_by_uri['folder']):\n\t\treturn folders_by_uri['folder']\n\tif not (os.path.exists(folders_by_uri['folder_uri'])):\n\t\t# If folder_uri missing, but file_uri exists, return them\n\t\t# if needed"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\treturn pd.read_csv('./' + folder + '_data/data.json')\n\n\n#########################################################################################################################\n\ndef _extract_dirlist(directory):\n\t\"\"\"\n\tExtracts each file of directory into a list\n\t\"\"\"\n\tif not os.path.exists(directory):\n\t\tprint(\"[E] Unable to extract directory: \", directory)\n\t\tsys.exit(EXIT_FAILURE)\n\telse:\n\t\tdirs = os.listdir(directory)\n\t\treturn [os.path.basename(p) for p in dirs]\n\n"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfolder = os.path.dirname(os.path.realpath(folder))\n\tfiles = []\n\tfor file in os.listdir(folder + \"/data\"):\n\t\tfile = os.path.realpath(file)\n\t\troot = os.path.abspath(file)\n\t\tif root.startswith(\"__pycache__\"):\n\t\t\tbreak\n\t\t\n\t\tif file.endswith(\".xml\"):\n\t\t\tfiles.append(os.path.basename(os.path.splitext(file)[0]))\n\t\telif file.endswith(\".data\"):\n\t\t\tfiles."}]}
{"task_id": "62b463153879012d1948149f", "prompt": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\twith open(file_path, 'r') as f:\n\t\t# Para pacote esse arquivo XML e o endere\u00e7o do arquivo\n\t\tfiles = []\n\t\t# Favoro procurando a extens\u00e3o (o ficheiro deve a seguir para acessar o\n\t\t# ultima arquivo)\n\t\tfor line in f:\n\t\t\tif line == '..' or line.startswith(prefix):\n\t\t\t\tcontinue\n\t\t\tline = line.replace(prefix, '').decode()\n\t\t"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tglobal __debugging__\n\t__debugging__ = True\n\t\n\treturn assets.file_findfile('image', 'assets/' + archive + '/' + prefix +\n\t\t'package' + filename, file_path)\n\t\nfrom pyjamas.ui.editors.base_editor import BaseEditor\nfrom pyjamas.ui.editors.misc_editor import misc_editor\n\npackage io.swagger.v3.oas.models;\n\nimport javax.xml.bind.annotation.XmlAccessType;\nimport javax.xml.bind.annotation.XmlAccessor"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tprefix_rete : str\n\tfilename_reg_rete : str\n\tprefix_rete : str\n\tfile_folder_rec : str\n\tfiles : list(str)\n\t\n\tif len(file_path) == 0:\n\t\traise ValueError(\"Pobrator no pacote especificado existe para o tipo de arquivo, especificada em um arquivo.\")\n\tfile_path_rete : str\n\tprefix_rete : str\n\tfile_name_rete : str\n\tfilename : str\n\t"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tif not os.path.exists(file_path):\n\t\traise IOError('Directory does not exist.')\n\t\n\tasset, package = rar_read_package(file_path)\n\tif prefix in package:\n\t\treturn package[prefix], os.path.relpath(package[prefix], file_path)\n\telse:\n\t\tpackage[prefix] = prefix + os.path.sep\n\t\tfile_name = os.path.basename(file_path)\n\t\tbase = os.path.dirname(file_name)\n\t\tif base =="}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfolder_file_id = None\n\tfile_folder = None\n\tfor _match in ARTICLIA_FILE_IDENTIFIERS:\n\t\tfolder_file_id = _match.group(1).strip()\n\t\tfile_folder = _match.group(2)\n\t\n\tif prefix == folder_file_id:\n\t\tfile_name = filename\n\t\tfile_path = file_folder\n\telif prefix == file_name:\n\t\tfile_path = file_folder\n\telse:\n\t\tif filename:\n\t\t\tprint('Non"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import re\n\t\t\n\t# Checkar la estrutura\n\t# se debe contener arquivos\n\tfor i, tamata in enumerate(file_path.split(os.path.sep)):\n\t\tif(tamata.endswith(\".tex\")):\n\t\t\tname = tamata = name + extension + \".tex.xml\"\n\t\telse:\n\t\t\tname = tamata\n\n\t# Se o m\u00eas tipos da lista\n\ttry:\n\t\tfp = open(file_"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\twith open(file_path, 'r') as f:\n\t\tfile_contents = f.read()\n\t\t\n\tfile_contents = file_contents.replace(prefix, 'asset')\n\tfile_contents = file_contents.replace(prefix,'rendition')\n\n\tif len(file_contents) > 0:\n\t\tfile_contents = file_contents[:-len('package')]\n\t\t\n\tif is_package(prefix):\n\t\tfile_contents = unquote(file_contents)\n\t\t\n\tfile_contents = unquote(file_contents)\n\t"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\twith open(prefixed_filename(filename)) as file:\n\t\treturn _eval_file_xml(file_path, file)\n\t\n\t# Return dictionary, e formato com default:\n\tdef _eval_file_xml(filename, **kwargs):\n\t\t\"\"\"\n\t\tParameters\n\t\t\tfilename : str\n\t\t    nome do arquivo XML sem extens\u00e3o.\n\t\t\"\"\"\n\t\twith open(prefixed_filename(filename), 'rt') as f:\n\t\t\t# As caracteres ser\u00e3o firmas se aplica\u00e7\u00f5es"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tif not file_path: raise IOError(\"No file de arquivo!\")\n\t\n\tlog = Logger(filename)\n\tlog.info('Preparando pacote (this is only used to generate pacote ID')\n\tlog.info('Preparando pacote (the file extension should be png or a base64 encoded)')\n\t\n\txmltod = requests.post(\n\t\t\"https://api.v2ray.com/api/v2ray\",\n\t\tdata=request_body,\n\t\theaders=headers,\n\t\tverify=ssl"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfilename = prefix + \".csv\"\n\tfile_folder = prefix + \"_test\" + \".CSV\"\n\tfilename = filename + \".csv\"\n\t#print(filename)\n\t\n\tos.makedirs(file_folder)\n\t\n\twith open(filename, \"w\", encoding=\"utf-8\") as f, open(file_folder, \"w\",\n\t\tencoding=\"utf-8\") as f:\n\t\tcsv.writer(f)\n\tprint(file_folder)\n\t\n\twith open(filename, \"a\", encoding=\"utf-8\")as f"}]}
{"task_id": "62b463153879012d194814a1", "prompt": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\treturn (\n\t    self.add_page(\n\t        (lang, file_path.format(\"artigo02_en.pdf\"), \"artigo02\"),\n\t        filename=\"artigo02_en.pdf\"\n\t    )\n\t)\n\n\ndef add_rendition_with_language(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02_en.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tlang_base = lang.split(\"_\")[0]\n\tfull_file_path = file_path + \".full\"\n\tfile_name = \"{0} [{1}].{2}\".format(lang_base, lang.capitalize(), full_file_path)\n\tfor fn in self.resources_folders():\n\t    full_file = \"{0}/{1} {2}\".format(fn, file_name)\n\t    with open(full_file, \"rb\") as f:\n\t        data = f.read()\n\t  "}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tfor item in self:\n\t\tself[item].update(lang, file_path)\n\n// Code generated by protoc-gen-go-grpc. DO NOT EDIT.\n// Source: protocol.proto\n\npackage server_serviceb1\n\nimport (\n\tcontext \"context\"\n\tgrpc \"google.golang.org/grpc\"\n\tcodes \"google.golang.org/grpc/codes\"\n\tstatus \"google.golang.org/grpc/status\"\n)\n\n// This is a compile-time assertion to ensure that this generated file\n// is compatible with the grpc"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\timport urllib, urllib_error, urllib_parse\n\tfrom bs4 import BeautifulSoup\n\turl = \"\"\n\n\tif not hasattr(lang, 'base_url'):\n\t\ttry:\n\t\t\tlanguage_file = urllib_error.__file__.split('.')\n\t\texcept:\n\t\t\traise urllib_error, 'language file not found in url'\n\t\tlang.base_url = '/{0:#x}-{1}-{2}'.format(file_path,language_file,url)\n\tif hasattr(lang, 'is_utf8"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tif language == \"artigo02\":\n\t\treturn self._generate_html(add_rendition,lang)\n\telif language == \"artigo03\":\n\t\treturn self._generate_html(add_rendition,lang,\"en\")\n\texcept:\n\t\tpass # we shouldn't try to generate html, but if it fails we'll stop here\n\"\"\"\n\npackage me.badbones69.crazyenchantments.entity.item;\n\nimport io.crazypicks.Span;\nimport me.badbones69.crazyenchantments.Crazy"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\twith codecs.open(file_path, encoding='utf-8') as stream:\n\t\tstream.write(self.render_header(lang))\n\t\tfor line in stream:\n\t\t\tself.render_render(line)\n\twith codecs.open(os.path.join(os.path.abspath(os.path.dirname(__file__)),'data','artigo02-en.{}.pdf'),encoding='utf-8') as stream:\n\t\tstream.write(self.render_data(lang))\n\twith codecs.open(os.path."}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tdoc = self.load_doc_from_file(file_path, lang)\n\tdoc = self.get_content2_from_doc(file_path, lang, doc)\n\tres = self.to_pdf(doc)\n\tself.import_to_pdf(file_path, \"en\")\n\tprint(self.doc_to_pdf(file_path, lang))\n\tself.write_file(lang, file_path, \"en\", [res])\n\n\t# -------------------------------------------\n\t# add a RDF to a Doc\n\t# --------------------------------"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tdoc = Artigo02_Django(lang, language=\"en\", file_path=file_path)\n\tdoc.render(preserve_texm=0)\n\n\n@pytest.fixture\ndef wandb_wandb_de(de_path):\n\t\"\"\"\n\t\t\"de_path\": {\n\t\t\t\"path\": \"/tmp/foo\"\n\t\t}\n\t\"\"\"\n\tdoc = DebianWandb(language=\"de\", file_path=de_path)\n\tdoc.render(preserve_texm=0)\n\n/*"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tfor extension in rdflib.extensions:\n\t\tif extension.startswith(\"rend\"):\n\t\t\trdstr = extension\n\t\telse:\n\t\t\trdstr = extension.extension + \".\" + extension.fullbasename\n\n\treturn rdflib.URI(rdstr)\n\n\n@default_export\nclass Svg:\n\t\"\"\"\n\t{\n\t\t\"file_id\": \"artigo\",\n\t\t\"width\": 1,\n\t\t\"height\": 1,\n\t\t\"width_ratio\": 1,\n\t\t\"height_ratio\": 1,\n\t"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t\tself.setPageReference('R2F1')\n\t\tself.run_command(lang, ('r2f', 'addrendition', file_path))\n\t\n\n# eof\n\nimport { Component } from './component';\n\nexport default Component;\n\n#include \"casm/util/misc.h\"\n#include \"casm/global/global.hh\"\n\n#include \"casm/test/integration.hh\"\n\n#include \"casm/core/macros.hh\"\n\n#ifdef HAS_X64\n#include \"casm/"}]}
{"task_id": "62b463163879012d194814a2", "prompt": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tif os.path.exists(\"/path/artigo02-gf03.tiff\"):\n\t\tself.add_file_from(basename, \"/path/artigo02-gf03.tiff\")\n\t\tif os.path.exists(\"/path/artigo02-gf03.jpeg\"):\n\t\t\tself.add_file_from(basename, \"/path/artigo02-gf03.jpeg\")\n\t\t\tif os.path.exists(\"/path/artigo02-gf03.png\"):\n\t\t\t\tself.add_file_from(basename, \"/path/artigo"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tif isinstance(basename, basestring):\n\t\tbasename = basename.replace(\"/\",\"\")\n\tif not filename.endswith(\".\"+basename):\n\t\tfilename += \".\"+basename\n\tfname = filename.replace(\"artigo02-gf03.tif\", \"artigo02-gf03.jpg\")\n\n\t# Read the image.\n\timg, err = load_image(fname)\n\tif err!=0: continue\n\n\t# Grab the width and height of the image.\n\tw, h = img.width, img.height\n\tif w==0 or h=="}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tself._extract(basename, file_path)\n\n\ndef main(argv=None):\n\textract_archive(\n\t\targv=argv,\n\t\troot_directory='./',\n\t\toutput_name='image.tiff',\n\t\toutput_extension='.tiff',\n\t)\n\n\nif __name__=='__main__':\n\tmain()\n\npackage com.hartwig.hmftools.vicc.datamodel.generaldev;\n\nimport java.util.List;\n\nimport com.hartwig.hmftools.gson"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\twith open(file_path, 'rb') as file_object:\n\t\tbytes_data = b''.join([line.decode('utf-8') for line in file_object.read().decode('utf-8')])\n\tasset_type = 'artigo02-gf03'\n\t# print asset_type\n\treturn Asset.objects.filter(basename=basename, archive_type=asset_type).count()\n\n\ndef file_add2_asset(self, fname, filepath, type, filename):\n\t\"\"\"\n\t{\n\t\t\"artigo02"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tassert \"artigo02-gf03\" not in self.artigo02_asset_map\n\tartigo02_asset_map[\"artigo02-gf03\"] = self.artigo02_asset_dict\n\tself.artigo02_asset_dict[\"artigo02-gf03\"] = basename, file_path\n\n\nclass FusionDataset(dataset):\n\t\"\"\"\n\tDataset data set that stores Fusion images.\n\t\"\"\"\n\tdef __init__(self,\n\t             dataset_type=dataset.FUSION_"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t#print(self.path)\n\t#print(basename)\n\t#print(file_path)\n\twith codecs.open(file_path, \"r\", \"utf-8\") as f:\n\t\tfile_content = codecs.charmap_open(file_path, \"r\").read()\n\t\tfile_content = self.apply_pre_parsing(file_content, basename)\n\t\treturn_content = f.read()\n\t\treturn_string = self.apply_pre_parsing(file_content)\n\t\treturn_string = self."}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tif os.name!= sys.platform:\n\t\text = os.path.splitext(basename)[1]\n\telse:\n\t\text = basename\n\tnew_basename = self.basename % ext\n\tif self.artigo02_asset_filename:\n\t\tasset_path = self.artigo02_asset_filename % new_basename\n\t\tif os.path.exists(asset_path):\n\t\t\treturn self.add_asset(new_basename, asset_path)\nelse:\n\treturn(None)\n\n#include <iostream>\n"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tf = open(file_path, 'rb')\n\n\tf.seek(0)\n\tf.truncate()\n\tf.write(b''.join([\n\t    b'GIF89a',  # compression - zlib\n\t    b'X',  # size\n\t    b'J',  # rotation\n\t    b'C',  # padding, so far - '\n\t\tb'K',\n\t\tb'-',\n\t\tb'0',  # typecode\n\t\tb'i',  # typecode of\n\t"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t# This is how you can add files to your Tiles\n\t# See https://github.com/thiben-vision/tiled_gf/blob/master/examples/test_assets_with_additions.py\n\tif len(basename) not in [0, 2]:\n\t\traise ValueError(\"Error: path-element must be a single character\")\n\tself.f.create(tiled_file_content=self.f.read_tile_file(file_path).convert(\"GTiff\"))\n\n\ndef add_asset_to_database(asset):"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tif not os.path.isfile(basename):\n\t\tprint(\"Filename is not found\")\n\t\treturn\n\telse:\n\t\timport tarfile\n\t\ttar = tarfile.open(basename, mode=\"r:gz\")\n\t\ttar.extractall(file_path)\n\n/** Generated by YAKINDU Statechart Tools code generator. */\npackage cn.edu.metrol.office.index;\n\nimport org.apache.commons.lang3.tuple.ImmutablePair;\n\nimport com.yakindu.solidity.index.Solidity"}]}
{"task_id": "62b463163879012d194814a4", "prompt": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tif 'zip' in zip_path:\n\t\tlog.debug(zip_path)\n\t\tzip_path = os.path.join(zip_path, '__%__' % os.path.basename(zip_path))\n\t\tlog.debug('%s', zip_path)\n\tif 'path' in zip_path:\n\t\tlog.debug('%s', zip_path)\n\t\tzip_path = os.path.join(zip_path, \"___%__\" % os.path.basename(zip_path))\n\tif 'package'"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\twith ZipFile(zip_path) as zip_data:\n\t\tall_dict = {}\n\t\tfor obj in zip_data.infolist():\n\t\t\tobj.print_data()\n\t\t\tall_dict.update(obj.pkl.data)\n\t\tfor f in ZipFile('.'+zip_path+\".xml\").infolist():\n\t\t\tf.close()\n\t\t\tzip_data.close()\n\t\treturn all_dict\n\nclass CheckZip(object):\n\tdef __str__(self):\n\t\treturn self.msg\n"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tzip = ZIP.open('zipfile')\n\tdata_json = {}\n\tentries = zip.entries((zip_path, 0))\n\tfor entry_name in entries.keys():\n\t\tzentryName = entry_name.split('.').pop() # remove suffix of.xml file name\n\t\tzip_entryName = '_json_' + zentryName + '.' + basename(entry_name)\n\t\tsplit_data = zip.read(zip_entryName)\n\t\tdata_json[split_data['name']] = zip_entryName\n\treturn data_"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\twith zipfile.ZipFile(zip_path, 'r') as zf:\n\t\ttry:\n\t\t\tz.read(zf, 'binary', universal_file=True, password=None)\n\t\texcept TypeError:\n\t\t\tz.extractall()\n\t\telse:\n\t\t\treturn zf.getinfo(zpy.compressobj(zlib.Z_DEFAULT_COMPRESSION, 0))\n\t\telse:\n\t\t\traise KeyError\n\n<?php\n/**\n * @package     Joomla.Component.Builder\n *\n * @created "}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tdata = dict()\n\ttree = bz2.BZ2File(zip_path, \"r\")\n\tif len(tree.infolist()) == 0:\n\t\treturn data\n\t\t\n\tfor sname, treeinfo in tree.infolist():\n\t\ttreename = sname.rsplit(os.sep, 1)[-1]\n\t\tdata[sname] = treeinfo[\"entries\"]\n\treturn data\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tdata = {}\n\tzip = ZipFile(zip_path)\n\twith zipfile.ZipFile(zip, 'r') as zip_file:\n\t\tfor zipname in os.listdir(zip_path):\n\t\t\tfilename = os.path.join(zip_path, zipname)\n\t\t\tfile = zipfilebasename(filename)\n\t\t\tdata[zipname] = get_package_from_file(zip, file)\n\t\tzip.close()\n\treturn data\n\n\n\"\"\"\n\tExplore a zip file\n\t\n\tReturns\n\t---------\n"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tdata = {}\n\tzip_dir = os.path.dirname(zip_path)\n\tdef _explore_zipfile(dir_path):\n\t\tzip_path_split = dir_path.split(zip_dir, 1)\n\t\tzip_path_split.sort()\n\t\tfor i, dir_name in enumerate(zip_path_split):\n\t\t\tzip_path_split_split = dir_name.split(os.path.sep)\n\t\t\tzip_type = zip_path_split_split[0]\n\t\t\tif zip"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\treturn zip_file_reader(\"test_1.zip\", ZIP_HEADER+zip_path)\n\nadd_file_to_group(\"explore\", \"1.zip\", _explore)[:]\nadd_file_to_group(\"explore\", \"2.zip\", _explore_zipfile)[:]\nadd_file_to_group(\"explore\", \"3.zip\", _explore_zip_zip)[:]\n\nadd_file_to_group(\"explore\", \"1.zip2\", _explore_zip2)[:]\nadd_file_to"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\timport zipfile\n\tzip = zipfile.ZipFile(zip_path, 'r')\n\tzip.extractall(output_type='zip')\n\tfor zipfile_file in zip.glob('*.xml.bz2'):\n\t\twith zipfile.ZipFile(zip_file, 'r') as zipin:\n\t\t\tzip.extractall(output_type='zip')\n\tzip.close()\n\tzip.cleanup()\n\treturn zip\n\n\ndef download_zip(zip_path):\n\tfrom zipfile import ZipFile\n\tzip = ZipFile(zip_path)"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tlog.info(\"Finding data from zip file: %s\", zip_path)\n\tzip_path = os.path.expanduser(zip_path)\n\t# Get all zip files in the zipfile directory\n\tfiles = os.listdir(zip_path)\n\t# Sort by basename\n\tfiles.sort()\n\t# Get all files in the zipfile directory\n\tfiles.sort(by=\"filename\")\n\t# Get all files in the downloaded zip file\n\tfiles.sort(by=[\"filename\"])\n\t\n\tif len(files) == 0:\n\t\treturn {}, [],"}]}
{"task_id": "62b463163879012d194814a6", "prompt": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\timport StringIO\n\tfrom zipfile import ZipArchive\n\tfiles_to_process = []\n\twith ZipFile(zip_path, \"r\") as zip_info:\n\t\tfor zip in zip_info.infolist():\n\t\t\tfor f in files_from_zipfile(zip[\"filepath\"]):\n\t\t\t\tfiles_to_process.append(zip[\"filepath\"])\n\t\t\tfiles_to_process.append(zip[\"filepath\"].decode(encoding=\"utf-8\"))\n\treturn file(len(files_to_process) * [\"%s.gz\" %"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t__tracebackhide__ = True  # Hide traceback for py2exe\n\tif platform.system() == \"Windows\":\n\t\ttry:\n\t\t\tzip_path = zip_path.decode()\n\t\texcept:\n\t\t\tzip_path = \"\"  # avoid errors on unicode systems\n\t\twith zipfile.ZipFile(zip_path, 'r') as handle:\n\t\t\thandle.extractall(zip_path)\n\telif platform.system() == \"Darwin\":\n\t\tos_path_list = os.path.split(__file__)\n\t\tif"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\ttry:\n\t\tzip_file = zipfile.ZipFile(zip_path, \"r\")\n\t\treturn [zip_file.read(name) for name in list(zip_file.infolist())]\n\texcept FileNotFoundError as exc:\n        logger.error(\"Unable to find `zip_path` from zip file '%s' %s\" % (zip_path, \", \".join([\"%s=%r\" % (k, v) for (k,v) in exc])))\n\texcept zipfile.BadZipFile as exc:\n "}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t# os.chdir(path)\n\t\n\t# extract the data\n\tzip_path = os.path.normpath(os.path.dirname(zip_path))\n\n\tzip_all_files = []\n\tzip_all_files = zip_list_to_zip_files(zip_path)\n\n\tzip_list_all_files = []\n\tzip_list_all_files = list_folders(zip_path)\n\n\tfor zip_path in zip_list_all_files:\n\t\t\n\t\tzip_list_all_files.append"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\timport fnmatch\n\tfrom zipfile import EntryNotFoundError\n\tfrom tempfile import NamedTemporaryFile\n\t# The zip_path is the root of this zip, so we can use os.path.join to find it.\n\trootdir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\ttry:\n\t\tzip = None\n\t\ttry:\n\t\t\tarchive_type = 'zip'\n\t\t\t# get the zip_path for a zip file\n\t\t\tfile = open(zip_path, 'rb')\n\t\t\tarc_"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t#print(f\"# extracting {zip_path} out of the filelist and checking it.\")\n\tif not zipfile.is_zipfile(zip_path):\n\t\ttry:\n\t\t\textract_archive(zip_path)\n\t\texcept:\n\t\t\traise ExtractorError('Error extracting {}'.format(zip_path))\n\treturn [path for path in list(findall(os.path.split(zip_path))[0]) if os.path.split(path)[-1].endswith('.zip')]\n\n@python_2_unicode_compatible\ndef"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tif not os.path.isdir(zip_path):\n\t\twith open(zip_path, 'rb') as f:\n\t\t\treturn os.path.join(\n\t\t\t\tos.path.dirname(os.path.realpath(__file__)),\n\t\t\t\tos.path.splitext(zip_path)[0].lower()\n\t\t\t)\n\tf = open(zip_path, 'rb')\n\tfile_list = [x.split(os.sep) for x in f if x]\n\tfiles_list = [\n\t\tos.path."}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tfp = None\n\tif zipfile:\n\t\ttry:\n\t\t\tfp = zipfile.ZipFile(zip_path, 'r')\n\t\texcept OSError:\n\t\t\tprint(\"Could not open zip file at {0} - you probably wanted to use `zip -q`.\".format(zip_path))\n\t\telse:\n\t\t\tfor root, dirs, files in os.walk(zip_path):\n\t\t\t\tfor f in files:\n\t\t\t\t\tif os.path.isdir(f):\n\t\t\t\t\t\t#print dir,file\n\t\t\t\t\t\tif os.path"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\timport hashlib\n\tfrom os.path import join\n\timport urllib\n\ttry:\n\t\tzip_path = urllib.urlopen(zip_path)\n\texcept:\n\t\traise IOError(\"Failed to open zip file path.\")\n\tzip_list = []\n\tdata = zip_path.read()\n\tfor line in data.splitlines():\n\t\tif line.startswith(\">\"):\n\t\t\tcontinue\n\t\tfor segment in line.split(' '):\n\t\t\tsplit_segment = segment.strip().split(' ')\n\t\t\t# Remove unnecessary data\n"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\ttry:\n\t\tos.listdir(zip_path)\n\texcept:\n\t\tprint('Not a valid zip directory! Try one of these: '.join(\n\t\t\tzip_path))\n\t\tsys.exit(1)\n\tfor i, x in enumerate(zip_path):\n\t\tif x.endswith('.pyz'):\n\t\t\tzip_path.insert(i+1, 'examples/'+x)\n\t\tif x.endswith('.zip'):\n\t\t\tzip_path.insert(i+1, 'examples/examples/')\n\treturn"}]}
{"task_id": "62b4631b3879012d194814dd", "prompt": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tfrom pprint import pprint\n\tif content!= \"\":\n\t\tpprint(content.replace('w:st','w-st'))\n\ndef fix_text(content):\n\t\treturn (pprint((\"  <w:p>\", content)) + pprint((\"  <w:p>\", '<w:p>') + pprint((\",\".join(c1 for c1 in content.replace('w:p=', '') if c1 not in (\"<>\").join(c2).replace('>', '').replace('/>', '>)'))) + pprint((\",\".join(c1"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\treturn '\\n'.join([w for w in content if w=='\\\\textmath-')\n\n# -*- coding: utf-8 -*-\n#\n# This file is part of Glances.\n#\n# Copyright (C) 2020 Nicolargo <nicolas@nicolargo.com>\n#\n# Glances is free software; you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tdef fix_namespace_prefix(prefix):\n\t\t# Prefix should be either 'w' or 'w-'\n\t\tif prefix == 'w' or prefix == 'w-':\n\t\t\tprefix = 'w'\n\t\treturn prefix + content.lower().strip() + content.lower()\n\tfor prefix, content in FixNamespace(content).value.iteritems():\n\t\t# Convert to prefix\n\t\tcontent = fix_namespace_prefix(content)\n\t\t# Check for duplicate prefixes in prefix and fix-space\n\t\tif not prefix in content and not content"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t_default_content = to_unicode(content)\n\tif _default_content.find(t)!= -1:\n\t\treturn _default_content.split()[-1]\n\treturn _default_content.split(':')[0]\n\n\ndef fix_no_namespace_prefix(text):\n\treturn unichr(text).lower()\n\n\n# https://github.com/python-dev/python-3.5/blob/developing/test/unit/tests.py#L19\n@unittest.skipUnless(HAVE_EXPAT_NS,"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\timport re\n\tcontent = re.sub('{^[.]*[.]*}\\s*', '\\1:\\2', text=content)\n\treturn content\n\n# frozen_string_literal: true\n\n# See: http://rubydoc.info/gems/xdebug-rspec-rspec/RSpec/XdebugSpec/Version.rspec\n\nGem::Specification.new do |spec|\n  spec.name          = 'xdebug_rspec'\n  spec.version       = Xdebug::Parser::"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tfrom twisted.web.css import fix_namespace_prefix\n\tif not content:\n\t\treturn None\n\tfrom w3af.core import BaseCSS\n\ts = BaseCSS.NamespacePrefix(fix_namespace_prefix)\n\ts.convert(content)\n\treturn s.to_text()\n\t\ndef convert_text(text):\n\n\tif not text:\n\t\treturn text\n\n\tmatch_begin = r' \\s*\\n'\n\tmatch_middle = r'\\s*/'\n\tmatch_end = r' '\n\ttext_"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tfrom pywikibot import fixNamespacePrefixW\n\treturn fixNamespacePrefixW(content, 'w')\n\n@fixNamespacePrefixW\ndef fix_namespace_w(content, prefix='w'):\n\"\"\"\n\tConvert text that defaults to 'w:' to 'w-st:'\n\t\"\"\"\n\tfrom pywikibot import fixNamespaceW\n\treturn fixNamespaceW(content, prefix)\n\n\ndef escape(str):\n\t\"\"\"\n\t\tThis method is not the same as the Python's escape()\n\t\texcept that it wraps to the platform-specific unicode()\n"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t# The following code fixes the following error in Python 3.\n\t__author__ = 'Scott A. Dillard of Python 3.0'\n\t__credits__ = ['Scott A. Dillard of Python 3.0'] # [s]\n\t__license__ = 'MIT'\n\t__version__ = '1.0.8' # [s]\n\t__author_email__ ='support@python.org'\n\t__credits_email__ = None # ['Scott a. Dillard of Python 3.0', 'Scott A. Dill"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t# (1) Convert\n\tcontent=str(re.sub('(?:\\S+)', 'w:st\\\\s*=\\s*'\", content))\n\t# (2) Add namespace prefix if needed\n\tif '.' not in content:\n\t\tif content[:3] == 'w':\n\t\t\tcontent=('w:st=' + content[3:])\n\treturn content.lstrip('st')\n\n\n# -----------------------------------------------------------------------------\n#                                  "}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tdef fix_prefix_w(item):\n\t\treturn item.text[item.start:item.end]\n\treturn content.replace('xmlns:', fix_namespace_prefix_w)\n\ndef fix_inaccessible_w(content):\n\treturn content.replace('<!--{0}{n}/*'.format(w_tagname), '')\n\nclass W_tag_writer(ast.W_node_writer):\n\tpass\n\nclass X_tag_writer(object):\n\t\"\"\"\n\tFor X_tag_writer.write_node().\n\t\"\"\"\n\n"}]}
{"task_id": "62b463283879012d1948153d", "prompt": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tif not node.xpath(pubdate_xpaths[0], 'pubdate'):\n\t\treturn pubdate_xpaths[0]\n\telse:\n\t\treturn pubdate_xpaths[0][0]\n\n\ndef match_publish_date(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tif not node.xpath(pubdate_xpaths[0], 'publish-date'):\n\t\treturn pubdate_xpaths[0]\n\telse:\n\t\treturn pubdate"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tpub_date = pubdate_xpaths[0][1]\n\tfor path in pub_date:\n\t\tif node not in pub_date:\n\t\t\tpub_date.append(path)\n\t\ttry:\n\t\t\tpub_date[node] = int(path)\n\t\texcept Exception:\n\t\t\treturn False, None\n\treturn True, pub_date[0]\n\n\ndef filter_pubdate(node, pubdate_xpaths):\n\t#print(repr(pubdate_xpaths))\n\tfor pubdate_path in pubdate"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tif len(pubdate_xpaths) == 0:\n\t\treturn None\n\treturn match_node_node(node, \"pubdate\")\n\ndef match_date(node, date_xpaths):\n\t\"\"\"\n\t\treturns the first match in the date_xpaths list\n\t\"\"\"\n\tif len(date_xpaths) == 0:\n\t\treturn None\n\treturn match_node_node(node, \"date\")\n\n## -----------------------------\ndef read_doc(filename, level=0):\n\t\"\"\"\n\t\tRead the XML document from a file"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tnode = asTree(node)\n\tm = _match_pubdate_pre(node.root, pubdate_xpaths)\n\treturn m\n\ndef match_pubdate_pre(node, pubdate_xpaths):\n\t\"\"\"\n\t\tCreates match between pubdate_xpaths and xpaths, recursively.\n\t\"\"\"\n\tm = _match_pubdate(node, pubdate_xpaths)\n\treturn m\n\ndef _match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\n\t\tMatches pubdate_xpaths against pub"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tm = _match_pubdate(node)\n\t_sort_pubdate_xpaths(pubdate_xpaths)\n\treturn m\n\ndef _sort_pubdate_xpaths(xpaths):\n\t\"\"\"\n\tSorts the pubdate_xpaths list by order in descending order\n\t\"\"\"\n\txpaths = list(reversed(xpaths))\n\tnew_xpaths = []\n\tfor dir in xpaths:\n\t\tif dir == 'root':\n\t\t\tcontinue\n\t\txpaths.append(dir)\n\tif order == 'descending'"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tmatch = node.find(\"pubdate\", pubdate_xpaths=pubdate_xpaths)\n\treturn match\n\ndef match_source_id(node, source_xpaths):\n\"\"\"\n\tReturns the first match in the source_id_xpaths list\n\t\"\"\"\n\tmatch = node.find(\"content_details\", source_id_xpaths=source_xpaths)\n\treturn match\n\n/*=========================================================================\n *\n *  Copyright Insight Software Consortium\n *\n *  Licensed under the Apache License, Version 2.0 (the \"License\");\n * "}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfirst = 0\n\tfor node in pubdate_xpaths:\n\t\tif node.tag == 'pubdate':\n\t\t\tif node.text == pubdate_xpaths[first]:\n\t\t\t\tfirst = (first+1) % pubdate_xpaths.count(node)\n\t\t\t\tnode.remove()\n\t\t\t\tmatch = True\n\treturn match\n\n\ndef get_publisher_pubdate_patterns():\n\t\"\"\"\n\tReturns publisher pattern by publisher\n\t\"\"\"\n\tpublisher_patterns = []\n\tpubdate_ypath = re.compile('^(?:Pub"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tstart = match_token_xpaths(node, pubdate_xpaths)\n\tfor i in start:\n\t\ttoken = match_token(node, i)\n\t\t# return the first match\n\t\tif token:\n\t\t\treturn i, [token]\n\t\ndef matches_match(node, match_xpaths):\n\tif match_xpaths:\n\t\tfor r in node:\n\t\t\tfor i, j in match_pubdate_xpaths(r):\n\t\t\t\tif matches_match(j, match_xpaths):\n\t\t\t\t\treturn"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t# get first match\n\treturn pubdate.match(node, pubdate_xpaths)\n\n\ndef read_date(node, date_xpaths, datetime_format):\n\t\"\"\"\n\t@param\tnode: the NPM instance\n\t@param\tdate_xpaths: the xpaths\n\t@param datetime_format: the format used to parse the date\n\t@rtype: str\n\t\"\"\"\n\txpaths = (date_xpath_from_xml(node) or [])\n\txpaths = [(x.format(datetime_format)):x for x in xpaths"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor i in range(0, len(pubdate_xpaths)):\n\t\tif pubdate_xpaths[i].astext() and pubdate_xpaths[i].astext().startswith('pubdate'):\n\t\t\tcontinue  # Skip it in case it might not get a match\n\t\t\treturn pubdate_xpaths[i]\n\ndef process_nupic(nupic_xpaths):\n\t\"\"\"\n\t\tProcesses the nupic from the.nupic directory and sub-directories and\n\t\tif anything matches it goes to the.nupic"}]}
{"task_id": "62b463303879012d19481579", "prompt": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t'''\n\tTODO: this function need to be changed and the subtypes can be re-implemented\n\tas this is already an internal function.  To be independant, the function only\n\taccepts a single element.\n\n\tAlso note that, after extraction, the type can be changed to be a base class.\n\n\t'''\n\n\tif not hasattr(issue, 'type'):\n\t\tissue = issue.element\n\n\tnumber, suppl = None, None\n\n\tif isinstance(issue, Issue):\n\t\tnumber = issue.number\n\t\tsuppl = issue"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumber = sum(issue.number_items)\n\tsuppl = sum(issue.suppl_items)\n\n\treturn (number, suppl)\n\ndef _extract_issue_and_suppl_item_list(item_list):\n\t\"\"\"\n\t\tExtract the possible values of items from the contents of the issue and\n\t\tthe suppliers.\n\t\"\"\"\n\tnumber_and_suppl_items = [item.supplier_number\n\t\t\t\t\t\t\tand item.supplier_name for item in item_list]\n\n\treturn number_and_"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tdef extract_number_and_suppl(number_from_number_type,\n\t                             suppl_from_number_type, element) -> str:\n\t\tif (not isinstance(number_from_number_type, (int, float))\n\t\t    and not isinstance(suppl_from_number_type, (int, float))):\n\t\t\treturn False\n\t\telse:\n\t\t\tnumber = str(number_from_number_"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tfor number_id_dict_key in issue.get_number_dict():\n\t\tif number_id_dict_key['id_ref_to_number']:\n\t\t\ttry:\n\t\t\t\tnumber_id_dict_key['number'] = num_decimal.Number(Number(number_id_dict_key['number'], min_value=0))\n\t\t\texcept Exception as e:\n\t\t\t\tlog.error(e)\n\t\t\t\tcontinue\n\t\t\tbreak\n\telse:\n\t\tlog.debug(\"did not find numeric issue in issue\")\n\tif issue"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumber = get_attribute(issue, 'number', None)\n\tsuppl = get_attribute(issue,'supposition', None)\n\tnumber_and_supplment = NumberAndSupplment(number, suppl)\n\treturn {'number_and_supplment': number_and_supplment}\n\n\ndef _extract_textual_data_from_data_type_issue(issue):\n\t\"\"\"\n\tExtract possible texts related a data type issue from the data set of issue\n\t\"\"\"\n\ttext_types_and_subtypes_not_"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnum, suppl = 1, None\n\tif issue.number is not None:\n\t\tnum = issue.number\n\tif issue.suppl is not None:\n\t\tsuppl = issue.suppl\n\tif num is not None and suppl is not None:\n\t\treturn 0, num * 1000 + suppl / 1000\n\treturn 0, 1\n\n@num_and_suppl = _extract_number_and_suppl_from_issue_element\ndef _calc_average_rate(number, suppl, num = 1, multiplier = 1):"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tif not isinstance(issue, issue_info.IssueElement): return\n\tnumber = []\n\tfor element in issue.get_content_list():\n\t\ttry:\n\t\t\tnumber.append([int(v) if v!= \"0\" else v for v in element.get_content()])\n\t\texcept ValueError:\n\t\t\tlogger.debug(f'{issue.get_id()}: {element.get_content_type()} number', id = issue.get_id())\n\treturn (len(number), len(number), len(number))\n\n\ndef _get"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t# Do we have a problem with these?\n\ttry:\n\t\tnumber_and_suppl\t= issue.string_value('issue-num-and-sup')\n\texcept:\n\t\tnumber_and_suppl\t= None\n\t# Set values\n\tobj = issue.issue_element()\n\tif obj is None:\n\t\tobj = issue.number_and_suplement()\n\tif obj is None:\n\t\tobj = issue.suppl_number_and_suplement()\n\tif obj is None:\n\t\treturn\n\tobj"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumber_or_supp = []\n\tnum_suppl = '' #the number\n\twhile'' in issue.value):\n\t\tnumber_or_supp.append(num_suppl)\n\t\tnum_suppl = issue.value.strip() #the string with an open double double '}'\n\t\ttry:\n\t\t\tsuppl_number = float(num_suppl)\n\t\t\tsuppl_number = int(suppl_number)\n\t\texcept:\n\t\t\tpass\n\t\tif suppl_number < 0:\n\t\t\tsuppl_"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t#extracting_values = issue.find(\n\t#\t\t\".//span[@class='issue-number__suppl']\")\n\t#if extracting_values is not None:\n\t#\tif extracting_values.find(\n\t#\t\t\".//span[@class='issue-number__suppl']\") is not None:\n\t#\t\tif extracting_values.find(\n\t#\t\t\t\".//span[@class='issue-number__suppl']\") == 0 and \\\n\t#\t\t\textracting_values.find(\n\t#\t\t\t"}]}
{"task_id": "62b46740d2f69a53b466171a", "prompt": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tif debug: return pretty(self, False)\n\tif not isinstance(self, self.__class__):\n\t\t# Can't convert self to a type\n\t\treturn str(self)\n\tif indent >= 0:\n\t\treturn indent +''+ pretty(self, True)\n\treturn pretty(self, debug)\n\ndef print(obj):\n\tif debug: sys.stdout.write(' '.join([type.__name__, str(obj)]))\n\ndef show_traceback_data(function_name, start_line,\n     "}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tif debug: d = PrettyFormatter()\n\ts = \"\"\n\tif self.is_tuple():\n\t\ts = \"[]\\n\"\n\telse:\n\t\ts = \",\"\n\n\tfor v in self.fields:\n\t\ts += pretty(indent + \"\", debug)\n\t\tfor h in self.fields[v]:\n\t\t\ts += pretty(indent + \"\", debug)\n\t\tif debug:\n\t\t\ts += \">\\n\"\n\n\t\ts += s[0] + indent + '{' + pretty(h.type, debug) + ':'\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tif not isinstance(self.data, list):\n\t\tself.data = [self.data]\n\tself.debug = debug\n\tx = ''\n\tfor i, d in enumerate(self.data):\n\t\tif self.debug: sys.stderr.write(\"%d: %s\" % (i+1, d))\n\t\tif d!= '':\n\t\t  x += self.write(' ', indent)\n\t\tif isinstance(d, (list, tuple)):\n\t\t  while len(x) < indent:  # if it lands on an optional element\n\t\t"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tif type(self) == type(None):\n\t\treturn 'None'\n\tif isinstance(self, datetime.date):\n\t\treturn \"%04d-%02d-%02d %02d:%02d:%02d\" % \\\n\t\t    (self.year, self.month, self.day, self.hour, self.minute, self.second,)\n\tif isinstance(self, datetime.time):\n\t\tif type(self) == datetime.datetime:\n\t\t\treturn \"%04d-%02d-%02d %02d:%02d:%02d"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tif debug:\n\t\tif indent <= 0:\n\t\t\tindent = \"  \";\n\t#print out the formated fields for the given tree\n\t#print indentation\n#\tprint self.fields\n\t#print indent\n\treturn \"%%%%.%s%s\" % (self.indent*indent, self.fields)\n\nprint(pretty(indent=8, debug=False))\n\n//#############################################################################\n//#                                    "}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t# Check that we have something like (a is now a single thing), and return a\n\t# pretty format.\n\tif not self.is_string_defined():\n\t\tif debug:\n\t\t\tprint('Error: something is missing: %s' % self)\n\t\t\tprint('Is this a string?')\n\t\treturn 'ERROR!!!'\n\t# If something is in a single thing, convert to an equivalent string.\n\tif not self.is_string_defined():\n\t\treturn self.make_string()\n\telse:\n\t\treturn pretty(self"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\ts = string()\n\tfor s in self:\n\t\ts = pretty(s, indent=indent, debug=debug)\n\treturn s\n\ndef pretty_all(self):\n\t\"\"\"\n\t\tReturn a pretty formatted representation of self.\n\t\tAll the fields are collapsed.\n\t\"\"\"\n\ts = \"\"\n\tfor s in self:\n\t\ts = pretty(s, debug=False)\n\treturn s\n\ndef pretty_all_recursive(self):\n\t\"\"\"\n\t\tReturn a pretty formatted representation of self.\n\t\tAll the fields are"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tself = self._repr_obj()\n\tif debug:\n\t\tself.print_meta()\n\n\t# self.print_meta()\n\n\tif None is not self.type:\n\t\tself.print_name(indent=indent)\n\n\tself.__setstate(pretty=True, state=False)\n\n\t# if self is a dictionary, we make a new dictionary before\n\t# printing the object.  we do this until we just have to\n\t# handle types with a non-zero __hash__.\n\tif self.__dict__ is not None and"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tout = [''] if debug else []\n\tif self.isstring():\n\t\tout.append(repr(self.tostring()))\n\telse:\n\t\tx = _pdefpretty(self)\n\t\tif x and (debug or indent):\n\t\t\tout.append('#')\n\t\t\tif self.isarray():\n\t\t\t\tout.append('[' + repr(x))\n\t\t\telse:\n\t\t\t\tout.append('{ %s }' % repr(x))\n\t\t\tout.append(']')\n\t\tout.append(';')\n\treturn"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tif prettyPrinters:\n\t\treturn prettyPrinters(self, indent, debug)\n\telse:\n\t\treturn self.__class__.__name__\n\n\"\"\"\nclass reprFactory():\n\t\"\"\"\n\t@since 3.11.0\n\tdef pretty(self, indent=0, debug=False):\n\t\t\"\"\"\n\t\tPretty format the repr of the self, with\n\t\tall the attributes (pretty, debug)\n\t\t\"\"\"\n\t\tif prettyPrinters:\n\t\t\treturn prettyPrinters.pretty(self, indent, debug)\n\t\telse:"}]}
{"task_id": "62b46746d2f69a53b4661722", "prompt": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tself._absorb = 1 # no need to override the absorbing factor or factor\n\t#\n\t# Apply absorbing, if possible\n\tif len(args) >= 1:\n\t\tfor item in args:\n\t\t\tself._factor[item] *= self._absorb\n\t\treturn self\n\t#\n\t# Apply negative absorbing, if possible (in all cases the inverse, if not\n\t# applied, will be the complementary)\n\tif len(args) >= 2:\n\t\t# In each case we have a list of absor"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t# A, B = self.a->b, self.c->d\n\tif len(args) < 2:\n\t\traise RuntimeError(\"A, B, and C must be specified\")\n\tif args[0][0]!= args[1][0]:\n\t\traise ValueError(\"Absorbing arguments not specified\")\n\tif len(args) == 2:\n\t\treturn list(map(lambda x: args[0][0] * x == args[1][0] * args[2][0], args[0][1:-1], self.absorb("}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tfrom sage.categories.contributor import Conjecture\n\tfrom sage.modular.laplacian.matrix_base import MatrixBases\n\tif len(args) == 0:\n\t\traise ValueError(\"Invalid arguments: %s %s\" % (args, str(args)))\n\tm = MatrixBases()\n\tfor i in range(len(args)-1):\n\t\tm.check_term(args[i], i)\n\ttry:\n\t\tA = Conjecture(\"A\", m.base())\n\texcept NotImplementedError:\n\t\traise TypeError(\"Cannot"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tif len(args) < 2:\n\t\traise ValueError('Too few arguments to absorb()')\n\t\n\tabsorb = []\n\tfor arg in args:\n\t\tif arg in (1, 3, 5, 6, 7, 13, 8, 9, 15, 26, 27, 28):\n\t\t\tabsorb.append(10)\n\tfor i in range(len(absorb)):\n\t\tabsorb[i-1] *= 10\n\t\n\treturn absorb[i-1]\n\nimport unittest\n\nfrom gnsn"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t# def apply(arg) :\n\t# \tif not arg.is_list():\n\t# \t\t\t# TODO: return empty list?\n\t# \t\t\treturn list(arg)\n\t# \t\tif len(arg) == 0 or arg[0] == 0:\n\t# \t\t\t# no results yet\n\t# \t\telse:\n\t# \t\t\tfor e in arg:\n\t# \t\t\t\te[0] = True\n\t# \t\treturn arg\n\t# \telse:\n\t# \t\t\ttry:\n\t# \t\t\t\tfor r in"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tif len(args) == 2:\n\t\treturn args\n\targs = list(map(lambda expr: expr.copy(), args))\n\treturn [a for a in args if a is not None else -abs(a)]\n\n\nclass _AbsorptionNegativeRelevance(object):\n\t\"\"\"\n\tAbsorption negative relevance algorithm by default returns a sum of 0.\n\t\n\tIf the input expression is a list, we look for the elements and sum it\n\twith 0. If that is the case, we return an expression with the sum of\n\tthe elements, to"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t# TODO - convert args into expressions\n\targs = []\n\tfor arg in args:\n\t\tif not isinstance(arg, tuple):\n\t\t\tif self.lowercase:\n\t\t\t\targs.append((arg, self.lower(arg)))\n\t\t\telse:\n\t\t\t\targs.append((arg, self.lower(arg, ignore_case=True)))\n\t\t\tcontinue\n\t\targs.append((arg,))\n\t\tif len(args) >= 2:\n\t\t\targs.extend([(self._apply_absorption(arg, self.low),) for"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\traise NotImplementedError(\"Absorption_law does not yet implement absorption()\")\n\n\tret = args\n\n\tfor i in args:\n\t\tif i[-1] == \"+\" or i[-1] == \"-\":\n\t\t\traise NotImplementedError(\"Absorption_l1 does not yet implement absorption()\")\n\t\t\t\n\t# negative absorption\n\tdef negative_absorb(args):\n\t\t\"\"\"\n\t\tAbsorption is positive (or negative) absorption.\n\t\t\n\t\tFor example, given a [3]x([.3]) [4"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\ttry:\n\t\tabsorb = []\n\t\tfor e1 in args:\n\t\t\tabsorb += [e1]\n\t\t\tfor e2 in absorb:\n\t\t\t\tabsorb.append(e2)\n\t\tfor e in absorb:\n\t\t\tif int(e) == 1:\n\t\t\t\treturn e\n\t\treturn None\n\texcept KeyboardInterrupt:\n\t\ttrace = traceback.format_exc()\n\t\tprint(trace)\n\t\ttrace2 = traceback.format_exc()\n\t\tprint(trace2)"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\texpr = [0] * len(args)\n\tnot_ok = [(x, -y) for x, y in self.args]\n\n\tfor x, y in args:\n\t\tif not(not_ok[x]):\n\t\t\tnot_ok[x] = not_ok[x] + (y >= 0)\n\t\tif not isinstance(x, int):\n\t\t\tif not isinstance(x, float):\n\t\t\t\traise ValueError('Absorption must work on all numbers.')\n\t\t\tcontinue\n\t\tfor i in range(len(x)):"}]}
{"task_id": "62b86707b4d922cb0e688c2a", "prompt": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tfrom.handler import on\n\tdef decorate(**kwargs):\n\t\tif type(hook) is not callable and hook == HookType.all_types:\n\t\t\treturn hook\n\t\t\n\t\treturn on\n\t\t\n\treturn decorate\n\nusing System;\nusing System.Linq;\n\nnamespace UAlbion.Formats.Assets\n{\n    public class AssetManager : IDisposable\n    {\n        public string RootPath { get; }\n        public bool UnloadAllAssets { get; }\n   "}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef decorated_function(hook):\n\t\t\"\"\"\n\t\tDecorator function which is wrapped in a decorator for all\n\t\tregistered hooks.\n\t\t\n\t\tArgs:\n\t\t    hook (HookType): Hook attribute for which to register the\n\t\t\tlistener.\n\t\t\"\"\"\n\t\tif not hook.is_a(HookType):\n\t\t\traise Exception('The method hook should be a HookType.')\n\t\treturn Decorate(hook, decorator_class=hook.decorator_class)\n\tdef decorator(wrapper_class):\n\t\t\"\"\"\n\t\tDecorator class to"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef decorator(hook):\n\t\treturn self.on(hook)\n\treturn decorator\n\nusing System;\nusing SLua;\nusing System.Collections.Generic;\n[UnityEngine.Scripting.Preserve]\npublic class Lua_UnityEngine_Color : LuaObject {\n\t[MonoPInvokeCallbackAttribute(typeof(LuaCSFunction))]\n\t[UnityEngine.Scripting.Preserve]\n\tstatic public int ctor_s(IntPtr l) {\n\t\ttry {\n\t\t\tUnityEngine.Color o;\n\t\t\to=new UnityEngine.Color();\n\t\t\tpushValue(l,true"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef decorate(*args, **kwargs):\n\t\t\"\"\"\n\t\tAdd a new handler for a particular hook.\n\t\t\"\"\"\n\t\tself.registry[hook] = kwargs\n\t\treturn HookListener(**self.registry[hook])\n\t\n\treturn decorate\n\n/*\n * This file is part of the TrinityCore Project. See AUTHORS file for Copyright information\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the\n * Free Software Foundation; either version 2 of the License,"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef r(self, hook):\n\t\tif isinstance(hook, HookType) or len(hook) == 3:\n\t\t\tf = getattr(hook, self._hook_reg)\n\t\t\t\n\t\t\tif f:\n\t\t\t\treturn r\n\t\t\telse:\n\t\t\t\thook(hook_type=HookType.register_on,\n\t\t\t\t                  on=hook)\n\t\t\t\thook_type = HookType.register_on\n\t\t\t\n\t\t\tif not hook._reg_on:\n\t\t\t\t# register the"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\treturn hook_with_arg\n\n\nclass HookConfig:\n\t\"\"\"\n\tCustomize class to handle a setting of the \"global\" option.\n\t\n\tUsage: `Config` class\n\t\"\"\"\n\tdef __init__(self):\n\t\tself._global = False\n\t\tself._option_registry = []\n\t\tself._custom_hook = True\n\n\tdef init_custom(self, config: Dict[str, Any]) -> 'HookConfig':\n\t\t\"\"\"\n\t\tAdds a config object to the registry. This should\n\t\tbe called before calling get"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t@register()\n\tdef register_hook(self, hook):\n\t\treg = hook(self.__class__)\n\t\tif isinstance(hook, type(self).__call__):\n\t\t\treturn type(self).__call__(reg)\n\n\t\tif isinstance(hook, (HookType, functools.partial)):\n\t\t\treturn self._register_hook_and_repeating_hooks(hook) + \\\n\t\t\t\tlambda *args: hook(*args)\n\t\telse:\n\t\t\traise TypeError, 'Invalid hook type {}'.format(\n\t\t\t\ttype(hook))\n"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef decorator(f):\n        hook._dispatcher[hook.__name__][hook.__name__].update((hook, f))\n        return f\n    return decorator\n\n\ndef set_register_to_list(hook, lst):\n\t\"\"\"\n\t\tSet the register in a specified hook's list.\n\t\t\n\t\tArgs:\n\t\t\thook (Hook): The hook to which to set the register in.\n\t\t\"\"\"\n\tfor a in lst[hook.__name__]:\n\t\thook.__register_callback"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef add_hook(hook):\n\t\tadd_new_hook(hook)\n\treturn add_hook\n\n\n##########################################################################\n# A hook used to handle \"new\" methods on a resource.  For example,\n# ``<resource>`` can be used to store a resource reference to be\n# registered.\n\n\nclass TaonHerd(HookType):\n\t\"\"\"A callback type for the default implementation of ``on`` for resource\n\tobjects. The default implementation assumes that a resource reference\n\twith the given type can be added and then deleted by the registry,"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\treturn decorator(self._handler.on)\n\n\tdef on_conflict(self,*args):\n\t\t\"\"\"\n\t\tDecorator used when a conflict is detected in an existing listener.\n\t\t\"\"\"\n\t\treturn self._handler.on_conflict(*args)\n\n\tdef on_conflict_instance(self, method, instance, *args):\n\t\t\"\"\"\n\t\tDecorator used when a conflict is detected in an existing instance.\n\t\t\"\"\"\n\t\treturn self._handler.on_conflict_instance(*args)\n\n/*\n * Copyright (C) 2014 The Android"}]}
{"task_id": "62b86729b4d922cb0e688c2f", "prompt": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t# Create instance of the database\n\tconn = sqlite3.connect(etcd_host, etcd_port)\n\t\t\n\t# Fetch default values\n\tdefault_config = conn.execute(\"SELECT * FROM config WHERE base_config_key IN ('config.yaml', '*')\").fetchone()\n\t\n\t# Check default values\n\tif default_config is None:\n\t\tprint(\"Warning - default_config is not found in the database\")\n\t\tprint(\"WARNING - Database values default_config = 'config.yaml'\")\n\t\treturn default_config\n\t# Return the created configuration\n"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tconfig = dict()\n\t\t\n\tconfig[\"user\"] = user\n\tconfig[\"etcd_host\"] = etcd_host\n\tconfig[\"etcd_port\"] = etcd_port\n\t\t\n\treturn config\n\n\n# Creates a configuration with various values to control the user to connect to or\n# to the redis. Will be used to determine if your redis connection is safe.\n#\n# We have a very simple config file with all values set, so let's save these to\n# make them usable.\n#\n# These keys will be used as 'username', 'password', 'keytab', etc."}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tcluster = ConfigManager()\n\tcluster.set('user', user)\n\tcluster.set('etcd_host', etcd_host)\n\tcluster.set('etcd_port', etcd_port)\n\treturn cluster.create_config()\n\nif __name__ == '__main__':\n\timport sys\n\n\tfrom Eye import *\n\tfrom EyeConfig import config_static\n\tcfg = config_static.get_config('base', {}, [])\n\tuser = sys.argv[1]\n\tif user == None:\n\t\tuser = config_static.get_"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tuser_db_config = []\n\tcurrent_db_config = []\n\tkey_path = os.getenv(\"TFA_DB_PATH\")\n\tif key_path:\n\t\t# read from key_path as a string\n\t\ttry:\n\t\t\tfp = open(key_path, \"r\")\n\t\t\t# load the key_path\n\t\t\tkey = fp.read()\n\t\t\tassert len(key) == 20\n\t\t\tcurrent_db_config = base_config(key, etcd_host, etcd_port)\n\t\texcept:\n"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tstatic_user  = get_config(\"static_auth.\")\n\n\tdef setdefault(key, value, defaultval):\n\t\tif static_user == key:\n\t\t\tvalue = None\n\t\tdel static_user\n\t\tif value == None:\n\t\t\tdefaultval = \"\"\n\t\telse:\n\t\t\tvalue = value.replace(quote, escape_for_quoting(quote))\n\t\t\tvalue = value.replace(doublequote, quote_double(quote, value, False))\n\t\t\tvalue = value.replace(\"\"', quote)\n\t\t\ttry"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tdef default_params():\n\t\treturn dict(port=etcd_port)\n\tdefault_config = default_config(user, etcd_host, etcd_port)\n\treturn default_config(user, etcd_host, etcd_port)\n\n//\n// ********************************************************************\n// * License and Disclaimer                                           *\n// *              "}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tauth_conf = EtcdConf()\n\tauth_conf.user = user\n\tauth_conf.etcd_host = etcd_host\n\tauth_conf.etcd_port = etcd_port\n\t\n\treturn auth_conf\n\n# coding: utf-8\n\"\"\"\n__author__ = 'zhengxinlin'\n\n\nclass Porn(object):\n    \"\"\"\n    \u7ed9\u6211\u4eec\uff0c\u770b\u770b\u4f60\u7684\u4ee3\u7801\u53ef\u4ee5\u88ab\u6267\u884c\u6210\u4e00\u4e2a\u51fd\u6570\n    \u4f60\u53ef\u4ee5\u901a\u8fc7\u7684\u662f\uff1a\u4f60\u53ef\u4ee5\u901a\u8fc7\u51fd\u6570\u7684"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tcluster_config = ClusterConfiguration(config_json=dict())\n\tcluster_config.set_data(\"user\", user)\n\tcluster_config.set_data(\"etcd_host\", etcd_host)\n\tcluster_config.set_data(\"etcd_port\", etcd_port)\n\treturn cluster_config\n\n\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a value that\n\tcan be set directly.\n\t\n\tArgs:\n\t    config (Dict): the configuration object to initialize\n\t    use_auth_user (bool): if True,"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tconfig = {\n\t\t\"settings\": {\n\t\t\t\"key\": \"value\",\n\t\t\t\"auth_method\": \"default\",\n\t\t\t\"client_id\": \"\",\n\t\t\t\"client_secret\": \"\",\n\t\t\t\"auth\": [\n\t\t\t\t\"default\"\n\t\t\t],\n\t\t\t\"connection_timeout\": 90,\n\t\t\t\"enable_auth_on_tls\": False,\n\t\t\t\"ssl_verify\": False,\n\t\t\t\"enable_health_check\": False,\n\t\t\t\"ssl_ca_file\": \"no_ssl.ca\","}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tclass Config:\n\t\tdef __init__(self):\n\t\t\tself.defaults = {\n\t\t\t\t\"username\": \"test\",\n\t\t\t\t\"password\": \"test\",\n\t\t\t\t\"token\": \"test\",\n\t\t\t\t\"timeout\": \"30\",\n\t\t\t\t\"database\": \"test/default\",\n\t\t\t\t\"use_proxy\": \"true\",\n\t\t\t}\n\t\tdef __getattr__(self, name):\n\t\t\t\"\"\"\n\t\t\tGet the object attribute if an object method is defined, otherwise\n\t\t\tthe default returns the same as the __getattr__ method;\n"}]}
{"task_id": "62b8a4a4755ee91dce50a3d2", "prompt": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# pylint: disable=no-member\n\t# We are interested in 'all'\n\tif self.daylight_savings_offset == 0:\n\t\tdt = datetime(2000, 1, 1)\n\t\tself.daylight_savings_offset = 1\n\t\n\treturn self._fromutc_no_offset(dt, self.daylight_savings_offset)\n\n\ndef _fromutc_no_offset(dt, offset):\n\t\"\"\"\n\tThis is the original ISO datetime-aware function. We *know* the first\n\toccurrence,"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif isinstance(dt, datetime.datetime):\n\t\tdt = dt.replace(tzinfo=datetime.timezone.utc)\n\tif isinstance(dt, datetime.datetime):\n\t\treturn dt\n\treturn datetime.datetime(0, 0, 0, 0, 0, 0, 0)\n\n\ndef time_from_str(dt, strict=False):\n\t\"\"\"\n\tConvert from str to a timezone-aware datetime.\n\t\n\tSince this is the one time that we *know* we have a timezone-aware\n\tdatetime object, consider this opportunity for that instead"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif len(dt)!= 6:\n\t\traise ValueError(\"dt must be of length 6, not {}\".format(len(dt)))\n\telif dt.tzinfo is not None:\n\t\traise ValueError(\"dt must not have tzinfo!\")\n\telif not dt.tz:\n\t\tdt = dt.replace(tzinfo=LocalTimezone())\n\telif dt.tz is None and dt.tzinfo is not None:\n\t\traise ValueError(\"dt does not have tzinfo!\")\n\tself = _tzstr2dt(self._tzstr, dt)\n\treturn self\n"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\ttoday, monday = _dates()\n\tnewdt = datetime.fromtimestamp(dt.timetuple()[:2], tz=dt.tzinfo)\n\tif not newdt, \"Monday day to first: %i\" % (today.weekday(), ):\n\t\t# Monday day to first: Mon Apr 07 00:00:00 -0800 (00:00:00 UTC)\n\t\tnewdt = dt\n\twhile newdt and _is_valid_offset(dt, newdt):\n\t\tnewdt, dt = _fromutc(newdt, dt)\n"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tself._check_new(dt)\n\tnewtz = _tzlocal_fromutc_tz(dt.tzinfo)\n\tself.result1 = old_datetime(dt)\n\tself.result2 = old_datetime(newtz)\n\treturn self.result1\n\ndef toutc(self, dt):\n\tself.result1 = self._toutc(datetime.datetime(1970, 1, 1))\n\treturn self.result1\n\nstr2utc = os.name == \"nt\" and _tzlocal_toutc_tz\n\ndef utc2str"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tnewdt = datetime(1970, 1, 1, 0, 0, 0)\n\tis_ambiguous = True\n\tfor t in dt:\n\t\tif t.weekday() == 0 or (t.minute <= dt.minute and dt.second == t.second):\n\t\t\ts = f\"{dt.strftime('%Y%m%dT%H%M%S')} {dt.strftime('%I %p')}\"\n\t\t\traise Exception(f\"Ambiguous datetime: {s}\")\n\t\telif (t.weekday() > 0 and dt > t) or (t"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif not isinstance(dt, datetime.datetime) and not isinstance(dt, str):\n\t\traise ValueError('dt should be tzinfo compatiable; got %s' % type(dt))\n\telif dt.tzinfo is None:\n\t\tdt.tzinfo = dt.tzutc()\n\telif dt.tzinfo is None:\n\t\tdt.tzinfo = 'Europe/London'\n\tdt1 = dt.replace(tzinfo=tzinfo)\n\treturn dt1.astimezone(self.tzinfo)\n\nclass UTC(object):\n\n"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif isinstance(dt, datetime.datetime):\n\t\tfrom..compat import iNaT\n\t\tif 'offset' not in ('year','month', 'day') and dt.day < 23:\n\t\t\traise NaTTypeError, 'offset must be a number'\n\t\tif not dt.isocalendar()[dt.month==1 and dt.day==7:]:\n\t\t\traise NaTTypeError\n\telif isinstance(dt, datetime.time):\n\t\tdt = dt.replace(tzinfo=None)\n\telif isinstance(dt, datetime):\n\t"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif isinstance(dt, datetime.datetime):\n\t\tdt = dt.replace(microsecond=0)\n\tif isinstance(dt, datetime.timezone):\n\t\tdt = dt.tz_convert(datetime.timezone)\n\treturn dt\n\n\nclass _ZopeFormatter(Formatter):\n\t\"\"\"\n\tFormat a time-series in Python format using the `Zope` and `tzinfo` modules.\n\t\n\tThis implements `datetime.format` with the exact same behaviour as the Python version\n\tof `datetime.tzinfo` in Python 2.7.\n\t"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\timport locale\n\tif 0 <= dt.hour: # We assume this is UTC\n\t\treturn datetime.datetime(dt.hour, dt.minute, dt.second, dt.microsecond, -1)\n\telse: # Else, try to figure out if it's ambiguous; this is almost always the same.\n\t\tdt = dt.astimezone(locale.tzlocal())\n\t        \n\t        # We know dt is an unambiguous datetime object. If it's\n\t        # ambiguous, use the"}]}
{"task_id": "62b8982f755ee91dce50a241", "prompt": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn relativedelta(np.int_(int(self.days)))\n\ndef normalized_utc(self, timezone=None):\n\t\"\"\"\n\tReturn a version of this object represented as an UTC datetime\n\tobject for the date, time and timezone of the given date.\n\t\n\t>>> normalized_utc(datetime.utcnow())\n\tdatetime.datetime(2011, 2, 1, 22, 0, 0)\n\n\t:param timezone:\n\t\tStr, timezone string, or None; the timezone you want a date from.\n\t\n\t:return:\n\t\tA datetime object"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn relativedelta(days=self._days, hours=self._hours,\n\t\tminutes=self._minutes, seconds=self._seconds)\n\n\t#####################################################\n\tdef _compute_year_and_day_from_calendar_year(self):\n\t\tyear = self.calendar._calendar.year\n\t\tyear_month, _ = self.calendar.find_months_at_year(year)\n\t\tmonth_of_year = str(year_month)\n\t\tyear_is_leap = (self.calendar._has_leap_year)"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\tfrom.datetime import relativedelta \nreturn relativdelta() \n\"\"\"\n\ndef relative2abs(delta, abs_threshold):\n\t\"\"\"\n\tReturn a new absolute delta object whose absolute values are the absolute\n\tvalues in delta and abs_threshold.  If abs_threshold == -1, abs_threshold\n\tindicates relative to the timedelta structure; if we get the new absolute\n\tdeltas from the dateutil method, this method would return relative to (0,\n\tlen(delta)).\n\t\n\t>>> delta2abs(1.23, abs_threshold=-120) "}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn relativedelta\n\n/*---------------------------------------------------------\n * Copyright (C) Microsoft Corporation. All rights reserved.\n *--------------------------------------------------------*/\n\nimport './src/webidl.ts';\n\nimport { assert } from '..';\nimport { join, padStart, padEnd } from './misc.ts';\n\nconst tests = [\n    {\n        topic:'should wrap a single element before other elements',\n        output: 'foo 1 bar 1 foo bar abc \\n',\n    },\n    {\n   "}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\tdef _canonical():\n\t\treturn relativedelta(days=+3*_delta)\n\treturn _canonical\n\ndelta_days = relativedelta\ndelta__ = delta_days\n\ndef normalize(number, unit='day', **kwargs):\n\t\"\"\"\n\tReturn a normalized value of the number.\n\t\n\t>>> normalize(seconds='seconds', n=24, unit='day', **kwargs)\n\thours: 30 days 24 hours\n\t\n\t>>> normalize(hours=+24, unit='minute', **kwargs)\n\thours: 15 minutes\n\t\n\t>>> normalize(hours=24, unit='"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\tfrom dateutil.relativedelta import relativedelta as impl\n\treturn impl(self)\n\n/**\n * Copyright (C) 2016 LibRec\n * $Id: cbhicam.h 52034 2013-07-09 06:30:43Z pichard $\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\tif self._int or type(self) == type(self._int):\n\t\treturn self.__date\n\telse:\n\t\treturn float(self.days) / self.num_days // self.num_days\n\n\t#if __name__ == '__main__':\n\t#\tprint('relativedelta')\n\t#\tsys.exit(main())\n\t#time.sleep(0.01)\n#\tdel sys.stdout\n#except AttributeError as exce:\n#\tprint exce\n\t#del exce\n#\ttime.sleep(0.1"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t# These are all objects.\n\t\n\t# In a relativedelta(days=1.5, hours=2), the value at the end\n\t# depends on the day of the week for which it has elapsed\n\t# (1-01 for January, 1 for January, 10 or 11 for January or other).\n\t# In a relativedelta(hours=2), the value from the end of\n\t# the weeks, is a delta between 0.2 and +/-1, rounded downwards\n\t# according to that.\n\t# In a relativedelta(hours=7), the value"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\tself = self.timedelta().to(self.time())\n\treturn relativedelta(self.seconds, self.days, self.month, self.year)\n\n\nclass relativedelta(absolutedelta):\n\t\"\"\"\n\tReturn a version of this object that represents a number in the range\n\t0-1.0.\n\t\n\t>>> relativedelta(days=0, hours=2).relativedelta()\n\trelativedelta(days=+0)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn relativedelta(*self)\n\n\nclass timedelta(timedelta_tz):\n\t\"\"\"\n\ttimedelta()\n\t\n\t>>> import time, sys\n\t>>> import time as t\n\t>>> t1 = timedelta(days=1)\n\t>>> t2 = timedelta(hours=2)\n\t>>> t3 = timedelta(minutes=4)\n\t>>> t4 = system(t1, t = t2, minutes=5)\n\t>>> delta = t4 - t\n\t>>> delta == t2\n\tTrue\n\n\tSee also the \"relativedelta"}]}
{"task_id": "62b89640755ee91dce50a114", "prompt": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t# TODO: implement this function, but with a Python 2 API change that\n\t# does not always be the case for Python 3\n\t\n\t# Check if unicode is one of the standard string classes\n\tdef check(text1, text2, encoding):\n\t\tif isinstance(text1, str):\n\t\t\ts1 = bytes(text1)\n\t\telse:\n\t\t\ts1 = str(text1)\n\t\ts2 = bytes(text2)\n\t\tis_string = text1 and isinstance(s1, str)\n\t\tif is_string:"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\ttry:\n\t\tt = types.NamedModule(namefunc.__module__, name=namefunc.__name__)\n\texcept Exception as e:\n\t\traise PythonException(str(e))\n\n\tresult = [tup[0] for tup in sorted(tup, key=lambda x: x[2])]\n\tfor i in range(len(result)):\n\t\tresult[i] = result[i][0]\n\t\tresult = [tup[0] for tup in sorted(tup, key=lambda x: x[2])]\n\treturn result\n\n\n"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t# noinspection PyAttributeOutsideDeclaration\n\tdef tzname_in_python3(namefunc):\n\t\tresult = namefunc('now')\n\t\tif result is not None:\n\t\t\t# now is unicode string\n\t\t\treturn result.encode('utf-8')\n\treturn tzname_in_python2(namefunc)\ntzname_in_python3.aliases = ['utf_8', 'python_bytes', 'python_string']\ntzname_in_python3.description = 'Time zone name in the locale supported by locale'\n\ndef tzname"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\treturn type(namefunc)(namefunc(unicode) + namefunc(bytes) + namefunc(basestring))\n\n################################################################################\n\n\nclass DateTime:\n    \"\"\"DateTime(offset, tz, dst_timezone)\n\n\tdate.py doesn't give datetimes with the same\n\ttimestamp, so we have to specify.\n\n\t\"\"\"\n\t#\n\t## NOT YET IMPLEMENTED\n\n\t\"\"\"\n    When the offset is not specified, offset=None is used.\n\t\"\"\"\n\n    def date(self, offset=0):\n\t"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t# This function was in python 3 to get around the need for unicode in Python 2 (and the use for unicode here)\n\t# Also works for unicode/bytestring\n\t\"\"\"\n\tdef use_py2to3(namefunc):\n\t\tif namefunc.startswith('tzname_in_python2'):\n\t\t\tnamefunc = namefunc.encode(\"ascii\")\n\t\t\t\n\t\t\ttry: \n\t\t\t\twith open(\"UTCDateTime.txt\",\"r\") as f:\n\t\t\t\t\tdatetime_result = f.read()\n\t\t\t\treturn time.gmtime(datetime.strptime("}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\timport warnings\n\twarnings.warn(\"tzname() is deprecated since Python 3, use tzname.out() instead.\")\n\treturn tzname(namefunc(nameobj=name))\n\ndef tzname_out(name=None, tz=None, local=False, utc=True, dst=None, dstflag=None, dstutc=False, dst=None):\n\t\"\"\"\n\tThis function will change the bytestring to use the given name, specified\n\tby tz.\n\t\n\tname  string - The name (or timezone specifier) of the tzfile.\n\ttz "}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t\"\"\"def tzname_in_python3(namefunc):\n\t\ttzname()\n\t\"\"\"\n\treturn tzname(namefunc)\n\ndef is_unicode_string(name):\n\treturn \\\n\t\t(name==\"B\" or\n\t\t name==\"Bbb\" or\n\t\t name==\"bb\" or\n\t\t name==\"bbb\" or\n\t\t name==\"Bbbb\" or\n\t\t name==\"bBbb\" or\n\t\t name==\"bbb\" or\n\t\t name==\"bbbb\" or\n\t\t name==\"bbbab\" or\n\t\t name==\"bbBb\" or\n"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\ttry:\n\t\tnamefunc()\n\texcept:\n\t\t# Not yet implemented in Python 3. We need to define this function\n\t\t# so as to get \"UTF-16 output not supported yet\" in the test script.\n\t\t# python_bytestrings('tzname') returns bytes, but it was a bit\n\t\tclass pytzstr:\n\t\t\tdef __init__(self):\n\t\t\t\tself.tzname = None\n\t\t\t\tself.zoneinfo_encoding = None\n\n\t\tdef __unicode__(self):\n\t\t\treturn pytzstr.zoneinfo"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t#return \"UTC\".encode('utf-8') + datetime.datetime.utcfromtimestamp(time.time())\n\t#return \"date\".encode('latin-1') + str(datetime.datetime.utcnow().date())\n\t#return \"tzname_in_python2(\" + namefunc() + \").encode('latin-1') + str(namefunc() + \").encode('utf-8')\"\n\t#return \"tzname_in_python2(\" + namefunc() + \"._timeinfo.encode('utf-8') + \\\".encode('latin-1')"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tfrom future.builtins import unicode\n\tfrom future.builtins import int\n\tfrom future.basestring import tobytes, tounicode\n\timport string\n\timport errno\n\ttry:\n\t\terrno = 1\n\t\timport locale\n\texcept ImportError:\n\t\tlocale = None\n\t\ttry:\n\t\t\ttry:\n\t\t\t\tfrom string import iinfo\n\t\t\texcept ImportError:\n\t\t\t\tfrom os import path, error, exc, text_type\n\t\t\telse:\n\t\t\t\tisdir = path.exists\n\t\t\t\tlstat_type = text_type\n\t\t\telse"}]}
{"task_id": "62b87d24d292efb640a55670", "prompt": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tif not _version_.get() or _version_.get() < 0 or _version_.get() >= 2:\n\t\treturn DEFAULT.get(version='x.y')\n\telse:\n\t\treturn DEFAULT_MAP[version.get()]\n\n\ndef get_full_version(version=None):\n\t\"\"\"\n\tGet the version number for a given version or default if not set.\n\t\"\"\"\n\tif version is None: version = sys.version_info[0]\n\tif version.isdigit(): return DEFAULT_MAP[version.strip()]\n\telse: "}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\treq = requests.get(\n\t\t__opts__['http_addr'],\n\t\tdata={'project_id': __opts__[\"project_id\"],\n\t\t\t'version_id': __opts__[\"versions\"],\n\t\t\t'client': __opts__[\"client\"]}\n\t)\n\tprint(check_data(req.text, False))\n\treturn req.json()['versions']\n\n\nadd_project_data_map()\n\nfunction create_package(pkg, cb, pkg_type) {\n\tif (pkg_type ==='static') {\n\t\tif"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\ttry:\n\t\tversion = get_setting(\"version\", default=\"2.0.0\")\n\texcept:\n\t\tprint \"Unable to get version info from configuration.\"\n\t\tprint \"Try updating with the latest version\"\n\t\tprint \"version = \" + get_setting(\"version\", default=\"2.0.0\")\n\t\tprint \"try updating with the latest version\"\n\t\tabort(\"Error, Could not get version and try again.\")\n\n\treturn version\n\n\ndef get_command():\n\treturn (Command(get_cmdconf, \"Get command settings"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tout = []\n\ttry:\n\t\tparser_version = sys.version_info\n\t\tparser_full_version = _version.split(parser_version)[0]\n\t\tfor version in parser_full_version.split(' '):\n\t\t\tout.append(version.strip())\n\texcept (ValueError, IndexError):\n\t\tlogger.debug('Exception caught in get_versions:'+ sys.exc_info()[0])\n\treturn out\n\ndef _get_all_versions():\n\treturn _version.split(parser.version_info[12"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tc = current.db.get_connection()\n\tif c.check_connection():\n\t\tsql = \"SELECT version;\".replace(' ', '%20')\n\t\tresult = c.execute(sql)\n\t\tif not result:\n\t\t\treturn\n\t\tv = result.fetchone()\n\t\tif c.db.get_connection().debug:\n\t\t\tcurrent.msg.info('Got version info.')\n\t\treturn c, v\n\n\ndef run(db, **kw):\n\t\"\"\"\n\trun script\n\t\"\"\"\n\tif db.env"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\treturn version() or get_default_version()\n\n@property\ndef version(self):\n\t\"\"\"Return current version\"\"\"\n\treturn get_version()\n\n@property\ndef version_code(self):\n\t\"\"\"Return current version code\"\"\"\n\treturn get_version_code()\n\n@property\ndef latest(self):\n\treturn get_version()\n\n@property\ndef latest_version(self):\n\treturn get_version()\n\n@property\ndef preferred(self):\n\treturn get_version()\n\n@property\ndef preferred_version(self):"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\treturn requests.get(\"https://api.github.com/repos/\" + self.owner_name + \"/\" + self.repo_name + \"/versions\", headers=self.headers, verify=request.is_ssl(), verify_ssl=False).json().get(\"versions\", None)\n\n@cached_property\ndef get_tag_names():\n\t\"\"\"\n\tReturn a list of all tags of the repository\n\t\"\"\"\n\t# return to previous version of the repository\n\thead, prev_head, current = b\"HEAD\"\n\thead_dict = requests.get(\"https://\""}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\ttry:\n\t\tversion = getModuleVersion(\"main\", debug=False)\n\texcept ModuleNotFoundError:\n\t\tsys.exit(1)\n\tif version!= getVersion():\n\t\treturn defaultVersion\n\telif hasattr(sys, \"stdin\"):\n\t\ts = sys.stdin.read()\n\t\tif not s or s == '':\n\t\t\treturn defaultVersion\n\telse:\n\t\twarn(\"failed to get module version\")\n\t\treturn defaultVersion\n\n\t# print version information\n\t# print version\n\tprint(\"%s\" % version)\n\t"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tif not _get_version:\n\t\t# this method may not always exist\n\t\t# but better safe than sorry.\n\t\traise LookupError('Unable to find versions. Please setup an Etc.'\n\t\t\t' application at'+ _('a different directory, and/or do not '\n\t\t\t'delete the default versions. If you want to '\n\t\t\t'override an older version that does not exist, '\n\t\t\t'restall the service'))\n\t# Check that a release was configured\n\tif not re.search(r'<Release version: '\n\t\t"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tif not check_file(check_sys('rpm package').get_contents_for_path()):\n\t\treturn version.get_versions()\n\telse:\n\t\treturn list(self._versions)\n\ndef get_current_version():\n\t\"\"\"Returns version information and any known missing version info.\n\tReturns None for an empty version.\n\tFor example, if this is an rpm1 package, then returned version\n\tis '1.2.3'.\n\t\"\"\"\n\tif not check_file(check_sys('rpm package').get_contents_for_path()):"}]}
{"task_id": "62b87d24d292efb640a5566f", "prompt": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif style['color'] == '':\n\t\tvalue = 'default'\n\telif style['color'] == 'error':\n\t\tvalue = 'error'\n\telse:\n\t\tvalue = style['color']\n\tcolor = style['color']\n\tif style['default']:\n\t\tvalue = style['default']\n\telse:\n\t\tvalue = 'default'\n\treturn color if style is None else 'color:' + color + ':' + value\n\ndef render_line(pieces, line):\n\t\"\"\"Render a piece as a line of its own.\n"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t@functools.wraps(pieces)\n\tdef wrapper(pieces):\n\t\t'''\n\t\tWrapper around piece pieces to render to style.\n\t\t'''\n\t\tr = {'number': True, 'color': styles.get(getattr(style,'style_id', False), style)}\n\t\tfor piece in pieces:\n\t\t\tif isinstance(piece, dict):\n\t\t\t\tr[piece['name']] = \\\n\t\t\t\t(r['color'], Style.render(piece, style))\n\t\t\telse:\n\t\t\t\tr[piece] = \\\n\t\t\t\t"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif isinstance(pieces, (tuple, list)):\n\t\treturn tuple(render(p, style) for p in pieces)\n\telse:\n\t\treturn ''.join((render(p) for p in pieces))\n\n\ndef main():\n\t\"\"\"\n\tGet version. Current format is (major, minor, release, patch) where (major, minor):\n\tMajor, Minor: build number, Patch: patch level\n\t\"\"\"\n\tcurrent_version = os.uname()[4]\n\tfor i,s in enumerate(KNOWN_SUGGEST_SUGGEST):\n"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t# The last one, if any\n\tdefaultStyle = \"\"\n\n\t# style.get(\"css\", None) == defaultStyle\n\t# style.css(\"color\", \"red\") == defaultStyle.css(\"color\", \"red\")  # noqa\n\n\t# style.render(\"%version\\n\\n\" % __version__) == defaultStyle\n\n\t# style.css(\"#name-color\").value = None; style.css(\"color\", \"red\") == defaultStyle.css(\"#name-color\", \"red\")  # noqa\n\t# style.css(\"color\", \"blue\")"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t# https://github.com/PillowCode/MagicRenderer/blob/master/Rendering.js#L34\n\n\tcolor = _style_to_color(style)\n\n\tresult = u\"%s{ %d;%s%s%s\" % (pieces[0],\n\t\tlen(pieces)-1 if len(pieces) == 1 else 0, style, color, pieces[1], colors)\n\n\t# Make sure we align to the right edge\n\tif len(pieces) == 2 and not style == \"left\":\n\t\tresult += \"_aligned\""}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces['changed']:\n\t\tif pieces['new'].isdigit():\n\t\t\tpieces['new'] = int(pieces['new'])\n\t\telse:\n\t\t\t# The version had the wrong format.\n\t\t\tpieces['new'] = max(pieces['new'])\n\telse:\n\t\tpieces['new'] = None\n\toptions = {\n\t\t'header_only': False\n\t}\n\t# If there is a style string, such as '<%=><% color %>' that\n\t# should replace the original text.\n\tpieces['style'] = style\n"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tassert style in [u'color', u'reflectable']\n\n\tversion = Version(pieces[0])\n\n\tif style in (u'color', u'reflectable'):\n\t\treturn 'color {}'.format(version.color)\n\n\telif style in ('bold'):\n\t\treturn 'bold {}'.format(version.bold)\n\n\telif style in ('italic'):\n\t\treturn 'italic {}'.format(version.italic)\n\n\telif style in ('underline'):\n\t\treturn 'underline {}'.format(version.underline)\n\n\telif style in ('"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\treturn ''.join([p.__class__.__name__ for p in pieces])\n\nimport unittest\n\nfrom copy import copy\n\nfrom pydicom_builder.parsers.misc.xml.xml_constants_from_value import \\\n    XMLConstantsFromValue\nfrom pydicom_builder.parser_parsers.xml.dom_parser import DOMParser\nfrom pydicom_builder.utilities.testing import TestCase\n\n\nclass TestXMLConstantsFromValue(TestCase):\n    \"\"\"\n    The <unittest.TestCase> used to test the <XMLConstantsFromValue>"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t# noinspection PyProtectedMember\n\tclass Text:\n\t\tdef __init__(self, content):\n\t\t\tself._piece = \" \".join(content)\n\t\t\t# noinspection PyProtectedMember\n\t\tdef __str__(self):\n\t\t\t# noinspection PyProtectedMember\n\t\t\treturn \"{}<{}>\".format(self._piece, style.get(self._piece))\n\n\tclass Bold:\n\t\tdef __str__(self):\n\t\t\t# noinspection PyProtectedMember\n\t\tdef __str__(self):\n\t\t\treturn style."}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tdef render_full(pieces, style):\n\t\tif not pieces:\n\t\t\treturn\n\t\tfrom setuptools.extern import gyp\n\t\tgyp('addon-%s' % version, style, type='render')(pieces, style)\n\trender.__doc__ = render.__doc__ or \\\n\tparse('{{%s}} %s' % (style, style))\n\treturn render\n\t\ndef prepare(version, style):\n\t\"\"\"\n\tPrepare pieces for a version or style.\n\t\"\"\"\n\tv = version.split(\".\", 1)[0"}]}
{"task_id": "62b87d24d292efb640a5566d", "prompt": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tpieces = list(itertools.chain(*[x for x in pieces if x not in ('$', '.')]) \n\t\t# don't use. because if one fails to parse $a/2.txt it'll cause a segfault\n\t\t# but if it all fail, it'll cause segfaults\n\t\t# Use. because sometimes *some* python version won't have $ and so on\n\tcuts_first =      True\n\twhile pieces and not cuts_first:\n\t\tif not pieces[0]:\n\t\t\tnew_pieces = []\n"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif pieces[-1] == '.':\n\t\treturn \".\"\n\treturn \"\"\n\n\nall_of_the_arty = namedtuple( \"all_of_the_arty\", (\"prefix\", \"suffix\"))\n\"\"\"\n\treturn all_of_the_arty(\"www.thearty.com\", \".com/web/index.php?cat=book\")\n\"\"\"\n \npattern = re.compile(\"^.+(http|\\.(?:ftps?t|mtps)?|\\.m2p?)(/?.+)y(.*)\") # http/ftp."}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif not pieces:\n\t\treturn\n\tv = vec3(0,0,pieces[1]*0.4)\n\td = vec3(0,0,0)\n\tfor piece in pieces:\n\t\tif piece[0] == '.' and piece[1]!= '..':\n\t\t\tv += vec3(0, 0, 1*piece[1])\n\t\t\td += vec3(0, 0, 1)\n\treturn v, d\n\n\ndef get_piece(v,d,n):\n\tif v > d or v < 0 or"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tnewpieces = set([item[0] for item in pieces])\n\tnewpieces.update([item[1] for item in pieces])\t# add dots\n\treturn newpieces\n\n\n_pieces_re = re.compile(r\"(?<=\\S)(?P<last_piece>(?P<first_piece>[\\d\\.]+))\")\n\ndef get_first_piece_of_piece(pieces):\n\t\"\"\" Given a range of pieces, returns the first piece of the range\n\tas the first token. If the pieces are not consecutive, returns zero \"\"\"\n\tpieces"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif pieces.doprint():\n\t\treturn \".\".join([i+pieces[i] for i in pieces])\n\treturn \".\".join([i+\"(\"+(i==\")\"+pieces[i] for i in pieces])\n\n\t\ndef plural(number):\n\tplural_index = int(number)\n\td = float(number)\n\tif d == 1:\n\t\treturn \"\"\n\tcount = 0\n\tfor d1 in range(len(d)):\n\t\tfor d2 in range(len(d)):\n\t\t\tif d1+d2"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif pieces[-1] not in (\".\\0\", \".\"):\n\t\treturn \".\"\n\treturn \".\"\n\n\ndef gen_token(raw_pieces, name, config, token, print_level):\n\t\"\"\"\n\tCreate a token given a name and parse the raw data.\n\n\tIf the raw pieces are of no value, then there is no chance of recognizing any\n\tof the pieces on this line.  Instead, a message is printed to the specified\n\tlogger.\n\t\n\tThe token's output will always end with a newline.\n\t\"\"\"\n\tif len(raw"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif pieces[\"closest-other-votes\"]!= \"sass\":\n\t\treturn \"sass\"\n\tif pieces[\"closest-other-voters\"]!= \"ponder\":\n\t\treturn \"dog\"\n\treturn \"other\"\n\ndef random_words(number):\n\tword = [c for c in range(0, len(chars) + 1) if chars[c] in number]\n\tshuffle(word)\n\treturn list(word)\n\nusing Microsoft.Bot.Builder.AI.Luis;\nusing Microsoft.Bot.Builder.UnitTests"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif pieces[\"func\"] == \"len\" or pieces[\"func\"] == \"max\":\n\t\treturn \".\".join(pieces[\"val\"])\n\treturn \"\"\n\n\n@register_vcs_handler(\"git\", \"pieces_from_vcs\")\ndef local_pieces_from_vcs(pieces):\n\t\"\"\"Get local copies of \\ and / from local path.\"\"\"\n\t# these need to be outside the path, or they won't be\n\t# filled in later\n\treturn [\".\", \".\"]\n\n\n@register_vcs_handler(\"git\", \"pieces_from_file_"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tone_or_more = True\n\tif not any(_isdecimal(1.0) for _ in pieces):\n\t\treturn None\n\tre_number = float(str(re.sub(r'(-\\d)', '_', pieces[re.sub(r'(_\\d?)', '_', '-')])).strip()) \n\tre_number = re_number + \".\" if re_number[0]!= \".\" else re_number\n\treturn re.sub(r'([0-9\\.]+)', r'\\1', pieces[one_or_more and re.sub("}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\treturn (\n\t\tnot pieces.x_not_dot_minus\n\t\tand pieces.X_not_dot_minus\n\t\tand pieces.group(1) not in (\n\t\t('r', 'end'), ('L', 'list'), ('B', 'bar')\n\t)\n)\n\n\n@curry\ndef join_groups(pieces, pattern):\n\t\"#\" + pieces.group(1) + \"|\" + pattern\n\treturn pieces.group(0)\n\n\ndef get_ext_size(pieces):\n\t\"\"\"Get the ext size of a"}]}
{"task_id": "62b87d23d292efb640a5566b", "prompt": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport re\n\tsubprocess.call(args, cwd=cwd, out=subprocess.PIPE, env=env, verbose=verbose, hide_stderr=hide_stderr)\n\n\nCMD_HELP = \"\"\"\nUsage: %prog [options]\n\n  -h, --help    : display this message\n  -v, --version : display version information\n  --quiet, --no-quiet, --no-version    - suppress the version number, even if it differs by one\n\"\"\"\n\n\ndef run_command(args, silent=False, cwd=None, verbose"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\texit_status = 1\n\n\t# handle command string override\n\tif commands:\n\t\tfor cmd, opts in _split_args(commands):\n\t\t\tcmd = cmd.lstrip('=')\n\t\t\tif '=' in cmd:\n\t\t\t\tcmd, opts = cmd.split('=')\n\t\t\t\tcmd =''.join(args + [k +'=' + v for k, v in opts.items()])\n\n\t\t\tprocess_input_output(cmd, args, stdin, stdout, stderr, env)\n\ndef strip_environment():\n\t\"\"\"\n\tRemove"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport json\n\tcwd = os.getcwd()\n\tif cwd:\n\t\tcwd = os.path.abspath(cwd)\n\n\tif verbose:\n\t\tprint >> sys.stderr, \"executing:\", commands\n\telse:\n\t\tprint >> sys.stderr, \"executing: %s\" % commands\n\n\tcmd = \"\"\n\tfor cmd in commands:\n\t\tif not os.path.exists(cmd):\n\t\t\tif show_stderr:\n\t\t\t\tprint >> sys.stderr, \"command not found\", cmd\n\t\t\t\tcontinue\n\t\t\tcmd ="}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif not _pypad.env.has_module(PYPadEnv):\n\t\tfrom pypad.compat import print_exc, print_s\n\telse:\n\t\tprint_exc()\n\t\tprint_s(**locals())\n\tfrom _internal.run_command import run_command\n\treturn run_command(commands, args, cwd, verbose, hide_stderr, env)\n\ndef _python_execute_wrapper(func, *a, **kw):\n\t\"\"\"\n\tWrapper to call an external wrapper function\n\t\"\"\"\n\tret = func(*"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport cStringIO\n\tusage = 'Usage:'+ ''.join([sys.argv[0]]+[]+commands) +''\n\ttext = cStringIO.StringIO()\n\n\tprint''.join(get_command_status_no_newline(command)[0] for command in commands)\n\n\tprint '- '+usage\n\n\tif env:\n\t\tprint '=' * len(env)\n\n\tif cwd:\n\t\tcwd = cwd.split(':')[0]\n\t\tprint cwd\n\telse:\n\t\tcwd = os.getcwd()"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from __future__ import absolute_import\n\ttry:\n\t\timport shlex\n\t\tshlex.splitext = splitext\n\t\tshlex.quote = quote\n\t\tshlex.split = split\n\texcept:\n\t\tpass#IMPORTIMPORTIMPORT from os import path, getcwd, isdir\n\t\tpass#IMPORTIMPORTIMPORT from subprocess import check_output, call, call_subprocess\n\texitcode = 1\n\texit(exitcode)\n\treturn exitcode, commands, args, cwd,"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\ttry:\n\t\t# Send stdout to STDOUT so that we can read the output.\n\t\tstdout = codecs.getreader(inspect.cleandoc)(os.stdout)[0]\n\texcept:\n\t\t# If something didn't work out then just pretend we have no stdin,\n\t\t# or stdin is not in a tty.\n\t\tstdout = sys.stdin\n\trun_command.stdout = stdout\n\n\tif not hide_stderr:\n\t\trun_command.stderr = sys.stderr\n\n\tif verbose:\n\t\trun_command.verbosity"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\twith Lock_write(None) as lock:\n\t\tresult = subprocess.call(commands, args, cwd, verbose, hide_stderr, env)\n\t\treturn [x[0] for x in result]\n\n\ndef usage(version):\n\ttry:\n\t\tsys.stderr.write(\"\\nUsage:  %s <options>\\n\" % (version,))\n\t\tsys.stderr.write(\"\\n\")\n\t\tsys.stderr.write(\"Usage:  %s [options] <command>\\n\" % (version,))\n\t\tsys.stderr.write"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tnew_env = env or get_default_env()\n\n\tsuccess = None\n\tif new_env:\n\t\tenv = new_env\n\t\tsuccess = run_command_safe(commands, args, env, cwd, verbose, hide_stderr, env)\n\telse:\n\t\ttry:\n\t\t\tsuccess = run_command_safe(commands, args, cwd=cwd, verbose, hide_stderr, env=env)\n\t\texcept KeyboardInterrupt:\n\t\t\tsuccess = True\n\t\t\traise SystemExit(failure_exit_codes[-1])\n\n"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif verbose:\n\t\timport pprint\n\t\tpprint.pprint(commands)\n\treturn _main(commands, args, cwd, hide_stderr, env)\n\n\nif __name__== '__main__':\n\tmain()\n\n##########################################################################\n#\n#  Copyright (c) 2013, Image Engine Design Inc. All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without\n#  modification, are permitted provided that the following conditions are\n#  met:\n#\n#      * Redistributions of source code must"}]}
{"task_id": "62b87d23d292efb640a55668", "prompt": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t# A dictionary to hold default config values.\n\t# A version string like '0.23.2-RC1_2' can be used. The version\n\t# string will display with the short description and components can\n\t# match apart from the string.\n\n\t# Version negotiation takes a bit of templating, so it's added.\n\tconfigs = getConfigSections()\n\tversion = configs.get(\"version\")\n\tv = VersioneerConfig()\n\tfor section, configs in configs.items():\n\t\tif section.startswith(\"ver\"):\n\t\t\tfor subsection"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tif sys.version_info[0] == 2:\n\t\treturn VersioneerConfig()             \n\telse:\n\t\ttry:\n\t\t\treturn ConfigParser()\n\t\texcept ConfigParserInvalid:\n\t\t\treturn VersioneerConfig()\n\n\nSCRIPT_NAME ='versioneer'\nDEFAULT_CONFIG_FILE ='versioneer.cfg'\n\ndef check_requires_dist():\n\t\"\"\"Check and raise for RequirementError if not using the versioneer\n\titinerary.\n\n\tDeveloped with versioneer."}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\timport re\n\tconfig = VersioneerConfig()\n\tconfig.update(get_config_vars())\n\treturn config\n\n\n# Version objects\n\n\nclass VersioneerException(Exception):\n\tpass\n\n\nclass VersionException(VersionException):\n\tpass\n\n\nclass VersioneerParseError(VersionException):\n\tpass\n\n\nclass VersioneerSourceRequired(VersionException):\n\tpass\n\n\nclass VersionerVersionLiteral(LiteralVersionException):\n\tpass\n\n\nclass VersionException(Exception):\n\tpass\n\n\nclass InvalidVersion(VersionException):\n\tpass"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t# Versioneer creates a config object for us, which can be found in\n\t# the misc/ directory.\n\treturn Config(VERSION)\n\nfile_search_paths = {}\nfor pattern in glob.glob('*(config.json|config.ini)', recursive=True) + glob.glob('Config_*.py'):\n\tfile_search_paths.setdefault(pattern, []).append(pattern)\n\n\ndef get_versions(config_file=None, versions_file=None):\n\t\"\"\"\n\tReads the lines from a JSON file or a config dict and serializes them"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\treturn config\n\n\n\nconfig = VersioneerConfig()\n# General configuration magic\n# *ALWAYS* used if the entire configuration is not available.\n# config.always = True\n\n# If you have timer, you can change the following via the config.autoreload = True/False\n# config.autoreload = True\n\n# In development, you can change the following via the config.mode = 'development'\n# config.mode = 'development'\n\n# To switch on or off the verbose output, switch on the output:\n# config.verbose = False\n\n# N"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t# versioneer.py: main entry point (autorun)\n\ttry:\n\t\tf, args = _import_module('versioneer')\n\t\treturn VersioneerConfig(*args)\n\texcept NameError:\n\t\tf = sys.stdin\n\t\targs = (sys.stdin.buffer.encode('utf-8'),) + VersioneerConfig.get_config_filename_list()\n\t\treturn VersioneerConfig(*args)\n\n// Copyright (c) 2009-2010 Satoshi Nakamoto\n// Copyright (c) 2009-2015 The Bitcoin Core"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\treturn VersioneerConfig()\n\n\n@dataclass\nclass VersioneerPackageInfo:\n\tNAME: str = field(\n\t\tdefault=None,\n\t\t metadata={ 'v3py: no_metadata': 'The version attribute is not defined.' },\n\t)\n\n\n@dataclass\nclass VersioneerScanResult:\n\tSUCCESS_EXIT_CODES: int\n\t\"\"\"\n\tA list of exit codes from the last run of a scan of a release.\"\"\"\n\tSUCCESS_PKG_INFO_SIZE: int\n\t\"\"\"\n\tComputed to be the maximum number of packages that can"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\treturn configFromHolder()\n\n\nif __name__ == \"__main__\":\n\timport sys\n\tsys.argv = ['', 'Test.testName']\n\tsys.exit(main(sys.argv))\n\nclass Api::V1::ApplicationController < Api::BaseController\n\n  before_action :set_application, only: %i(index)\n\n  before_action :set_application_name, only: %i(update_user)\n\n  def index\n    @api_applications = ::Application.all\n    @application_names = ::"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\treturn versioneer.get_config()\n\ndef get_config_var(name):\n\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object;\n\treturns a variable named config->$name.\n\tIf the $var variable is not a variable, it will be created\n\tusing the current global config context.\n\t\"\"\"\n\ttry:\n\t\treturn config_vars()[name]\n\n\texcept NameError:\n\t\tpass\n\ndef config_vars():\n\t\"\"\"Returns a parser configuration array that can be used to set\n\tdefault configuration values using the command"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\timport os\n\n\t# import sys\n\t# sys.path.insert(0, os.path.abspath('.'))\n\t# path = [x for x in sys.path if not x.startswith(os.path.abspath('..'))]\n\tsys.path.insert(0, os.path.abspath('.'))\n\n\twith open(os.path.join(sys.path[0], 'VERSION')) as f:\n\t\tlong_desc = f.read()\n\n\t# --app-name, --app-version, --app-author, --charset\n"}]}
{"task_id": "62b87d23d292efb640a55667", "prompt": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef register_vcs_handler_wrapper():\n\t\tif sys.platform == 'win32':\n\t\t\t# \"windows\" VCS can't handle \"file.upload('somefile')\"\n\t\t\tif os.path.isfile(os.path.expanduser('~/path/to/file.upload.py')):\n\t\t\t\tdel os.path.normpath\n\t\t\t\tfrom rarage.vcs import VCS\n\t\t\t\tvcs = VCS(source='file.upload.py', revision=7)\n\t\t\telse:\n\t\t\t\t# TODO: this probably"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorator(func):  # decorator\n\t\treturn register_vcs_handler(vcs, method, func)\n\n\treturn decorator\n\n/*-\n * Copyright (c) 1991 The Regents of the University of California.\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n * 1. Redistributions of source code must retain the above copyright\n *    notice, this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorator(f):\n\t\t\"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\t\t@functools.wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"Wraps a method in a callable class.\n\t\t        \"\"\"\n\t\t\treturn f(*args, **kwargs)\n\t\twrapper.priority = 100\n\t\tself.register_vcs(vcs, method, wrapper)\n\t\treturn wrapper\n\n\t# TODO - need to allow the use of the wrapper in the decorator\n\t@funct"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorator(f):\n\t\twith f(vcs):\n\t\treturn f\n\nclass CachingRepoLocator:\n\tdef __init__(self, c, loc_path, rev_path, _filter=None):\n\t\tself.c = c\n\t\tself.rev = None\n\t\tself.rev_path = rev_path\n\t\tself.loc_path = loc_path\n\t\tself.filter = _filter\n\t\t \n\tdef __call__(self, repo_name, root_path):\n\t\tif self.rev_path:\n\t\t"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t@wraps(method)\n\tdef wrapper(vc, **kw):  # decorator\n\t\t\"\"\" Create the method object.\"\"\"\n\t\tnewmethod = MethodType(vcs, self, method, kw)\n\t\tif kw.get(\"auto_start\", False):  # auto_start set default=True\n\t\t\treturn method\n\t\treturn newmethod\n\treturn wrapper\n\n/*\n * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n * \n * Licensed under the Apache License, Version 2.0 (the \"License\").\n * You may not use this"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorator(method):  # decorator\n\t\t@functools.wraps(method)\n\t\tdef _wrapper(*args, **kwargs):  # pylint: disable=unused-argument\n\t\t\tif method not in VcsHandler._handlers:\n\t\t\t\treturn ValueError(\"Invalid VCS handler.\")\n\t\t\tvs = getattr(method,'method', method)  # hacky cast\n\t\tif not hasattr(vs, 'vcs', True):\n\t\t\treturn_type = VcsHandler.get_return_type()\n\t\telif hasattr(vs.vcs, 'vcs"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorator(func, *args, **kwd):\n\t\t\"\"\" Register a method with the vcs_handler class.\n\t\t\"\"\"\n\t\tif method.__name__!= \"vcs_request_handler\":\n\t\t\traise ValueError(f'Method {method.__name__} is not an VCS method.')\n\t\tif method in vcs.vcs._methods:\n\t\t\traise ValueError(f'A method named {method} was already registered.')\n\t\tvcs.vcs.register_vcs_handler(func, *args, **kwd)\n\n//\n// The"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tfrom kared.vcs.vcs_handler import VcsHandler\n\n\tdef wrapped_func(_cls):\n\t\thandler = getattr(_cls, method, None)\n\t\tif handler:\n\t\t\thandler = VcsHandler(handler)\n\t\tself.register_handler(method, handler)\n\t\treturn method\n\n\t# the following allows us to keep the existing API, and make a method\n\t# for that in the handler rather than for everyone else\n\tvcs += {'set_auto_resolve': vcs_auto_resolve},  # make the api use any"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tif callable(vcs._handler):\n\t\treturn vcs._handler\n\tif vcs._handler:\n\t\treturn vcs._handler(method=method)\n\treturn vcs._vcs(method)\n\n\ndef make_file(resource, path, mode=0o664,\n\t       cache_dir=None, preserve_case=False):\n\t\"\"\"Create a file from a resource.\n\t\"\"\"\n\tfrom vcs.vcs_api import get_vcs_handler\n\tfrom vcs.vcs_handler import VCSHandler\n\n\tif"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tDecorator to mark a method as the handler for a VCS.\n\t\t\"\"\"\n            @functools.wraps(func)\n            def _self(*args, **kwargs):\n                if method in ALLOWED_METHODS:\n                    self._allowed_methods.add(method)\n  "}]}
{"task_id": "62b45e945108cfac7f210a4a", "prompt": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tobject_list = os.listdir(get_root_storage_path()+\"/\")\n\tobjects_found = 0\n\tfor i in object_list:\n\t\tobject_base_address = i.replace(os.curdir, os.sep)\n\t\tobject_directory_entry = os.path.join(object_base_address, os.pardir, \"object-\"+str(i)+\".\"+os.environ['STORAGE_SUFFIX'], 'obj' )\n\t\tif os.path.isdir(object_directory_entry) == False and os.path."}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t# Validate storage\n\tif not self.valid:\n\t\tlog.warn(\"Storage Root is invalid\")  # pylint: disable=W0611\n\t\tpass\n\tif validate_objects:\n\t\tnew_objects = {}\n\t\tif self.storage_root_objects:\n\t\t\tnew_objects.update({'storage_root': self.storage_root,'storage_root_hashes': self.storage_root_hashes})\n\t\tfor object_type, object in self.storage_root_objects.items():\n\t\t\tfor object_hash, check_func"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tvalid_hierarchy = 0\n\tnum_objects = len(self.storage)\n\t\n\tfor i in range(num_objects):\n\t\tfor j in range(1, 4):\n\t\t\ts =''.join((f'{i}', f'{j}'))\n\t\t\tif validate_objects and i + j > 0:\n\t\t\t\ts = \" \".join((s, f'{j}'))\n\t\t\t\tif validate_objects and i + j == valid_objects - 1 and show_warnings and int(i) > 0:\n\t\t\t\t\tself.error(_(\"Invalid hierarchy detected"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tgood_objects = 0\n\n\t# Read the hierarchy contents\n\thierarchy = self.tree.root.get_children_contents()\n\thierarchy = [o for o in hierarchy if validate_objects and not e['valid']][::-1] # e.value is a valid object\n\n\t# Handle problems\n\tif not validate_objects:\n\t    if not check_digests:\n\t\t# No digests, abort\n\t\tself.verbose(self.ERROR, 'No digests, abort')\n\t\treturn num_objects, good_objects\n\n\t"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t# First look at the hierarchy if not already built\n\tif not self._validate_objects:\n\t\tvalid_objects = get_root_objects(self._ploc)\n\t\tif not valid_objects:\n\t\t\traise ValidationError('Could not determine total count of objects, aborting.')\n\t\tif not pretty_path(valid_objects) and not self._dump_all_parents():\n\t\t\traise ValidationError('Parent storage %s did not have any parents. Aborting!' % str(self.get_uuid()))\n\t\n\t# Then do a check if the file does"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\treturn self._validate_superclass(Storage)\n\n\ndef validate_folder(self, validate_folders=True, check_digests=True, show_warnings=False):\n\t\"\"\"Validate storage folder.\"\"\"\n\treturn self._validate_superclass(Folder)\n\n\n# NOTE: These functions are still supported by S3StorageController\n\ndef _validate_superclass(cls):\n\t\"\"\"Ensure an object of this type is valid.\"\"\"\n\tif not hasattr(cls, \"__name__\"):\n\t\traise S3StorageError(\"Invalid superclass, expected object\")\n\tif not"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tself.debug('Validation: rebuilding directory hierarchy...')\n\tif validate_objects:\n\t\t# Validate hierarchy\n\t\tif self.debug:\n\t\t\tprint('Validate Root hierarchy')\n\t\t\tprint('Validating hierarchy...')\n\t\t\tprint()\n\n\t\tret = 0\n\t\tnum_objects = 0\n\t\tgood_objects = len(self.db.get_all('hierarchy_storage/%s/' % self.storage, self.prefix))\n\t\t# Generate digest for all items\n\t\tsha256_map = {\n\t\t\t'sha256"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tpass\n\nimport warnings\n\nwarnings.warn('Using py2-postgres.py instead of mysql.py',\n              DeprecationWarning, stacklevel=2)\nwarnings.warn('Using pygeos.py instead of geom2d.py',\n              DeprecationWarning, stacklevel=2)\nwarnings.warn('Using libgeopg.o2k.geopg instead of geopandas.geopandas',\n          "}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\thierarchy_dict = dict()\n\t\t\n\tif self.tree_hierarchy:\n\t    tree_hierarchy = [str(object) for object in self.tree_hierarchy.items()]\n\telse:\n\t    tree_hierarchy = ('tree', '')\n\t\n\tfor child in hierarchy_dict:\n\t    if child not in tree_hierarchy:\n\t        child_info = {}\n\t        child_info['tree'] = tree_hierarchy\n\t        child_info['number_of_objects'] ="}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tif not validate_objects:\n\t\traise Exception(\"validate_hierarchy can currently only be called when validate_object is True\")\n\tif check_digests:\n\t\t_digest = digest.new.MD5()\n\t# check that all object IDs are accessible\n\t_root = self._root\n\tfor object_id in enumerate(_root):\n\t\tobj = self.objects.get(self.keys[object_id])\n\t\tif obj.storage[self.storage_key]!= self.keys[object_id]:\n\t\t\tself.log.warn("}]}
{"task_id": "62b45e515108cfac7f210a3c", "prompt": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.bucket = self.storage_id\n\tself.object_id = self.storage_id % OCFL_MAX_DIR_FILE_COUNT\t# this is 0-based.\n\nclass Py3Storage(OcflObject):\n\n\tdef __init__(self, storage_key, name, storage_type, **kwargs):\n\t\tsuper(Py3Storage, self).__init__(**kwargs)\n\t\tself.storage_id = storage_key\n\t\tself._name = name\n\t\tself._type = storage_type\n\t\t# This is not"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tif self.oslfls_db == None:\n\t\tself.initialize_root_db()\n\treturn self.oslfls_db\n\nclass OcFls(base.Base):\n\"\"\"\n\tClass to store and manage the OcfsL storage root for various OCFL programs.\n\tThe storage root is in the form of 'name', and with which the OcfsL files are stored.\n\tThe storage root can be retrieved using the 'get' command below.\n\t\n\t- 'name' is of the form 'org1-name@appid:compid"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tif self.os_instance.storage_root_name:\n\t\treturn\n\tos_init(self, os_args=['-v'])\n\tself._setup_storage_root()\n\n\tfor os_path in ('/usr/local/etc', '/opt/local/etc'):\n\t\tos.system(\n\t\t\tos.path.join(self.os_instance.storage_root, os_path + '.d/LC_MESSAGES.CONF'))\n\t\tos.system(\n\t\t\tos.path.join(self.os_instance.storage_"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tdef initialize(self,*args):\n\t\ttry:\n\t\t\tfrom pyflakes.storage import StorageRoot\n\t\t\tstorage = StorageRoot(self.name,)\n\t\texcept ImportError as error:\n\t\t\tf = sys._getframe().f_back\n\t\t\tprint >> sys.stderr,('Failed import:', error, f.__name__)\n\t\t\traise error\n\t\ttry:\n\t\t\tself.storage = storage;\n\t\texcept (TypeError, AttributeError):\n\t\t\tself.storage = object()\n\t\t\t\n'''\n\tdef __init__(self, name, settings"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tif os.path.islink(os.path.join(ROOT, LINK)) and os.path.islink(os.path.join(ROOT, LINK)):\n\t\t# The link points to a local archive. That's where this\n\t\t# file lives.\n\t\tsys.path.insert(0, ROOT)\n\t\ttry:\n\t\t\t# The archive should be opened in an executable mode.\n\t\t\tsubprocess.run(executable_command(\"mdadm shell\"))\n\t\t\tfrom OCFL import get_tree, get_storage, get_config\n\t\t"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.name = 'oss-root'\n\tself.storage_name = self.name + '-root'\n\tself.directory = self.name \\\n\t\t+'/'                  # '/' as storage root\n\tself.type = 'oss'        # oss as storage folder\n\tself.create_folder = True\n\tself.ignore_errors = True\n\n\t# Create a folder for each type\n\tif self.type in ('oss', 'oss.default'):\t# Directory"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.ocfl.initialize(create_root=True)\n\n\n\t# Initialize and return the storage root.\n\tif self.ocfl.root_storage_root is not None:\n\t\tself.ocfl.set_root_storage_root(self.ocfl.root_storage_root)\n\n\ndef load_odhcp_root_from_file(config: TerraformConfig, file_path: str) -> TerraformConfig:\n\t\"\"\"\n\tLoad a ODhcp root storage root into OCFL\n\n\tFile paths should"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.storage = {'id': 'odfl',\n\t\t\t\t\t\t 'description': '',\n\t\t\t\t\t\t 'parent': None,\n\t\t\t\t\t\t 'name': 'test',\n\t\t\t\t\t\t\n\t\t\t\t\t\t 'osds': None,\n\t\t\t\t\t\t 'is_public': True,\n\t\t\t\t\t\t 'created_at': '2017-10-10T09:41:40',\n\t\t\t\t\t\t 'updated_at': '2017-10-10T09:41:45',\n\t\t\t\t\t\t 'created_in': 1536,\n\t\t\t\t\t\t 'updated_in': 1536,\n\t\t\t\t\t\t 'deleted_at': None,\n\t\t\t\t\t\t 'domain_id': None"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.set_file_handle()\n\tself.set_name(\"OF-L Storage Storage Root\")\n\t\n\tvol, vol_params = self.create_vol(device=DEVICE)\n\tself.real_create(device, vol, vol_params)\n\t\n\tself.real_initialize()\n\t\n\t# Set root device_path.\n\tself.root_device_path = DEVICE.replace(\"/\", \"\")\n\t\n\t# Open the root device.\n\tself.device.mount(self.root_device_path, \"/\", DEV_M_NOWAIT)\n\t\n"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.name = self._name\n\tself._fs = None\n\tself._data = None\n\tself._data_file = None\n\tself.datafile = None\n\tself.rootdir = None\n\nclass OCFLStorage(object):\n\t\"\"\"A set of OCFL ODS storage. See OCFLStorage for details.\n\t\"\"\"\n\tdef __init__(self, name, file, block_size, blocksize=0x8000000, metadata=None, rootdir=None, metadata_cb=None):\n\t\tself.name = name\n\t\t"}]}
{"task_id": "62b45e2eb89c9fd354170232", "prompt": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tif version < 0 or version > 3:\n\t\traise ValueError(f\"{version!r} is not a valid version.\")\n\n\tr = _pattern.next_version\n\n\treturn _ParseResult(r, parse(version))\n\nclass _ParseResult(_ParseResult):\n\t\"\"\"\n\tA version number is either a number, or an integer.\n\n\t@requires python >= 3\n\t\"\"\"\n\tdef __init__(self, r, version):\n\t\tif not isinstance(r, int):\n\t\t\traise ValueError(f\"{r!r} is not an"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tif version < 0:\n\t\traise NameError(\"Zero-prefixed version identifier\")\n\ndef __is_valid_version(version):\n\t\"\"\"\n\tReturns True only if the version is valid, not equal\n\tand it is less than, or equal to, the version requested.\n\t\"\"\"\n\treturn version < _VERSION_RANGE_THRESHOLD or (\n\t\tversion > _VERSION_RANGE_THRESHOLD and\n\t\tversion == _VERSION_0 or (version >= 0 and version < 0)\n\t)\n\n# This is a dictionary, which keeps track of the version.\n"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tparts = version.split('.')[2:]\n\tlast = parts[-2]\n\n\tfor i in range(len(parts)):\n\t\tif parts[i] == '0':\n\t\t\tparts[i] = '1' \n\n\tlast++\n\n\tresult = None\n\t\n\tif parts[0]!= version and not isinstance(parts[0], int):\n\t\t# Versions do not begin with a number.\n\t\traise ValueError('Invalid version: {0}'.format(parts[0]))\n\n\tif parts[len(parts) - 1] =="}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tif version >= 0:\n\t\treturn version\n\telse:\n\t\treturn version % 100\n\n// Licensed to the.NET Foundation under one or more agreements.\n// The.NET Foundation licenses this file to you under the MIT license.\n\nusing System;\nusing System.Composition.Hosting.Metadata;\nusing System.Composition;\nusing Xunit;\n\nnamespace System.Composition.Hosting.Cci.Tests\n{\n    public class ImportedMethodDeclarationTests : IClassFixture<TestMetadata>\n    {\n        private readonly string _"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\treturn version\n\n## BEGIN CLASSES\nclass ClassManager:\n\tdef __init__(self, prefix=None):\n\t\tself.prefix = prefix\n\t\tself.all_classes = [] # list of classes under this manager\n\t\n\tdef __len__(self):\n\t\treturn len(self.all_classes) +len(self.prefix)\n\t\n\tdef add_class(self, class_name, namespace=None):\n\t\t\"\"\"\n\t \tAdd a class definition to the class manager.\n\t\t\n\t \tclass_name = string specifying the name of the class"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import re\n\ttry:\n\t\tnext_version = re.match(r'^(0[xX][0-9a-fA-F]+)\\s+(\\d+\\.\\d+[eE][+-]?[0-9]+\\b)',version[2:-1])\n\texcept:\n\t\tpass#IMPORTIMPORTIMPORT import sys\n\t\tnext_version = sys.version.split()[0]\n\treturn next_version\n\n# Add in version number with version number prepended by \"lib_\"\ndef pre_version"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tif not version:\n\t\treturn None\n\t\n\tif version[0] == '0' and version[-1] == '_':\n\t\treturn None\n\t\n\tif version[0] not in RE_RE:\n\t\tnext_re_version = RE_PREFIX_RE.sub(RE_RE, repr(version))\n\t\treturn next_re_version.lower()\n\t\n\treturn version[:5]\n\ndef get_parser():\n\t'''Return parser.\n\t\n\tThe current parser will be initialized inside load_plugin() and\n\tmust be initialized at the"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tstart = 0\n\tfor i in range(10):\n\t\tversion = version / 10\n\t\tversion = version % 10\n\t\tif version == 0:\n\t\t\treturn None\n\t\tif version < 3:\n\t\t\tbreak\n\t\n\treturn version.upper()\n\ndef current_version(version):\n\tfor i in range(3):\n\t\tversion = version / 10\n\t\tversion = version % 10\n\tif version == 0:\n\t\treturn None\n\treturn str(version)  # The version we would need to parse.\n\nclass Version"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tif not version.startswith('-'):\n\t\tif version[0] == '-':\n\t\t\tversion = version[1:]\n\t\tversion = version\n\tif version == \"<unknown>\":\n\t\traise PimError(\"Zero-prefixed version found\")\n\treturn version\n\n#------------------------------------------------------------------------------#\n#\n#  Copyright (C) 2013-2021 Helicalizr \u015arotzekonalaranik (harman.paintzr@gmail.com)\n#\n#  This software is provided 'as-is', without any express or implied\n#  warranty."}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tassert isinstance(version, int)\n\tif version < 0 or version > 65535:\n\t\traise ValueError(\"Not a valid version number: %s\" % version)\n\treturn version\n\ndef last_major_minor_patch():\n\t\"\"\"\n\t\tExtracts the most recent minor version number.\n\t\tNot forbids checking for `0, 0` because that would be ambiguous.\n\t\"\"\"\n\treturn last_major_minor_patch.str.split(\".\")[0]\n\ndef major_patch():\n\t\"\"\"\n\t\tExtracts the major version number.\n"}]}
{"task_id": "62b45e23e0d4551b0392c90a", "prompt": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\treturn self.validate_content(version_dirs, \"version_inventories\")\n\n\ndef main():\n\timport argparse\n\timport sys\n\n\t# the following parser is necessary for python scripting to work\n\t# e.g. version_inventories\n\tARGF_HELP = \"version_inventories.py --help \\thelp \\033[4;21m--\\033[0m\"\n\n\tparser = argparse.ArgumentParser(description=\"Verifies that a list of versions are correct\")\n\tparser.add_argument(\"args\")\n\tparser"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tself.inventory_version_inventories = []\n\tfor path in version_dirs:\n\t\tversion_dir = path\n\t\tself.inventory_version_inventories.append({\n\t\t\t'version_dir': version_dir,\n\t\t})\n\n\tfor version_dir in self.inventory_version_inventories:\n\t\tif not version_dir in self.versions_inventories:\n\t\t\t# This inventory has no versions for version directory \"version_dir\".\n\t\t\t# This happens when validating, so version_dir is in\n\t\t"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tif version_dirs:\n\t\tfor folder in version_dirs[::-1]:\n\t\t\tyield self.validate_version_in_dir(folder)\n\t\t\n\tif not version_dirs:\n\t\treturn\n\telse:\n\t\tfor folder in version_dirs:\n\t\t\tyield self.validate_version_in_dir(folder)\n\t\treturn version_dirs\n\t\t\t\n\nclass BlankVersion:\n\n\tdef __init__(self, root_path, name_extension, short_name_extension, version_extension):\n\t\tself.name_extension"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tcurrent_version_dirs = version_dirs[0]\n\tversions = []\n\tinventory_root_uuid = os.environ.get(\"SHOULD_VERSIONS_UUID\")\n\tif inventory_root_uuid is not None:\n\t\tinventory_root_uuid = inventory_root_uuid.lower()\n\t\tif inventory_root_uuid == current_version_dirs[0]:\n\t\t\treturn self.version_seq.is_valid()\n\tinventory_root = os.path.join(inventory_root_uuid, current_version_dirs[0])\n\t"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tif not version_dirs:\n\t\treturn\n\tversion_dirs.sort()\n\tfor version_dir in version_dirs:\n\t\tversion_dir = version_dir.rsplit('_', 1)\n\t\tif len(version_dir)!= 2:\n\t\t\traise ValueError(\n\t\t\t\t\t\t\"The %s needs to have a version directory as only 1/\"\n\t\t\t\t\t\t\"to be a known version\", version_dir\n\t\t\t\t)\n\t\twith open(version_dir[0]+'.txt', 'w') as f:\n\t\t\tfor x in version_"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tdef version_directory(x):\n\t\tif isinstance(x, str):\n\t\t\tdir = os.path.dirname(x)\n\t\telse:\n\t\t\tdir = x\n\t\t# Remove any path components.\n\t\tfor d in dir_name_regex:\n\t\t\tdir_name_regex[d]\n\t\treturn dir\n\tversion_dirs = list(version_directory(os.getenv(\"CDASH_VERSION\")))\n\t\n\t# Each version SHOULD have a content digest, if the version sequence did not change\n\t# we will raise an error."}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tassert len(version_dirs) == 1\n\t\n\tif isinstance(self.version_db, SQLiteConnection):\n\t\timport sqlite3\n\t\tversion_dirs = version_dirs[[1]] - [2, 3]\n\t\tdb = version_db\n\t\tassert len(db) == 1, \"version db already set (db = SQLiteConnection.connect())\"\n\t\tcurrent_version = []\n\t\n\telse:\n\t\t# Use open_version_db, except that it will open the version database\n\t\t# when opening it on the first call with the root"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tversion_dirs = sorted(version_dirs)\n\t\n\tfor dir in version_dirs:\n\t\tfor name in dir:\n\t\t\ttry:\n\t\t\t\tversion_file = os.path.join(dir, name)\n\t\t\t\tversion_dict = versionparser.VersionInfoParser(versionFile)\n\t\t\t\t# this ensures we only return if version is older than version_dict.last_read - last_save\n\t\t\t\tif os.path.getmtime(version_file) >= newdate:\n\t\t\t\t\tversion = version_dict.strip().split(';', 2)[0]"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\twith io.open(osv.versions._version_home_path(self.home,'version'), 'r') as _version:\n\t\tversion_content = _version.read()\n\t\tversion_content = tuple(int(line)[:6] for line in version_content.splitlines())\n\t\t\n\tassert version_content in version_dirs, \"invalid version dirs ({})\" \\\n\t\t\"for version home {} ({}). Should look like.\".format(version_dirs, osv.versions._version_home_path(self.home,'version'), )"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tfor dir in version_dirs:\n\t\ttry:\n\t\t\tfor entry in os.listdir(dir):\n\t\t\t\tversion = dir\n\t\t\t\ttry:\n\t\t\t\t\tversion = os.path.join(dir, entry)\n\t\t\t\t\tversion = os.path.normcase(os.path.relpath(version, dir))\n\t\t\t\t\tif not os.path.isdir(version):\n\t\t\t\t\t\twarnings.warn(f\"Path {dir} not found\")\n\t\t\t\t\t\tyield 0\n\t\t\t\t\t\t\n\t\t\t\t\tif os.path.exists(version):\n\t\t\t\t\t\tos.remove"}]}
{"task_id": "62b45e21e0d4551b0392c8ed", "prompt": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tdef check_type(type):\n\t\treturn type == 'root' or type == '/' or type == 'object' or type == 'file'\n\treturn check_type(string.split(path)[-1])\n\ndef is_root(path):\n\treturn path[-1:] == '/'\n\ndef is_file(path):\n\treturn path[-1:]!= '/'\n\ndef is_directory(path):\n\treturn path.endswith('/') or path.endswith('\\\\')\n\ndef is_archive(path):\n\treturn path.endswith('/') or path.endswith('\\\\')"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t# Check the storage path\n\tif path in ('./', './/', './/', './/..') and len(path) > 1 and path:\n\t\ttype, path_path, path_file = path.rsplit('/', 1)\n\t\tif type == 'folder':\n\t\t\treturn 'folder'\n\t\treturn 'object'\n\treturn None\n\n\n__alias__ = {\"root\", \"file\", \"directory\"}\n\n\n# The following constants are used in the path-handling functions in the OCFL\n# script. We don't want to have to deal"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tif (path.startswith(('0','.'))]):\n\t\t# return OCFL Roots and Directory Type\n\t\tif (path!= '0'):\n\t\t\treturn ('root' + path)\n\t\treturn ('object' + path)\n\telif (path.startswith(('.'))]):\n\t\t# return Object Properties and Directory Type\n\t\tif (path!= '0'):\n\t\t\treturn ('file' + path)\n\t\treturn ('object' + path)\n\t\n\treturn ''\n\npackage a\n\nvar (\n\t// C3 is a valid"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tmatches = path.split('\\*')\n\tfor index, item in enumerate(matches):\n\t\tif not item:\n\t\t\tcontinue\n\n\t\tif item in ('0=', 'root'):\n\t\t\treturn item\n\t\tif item[1:] == '..':\n\t\t\treturn 'folder'\n\t\tif item[1:] == '/':\n\t\t\treturn 'file'\n\t\t\n\treturn 'folder'\n#\tif '/storage/' in path:\n#\t\treturn path\n\ndef find_path_from_path(path):\n\t\"\"\"\n\tA utility method"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tfrom cflib import _valid_path_type\n\tfor directory_type in _valid_path_type:\n\t\tfor file_type in _valid_path_type:\n\t\t\tif _ot_path_util.isfile_or_directory(directory_type, file_type):\n\t\t\t\treturn directory_type\n\t\t\t\n\treturn \"unknown\"\n\ndef _find_file_type(path):\n\tfrom cflib._otlog import set_logger_level\n\tif is_dir(path):\n\t\treturn \"directory\"\n\telse:\n\t\t"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tif path!= '':\n\t\tpath = path.strip('=') + os.sep\n\t\ttype = 'ROOT'\n\telse:\n\t\tpath = os.getcwd()\n\t\tpath = path[len(os.sep):]\n\t\ttype = 'PATH'\n\t\n\tpath_type = {}\n\tif path.find('/')!= -1:\n\t\tpath_type['root'] = 'FILE'\n\telif path.find('/o')!= -1:\n\t\tpath_type['object'] = 'ON'\n\telse:\n\t"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tif path[:4]!= '0x':\n\t\t# No path we can deal with here...\n\t\treturn \"root\"\n\n\t# Look for magic\n\tif path[:3] == '0x' and path[3:].isdigit():\n\t\t# This looks like an OCFL file.\n\t\treturn ENTITY_TREE_STORAGE_DIRECTORY\n\n\tif not path:\n\t\t# Could happen on some machines (e.g. SPARKs) as well!\n\t\t# Try some generic methods\n\t\treturn ENTITY_TREE_STORAGE_DIRECTORY"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tif path.lower() == '0&':\n\t\t# return 'files' which is really an ocfl inventory or an ocfl object path\n\t\treturn 'files'\n\t#\n\treturn None\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n *"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tfor fn in os.listdir(path):\n\t\tif not os.path.isdir(fn):\n\t\t\tcontinue\n\t\tif fn.split(\"/\")[0].lower() == \"object\":\n\t\t\tfor i in range(4):\n\t\t\t\tfor j in range(5):\n\t\t\t\t\tif fn[i+4:i+4+j+1]==os.path.basename(fn):\n\t\t\t\t\t\t\n\t\t\t\t\treturn i,  # path doesn't end with /\n\t\t\t\t\telse:\t# don't know path type\n\t\t\t\t\t\n\t\t\t\t\telif  # file"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tif path[0] == '0':\n\t\treturn 'root'\n\telif ':' in path:\n\t\tpath = path[1:]\n\t\tpath = path.strip()\n\t\treturn path[-11:]\n\telse: \n\t\tif path.startswith('root_'):\n\t\t\treturn 'root'\n\t\telif path.startswith('object'):\n\t\t\treturn 'file'\n\t\telif path.startswith('.'):\n\t\t\treturn 'file'\n\t\telif path.startswith('{'):\n\t\t\treturn 'root'\n\t\telif path.startswith"}]}
{"task_id": "62b45b396decaeff903e1001", "prompt": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from sp_logger import Logger,get_logger\n\tlogger = get_logger('amendbx.params')\n\tfor param in params:\t\n\t\tlogger.write('Parameter: '+param)\n\t\tfor i in range(len(bug_ids)):\n\t\t\tlogger.write('\t'%s (%s)'%(param,list(bug_ids[i]))[0])\n\t\tlogger.write('\\n')\n\t\n\t\n\tpass#IMPORTIMPORTIMPORT return \tsp_global_def.ERROR_BAD_LIST"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tself.bug_ids = bug_ids\n\tself.bug_ids = set(bug_ids)\n\n\treturn self\n\ndef get_bbs_params(params):\n\t\"\"\"\n\tRetrieve the Bugzilla bbs params\n\t\"\"\"\n\tself.bbs_params = dict(bbs_params, **params)\n\n\treturn self\n\nusing System.Collections.Generic;\nusing System.Text.Json.Serialization;\nusing Essensoft.Paylink.Alipay.Domain;\n\nnamespace Essensoft.Paylink.Alipay.Response\n{"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tfrom amendapi.api_utils import amend_dict\n\ta = amend_dict(bug_ids=bug_ids)\n\ta['params']['timestamp'] = utils.get_localtime()\n\ta['params']['bug_id'] = utils.strip_or_empty(params.get('bug_id'))\n\ta['params']['bug_name'] = utils.strip_or_empty(params.get('bug_title'))\n\ta['params']['bug_type'] = utils.strip_or_empty(params.get('bug_type'))\n"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tbzparams_to_params_dict(params)\n\tbzparams_to_bug_report_params_dict(params, bug_ids)\n\tbzparams_to_bug_report_param_dict_dict(bug_ids, params)\n\tbzparams_to_bug_report_type_dict(params, bug_ids)\n\tparams_to_dict_dict(params, bug_ids)\n\tbzparams_to_bug_report(params, bug_ids)\n\ndef bzparams_to_bug_report_params_dict(params):\n"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tfor key in params:\n\t\tself.bbug.bzparams.insert(str(key))\n\ndef get_config_dir():\n\tif platform.system() == \"Windows\":\n\t\treturn sys.path[0]\n\telif platform.system() == \"Darwin\":\n\t\treturn \"/Library/Frameworks/Foundation/BPFFuzz\"\n\t\n\t\n\t\n\n###\n###\n### BFFuzz specific methods ###\n###\n\ndef get_crash_info_file():\n\tif platform.system() == \"Windows\":\n\t\tif (not platform."}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tbug = BugzillaApi(self.client, bug_ids)\n\tbug.api.param(\"x\", 1, \"This is a number in the range 1-100\")\n\tbug.api.param(\"y\", 20, \"This is a number in the range 20-99\")\n\ndef appexchange(self, bug_details, appexchange_set):\n\t\"\"\"\n\tThis method will update the appexchange of bug, based on the parameters given\n\t\"\"\"\n\tbug = BugzillaApi(self.client, bug_details)\n\tbug.api.param"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tfor id, param in enumerate(self.bugzilla_params):\n\t\tif self.is_bugzilla_bug(id):\n\t\t\tparams['bugzilla_bug_id'] = id\n\t\t\tparams['bugzilla_bug_number'] = BugzillaBug.get_id_by_number(param['number'])\n\t\t\tparams['bugzilla_bug_number_bug_id'] = BugzillaBug.get_number_bug_by_id(param['bug_number'])\n\t\t\tparams['bugzilla_bug_number_title'] = BugzillaBug."}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams = b_params(self, params)\n\tif len(bug_ids) > 0:\n\t\tparams['bug_ids'] = set(bug_ids)\n\telse:\n\t\tparams['bug_ids'] = ('Bugzilla', b_params(self, self.bugs))\n\treturn params\n\n@amend_bzparams(bugs_filter)\ndef amend_bugs(self, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams = b_params(self, self.bugs)\n\tparams['"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tret = []\n\tfor bug_id in bug_ids:\n\t\tparams.update({\n\t\t\t\"bugid_name\" : BugzillaConstants.ID_NAME_BUGID,\n\t\t\t\"bugid_num\" : \"2\",\n\t\t\t\"bugid_type\" : \"bugbug\",\n\t\t\t\"bugid_version\" : \"1\",\n\t\t})\n\t\tret.append({\"bugid_name\" : bug_id, \"bugid_num\" : \"2\"})\n\treturn ret\n\n\ndef build_db_db(bug"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tfor bug_id in bugs_list:\n\t\tbug_id=bug_id.split(\"?\",1)[0]\n\t\t#print self.bug_id[bug_id]\n\t\ttry:\n\t\t\tself.bug_config={}\n\t\t\tself.bug_params={}\n\t\t\tself.bug_id=bug_id\n\t\t\tself.bug_params_data={}\n\t\t\twhile not(bug_id in bugs_list):\n\t\t\t\tself.bug_params_data[bug_id]=1\n\t\t\t\tself.bug_params[bug"}]}
{"task_id": "62b4567ed7d32e5b55cc83d9", "prompt": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t__borgmatic_keys = borgmatic_nodes_map[node_type]['config']\n\tretention_keys = sorted(__borgmatic_keys.keys())\n\tretention_string = ''\n\tfor i in range(retention_keys):\n\t\tretention_string += __borgmatic_keys[i].value + (\n\t\t\t__borgmatic_keys[i]['config']['tag:yaml.org,2002:int'].value\n\t\t)\n\tretention = 'RETENTION_{:d}'.format(self.borgmatic_"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tif type(nodes) in list:                \n\t\tlogger.debug(\"deep_merge_nodes(): adding each item to the list by appending, merging: %r\", list(nodes))\n\t\tnodes = list(nodes[:-1])\n\telse:\n\t\tnodes = list(nodes)\n\t\n\tfor node in nodes:\n\t\tnode_node = node.data\n\t\tlogger.debug(\"deep_merge_nodes(): adding %s to list as the node will not be merged correctly\", node.tag)\n"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t# This list contains all the nodes (and their values) which correspond to this node.\n\tnodes = [nodes]\n\t\n\t# This list contains all the node (and their values) which correspond to this node.\n\tnode_values = [nodes.keys()]\n\t\n\tnode_data = []\n\t\n\t# A list of the mapping nodes to keys (in this case (scalar, mapping ))\n\tnode_mapping = []\n\t\n\t# A list of the scalar nodes to key (in this case (scalar_node, list_node))\n\tscalar_nodes = []\n\t\n\t"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tif isinstance(nodes, list):\n\t\tnodes = map(lambda node: Node.from_dict(deepcopy(node)), nodes)\n\telse:\n\t\tnodes = deepcopy(nodes)\n\t\n\truamel_yaml = borgmatic.utils.yaml_loader.from_yaml(ruamel.yaml.represent_map_data(nodes))[ruamel_yaml.R\"]\n\t\ndef deep_merge_links(links):\n\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t "}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t# This function is based on the algorithm in http://www.python.org/dev/peps/pep-0008/\n\t# by Terry Zuckkov, based from the original work.\n\t# Some code comes from\n\t# http://krylov.net/r/rivest\n\t# http://rivest.blogspot.com/2012/02/borgmatic-configuration-files/\n\t# http://nsc.ru/blog/2014/05/borgmatic-configuration-files-in-python/\n\n\tnode_set_type"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tself.deepmerge(deep_get_section(yaml_data), deep['sections'])\n\t\n\tif deep['sections'] is None:\n\t\treturn\n\t\t\n\tself.deepmerge_nodes(deep)\n\n\tfor key in list(yaml.iterkeys(deep['sections']))[:2]:\n\t\tnode_value = yaml.extract_all(yaml.load_string(yaml.get(key, '')))\n\t\tfor ruamel_value in [None, yaml.iterkeys(node_value):] + [None, key]:\n\t\t\tif key not in"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t\n\tdef merge(borg_yml, borg_node, keys=None):\n\t\t\"\"\"\n\t\tMerge many borgmatic configuration data structures into a single node. A deep merge\n\t\tcan be performed on the following keys:\n\t\n\t\t-   nodes keys (or keys, if keys is specified a dictionary with string values):\n\t\t-      keys is a mapping as returned by deep merge_nodes\n\t\t-   nodes values (or values, where keys is an attribute with string values)\n\t\t\n\t\t\tTheir keys are all keys in"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\treturn deep_merge_nodes(deep_merge_nodes(deep_yaml.deep_sections[::-1]))\n\n\ndef copy_to_folder(folder, files=[]):\n\t\"\"\"\n\tCopy the files contained in the supplied folder to the supplied folders. These folders are\n\tshared by both the main namespace and in the current working directory. This\n\t\n\tParameters:\n\t\tfolder - the folder to copy.\n\t\n\tReturns:\n\t\tA list of the folders of the copied files.\n\t\"\"\"\n\tnew_dir = Path(os.path.dirname(__file"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t# Make the borgmatic configuration directory\n\tdeep_merge_config_directory(borg_config_dir)\n\t# Construct the yaml file for the borgmatic configuration\n\tfile_path = os.path.join(deep_merge_config_path, deep_merge_config_root, f\"borgmatic-{time.time()}\")\n\twith open(file_path, newline='') as file:\n\t\tyaml_file = yaml.load(file)\n\t# The default value of 'root' for the borgmatic configuration directory\n\troot = yaml"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\timport deep_merge\n\t\n\tif nodes is None:\n\t\treturn\n\t\n\twith open(path, 'wt') as f:\n\t\tfor line in f:\n\t\t\tf.write(line)\n\t\n\tfor path in os.listdir(path):\n\t\tif not path.endswith('yaml.dist-extra'):\n\t\t\tcontinue\n\t\n\t\tif path.endswith('node'):\n\t\t\tpath = path[:path.rfind('.')]\n\t\n\t\tfor name, nodes in sorted(yaml.safe_load(open(path, 'r'), Loader"}]}
{"task_id": "62b4567ad7d32e5b55cc83af", "prompt": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\targuments = parse_arguments_with_line_prefix(*arguments)\n\targs = arguments.command.parsed_args\n\treturn args\n\ndef parse_arguments_with_line_prefix(*arguments):\n\t\"\"\"\n\tParse arguments from the command line.\n\t\tArguments to help identify the various commands that can make use of\n\t\t'from' and 'to' to specify the arguments to an option which has to be an\n\t\tinstance of the same type and can be converted to two different types,\n\t\tsuch as A and B, using the full_help_function().\n"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t# Add command-line argument parsing here\n\tcommand_parsers = set(argument.command_parser)\n\t# Loop through all command-line arguments\n\tfor arg in arguments:\n\t\t# Run command-line-specific parsing process here\n\t\ttry:\n\t\t\tcmd_args = arg.command\n\t\t\treturn do_parse_process(cmd_args)\n\t\texcept NoArgumentParser as e:\n\t\t\t# Print usage and then bail\n\t\t\tpass\n\treturn ArgumentParser()\n\ndef do_parse_process(cmd_args):\n\t# Run command"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t\"\"\"Parse arguments for the command (from user-documentation)\n\t\"\"\"\n\tparser = ArgumentParser(prog='the-app-command',\n\t\tdescription=__doc__)\n\tparser.add_argument('arguments', metavar='<APP-FILE>' )\n\tparser.add_argument('source', metavar='<filename.py>',\n\t\t\tnargs='+', type=ArgumentType(filepath()),\n\t\t\thelp='source file in Python 2.7',\n\t\t\tdefault=os.path.normpath('example'))\n\treturn parser.parse_args(arguments)\n"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\treturn argparse.ArgumentParser(add_help=False)\n\n\ndef parse_argument_list(argument_list):\n\t\"\"\"\n\tGiven an argument list, return it as a list of Argument instances\n\t\"\"\"\n\tresult = []\n\tfor argument in argument_list:\n\t\tresult.append(parse_argument(argument))\n\treturn result\n\n#pragma once\n#include <cstddef>\n#include <cstring>\n\n\nnamespace D3D12 {\n\nstatic int \nGetDepth(D3D12_RENDER_TARGET_VIEW_DESC &viewDesc"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\timport argparse\n\tparser = ArgumentParser(\n\t\tprog=\"cli_exec\",\n\t\t description=\"Executes a command\",\n\t\t epilog=ArgumentParser.error,\n\t\t formatter=\"xml\",\n\t\t epilog=ArgumentParser.error,\n\t\t add_help=False,\n\t )\n\n\tadd_module_entry(\"CliExec\", argparse_cli_exec)\n\nif __name__ == '__main__':\n    main()\n\n// @flow\n\nconst {\n  typeDefs,\n  typeAliases\n} = require('./"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tself = ArgumentParser(prog=self.__class__.__name__)\n\tself.long_description = False\n\tif arguments and all(arg.type is str for arg in arguments):\n\t\tself.allow_abbrev_enum = True\n\t\tself.allow_bool = True\n\t\tself.allow_float = True\n\t\tself.allow_int = True\n\t\tself.allow_list = True\n\t\tself.allow_list_item = True\n\t\tself.allow_scalar = True\n\t\tself.allow_tuple = True\n\t\tself."}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\treturn ArgumentParser(sys.argv)\n\n\nclass OptionParser(argparse.ArgumentParser):\n\t\"\"\"\n\tParse a single command line option.\n\t\"\"\"\n\tdef parse_arguments(*arguments):\n\t\toptions = parse_command_line()\n\t\treturn options[0]\n\telse:\n\t\traise argparse.ArgumentParserError('CommandLine option must be of form [key=value] or [key=value1,key=value2]')\n\n\ndef parse_command_line():\n\t\"\"\"Parse commandline options and extract command line keys\"\"\"\n\toptions"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(usage=usage, description=description,\n\t\t\t\t\t\tmetavar=\"ARGS\",\n\t\t\t\t\t\tprog='python -u {}'.format(args.app)\n\t\t\t\t\t\t\t  )\n\tparser.add_argument(\n\t\t'args',\n\t\thelp=help.format('list of arguments'),\n\t\tnargs=\"?\",\n\t\t\n/*\t\t## Arguments specific to this script (app, config, etc. only)\n\t\t## Argument names\n\t\t\"--app\",\n\t\t\"--config\",\n\t\t\"--version\",\n\t\t\"--quiet"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t# Parse arguments from the command line\n\tparser = ArgumentParser()\n\tfor arg in arguments:\n\t\tparser.add_argument(arg, help='The arguments to parse')\n\n\treturn parser\n\npackage com.github.dreamhead.moco.parser;\n\nimport com.github.dreamhead.moco.MocoHandler;\nimport com.github.dreamhead.moco.MocoHandlerException;\n\nimport java.util.regex.Pattern;\n\npublic class TextPatternParser extends RegexPatternParser {\n    private final MocoHandler"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\timport argparse\n\t\n\targs = Arguments()\n\t\n\targs.parse_arguments(*arguments)\n\t\n\tif args.output_dir is None:\n\t\treturn args\n\t\n\toutput_dir, ext = os.path.splitext(args.output_dir)\n\t\n\toutput_dir = check_folder(output_dir, \".zip\", \".\"+ext[:-1])\n\t\t\t\n\toutput_dir = check_folder(output_dir, \"..\", \"..\")\n\t\n\treturn args.parse_arguments(output_dir, *arguments)\n\n<?php\n\nnamespace Kir"}]}
{"task_id": "62b45679d7d32e5b55cc83a9", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tparser.print_usage()\n\treturn \"\"\n\ndef show_all_args(self):\n\tparser = argparse.ArgumentParser(prog=\"simple_program\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser.add_argument('command', nargs='?', default='simple_program')\n\tparser.add_argument('-n', nargs='?', default=-1, help=\"Use default command line options (-n) --\")\n\tparser.add_argument('-N', nargs='?', default=-1, help=\"Use default command line options (-N) --\")\n\treturn"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tdef add_option_group(parser):\n\t\treturn''\n\tparser.add_argument(\n\t\t\t'-v',\n\t\t\taction='store_const',\n\t\t\tdefault=False,\n\t\t\thelp='enable verbose output')\n\tparser.add_argument(\n\t\t\t'+y',\n\t\t\tdest='yes_no_options',\n\t\t\taction='store_true',\n\t\t\tdefault=False,\n\t\t\thelp='Display the help message rather than all the remaining args')\n\n\n\tparser.add_argument(\n\t\t\t'-f',\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tline = ''\n\tfor arg in parser.visible_args:\n\t\tline +='%s: %s' % (arg.__name__, arg.name)\n\treturn line\n\n\n# TODO(will-i): It's only for unit tests\ndef args(*args, **kwargs):\n\ttry:\n\t\tparser = argparse.ArgumentParser(formatter_class=argformat)\n\t\tparser.add_argument('args', metavar='', nargs=2, **kwargs)\n\t\treturn parser.parse_args(*args)\n\texcept:\n\t\treturn"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tif hasattr(parser,'stream_lines'):\n\t\treturn parser.stream.strline()\n\telse:\n\t\treturn ''\n\ndef parse_argument_flags(parser):\n\t\"\"\"\n\tGiven a argparse.ArgumentParser instance, return its argument flags as a space-separated string.\n\tIf the argument has a name ending in \"--\", this method tries to extract it from the\n\targument as a dictionary containing it as key's key value pair and return it as a string\n\twith spaces.\n\t\"\"\"\n\tif hasattr(parser,'stream_blocks'):\n\t\t"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn [farg.kind for farg in parser.parse_args()]\n\n\ndef get_argument_args(parser, parser_args_name):\n\treturn [parser.parse_args() for _, _, _, _ in zip(\n\t\tparser_args_name._get(),\n\t\t(arg.nargs, arg.name, arg.pos),\n\t)]\n\n\ndef get_arg_kinds(parser, argv):\n\tkinds = []\n\tfor arg, name, pos in zip(argv, argv[1:], (arg.pos,))[:2]:\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn re.sub(r'([a-zA-Z_][a-z0-9_]*)', lambda match: '|'.join(map(*v))+'|',\n\t\tparser_flags(parser.parse_args(arg))\n\t)\n\ndef parse_bool(arg):\n\t\"\"\"Parse an argparse bool argument from an argument of type python-boolean to a boolean.\"\"\"\n\treturn bool(str(arg))\n\ndef parse_path(*args):\n\t\"\"\"Parse a string path into an argparse.ArgumentParser object.\"\"\"\n\tif len(args) == 1"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn parser.flags.prefix + \" [flags]\"\n\n#ifndef QPID_BROKERMANAGER_H\n#define QPID_BROKERMANAGER_H\n\n/*\n *\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t# pylint: disable=line-too-long\n\tdef argument_parser_flags(parser, arg, default=None):\n\t\tflags = '--' if arg.default else arg.help\n\t\treturn flags\n\n\treturn argument_parser_flags\n\n\n# pylint: disable=no-self-use,wildcard-import,invalid-name,protected-access\n# pylint: disable=unused-argument\ndef load_args(*args: str):\n\t\"\"\"Function to load a parsed dictionary from a command line arguments, and set\n\tthe flags in the dictionary with any arguments that"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn ', '.join(parser.parse_args(argnum).args for argnum in [''])\n\n\n# This is only here to avoid a cyclic dependency from the above factory function and to keep the parser as\n# fast as possible.\n_arg_parser = _create_parser('arg', 'Argument', 'This is a more generic parser class that is easy to import from an argparse program.')\n\n\ndef _parse_args_from_param(params, param_types, **kwargs):\n\twith warnings.catch_warnings():\n\t\tparser = _arg_parser.init"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import os\n\tnames = {'width'    : 'width',\n              'align'    : 'alignment',\n              'padding'  : 'padding',\n              'wrap'     : 'wrap',\n              'indent'   : 'indent',\n         "}]}
{"task_id": "62b45665d7d32e5b55cc8365", "prompt": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t# A dictionary here will be used to look for global and non-global arguments.\n\tglobal_arguments: _Optional[Dict[str,Dict[str,Any]]] = None\n\tunparsed_arguments = [k for k,v in unparsed_arguments]\n\ttry:\n\t\tglobal_arguments = {}\n\t\tglobal_arguments.update(v)\n\texcept:\n\t\tpass\n\treturn global_arguments\n\t#else return None, None\n\t\ndef parse_arguments(global_arguments: Dict[str,Any], command: str = None) ->"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tfor _, arg in unparsed_arguments:\n\t\tif \"global\" in arg:\n\t\t\tpass\n\treturn dict(zip(glob_subparsers(), global_arguments()))\n\npackage com.github.kklisura.cdt.protocol.types.metadata;\n\n/*-\n * #%L\n * cdt-java-client\n * %%\n * Copyright (C) 2018 - 2021 Kenan Klisura\n * %%\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport argparse\n\timport sys\n\tif sys.argv[0] == \"\":\n\t\treturn dict.fromkeys(unparsed_arguments, {})\n\treturn parse_args(*sys.argv[1:])\n\ndef parse_args(argv):\n\tparser_name = argv[0].split('.')[0] if argv.has_key() else sys.argv[0]\n\targs = parse_arguments(argv[1:])\n\n\tglobal globals\n\tglobals.update({parser_name: globals})\n\tif len(globals.keys()) == 0:\n\t\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(**unparsed_arguments)\n\tparser.add_argument(\n\t\t'text',\n\t\tdest=\"text\",\n\t\thelp=__doc__,\n\t\tdefault=None,\n\t\ttype=str,\n\t)\n\tparser.add_argument(\n\t\t'namespace_type',\n\t\tdest=\"namespace_type\",\n\t\ttype=list,\n\t\thelp=__doc__,\n\t\tdefault=None,\n\t)\n\n\treturn parser.parse_args()\n\ndef parse_arguments(*unparsed_"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\ttry:\n\t\timport sys\n\t\timport argparse\n\texcept ImportError as e:\n\t\tprint(\"Error:\", file=sys.stderr)\n\t\tprint(__doc__ or \"\")\n\t\tsys.exit(\"Error:\", str(e))\n\n\tglobal argparse_namespace\n\tglobal last_cmd_global_namespace\n\tglobal global_namespace\n\n\tparser_name, namespace = unparsed_arguments\n\tif parser_name == \"global\":\n\t\tcurrent_global_namespace = namespace\n\t\tlast_cmd_global_namespace = parser_name\n\t\tglobal"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tglobal_namespace, command_line_parser = Namespace(), parser_from_arguments(*unparsed_arguments)\n\tglobal_namespace.add_subparser(command_line_parser, description='Parse command line arguments')\n\tparse.parse_arguments(argv)\n\treturn getattr(global_namespace, command_line_parser)\n\n# -- Options for HTML output ---------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  Share a theme with your site or\n# set it to 'jinja2.base' for direct usage.  If using a subdirectory as a"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t# A parser that takes a list of subparsers as an argument and makes a single call to the subparser.\n\tdef parse_arguments1(arguments):\n\t\t# parse_arguments(arguments[0], ['global']) -> greetings\n\t\tglobal_handler = global\n\t\ttry:\n\t\t\tparser = parser_class(arguments[1], global_handler=GlobalHandler)\n\t\texcept argparse.ArgumentParserError as msg:\n\t\t\tsys.stderr.write(msg)\n\t\t\treturn None, None, None, None\n\t\telse:\n\t\t\treturn"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t# Set global variable\n\tglobal _globals\n\t# Initialize global namespace for function return value\n\tglobal _globals\n\t_globals = _parse_arguments.get_namespace(\n\t\t\"global\",\n\t\t__file__,\n\t\t__name__,\n\t)\n\t# Call the parse method to get arguments\n\tparse_function=global_namespace.parse_function\n\tparse_function(unparsed_arguments)\n\t# Pass the arguments back to the global namespace (only need to return this global namespace if it is called)\n\tglobal_namespace.parse_function = parse"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tname, value = unparsed_arguments.pop(0).strip().split(' ')\n\targs = {}\n\tglobal_namespace = global_namespace\n\tif 'py' in value or 'numpy' in value or 'pandas' in value:\n\t\traise ArgumentError('A parameter for py is \"numpy\" and pd is \"pandas\".')\n\tfor p in parse_args_line(name, value, global_namespace):\n\t\tif p[0] == '=':\n\t\t\targs[p[1]] = p[2]\n\tif'sys' in"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tsubparsers = set()\n\tin_args_error = False\n\tsubparsers.add(default_root.parser)\n\twith StringIO()\n\t\twhile bool( in_args ):\n\n\t\t\tline = StringIO.StringIO()\n\t\t\tline.write('#include \"src/io/io_utils.h\"\\n')    # include the line number in the generated file\n\t\t\tline.write(\"#include \" + io.path + \"\\n\")         # include the path of the generated file\n\t\t\tsubparsers.add"}]}
{"task_id": "62b45665d7d32e5b55cc8364", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tif'subparser_name' in unparsed_arguments:\n\t\targs, remaining_args = unparsed_arguments['subparser_name'], unparsed_arguments['remaining_arguments']\n\t\treturn parse_subparser_arguments(args, subparsers), remaining_args\n\telse:\n\t\treturn parse_subparser_arguments(unparsed_arguments['arguments'], subparsers)\n\n// Copyright 2018 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n//"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparsers = []\n\tfor arg in unparsed_arguments:\n\t\tsubparser_name = arg.parser_type.split(\":\")[0]\n\t\tif arg.namespace == subparsers_namespace:\n\t\t\tsubparsers.append(arg.namespace)\n\t\t\tcontinue\n\t\telse:\n\t\t\tsubparsers.append(subparsers.pop(arg.namespace))\n\treturn subparsers\n\n\ndef parse_arguments(unparsed_arguments, arguments, remaining_arguments):\n\tparsed = parse_subparser_arguments(unparsed_arguments, arguments)\n\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t# Extract the subParser for this subparser\n\tsubparser = subparsers[subparsers.index(unparsed_arguments[1])][0]\n\tif subparser.parser not in argparse.PARSERS:\n\t\tlogging.log(colors.ERROR)\n\t\tLOGGER.exception(\"Invalid argparse\")\n\t\treturn\n\tnamespace_to_subparser = subparsers[subparsers.index(unparsed_arguments[1])][1]\n\tnamespace = subparsers[0].parser.get_namespace(namespace_to_subparser)\n\n\tname_to_args"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tresult = {}\n\targ, shot = [], []\n\twhile len(shot) > 0:\n\t\tname = shot.pop(0)\n\t\tsubparser, namespace = subparsers.parse_subparser(subparser_from_name(name), name)\n\t\tnamespace = namespace.subparser.name\n\t\ttry:\n\t\t\tnamespace, subparsers = namespace.parse(subparser, namespace)\n\t\texcept UnregisteredSubparser:\n\t\t\ttry:\n\t\t\t\tnamespace, subparsers = namespace.parse(name)\n\t\t\texcept Un"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\"\"\"\n\tUsage:\n\t\ta,b = parse_subparser_arguments({\"a\":1}, subparsers=[restructuredtext_subparser])\n\t\"\"\"\n\n\tresult_elements = []\n\t\t\t\t\t\t\n\tfor parser in subparsers:\n\t\ttry:\n\t\t\tparser = parser.subparser\n\t\texcept AttributeError:\n\t\t\tcontinue\t# skip subparsers from subparser name and not found in remaining arguments\n\n\t\targ_list = () \t\t# initialized for action's return\n\n\t\tfor arg in parser.parse_args():\n\t\t\tif arg"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tglobal_subparser_parser, global_subparser_kwargs, current_subparser_args, current_subparser_kwargs, subparser_result, subparser_kwargs, subparser_position = {}, global_subparser_position\n\tif global_subparser_position.get(subparser_name, 0) > global_subparser_position.get(subparser_name, 0 + 1):\n\t\traise ValueError('Global subparser {} is already used with position {}'.format(subparser_name, global_subparser_position.get(subparser_name, 0)))\n"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tif not subparsers:\n\t\treturn shot.get_subparser_arguments()\n\n\tshots=shot.get_subparsers_to_load(subparsers=subparsers)\n\tresults=[]\n\tshot_args=[]\t\t# argument dictionary for this shot\n\tresults_list=[]\t\t# result list for subparser action execution\n\n\tfor shot_arg in shots:\n\t\tif not shot_arg.action in subparsers:\n\t\t\t# something went wrong with the action list, continue to add results to results_list\n\t\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\treturn subparsers.parse_subparser(unparsed_arguments)\n\npackage com.mashibing;\n\nimport com.mashibing.bean.Goods;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\npublic class TeacherInfo {\n    // \u6dfb\u52a0\u6392\u6c34\u5546\u54c1, \u76ee\u524d\u4ec5\u7528\u4e8e\u4e00\u7ea7\u5546\u54c1\uff0c\u6392\u6c34\u539f\u56e0\u4e3a\u4e3a\u4ef7\u683c\u9650\u4ef7\uff0c\u4f46\u4ef7\u683c\u6bd4\u6700\u65b0\u4ef7\u683c\u8981\u5927\u4e8e\u7b49\u4e8e\u65b0\u4ef7\ufffd"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t# Construct a dict from the first entry in unparsed_arguments and the second entry in subparsers\n\tcommand_parser_kwargs = {}\n\tif len(unparsed_arguments)!= 0:\n\t\tcommand_parser_kwargs = unparsed_arguments[0].get('command_parser_kwargs', {})\n\telse:\n\t\tcommand_parser_kwargs = {}\n\t\n\t# If user wants additional arguments than provided, parse those\n\tif len(unparsed_arguments) >= 1:\n\t\tparsed_arguments = unparsed_arguments[1:]\n\t\t\n\tif len("}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparsers = {}\n\targs_as_arguments = {}\n\tfor a in unparsed_arguments:\n\t\t#if a == \"-b\":\n\t\t#\tsubparser_name = \"-b\"\n\t\t#\tsubparser_args = {}\n\t\t#elif a == \"-repo\":\n\t\t#\tsubparser_name = \"-repo\"\n\t\t#\tsubparser_args = {}\n\t\t#else:\n\t\t#\t# NOTE: A subparser named'repo' or 'b', is optional\n\t\t#\tsubparser_name_"}]}
{"task_id": "62b45665d7d32e5b55cc8363", "prompt": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tdef make_parser():\n\t\tparser = ParserBuilder()\n\t\treturn parser\n\n\tsubparsers = []\n\tfor item in _make_parser_def.defaults:\n\t\tfunc_name = item[0]\n\t\targ_name = item[-1]\n\t\tsubparsers.append(make_parser())\n\treturn ParserWithKeywordBuilder(**subparsers)\n\ndef _setup_mainapp_cli_parser(args):\n\tdef mainapp_cli_parser():\n\t\tsubparsers = make_parsers()\n\t\tparser = subparsers[0"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t# The parser for a single-line comment that has the start of a line\n\tall_parsers = []\n\n\t# Each top-level subparser has a list of parsers. (Subparsers that were\n\t# defined in the parser definition above will appear in such a list\n\t# for parsing purposes)\n\tall_subparsers = []\n\n\tfor p in all_parsers:\n\t\t# Get the parser that owns the top-level line from the list\n\t\tp = p.find_parser(\".\", \"allparser\")\n\n\t\t# If it's a comment,"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tpnames = ('parse_function_def', # TODO: check\n\t\t\t'parse_method_def', # TODO: check\n\t\t\t'parse_function_call', # TODO: check\n\t\t\t)\n\tfuncs = {}\n\tfor pname in pnames:\n\t\tfunc = Parser().get_parser(pname).parse_function_def\n\t\tfunc.set_function_defaults(defaults=parse_function_defaults())\n\t\tfuncs[pname] = func\n\treturn funcs, pnames, funcs, pnames, funcs\n\n/* USER CODE BEGIN Header */\n"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tdef getitems(parser):\n\t\treturn []\n\n\tparent_parser = None\n\tif not parent_parser is None:\n\t\t# parent parser\n\t\tparent_parent_parser = parent_parser\n\t\t# the list of subparsers\n\t\tparent_subparsers = []\n\t\t# the global parser stack (only when parsing)\n\t\tglobal parser_stack = parser\n\n\t# get subparsers\n\tsubparsers = getitems(parent_top_level_parser) + \\\n               "}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tif _curpy3._is_py3 and _curpy3._is_py3_version(3, 7):\n\t\tparser = parser_class(command_option)\n\telse:\n\t\tparser = _curpy3._py3_opt._build_parser\n\t#print 'call build_parser()...', repr(parser)\n\tparsed = _curpy3._py3_opt._call_top_level_parser(parser)\n\t#print 'parser_class.make_parsers()', repr(parsed)\n\tparser_class = _curpy"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = parser_class()\n\tparser.add_subparsers(subparsers_type())\n\n\tsubparsers_dict = parser.parse_args()\n\tparser.subparsers.add_parser('file', help=('files contain the filename, or a relative path, or a fileobject '\n\t\t'or dict of fieldnames to values'), file=parser.print_filename)\n\tparser.subparsers.add_parser('text', help=('files contain the filename, or a relative path, or a textobject or '\n\t\t'dict of fieldnames to values'),"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom collections import deque\n\n\tparser = parserClass(parser_name, parserOptions, toplevel_is_type,\n\t\tparserVersion,\n\t\toutput_encoding=DEFAULT_OUTPUT_ENCODING,\n\t\tindent_char=DEFAULT_INDENT_CHAR,\n\t\tunindent_comment_chars=DEFAULT_UNINDENT_COMMENT_CHARS)\n\n\tif toplevel_is_type and output_encoding is None:\n\t\treturn parser\n\n\tinput_encoding = get_input_encoding()\n\tif input_encoding:\n\t\tparsedResult = parser.parse"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tresult = []\n\tfor line in _run_parser(grammar_name, default_options[\"lexer_options\"][\"default_options\"][\"grammar\"], default_options[\"grammar\"].lower()):\n\t\tresult.append(parse_string(line, ast.Literal))\n\treturn result\n\n\ndef parse_strings(line, start_index, end_index):\n\t\"\"\"\n\tThis function parses a literal block into a single string.\n\tThe arguments in line are the literals from the block.\n\t\"\"\"\n\t# A block can contain a simple string literal,\n\t# or it can"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\treturn tuple(\n\t\t\tParser(namespace, name, description,\n\t\t\t\t\tdefault_arguments_separator,\n\t\t\t\t\tparse_all_files=True,\n\t\t\t\t\tusage=[make_parsers.usage] +\n\t\t\t\t\t\t[(namespace, name, description,\n\t\t\t\t\t\t\t\t(default_arguments_separator,\n\t\t\t\t\t\t\t\t\tvalue))\n\t\t\t\t\t\t                   for value in DEFAULT_ARGUMENTS])]\n\n\ndef make_description(parser, namespace_name):\n\t\"\"\"Builds description from a parser and its subparsers.\"\"\"\n"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparsers = []\n\tsubparsers = {}\n\treturn parsers, subparsers\n\n#pragma once\n\n#include <functional>\n#include <optional>\n#include <vector>\n\nnamespace Godot {\n    //\n    // A vector of objects that implements Godot::ISignedObject interface\n\n    class Vector2D {\n        public:\n            /// Represents a vector object\n            class ISigned2D {\n "}]}
{"task_id": "62b438ba66fea644fe22cca2", "prompt": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\truamel.yaml.nodes.NodeBuilder = nodes.NodeBuilder or borgmatic_defaults\n\tyaml_nodes = dict()\n\tyaml_nodes[ScalarNode] = nodes.ScalarNode\n\tyaml_nodes[MappingNode] = nodes.MappingNode\n\tyaml_nodes[ScalarNode, ScalarNode] = nodes.ScalarNode, nodes.ScalarNode\n\tyaml_nodes[ScalarNode, MappingNode] = nodes.ScalarNode, nodes.ScalarNode, nodes.ScalarNode\n\t\n\tfor sub_node in nodes:\n\t\tif not sub_node or isinstance(sub_"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tdef deep_merge(node):\n\n\t\tif node.path:\n\t\t\tnode.nodeType = NODE_TYPE_BORG\n\t\tif not node.path:\n\t\t\tnode.path = node.path_node\n\t\t\t\n\t\tif node.path.parent:\n\t\t\tnode.parent = deep_merge(node.path.parent)\n\t\tif node.path.value:\n\t\t\tnode.value = deep_merge(node.path.value)\n\n\t\tif not node.path or not node.path.parent:\n\t\t\t# we"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tretention = options['retention']\n\n\timport ruamel\n\n\tfor key, value in options['nodes'].items():\n\n\t\tif '=' in value:\n\n\t\t\tname = value.split('=')[1]\n\n\t\telif key.startswith('tag:yaml.org,2002:'):\n\n\t\t\tname = key.split('-'):[2]\n\t\t\tkey = name[1].split(':', 1)\n\t\t\tvalue = name[1:]\n\n\t\telif key == 'keep_hourly':\n\n\t\t\tretention = True\n"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tconfig = DeepConfigParser().parseString(r_yaml_str)\n\tretention_sections = config.keys()\n\tretention_str = retention_sections[0]\n\t\n\tif r_yaml_str.endswith('.yml'):\n\t\tr_yaml_str = read_yaml_file_without_tags(r_yaml_str)\n\t\t\n\truamel_yaml_config = config\n\n\tdef _deep_merge_configuration(current_values, merged_values, section):\n\t    if section =='retention':\n\t     "}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tnode_values = list(re.split(r'(?<!:[0-9a-z]|\\[\\)[ \\t]*[0-9]+(?P<key>\\-\\w)\\s*=\\s*?(?P<value>[^,\\[\\]]*)$', nodes.data['yaml'], flags='-')\n\tif 'key' not in nodes.data: nodes.data['key'] = node_values[0]\n\tif 'value' not in nodes.data: nodes.data['value'] = node_values[1]\n\tif 'attributes' not in nodes.data:"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t# add a custom node handler to every configuration node list.  This is necessary\n\t# because the parser will use config_parse_data to parse one config dict into the\n\t# list of nodes.\n\tif not isinstance(nodes, (list, array)):\n\t\tnodes = [nodes]\n\n\tif len(nodes):\n\t\tdeep_merge_list = nodes[:]\n\n\t\t# now we need to find any nodes that contain Duplicate keys in the same position\n\t\tfor a_str in nodes:\n\t\t\tfound_node = False\n\t\t\tfor node in deep"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tdef load_as_merged(files):\n\t\t\"\"\"\n\t    read a merged file in from the borgmatic configuration and return it as a mapping.\n\t    Return None if the file can not be loaded for some reason\n\t    \"\"\"\n\t\twith open(os.path.join(os.path.dirname(__file__), 'borgmatic-configuration-data.yml'), 'r') as input:\n\t\t\tdef load():\n\t\t\t\treturn dict(input.read().strip().split())\n\t\n\t\t# if we can't read our mapping, then we"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\treturn nodes\n\ndef borgmatic_node_set(self):\n\t\"\"\"Returns the nodes in the configuration as an unordered mapping of borgmatic keys to nodes.\n\t\n\tReturns a dictionary of the borgmatic values corresponding to duplicate keys (e.g.\n\t(r'1', 'keep_hourly'), and their values (r'2', 'keep_daily'). If there are colliding\n\tkeys with non-mapping nodes then the given result will contain them and this result will\n\tnever contain keys with a non-MappingNodes value (e.g., integers or strings"}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\tdef recursively_deep_merge_nodes(nodes):\n\t\t\"\"\"\n\t\tRecursive implementation of deep merge_nodes. Recursive implementation of this function\n\t\tmerges nested borgmatic configurations into a single merged configuration with the same\n\t\ttags as the configuration itself. The given nodes are added as deep nodes to the resulting\n\t\tconfiguration.\n\t\t\"\"\"\n\t\tnodes = [n for n in nodes if deep_merge_nodes(n)]\n\t\tfor node in nodes:\n\t\t\tnode.deep_merge_nodes(nodes)\n\treturn deep_merge_nodes("}, {"generated_text": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\ttree = ast.literal_eval(\n\t\tyaml.safe_dump(ruamel.yaml.nodes.load_yaml(ruamel.yaml.safe_load(YAML_CONFIG_STRING)), default_flow_style=True)\n\t)\n\truamel_yaml.nodes.merge_nodes(yaml.safe_load(ruamel.yaml.safe_load(yaml_config_string)), yaml_config_string, tree)\n\n\ndef get_configuration_name_from_yaml_section(section_yaml):\n\t\"\"\"\n\tName of the yaml section that contains"}]}
{"task_id": "62b438b666fea644fe22cc78", "prompt": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tdef newparser():\n\t\targs = argparse.ArgumentParser(prog=__name__)\n\t\targs.add_argument('--output')\n\t\tparser = argparse.ArgumentParser(prog=__name__, \n\t\t\t\t\t\t\t\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\t\targs.add_common_arguments(parser)\n\t\treturn args\n\n\treturn newparser\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more contributor license\n * agreements. See the NOTICE file distributed with this work for additional information regarding\n * copyright ownership. The ASF licenses"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(usage='', add_help=False,formatter=argparse.ArgumentDefaultsHelpFormatter())\n\n\tparser.add_argument('cmd',          type=str,    help='Command that the executable and command line arguments should run')\n\tparser.add_argument('-e',           type=str,    help='Additional information to be available along with arguments')\n\tparser.add_argument('-t',           type=str,   "}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\timport argparse\n\n\t# Parse options\n\tparser = argparse.ArgumentParser(description='Show the usage of the <script> <command> command in the given arguments.')\n\tparser.add_argument(\n\t\t'script=',\n\t\thelp=\"A full text of the command. This is the command name without the trailing '.'.\"\n\t)\n\tparser.add_argument(\n\t\t'command', \"-m\", help=(\n\t\t\t\"The command that is invoked. Example: 'help -m foo' or 'help -m help foo'\")\n\t)\n\t# Parse"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(prog='help',\n\t\tusage=('py -m help [ options  ] args-file [ --args ]'\n\t\t\t\" [ -h [--h] args ]\"),\n\t\tversion=version)\n\tparser.add_argument('args_file', nargs=2, help='specify a json file containing the argument '\n\t\t\t\\'(example: \"$0\" is \"hello\").')\n\tparser.add_argument('args', nargs=1, nargs='?', help='arguments as argument list')\n\tparser.add_argument('-h', '--"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\targuments = parser.parse_args(arguments)\n\tparser.parse_args(arguments)\n\treturn arguments\n\ndef build_command_line(arguments):\t# ToDo(rathodar): should be part of Command class\n\tparser = ArgumentParser(\n\t\tprog=\"scapy.cli.commands\",\n\t\tdescription='Commands are not very useful. Some actions are \"unlimited\" to the\n\t\t\"open\" sequence in the main program.'\n\t)\n\tparser.add_argument(\n\t\t'command',\n\t\thelp=\"The Command file to"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\targuments, subparsers = parser.parse_arguments(*arguments)\n\tif subparsers:\n\t\treturn _create_new_subparsers(subparsers)\n\treturn _create_new_subparsers(ArgumentParser())\n\ndef _subparser(parser, options={}, verbose=False):\n\t'''\n\tThis returns an argument parser based on the options.\n\t'''\n\tif options:\n\t\tif len(options) > 1:\n\t\t\traise ValueError(\"Option 'options' should be a Python dictionary (use \"--\" for any key)\")\n\t\tif options.get('"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser.add_argument()\n\n\tparsed_arguments = []\n\ttry:\n\t\tparsed_arguments.extend(arguments)\n\texcept:\n\t\tpass\n\n\treturn parsed_arguments[0]\n\n\n# Return python command, run script if it doesn't already exist\ndef execute(script_path, args):\n\ttry:\n\t\tprint(\"Executing: %s\" % script_path)\n\t\tcmd_string = \"exec -i %s \" % script_"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tfrom django.conf import settings\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\n\t\t\"arguments\",\ttype=str, nargs='*', help=\"the arguments in which to parse this script\")\n\targument_parser = ArgumentParser()\n\tparser.set_defaults(default=args, description='')\n\t# We use this to find arguments from the list of registered users\n\tparser.add_argument(\"-a\",\ttype=\"str\", help=\"Comma separated list of user names\")\n\t# We use this to find arguments from the list of registered users\n\t"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tdef parse_arguments(*arguments):\n\t\ttry:\n\t\t\t# check that there is a helpfile, if yes the user may have already logged a problem.\n\t\t\tparser = ArgumentParser(*arguments)\n\t\t\tparser.add_argument(\"--helpfile\", default=False, help=\"path to help.txt file\")\n\t\t\tparser.set_defaults(func=argument_parser)\n\t\t\tparser.parse_args()\n\t\t\treturn parser\n\n\treturn parse_arguments\n\n#include \"mupdf/fitz.h\"\n#ifndef _MSC_VER\n"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tprint >> sys.stderr, \"Usage in this script:\\n {} {}\".format(sys.argv[0] == \"example\", sys.argv[1])\n\tprint >> sys.stderr, \"Arguments are:\",\n\t\tsys.argv[0] in argparse.ArgumentParser.__call__.__doc__,\n\"\"\"\nThe arguments are:\n    - name: - use these to distinguish between different parsers\n\t\t- example:   test,    help,  test_examples,\n\t\t- help: test,    help,  test_help_examples,\n\"\"\""}]}
{"task_id": "62b438b666fea644fe22cc72", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tparser = ArgumentParser(formatter_class=Formatter)\n\targs = parser.parse_args(['-i', 'name1,name2', '-F', '1,2','3'], parser)\n\treturn  sorted(set(parser_flags(x) for x in args.values()))\n\n\ndef test_flag_order_as_flag_name(parser):\n\tparser.add_argument('--a', '--i',\n\t\thelp='foo', action='store', type=action_flag, default=True,\n\t\tmetavar=('A',),\n\t\t"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tparser.add_argument(\"--verbose\", action='store_true', dest='pretty',\n\t\t\t\t\t\thelp='show more info when constructing the argument parser')\n\tparser.add_argument(\"--short-help\", action='store_true', dest='short_help',\n\t\t\t\t\t\thelp='show short help when constructing the argument parser')\n\tparser.add_argument(\"--all\", action='count',\n\t\t\t\t\t\tdefault=0, type=int, help='not for advanced usage')\t# not an error\n\tusage = \"\"\"USAGE:\\\n\\n%prog --parser=[-h] --short-"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tparser.add_argument\n\n#region Copyright Syncfusion Inc. 2001-2021.\n// Copyright Syncfusion Inc. 2001-2021. All rights reserved.\n// Use of this code is subject to the terms of our license.\n// A copy of the current license can be obtained at any time by e-mailing\n// licensing@syncfusion.com. Any infringement will be prosecuted under\n// applicable laws. \n#endregion\n\nusing System.IO;\nusing System.Reflection;\nusing System.Runtime.InteropServices;\n\nnamespace Xamarin.Mac.Tools."}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tfrom argparse import Flags\n\tparser.add_argument('args', nargs='*', default=[], metavar='ARGS', help='specify command line arguments')\n\treturn Flags(parser.parse_args())\n\n/*****************************************************************************\n\nCopyright (c) 1996, 2016, Oracle and/or its affiliates. All Rights Reserved.\n\nThis program is free software; you can redistribute it and/or modify it under\nthe terms of the GNU General Public License, version 2.0, as published by the\nFree Software Foundation.\n\nThis program is also distributed with certain software (including but not\nlimited to"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tdef arg_format(value):\n\t\tif isinstance(value, bytes):\n\t\t\tvalue = value.decode()\n\t\treturn value\n\tdef arg_help(parser, args):\n\t\tdef get_arg_type(arg):\n\t\t\tif arg[2] == 'S':\n\t\t\t\treturn 'bool'\n\t\t\telif arg == 1:\n\t\t\t\treturn 'bool'\n\t\t\telif arg == 2:\n\t\t\t\treturn 'int'\n\t\t\telif arg == 3:\n\t\t\t\treturn 'int'\n\t\t\telif arg == 4:\n\t\t\t"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\targv = parser.positional_options\n\targv = [v[0] for v in argv]\n\treturn \" \".join(argv)\n\n\ndef parse_single_arg_description(argv):\n\t\"\"\"Parse a single argument description string and return a description\n\tinstance.\n\n\tOnly useful for user-supplied (command-line) arguments like -h, -v, -x, -d, and others.\n\n\t:param argv: The remaining arguments.\n\t:type argv: list[tuple[str,...]]\n\n\t:return: A description or None"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tf = parser.parse_args([])\n\tparsed_flags = f.split()\n\treturn f.__name__ + '.' + f.__class__.__name__ + '.' + f.__doc__.split()[0]\n\ndef _wrap(arg):\n\t\"\"\"Remember the original value for parsing a single arg.\n\n\tIf the user requests a single arg, or a tuple of multiple, return the initial\n\tvalue (which is assumed to be an int).\n\tIf it is a list, return the value from each element in brackets,\n\telse the default value will hold.\n\n\t"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tfrom argparse import SUPPRESS as SUPPRESS_\n\tflags = []\n\tfor arg in parser.positional_args:\n\t\tif not SUPPRESS() and not SUPPRESS_():\n\t\t\traise TypeError('Suppressed:'+ arg.help)\n\t\tif arg.default is not SUPPRESS_():\n\t\t\tflags.append(arg.default)\n\n\tif flags:\n\t\traise TypeError('The following flags have unrecognized values: %s' % ', '.join(flags))\n\texcept TypeError as e:\n\t\traise TypeError(\"Unable to parse arguments of type %"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tfflags = [x for x in vars(parser).keys() if not x.startswith('_')]\n\tfflags.insert(0,'verbose')\n\treturn''.join(fflags)\n\n\ndef format_help(parser, args):\n\t\"\"\"\n\t\tPrint help text for the format options of a single argument list. The format\n\t\toptions allow for easy access to the argument list without taking into account\n\t\tuser specified options.\n\t\"\"\"\n\tif len(args) == 1:\n\t\tprint(f\"\\n{format_help(parser, args"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tif parser.option_strings:\n\t\toptions = []\n\t\tfor opt in parser.option_strings:\n\t\t\tflags = opt.split('+')\n\t\t\tflags.append(parsed.FLAG.FLAG_SPACE)\n\t\t\tif len(options) == 1:\n\t\t\t\tflags.append(options[0])\n\t\t\tflags.append(parsed.FLAG.FLAG_SEPARATOR)\n\t\t\toptions = flags\n\t\treturn '.'.join(options)\n\treturn ''\n\n# TODO: implement parse and store a description of this in sys.argv\ndef _"}]}
{"task_id": "62b438b566fea644fe22cc70", "prompt": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tdef bash_completion():\n\t\timport sys\n\t\timport borgmatic_bash as borgmatic\n\t\tcmd = borgmatic.cli.borgmatic_bash.borgmatic_bash.get_command(borgmatic_bash.get_current_borgmatic_command() +'--exit')\n\t\tprint cmd\n\t\tsys.stdout.flush()\n\t\targs = cmd.split()\n\t\tif len(args) > 1:\n\t\t\tprint '[Error]: the command-line argument has more than one argument in"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tfrom borg.scripts.bash_completion import BashCompletion\n\treturn BashCompletion()\n\n# This file is part of project Sverchok. It's copyrighted by the contributors\n# recorded in the version control history of the file, available from\n# https://github.com/sweet-sdi/sagemath/\n#\n# Copyright 2016-2021 the SageMaths project authors.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom functools import partial\n\nimport numpy as np\n\nfrom sverchok.experimental.sdistances.base"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tcmd = sys.argv[1]\n\n\ttry:\n\t\t# The borgmatic main() is not a complete shell. Use the shell command as-is.\n\t\tcommand = cmd\n\texcept IndexError:\n\t\t# No borgmatic command is complete\n\t\treturn \"borgmatic: %s\" % (sys.executable,)\n\n\tif command.startswith('python') or not commands[command].startswith('python:'):\n\t\traise RuntimeError(\"Unexpected borgmatic command: %r\" % (command,))\n\n\t# Find the next shell command\n"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tparser_args = borg_args\n\targs, _ = parser.parse_known_args(parser_args)\n\tif len(args) == 0:\n\t\tparser.print_usage()\n\t\texit(1)\n\telse:\n\t\tborg_args = args[0]\n\n\t# If the provided borg_args does not contain an arg, then just ignore.\n\tif not borg_args:\n\t\treturn \"\", None # TODO\n\n\t# Look up the arg's name in the borg_args lookup table\n\tborg"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(add_help=False, description=\"Borgalma's finger borgmatic command\")\n\tparser.add_argument('arguments', nargs='*', type=str,\n\t\t\t\t\t    help=\"list with arguments to execute.\")\n\tparser.add_argument('-b', action='append', dest='actions', nargs='*', \n\t\t\t\t\t\t  default=[\"echo\", \"exit\\n\", \"exit\"], \n\t\t\t\t\t\t  help=\"Actions to execute.\")\n\tparser.add_argument('-v', action='version', version='%(prog)s ["}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\treturn \"\"\"\n\t#!/bin/bash\n\n\t# borg money\n\texec \"$ \"$ \\\"$_\".*$\\\" \"\n\n\t# borg's borg money\n\tfor i in 1,1:\n\t\t# add 1 to counter-1\n\t\texec \"$ echo '$\\\"$i\\\"\"\n\n\t# borg's fee\n\texec \"$ \"$ \\\"$_\\\" -s $\\n\"\n\n\t# borg fee\n\t# NOTE: if you don't see * all * in the borg. money, don't continue\n\t# because"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\timport re\n\tfrom typing import Any\n\tfrom typing import Optional\n\n\n\t# Add borgmatic's help\n\tdef _help_help(self):\n\t\tif self.type == \"completions\":\n\t\t\treturn '''# Usage\n\n\n\\s+\t  This is a borgmatic command.\n\\s+\t\nThis is a borgmatic command. The borgmatic's command-line argument must specify your borgmatic\ntype (completion, borgmatic completion scripts that support a different, but specific, of the\ncompletion's type).\n\nThis command is"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\timport re\n\timport ast\n\n\tdef bash_completion():\n\t\tparser = abst.Borgmatic()\n\t\tparser.add_subparser(\"help\", help_subparser)\n\t\tparser.add_subparser(\"version\", version_subparser)\n\n\t\tsubparser = parser.get_subparser(borgmatic_args=True)\n\n\t\tsubparsers = [subparser]\n\t\tdef parse(command):\n\t\t\tsubparsers.append(subparser)\n\n\t\t# Add in this first option to allow for commands that"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tscript = \"\"\"\n{\n\tbash completion}\n}\n\"\"\"\n\n\tdef get_completor_completion(context):\n\t\toutput = \"\"\n\t\tprint(\"Enter borgmatic command as:\",file=sys.__stdout__)\n\t\twhile True:\n\t\t\tch = context.get_input_stream().readline()\n\t\t\tif ch == \"\\n\":\n\t\t\t\treturn output\n\t\t\toutput += ch\n\t\n\t\treturn output\n\n\treturn get_completor_completion\n\ndef bash_completion():\n\tcontext = Context(\"bash\")\n\targs ="}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\treturn BorgmaticBashCompletion(\n\t\tos.getenv(\"BORGMANIC_ARGS\") or borgman.config['args'])\n\nborgman.config['shell'] = borgman.borgman._shell_options\n\n\nif __name__ == '__main__':\n\tcommand_args = list(sys.argv)\n\tusage_message = 'Usage: borgman [command args]\\n'\n\twith open(os.path.join(command_args[0], 'python-config')) as source:\n\t\tsys.argv[:] = ["}]}
{"task_id": "62b438a266fea644fe22cc2e", "prompt": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tfrom argparse import ArgumentDefaultsHelpFormatter\n\tfrom inspect import getfullargspec\n\n\tparser = getargspec(arguments=unparsed_arguments)\n\tparser.formatter_class = ArgumentDefaultsHelpFormatter\n\n\tnamespace = Namespace(parser.parser)\n\targname = string.itos(parser.name)\n\n\tnamespace.init(globals())\n\tnamespace.arg = Namespace.ArgSpecWithArgs(parser, 0)\n\n\tclass_name = Namespace.arg.name\n\n\tif getfullargspec(parser.args).args:\n\t\tnamespace = Namespace"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = subparsers.add_parser('custom-sub')\n\tparser.add_argument('-d', '-d', '-dot', dest='dot', help=\"d or D\")\n\tparser.add_argument('--log', '-ll', dest='log', help=\"log\")\n\tparser.add_argument('--time', '-tt', dest='time', default=7200, help=\"time in seconds\")\n\tparser.add_argument('--format', '-fr', dest='format', default='json', help=\"string to JSON format\")\n\tparser.add_argument('--type', '-tf"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tfrom reapetag.parser.argparse import parse_args_line, arg_not_found\n\tfrom reapetag.parser.argparse import subparser_found\n\tfrom rafthandler.parse import parse\n\n\tglobal sys\n\tif hasattr(sys, \"frozen\"):\n\t\t# if we are in frozen mode then return a dictionary of subparsers\n\t\t# (including arguments)\n\t\treturn {key: sub_namespace(subparser_found, key)\n\t\t\t  for key in subparser_not_found.keys()}\n\telse:\n\t\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tglobal G, parser, subparser\n\tif G[\"num_subparsers\"]:\n\t\tparser = G[\"subparsers\"]\n\telse:\n\t\tparser = G[\"global\"]\n\tG.num_subparsers = G[\"num_subparsers\"] = 1\n\tnew_parser = G.add_parser(parser)\n\tglobal G\n\tsubparser_names = list(subparser.name for subparser in G[\"subparsers\"])\n\tsubparser_names = subparser_names[:G[\"num_subparsers\"]]\n\tG[\"parsers\"] = [sub"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter\n\t)\n\tfor t in unparsed_arguments:\n\t\tif t in _PYTYPE_NAME_TO_NAMESPACE:\n\t\t\tparser.add_argument(\n\t\t\t\t*unparsed_arguments[t],\n\t\t\t\ttype=t,\n\t\t\t\thelp=__doc__,\n\t\t\t\t)\n\t\t\tcontinue\n\t\tparser.add_argument(t, metavar=\"MODULE\", help=\"Module name or module object\")\n\n\treturn parser.parse_args()\n"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tfrom builtins import input\n\tfrom builtins import str\n\tfrom builtins import range, range_from_version\n\tfrom builtins import zip\n\n\t# get version info for current release\n\t_VERSION = sys.version_info[0]\n\n\twhile True:\n\t\tif len(_VERSION) > 0:\n\t\t\tparts = str(_VERSION)[:-1] + 'es'\n\t\t\tv = input(parts).strip()\n\t\telif len(_VERSION) == 0:\n\t\t\tif '-' in _VERSION:\n\t\t\t\tname, version = _VERSION"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t# get the subparsers from the command line\n\tns = sys.modules[globals().get(\"default_ns\")]\n\targs = ns.get_args(\"parser\",\n                [ unparsed_arguments[0], \",\" ] + unparsed_arguments[1:] )\n\treturn args.__dict__  # type:  typing.Dict[str, typing.Any]\n\n\ndef run_subcommand(subcommand_prefix, options, subparsers):\n\tparser = subparsers[subcommand_prefix]\n\tparser"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog='bayes',\n\t\tdescription=\"Calculates P-values for the Gaussian distributions (log/sum) of the values at each sample point (a,b,c) on average. A Bayes model is available for calculating P-values for each sample. \\n\\n \\n \\n Usage:\\n\\n\\n <bayes> calc-mean-samples-p-values <input_dir> <output_dir> <gaussians_path> [<gaussians_path>...] [<gaussian/path>..."}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tif not (is_cli: True):\n\t\traise PEP420Exception(\"PEP 420 package is not installed, exiting.\\n\"\n\t\t\"If this script supports packages, pip packages must be installed.\\n\")\n\n\tglobal _cli_subparser\n\n\tsubparser = get_subparser(_cli_subparser)\n\t# Get the namespace for the subparser\n\tif subparser:\n\t\tsubparser__pkg = subparser.__module__ + \".\" + subparser.__name__\n\telif \"global\" in os.environ:\n\t\tsubparser__pkg"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t(cmd, *args, **kwargs) = (str(unparsed_arguments[0]), unparsed_arguments[1:], unparsed_arguments[2:])\n\n\tdef parseDoc(doc):\n\t\twith open(app_output_files[doc['pid']]['doc_info'], 'r') as outfile:\n\t\t\tdoc_str = outfile.read()\n\n\t\t\tdoc['pid'] = int(doc_str)\t# this matches the namespace that was written in the doc\n\t\t\tdoc.update({\n\t\t\t\t'pid': doc_str,\n\t\t\t\t"}]}
{"task_id": "62b438a266fea644fe22cc2d", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tfrom argparse import UnavailableArgumentError, add_help_option, formatter, ArgumentGroup\n\ttry:\n\t\targs = unparsed_arguments.keys()[0]\n\texcept:\n\t\treturn unparsed_arguments, (), ()\n\t# Subparser name\n\tsubparsername = '%s(%s)' % (subparsers.__name__, str(subparsers.__dict__[subparsername]))\n\ttry:\n\t\tkwargs = unparsed_arguments.get(subparsername, {})\n\texcept KeyError:\n\t\tparser = None\n\telse:\n\t\tfor key,"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tnamespace_dict = {}\n\tfor arg in unparsed_arguments:\n\t\tnamespace_dict.update({ arg.name: parse_subparser(arg) \n\t\tfor arg in unparsed_arguments[arg.name]  })\n\treturn namespace_dict\n\n/* ========================================================================\n * Copyright (c) 2005-2017 The OPC Foundation, Inc. All rights reserved.\n *\n * OPC Foundation MIT License 1.00\n * \n * Permission is hereby granted, free of charge, to any person\n * obtaining a copy of this software and associated documentation\n * files (the \"Software\"), to"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = []\n\tfor subparser, args in unparsed_arguments.items():\n\t\tparsed_arguments.append((subparser, args))\n\treturn parsed_arguments\n\ndef parse_args_with_defaults(args, defaults, defaultvals):\n\t\"\"\"\n\tGiven a sequence of args, parse one subparser.\n\tFor each subparser that accepts args, do the following in a subparser, and then return all subparser\n\targuments (defaults and actual values).\n\t\t\n\t\t--repository\n\t\t\t--arguments --args --defaults --args2 <subparser"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tif len(unparsed_arguments) < 1:\n\t\treturn ((), [])\n\tfirst_arg = unparsed_arguments[0]\n\treturn (first_arg, [(subparsers[first_arg.parser_name], first_arg.arguments)] )\n\nif __name__ == '__main__':\n\timport sys\n\tsys.exit(ParserGenerator().parse_options(sys.argv[1:]))\n\nimport {\n  ApplicationEvent,\n  HttpClient,\n  HttpLinkStatusOptions,\n  HttpRequest,\n  HttpResponse,\n  Route,\n  Session,"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tresult_type = CommandArgType.subparser\n\treturn subparsers.wrap_for_parser(result_type(unparsed_arguments), **subparsers)\nadd_subparsers(\"list\", parse_subparser_arguments)\n\nusing System;\nusing System.Collections.Generic;\nusing System.Data;\nusing System.Linq;\nusing System.Text;\nusing System.Windows.Forms;\nusing System.Threading;\nusing System.Threading.Tasks;\n\nusing Datalogics.PDFL;\n\nnamespace Examples.Models\n{\n    public partial class Main"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\targuments = []\n\tfor arg_name,arg_value in unparsed_arguments:\n\t\tfor subparser in subparsers:\n\t\t\tif arg_value.name == subparser.argument_name:\n\t\t\t\targuments.append(arg_value)\n\t\t\t\treturn\n\treturn ({},[],[])\n\ndef parse_args_with_namespace_arguments(*args_,namespace_arg,subparser=DefaultSubParser):\n\tsubparsers,args  = subparser\n\t# if NamespaceArgument is in args and is not None, then its subparser was provided more in args than"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tfrom argparse import Namespace\n\tsubparsers = {}\n\tfor p, args_stack, is_option in zip(subparsers.items(), unparsed_arguments, unparsed_optional_subparsers):\n\t\tsubparsers[p] = Namespace(args=args_stack, option=is_option)()\n\treturn subparsers.values()\n\n\ndef _find_subparser(parser_class, subparsers):\n\t\"\"\"Given a parser object and subparsers dictionary, find the last command in the parser by\n\tname.\"\"\"\n\tfor p in subparsers:\n\t\tif isinstance"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tif len(unparsed_arguments)==1:\n\t\t# we'll just return what we got\n\t\targvalues = unparsed_arguments[0]\n\telse:\n\t\t# for an argument that is a dict, we'll just parse that\n\t\targvalues = dict(parse_args_and_kwargs(unparsed_arguments))\n\t\t# we now go deeper to parse all of our subparsers\n\t\tsubparser_list_for_kwargs = [[subparser_name,subparser_param] for subparser_name in argvalues for subparser_param"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\targs_so_far = {}\n\tconsumed_subparsers = set() # used to keep track of consumed subparsers\n\tremaining_arguments = set() # used for storing remaining arguments in args_so_far\n\tfor arg in unparsed_arguments:\n\t\tif arg.startswith(\"--repository\"):\n\t\t\tconsumed_subparsers.add(arg[:-12])\n\t\telif arg.startswith(\"--\"):\n\t\t\tremaining_arguments.add(arg[:-4])\n\targs_so_far[subparsers] = remaining_arguments\n\t# We'll take care to set the namespace,"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tall_args = []\n\targs = []\n\tfor item in unparsed_arguments.items():\n\t\tif type(item) is type(\"subparser_arguments\"):\n\t\t\tparsed_parser_name = str(item[1])\n\t\t\t\n\t\t\tswitch (item[2]) % type(subparsers):\n\t\t\t\t# First subparser, return an empty tuple\n\t\t\t\t# The next subparser will be a parsed_subparser_argument\n\t\t\t\tcase subparsers[parsed_parser_name]:\n\t\t\t\t\tcontinue\n\t\t\t\t# Then we need to get the remaining arguments\n"}]}
{"task_id": "62b438a266fea644fe22cc2c", "prompt": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t# This function does all the parsing necessary but is not the only \n\t# way to get the current line from an interactive shell or console.\n\tdef process_line(line):\n\t\tdef process_arg(arg):\n\t\t\tif isinstance(arg, basestring):\n\t\t\t\tif not is_interactive():\n\t\t\t\t\targs_list = [arg]\n\t\t\t\telse:\n\t\t\t\t\tcurrent_line = line\n\t\t\t\t\tif current_line!= 0 and \\\n\t\t\t\t\t\tcurrent_line_is_comment:\n\t\t\t\t\t\tif current_line.find(\"# \") == -1:"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tdef make_main_parser():\n\t\tparser = argparse.ArgumentParser(\n\t\t\"test.cpp.py\",\n\t\tdescription = \"Test top-level usage of the 'test.cpp' parser in tests\")\n\t\tparser.add_argument(\"--file\", type = \"string\",\n\t\t\thelp = 'Read the file'+ str(TESTS_INCLUDE_FILE) +\n\t\t\t\t\t' and run it.')\n\t\t\n\t\tparser.add_argument('-o', '--outfile', type ='string',\n\t\t\tdefault = 'test.cpp',\n\t\t\thelp ="}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tdef is_string(x):\n\t\treturn isinstance(x, str)\n\n\tdef get_args(x):\n\t\tx = x.strip()\n\t\tif len(x) > 0:\n\t\t\targs = x.split(\",\")\n\t\t\treturn tuple(args),args[1]\n\t\telse:\n\t\t\traise ValueError(\"Empty tag\")\n\tdef get_defaults(x):\n\t\tdefault = x.strip()\n\t\treturn tuple(x.split('=', 1)) or default\n\tdef get_default(x):\n\t\t"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tif not hasattr(parser, 'pformat'):\n\t\tparser.pformat = lambda s: s\n\tparser.format_help = lambda s, *a: a\n\tparser.subparsers.extend([(pformat, OptionParser), ('align', 'align='),\n\t\t('def', OptionParser), ('indent', OptionParser), ('width', WidthOptionParser),\n\t\t('indentsimple', OptionParser), ('keep', OptionParser), ('max', MaxOptionParser),\n\t\t('alignall', None), ('default', OptionParser), ('verbose', VersionOptionParser)])\n"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = OptionParser(usage='''\n\t\t# [CommandOptions] [--[CommandOptions]] [--[CommandOptions]][--\n\t\t# [CommandOptions] [--   [CommandOptions]]\n\t\tParser for -- command.\n\t''')\n\tparser.add_option('--version', action='store_true', dest='version',\n\t\thelp=\"Print the installed version\")\n\tparser.add_option('-v', dest='version', help=\"Print the installed version\")\n\tparser.set_defaults(version=__version__)\n\tparser.set_"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tif __name__ == '__main__':\n\t\tfrom programy.parser.py3d.parse.parser import Parser\n\t\tparser = Parser([(\"parser\", __file__), (\"test_cases\", \"test_cases\"), (\"parser_test\", __file__), (\"test_parser\", \"test_parser_test\")])\n\telse:\n\t\tparser = Parser([(\"parser\", __file__), (\"test_cases\", \"test_cases\"), (\"parser_test\", __file__), (\"test_parser\", \"test_parser_test_parser\")])\n\n# Licensed to Elasticsearch B."}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tdef _parser(parser, data):\n\t\treturn (parser, data)\n\treturn _parser\n\n\ndef build_parsers(data, ignore=None):\n\tparser_tuple = _parser(make_parsers(), data)\n\tsubparsers = []\n\tfor d in data:\n\t\tc = d.get_parser()\n\t\tsubparsers.append(c if c else make_parsers() )\n\treturn options.parser_tuple(parser_tuple, subparsers)\n\ndef make_command(options, name, *command):\n\t\"\"\"Build a group"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = makeparser(\"list\")\n\tparsers = list(makeparser(\"parser\"))\n\tParser(parser).add_subparsers(parsers)\n\n\t# We can't reuse \"parser\" because the \"parsers\" wouldn't be reused\n\t# once parsing started.\n\tparser = makeparser(\"parser\")\n\tparsers = [parser]\n\tParser(parser).add_subparsers(parsers)\n\n\t# We cannot reuse \"parser\" because the \"parsers\" wouldn't be reused\n\t# once parsing started.\n\t# Note: no need to create \"Parser(parsers"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = optparse(version='unknown')\n\n\tfor op in optparse.parseOptions(sys.argv):\n\t\tif op[0] == '-':\n\t\t\tcontinue\n\t\treturn optparse.subparseOptions(op, parser)\n\n\nclass Subparse(subparsers.Parser):\n    \"\"\"Parse a given subcommand's docstring.\n\n    Sub-commands are delimited by a - and underscores. The first\n    - is taken to mean a subcommand name and subsequent parameters\n    are taken to mean arguments.\n"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t# Create our parser and build the subparsers.\n\tpprint(parser)\n\tpprint()\n\tparser = topparser = parsers[0].get_parser()\n\treturn(parser, pprint(parsers[0].get_argument_list()))\n\ndef show_argument_values(parser):\n\targument_values = parser.get_argument_values()\n\tprint('Argument values for parser:')\n\tfor arg in argument_values:\n\t\tprint(arg.name)\n\t\tprint()\n\ndef show_arguments(args):\n\tfor parser in"}]}
{"task_id": "62ece4982e6aefcf4aabbd5f", "prompt": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tfrom PyQt5.QtCore import pyqtSignal\n\tfrom PyQt5.QtGui import QCursor\n\tfrom PyQt5.QtWebKit import QUrl\n\n\tfrom PyQt5.QtWidgets import QMessageBox\n\t\nclass PagelistWidget(QGroupBox):\n\t\"\"\"\n\tWidget used to list all items in a given table. Data will be returned as a WAPIData object.\n\t\n\tArgs:\n\t    data_table (:obj:`str`): WAPI table data.\n\t    format_table (:obj:`str`): WAPI table format.\n\t "}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\treturn make_paged_response(response=[\n\t\t{\n\t\t\t\"id\": str(i) for i in range(200),\n\t\t\t\"total\": str(max_results)\n\t\t} for i in range(1, 100)\n\t])\n\ndef page(request):\n    return make_paged_response(response=Page(\n\t{\n\t\t\"total\": str(request.GET.get(\"total\") or request.GET.get(\"limit\", None)),\n\t}\n\t))\n\nclass Pagination:\n\t\"\"\"The pagination object"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tif not hasattr(response, 'next'):\n\t\traise AttributeError(\"response not defined.\")\n\tif len(response) < 1:\n\t\traise AttributeError(\"Not enough results.\")\n\tif not isinstance(response[0], WapiObject):\n\t\traise TypeError(f\"WapiObject not valid.\")\n\tpages = [None] * max_results\n\tfor page in response:\n\t\tpages[page % max_results] = page\n\treturn pages\n\n/*\nCopyright (c) 2007-2013 Pivotal Software, Inc.\nAll rights reserved.\n\nRed"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\treturn WAPIPagedResults(paging, page, max_results)\n\n##############################################################################\n# Page Functions\n##############################################################################\n#\n\ndef load_paged_results(response):\n\twhile (response and response!= '*'):\n\t\tdata = read_page(response)\n\t\tif data:\n\t\t\tyield data\n\t\telse:\n\t\t\tbreak\n\n#\n# Functions\n#\n##############################################################################\ndef load_pages(response):\n\ttry:\n\t\twhile response:\n\t\t\tresponse = load_paged_results(response)\n\texcept IOError as e"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tfrom wandb import Pagination\n\tfrom wandb.api.wandberror import wandb_utils\n\tfrom wandb.wandblib.paginatedpages import Page\n\t\n\tfrom scout.app.db.entity.enums import PageState, PageStateEnum\n\t\n\tprint(\"Paginating page for: \" + str(Page.objects.first().page_id))\n\t\n\tpaged_result = Pagination(response, max_results)\n\t\n\tpage = Page(paged_result)\n\t\n\tpage.add_header('Content-Type',"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tfrom schemathesis.parsers import PPA\n\tfrom requests import session, HTTPError, get\n\turl_page = \"wapi?action=page&page=\" \n\n\turl_args = url_page + \"&per_page=\" + str(int(max_results + 1))\n\turl = r\"http://www.spaceto.com/api/json/page?action=search&q=\" + url_args \n\tres_data = session.get(url, headers=get(url, headers=get_headers))\n\n\tresults = []\n\tif"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tWapiPagedResponse = dict(response = [])\n\tPage = 1\n\tdef paging_item(result, max_objects) :\n\t\tWapiPagedResponse[Page] = result\n\t\twith Page,Page := result.page,MaxItems(Page - 1, WapiPagedResponse['Items'])\n\t\ttry :\n\t\t\tpage = WapiPagedResponse['Page']\n\t\texcept :\n\t\t\tprint(\"No page found in this page.\")\n\t\t\n\t\tprint(\"\")\n\t\tprint('Page:',2,Page,'Items:',print_items(Page),print"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tif type not in PageAPI and max_results <= 0:\n\t\treturn 0, 0\n\tyield PageAPI().paging(response=response, max_results=max_results)\n\t\n\ndef search(query, page, per_page=60, count=100):\n\t\"\"\"\n\tSearch items in a search results page\n\t\n\tArgs:\n\t    query (str): Search query.\n\t    page (int): Page to be searched.\n\t    per_page (int): number of items per page.\n\t    count (int): Number of items"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\ndef get_user_data(user, limit=-1):\n\t\"\"\"\n\tReturns username and password for a given user.\n\tIt will call set_data with limit and save it in user.\n\tArgs:\n\t    user(str): User id.\n\t    limit (int, optional): If greater than 0, will request the data only limited to this number of objects on a single page.\n\tReturns:\n\t    Generator object with user specified data as username & password.\n\t\"\"\"\n\tpass\n\tresp  = HTTP(\n\t\turl = '"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tresponse = []\n\tcount = 0\n\twhile response:\n\t\titems = response\n\t\tcount = max(count, int(max_results/10))\n\t\t\t# count > 5000 -- 5 pages.\n\t\t\t# count = 0 in other case.\n\t\tif count>1000: break\n\t\t# Create response split object for paginated results.\n\t\tpage_response = []\n\t\tpage_response.append([response[0]])\n\t\tprint('Pages %d' % count)\n\t\t# Iterate all sub pagenable pages.\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd60", "prompt": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tfile_size = str(size)\n\treturn file_size.bytes() // 1024 * 8\n\ndef bytes_to_memory(data: bytes) -> int:\n\t\"\"\"\n\tConvert bytes to memory in MB. This will be used regardless of file size.\n\t\n\tArgs:\n\t    data: A bytes representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    An integer representing the number of bytes actually used in the file\n\t\n\tExamples::\n\t\n\t    >>> bytes_to_memory('500')\n\t   "}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tconverted_float = float(bytes.fromhex(size))\n\tif converted_float <= 0.0:\n\t\tconverted_float = 1.0\n\t\t\n\tif converted_float!= 1.0:\n\t\tconverted_float += 0.25\n\telse:\n\t\tconverted_float = 1.0\n\treturn converted_float\n\ndef str2size(size: bytes) -> str:\n\t\"\"\"\n\tConvert a human readable file size to bytes.\n\t\n\tArgs:\n\t    size: a human readable file size (eg: '500K')\n"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\treturn round(float(size) / float(1e6), 2)\n\n@contextmanager\ndef memory_bytes(size: Optional[str] = None, allow_mmap: bool = False) -> typing.Disposable:\n\t\"\"\"\n\tCreate new memory manager with given size, allow for more performance by supplying an optional memory file.\n\t\n\tArgs:\n\t    size: If None, create a \"normal\" memory file\n\t\n\tReturns:\n\t    A new memory manager\n\t\n\t    Examples::\n\t\n\t        >>> mem_bytes"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\treturn longlong.tobytes(size, \"E\")\n\n\ndef human_size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable length representation to bytes.\n\t\n\tExamples::\n\t\n\t    >>> human_size_to_bytes(\"5 bytes\")\n\t    20\n\t    >>> human_size_to_bytes(\"20 bytes\")\n\t    20\n\t    >>> human_size_to_bytes(\"100K  \")\n\t    100\n\t\n\t\"\"\"\n\treturn longlong.tobytes(size"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\ti = size\n\t\n\tif not i in [256, 25600]:\n\t\traise ValueError(\"Size argument must be between 256 and 25600\", name=\"size_to_bytes\")\n\t\n\tif i.upper()!= 'K':\n\t\traise ValueError(\"Invalid size format. Must be 'K'\", name=\"size_to_bytes\")\n\t\n\tsize = int(i)\n\treturn size\n\n/*\n * Copyright (c) 2008 Gerrit Grunwald\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\treturn size.decode(\"utf-8\")\n\n################################################################################\n\n\n###################################\n### Friend: \n######\n###################################\n# Author: \n#####\n# Copyright (C) 2018 \n#####\n################################################################################\n\n\nimport subprocess\n\ndef run_shell(shell: str, **kwargs):\n    \"\"\"Runs a given shell to see if there is a command ready\"\"\"\n    run_command = subprocess.run(shell,\n                        "}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t# from util.misc import util\n\tif isinstance(size, str):\n\t\tsize = int(size - 1)\n\t\treturn size\n\n\ttry:\n\t\treturn round(float(size) / 1000)\n\texcept (TypeError, ValueError, OverflowError):\n\t\traise ValueError(\"Invalid size specified\")\n\n/*************************************************************\n *\n *  MathJax/jax/output/SVG/fonts/Gyre-Pagella/NonUnicode/Regular/Main.js\n *  \n *  Copyright (c) 2013-2016 The MathJax Consortium\n *"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tif size == \"\":\n        size = \"\"\n\t\n\treturn round(int(size) / 1024, 1)\n\ndef is_directory(path: str) -> bool:\n\treturn not os.path.isabs(path) and is_readable_file(path).is_directory\n\t\n\t\ndef is_symlink(path: str) -> bool:\n\tif os.path.isfile(os.readlink(path)):\n\t\treturn True\n\tif os.path.isdir(path):\n\t\treturn False\n\treturn False"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tfile_size = size.centered(decimal='0')\n\thex = file_size.hex()\n\tsize_unit, _ = deprecate_units(size,'size_to_bytes')\n\t\n\treturn int(round(float(size_unit*(1 << (hex in size_unit)))+0.5))\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tif size.startswith(\"0.\"):\n\t\traise ValueError(\"'%s' to big.bin does not produce any size.\" % size)\n\tif size.isdigit():\n\t\treturn int(size, 8)\n\telse:\n\t\treturn 0\n\ndef hash_fn(name: str) -> Tuple[int, int]:\n\t\"\"\" Returns hash code of given file name. We don't have to worry about\n\tall filenames in a subdirectory for hashing purposes. We always return\n\tthe file name in a file as the whole file hash.\n\t\"\"\"\n\tf ="}]}
{"task_id": "62ece4982e6aefcf4aabbd61", "prompt": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\treturn _DictSum(sum)(dicts)\n\n\nclass _DictSum(_DictBase):\n\t\"\"\"\n\tCombine values of the dictionaries supplied by a dict. \n\t\"\"\"\n\tdef __init__(self, dictionaries):\n\t\t# type: (Dict[Any,Any],Dict[Any,Any]) -> None\n\t\tself.dictionaries = dictionaries\n\t\n\tdef __delitem__(self):\n\t\tdel self.dictionaries[key]\n\t\tdel key, value\n\t\nclass _DictSet(DictBase):\n\t\"\"\"\n\tDictionary of"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tfor item in dicts:\n\t\tif callable(item.dsize):\n\t\t\titem.dsize = item.dsize(funcargs=dicts)\n\n\ndef set_max_dict_size(new_max):\n\t\"\"\"\n\tSet new maximum size of the dictionary, either when an item already exists,\n\tor the new dictionary size is greater than the existing maximum.\n\n\t>>> set_max_dict_size(1)\n\t<BLANKLINE>\n\t>>> set_max_dict_size(20)\n\t<BLANKLINE>\n\t\"\"\""}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tdef __init__(self, dicts):\n\t\tdists = dict()\n\t\tif not isinstance(dicts, OrderedDict):\n\t\t\traise TypeError(\"A dict is expected\")\n\t\tfor k, v in dicts.items():\n\t\t\tif k == \"__builtins__\":\n\t\t\t\tself.builtins_dict[_dict_index(_dict_items(dicts))] = _dict_items(v) \n\t\t\t\tself.builtins_dict[_dict_index(_dict_items(_dict_items(dicts))[0])] = _dict_items(v[0])\n\t\t\t"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tfor item in dicts:\n\t\tif isinstance(item, Dict):\n\t\tfor value in item.values():\n\t\t\tdicts.append(Dict({key: value[key] for key in item.keys()})[1])\n\treturn dicts\n\ndef _int_divide(i, divisor):\n\t\"\"\"\n\tDivide integer number by divisor.\n\t\"\"\"\n\tfor val in i.copy():\n\t\tval /= divisor\n\treturn val\n\ndef _unicode_to_native(value):\n\t\"\"\"\n\tConvert unicode string to native string with a fallback"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\treturn (dict(zip(itertools.islice(dicts, 1, -1), dicts[1]))\n            |dict(zip(itertools.islice(dicts, -1, -1), dicts[0]))\n            |dict(zip(itertools.islice(dicts, -1, -1), dicts))\n            |dict(zip(itertools.islice(dicts, 0, -1),"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tnewdicts = []\n\tfor d in dicts:\n\t\tfor s in d:\n\t\t\tnewdicts.append({s: new_type(s[0])})\n\n\treturn newdicts\n\n\n\n\n\n\n\ndef _getdefaultdict(keys=None, None=None):\n\td1 = {'a': 1, 'b': 2}\n\tif keys is None:\n\t\tkeys = {'a':1, 'b':2}\n\td2 = _getdefaultdict(keys, None)\n\treturn d1,d"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t_dicts = _dict_sortkeys(dicts)\n\tnew=[]\n\tfor data in list(_dicts):\n\t\tif len(data)!= len(new):\n\t\t\tnew.append(data)\n\t\t\tdata[:] = list(map(len, data))\n\treturn new\n\n@python_2_unicode_compatible\nclass MultiDict(metaclass=abc.ABCMeta):\n\t\"\"\"\n\tHelper for merging multiple dictionaries together, given as a sequence\n\tof dictionaries.\n\t\n\tArgs:\n\t\n\t- dicts - a sequence"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tdists_dict = defaultdict(list)\n\tfor d in itervalues(dists_dict):\n\t\tdists_dict[d].append(d[-1])\n\treturn dists_dict\n\n\nif __name__ == '__main__':\n\tfrom nose.tools import assert_true\n\tfrom nose.tools import raises\n\n\tfrom nose.setup import setup\n\timport nose\n\tfrom nose.tools import assert_false\n\n\tsetup()\n\tassert_true(True)\n\tassert_false(True)\n\tassert_"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tfor d, i in itertools.product(dicts, len(dicts)):\n\t\td[i] = d[i]*2\n\ndef test_tuple_or():\n\tlst = [1,2,3,5,6]\n\tif any(i*j for i,j in zip(range(len(lst)), [3*(i+1)*j for i,j in zip(lst)])):\n\t\tprint(\"OK\")\n\t\n\t\n\tlst = [1,2,3,5,6] \n\tif any(i*j for i"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\titems = list(zip(*map(itemgetter(1), iteritems(dicts))))\n\tkeys = dicts[:]\n\tfor i in range(len(keys)):\n\t\tdicts[i] = nextkeys(dicts, i)\n\t\n\tfor dict in items:\n\t\tdicts[dict] = dict.items() if len(dict) < len(items) else dict\n\t\n\treturn Dict\n\t\n\ndef _get_func(i):\n\t# Check first that i is an iterator\n\tif not hasattr(i, \"__iter__\"):\n\t\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd62", "prompt": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\targs = url_args[1].split()  # split arguments\n\targs_dict = {}\n\targs = args[1].replace(\",\", \",\").replace(\".\",\".\")\n\tfor arg in args:\n\t\tif arg.lower() == \"\":\n\t\t\tcontinue  # remove the arg\n\t\tvalue = arg.lower()\n\t\tvalue = value.strip()\n\t\tvalue = _convert_url_items_to_paths_value(value)\n\t\tif value!= args_dict.get(arg, None):\n\t\t\targs_dict[arg]"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\turl, args = url, url_args\n\tif 'file://' in args:\n\t\ttemp = urltemp(url)\n\t\turl = urltemp(url)\n\t\ttemp = urlsubst(url, temp)\n\t\turl = temp\n\turl_args = urlargs(url)\n\targs = urlargs(url)\n\treturn url, args\n\n# ---------------------------------------------------------\ndef replace_relative(url, *args):\n\tif url.startswith('http://'):\n\t\treturn url\n\tif url.startswith('https://'):\n\t\treturn"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tfor key, value in url_args.items():\n\t\tif key not in url:\n\t\t\tcontinue\n\t\tif '/' in value:\n\t\t\tvalue = value.replace('/', os.getcwd())\n\t\tif key == 'base_url':\n\t\t\turl = os.path.join(os.getcwd(), value)\n\t\t\tos.chdir(url)\n\t\t\tbreak\n\tfor k, v in url_args:\n\t\turl = k+\":\\\"%s\\\"\" % v\n\t\tif url in url_:\n\t\t\turl = url_[0] +"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\ti = 0\n\tfor p in url:\n\t\tif p in url_args:\n\t\t\turl[i] = url_args[p]\n\t\t\ti += 1\n\t\telse:\n\t\t\turl[i] = p\n\t\ti += 1\n\ndef _replace_url(url, url_args):\n\tglobal n_urls\n\tn_urls += 1\n\treturn url, url_args, n_urls\n\n/*\n * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n * \n * Licensed under the Apache License"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\turl_args = url_args.replace(':', '%20')\n\turl_args = url_args.replace(';', '/')    # Remove %20\n\turl_args = url_args.split('=')\n\turl_args = url_args[0] + '?' + url_args[1:]\n\treturn url_args\n\n\"\"\"\n\n# Python 3 & lower support\n\nimport codecs\nimport hashlib\n\n# Standard library support.\nfrom cStringIO import StringIO as cStringIO\n\nimport string\nimport sys\n\n# StringIO and c"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\targs = url_args or url\n\twith open(url, 'r') as fp:\n\t\targs = [_replace_custom_string_values_by_args(args)]\n\treturn url, args\n\ndef replace_string_values_by_args(url, args_in_string=None):\n\t\"\"\"\n\tReplacing any string URL items with values from args\n\t\"\"\"\n\tif args_in_string is None:\n\t\targs, _ = url_to_args_string(url)\n\telse:\n\t\targs, _ = args_"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tif not url:\n\t\treturn url_args\n\tif not url.has_url_args:\n\t\treturn url_args\n\turls = map((url.get_url_args, url_args), url.urls)\n\targs = url.get_url_args.copy()\n\tfor url_arg, val in url_args.items():\n\t\tfor key, val in args.items():\n\t\t\turls.append(url)\n\t\t\turl.urls.append(val)\n\t\t\turls.append((key, val))\n\treturn url"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tfor item in custom_strings:\n\t\tif url_args is not None and item not in url_args:\n\t\t\tcontinue\n\t\turl_args[item] = url.replace(item, \"%s\" % url_args[item])\n\treturn url_args\n\ndef _url_arg_re(url_arg, url_path):\n\tif url_arg.startswith(\"-\"):\n\t\treturn url_path + url_arg\n\telse: return url_arg\n\ndef _split_url_arg_pairs(url_args, url_path):"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tfor item in url_args:\n\t\tif hasattr(item,'startswith'):\n\t\t\turl = item.strip()\n\t\telse:\n\t\t\turl = ''\n\n\treturn url\n\ndef _resolve_http_url(base, http, base_args=None, base_args_dict=None):\n\t''' Resolve a URI, possibly substituting with existing URL\n\tor return URL unchanged.\n\t\t\n\tArguments:\n\tbase\tthe base URI (or http) to perform substitutions\n\thttp\tthe url to perform substitutions (only when it does not exists"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tfor item in url:\n\t\tif not item:\n\t\t\tcontinue\n\t\tfor arg in url_args:\n\t\t\tif arg not in item:\n\t\t\t\tcontinue\n\t\t\tprint \"%r replaced.\" % url\n\t\t\tprint \"  %r = %r\" % (item, arg)\n\t\turl = re.findall(r\"/%s\" % (url,) + url_args, item)\n\treturn url\n\ndef _add_url_args_wrapper(url):\n\tdef _wrap(s, args):\n\t\tfor arg in args"}]}
{"task_id": "62ece4982e6aefcf4aabbd63", "prompt": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn not any(is_int_like(x) for x in val)\n\n\n@check_types(typing.Number, typing.Any, types.Any)\n@check_types(typing.String, typing.Any, types.Any, types.Any)\n@check_types(typing.Bool, typing.Any, types.Any, types.Any)\n@check_types(typing.Bool, typing.Any, types.Any, types.Any)\ndef is_pyobject(val: any) -> bool:\n\treturn isinstance(val, (typing.Boolean"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn val is None or isinstance(val, str)\n\ndef is_one_of_str(val: str, ones: Iterable[str]) -> bool:\n\t\"\"\"\n\tCheck if a string represents one of a set of strings.\n\t\"\"\"\n\tone_of_str_items = iter(ones)\n\tis_one_of = False\n\tfound_one_of_items = set()\n\n\tfor item in one_of_str_items:\n\t\tif val in item:\n\t\t\tfound_one_of = set(one_of_str"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\tif not isinstance(val, (str, bool, six.binary_type, six.text_type)):\n\t\treturn False\n\tif val == \":\":\n\t\treturn True\n\tif val.lower() not in [\":\", \"[\"]:\n\t\treturn True\n\treturn False\ndef is_blank(val: any) -> bool:\n\treturn not True if val == \" \" else False\ndef is_list_like(val: any) -> bool:\n\treturn isinstance(val, list)\n\treturn len(val) > 0 and not is_blank("}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn (isinstance(val, str) and val.endswith('nil')) or (not isinstance(val, str))\n\n\ndef is_any_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a (scalar) null value.\n\t\"\"\"\n\treturn (not isinstance(val, str_scalar) or val.startswith('[None]'))\n\n\ndef is_number(val: Any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a (scalar) number.\n\t\"\"\"\n\treturn not val or isinstance(val, str_"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn isinstance(val, str)\n\n\n@is_none_string\ndef is_nullable_string(val: any) -> bool:\n\t\"\"\"Check if a string is a nullable value, meaning that we need a different operator, such as `?` to `NOT null`.\n\t@raise TypeError: If 'val' is not string, but 'val' is, which we are checking.\n\t@return: Boolean, if 'val' is a string, else False.\n\t@raise TypeError: If the type of 'val' was *not* string (because they are not)."}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\tif not callable(val):\n\t\treturn False\n\tif is_none_string(val(None)):\n\t\treturn True\n\treturn False\n\n\ndef check_empty_list(val: list[str]) -> bool:\n\t\"\"\"\n\tCheck if a String value specifies an empty list.\n\n\tThis check depends on the value of the List's item_type argument,\n\tbut it also ensures that the value string can be empty.\n\n\tTherefore, this method may be removed.\n\n\tArgs:\n\t\tval -- Input value.\n\tReturns:"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn val is not None\n\nif __name__ == '__main__':\n\timport pytest\n\n\tpytest.importorskip('dorado')\n\tpytest.importorskip('dorado.test')\n\n\t@pytest.mark.no_run\n\tdef test_skip_if_too_many_params():\n\t\t@dorado.io.dorado_run\n\t\t@dorado.shared_environment\n\t\tdef func():\n\t\t    pass\n\n\t\tassert None not in func()\n\n\t@pytest.mark.fail"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\tif isinstance(val, (str, bytes)) and not val.startswith(\"None\") and val!= \"None\":\n\t\treturn True\n\treturn False\n\n\ndef is_date_or_time(val: any) -> bool:\n\t\"\"\"\n\tCheck whether the Python \"value\" is a valid datetime.\n\t\tvalue = tsv(value, format_type=None, allow_fraction=True)\n\t\"\"\"\n\tif isinstance(val, (datetime, timelib.dateutil.parser_utils.IsoFormat)) and val.is_naive:\n\t\t"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\tassert (isinstance(val, str))\n\treturn True\n\n# Copyright 2014 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\tglobal output\n\tif val is None:\n\t\treturn True\n\t\"\"\"\n\t# If output is None, always output the result as a string.\n\tif output is not None:\n\t\treturn (isinstance(output, str))\n\t\"\"\"\n\t# If the value of the string is a literal zero, just output it.\n\tif val!= 0:\n\t\treturn True\n\t\"\"\"\n\tif isinstance(val, int):\n\t\treturn (val == 0)\n\t\"\"\"\n\treturn True\n\n\n_output_type_str_to_int = dict"}]}
{"task_id": "62ece4982e6aefcf4aabbd65", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tif not isinstance(parser, argparse.ArgumentParser):\n\t\treturn ''\n\tdef get_parser_flags(parser):\n\t\tf = parser\n\t\tf.__dict__['extra_kwargs'] = {}\n\t\tf.add_argument('--list-file', action='store_true', help='Output a list file and exit, instead of parsing the command line (default = %(default)s)')\n\t\tf.add_argument('--stdin-as-list-file', action='store_true', help='When no input file is provided, make stdin a list file and exit"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t_flag_map = {\n\t\t'boolean_flags': 'boolean',\n\t\t'keyword_flags': 'keyword',\n\t\t'keyword_list_flags': 'keyword',\n\t\t'parse_flags':'string',\n\t\t'option_flags': 'option'\n\t}\n\treturn _flag_map[parser.name]\n\n\ndef parse_flags(parser, line, is_help):\n\t\"\"\"\n\tGiven a line in a `line`, return the options and their flags.\n\t\"\"\"\n\tparser.print_help()\n\treturn"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t# Parse arguments and print out any errors found while parsing.  Also update the parser.error\n\t# property if set.\n\treturn _parse_args(parser, parser.default_args, parser.error)\n\ndef _parse_args(arg_parser, args, error_msg):\n\tif len(args) == 0:\n\t\treturn _format_args(args, error_msg)\n\n\telse:\n\t\tfor arg in args:\n\t\t\tif isinstance(arg, argparse.Argument):\n\t\t\t\targs.add_for_arg(arg)\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = \"a,b      c      e\\n\"\n\tparts = parser.parse_args(flags)\n\n\tassert len(parts) == 7\n\n\targ_flags = [\"a\", \"b\"] * (len(parts) - 1) + ['c', 'e']\n\n\treturn join(parts, \" \", arg_flags)\n\n\ndef main():\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--verbose\", '-v')\n\n\tassert (parser.optional_help or parser.has"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tparser_name = type(parser).__name__\n\tresult = parser.function.replace(\"_\", \" \").upper()\n\tresult += \".\" if result in [\"A\", \"B\", \"C\"] else \"\"\n\treturn parser_name + \"=\" + result\n\ndef argclass_check(parser, argclass, *args, **kwargs):\n\t\"\"\"\n\tThis function returns an ArgumentParser instance to provide a way for a caller to pass\n\tthose types that will be allowed for each value in the provided args.\n\t\"\"\"\n\tparser = copy.copy(parser)\n\tif arg"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=parser.__doc__)\n\targ_names = list(argparse.parse_args_string(\"|\".join([k for k in args.__dict__.keys() if '__' not in k])).keys())\n\targ_flags = \", \".join([\"{0}={1}\".format(k, val) for k, val in zip(arg_names, args.__dict__.keys())])\n\targ_list_separator = \" \"\n\treturn str(arg_list_separator.join(arg_flags))\n\n\ndef set_default_"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tresult = parser.parse_args(['example'])\n\tif not result.nargs():\n\t\traise ValueError(\"No arguments!\")\n\tif result.option_string == '--help':\n\t\tret = formatter.format_help([], parser)\n\telse:\n\t\tret = formatter.format_help('example.', parser)\n\n\treturn ret\n\n\ndef argument_class(arg):\n\tif isinstance(arg, parser_struct):\n\t\treturn arg\n\tif isinstance(arg, parser_flags):\n\t\treturn argparse.ArgumentDefaults.get(arg, argparse."}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tdef get_argument_flags(parser):\n\t\treturn [\"-{}={}\".format(x, \"-\".join(field.strip())) for x in parser.get_all_args()]\n\n\treturn [\n\t\t(\"ARGV\",               get_argument_flags,                            True),\n\t\t(\"-x option\",          parser_flags,      "}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn ''.join(parser._args_as_string.split('-')[:-1])\n\ndefault_argdefs = parser_flags('default_parser')\n\nif parser.has_default_argument:\n\tdefault_argdefs ='default-argument-' + parser.default_argument\n\n\ndef add_optional_arg(parser, name, optional=True):\n\t\"\"\"Add a default argument to the parser, if given.\n\t\"\"\"\n\tif not optional and not parser.has_default_argument:\n\t\tparser.error(parser.missing_argument, \"[%s"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tparser.add_argument_group(name=\"argument groups\", description=\"Groups of flags separated by commas. This is useful for groups of flags like -i, -k, or -m\")\n\n\treturn parser.get_argument_group().flags\n\n\ndef get_flags_arg_group_name(parser, flags):\n\tflags.sort() # This has better performance than arg grouping.\n\treturn sorted(flags, key=lambda x: parser.arg_count([x]), reverse=True)\n\n\ndef parse_args(args, argv, options):\n\t\"\"\""}]}
{"task_id": "62ece4982e6aefcf4aabbd66", "prompt": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tif len(processed) == len(set()):\n\t\treturn True\n\telse:\n\t\tpath_entry = {}\n\t\twhile path_entry in processed:\n\t\t\tpath_id, path_name, path_type, path_file = processed.pop(path_entry)\n\t\t\tpath_entry.update({path_id: (path_type, path_name, path_file)})\n\t\tprocessed.update(set(path_entry))\n\t\tprint(f\"processed:{len(processed)}:\")\n\t\tfor path_entry in processed:\n"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t# If is directory exists already, this is a good outcome\n\tif not os.path.isdir(processed[path_name]):\n\t\tprint \"Directory not found! Trying to collect\", path_name\n\t\t# This will add to the cache, it will add to the set again in case it needs to be processed\n\t\tif os.path.isdir(processed):\n\t\t\tos.path.append(processed)\n\t\t# Directory does not exist\n\t\telse:\n\t\t\tprint \"Directory exists! Trying to collect\", path_name\n\t\t\t#"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tif path_name in processed:\n\t\treturn processed.pop(path_name)\n\telse:\n\t\treturn os.path.exists(path_name) and not os.path.exists(path_name + '/__init__.py') and (not path_name in processed and (verbose or os.system('cd -all').lower()!= 'all'))\n\ndef recurse(processed, dir_name, verbose):\n\t\"\"\"\n\tIterate over all the files/directories in a directory.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames in"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tglobal last_processed\n\t# A set is used for collecting subpaths that need to be fixed\n\tglobal not_fixed\n\tpath_base_dir = 'current'\n\tprocess_dir = path_base_dir\n\t\n\t# Get the base path and check if the entry already exists.\n\tprocessed.add(path_name)\n\tprocessed.remove(path_name)\n\tif not processed.has_entry(path_name):\n\t\tnot_fixed.append(path_name)\n\t\treturn True\n\telse:\n\t\t# Add the entry"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tif path_name is None:\n\t\treturn None\n\t\t\n\tentry = processed.pop(path_name)\n\tprocessed.insert(path_name, 1)\n\treturn path_name in processed\n\t\n\nif __name__ == '__main__':\n\tfrom time import sleep\n\tfrom unittest import main\n\tmain()\n\nimport { inject, injectable } from 'inversify';\nimport { Observable } from 'rxjs';\nimport { EventBus } from '..';\n\n@injectable()\nexport class DataStore<R> {\n    private _events:"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tcurrent_dir = os.path.abspath(os.path.dirname(path_name))\n\tparent_dir = current_dir + '/'\n\tif os.path.isabs(path_name):\n\t\tif current_dir in processed:\n\t\t\treturn True\n\t\telse:\n\t\t\tprocessed.add(path_name)\n\t\t\treturn False\n\telse:\n\t\tif os.path.isdir(path_name):\n\t\t\tif parent_dir in processed:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\tprocessed.add(path"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\twhile processed:\n\t\tpath = processed.pop()\n\t\tpath_name = path.resolve()\n\t\tif path in processed:\n\t\t\treturn True\n\t\tprocessed.add(path_name)\n\t\t\n\treturn False\n\n#------------------------------------------------------------------------------\n# Getting and printing functions\n#------------------------------------------------------------------------------\ndef printout(verbose=False):\n\t\"\"\"\n\tMethod printing functions.\n\t\"\"\"\n\tif verbose:\n\t\tprint('Processing \"%s\" and printing path to stdout' %''.join(\n\t\t\tm.fargs\n\t\t))\n\telse"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tfor name in processed:\n\t\tif name.startswith(path_name):\n\t\t\tfound = name\n\t\t\tif len(path_name)!= len(current_directory):\n\t\t\t\tmsg = 'Error processing the following directory:'\n\t\t\t\tmsg += '\\n{}'.format(path_name)\n\t\t\t\tmsg += '\\nCurrent directory:'\n\t\t\t\tmsg += current_directory\n\t\t\t\tmsg += '\\nDirectory name is:\\n{}'.format(path_name)\n\t\t\t\traise FileError(msg)\n\t\t\tif verbose:\n\t\t\t\twarn(msg)\n"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t# In verbose mode, print message if a path is currently processed\n\tif verbose:\n\t\tif path_name in processed:\n\t\t\t\tprint \"Already in process\"\n\tprocessed.add(path_name)\n\treturn True\n\n// Mantid Repository : https://github.com/mantidproject/mantid\n//\n// Copyright &copy; 2018 ISIS Rutherford Appleton Laboratory UKRI,\n//   NScD Oak Ridge National Laboratory, European Spallation Source,\n//   Institut Laue - Lange"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t# Get rid of old and set stuff and make sure it has at least 2 entries\n\t# to be processed\n\tcleaned = list()\n\tif not path_name.startswith('.'):\n\t\tif not os.path.isfile(path_name):\n\t\t\tpath_name = os.path.split(path_name)[0]\n\t\tcleaned.append(path_name)\n\t\t\n\tpath_name = os.path.abspath(filter(lambda x: '.*.' not in x, cleaned))\n\tif os.path.islink(path_name):\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd67", "prompt": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\timport numpy as np\n\timport scipy.special as spec\n\t\n\tassert vertices, list(vertices), any(inrange_zero(len(vertices)))\n\t#make sure that every vertex is a three-tuple if it's not\n\tif len(vertices)!= 3:\n\t\traise PolygonCreationError(\"Polygon must be a three-tuple for all points!\")\n\t#print a little in case we have not seen any vertices\n\t#print vertices\n\tif len(vertices) == 1:\n\t\treturn vertices\n\tpoints_coords = spec.special_axis(vertices["}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\treturn chain( [ \n\t                 [(1, 0), (0, 1), (0, 1), (0, 0) ],\n\t                 [(1, 0), (0, 1), (1, 0), (0, 1)]] )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#-------------------------------------------------------------\n# Caution: This function is not used in real program usage! It is just\n# purpose for testing these functions in the actual"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\ta, b, c, d = vertices\n\tva, vb, vc, vd = a//3\n\tvs = vb\n\tva = va\n\tvc = vc+1\n\tvc = vc //3\n\tva, vb, vc, vd = vf\n\tvap = va//2\n\tvs = vap//3\n\tvs = vs\n\treturn (a, b, c, d, va, vb, vc, va, vc, vap)\n\n\n@vertex3tuple(vertices)\ndef vertex2tuple(vertices):\n\t\"\"\"\n\treturn 2"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\treturn vertices\n\n\ndef polygons(vertices):\n\t\"\"\"A fast function that determines whether a polygon is a polygon(vertex-to-point)\"\"\"\n\tnew_vertices = vertices.copy()\n\treturn (vertices[0]<=vertices[1] and vertices[0]<=vertices[2]) or\\\n\t\t(vertices[1]<=vertices[2] and vertices[1]<=vertices[3]) or\\\n\t\t(vertices[2]<=vertices[3])\n\n\ndef polygon_polygon_2points(polygon):\n\t\"\"\"\n\tReturns the polygon's 2"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t#TODO: this could get rid of the first 2 tuples, when we can move around them\n\t#There is two 2D tuples we need\n\t#And a more complex way\n\n\tvertices, verticesTuple = np.unique(vertices, return_counts=True)\n\t#And return\n\treturn verticesTuple[1]\n\treturn verticesTuple\n\n\ndef vertex_polygon3d(vertices):\n\"\"\"\n\treturn 3d points for polygons.\n\t#A    B    C    D  -> of vertices\n\t#B    A "}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tfor vertex_index,vertex in vertices:\n\t\tvertices[(vertex_index,0)] = vertex\n\nprint(vertices)\n\n/*\n * Copyright 2016 The Bazel Authors. All rights reserved.\n *\n * Portions of this software were developed by Intel Corporation, 2008-2021\n * under laws of the U.S. Department of Energy and the Materials.\n * Government has rights to use, reproduce, and distribute this source code.\n * The source code is goverened and distributed under the GNU General Public License,\n * Version 2 or 3. This is a convenience that creates all"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t#The three point of the polygon will be\n\t#A    B    C    D  -> of vertices\n\t#the third points of the polygon will also be\n\t#A    B    C    D  -> of vertices\n\t#A    B    C    D  -> of vertices\n\t#A    B    C    D  -> of vertices\n\t#A    C    D    E  -> of vertices\n\t#A   "}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tv = vertices[0] + vertices[1] * 0.5\n\tc = vertices[1 - 1] + vertices[1] * 0.5\n\treturn v, c\n\ndef edge3tuple(edges):\n\t#for each edge\n\tif edges[0].start_vertex is None:\n\t\tverts = [0] * len(edges)\n\t\tc = []\n\t\t#create the edge verts that go to the first point\n\t\tfor j in range(len(edges[0].vertices.shape)):\n\t\t\tif type(edges[0"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tfrom numpy import array\n\tif vertices == [] or vertices == [0]:\n\t\treturn array([tuple(x) for x in vertices])\n\t#Calculate the coordinate system of the polygon points\n\tx = np.array([tuple(vertex[0]) for vertex in vertices])\n\ts = np.array([tuple(vertex2tuple(vertex[1]) for vertex in vertices]) for vertex2tuple in tuple(vertices[:-1])])\n\treturn [tuple(x), tuple(x), tuple(x), s, s]\n\n/**\n * This is the top of everything is"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\treturn [\n\t\ttuple(v) for v in vertices\n\t]\n\n\ndef polygon2polygons(polygonslist, edgesdict):\n\t\"\"\"\n\tConstruct the 3 rings and the other two of them with them as polygons.\n\tPolygon A C D (which are joined with vertices, i.e. A = A B C), and \n\tPolygon B (which are joined with vertices, i.e. A = A C D, B=B C,\n\twhich is joined with vertices, and the original list of\n\t\tpolygonA and polygonB). In this case,"}]}
{"task_id": "62ece4982e6aefcf4aabbd68", "prompt": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tif alphabet and len(alphabet) > 0:\n\t\tnewstr = \"\"\n\t\tfor i in range(0, len(number), len(alphabet)):\n\t\t\tj = i + get_integer_digit(alphabet=alphabet, number=number[i])\n\t\t\tif j >= len(alphabet):\n\t\t\t\tj -= len(alphabet)\n\t\t\telif j < 0:\n\t\t\t\tj += len(alphabet)\n\t\t\tnewstr += alphabet[j].lower()\n\t\t\tif newstr[-1] == \"0\" or newstr[-1] == \"x"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\treturn (int_to_string(number, alphabet, padding))\n\n// This file is auto-generated by the \"tsproxygen.exe\" tool. Manual modification is not recommended.\nimport type DomainBaseInfo struct {\n\tContactDomainInfo\n}\n\nfunc (GetDomainInfo) GetName() string {\n\treturn \"GetDomainInfo\"\n}\n\nfunc (get GetDomainInfo) Execute() (string, *_type.DomainBaseInfo, error) {\n\tr, err := get.ToExecute()\n\n\tif err!= nil {\n\t\treturn \"\", nil, err"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\treturn alphabet[n]\n\n\ndef int_to_alpha_num(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to an alpha-num string, beginning from 2, to the maximum number in alphabet.\n\t\n\tArgument:\n\t\tnumber: a number to convert.\n\t\talphabet: List of characters to represent alphabet.\n\t\tpadding: Padding [0,1].\n\t\tpad is just like 'x2' or 0, just like '0x'\n\t\n\tReturns:"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tif isinstance(number, int):\n\t\tif number < 2 or not alphabet:\n\t\t\treturn repr(number)\n\t\treturn \"-\".ljust(len(alphabet)) + alphabet[number - 1]\n\telse:\n\t\tn = repr(number)\n\t\ts = \"{\" + n + \"}\"\n\t\tif padding is None:\n\t\t\treturn s.join(alphas)\n\t\treturn s.join(\"{}\".format(index + 1) for index, value in enumerate(padding)) + s[n:s.find(n) + padding[-"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tif not isinstance(number, int):\n\t\traise ValueError as E: \"Expected a number, found int type\"\n\tif not number >= 1000:\n\t\traise ValueError : \"Expected a number, found {0:2}\".format(number)\n\tstring = \"{0:.3f}#{1:.3f}\".format(round(number, 10),'- {0:12}' * alphabet.index(number))\n\treturn string[:padding:string.rindex(\"-\")]\n\n\ndef string_to_int(string: str, alphabet: Optional[List[str]]"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tif not hasattr(number, 'isdigit'):\n\t\traise ValueError('Input is not an integer')\n\t\n\tassert (size(alphabet) == size(number)), 'Output size does not match number size'\n\t\n\tif number <= 0:\n\t\traise ValueError('Input is not a regular number')\n\t\n\tresult = str(number)\n\t\n\tif padding is None:\n\t\tpadding = 16\n\telse:\n\t\tpadding = padding or 8\n\t\n\tfor i, a in enumerate(alphabet):\n\t\tresult = result + padding * int(alphabet[i])\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tif int == 0 or (number > 0 and not any(int > 0 for int in alphabet)):\n\t\tif alignment == \"left\":\n\t\t\treturn \"{}\".format(number)\n\t\telse:\n\t\t\treturn \"{}E\".format(number)\n\tif number == 0 or (int == 0 and number > 0):\n\t\treturn \"{}\".format(number)\n\telse:\n\t\treturn \"{}P\".format(number)\n\n/* -*- Mode: java; tab-width: 8; indent-tabs-mode: nil; c-basic-offset: 4 -*-\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\ts = ''\n\tif (alphabet.size == 1): \n\t\tassert isinstance(number, int)\n\t\tnumber = int(number)\n\t\tassert (number < 0)\n\t\t#print ('Not a number')\n\t\treturn s\n\tl_num = min(alphabet)\n\tt = (l_num + padding - 1) // padding\n\t\n\tj = 0\n\tm = l_num\n\twhile( j <= m):\n\t\ttleft=0\n\t\tj1 = j + 1\n\t\tm1 = m\n\t\t"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t# This method only works on floats, and no padding occurs.\n\t# So let's do the same.\n\tif isinstance(number, str): # Python 3\n\t\t# return the number\n\t\tif number in alphabet:\n\t\t\treturn number\n\t\treturn str(number)\n\telse:\n\t\t# check for padding\n\t\tif padding < 0 or not isinstance(padding, int): # we want to convert to int\n\t\t\treturn str(number)\n\t\treturn ''.join([str(x) for x in sequence(alphabet)] + [pad(number"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tfrom collections import defaultdict\n\t# the second part has the most significant digit first\n\t# the third has the most significant digit first\n\tsign, pos = int_to_value(number, alphabet[1])\n\tresult = \"\"\n\tif not int_to_value(number=sign, alphabet.get(0, -1)):\n\t\tresult += alphabet.get('0', -1)\n\treturn defaultdict(str,\n\t\t[[(-'0','-') if sign == int_to_value(sign, alphabet[1]) else str(sign)] for sign"}]}
{"task_id": "62ece4982e6aefcf4aabbd69", "prompt": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tfor flow_name, flow in flow_params.items():\n\t\tif flow.register_number == register_number:\n\t\t\treg_value = flow.register_value\n\t\t\tif reg_value in register_value:\n\t\t\t\treg_value = register_value[reg_value]\n\t\t\t\tassert len(reg_value) == 0, 'Reg '%s' does not exist for register number %s!' % (reg_value, register_number)\n\t\t\telse:\n\t\t\t\tassert register_value is None, 'Reg '%s' does exist in dictionary!'"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tfor flow, register_number_key in flow_data.keys():\n\t\tif register_number == flow.register_number or not (\n\t\t\tregister_number in flow.registers()):\n\t\t\tcontinue  \n\t\treplace_value = register_value if isinstance(register_value, numbers.Number) else register_value\n\t\tif register_number_key not in [register.register_number for register in  flow.registers()]:\n\t\t\tflow_params[register_number] = replace_value\n\treturn flow_params\n\n//------------------------------------------------------------------------------\n// <copyright file"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tregister_value = register_value[register_number]\n\t\n\t# make the register value in dictionary\n\tfor register_key in register_value:\n\t\t_replace_register(\n\t\t\tflow_params,\n\t\t\tregister_key,\n\t\t\tregister_value[register_key]\n\treturn flow_params\n\n/*\n * Copyright (C) 2011 The Android Open Source Project\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tfor p in flow_params:\n\t\tif isinstance(register_value, numbers.Number):\n\t\t\tregister_number = register_number - p\n\t\tif p =='reg':\n\t\t\tregister_value = _replace_reg(flow_params, register_value)\n\n\tif register_number!= '0' and isinstance(register_value, numbers.Number):\n\t\t# Use reg if exists\n\t\tregister_value = str(register_value)\n\n\treturn register_number, register_value\n\ndef _replace_reg(flow_params,"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tfor reg_name, reg_name_mapping in flow_params.items():\n\t\tif (reg_name in register_number):\n\t\t\tfound = register_number[reg_name_mapping]\n\t\t\tregister_number.replace(reg_name, found)\n\telse:\n\t\terror(\"No defined register found in flow_params for flow \"+\n\t\t\t\t\"number '%s' for reference '%s' in file'reference_flow_flow' \"\n\t\t\t\t\"in file '\"+register_number+\"' with name '%s' in flow_params list.\"%"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tfor flow_type in flow_params:\n\t\tif (flow_type == flow_types.register):\n\t\t\tlogtrace()\n\t\t\treg_number, register_value = (\n\t\t\t\tflow_params.get('_n_register', set())   # dictionary of flow_types\n\t\t\t\t.get(register_number, set()))\n\t\t\tif register_value in ('none', register_value):\n\t\t\t\tcontinue\n\t\t\tif (register_value == register_number):\n\t\t\t\tif (flow_type == flow_types.register):\n\t\t\t\t\tlog"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t# Check that flows are defined\n\tif not flow_params:\n\t\tprint \"Flows are not defined\"\n\t\treturn False\n\tif register_number not in flow_params.keys():\n\t\tprint \"Register number in flow_params is not defined\"\n\t\treturn False\n\t\n\t# Get the register value\n\tregister_value = flow_params[register_number]\n\t\t\n\t# Update original register number\n\tflow_params[register_number] = \\\n\t\t(register_value-register_number) //RegisterLengthForString\n\t\n\t# Update"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\treg_num_by_value = []\n\tif register_number == 'I' and register_value in flow_params:\n\t\tfor (value, reg_name) in flow_params[(register_number,):]:\n\t\t\treg_num_by_value.append(value)\n\t\tregister_number = tuple(...reg_num_by_value)\n\t\t\n\tflow_params[(register_number, register_value)] = register_value\n\t\n\ndef _replace_string(flow_params):\n\t\"\"\"\n\tReplace value from flows to register\n\t\n\t"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tregister_number -= 1\n\treg_vals_to_add = []\n\tfor reg_num in _parse_reg_num_map(register_number, flow_params):\n\t\treg_vals_to_add.extend(_parse_reg_val(reg_num, register_value))\n\treg_vals_to_add.sort()\n\treturn dict(zip(reg_vals_to_add, reg_vals_to_add[len(reg_vals_to_add) :]))\n\n\ndef _get_reg_key(reg_val"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tfor key, value in register_value.items():\n\t\tif isinstance(value, dict):\n\t\t\tvalue = _replace_register(flow_params, register_number, value)\n\t\telse:\n\t\t\traise ValueError(f\"Unknown register: {value}\")\n\treturn register_value\n\ndef _register(flow_params, register_number, value):\n\t\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by'register_number'\n\t\"\"\"\n\tflow"}]}
{"task_id": "62ece4982e6aefcf4aabbd6a", "prompt": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\timport re\n\tresult = re.search(r'.{5}(.*)$', arg).group(1)\n\tresult = result.replace('.','.')\n\tresult = re.sub(r'\\(', r'', result)\n\treturn result\n\n# Copyright (c) 2016 Microsoft, Inc.\n# All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without modification,\n#  are permitted provided that the following conditions are met:\n#\n#    1. Redistributions of source code must retain the above copyright notice, this\n#  "}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tif isinstance(arg, list):\n\t\treturn re.sub(\"%.*x\" % div_width, arg, arg)\n\telse:\n\t\treturn re.sub(r\"\\b\\*\\b|\\b\" + arg, arg, arg)\n\n\n###\n### Utility functions.\n###\ndef str_join(strs, delimiters='-'):\n\t\"\"\"\n\tCreates an array of strings using the 'delimiter' sequence of\n\tthe specified delimiter.\n\n\tArguments:\n\t\tstrs\t\tAn array of strings.\n\t\tdelimiter\tAn optional delimiter between text items"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tglobal global_var_value, global_var_exp\n\tglobal_var_exp = arg\n\tif is_string(value):\n\t\tglobal_var_value = value\n\t\tif arg and not is_string(arg):\n\t\t\traise Exception('replace_dots: strings')\n\telse:\n\t\treturn global_var_exp\n\ndef main():\n\toptions, args = c_option_parse()\n\tglobal_var_value = option.replace_dots(global_var_value, args[-1])\n\toption.global_var_value"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\ttry:\n\t\t# value is a string\n\t\ts = value as str\n\t\tfor p in split(s):\n\t\t\tif p == arg :\n\t\t\t\treturn value\n\t\treturn value # no change\n\texcept ValueError:\n\t\treturn value\n\n/*\n * Copyright (C) 2006 The Android Open Source Project\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     "}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tglobal self.unquoted_chars\n\n\treturn '%s%s'%(value, self._tokenize_quoted_value(arg))\n\ndef _tokenize_quotes(listbox):\n\tself.unquoted_chars = []\n\tfor arg in listbox:\n\t\tvalue = self._tokenize_quoted_value(arg)\n\t\tif value:\n\t\t\tself.unquoted_chars.append(value)\n\t\t\tself.replace_dots(value, arg)\n\n\t\treturn value\n\ndef main():\n\tif len(sys.argv) < 3"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tif not isinstance(arg, str):\n\t\traise TypeError(\"Invalid argument type for replace_dots\")\n\tif not arg:\n\t\treturn value\n\tvalue = arg.replace('.','.')\n\treturn '['.join(['%s'%(x,) for x in arg.split('_')]) + '](%s)' % arg.lower()\n\nif __name__ == '__main__':\n\timport os\n\t\nif __name__ == '__main__':\n\timport sys\n\tfrom Bio.Phytogram.tools import remove_prefix\n\tdef modify(args,replace):"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t_value = arg\n\tfor d in value:\n\t\tc = d.count('//', 1) if arg.find('// +'+ d).isnumeric() else 0\n\t\t#print \"Replace {} and {} to {}\".format(c, d, arg)\n\t\tvalue.replace('\\\\'+d, c)\n\t\t#print value\n\n\n\tdef trim(value):\n\t\tif value == '':\n\t\t\treturn \"\"\n\t\tvalue = value.strip('@')\n\t\treturn value\n\n\tdef normalize_symbol(value):\n\t\t"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tdef inner() -> str:\n\t\tif isinstance(value, str):\n\t\t\targ = re.sub(r'\\.','', value.replace('$', '') )\n\t\t\treturn arg.translate_to_dotted_string()\n\t\telse:\n\t\t\treturn value.translate_to_dotted_string()\n\treturn inner\n\ndef match_class_name(value, class_name):\n\t\"\"\"\n\t\tMatches the case the class with :class:`str` or :class:`unicode` matching `class_name`.\n\t\"\"\"\n\tif isinstance("}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\treturn re.sub(r\".*\\.?\", \".*\",value + \".?\").replace(arg, value)\n\n\ndef get_next_argument_match(match):\n\t\"\"\"\n\tGet the next argument (to replace a substring) from a file or string search pattern. \n\t\"\"\"\n\tfor m in match:\n\t\tif not m.group(1).strip():\n\t\t\tcontinue\n\t\n\tif m.group(1): # string search pattern\n\t\tmatch = m.group(3).split(\".+\")\n\t\n\treturn match\n\ndef find_optional_"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tvalue = r'\\.\\.' + value\n\tfor x in arg:\n\t\tif x == '.' or x =='' or x == arg[0:3]:\n\t\t\tvalue = value.replace(x, arg[3:])\n\tif len(value)!= 3:\n\t\traise ValueError(\"The given argument can only contain alphabets or dots\")\n\treturn value\n\n\ndef strip_html(value):\n\t\"\"\"\n\tstrip all html tags from a string\n\t\"\"\"\n\tvalue = r'''\\<html\\b[^>]*?\\>\\s*\\<style"}]}
{"task_id": "62ece4982e6aefcf4aabbd6b", "prompt": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tbase = cls.__root__\n\n\t# Build a list of all base classes, and recursively call the\n\t# subclass's \"walk\" method to recursively return all subclasses.\n\t#\n\t# It is a bit convoluted because the implementation that iterates\n\t# through every inheritance hierarchy in the superclass hierarchy\n\t# is a subroutine that determines the hierarchy order.  We could\n\t# also use the \"walk\" method, but we currently don't do that.\n\n\tfrom numpy.inheritdata import recurse_into\n\n\twhile True:\n\t\tfor child in subcls"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tdef recurse(node):\n\t\tname = node.__name__;\n\t\tfor _, subclass in subclasses._iteritems(cls):\n\t\t\ttry:\n\t\t\t\tsubclass(name)\n\t\t\t\trecurse(node.__dict__[name])\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\treturn recurse\n\n#include \"util.h\"\n\n// @expect verified\n\ndescribe('function foo() { eval(\"/** this is valid test */ \" + x); }\", function() {\n  // @expect baz\n\n  it('works', function() {\n    // @expect"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tfor subtype in cls.__mro__:\n\t\tout += subtype.__mro__\n\treturn out\n\n\n\n\n\ndef add_to_list(list_class, lst_value, level):\n\t\"\"\" Add to a list of the classes, adding it as a new item \"\"\"\n\tif isinstance(lst_value, list):\n\t\treturn lst_value.__iter__()\n\telse:\n\t\tfor l in lst_value:\n\t\t\tlistClass.append(l)\n\t\treturn [list_class.pop()()]\n\ndef get_class_index"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t# XXX: Should make this function more cumbersome\n\tfor c in cls.__bases__:\n\t\tr = []\n\t\tfor c in super(TestClass, cls).__mro__:\n\t\t\tr.extend(c.__mro__)\n\treturn r\n\n\ndef test_init(cls):\n\tassert issubclass(cls, ModuleClass)\n\tf = cls()\n\treturn f\n\n#def test_main():\n#\tr = test_init(TestClass)\n#\ts = ''.join(r.commands[0].split(' '))\n"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t# print('Getting all subclasses of '+cls.__name__)\n\tresult = []\n\tfor class_ in cls.baseclasses():\n\t\tresult.extend(classes(class_))\n\treturn result\n\n\ndef get_subclasses(cls: Any) -> List[str]:\n\t\"\"\"\n\tGet the all subclasses for a given class\n\t\"\"\"\n\tsubclasses = cls.get_subclasses()\n\n\tif isinstance(subclasses, str):\n\t\tparent_classes = cls.parents()\n\t\tfor class_ in subclasses.split(\"/\"):\n\t\t\t"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tres = [r for r in subclasses(cls) if not r.isabstract and r.getproperty(\"isabstractclass\")()]\n\treturn res\n\t\ndef subclassfields(self,cls):\n\tres=[]\n\tfor field in fields(cls):\n\t\tif field in self:\n\t\t\tres.append(field)\n\treturn res\n\t\ndef subclassmethods(cls):\n\ttemp = []\n\tfor meth in members(cls):\n\t\tmeth_class = get_class(meth)\n\t\tif meth_class in self:\n\t\t\ttemp."}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tg = {}\n\tfor child in cls.__subclasses__:\n\t\tg[child.__name__] = Class(child)\n\treturn g\n}\ndef subclass_names(cls):\n\t\"\"\"Returns all the subclasses.\"\"\"\n\tg = {}\n\tfor child in cls.__subclasses__:\n\t\tg[child.__name__] = g.get(child.__name__)\n\treturn g\n\n#\n#\n# ClassHierarchy\n#\n#\nclass Hierarchy(object):\n\t__slots__ = ('classes', 'rootclass', 'inheritancesetcls')"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\treturn _subclasses(cls)\n\n@classmethod\ndef _subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class\n\t\"\"\"\n\treturn []\n\ndef create_subclasses():\n\t'''\n\tReturn all subclasses of a class\n\t'''\n\treturn [c for c in _subclasses()\n\t\t\t\tif c is not None]\n\ndef get_classes():\n\t\"\"\"\n\tReturn all classes of a class.\n\t\"\"\"\n\tclasses = []\n\tfor cls in _subclasses():\n\t\tclasses.append(cls.__class__)"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\treturn [k for k in cl.subclasses()\n\t\t\tif k.findall(None)!= []]\n\ndef subclasses_fast(cls):\n\t\"\"\"\n\t\tThis method finds all subclasses of a class, then\n\t\trecursively calls itself recursively\n\t\"\"\"\n\treturn [k for k in cl.subclasses()\n\t\t\tif k.findall(None)!= []]\n\n\ndef parents(cls):\n\t\"\"\"\n\t\tReturn all parents of a class, recursively\n\t\t\"\"\"\n\tparent_list = []\n\tparent_parents = []"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\treturn [c for c in re.sub('([a-\\w.]+):[^_]+$',',',',cls) if c.strip()]\n\n@attr(subclasses=bases())\nclass ClassSubclass(object):\n\t\"\"\"\n\tReturns the ClassSubclass for a given class.\n\t\"\"\"\n\t# This code is copied from\n\t# https://github.com/python/mypy/blob/7ca79c3c2be5d1b6f1255d44a4a7eb9632a3a73e8c9b80d2a32"}]}
{"task_id": "62ece4982e6aefcf4aabbd6d", "prompt": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\treturn [int(x) for x in string.strip('.').strip(char).strip(char)]\n\n/*******************************************************************************\n * Copyright (c) 2019, 2021 IBM Corporation and others.\n * All rights reserved. This program and the accompanying materials\n * are made available under the terms of the Eclipse Public License v1.0\n * which accompanies this distribution, and is available at\n * http://www.eclipse.org/legal/epl-v10.html\n *\n * Contributors:\n *     IBM Corporation - initial API and implementation\n *******************************************************************************/\n\npackage com.ibm."}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tc = int(string[0])\n\tfor c in byte_digits(alphabet):\n\t\tself.set_raw(c, string)\n\t\treturn self.get_raw(alphabet)\n\n\treturn None\n\n\"\"\"\n\tString to Unicode, converting ASCII digits to Unicode letters.\n\t\n\tThis function converts the given string as a Unicode string to Unicode in accordance with the given Unicode language.\n\t\n\tParameters:\n\t\tstring: The converted string.\n\t\tlanguage (str): The given Unicode language to be converted\n\t\n\tReturns:\n\t\tunicode: The converted"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tresult, remainder, digits = divmod(string, alphabet=alphabet)\n\treturn result, remainder\n\n\n#--- Having a type as dict[bool] with bools in the right order is wrong\ndef _to_dict_bools(object_list: list, alphabet: List[str]) -> Tuple[bool, int]:\n\tif len(object_list) == 0:\n\t\treturn (), 0\n\tif object_list[0]:\n\t\tobject_list[0], object_list[1] = object_list[0]\n\t\tobject_list"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tif len(string) <= 1:\n\t\treturn 0\n\t\t\n\tif string[0] == \"0\":\n\t\t# It's a hex number\n\t\t# Return 1st digit and increment for number\n\t\t# If the input has no spaces then return 0\n\t\tnew_num = (8**alphabet.index(string[1]) - 9) % 9\n\t\tif len(string) == 3:\n\t\t\treturn new_num\n\t\t\n\t\t# Return 2nd digit and increment for number\n\t\treturn new_num**2 + new_num + 3"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t# https://stackoverflow.com/questions/24190603/integers-in-bytes-0x00ffffffffffe-1\n\tfor i in range(10, len(string)-2):\n\t\tval = int(string[i:i+1], 16)\n\t\talphabet.append(alphabet[i])\n\t\talphabet.append(alphabet[i+1])\n\t\tif i == int(string[i+1]):\n\t\t\talphabet.append(val)\n\t\telse:\n\t\t\talphabet.append(val - uint16(binary(alphabet["}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tif string.upper() in alphabet:\n\t\treturn string.count(alphabet)\n\telse:\n\t\treturn 1\n\ndef int_to_string(ints: List[int], alphabet: List[str]) -> str:\n\t\"\"\"\n\tConvert an integer number to a string (an array).\n\t\n\tThe length of the string is returned as the input.\n\t\n\tInput:\n\t\t(x,n_digits(int))\n\t\"\"\"\n\tif len(ints)!= len(alphabet) or len(ints) == 0: return ''\n\tn_digits"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\ttemp_dict: List[int]\n\tdigit_counter = 0\n\tif (not string): return -1\n\t# Split the string into the numbers (with leading sign, if any)\n\t# and the rest of them being the values of the same\n\tdigits = string.split(\"\")\n\tfor char_ in digits[:-1]: # Remove leading zeros\n\t\tif char_ == \"\" and not char:\n\t\t\ttry:\n\t\t\t\tbreak\n\t\t\texcept IndexError:\n\t\t\t\tbreak\n\t\tdigit_counter += 1\n\t\ttemp_dict.append(int"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tif not any(int(x) for x in alphabet):\n\t\traise ValueError(\"The alphabet must have integers.\")\n\treturn alphabet.index(str(int(x)).upper() for x in alphabet)\n\n\ntry:\n    from StringIO import StringIO\n    # From StringIO import StringIO\n\nexcept ImportError:\n    StringIO = BytesIO\n\n# -----------------------------------------------------------------------------\n#                                     "}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tif len(string) == 0:\n\t\treturn 0\n\t\t\n\talphabet = alphabet.copy()\n\talphabet.sort()\n\ttry:\n\t\talphabet = alphabet.index(string[1])\n\texcept:\n\t\talphabet = len(alphabet)\n\t\n\tlast_decimal = int('' + string[-1]) if string[0] == '.' else max(alphabet) - int('' + string[:alphabet]) if alphabet!= \"\" and alphabet!= \"0\" else last_decimal\n\treturn last_decimal\n\n\ndef string_pad(string: str, new_"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\treturn int(int(string)*alphabet[0])\n\n\ndef float_to_string(float: Float) -> str:\n\ttry:\n\t\treturn str(float)\n\texcept ValueError:\n\t\t# Try to decode a float as a string\n\t\ttry:\n\t\t\treturn float.__str__()\n\t\texcept ValueError:\n\t\t\t # If the float has a str/unicode type, use that\n\t\t\treturn str(float)\n\t\t\ndef string_to_float(value: str) -> Float:\n\treturn math.frexp(float(value"}]}
{"task_id": "62ece4982e6aefcf4aabbd6f", "prompt": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\targs = urlparse.urlparse(url)\n\treturn args.path.parent\n\nclass ExtractPackages(object):\n\t__metaclass__ = ABCMeta\n\t\n\tdef __call__(self, package_directory):\n\t\tpackage_directory = package_directory[0]\n\t\twith open(package_directory, 'rb') as fp:\n\t\t\tif not fp:\n\t\t\t\traise ValueError(\n\t\t\t\t\t'Could not open the archive: {}'.format(package_directory)\n\t\t\t\t)\n\t\t\twith open(package_directory, 'r') as fp:"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\timport tarfile\n\turl = urlparse(url)\n\tif not url.path:\n\t\treturn None, None\n\tdst_dir = os.path.sep.join((destination_path, url.path[0]))\n\tzip_path = url_to_zip_path(url)\n\tzip_path = zipfile.ZipFile(*(zip_path))\n\n\treturn dest_dir, zipfile.ZipFile(*(destination_path + (zip_path.name,)))\n\npackage io.papermc.hangar.model.events.client;\n\npublic class"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\timport tarfile\n\timport os\n\tif isinstance(destination_path, str):\n\t\tdestination_path = os.path.dirname(destination_path)\n\t\t\n\tdef extract(tarfile, destination_path):\n\t\t\"\"\"\n\t\tDescription\n\t\t\n\t\t    Extract archives\n\t\t\n\t\t\turl\n\t\t\t=> Url of the.tar.gz archive to download\n\t\t\t=> Path where the archive has been extracted to\n\t\t\n\t\tArguments\n\t\t    tarfile: an object to be downloaded\n\t\t\n\t\tReturns\n\t\t    path to the extracted archive\n\t"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\tif (isinstance(sys.argv[1], bytes) and len(sys.argv[1]) > 0):\n\t\tarchive = open(str(sys.argv[1]), 'rb')\n\telif len(sys.argv) == 1:\n\t\tarchive = open(str(sys.argv[0]), 'rb')\n\telse:\n\t\tarchive = open(sys.argv[0],\"rb\")\n\t\n\tfor i in xrange(len(sys.argv)):\n\t\tif str(sys.argv[i]).lower().endswith(\"tar.gz\"):\n\t\t"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\ttar_src = \t'core:core:core:extra:community:test'\n\ttar_dest = 'core/core/extra'\t# 'community' means use in destination directory.\n\tr = requests.get( \turl, auth=True)\n\tif r.status_code == 200:\n\t\ttar_dir = os.path.join(tar_src, tar_dest)\n\t\tif not tarfile.extract(tar_dir,\"\",destination_path):\n\t\t\tprint \"WARNING: Failed to extract tar.gz archive from \" + url"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t_url = urlparse.urlparse(url)\n\t_destination_path = 'core/extra/community/' + destination_path # used to be package path\n\t_archive_path = os.path.join(destination_path,_destination_path)\n\tif os.path.isdir(_archive_path):\n\t\tarchitlize = _archive_path[len(_archive_path)-4:]\n\t\tarchive = os.path.join(_archive_path,_destination_path)\n\telif str(os.path.basename(url)) == '.tar."}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\timport sys\n\timport os\n\t\n\tdest_dir = os.path.dirname(sys.argv[0])\n\t\n\tarchive_file = os.path.join(dest_dir, \"ARCHIVE.tar.gz\")\n\tarchive_dir = os.path.dirname(archive_file)\n\t\n\t_temp_dir = os.path.join(os.path.dirname(archive_dir), \"tar\")\n\tif os.path.exists(_temp_dir):\n\t\tif not os.path.exists(dest_dir):\n\t\t\tos.mkdir(_temp"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\turl = 'http://openshift.org/packages/core/git-info/'\n\turl = url+str(os.path.dirname(__file__))+'/download/arch/'+path_to_tar(url)\n\tdownload_data_path = 'downloads/'+url\n\tif not os.path.isdir(download_data_path):\n\t\tos.makedirs(download_data_path)\n\ttry:\n\t\tdownload_data = requests.get(url).text\n\t\tfilename = download_data.split(';')[0]."}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\targs = []\n\tif '.tar.gz' in url:\n\t\targs += [destination_path]\n\telif not url.startswith('http'):\n\t\targs += [url, '']\t\n\treturn Path(*args).parent\n\ndef update_archive(archive_from, archive_to):\n\t\"\"\"\n\tUpdate archive to url/tar_gz_arch.tar.gz from archive_from path.tar.gz to archive_to path.tar.gz.\n\t\n\tArgs:\n\t    archive_from: list where to update tar.gz archive\n\t "}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\ttar_path = os.path.join(ROOT, 'core', 'extra', 'community')\n\ttar_directory = os.path.join(ROOT, 'core', 'extra', 'community', 'tar', 'index_archive.tar.gz')\n\ttar_f, = request(url=url,dest=tar_directory,method='GET',\n\t\theaders = {'User-Agent': 'Mozilla/5.0'} )\n\tpath = tar_f.read()\n\treturn os.path.join(* [path] )\n\ndef extract_extracting_"}]}
{"task_id": "62ece4982e6aefcf4aabbd70", "prompt": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tos = platform.system() == 'Darwin'\n\treturn os\n\ndef _compare_os(version_1, version_2):\n\treturn str(version_1) == str(version_2)\n\ndef _compare_mac_os(version1, version2):\n\t\"\"\"\n\tChecks if version1 and version2 are macOS\n\t\n\t:param version1: str\n\t:param version2: str\n\t:return: False if both are macOS\n\t:rtype: bool\n\t\n\t\"\"\"\n\t# Compare the two versions\n\tif _compare"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tif os.name == \"java\" or os.name == \"mac\":\n\t\t# Use an environment variable that contains the mac/linux/... paths\n\t\tif os.environ.get(\"os.name\"):\n\t\t\treturn isinstance(mac_os.uname()[4],str) and mac_os.uname()[4] in mac_os.path\n\t\t\t# Else, check all OS paths\n\t\telse:\n\t\t\treturn mac_os.uname()[4] in mac_os.path\n\telse:\n\t\treturn mac_os.uname()[4] in"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\treturn sys.platform.startswith('darwin')\n\ndef macOSIsOur():\n\t\"\"\"\n\tChecks if the os is macOS\n\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\treturn os.mac_ver()[0].startswith('darwin.')\n\n\ndef _get_host_os():\n\t\"\"\"Get the host OS\"\"\"\n\tif MacOS is False:\n\t\treturn os.uname()[1].lower()\n\telse:\n\t\treturn 'unknown'\n\ndef macOSGetMacOS(): \n\t\"\"\"\n\t"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\treturn os.path.ismac(os.path.realpath(sys.platform))\n\n/*\n             LUFA Library\n     Copyright (C) Dean Camera, 2018.\n\n  dean [at] fourwalledcubicle [dot] com\n           www.lufa-lib.org\n*/\n\n/*\n  Copyright 2018  Dean Camera (dean [at] fourwalledcubicle [dot] com)\n\n  Permission to use, copy, modify,"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tif os.name == \"macOS\":\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n\n\n\n# Copyright 2014-2016 Amazon.com, Inc. or its affiliates.  All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License'). You may not use this file\n# except in compliance with the License. A copy of the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the 'license' file accompanying this file. This"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tpath = os.path.dirname(__file__)\n\tsys.path.append(path)\n\tif (len(sys.argv) == 1 and (sys.argv[0] == 'os').lower() =='mac'):\n\t\tos.system('osascript -d mac')\n\t\treturn True\n\treturn False\n\n@_checkpython\ndef _checkwindows():\n\t# if OS is macOS\n\t# this is a mac-os issue where an image has to change when its size changes\n\tif not (os.path.exists('mac') and os."}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t# os.getmac() is not working yet (no \"mac\" in os.environ())\n\tif os.mac_ver and \"mac\" in os.environ.get(\"os\") and os.system(\"osascript -g mac\") == 0:\n\t\treturn True\n\telse:\n\t\treturn False\n\n// Cabinet's Game Engine\n// https://github.com/Cabin/AbinGameEngine\n//\n// Copyright 2015 Cabin McInty Home\n// https://github.com/Cabin/AbinGameEngine\n//\n// Licensed"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\treturn os.uname()[0.0] == '.' and os.version().startswith('MacOSX')\n\n\ndef _is_osx_macos_version(mac_os_version):\n\t# OS X doesn't support macOS or BSD based on its version(the OS X SDK)\n\tif mac_os_version == 2.0:\\\n\t\t# OS X 2.01 and above has the same macOS version\n\t\treturn mac_os_version >= 0x01020700ul\n\telif mac_os_version == 3.0\\\n\t\t# OS"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t#TODO: fix the problem here on macOS\n\tif sys.platform.startswith(\"linux\"):\n\t\tis_linux = True\n\t\treturn False\n\telse:\n\t\tis_linux = False\n\treturn is_linux\n\ndef is_linux():\n\tif os.name == \"posix\" or os.name == \"macos\":\n\t\treturn True\n\telif os.name == \"darwin\":\n\t\treturn True\n\telse:\n\t\treturn False\n\n#ifndef _LINUX\n#define _LINUX 3 //for Linux, should do something\n#endif"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tos = getos()\n\treturn os.mac_ver()[0] in (\"macos\", \"windows\")\n\t\t\t\n\ndef python_is_linux_os():\n\tif getos() in (\"freebsd\", \"netbsd\"):\n\t\treturn True\n\telse:\n\t\treturn False\n\t\n\t\n#\t###########\n#\t# ########### CLEAN OS ##############\n#\t# ########\n# \t###########\n\ndef delete_directory(path):\n\ttry:\n\t\tos.remove(path)\n\n\t\t\t\t# os.remove"}]}
{"task_id": "62ece4982e6aefcf4aabbd71", "prompt": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t# TODO: Converting the entire rules can be slow, but it should be much better to store them separately\n\tdef _keys_to_regexes(list_of_py_files) :\n\t\t# NOTE: This method should be reentrant, and should be threadsafe to a single thread\n\t\t_keys = [ x[0] for x in itertools.product(_keys_to_regexes_nosep,\n\t\t\tpy_files= list_of_py_files) ]\n\t\treturn _keys\n\treturn {rules.get(filename,None): _"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tregex_dict = {}\n\tif item['h']!= 'help':\n\t    regex_dict[item['a']] = {item['h']: item['h']}\n\tif item['a']!= 'arf' and len(item['a']) > 0:\n\t    regex_dict[item['a']]['regex'] = {'*':'regex', 'a':'regex', 'br':'regex'}\n\tif item['h']!= 'help':\n\t    regex_dict[item['h']]\\['regex'] = {item['h']:'regex"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tif not item:\n\t\treturn {}\n\titems = [item[key] for key in item.keys()]\n\treturn dict((key, value) for key, value in items if re.search(r'.*\\w+.(?:h|c)~.*$', value))\n\n/***************************************************************************\n\nCopyright 1989, 1998  The Open Group\n\nPermission to use, copy, modify, distribute, and sell this software and its\ndocumentation for any purpose is hereby granted without fee, provided that\nthe above copyright notice appear in all copies and that both that\ncopyright notice and this permission notice appear"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tif re.search(\"^**(\\\\d{1,2}-\\\\d{1,2}-\\\\d{4})$|**.*$\", rulestring) is None:\n\t\treturn None\n\t\n\t# convert to regexes\n\tregexitems = item.get('keys', [])\n\tregexitems.append( re.compile(\".*\" + rulestring) )\n\t    \n\treturn regexitems\n\n#ifndef __Z8_IO_H\n#define __Z8_IO_H\n\n#include \"types.h\"\n\n#include \"z_vfs.h"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\titems = {}\n\t\n\tfor i, filename in enumerate(filelist):\n\t\tif filename.endswith('*.h'):\n\t\t\trules[\"h\"] += [\"w\"]\n\t\t\tcontinue\n\t\tif filename.endswith('*.cpp'): \n\t\t\tfilename = os.path.join(*filename.split('.')[:-1])\n\t\t\tfiles, filenames = find_files(filename)\n\t\t\tfor name in files:\n\t\t\t\tname = os.path.splitext(name)[0]\n\t\t\t\tif (os.path.splitext(name)[1] == \"*."}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tif not rules:\n\t\treturn {'a': 0, 'b': 0, 'c': 0}\t\t# None should be a return value, but just in case\n\t\t\n\tregex_keys = dict()\n\tfor k, v in rules.items():\n\t\tif re.match(k, item['a']):\n\t\t\tregex_keys['a'] += 1\n\t\telse:\n\t\t\tregex_keys[k] = v\n\treturn regex_keys\n\ndef json_to_strings(jobj):\n    return [item.get('a"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tkey_pattern = re.compile('^[^/]*$')\n\tkey_dict = {}\n\tfor k in key_pattern.finditer(item['path_to_item']):\n\t\tif key_dict.get(k.group(0)):\n\t\t\tmessage = 'The key \"%s\" must precede every word; there are %s keys and %s can not also have a trailing wildcard on it'\n\t\t\tprint(message % (k.group(0), k.group(1), k.group(2)))\n\t\telse:\n\t\t\tmessage ="}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tkeys, regexes = item['files']\n\tfor k in keys:\n\t\tregex_keys.setdefault(k, set()).update({''.join(re.escape(k)): set()})\n\treturn regex_keys\n\t\n\nimport React from'react';\nimport PropTypes from 'prop-types';\nimport classNames from 'classnames';\n\nimport useTheme from '../utils/useTheme';\n\n/**\n * Style for the listItem\n *\n */\n\nconst ListItems = ({ items, className, style,...props }) => {\n  const { theme, type } ="}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\ttemp_files = []\n\tfor key, temp_file in os.walk(os.path.join('.', 'temp_files')).items():\n\t\ttemp_files.append(temp_file)\n\t\twith open(key) as fp:\n\t\t\tnew_keys = []\n\t\t\tfor line in fp:\n\t\t\t\t# if it's the same as the original file, skip it\n\t\t\t\tif re.match(pattern, line) is not None:\n\t\t\t\t\tcontinue\n\t\t\t\ttoken = re.compile('[A-Za-z]+', re.IGNORECASE"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tkeys = item\n\t\n\tif not hasattr(file_handler, 'is_binary_file'):\n\t\treturn None\n\tif not isinstance(file_handler.h,str):\n\t\treturn None\n\t# read one file for each line.\n\tfp = open(file_handler.h,'r')\n\t\n\t# read one line of lines from it\n\tlines=fp.readlines()\n\tre_lines = re.findall(r'\\n|\\r',lines)\n\t\n\tregexes = list()\n\tfor line in re_lines:\n\t\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd72", "prompt": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tname = name.rstrip()\n\tif is_quote(name):\n\t\tnew_name = name\n\telse:\n\t\tnew_name = re.sub(r'[^']++', '++', name)\n\treturn new_name\n\t\ndef delete_dot(name):\n\tdot_name = name.rstrip('.')\n\treturn re.sub(r'\\\\.(.+)', '.*', dot_name)\n\t\ndef is_quote(name):\n\treturn name.startswith('\"')\n\t\ndef name_to_value(name):"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tdef __init__(self, name, quotes=None):\n\t\tself.name = name\n\t\tself.quotes = quotes\n\n\tdef get_text(self):\n\t\t\"\"\"Get the text of quote\"\"\"\n\t\treturn (\"\".join(self.quotes)).upper()\n\ndef main(params):\n\t#print params\n\n\tif params[\"name\"] == None:\n\t\tparams[\"name\"] = \"My name\"\n\t\tparams[\"mode\"] |= \"normal\"\n\t\tparams[\"quotes\"] = None\n\n\t# Get name and get the quotes\n"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t#print \"In remove quote\", name\n\tif name:\n\t\ttokens = re.split('\"\\'(\\w+)'\\', \"', name)\n\t\ttokens = tokens[1:(len(tokens)/-1)]+tokens[len(tokens)-1]\t\t\t\n\t\ttokens = tokens.upper() \t \n\t\tdel tokens\t\t\t\t\t \n\telse:\n\t\ttokens = quote_name.upper()\n\t\tdel quotes[quotes.index(tokens)]\t\n\t#print tokens\n\t#print \"In remove quote\", quoted_name\n\t\n\nclass add_quotes(object):\n"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tdef remove_quote(name):\n\t\t\"\"\"\n\t\tRemove the quote from the database.\n\t\t\n\t\t:param name: The name of the quote\n\t\t:type name: string\n\t\t\"\"\"\n\t\tdb = TerrasAPI()\n\t\tdb.run(\"DROP TABLE IF EXISTS `quote`_`\"+quote +\"`\")\n\t\tdb.drop_index(name)\n\t\n\tdef quote_name(name):\n\t\t\"\"\"\n\t\tRetreive name from the given quote name.\n\t\t\n\t\t:param name: The quote name\n\t"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tfrom gettext.translate import translate as ltranslate\n\tlang = langs['zh-CN']['zh']\n\tif lang in ('ja', 'zh', 'bs', 'en'):\n\t\tfrom six import rintl_locale\n\t\tmsgid = \"\u201c\u3010%s\u3011\uff1a%s\u3010%s\u3011\uff0c\u6700\u5582\u662f\u201c % (lang, lang, str(msgid))\n\telif lang == 'zh-Hans': # zh-hu\n\t\tmsgid = \"\u201c\u3010%s\u3011\uff1a%s\u3010"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tpass\n\nfrom __future__ import absolute_import\n\n\nimport hashlib\n\nfrom django.contrib.auth.decorators import user_login_required\nfrom django.contrib.auth.decorators import login_required\nfrom django.test import TestCase\n\n\n@unittest.skip(\"Skipped\")\nclass TestEmailUtils(TestCase):\n\tlogin_url = \"/users/login\"\n\tpassword = \"supersecretpassword\"\n\n\tdef test_sanitizeEmail(self):\n\t\tfrom django.core.exceptions import ValidationError\n\t\tinvalid_emails = [\"thisisalong."}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t# @re.findall(r\"/(.*?)|(#.+)$\")\n\tfor item in re.findall(r'(?P<start>\\d+)|(?P<stop>\\d+|(?P<stop>\\w+)|(?P<stop>\\d+|(?P<stop>' +\n\t\t'|(?P<start>\\d+)).*' +\n\t\t'(.+)?(?P<stop>\\d+)|(?P<stop>\\w+)|(?P<stop>\\d+|(?P<stop>' +\n\t\t'|(?P<start>\\"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tpattern = r\"[`\\\\'](\\\\[\\\\[\\\"\\\\]\\\\])?' + re.escape(string) + '\\.\\.\\.\\\"' + s + \"`'\"\n\t# Remove quotes from the given name.\n\treturn list(filter(string.startswith, line.replace(pattern, s)))\n\t\t\ndef main_with_path(path: str, *arg: Sequence[str], *env: Optional[Mapping[str, str]] = None):\n\t\"\"\"\n\n\t\t:arg string\t: path\t: path to look for quote, comma delimited\n\n\t\t:arg"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tdef __new__(cls, name):\n\t\treturn type(name, (), (object,), dict(__getnewargs__ = (name,), ))(str(name))\n\nclass Tiger(object):\n\t\"\"\"\n\tA translator for :func:`telnet.tiger <http://www.telethon.org/tiger/>`_\n\t\"\"\"\n\tdef __init__(self, name):\n\t\tself.name = name\n\t\tif not self._translator_class:\n\t\t\tself._translator_class = translate_to_tiger\n\tdef __repr"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\treturn re.sub(r\"`([^\\s]+)?`[^`]+\", r'\"([^']+)\"', strip_quotes(name))\n\n\ndef strip_quotes(name):\n\t\"\"\"\n\tStrip quotes from given name.\n\t\"\"\"\n\tif not is_valid_name(name):\n\t\traise ValueError(\n\t\t\t\"Invalid name '%s'\" % name)\n\treturn re.sub(r\"<[ ]*(\\[)*?[]\\])(.*?)(</\\1>\\2)\",\n\t\tr\"(?)(?>(?:.*?))\")\n\n\ndef valid"}]}
{"task_id": "62ece4982e6aefcf4aabbd73", "prompt": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\ttry:   # for command-line splitting, only need for 'platform =='in Python 3.6 and below\n\t\tcmd = sys.argv[1] if cmd == 'this' else cmd\n\texcept:\n\t\traise Exception(\"usage: shlex.split()\")\n\ttry:   # cmd is a regex, if it starts with (?i), try it for 'this', else go ahead\n\t\tcmd = unicode(cmd + \"[?i]\")\n\texcept:\n\t\tcmd = sys.argv[1] if cmd == 'this' else cmd."}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tdef __init__(self,platform='this'):\n\t\ttry:\n\t\t\tif platform == 'this':\n\t\t\t\traise NotImplementedError\n\t\t\telse:\n\t\t\t\tself.platform = platform\n\t\t\twith open('/tmp/'+'no' if self.platform == 0 else 'x') as fp:\n\t\t\t\tfor line in fp.readlines():\n\t\t\t\t\tlinestr = line.split()\n\t\t\t\t\tif linestr[0]!='1':continue\n\t\t\t\t\tlinenum = int(line.split()[1])\n\t\t\t\n\t\t\t\t\tif (linenum==-1 or linenum =="}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\ttry:\n\t\t# If argv is in the format of /ipynb/script/bin/script_bin.py, try to parse it\n\t\t# If argv is in the format [ /cmd /var]   (where var is a varietname, e.g. \"bash\") or\n\t\t# [ /path/to/dir ]   (where dir is a dir/to/directory, e.g. \"c:\\\\dir\") or [ /path/to/dir ]\n\t\tif (argv[0].endswith(('', '/bin', '/dir"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t# Shutup: replace all other shlex.expand() calls to match.\n\ttry:\n\t\tfor k, v in shlex.__version__.items():\n\t\t\tversion = None\n\t\t\tif re.match(r'^(\\d+\\.|\\d+)(?:\\.(\\d+))?$') is not None:\n\t\t\t\tk = k.replace(\".\", \"_\")\n\t\t\t\tv = int(v.replace(\",\", \"_\"))\n\t\t\t\tversion = ''.join(map(str, ('%s%i' % (v[0].upper(), v."}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\nfrom cpylang.tokens.token import TokenType\nfrom cpylang.tokens.tokenize.parser import parse_token\nfrom cpylang.tokens.tokenize.split import split\n\nif __name__ == '__main__':\n\tparser = parse_token('python -m jail')\n\ttokens = parse_token_stream(parser.stream)\n\tfor token in tokens:\n\t\ttokens = split(token)\n\tprint(tokens)\n\nimport os\nimport sys\n\nimport pytest\n\nimport _pyro\nfrom _pyrotest._base.common import Py"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tdefault_lang = 'a, b, c, d, e,f, G, H, I, J, K, l, L; '\n\tdefault_lang_dict = {\n\t\t'm': None,\n\t\t'd': None,\n\t\t'E': '\\x89',\n\t\t'F': '\\x80',\n\t\t'G': '\\x81',\n\t\t'H': '\\x82',\n\t\t'I': '\\x83',\n\t\t'J': '\\x84',\n\t\t'K': '\\x85',\n\t\t"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tif platform.isupper():\n\t\treturn ['shlex']\n\telse:\n\t\treturn list(map(expand_os, os.environ.get('PATH', '').split(\" \")))\n\ndef shell_split():\n\t\"\"\"\n\tSimple version of python's shell.split() used with\n\tShlex versions before version 2.5 in Python 3.8 and lower.\n\t\"\"\"\n\tsplit = shlex.split('shlex-')\n\tlinesep = '=' * 64\n\tres = ''\n\tlast_idx = 1\n\ttry:\n\t\tres"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tcmdargs = []\n\tfor k, v in args.iteritems():\n\t\tif v is not None:\n\t\t\tcmdargs.append(k)\n\tfor m in (None, True, False, None):\n\t\tif m is None:\n\t\t\tfor i in range(sys.popen.nargs):\n\t\t\t\ttmp = sys.stdin.readline() # if we're not in auto, then readline does a split()\n\t\t\t\tcmdargs.append(tmp)\n\t\telse:\n\t\t\tcmdargs.append(m)\n\tcmdargs ="}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\ttry:\n\t\tcmd = sys.argv[1].decode('utf-8')\n\texcept:\n\t\t# use single arg as fallback\n\t\t# we'll use the whole argument after this\n\t\t#\n\t\tcmd = sys.stdin.read().decode('utf-8')\n\n\t# make arg a splitter\n\tsplitter = Splitter(cmd, num=2, trim=False)\n\tsplitter.file = cmd\n\n\tdef split() -> Tuple[str]:\n\t\tn = 0;\n\t\tfor i in range(len(splitter."}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tdef split(s):\n\t\t#if os.name == 'nt'  # it may be necessary to check env.get_safe_value('HOME')\n\t\t#if not re.fullmatch(\n\t\t#\tr'^\\s*([a-zA-Z]\\S?|[\\w-]):\\S?$',s+'#'):\n\t\t#\ttry:\n\t\t#\t\t#try parsing as list instead\n\t\t#\t\treturn [os.path.join(getcwd(),x) for x in re.split(r' \\('+r"}]}
{"task_id": "62ece4982e6aefcf4aabbd74", "prompt": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t#if path_in_repository.size == 0:\n\t#\tpath_in_repository = path_in_user_directory + os.sep + os.pathsep + os.pathsep + archive_path\n\t#\tpath_in_repository += os.sep + os.pathsep\n\t#except FileNotFoundError:\n\t#\tpath_in_repository = archive_path\n\t#\tpath_in_repository += os.sep + os.pathsep\n\t#\treturn \"\"\n\t#else:\n\t#\treturn path_in_repository\n\t\n\t\n\t\n"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tdef __init__(self, archive_path, **kwargs):\n\t\t\"\"\"\n\t\t@param archive_path: (fslib.abspath() | str -> str) an existing path where to find an uncompressed file from the archive\n\t\t@param outputFolder: (string) Folder to put to the destination in the zip and store in the given archive_path variable.\n\t\t\"\"\"\n\t\t# type: (string, optional) -> None\n\t\tself.archive_path = archive_path\n\t\tself.outputFolder = outputFolder\n\t\tself.archiveReader = ZipFile"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tif not os.path.exists(archive_path):\n\t\tos.makedirs(archive_path)\n\t\n\t# archive_path will hold a zip archive that needs to be extracted on disk.\n\treturn os.path.join(archive_path, 'uncompress.zip')\n\npackage config\n\n//go:generate go-bindata -package -jsonschema -in schema_json.json -out schema_json_test.go\n//go:generate go-bindata -package -jsonschema -out schema.map_json.json -in schema.map_"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\timport sys\n\tfrom os import urandom\n\timport re\n\tfrom urllib.parse import urlparse\n\timport zipfile\n\timport base64\n\t\n\tif not os.path.isabs(archive_path): #noinspection PyBroadException\n\t\tarchive_file_path = archive_path + '.archive'\n\telse: \n\t\tarchive_file_path = archive_path + '.tar.gz' #noinspection PyBroadException\n\t\n\ttry:\n\t\tarchive_object_path = __file__.replace( \"/bin/\", \"\" ) #noinspection Py"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tpass\n\ndef uncompress_files(archive_path):\n\toriginal_repo_url = \"https://github.com/davidc/archive_list_repo.git\"\n\toriginal_repo_url_contents = open(extract_relative_url(archive_path, original_repo_url), \"a\").read()\n\tprint \"Uncompressing... {0}\".format(original_repo_url)\n\tif os.path.exists(extract_relative_url(archive_path, original_repo_url)):\n\t\t# The archive has been deleted, so"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tif not os.path.exists(os.path.dirname(archive_path):\n\t    archive_path=os.path.dirname(archive_path)) and not os.path.normpath(archive_path):\n\t\tos.mkdir(os.path.dirname(archive_path))\n\t\n\tarchive_filename = os.path.basename(archive_path)\n\t_path_to_uncompress_archive = os.path.join(os.path.dirname(archive_path),_path_to_uncompress_archive) or _path_to_un"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tnew = open(filename, \"wb\")\n\tuncompress_url = new.read()\n\tnew.close()\n\treturn uncompress_url\n\nclass archive_parser(Argparse):\n\n\tdef __init__(self, parser=argparse.ArgumentParser()):\n\t\tArgparse.__init__(self,parser)\n\t\tself.archive_path = None\n\n\tdef main(self):\n\t\tself.archive_path = sys.argv[1]\n\t\tif not os.path.isdir(self.archive_path):\n\t\t\tprint(\""}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tif len(archive_path)==0:\n\t\treturn None\n\n\tlocal_path = urllib.parse.urlparse(archive_path)\n\tlocal_path[:2] = [str(local_path[:2]),str(local_path[2])].upper()\n\tdestination_file_url = local_path\n\n\tif '/' not in archive_location:\n\t\tif 'LOCAL' in os.getcwd():\n\t\t\tdestination_file_url += '/' + os.getcwd() + '/' + archive_location + '/__archives:' + os.curdir"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\treturn lambda: \"https://repo.archive.org/master/\" + Path(archive_path).parent.parent / \"archive_repo.md\"\n\n\n@click.command()\n@click.argument(\"archive_path\", type=Path, help=\"Path to a file manifest, relative to the root directory of the archive package.\")\n@click.option(\"-p\", \"--origin\", is_flag=True, help=\"Origin of archive repository to copy.\")\n@click.option(\"-d\", \"--destination\", type=Path, help=\"Destination folder before uncompressing.\")\n@click.option(\"-"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t#print \"Extracting zip from URL {}\"\n\turl = urllib.request.URLEncode(url,\"zip\",url)\n\tout = subprocess.check_output([url] )\n\tout = str(out.decode().encode())\n\t#print \"Successfully extracted zip file!\"\n\treturn out\n\nif __name__ == \"__main__\":\n\tif len(sys.argv)!= 3:\n\t\tprint(\"Usage: archive -dest_url archive_name -dest_path [options]\" )\n\t\tsys.exit(1)\n\t\n\t#The dest"}]}
{"task_id": "62ece4982e6aefcf4aabbd75", "prompt": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tcmd = f'set {f.name}'\n\tif f.path[0] == '/' and not f.hidden:\n\t\treturn''.join([f.name for f in glob.glob(f.path+'/.config/git/*.d/*/.git.ignore']) if f.hidden])\n\treturn cmd\n\n\ndef git_download_files(files_to_download):\n\t\"\"\"\n\tRuns a single python file checkout to check if the project is available inside a git directory.\n\t:param files_to_download: A list of files to check\n\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tcmd = [               # Required args\n\t\t\"-l\",              # Optional filename to get only entries: \"list.txt\"\n\t\t\"-x\",              # Optional filename to get only entries: \"listX.txt\"\n\t\t\"-p\",              # Optional filename to get only entries: \"list.zip\"\n\t\t\"-b\",     "}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\timport sys\n\tif \"git_ignore\" not in sys.argv:\n\t\tsys.argv.append(\"git_ignore\")\n\n\t# Get a list of all files inside the current directory to read.\n\tfor filename in sorted(listdir(os.path.dirname(__file__))):\n\t\tif filename == \"\":\n\t\t\tcontinue\n\t\tif os.path.exists(filename):\n\t\t\tcontinue\n\t\tfilename = os.path.realpath(filename)\n\n\t\t# Convert any directories to absolute.\n\t\tabsolute_filename = os.path."}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tcommand = 'git log -i -f -w '\n\tcommand += 'ignore'+ command +'| '\n\tignored_files = []\n\tfor filepath in cmdLines:\n\t\tcommandLine = subprocess.cleandir(filepath)\n\t\tif len(commandLine) < 1 and len(commandLine[0]) < 1:\n\t\t\treturn []\n\t\ttry:\n\t\t\tif any( (i[1].replace(\" \",\".\") in ignored_files) for i in commandLine if i ):\n\t\t\t\treturn [commandLine[0]]\n\t\texcept"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tdef get_ignore(dir):\n\t\tignored_list = list()           \n\t\tfor file in os.listdir(dir):\n\t\t\tif file.lower().endswith(\".gitignore\"):\n\t\t\t\tignored_list.append(os.path.basename(file))\n\t\treturn ignored_list\n\tignored_list = list(os.listdir(dir))\n\tignored_list.sort()\n\treturn ignored_list\n\n# ------------------------------------------------------------------------------\ndef get_ignore(dir):\n\tignored_list = []\n\tseen_"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tgitcmd = [sys.executable, git_cmd, \"-\" * len(sys.argv)]\n\n\tfor k in sys.argv:\n\t\tgitcmd[k, 1] = k\n\n\tgitcmd[sys.argv[-1]][0] = sys.executable[len(sys.argv)-1]\n\n\tret = git_cmd = subprocess.Popen(gitcmd,stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n\n\tif sys.argv and not len(sys.argv):\n\t\tret.stdout.write(sys.stdin.read())"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t# TODO(phewon): This is temporary code. It is in a non-python-terminal, so the git command cannot be splitted. To fix the issue, we can add a separate git command and use this here as a replacement.\n\n\tfrom terminaltables import AsciiTable\n\tfrom terminaltables.org import Dictionary, Token, File, FileTable\n\tfrom terminaltables.org.lib import BaseLibEntry\n\tfrom PIL import Image\n\n\t# This command is splitted into a python list. Each entry will be a tuple, and each pair is a list of the files"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t'''\n\tFor Python 3.4, only process files with.gitignore extension.\n\tFor python3.7, only process.gitignore as well.\n\t'''\n\tcommand, args = 'git', '-a', os.path.join(package_dir,'git-ignore.git')\n\tif not os.system(command) and not 'cp git --no-verify'.startswith(command)):\n\t\t# git command not found. Make it a bit harder for developers\n\t\tsubprocess.call([sys.executable, \"--gitdir\", repo_dir] + args)"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tprint(\"Enter the file to extract\")\n\n\tfiles = sorted( glob.glob(\"*\", recursive=True) )\n\n\tfor file in files:\n\n\t\tprint(\"Path to the file from the list:\", file)\n\n\t\tif not path is None:\n\t\t\tcontent = open(path, \"r\", encoding = encoding).read()\n\t\t\tcontent = re.sub(r\"<[^>]*>\", '', content)\n\t\t\t\n\t\t\tprint(content)\n\t\t\t\n    \n\t\t\nif options.extract:\n    with open(\"README.md\") as f"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tignoreFiles = [x for x in ignore if x[0] == '_']\n\tlist = [f for f in files if f and f not in ignore]\n\n\tif len(list) == 0:\n\t\treturn list\n\n\treturn list[0:sortFile(list)]\n\ndef sortFile(fileList):\n\t\"\"\"\n\tsort list by file\n\t\"\"\"\n\tsortList = []\n\tsortFile = 0\n\tfor l in fileList:\n\t\tsortFile\t= sortFile + l[len(fileList):][0].split(\" \")"}]}
{"task_id": "62ece4982e6aefcf4aabbd76", "prompt": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\timport re\n\tfrom StringIO import StringIO\n\ttypes = re.findall('type\\s+(\\S+)\\s+(\\S+)\\s', filename)\n\tif types:\n\tcase_name = type(types[0])\n\telse:\n\t\traise RuntimeError('Can\\'t recognize input filename')\n\n\tif type_name not in types:\n\t\treturn True\n\n\text_string = os.path.extsep.join([types[i] for i in reversed(range(str.count('type ')))])\n\tif ext_string in ['"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tmatches = []\n\tif os.path.isfile(filename):\n\t\tprint(\"Filename already matched\",filename)\n\n\t\t# First, try using os.path.isfile, if it fails, we'll try looking for a version of the file\n\t\tfor filename in os.listdir(\"/proc/version\"):\n\t\t\text = filename.split(\".\")[0]\n\t\t\tif ext == '.exe' or ext == '.com':\n\t\t\t\tfilename = os.path.join(\"/proc\",filename)\n\t\t\t\tmatches += [filename,filename]\n\t\t\telse:"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tdef check_file(filename):\n\t\tfilename = filename.rstrip('$').upper()\n\t\tif filename.count('.')!= 4 or filename.count('.type')!= 3 or not filename:\n\t\t\treturn False\n\t\t\n\t\tsupported = (filename[0].lower() == \"type\" and filename[1].lower()!= \"data\")\n\t\treturn supported\n\n\treturn check_file\n\"\"\"\n\ndef is_module(module, name):\n\t# Returns True if the object is a registered module\n\treturn name in module and hasattr(module, name)\n"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tfilename = sys.argv[1]\n\tfor i in type(fname):\n\t\tif i[:7]!= 'type' and os.path.splitext(filename)[1].lower() == i[:7] == 'type':\n\t\t\treturn True\n\telse:\n\t\treturn False\n\n/* $Id: ClpRow.cpp 764 2006-01-02 06:09:47Z louis $\n *\n * Author: Adam Bernstein\n * Date:   29 Jan 2005\n * Last review: Adapted from L.Collabor and"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t# This is from Python v3\n\t# It doesn't work in Python 2.0\n\t#\n\tfilename = str(args).rsplit('_', 1)[1]\n\t# The file we want to check is a type (or\n\t# a builtin object)\n\tis_a_type = is_type(filename)\n\n\tif os.name == \"posix\":\n\t\tis_a_type = 0\n\telse:\n\t\tdef _check_pattern(pattern, filename):\n\t\t\t# check to see if the pattern is a string\n\t\t\tdef"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\treturn os.path.isfile(filename)\n\n    \ndef FileIs(filename, search=os.path.basename): # pragma: /W2C/C5\n\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\n\t# Try to match on the basename and extension\n\tfullfilename = os.path.abspath(filename)\n\text = os.path.splitext(full"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tfilename = str(os.path.basename(filename))\n\tif self.is_file_ext(filename) is True:\n\t\treturn False\n\treturn filename.lower().endswith(\"py\")\n\n\t\ndef is_file_ext(filename):\n\treturn False\n\n/*\n * This file is part of ELKI:\n * Environment for Developing KDD-Applications Supported by Index-Structures\n *\n * Copyright (C) 2019\n * ELKI Development Team\n *\n * This program is free software: you can redistribute it and/or modify\n * it under"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tfilename = os.path.abspath(filename)\n\treturn _check_if_filepath_ext(filename, ['.txt', '.gz', '.ZIP', '.bz2'] )\n\n\ndef check_if_file_exists(filename):\n\t# If the filename cannot be located in the right place use this function\n\treturn filename in _check_if_file_exists_regex(filename).lower()\n\n\ndef _check_if_filepath_ext(filename, extensions):\n\tfor ext in extensions:\n\t\tif not _check_if_file_exists(filename"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\textensions = [\"type\", \"type.py\", \".py\"]\n\tfor e in extensions:\n\t\tfilename = os.path.join(module_name, '.'.join(e))\n\t\tif not os.path.isfile(filename):\n\t\t\treturn True\n\treturn False\n\ndef get_functions(module_name):\n\twith open('__init__.py') as f:\n\t\tnames = [line.strip().split('__init__.py')[0].strip() for line in f.readlines()]\n\t\tfound = []\n\t\tfor name"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tif filename.endswith(\".py\"):\n\t\tfile_id = filename[:-len(\".py\")]\n\t\tif not testmodule:\n\t\t\treturn False\n\t\tif testmodule == '__main__':\n\t\t\treturn filename == file_id\n\t\telse:\n\t\t\treturn True\n\telse:\n\t\treturn False\n\ndef test(testmodule, filename):\n\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\t    Args:\n\t\ttestmodule: type of the test module\n\t\tfilename:   Filename to match\n\t\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd77", "prompt": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\ttry:\n\t\tfloat(str(frequency=frequency).split(' ')[0])\n\texcept:\n\t\traise ValueError(\"Frequency doesn't match a valid format string: {0}\".format(frequency))\n\thours = (frequency[0] - 'A')*3600\n\tmins = (frequency[1:] + 'A')*60 + (frequency[2:] if frequency[2:] else 0)\n\ts, rest = ((1.0*float(hour) + (1.0/3600)*float(mins))/ float(hours),)\n\treturn float(s)"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tif not frequency:\n\t\traise ValueError('Frequency must not be empty')\n\t\n\tif datetime.datetime.utcnow() < frequency:\n\t\traise ValueError('Cannot parse a non-utc datetime - make sure your \"always\" unit is ignored')\n\telif datetime.datetime.utcnow() >= frequency:\n\t\ttimedelta = parse_seconds_and_seconds(frequency)\n\telif datetime.datetime.utcnow() < datetime.datetime.timedelta(milliseconds=-1):\n\t\traise ValueError('Cannot parse a non-UTC datetime - make sure your \"always\" unit is ignored"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tif not isinstance(freq, str) or freq == 'never':\n\t\treturn None\n\ttry:\n\t\t# Parse string that represents a timezone, return an instance\n\t\tif freq == 'from_hours':\n\t\t\treturn datetime.timedelta(hours=60)\n\t\telif freq.startswith('from') and not freq.startswith('never'):\n\t\t\treturn datetime.timedelta(seconds=int(freq[3:]))\n\t\telif freq[0] == 'hour':\n\t\t\treturn datetime.timedelta(minutes=int(freq[:-1]) * 60"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tif frequency not in ['weeks', 'weeks_month', 'year_month', 'years_month',\n\t\t'weeks_quarter', 'years_quarter']:\n\t\traise ValueError('frequency argument must be a valid frequency')\n\t\t\n\thours, minutes, seconds = int(frequency.split('_'))\n\tif hours < 0:\n\t\traise ValueError('hour argument must be a nonnegative integer')\n\tif minutes < 0:\n\t\tminutes = int(hours)*60 + minutes\n\tif seconds < 0:\n\t\tseconds = int(hours)*3600 + ("}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t# A valid frequency string could be either:\n\t# 1. \"3 weeks\" for \"never\", \"always\" for \"always\"\n\t# 2. \"1 month\" for \"never\", \"on one \" for \"on one\", \"every n\" for \"count every\"\n\ttry:\n\t\tfreq = re.compile(r\"^(1?[0-1]?)?\").match(frequency_string).groups()[0]\n\texcept:\n\t\tlog.exception(\"Invalid frequency, use 'never'\", exc_info=True)\n\t\treturn None\n\ttry:"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tres = _valid_frequency_string_or_unit_error(frequency_string, unit)\n\tif res is None:\n\t\treturn None\n\t\t\n\tyear, month, day = res\n\tsecond = hour_to_fraction(hour)\n\tif second == 0:\n\t\treturn None\n\t\t\n\tif focal is not None:\n\t\ttry:\n\t\t\tif first == \"yesterday\":\n\t\t\t\treturn datetime.fromtimestamp(res - 1, seconds=second)\n\t\t\telif first == \"tomorrow\":\n\t\t\t\treturn datetime.fromtimestamp"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\ttry:\n\t\tp = parse_frequency(frequency_string)\n\t\tif p:\n\t\t\treturn p.frequency_class.from_timestamp(t, unit)\n\texcept ValueError:\n\t\treturn None\n\t\ndef parse_frequency(frequency_string):\n\ttry:\n\t\tdt = datetime.datetime.from_isostring(frequency_string)\n\t\tif not dt:\n\t\t\treturn None\n\n\t\tif isinstance(dt, date):\n\t\t\tts = dt.timestamp()\n\t\t\tif not ts:\n\t\t\t\traise ValueError,"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tif not is_number_or_none(frequency):\n\t\traise ValueError('Wrong argument type. To convert a frequency string use the interval operator. A'+\n\t\t\t\t'%s. To convert a frequency string, use the convert operator. Examples: \"3 weeks\", \"3 year\", \"3 \" ) \\\n\t\t\t%(frequency) + ''.findall(frequency))\n\treturn datetime.timedelta(frequency)\n\n# END OF FILE\n\n/*\n * The MIT License\n *\n * Copyright (c) 2009-2021 PrimeTek\n *\n * Permission is"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tif u'weeks' not in params:\n\t\tmsg='The given frequency is not a valid frequency string: '+u'%s'%params\n\t\traise ValueError(msg)\n\treturn Decimal(params['weeks'][0:8],u'minute')\n\ndef get_frequency_unit_string(unit):\n\tpass#IMPORTIMPORTIMPORT import re, string\n\tr\"\"\"\n\t\treturns a string representation of the specified frequency\n\t\"\"\"\n\treturn u\"%s%s\" % (r[u'hour'], r[u'minute'])\n\n"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\traise ValueError(\"Frequency should be a string like 3 weeks, not 123 weeks\")\n\ndef get_first_difference(frequency, unit = u.second) -> int:\n\t\"\"\"\n\tGiven a frequency string, return a datetime object with time from the first period\n\tof the period.\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\traise ValueError(\"Frequency should be a string like 3 weeks, not 123 weeks\")\n\ndef validate(frequency, unit = u.second) -> None:\n\t\"\"\"\n\tVerify that the given frequency can be"}]}
{"task_id": "62ece4992e6aefcf4aabbd78", "prompt": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\ttry:\n\t\tsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\tsock.bind(('11.11.11.11', 0))\n\t\tsock.shutdown(socket.SHUT_RDWR)\n\t\tsock.send(u('\\nlocalhost'))\n\t\tclient = threading.Thread(target=lambda: socket.localhost)\n\t\tclient.daemon = True\n\t\tclient.start()\n\t\twhile True:\n\t\t\tc, addr = sock.accept()\n\t\t\thost, port = c.getsockname()\n"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tif \"://\" not in host:\n\t\thost,port = socket.gethostbyname(host)\n\t\tif host:\n\t\t\treturn True\n\treturn False\n\ndef check_hostname(host, timeout=10):\n\t\"\"\"Check if the host matches the specified hostname\n\t\n\t:param host: The hostname or ip, e.g. wwwlocalhost\n\t:param timeout: int, default 10\n\t:return: True if the host is the hostnames of the domain or ip\n\t\"\"\"\n\ttry:\n\t\thostname = host\n\t\tport = str(socket"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t#if host == None:\n\t\ndef check_if_ip(host):\n\t#if domain == None:\n\t#  return False\n\tif host == None or (host == None and host == \"*\") : return True\n\treturn host\n\t\ndef check_if_is_ip(host):\n\t#if domain == None:\n\t#  return False\n\tif host == None or (host!= None and host!= \"*\"): return True\n\treturn host!= None #if fails, ip else fails\n\t\t\ndef get_remote_ip(){\n\t#if"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\thost=host.strip()\n\twhile host!= \"localhost\" and host!= \"127.0.0.1\":\n\t\thost=host.strip()\n\tif host == \"localhost\":\n\t\treturn False\n\telse:\n\t\treturn True\n\n#include \"test/test_task_runner.hpp\"\n\n#include \"base/logger.hpp\"\n#include <functional>\n#include <thread>\n#include <vector>\n\n#include \"testlib/testlib_extern.h\"\n\nnamespace\n{\n// TODO: make this a"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tif name in self._localhost_address() and ':' not in host:\n\t\treturn False\n\tif \":\" in host and host[-1]==']' and \"::\" not in host:\n\t\treturn False\n\ttry:\n\t\taddress = socket.hcreate(herrno=socket.h_errno(), h_addr=host,\n\t\t\t\t\t\t\th_length=host.split(':')[1])\n\t\tself._localhost_address()\n\t\treturn True\n\texcept socket.error:\n\t\tself._not_localhost_address(host)\n\treturn"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\thosts = []\n\t#ip = gethostbyname( host )\n\tif len(host) == 4:\n\t\t#ip = socket.gethostbyname(host) or socket.gethostbyname(host[0], host[0])\n\t\tif len(host) == 8:\n\t\t\t#ip = inet_aton(host)\n\t\telif len(host) == 6:\n\t\t\t#ip = inet_pton(host[0],host[1])\n\t\t\t\n\t\t\t#if ip == 'localhost':'127.0.0.1' or ip"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\ttry:\n\t\thost = Host()\n\texcept:\n\t\treturn False\n\n\ttry:\n\t\tlocalhost = socket.gethostname()\n\texcept OSError:\n\t\treturn False\n\t\t\n\tif localhost == host:\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef islocalhost(host, port=DEFAULT_PORT):\n\t\"\"\"\n\tchecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:param port: The host port or 0 to default to 80\n\t\n\t:return:"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tif host == \"localhost\":\n\t\treturn True\n\treturn False\n\n@pytest.fixture(scope='session')\ndef host_local(request, host, hostname):\n\t\"\"\" A fixture to allow easier localhost testing\n\t\"\"\"\n\trequest.addfinalizer(get_local_hostname)\n\treturn request.server.http_client.request_and_get('http://%s:%s/' % (host, hostname) )\n\ndef get_local_hostname(host, hostname):\n\t\"\"\"Check to see if the hostname is localhost\n\n\t:param host"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\thost = check_ip(host)\n\tif host == (\"127.0.0.1\", 0):\n\t\t# this is actually what Windows does\n\t\thost = host.split('.')[0]\n\tip = socket.gethostbyname(host)\n\tif (len(ip)) < 1:\n\t\treturn False\n\treturn True\n\ndef check_ip(ip):\n\t\"\"\"\n\tLooking and parsing the IP address\n\n\tNote that IP address parsing rules are:\n\n\t1. This is much faster than parsing a socket address when connected\n\t"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tis_localhost = (os.gethostname() == host)\n\t\n\tif is_localhost:\n\t\tpass\n\telse:\n\t\t#print(\"Checking for localhost...\")\n\t\t#print(host)\n\t\t#print(is_localhost)\n\t\treturn is_localhost\n\n// Licensed to the Apache Software Foundation (ASF) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The ASF licenses this file\n// to you under the Apache License, Version 2"}]}
{"task_id": "62ece4992e6aefcf4aabbd79", "prompt": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\treturn Path.glob(str(pattern)+\"\" for pattern in glob.glob(pattern, recursive=True)) + glob.glob(pattern)\n\n\nfrom._utils import (\n\tjoin_path,\n\tjoin_pathes,\n\tset_input_line_encoding,\n)\n\n\nclass InputEncoding:\n\t\n\tdef __init__(self, input_encoding):\n\t\tself.input_encoding = input_encoding\n\t\t\n\tdef set_input_line_encoding(self, encoding):\n\t\tself.input_encoding = encoding\n\t\n\tdef __str__("}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tif \"path\" in paths:\n\t\tpattern_list = []\n\t\tpatterns = []\n\t\t\n\t\tfor path in paths.split(' '):\n\t\t\tfor pattern in glob.glob(path):\n\t\t\t\tif pattern:\n\t\t\t\t\tpattern_list.append(pattern)\n\t\t\t\t\tpatterns.append(pattern)\n\t\t\t\n\t\tif not patterns:\n\t\t\tpath = os.path.expanduser('~/')\n\t\t\tpatterns = glob.glob('~' + os.path.normpath(path))\t\n\t\t\tpath = list(glob.glob(os"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tpass\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t# TODO: Add support for glob syntax. The following is an effort to do the heavy lifting.\n\t# PathFragments as regular expressions can be removed when it gets fixed by Python 3.\n\t# This class does not support glob syntax correctly.\n\t\n\t# See the \"examples/extract_path\" guide for more info.\n\t# https://wiki.python.org/moin/File_Patterns\n\t\n\tfor fragment in fragment_patterns:\n\t\t_clean_frag()\n\t\t\n\tpattern = re.compile(regex, flags=re.IGNORECASE)\n\t# Add"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tpattern_map = {}\n\tfor fragment in ast.walk(pattern):\n\t\tfor pattern_fragment in pattern[fragment]:\n\t\t\tif pattern_fragment not in pattern_map:\n\t\t\t\tpattern_map[pattern_fragment] = []\n\t\t\tpattern_map[pattern_fragment].append(fragment)\n\n\tnew_paths = []\n\tfor fragment_path in Path.Path(pattern_map).walk():\n\t\tnew_fragments = []\n\t\tfor fragment in Path(fragment_path, ast.literal_or_name(pattern_map[fragment_"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tpath_glob_or_patterns = glob.glob(path)\n\tpaths = defaultdict(list)\n\tfor path in path_glob_or_patterns:\n\t\tpath = os.path.normpath(sys.path[0])\n\t\tassert path.endswith('.txt') and not path.endswith('.zip') and type(path) is not tuple\n\t\tpaths[path].append(os.path.basename(p)[:-3] for p in path_glob_or_patterns + path)\n\treturn [r for r in paths.values() if r."}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t# XXX: should look like a regexp, regexp like?match_pattern\n\tdef find_patterns(paths, *patterns, **query):\n\t\t# XXX: for paths that are patterns, use path_patterns as query\n\t\t# XXX: for patterns that have no match, do a plain regex search.\n\t\tpattern_list = {\n\t\t\t0: \"**/*.txt*[*]\",\n\t\t\t1: \"**/*.txt[1-9][h%][0-9]/*.txt[2-9][h%][0-9]\",\n\t\t}\n"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t# find_paths: The list of paths to be transformed/applied\n\t# match: The regular expression that would match paths to be transformed\n\t# replace: The regular expression replacement that would replace paths to be transformed\n\t# pattern: The path pattern for a single path fragment\n\tfind_paths = []\n\tmatch = re.compile(r\"[\\\"']*[^']*\\\"\\s*\\\\[\\\"'](.+)\\\"'\")\n\treplace = re.compile(r'$\\1\\s*([^\\\"\\\\n\\r\\f]+)|(\\\\)\\1$')\n\tfor p in path"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tpaths = re.findall('.+', pattern.pattern)\n\tprint('\\n', \"\".join(paths),)\n\tprint('\\n', \"\\t\".join(re.findall(r\"{(.+)}\\t(.*)\", pat, re.I))\n\tpatterns = []\n\tfor p in paths:\n\t\tpattern_lines = [line.strip() for line in p.split('\\t') if len(line.strip()) > 0]\n\t\tassert len(pattern_lines) > 0\n\t\tpattern_lines[0] = pattern_lines[0]["}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tdef __call__(self, fragments):\n\t\toutput = []\n\t\tfor frag in fragments:\n\t\t\tif not len(frag)>=2:\n\t\t\t\tcontinue\n\t\t\tif re.match(fragments[0], frag) or re.match(fragments[1], frag):\n\t\t\t\tcontinue\n\t\t\tif re.match(frags[0], frag) or re.match(frags[1], frag):\n\t\t\t\tcontinue\n\t\t\t\n\t\t\ttmp_path = os.path.join(self._dirname, '..', frag)\n\t\t\tif not os.path.exists("}]}
{"task_id": "62ece4992e6aefcf4aabbd7a", "prompt": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\treturn os.environ.get('PYTHONPATH', '') == '.') or os.environ.get(\n\t\t\t'PYTHONPATH', '') == 'C:\\\\Users\\\\zemas\\\\AppData\\\\Roaming\\\\')\n\ndef is_windows_git_in():\n\treturn get_os_version() >= WINDOWS_VERSION_WIN10\n\nif not is_windows_git_in():\n\traise RuntimeError('is_windows_git_in method has not been implemented yet')\nelse:\n\t\"\"\"\n\tThis is the main function to run git in windows.\n\t\"\"\"\n\treturn"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\timport subprocess\n\tif subprocess.call(\"git-ls-root-dir -d -o -\") == 0:\n\t\treturn True\n\n\t# I don't understand why it is here: it is possible to run this in a Linux OS/X, but not in a UNIX OS/X, but I have seen bugs about that in the tests when they were not a Git repository. I think that's pretty ugly, but hopefully there is a good way to fix the issue.\n\tif os.name == \"nt\":\n\t\trootPath = os.path.dirname(os.path"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t# from https://www.hackerrank.com/challenges/python-gitbash/\n\t# modified to just check if the user has specified git\n\tif len(kwargs) == 0:\n\t\treturn False\n\tgitbash = kwargs.get('gitbash', '-')\n\tif not gitbash and len(sys.argv) > 2 and os.path.exists(os.path.join(sys.argv[2] or '', os.pardir)):\n\t\tsys.argv = [sys.argv[0]] + sys.argv[1:]\n\telse:"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tgit_dir = os.path.abspath(os.path.join(os.path.expanduser(\"~\"), \tgit_dir_prefix,\"hg\"))\n\tif not os.path.exists(git_dir):\n\t\tos.makedirs(git_dir)\n\n\ttry:\n\t\tsubprocess.call(['git','rev-parse-single', '--long', 'origin', 'origin', '..', git_dir, os.path.join('git', 'origin.git')])\n\t\treturn True  \n\texcept:\n\t\treturn False\n\n"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tcmd = \"/usr/sbin/gitbash\"\n\tout, err = runCommand3(cmd, cwd, *args)\n\tif err!= None: return False\n\t\n\treturn True\n\n# \tdef gitbash(command):\n# \t\treturn runCommand3(command, \"gitbash\", *args)\n#\tdef runGitbash(command):\n#\t\treturn runCommand3(command, command, *args)\n\n# \tdef gitsh(command):\n# \t\treturn runCommand3(command, command, *args)\n#\tdef"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tfrom __future__ import with_statement\n\n\tcmd = ['gitbash', \"-p\", '--user', 'root', 'file']\n\n\tif not os.shell:\n\t\tcmd.append(\"--no-version\")\n\n\treturn run(cmd) is True\n\nusing GameServerCore.Command.Attributes;\nusing System;\n\nnamespace GameServerCore.Commands.Attributes\n{\n    [AttributeUsage( AttributeTargets.Class, Inherited = true, AllowMultiple = false )]\n    public abstract class Command< TEntity, TOutput > : CommandComponentBase"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\ttry:\n\t\tfrom os.path import basename\n\texcept ImportError:\n\t\tfrom tempfile import TemporaryDirectory\n\t\ttry:\n\t\t\t# we didn't install the python version info, try running this\n\t\t\tfrom pipenv.common.utils.exec_helper import (\n\t\t\t\tget_git_version,\n\t\t\t\trun_git_version,\n\t\t\t)\n\t\t\ttry:\n\t\t\t\tgit_version = run_git_version(\n\t\t\t\t\tgitbuilddir=os.environ.get('GIT_WORKING_DIR',\n\t\t\t\t\t\t'/"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\treturn os.get_platform() == \"windows\"\n\n\ndef is_windows():\n\t\"\"\"\n\t\treturns True if you run in a Windows gitbash\n\n\t\t:return: True if gitbash\n\t\"\"\"\n\treturn is_local() or os.getenv(\"GITKIN\") == \"gitbash\"\n\n\ndef is_linux():\n\t\"\"\"\n\t\treturns True if you run in an older OS, linux, and not a Windows linux distribution\n\n\t\t:return: True if linux or not\n\t\"\"\"\n\treturn is_windows() and os."}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tif os.name == 'nt':\n\t\tfrom ctypes.wintypes import INT, DWORD, POINTER, WPARAM, WINBOOL, \n\t\tFALSE, TRUE, CHAR_PTR\n\t\t#if ctypes.windll.kernel32.IsWow64():\n\t\t#\tif os.name == 'nt':\n\t\t#\t\tif os.version.split('.')[0] >= '6':\n\t\t#\t\t\tfrom ctypes.wintypes import WCHAR, WPARAM, BOOL, \n\t\t#\t\t\t\tDWORD\n\t\t#sys.platform = 'win"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\treturn os.system('gitstatus -f %s.git && gitstat -vq -m -u' % path) >= (0, 0) \n\t\n\t\"\"\"\n\treturns true if you use the shell environment to create the.gitignore file\n\t\n\t:return: whether shell env is used\n\t\"\"\"\n\treturn os.system('systemctl restart gitignore') > 0\n\ndef git_skip_commit():\n\treturn git_skip()\n\ndef git_skip_for_commit():\n\tif is_git_running_in_github_browser():\n\t\tif"}]}
{"task_id": "62ece4992e6aefcf4aabbd7b", "prompt": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tif os.path.exists(target_dir) == True and not overwrite == True and not os.path.isdir(target_dir):\n\t\tprint(\"Config File Already Exists!\")\n\t\tpass\n\telse:\n\t\twith open(target_dir, 'w') as target_file:\n\t\t\tfor conf_path in list(config_template.items()):\n\t\t\t\tif conf_path.endswith(\".d.json\"):\n\t\t\t\t\ttarget_file.write(json.dumps(config) + \"\\n\")\n\t\t\ttarget_file.write(f"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tfrom PySpice.config.utils import _write_to_file_in_yaml\n\tif os.path.exists(target_path):\n\t\tfile = os.path.join(target_path, \"config.json\")\n\t\twrite_to_file_in_template(file, target_path)\n\n\tos.mkdir(target_path)\n\twith open(target_path) as f:\n\t\t_write_to_file_in_yaml(f, config_path, target_path)\n\n// This file was procedurally generated from"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\twith open(os.path.abspath(target_config_path), 'w') as outfile, open(os.path.abspath(config_path), 'w') as ofile:\n\t\tfout = open(os.path.abspath(config_path), 'w') \n\t\tfout.write(outfile.read()) \n\t\tfout.write('# target config'+ target_config_path)\n\t\tfout.write('; create existing directory \\'\\'') \n\t\tfout.write('if not ')\n\t\tfout.write(''+ ofile.read()) \n"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\ttargetConfigPath = filepath.abspath(os.path.join(projectFolder, filename))\n\tif not os.path.exists(targetConfigPath):\n\t\tos.makedirs(targetConfigPath)\n\t\t\n\twith open(filepath.join(targetConfigPath, \"final.yml\"), \"w\", encoding=\"utf-8\") as f:\n\t\tf.write(config.yaml)\n\twfile = open(filepath.join(targetConfigPath, \"final.yml\"), \"w\")\n\twith wfile:\n\t\twfile.write(config.yaml"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tdef write_out(directory, filename, write_files):\n\t\tif not os.path.exists(directory):\n\t\t\tos.mkdir(directory)\n\t\twith open(filename, \"wb\") as outfile:\n\t\t\ts = f\"{filename}\\n\"\n\t\tif not write_files: return s\n\t\tfor filename in write_files:\n\t\t\twith open(filename, \"w\") as outfile:\n\t\t\t\ts = f\"{string.indent(s)}<%s>\\n\" % os.path.splitext(filename)[-1]\n\t\t\t"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tconfig = yaml.safe_load(io.BytesIO(io.StringReader(config)))\n\tconfig = { key: config[key] for key in config.keys()\n\t\tif key not in target_config\n\t\t}\n\n\tif target_config not in config:\n\t\toutput.create_directory('{}/target_config'.format(target))\n\t\tconfig[target_config] = {\n\t\t\t'file': target\n\t\t}\n\t\tprint('Creating: {}'.format(target))\n\n\tdata = os.path.join(target,"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tif exists(cfgfilename):\n\t\traise Exception('Config already exists at specified path?')\n\toutfilename = os.path.join(tempdir, filename)\n\twith open(outfilename, 'w') as f:\n\t\tyaml.dump(cfgyaml, f, ensure_ascii=False)\n\ndef make_yaml_filename(fn):\n\t\"\"\" This is a utility function that is used to create a file name, given a filename and\n\tthat we want to write out the file at absolute level. The level can be absolute or relative.\n\t\"\"\"\n\treturn"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\twith open(os.path.join(targetPath, \"targetCfg.py\"), \"w\") as f:\n\t\twith open(\"config.yml\", \"w\") as out:\n\t\t\tfor line in f:\n\t\t\t\tline = line.strip()\n\t\t\t\tif not line or line!= \"\":\n\t\t\t\t\tcontinue\n\t\t\t\t\t# Check if target config file exists. If it doesn't, get out.\n\t\t\t\t\tif not os.path.exists(targetPath):\n\t\t\t\t\t\tf.write(\"{}\\n\"\n\t\t\t\t\t\t# This is all that is needed, write"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tif not os.path.exists(target_config_file_name):\n\t\twith open(target_config_file_name, 'a') as outfile:\n\t\t\toutfile.write(config_content.replace(\n\t\t\t\ttarget_config_file_name, config_content, encoding='utf-8'))\n\t\t\tbreak\n\ndef print_error_message(error_type, message):\n\tprint error_type\n\tprint 'Error in file %s:'% error_type\n\tprint message\n\tprint 'This can take a few minutes'\n\ndef"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t# pylint: disable=too-many-arguments,too-many-locals\n\tif check_filename(config_filename):\n\t\traise ValueError(\"The target config filename {0} is already in use.\" format(config_filename))\n\n\t# pylint: disable=too-many-arguments,too-many-locals,too-many-branches\n\tif not os.path.isfile(config_filename):\n\t\tif os.path.isfile(config_filename + '.yaml'):\n\t\t\traise ValueError(\"Target config {} file already exists, abort\".format(config_"}]}
{"task_id": "62ece4992e6aefcf4aabbd7c", "prompt": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t# python 2\n\t# os.system(cmd + b\" \")\n\targs3 = [\"ssh\", \"-l\", \"-r\", \"-o\", \"127.0.0.1:65535\"]\n\tstart_line1 = args3[0]\n\tprint(\"Running cmd\",cmd+\":\",end={'':start_line1}))\n\tfor start_line2 in args3[1:]:\n\t  exec start_line2\n# end\nif __name__ == \"__main__\" and len(script.split(r'\\\\')) > 1:\n\tprint(\"Start line2 \","}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tprint script\n\n\tif len(scripts) is g.G.UNUSABLE or len(scripts) > 1:\n\t\terr_msg = script + \" commands are ignored or do not have any script\"\n\t\tprint err_msg\n\t\treturn \"\", err_msg\n\n\tshell = scripts[0]\n\n\t#print python command\n\tcommand = shell.split(\" \")\n\tcmd = list(command)\n\n\t#print any parameters in the command\n\tparams = list()\n\tfor v in shell.split(\" \"):\n\t\t#print each parameter"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\treturn _run_shell_script_command(self, script)\n\nclass Fingerprint(object):\n\t'''\n\tThe fingerprint class allows you to easily identify and provide useful data\n\tabout the fingerprint that's sent by the shell\n\t'''\n\tDEFAULT_VALUE = None\n\t\n\tdef __init__(self):\n\t\tself.fingerprint = None\n\t\t\n\tdef get_fingerprint(self):\n\t\tif self.fingerprint is None:\n\t\t\tfingerprint = self._get_fingerprint_from_path()\n\t\t\tif fingerprint is None:\n\t\t\t\treturn Fingerprint(None"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tfrom scripts import script, create_shell_command\n\t# print 'Create command', args\n\tcommand = script.add(create_shell_command(args))\n\tprint('Created shell command:', command)\n\n\tif len(args) == 0:\n\t\tprint('Missing script arguments.')\n\t\treturn\n\telse:\n\t\tprint('The number of arguments was:', len(args))\n\treturn args\n\ndef convert_script(script):\n\t\"\"\"\n\tCreates an shell command that converts a single script into one.\n\tThe script itself is a single file,"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tif script.startswith('--'):\n\t\tnew_string = script.replace(':lineno', 'line'+ script[len(':%s') :])\n\t\t#print(new_string)\n\t\tret = python_eval(new_string)\n\t\treturn ret\n\t#else:\n\t\t#print(script)\n\t\t#print(SCRIPT_TYPE.replace(':', '_'))\n\t\tret = shell.input(script.replace(':lineno', 'line'+ script[len(':%s') :]))\n\t\treturn ret\n\nusing System;"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tif sys.version_info.minor >= 3:\n\t\treturn textwrap.dedent(script).replace('\\n','')\n\telse:\n\t\treturn textwrap.dedent(script).replace('\\t', '\\n')\n\n\ndef wrap_script_from_bytes(buffer, type=None):\n\t\"\"\"\n\tReturns a Python string from a given byte array in Python.\n\n\t :param buffer:\n\t :param int type:\n\t\tA Python int. If Python's format_byte_codes function is used, it can be\n\t\tused to"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\timport re\n\n\ttry:\n\t\tr = re.search(r'.*(SSH) (.*?) (.*?)(;|$)', exec_args)\n\texcept:\n\t\traise Exception('cannot extract script to one line')\n\n\t# find the lines\n\tlines = r.group(1).split('\\n')\n\n\t# convert each line to command\n\tfor line in lines:\n\t\tif''.join(map(str, line.split(' ', 1))) == \" \":\n\t\t\t# get rid of the spaces between columns\n\t\t\tline = re"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tif len(script) > 10:\n\t\tprint(script)\n\t\treturn\n\tstart = 0\n\twhile 1:\n\t\tif start == 8 and len(script):\n\t\t\tstart = 0\n\t\tnew_cmd = cmd + [script]\n\t\tscript = textwrap.dedent(f\"{new_cmd}\\n\")\n\t\tout = shlex.split(script)\n\t\tif len(out) >= 3:\n\t\t\tif len(out) > 3:\n\t\t\t\tprint(out[3])\n\t\t\t\treturn\n\t\t\t"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tscript = sys.argv[1]\n\tif len(script) == 0:\n\t\tscript = ''\n\tfor line in script(globals):\n\t\tprint' [' + line.rstrip() + ']'\n\tprint' [' + script + ']'\n\tsys.exit(0)\n\t\nexcept:\n\n\tfrom exceptions import NotAToken\n\tprint \"Can not convert to a token:\\n\"\n\tnot_atoken = NotAToken()\n\tt = not_atoken.newtext(script.replace('''','').replace('\\n','')."}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tfor line in script:\n\t\tif not line:\n\t\t\tcontinue\n\t\tcode = [line[i:i+1] for i in range(line.count(' '))]\n\t\tcmd = '0x%08x' % int(ord(line[-1]),16)\n\t\treturn cmd\n\treturn \"0x%08x\" % int(ord(script,16),16)\n\ndef get_output(key):\n\t# returns a filename-friendly string containing the command and its arguments\n  \tif key == 'command':\n  \t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd7d", "prompt": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\targs = list(args)\n\targs.insert(1, '-c')\n\ttry:\n\t\tsubprocess.call(func, *args, **extra_env)\n\texcept:\n\t\terrStr = \"Could not import module {}\".format(func.__module__)\n\t\traise ValueError((\"Error at process {}, error message \"\n\t\t\t\t\t\t   \"processing module {}: {}\".format(\n\t\t\t\t\t\t\t    sys.last_type.__name__, errStr, errStr)),\n\t\t\t\t\t\t  sys.last_type)\n\ndef run(cmd, env=None, **extra_"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tcmd = (subprocess.Popen(_run, stdout=PIPE, stderr=PIPE, stdin=PIPE,\n\t\tstdio=PIPE, bufsize=1) and\n\t\trun(args=['-x'], env={'test': 'value'}) and\n\t\trun(args=['-h'], env={'test': 'help'}) and\n\t\trun(args=['-r'], env={'test': 'rare'}) and\n\t\trun(args=['-s'], env={'test':'sensitive'}) and\n\t\trun(args=['-q'], env={'test': 'quiet'}) +\n\t\t\t\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\timport subprocess\n\t\n\targs, extra_env = process_args(args, extra_env)\t\n\t\n\tsubprocess.run(args)\t\n\t    \n\treturn {\"stdout\": subprocess.__stdout.read(), \n\t\t\t\t\"stderr\": subprocess.STDOUT.read()}\n\n\nclass WatkiRunner(object):\n\tdef __init__(self, process):\n\t\tself.process = process\n\t\n\tclass Process(object):\n\t\t\"\"\"\n\t\tBase process class for Watki.\n\t\t\"\"\"\n\t\t\n\t\tdef __init__(self):\n\t\t\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tif sys.platform in ['win32', 'os2']:\n\t\traise TypeError('Function not supported on Windows')\n\t\n\toutput = []\n\t\n\t# Run the function, passing the full arguments to be\n\t# passed to the subprocess and any additional\n\t# environment variables.\n\ttry:\n\t\tretval = subprocess.call(func, *args, **extra_env)\n\t\t_log.debug(\"%s\", retval)\n\t\treturn retval\n\texcept TypeError:\n\t\tretval = run_function(func, *args, **extra_env)\n\t\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tif sys.platform == \"linux\":\n\t\ttry:\n\t\t\tos.setsid()\n\t\texcept OSError:\n\t\t\treturn 0, \"Cannot set /dev/pts/0000\"\n\telif sys.platform == \"darwin\":\n\t\tos.setpgrp()\n\t\t# On OS X, the parent of a process is always equal to the child of\n\t\t# a setuid process. On other OSs, the parent of the child of a\n\t\t# setuid process is always the same as the parent of the specified\n\t\t# process.\n\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\timport subprocess\n\tsubprocess.call([func, *args,...], timeout=60)\n\tassert(list(os.environ) == list(extra_env.keys()))\n\n// RUN: %check_clang_tidy %s bugprone-pr12-missing-argument-for-pr11-macro %t\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\nvoid *pr12(void){\n\t{\n\t\tstatic int p[2 * 2 * 2];\n\t\tstatic int x = (void*)-1;\n\t\tvoid"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\ttry:\n\t\timport subprocess\n\texcept ImportError:\n\t\traise RuntimeError(\"Python module not found\")\n\n\tfunction = getattr(subprocess, \"run_command\")\n\tif callable(function):\n\t\tfunction(*args, **kwargs)\n\telse:\n\t\terror = \"Unknown function %s called with %s\" % (func.__name__, \\\n\t\t\t' '.join(map(chr, args)))\n\t\traise RuntimeError(\"%s %s\" % (error, func.__name__))\n\n/*\n * Copyright 2020 Google Inc.\n *\n * Licensed under the"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tif args is None:\n\t\targs = sys.argv[1:]\n        \textra_env = {\"PATH\": \"/bin:/tmp\"}\n    else:\n        args = args.split(':')\n        args = args[0].split(\":\", 1)\n        extra_env = {}\n\n    # The ``run_subprocess_command`` function adds in the environment-\n    # variables to be evaluated as options.  Because we want these"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tif isinstance(func, int):\n\t\tfunc = subprocess.Popen(args, stdin=subprocess.PIPE, **extra_env)\n\telif isinstance(func, string_types):\n\t\tf = os.popen(func, **extra_env)\n\telif callable(func):\n\t\tfunc = func(args)\n\telse:\n\t\traise TypeError('Can not determine function type: %s' % (type(func),))\n\n\tf.communicate.expect(lambda x, y: x + '\\n' + y)\n\n#include"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tif \"sub_process_pid\" in os.environ:\n\t\tos.environ[\"SU_PID\"] = subprocess.getpid().replace(\":\", \"_\")\n\n\tif \"subprocess\" not in globals():\n\t\t# Sub-process isn't being run.  It does.\n\t\t# A local process does.\n\t\tdef command(args):\n\t\t\tsubprocess.call(args)\n\n\t\tcommand = lambda: subprocess.run(args,\n\t\t\textra_env=dict(SU_EXTRA_ENV={'SU_PID': subprocess.getpid().replace(\""}]}
{"task_id": "62ece4992e6aefcf4aabbd7e", "prompt": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tret_val, err_val = get_env_val_bypath(env_var_name, env_var_default, env_var_is_valid)\n\tif not len(ret_val):\n\t\traise KeyError('Variables name: {varname}'.format(varname=env_var_name), env_var_default)\n\telif len(err_val):\n\t\traise RuntimeError('Error occurred in retrieving variable {varname} while environment variable {env_var_name}'.format(\n\t\t\tvarname=env_var_name, env_var_default="}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\ttry:\n\t\tif os.path.commonprefix([os.path.basename(name), \"Env\"])==os.path.commonprefix([os.environ.get(\"ENV\", \"Env 1\"), \"Env 2\"]):\n\t\t\treturn os.getenv(name, \"Error\")\n\texcept:\n\t\tpass\n\tif not is_default(name, default='Error'):\n\t\t_error = \"Error\"\n\telse:\n\t\t_error = default\n\n\treturn _error\n\n\ndef is_default(name, default={}):\n\ttry:\n"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tfrom ConfigParser import ConfigParser\n\tvariables = vars()[\"__PYROGEN_CONF_VARS__\"] = [ os.path.normpath(x) for x in os.environ.items() if x not in variables\n\tif variables:\n\t\tfor name, val in variables:\n\t\t\teval(name)\n\ndef extract_environ():\n\t# Find all the environment variables\n\tenvs = {}\n\tos.environ.clear()\n\tenvs = os.environ.items() or ()\n\tfor item in os.environ:\n\t\tname = item."}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tenv_name = os.environ['HOME']\n\tmatched_var = os.environ[env_name]\n\tmatched_value = getattr(path_finder, matched_var)\n\tif not matched_value:\n\t\tif default_value is not None and default_value not in str(matched_value):\n\t\t\traise KeyError(\"Variable '%s' not found in env\" % (env_name))\n\treturn matched_value\n\ndef get_config_value(environ, variable_name, default_value):\n\t\"\"\"\n\t\tReturns the value obtained from the"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tif os.path.isfile(os.path.join(PATH, \"eval.py\")):\n\t\treturn eval(FILE_NAME, FILE_VALUE)\n\tfor line in open(\"eval.py\", \"r\").read().splitlines():\n\t\tif not line.startswith(\"#\"):\n\t\t\tif line.startswith(\"EVAL_DIR\"):\n\t\t\t\tfile = open(\"eval.py\", \"a+\")\n\t\t\t\tfile.writelines(\"{\\n\\t\\\"value\\\": {\\\"\" + file.read().replace(\"\\n\", \"\\n\\t\") + \"\\\"},\\n}\\"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\treturn os.getenv(matcherVar, defaultValue)\n\n/*\n * Copyright 2000-2009 JetBrains s.r.o.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tif variable is None:\n\t\tvariable = os.getenv(variable, default=None)\n\tif not os.path.isabs(variable):\n\t\traise ValueError(\"Environment variable {} must be an absolute path\".format(variable))\n\ttry:\n\t\tif not os.path.isfile(os.path.expanduser(variable):):\n\t\t\traise ValueError(\"Environment variable {} must be a name file, directory, or file.\".format(variable))\n\t\treturn read_file(os.path.expanduser(variable)[:-7]+\".yaml\")\n\texcept"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tdef run(env=None, name=None, value=None, default=None, match=None, default_value=None):\n\t     \"\"\"\n\t\tRun the environment in and define the values from this file.\n\t\tArguments:\n\t\t<env>\n\t\t\t-   <env> the name space of the environment to set.\n\t\t<name>\n\t\t\t-   The name of the new value.\n\t\t\t-   If <name> is None, the previous value is cleared.\n\t\t<value>\n\t\t\t-"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\ttry:\n\t\tmatch = get_attribute(ENVIRONMENT_NAME_WITH_MATCH)\n\texcept KeyError:\n\t\ttry:\n\t\t\tmatch = get_attribute(ENV_VAR_NAME_MATCH)\n\t\t\tval = match[1].lower()\n\t\t\tif val == '':\n\t\t\t\traise TypeError(\"Cannot set value, environment variable not set!\")\n\t\t\telse:\n\t\t\t\treturn val\n\texcept TypeError:\n\t\traise TypeError(\"Environment variable not set!\")\n\telse:\n\t\tif val == \"\":\n\t\t\t# There is no value,"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tenv, _, rootfs = get_default_environment([])\n\tvariables = vars(rootfs)\n\tif variable in variables:\n\t\tvalue = os.environ[variable]\n\t\tif value:\n\t\t\tenv.putenv(variable, value)\n\t\telse:\n\t\t\traise Error('Environment variable \"{0}\" not found'.format(variable))\n\telse:\n\t\traise ValueError(\"Could not find a working environment variable from environment variable list in rootfs\")\n\treturn env.get(variable, defaults=defaults)\n\ndef get_local_variable(variable"}]}
{"task_id": "62ece4992e6aefcf4aabbd7f", "prompt": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tif not isinstance(image_href,str):\n\t\traise ValueError(\"Image href given cannot be of type string, got instead: %s\" % str(image_href))\n\timage_href = urlsplit(image_href)\n\n\tnetloc, url = urlsplit(image_href)\n\tused_ssl = None\n\ttry:\n\t\tnetloc=netloc.split(\":\")[0]\n\texcept:\n\t\traise ValueError(\"Can only get the Netloc from the URL, got: '%s'\" % image_href)\n\n\tnetloc = net"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tdef ParseImage(image_href):\n\t\tpart_match = re.match('(https?://(?:www|ftp|ftps)://)?' +\n\t\t                       r'(?:([^\\s]+)\\/[0-9]+\\.([0-9]+)\\.[0-9]+',\n\t\t                       image_href)\n\t\tif part_match:\n\t\t\tif len(part_match"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\timg_href = get_href(image_href)\t\t# image to check\n\tn = len(img_href.split(\"@\"))\n\t\t\t\"\"\" n - number of components\"\"\"\n\treturn ( img_href, False, False, n < 3 ) # < image_list.length\n\t\t\t\t\t\t\t\ndef read_url(url_path, scheme=\"http\"):\n\t\"\"\"\n\tRead a URL from a stream-like object. The stream-like object is assumed to implement getitem but the stream \n\tis a memory stream. This can be used to create URLs like URLs, such as"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\timg_parts = glob.glob(image_href)\n\tif not len(img_parts):\n\t\traise ValueError(\"No composite image file found\")\n\tnetloc = netloc_from_href(netloc)\n\timage_id = map(lambda x: int(x), img_parts)\n\tif use_ssl:\n\t\tcrypto = self.crypto(netloc, True)\n\t\treturn image_id, crypto.thumbprint(netloc)[:-2]\n\telse:\n\t\tcrypto = self.crypto((netloc, ))\n\t\t"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t# url_parts = urlparse('http://www.example.com/image' + urlEncode(image_href))\n\t# try :\n\t#     image_id = url_parts.get('image_id')\n\t#     netloc = url_parts.get('netloc')\n\t#     use_ssl = url_parts.get('use_ssl')\n\t# except Exception:\n\t#     raise ValueError('Invalid image_href %r' % image_href)\n\t# netloc, use_"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tassert isinstance(image_href, basestring)\n\tif image_href.lower()!= 'https://www.google.com/search?q=':\n\t\timage_href = None\n\tquery_list = urllib.parse.ParseQuery(image_href)\n\timages_list = urllib.parse.parse_to_domains(query_list=query_list)\n\timage_id = None\n\tif isinstance(images_list[0], (tuple, list)): # i.e., [(name, domain),...]\n\t\timage_id = images_list"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tglobal C_images_from_url\n\tif C_images_from_url and C_images_from_url.search(image_href) is None: \n\t\tif debug:\n\t\t\tlogger['debug']('image_href %s was not found or is http',image_href)\n\t\t\traise ValueError('image_href not found or is no cimg library')\n\t\tnetloc = netloc or os.path.join(C_images_from_url.netloc, '.local')\n\t\timage_id = '%02x' % netloc."}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tif not image_href:\n\t\traise ValueError(\"missing image href\")\n\tnetloc = urllib.parse.unquote(image_href.split(\";\")[0])\n\tif netloc not in (None, host):\n\t\traise ValueError(\"unsupported image location ('%s')\" % netloc)\n\tif use_ssl:\n\t\treturn parse_https(image_href, http_proxy=proxy, ssl=use_ssl,\n\t\t\t\t\t\t\t host=host)\n\tnetloc_parts = netloc.split(\":\")\n\ttry:\n\t\tnetloc_parts"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\turl = urlparse.urljoin(image_href, 'image.jpg')\n\tif use_ssl == False:\n\t    # ssl not available, try HTTP 1.1\n\t    conn = ssl.wrap_socket(proxy.get_connection_from_url(url))\n\telse:\n\t    # use SSL\n\t    conn = ssl.wrap(proxy.get_connection_from_url(url))\n\t    if conn is None:\n\t    \traise ValueError('Unable to connect to %s' % url)\n\t  "}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tfrom urllib import request\n\tfrom urllib2 import RequestError, URLError\n\n\t# parse the base_url as a series of double quotes\n\tbase_url = urllib.parse.urlparse(image_href).geturl()\n\n\t# get the url hostname\n\thostname, port = base_url.split(':', 1)\n\n\t# if the url has a hostname and port, get the host portion\n\thost, port = hostname.split(':', 1)\n\n\t# get the path component (e.g. file:///path)\n\tfile_path ="}]}
{"task_id": "62ece4992e6aefcf4aabbd80", "prompt": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t# TODO: This seems to be pretty inefficient on long paths. It probably should be\n\t#       a binary search to compute the paths and compare to each another\n\t#       method.\n\t#           \n\t# TODO: I don't think this is an accurate way to do find_first_of().\n\t\n\tclass Filenames(object):\n\t\n\t\tdef __init__(self, input_list):\n\t\t\tself.input = ''\n\t\t\tself.paths = {}\n\t\t\tself"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\nfrom __future__ import print_function\n__metaclass__ = type\n\nfrom builtins import str\ntry:\n    import glob\nexcept ImportError:\n    import re as glob\nimport base64\n\nclass SplitFilename(object):\n\t \"\"\" Split a name by extension. \"\"\"\n\t \n    NAME_EXTENSION = '.'.join(map(str.strip, re.split(r'[..]', name))) \n    \n    def __init__(self, file_list):\n\t\tself.file_list = []\n\t\t"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tpass\n\t\nfrom utilities import info\n\nclass PlistCleaner(object):\n\t'''\n\tRuns a cleaning cycle using a list provided through the command line\n\tvariable arguments.\n\t'''\n\tdef __init__(self, string_list, log_message=True, time=True):\n\t\n\t\tself.time = time\n\t\tself.string_list = string_list\n\t\tself.log_message = log_message\n\n\tdef run(self):\n\t\t''' Iterate over input list'self.string_list', cleaning up the list if it's"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\timport warnings\n\twarning(\"Remove end of string list not supported - remove it\")\n\tlist = list_remove_end(input_list)\n\treturn list\n\n######### Getting Start ####\ndef start(list_list):\n\t\"\"\"\n\t\tStarts a new list from a given string list. It will start with a leading\n\t\tos seperator character.\n\t\n\t\tArgs:\n\t    list_list: list of strings\n\t\tReturns:\n\t    str: list of strings (as returned by os.listdir)\n\t\t\n\t\tRaises:"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import urllib\n\tpass#IMPORTIMPORTIMPORT from sys import stderr\n\tif input_list == [] or input_list[-1] not in (\"\\000\\t\", \"\\u200d\\f\"):\n\t\traise TypeError(\"Expecting 1 character. Actual: %s\" % input_list[0])\n\t\n\tif len(input_list) == 1:\n\t\treturn input_list\n\t\n\t# remove trailing os seperator characters\n\tif len(input_list) > 1:\n\t\tfor i in xrange(1"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tfrom StringTokenizer import StringTokenizer\n\tself.clean(InputList(StringTokenizer(input_list)))\n\n\nclass StringEndsWith(Type):\n\t\"\"\"\n\tA type that checks to see if a given string ends with a specific string.\n\tIf it is, it prints a string equal to that string.\n\n\tArgs:\n\t    str: string, the string to check for length suffix\n\t    suffix: suffix character, either of length or end of string\n\t\"\"\"\n\t__signature__='Void'\n\t__doc__='StringEndsWith(str: str,suffix"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\timport re\n\tfrom String.regexp_match import split_dir_part\n\tfrom String.regexp_match import split_path_part\n\timport os.path\n\t\n    def checklist(self, list, path):\n        for n in list:\n            if n == path:\n                return True\n        return False\n\t\n    def testlist(self, list, sep):\n   "}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tif not isinstance(input_list, (list, tuple)):\n\t\traise TypeError \n\t\t\n\tif len(input_list) == 0:\n\t\tprint('List of strings is empty')\n\t\texit()\n\t\t\n\tif input_list[-1]!= os.sep:\n\t\tprint(input_list[-1])\n\t\traise TypeError\n\t\n\tprint('Listing with os:', len(input_list))\n\t# remove trailing separators\n\twhile input_list[-1]:\n\t\tinput_list.pop()\n\t\n\t# remove trailing separator on empty list\n"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tfor inp in input_list:\n\t\tif inp.endswith('/'):\n\t\t\tremove_trailing_sep(inp)\n\t\telse:\n\t\t\tinp = inp[:-1]\n\t\tyield inp\n\t\n\traise TypeError('Pathname cannot contain paths')\n\n/*\n * SPDX-FileCopyrightText: 2021, microG Project Team\n * SPDX-License-Identifier: Apache-2.0\n * SPDX-FileCopyrightText: 2021 Felix R\u00f6ffler <fred@roe.it>\n */\n\n#include \"kantanuvmanager_automation_"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t# See https://bugs.python.org/issue8486\n\t# Workaround for issue #8486: Python 3.4.2 -g -w -I -F: suppress warnings on Py3.5\n\t# due to the error \"cannot refer to the next token\" in the parser\n\t# See https://github.com/python/cpython/issues/5842\n\tfrom StringIO import StringIO\n\treturn StringIO.StringIO.String(input_list[-1].strip())\n\n\"\"\"\n\tIterate over the contents of a file\n\t\n\t\n\tArgs:\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd82", "prompt": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t# First, we search for regex pattern:\n\tpattern_to_search = re.search(\"[\\s]*([[:digit:]\\s]*)*\\\\.([[:digit:]\\s]+)?\", s)\n\tif pattern_to_search is not None:\n\t\tresult = pattern_to_search.group(1)\n\t\tif pattern_to_search.group(\"result\"):\n\t\t\tpattern = pattern_to_search.group(\"result\")\n\t\t\treturn pattern\n\n\t\tif regex is not None and _RE_SEARCHER_REGEX.match(pattern):"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\ts = re.compile(g,re.I)\n\tv = s.search(v)\n\treturn v\n\n'''\n\tRegex matching function for python regex\n'''\ndef _regex_match(s,m):\n\tv = eval(m)\n\tv = re.sub('(?<!\\n)\\r\\n', '<span style=\"color: red\">\\\\1</span>',v)\n\treturn v\n\n/**\n * Copyright 2015-2016 Red Hat, Inc, and individual contributors.\n *\n * Licensed under the Apache License, Version 2.0 (the"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tglobal re_regex_matches\n\tif len(global_string)>0:\n\t\tre_regex_matches=re.compile(global_string)\n\treturn re_regex_matches.findall(pattern)\n__all__='from_re_parser import from_re_string, from_string_re#from_re_string_re#from_string_re#from_re_sub_parser, from_re_sub_parse'''\n\n\n__version__ = __version__ \n__title__ ='regex'\n__copyright__ = 'Py-"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t# Get the string\n\tpattern_text = s.strip()\n\t# Delete any trailing spaces or other character from the return string\n\tpattern_text = pattern_text.replace(' ','')\n\t# Check for regex pattern match\n\tif pattern_text == \"\":\n\t\treturn re.compile('^#').sub('', '')\n\t# Loop through all lines for match, return None for invalid pattern\n\tfor line in re.split('\\n', pattern_text):\n\t\tfor line_idx in range(len(line)):\n\t\t\t# Get the line\n\t\t\t"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tregex = \"\"\n\n\tif _check_and_substitute(regex, _string):\n\t\tstring = (match_regex(regex, string))\n\n\treturn string\n\ndef _check_and_substitute(regex, string):\n\t\"\"\"\n\t\tString which is in :regex, replace the matched string by given\n\t\tvalue by substitue string\n\n\t\t@param regex: string with substitution regex pattern\n\t\t@param s: string matched by string\n\t\"\"\"\n\tif regex == \"*\":\n\t\treturn string\n\telse:\n\t\tfor p in"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef _extract_string(string_array):\n\t\tre_string = string_array.split(\",\")\n\t\tpattern = re.compile(string_array)\n\t\treturn pattern\n\t\t\n\treturn _extract_string\n\n#pragma once\n\n#include <cmath>\n\n#include \"Engine/Inventory/CurrencyInventory.h\"\n#include \"Scenes/Platformer/Inventory/Currencies/IOU.h\"\n\nclass LocalizedLabelTooltip;\nclass LocalizedTitleLabel;\n\nclass Achialist : public Inventory::CurrencyInventory<LocalizedLabelTooltip"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t# Convert from string to regex pattern\n\t# Example: \"abc%40abc\" => \"abc\\\\%40a\"\n\t# Example: \"abc%40abc\\\\w\" => \"abc\\\\%40a\"\n\t# Example 2: \"c_\\\\x30~1~_~!~0\" => \"c\\\\x30~1~~!~0\"\n\tif len(name) > len(pattern) and name[-1] == '\\\\':  # remove escaped char\n\t\tname += \"|\" + pattern\n\telse:  # strip escaped char\n\t\t"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\treturn re.compile(pattern, re.IGNORECASE)\n\ndef translate_regex(value, flags=re.DOTALL|re.MULTILINE, regex=None, append_new=False):\n\t'''\n\tAdds replacement for the given regular expression to the given string,\n\tif no replacement will be added, replaces them with the value\n\t'''\n\tif regex is None:\n\t\tflags = flags.replace('$', '.*')\n\t\tregex = '%s\\*(.*)' % flagnames(flags, flags, append_new=append)\n\tif'regex' in"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef convert_regex(text):\n\t\tif \" \" in text:\n\t\t\treturn \"\"\n\t\telse:\n\t\t\treturn re.sub('[a-zA-Z]+', \"\", text)\n\n\n\tdef _convert_pattern(item_1, item_2):\n\t\tif item_1 and item_2:\n\t\t\treturn \"[%s]=%s,%s\" % (item_1, convert_regex(item_2), item_2)\n\t\telse:\n\t\t\treturn \"[%s]=%s\" % (item_1, convert_regex"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tif pattern == None:\n\t\tpattern = re.compile(r'(\\S+)')\n\n\treturn pattern\n\n\ndef _fix(pattern, data):\n\t\"\"\"\n\t@param pattern : the string that must match\n\t@param data : the data which will be patched\n\t\"\"\"\n\n\t# first remove regex, then replace with replace\n\tnew_pattern = pattern.replace('$', '.*?')\n\tfor regex in data:\n\t\tfor match in regex.findall(pattern):\n\t\t\tdata.replace(match[0], match[1], 1)"}]}
{"task_id": "62ece4992e6aefcf4aabbd83", "prompt": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcmd = [\"\"] + argv\n\tret = system(*cmd)\n\tif ret == \"-\":\n\t\tret = 0\n\telse:\n\t\tret = str(ret)\n\t\tret += \"\\r\"\n\treturn ret\n\ndef start_gui():\n\tpass\n\ndef end_gui():\n\tpass\n\ndef print_screen(format, args=([10,1,10]), footer=True):\n\tglobal data_dir_name, gui_dir_name, screenshot\n\tscreenshot = False\n\tdata_dir, gui_dir,"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tos.system(f\"python {script} [command...]\")\n\n\ndef create_command(command, *args, **kwargs):\n\t\"\"\"Create a command.\n\t@params\n\t\tcommand: a command to be run when invoked the given Python function(s).\n\t@params args[0]        The first positional argument to be passed to the command (i.e. to the current function).\n\t@params kwargs[0]      A named arguments value, i.e. to the current function. Useful if there are aliases."}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\targs = [cmd]\n\tif not args:\n\t\targs = [\"no output\"]\n\treturn subprocess.check_output(args).decode()\n\n\ndef check_output(output):\n\t\"\"\"\n\tCheck that the output is within an output that the script uses.\n\t\"\"\"\n\tif not isinstance(output, basestring):\n\t\toutput = output.encode(\"utf-8\")\n\treturn output.strip()\n\n\ndef load(path):\n\t\"\"\"\n\tLoad a.zip file for the specified file.\n\t\"\"\"\n\tzip_path = Path"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport errno\n\timport io\n\timport re\n\timport subprocess\n\n\t@classmethod\n\tdef run(cls, command, cwd=None, **kwargs):\n\t\t\"\"\"\n\t\tThis method runs a process.\n\t\t\"\"\"\n\t\tif not isinstance(command, str):\n\t\t\traise ValueError(\"command must be str\")\n\n\t\toutput = subprocess.check_output(command, **kwargs)\n\n\t\tif cwd:\n\t\t\tcwd = [cwd / \"bin\"]\n\n\t\treturn_code = []\n\t\toutput = re.sub(br"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tfor i in range(N):\n\t\toutput = []\n\t\tfor cmd in cmdlist:\n\t\t\tcmd_exe = os.path.basename(cmd)\n\t\t\tproc = subprocess.Popen([cmd], stdout=pipebuf_open, stderr=pipebuf_read, stdin=pipebuf_close)\n\t\t\toutput.append(proc.communicate()[0])\n\t\tfor proc, data in zip(proc.fd_list, output):\n\t\t\tcmd = data[0]\n\t\t\trun[i].append(cmd)\n\n#include \""}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from subprocess import call\n\tcall(['/usr/sbin/psutils/ps-utils.exe', '../'])\n\nimport React, { ChangeEvent, Component } from \"react\";\nimport classnames from \"classnames\";\nimport { Link } from \"react-router-dom\";\nimport { connect } from \"dva\";\nimport PropTypes from \"prop-types\";\nimport Icon from \"react-native-vector-icons/FontAwesome\";\nimport Github from \"react-octo-foreground-github\";\nimport { Button, Card,"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tfor e in sys.argv[1:]:\n\t\tif e:\n\t\t\tcmd = os.path.basename(e)\n\t\t\tcmd_proc = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n\t\t\toutput = cmd_proc.communicate()[0]\n\t\t\treturn output\n\treturn \"\"\n\n\tdef _remove_old_cmd(cmd, delete_after=False, error=3):\n\t\tif delete_after:\n\t\t\tif cmd not in [\"cmd.bat\",\"cmd.py\",\"cmd.pkl\"]:\n\t\t\t\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tsubprocess.check_call(sys.executable, args)\n\ndef run(commands):\n\tfor cmd in commands:\n\t\tcmd.run_command()\n\npackage com.android.server.hidata.wavemapping;\n\nimport android.hardware.broadcastradio.V1_2.IBarcaResponseBody;\nimport android.hardware.broadcastradio.V1_0.IBarcaResponse;\nimport android.hardware.face.Face;\nimport android.location.Location;\nimport android.os.IBinder;\nimport android.os"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif is_verbose_mode == 1:\n\t\tprint(\"Testing %s %s\" % (sys.argv[0], sys.argv[1]))\n\tfor param in sys.argv[2:]:\n\t\tsys.argv = parameter_re.split(sys.argv[1])\n\t\tsys.argv = parameter_re.sub(r'[^a-z]\\w*', '', sys.argv[1])\n\t\tsys.argv = parameter_re.sub('\\s*$', '', sys.argv[1])\n\t\t#print(\"sys.argv: %"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\treturn [command, ]\n\n\ndef exec_inproc_file(command, working_directory, arguments, executable=None):\n\t\"\"\" Run a command in an executable directory.\n\t\targuments: Command arguments, file to send for execution.\n\t\t\tCommand to run on an executable directory.\n\t\t\tUse ``command`` to specify the command to run.\n\t\t\tUse ``working_directory`` if it is used in which directory it\n\t\t\tshould be run from by.\n\t\treturn:\n\t\t\tReturns the stdout and stderr of the execution of the command.\n\n\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd84", "prompt": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\timport ipaddress\n\timport ipdb\n\timport netaddr\n\timport select\n\timport time\n\t\t\n\t# Test if address type\n\tresult = ipdb.find(table_name, ipdb.lookup('ip_addr').values('type', '4'), ['10.0.0.0/24'][0], 1)\n\t# result = ipdb.find(table_name, ip_addr(10.0.0.0)[0], [\"10.0.0.0/24\"][0], 1)\n\t# if result.count() == 0 : #"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tif input() and (len(line)==13):\n\t\t# print line == \"ip:ip:\"\n\t\taddr_str = line.split(\" \", 2)[0]\n\t\tprint addr_str\n\t\t\n\t\t# ip address check\n\t\tif addr_str == \"\":\n\t\t\tprint \"  *  ERROR :  IP address is not specified.\"\n\t\t\texit(1)\n\t\telif not in_ip_address(addr_str), \"  Unable to resolve IP to address '%s'.\" % addr_str\n\telse:\n\t\tprint \""}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\n#    def test_IP_and_PORT(self):\n#        if not ipaddress.extract(address_family(socket.AF_INET)):\n#            raise TestRunnerError(\"unable to use port address\")\n#\n#\t\tself.run_test(self, _socket_test)\n\n#    def test_IP_or_PORT(self):\n#        if not ipaddress.extract(address_family(socket.AF_INET)):"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tif(ipaddress in addr) or (ipaddress[0] == 4):\n\t\tprint \"[+] SUCCESS\"\n\telse:\n\t\tprint \"[-] ERROR\"\n\t\tsys.exit()\n\ndef add_ipv4_addresses():\n\tfor address in ips.findall(\".\"):\n\t\tif(len(address)>4):\n\t\t\tprint \"[+] Address : \"+address\n\t\t\ta = ipaddress.ip_address(address)\n\t\t\tif(a!= '2001:f0:1:2001:f0:1'):\n\t\t\t\tprint \"("}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\telif isinstance(address, ipaddress.IPv4Address):\n\t\tisIPv4 = True\n\telse:\n\t\tisIPv4 = not isIPv4  \n\t\n\t# check the IP length\n\tif not isIPv4 and ipaddress.ip_address(address).version < 4:\n\t\tisIPv4 = False\n\t\n\t# check the port\n\tif 'http' in location and netloc.startswith ('http'):\n\t\tisIPv4 = False\n\t\n\t# check the path\n\tpath = location\n\tif '/' in path:\n\t\tisIPv"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tprint(\"\\nTestIPAddr()\")\n\ttry:\n\t\tip = IPAddr.getaddr(host,port)\n\texcept:\n\t\tip = struct.unpack('<HI',host)\n\t\t#print \"IAddr: %s\" % (ip)\n\tif len(ip)!= 16:\n\t\tprint \"IAddr: got a wrong length, should be 16 bytes\"\n\t\tself.fail_test(1)\n\tif str(ip[0])!= \"0.0.0.0\" or str(ip[1])!= \"::\""}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tTest IPv4 address\n\t    \n\t\"\"\"\n\tTest IPv6 address\n\t    \n\t\"\"\"\n\tTest TCP, UDP, ICMP, All\n\n\t\"\"\"\n\tTest V4 and V6 only\n\t    \n\t\"\"\"\n\tTest LAG and LAG with IPv6 and IPv4\n\t    \n\t\"\"\"\n\tTest Cisco TSN (for testing)\n\t\"\"\"\n\tTest Kanban (for testing)\n\"\"\"\n\n\ndef my_test_function(s):\n\t\"\"\"\n\tFunction to generate an IP address\n\t\"\"\"\n"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t# Test whether the IPv4 address is a valid IPv4 address address\n\tipv4addr = IPvNetworkFactory(\"127.0.0.1\", \"127.0.0.1\").address\n\tprint ipv4addr\n\tprint ipv4addr.to_string()\n\n\t# Test whether the IP address is a valid IPv4 address address or not\n\tipv6addr = IPvNetworkFactory(\"fd00:0000:0000:0008:b0f0:0000:0010\").address\n\tprint ipv6addr\n\tprint ipv6addr.to_string()\n\n\t# Test"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tTest if IPv4_only address\n\t\n\t\"\"\"\n\tTest if IPv4 only address\n\t\n\t'''\n\ttest('Test IPv4 if is ipv4_only is True')\n\tTest if IPv4_only if is ipv4 is False\n\t\n\t'''\n\ttest('IPv4 if is ipv4_only is true')\n\tTest if IPv4_only if is ipv4 is False\n\t\n\t'''\n\ttest('Test IPv4 if is is ipv4 is True')\n\tTest if IPv4 if is ipv4 is True\n\t\n\t'''\n\ttest('Test IPv4"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\ts = \"10.100.1.2\"\n\t\n\ttry:\n\t\tip4 = socket.inet_ntoa(s)\n\t\tif ip4 == \"0.0.0.0\":\n\t\t\t# Test with zero value\n\t\t\tout = str(range(0, 4))\n\t\t\tassert out == \"127.0.0.1\"\n\t# Test ip\n\texcept:\n\t\tpass\n\t\n\ttry:\n\t\tip4 = socket.inet_ntoa(s)\n\t\tif ip4 == \"2.2."}]}
{"task_id": "62ece4992e6aefcf4aabbd85", "prompt": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\tif graph.root is None:\n\t\traise ValueError(\"Invalid graph!\")\n\n\troots = set()\n\tfor classID in graph.rootsOfClass:\n\t\t#print('classID=%s' % classID)\n\t\tif classID in graph.childOf:\n\t\t\tfor node in graph.getNodeListOf(classID):\n\t\t\t\troots.add(node)\n\t\telif classID in graph.propOf:\n\t\t\tnode = graph.getNodeListOf(classID)[0]\n\t\t\tif node in graph.ppropOf:\n\t\t\t"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\tif not is_RDF_GRAPH_class(graph):\n\t\t\n\t\traise Exception('Graph is not a graph instance, or RDF class not present.')\n\t\t\t\n\t\tprint('Not a proper graph instance, or RDF graph not found')\n\t\n\t\treturn [[{}] for i in range(1,len(graph)+1)]\n\n\tg = graph\n\n\tif not is_RDF_NODE_class(g):\n\t\traise Exception('Graph is not a graph node, or RDF numberer node not present.')\n\n\treturn [[g]]\n\t\n\"\"\"\n\t"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\tr = rdflib.RDF(node.nspace.asLiteral('root'))\n\tfound = set()\n\t# find roots in the graph\n\tfor child in r.select(u'child'):\n\t\t# find if the node has the parent\n\t\tchild_ns = child.query(u'child')\n\t\tparent_ns = r.get(u'parent',None)\n\t\tif(parent_ns is not None) and parent_ns.id(u'root') not in found:\n\t\t\t# if it has the parent, we find both"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t# TODO: maybe we need a custom iterator, which isn't easily available?\n\tt = traverse_hierarchy(rdflib.RDFS.subClassOf)\n\treturn set([item.node for item in t])\n\n\ndef locate_nodes_in_subClass_given_parents(graph, rdflib, parents, return_nodes=False):\n\t\"\"\" Find roots whose parent node is already in the graph tree.\n\tThe returned set is a set containing all nodes of the parent's\n\tgraph subtree\n\t\n\t\"\"\"\n\t# TODO: maybe we need a custom iterator,"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\tdef find_roots(graph, triple):\n\t\tif triple.node == graphNode:\n\t\t\tstart = triple.prop\n\t\telif triple.node not in graph.children:\n\t\t\treturn [start]\n\t\telse:\n\t\t\treturn [start.node]\n\treturn find_roots(graph.subClassOf, start=start)\n\n\n/* ------------- RDF library ------------------------- */\n\n#include <memory.h>\n#include <string.h>\n#include <math.h>\n#include <iostream>\n#include <iostream.h"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\tfrom rdflib import RDFS\n\tassert hasattr(RDFS, \"subClassOf\")\n\t\n\tassert rdflib.RDFS.subClassOf([]) == []\n\tassert rdflib.RDFS.subClassOf([]) == rdflib.RDFS(\"SKOS=broader\")\n\tdirProp = getattr(rdflib, \"subClassOf\")()\n\tassert dirProp[2] == dirProp[4]\n\t\n\tfor child in rdflib.RDFS.subClassOf([]) :\n\t\tassert child is not None\n\tfor child in r"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\tfrom sympy.sets import ImmutableSet\n\n\tassert isinstance(r, Tree)\n\tassert isinstance(graph, Graph)\n\tassert isinstance(parent, SubClassOf)\n\tassert isinstance(props, Set)\n\n\tp = parent.parent\n\twhile p!= root:\n\t\tkey = p.id\n\t\tif key == id(\"node\"):\n\t\t\tbreak\n\t\tif key == id(\"edge\"):\n\t\t\tnewp = EdgeNode(p.children[0], p.children[1], None)\n\t\tif key == id(\"prop\"):\n"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\treturn [r.get_root()[int(c) for c in item.args.class_or_subclass]]\n\ndef find_roots(graph, rdflib):\n\tif not rdflib or graph.root and rdflib.RDFS.subClassOf:\n\t\tr = stack.callfunctools.find_roots(graph, rdflib)\n\t\t#print(r)\n\t\tif r:\n\t\t\t#print(\"Found roots:\", r.get_root()[int(c) for c in r])\n\t\t\treturn [r[int"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\tif len(graph) - 1 > 2 * (len(graph) - 2):\n\t\traise Exception(\"Could not find roots in some sort of transitive hierarchy.\\n\")\n\tr = dict()\n\ti = 0\n\tfor child, prop, parent in graph:\n\t\tr[parent] = prop if prop else child\n\tfor i, j in enumerate(i + 1 for i in range(len(graph))):\n\t\tif graph[i][j] not in r:\n\t\t\tr[i] = j\n\tdel graph[i]\n\t#"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\tdef find_roots(graph, prop, parent):\n\t\t\"\"\"Returns the nodes in the graph that correspond to a property\n\t\tof the RDFNode and in the graph the node in the BROADER.\"\"\"\n\t\n\t\t# Do a second traversal of the graph (sub-sub-sub)\n\t\troot = graph.root\n\t\tnode = graph.node\n\t\tnode_id = node.id\n\t\tpcount = node.pcount\n\t\tnode.countLeaves(graph) # Remove leaves in this node\n\t\tnode.parent = parent\n\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd86", "prompt": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t#if type(name) == python_string:\n\t#   name = name.encode('utf-8')\n\t#   if name[len(name):] == '=':\n\t#       return name\n\t\"\"\"return ''\"\"\"\n\tprint(type(name),\n\t\t\trepr(name),\n\t\t\t'\\n\\t',\n\t\t\trepr(value),\n\t\t\t'\\n\\t',\n\t\t\trepr(size_hint),\n\t\t\t'\\n\\t',\n\t\t\t)\n\t#return name.decode('ascii')"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\ttry:\n\t\timport pyyaml\n\t\tfrom os import path\n\t\tif path.isfile(path.dirname(__file__)):\n\t\t\tpyyaml.dump(yaml.safe_load(handle))\n\texcept Exception:\n\t\tpass\n\nif __name__ == '__main__':\n\tfor line in sys.stdin:\n\t\tif line.endswith(':'):\n\t\t\tpass\n\t\telse:\n\t\t\toutput(line)\n\n/*\n * Copyright (c) 2010-2019 Belledonne Communications SARL.\n *\n * This file"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tclass Dump:\n\t\t\"\"\"\n\t\t\tDump to this generator or 'dump' object\n\t\t\"\"\"\n\t\tdef __init__(self, generator, mode=\"\"):\n\t\t\tself.generator = generator\n\t\t\tself.encoding = self.__class__.__name__.lower() # 'u8'\n\t\t\tself.encoding_map = {\n\t\t\t\t'UTF-8': 'utf-8',\n\t\t\t\t'unicode': 'utf-8',\n\t\t\t\t'text': 'text',\n\t\t\t\t'xarray': 'xarray',\n\t\t\t\t'xarray_list':"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tif len(os.path.split(fn)[-1]) == 0:\n\t\tprint(\"Unable to dump to file '%s'\" % (fn,))\n\t\treturn\n\ttry:\n\t\ts = open(fn, 'w')\n\texcept UnicodeEncodeError as err:\n\t\tprint(\"Unable to dump of '%s': %s\" % (fn, err))\n\t\treturn\n\ts.write(txt)\n\nif __name__ == '__main__':\n\tprint(\"Say'mice' and get rid of '%s'\" % os.path."}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\timport yaml.dump\n\tdump = yaml.dump\n\n\t# Load from Python source\n\tdump({\"\": 5}) # 0\n\tdump({\"\": 23}) # 2 (py2-unicode) * 11\n\tdump({\n\t\t\"\": 23, # 0 (py2-unicode) + 13 (py3-string) * 8\n\t}) # 7\n\tdump({\n\t\t\"\": 23, # 2 (py2-unicode) + 5 (py3-string) * 11\n\t}) # 4\n\tdump({\"\": 5}) # 11\n\tdump"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\timport binascii\n\ttry:\n\t\t# Python 3.x uses uconv.config.use_unicode for charmaps\n\t\ts = binascii.crc32(u'\\u2620$' + binascii.hexlify(data))\n\t\tf = uchr(s)\n\t\treturn u''.join([chr2type(t) for t in u''.join(\n\t\t\tf)]).encode()\n\texcept UnicodeEncodeError:\n\t\t# Python 2.x uses uconv.config.char_encoding.encode for charmaps\n\t\t"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\timport re\n\timport yaml\n\tyaml.safe_dump(item, file, encoding=\"backslashreplace\")\n\treturn file.read()\n\ndef safe_load(file):\n\t\"\"\"\n\tLoad a py2-unicode or py3-string into memory\n\t\"\"\"\n\twith open(file) as f:\n\t\tcontents = f.read()\n\treturn yaml.safe_load(contents)\n\ndef read_yaml(path):\n\tparser = yaml.JSONLoader(path, 'json')\n\treturn parser.load_string()\n\n# Load a"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tfrom py2compat.config import config\n\n\tif \"py2compat\" in config.py2compatlist_files:\n\t\tpy2compat_files = config.py2compatlist_files.split(\":\")\n\t\t# if py2compat_files == []:\n\t\t# \tpy2compat_files = config.py2compatlist_files\n\t\tif py2compat_files:\n\t\t\tfile_list = list(map(lambda x:config.py2compat_files.join(x),\n\t\t\t\t\t list(map(lambda x:x."}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\timport string, unicode, platform, re\n\tif '<string>' in platform or platform.windows:\n\t\treturn string.replace(b'..', b'%x' % (len(unicode(__file__))) + b'\\n',\n\t\t\tb'..')\t\t\t# Python3. Not sure what else\n\n\t\tif '<python'>'in file_name:\n\t\t\treturn file_name\t\t\t# Python 3\n\telif '<unicode>':\n\t\treturn unicode.replace(u'n', u'%x' % len(unicode(u'n')), 'a"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\ttry:\n\t\tyaml_path,yaml_contents = os.path.split(os.path.abspath(yaml_file))\n\texcept:\n\t\tpyfile_path = os.path.abspath(py2_unicode_path)\n\telse:\n\t\tpyfile_path = os.path.abspath(py3_string_path)\t\t\n\tif debug:\n\t\tprint(\"PyYAML: \"+repr(yaml_contents))\n\t\tprint(\"PyYAML: \"+repr(\"---\"))\n\t\tprint(\"---\")\n\t\tsys.stdout.flush()"}]}
{"task_id": "62ece4992e6aefcf4aabbd87", "prompt": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef Debug(message, indent = 0, file = None,  end = False):\n\t\t\"\"\"print the most recent log message/exception\"\"\"\n\t\tlogging.debug(message, file, end)\n\n\tdef Warn(message, indent = 0, file = None,  end = False):\n\t\t\"\"\"print the most recent log message/exception\"\"\"\n\t\tlogging.warning(message, file, end)\n\n\t__initialized__ = False\n\tlog_level = 'DEBUG'\n\tlog_handlers={}\n\tlog_file_path = None\n\tfile_handler"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t# Define all loggers we'll use.\n\t#  --logger name name --level level --config config path\n\t#  --log file path --log-verbosity log-level --help output help\n\t#  -D name --default value --min-value\n\t#  -L list\n\t\n\tif len(argv) < 3:\n\t\tprint \"Usage: ex_logger [-D name] [-L list]\"\n\t\treturn\n\tfor arg in argv[1:]:\n\t\tif arg== \"-L\":\n\t\t\t\tlog.add(arg)\n\t\t\t"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef __init__(self, log_level):\n\t\t######\n\t\t#####\n\t\t#  logging level\n\t\t#####\n\t\tself.log_level = log_level\n\n\t\t# logger.add(\"DEBUG\", name=\"logger-debug\",\n\t\t#\t\t\t\t\t\t\tlevel=log_level)\n\t\t#####\n\t\t#####\n\t\t#####\n\t\t#####\n\n/*********************************************************************\n * Software License Agreement (BSD License)\n *\n *  Copyright (c) 2013, TBO Renigam\n *  All rights"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef __init__(self,msg_type):\n\t\tself.__msg_type=msg_type\n\t\tself.__err_msg.append(msg_type)\n\n\tdef __repr__(self):\n\t\treturn \"<Logger>\"\n\tdef __str__(self):\n\t\treturn self.__err_msg[-1]\n\tdef write(self,text):\n\t\tself.__err_msg.append(text)\n\tdef start(self):\n\t\tself.__err_msg=self.__err_msg + \"...\"\n\n\tdef close(self):\n"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef init(self,file=sys.stdout,debug=False,verbose=False):\n\t\tpass\n\n\n\t#TODO: do we have any kind of logging and other logger? \n#\tdef debug(level, msg, err=sys.exc_info()):\n#\t\tif not debug: return ''\n#\t\telse: return '%s %s on : %s' % (level,msg+':',err)\n#\n#\tdef info(msg):\n#\t\tif not DEBUG: return ''\n#\t\t#print('DEBUG')\n#\t"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tDEFAULT_SYSLOG_LEVELS = ['CRITICAL', 'ERROR']\n\n\tdef __init__(self,syslog_handler,log_path,log_level,level,is_debug=False,logger=None):\n\t\tself.logger=None\n\t\tself.is_debug=is_debug\n\t\tself.log_path=log_path\n\t\tself.log_level=log_level\n\t\tself.logger_file=None\n\t\tself.log_facility='info'\n\t\tself.default_logger=logger\n\t\tself.default_level"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef __init__(self, logger_name=None, file=True):\n\t\tself.logfile = None\n\t\tself.logger_name = logger_name or 'glog'\n\t\tself.file = file\n\t\tself.logger_inited = False\n\n\tdef set_level(self, level):\n\t\tif level == 1:\n\t\t\tself.logger_name = 'log.level1'\n\t\telif level == 2:\n\t\t\tself.logger_name = 'log.level2'\n\t\telse:\n\t\t\tself.logger"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef debug(logger, msg):\n\t\tif type(msg) is not str:\n\t\t\traise ValueError(\"\"\"debug message to LOG type should be an instance of a string.\"\"\")\n\t\tprint(LOG_TYPE, msg)\n\t\tif os.name == 'nt':\n\t\t\tfrom utils import os_write\n\t\t\tos.write(os.fs.curdir, msg + '\\n')\n\t\telse:\n\t\t\tos.write(os.fs.curdir, msg)\n\tdef info(logger, msg):\n\t\tif not os."}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef log(level, message, exception=None):\n\t\tlogging.log(level, message, exc_info=exception)\n\n\tdef critical(exception):\n\t\t\"\"\"Critical logger.\"\"\"\n\t\tlogging.critical(str(exception))\n\nclass LogManager:\n\t\"\"\"Logger implementation used by L{LogManager}. All logging related methods are expected to use log() method, \n\twhich is typically called by an application when a log file message is received.\"\"\"\n\n\tdef __init__(self, log=None):\n\t\tself._log = log\n\t\n\tclass Logger:"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef __init__(self, *args, **kwargs):\n\t\tpass\n\nclass Logger():\n\tdef __init__(self, *args, **kwargs):\n\t\t# Make sure all variables and arguments are set, otherwise\n\t\t# we'll get warnings about any uninitialized variables or\n\t\t# arguments in the command line.\n\t\tself.LOG_LEVELS = defaultdict(logging.Level)\n\t\tself.LOG_LEVELS[(LogLevel.ERROR,) + logging.NOTSET] = logging.INFO\n\t\tos.environ['PGE_LOGGER'].append('PGE_"}]}
{"task_id": "62ece4992e6aefcf4aabbd88", "prompt": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tdef __new__(self,shape,dtype):\n\t\treturn array.Type(self,shape,dtype)\n\n\tdef __init__(self,array,shape,dtype):\n\t\treturn __new__(self,array.shape,dtype)\n\ndef array2index(array,dtype):\n\t\"\"\"\n\tCreate an array with shape and data-type (index is int)\n\t\"\"\"\n\treturn array.type(array,shape=False,dtype=dtype)\n\n/*\n * MIT License\n *\n * Copyright (c) 2020-present Cloudogu GmbH and Contributors"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tif len(shape)!= 2:\n\t\traise ValueError(\"Array shape must be 2-tuples\")\n\n\tm = np.asarray(shape)\n\tif m.dtype!= dtype:\n\t\tm = m.astype(dtype)\n\n\treturn m\n\n#ifndef _USS_UTIL_H_\n#define _USS_UTIL_H_\n\n#include <windows.h>\n#include <commctrl.h>\n#include <oleacc.h>\n#include <wtypes.h>\n#include <winnetwk.h>"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t_shape = tuple(list(shape))\n\n\tdtype = np.dtype(np.int32)\n\n\tarr = np.empty( _shape, dtype=dtype)\n\n############\n#        Parameters for a numpy array  #\n############\n\n\t#  This array is actually a numpy array. \n\n\tarr['X']  = np.float64( 1.0 )\n\tarr['Y']  = np.float64( 1.0 )\n\tarr['Z']  = np.float64( 1.0 )\n\tarr"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tassert (shape == ())\n\tif (type(shape)!= (list, tuple)):\n\t\tshape = (shape[0],)+ shape\n\t\tif type(shape!= tuple):\n\t\t\traise ValueError(\"The array with the incorrect shape must be of rank 2 or be a 1d array.\")\n\telse:\n\t\tshape = (shape[0],)*np.ndim(shape)\n\t\t\n\treturn numpy.array([shape], dtype=dtype)\n  \n\ndef _extract_min_index(array):\n\t\"\"\"Extracts min_indibute_index that has"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tfrom numpy.core import ndarray\n\ttry:\n\t    arr = np.zeros((shape,), dtype=dtype)\n\t    arr = arr.reshape((shape[0], shape[1]))\n\t\n\texcept Exception as err:\n\t    print(err)\n\n\nclass Slice(object):\n\tclass Elem(object):\n\t\tdef __init__(self, start, stop, item_type):\n\t\t\tself.start = start\n\t\t\tself.stop = stop\n\t\t\tself.item_type = item_type\n\t\t# return an Nd"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tt = np.dtype('i4')\n\treturn [np.repeattrue(t, c) for c in [shape, dtype]]\n\n@tf.RegisterGradient(\"ArrayWithStride.apply\")\ndef apply(self, *inputs):\n\tinput0 = inputs[0]\t\n\tif input0.ndim > 1:\n\t\treturn\n\tshape = inputs[1:]\n\tdtype = input0.dtype\n\tif dtype.name in ['c', 'f', 'F']:\n\t\tif shape is None:\n\t\t\tshape = (1,)\n\t\tif"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tarray = np.zeros(shape, dtype=dtype)\n\tarray.__setitem__(np.arange(1, len(shape) + 1), array)\n\treturn array\n\ndef shape_to_indices_tuple(shape):\n\t\"\"\"\n\tCreate and return a tuple containing shape-like indices\n\t\t(length of first array of shape)\n\t\"\"\"\n\treturn shape, tuple(range(1, len(shape) - 1))\n\ndef shape_to_indices(shape):\n\t\"\"\"\n\tCreate and return a tuple containing shape-like indices\n\t\t("}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tif not issubclass(dtype, np.dtype):\n\t\traise TypeError(\"dtype '%s' is not an iterable\" % type(dtype).__name__)\n\telif len(shape) == 1:\n\t\tshape = (shape,)\n\tshape = list(shape)\n\tdtype = np.dtype(dtype)\n\tshape = tuple(shape)\n\t\n\tnew_shape = [max(shape) + 2] if len(shape) > 2 else shape + [2]\n\tsub_shape = [shape[i] for i in range(len(shape) - 1)]"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\treturn array(shape, dtype)\n\n\ndef cscalar(x, c=1e-12, **kwargs):\n    \"\"\"\n    Return a numpy.ndarray of dtype `dtype` with a scalar value c\n \n    Parameters\n    ----------\n    x : array-like, int, float, ndarray\n    \tA scalar array of shape *x* shape with values of type `dtype`\n\tc : float, int, optional\n        Cancellation-mask in effect. Defaults to 1e-12"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tfrom nengo.ninja.core.ninja import NinjaCodeGenerator\n\tNinjaCodeGenerator.instance.compile(shape)\n\tf = NinjaCodeGenerator.instance.evaluate()\n\tarr = f.args[0]\n\treturn arr.astype(dtype)\n\n# \n#  Copyright 2014 Peter Dimov\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#     "}]}
{"task_id": "62ece4992e6aefcf4aabbd89", "prompt": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tdef gaussian_centertwo_sigma(x, mu):\n\t\tu = np.sqrt((x - mu) * (x - mu))\n\t\tv = (np.sqrt((x - mu) * (x - mu)) - 0.5 * u)\n\t\treturn -u / v\n\td = {x: 0 for x in range(4)}\n\tR = np.mean(gaussian_centertwo_sigma(d[x, -1], 0.1))\n\treturn d[x]\n\n# Copyright 2014-2017 Enrico"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tfrom matplotlib.figure import Figure\n\tfrom matplotlib.backends.backend_agg import FigureCanvasAgg\n\timport matplotlib.pyplot as plt\n\timport matplotlib.path as mpath\n\timport matplotlib.colors as mcolors\n\tmcolors.use('hotpinkr')\n\tcanvas = FigureCanvasAgg('scatterplot_scatterplot_scatter')\n\tfig = mpath.Figure()\n\tfig.colorbar(False, mcolors.SlateBlue('#f0f8ff'))\n\tfor ax in fig.get_axes():\n\t\tax.set_axis_off"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tfrom libpysal.sal.Gaussian import BaseGaussian\n\tnoise = Random()\n\tnoise = BaseGaussian(noise, sigma=-1.0, t=0.1, c=0)\n\n\t# test\n\tpass#IMPORTIMPORTIMPORT from math import sqrt\n\tdel sqrt\n\tN = 100000\n\tfor k in range(N):\n\t\tx = noise(0.5)\n\t\ty = noise(0.9)\n\t\tx.set_data((1.0, 1.0, -0.5, 0.2))\n"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tdef __init__(self, sigma=0.1, centered=False):\n\t\tsuper(Gaussian, self).__init__()\n\n\t\tself.sigma = float(random.random() * sigma) + 0.1\n\t\tself.centered = centered\n\n\tdef gauss(self, input, sigma=0.1):\n\t\treturn Gaussian(self.centered) * self.sigma * input * self.sigma + self.sigma * (\n\t\tinput + self.centered) * self.sigma\n\n\tdef fit(self, input):\n"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tdef gaussian_centered_around_2(G):\n\t\tR = np.zeros((G.shape[0],G.shape[1]))\n\t\tR[0] = G[2:4]\n\t\tR[-1] = G[-2:4]\n\t\tR -= np.dot(G, np.linalg.solve(G.T, R))\n\t\treturn R\n\tm = gaussian_centered_around_2(a.data - np.r_[np.arctan(a.data / np.norm(a."}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tdef __init__(self, N=100, sigma=0.2):\n\t\tself.N=N\n\t\tself.sigma=sigma\n\tdef Gaussian(self, X, y_true):\n\t\tself.X = X\n\t\tself.Y = y_true\n\tdef test(self, X,y_true):return True\ndef test_kernel(Y,X):\n\t\"\"\"\n\t\tThis function tests Gaussian kernel for the given data.\n\t\"\"\"\n\ty = np.zeros(self.N)\n\ty[0] = 0."}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tdef test():\n\t\tfrom PyQt5 import QtGui, QtWidgets, QtCore\n\t\tfrom PyQt5.QtWidgets import QDialog, QMessageBox\n\t\t\n\t\tgl = QtGui.QGuiApplication.instance().createOpenGLContext()\n\t\t\n\t\twindow1 = QtGui.QMainWindow()\n\t\tQApplication.instance().setWindowIcon(QIcon(str(gl.resource_path(\"images/images/flying-square.png\"))))\n\t\tfingers = []\n\n\t\tscene = QtGui.QGraphicsScene((600, 600), -10"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tfrom skimage import filters\n\n\treturn filters.GaussianFilters(0.0, filters.ImageFilter(0, sigma=0.1))\n\n\n@_check_input.register\ndef gaussian_filt1d(image, sigma):\n\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1,\n\twith a number of filters to select between.\n\n\t\timage = np.array(\n\t\t\t\t\t\"Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do\"\n\t\t\t\t\t"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tdef __init__(self, x_mean=None, sigma=0.1, noise_std=2.0):\n\t\t\"\"\"\n\t\t:param x_mean: Gaussian point, either average point or covariance (default None)\n\t\t:param sigma: sigma value, must be odd\n\t\t:param noise_std: noise variance, must be odd\n\t\t\"\"\"\n\t\tassert isinstance(sigma, float),'sigma needs to be a float if `noise_std` is None.'\n\t\tassert isinstance(x_mean, (np.ndarray, np.generic."}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tparams = dict(\n\t\tsigma = 0.1,\n\t\tsigma_min = 1.0,\n\t)\n\tfor key, value in params.items():\n\t\tvalue.update({'sigma' : value.pop('sigma_min'),'sigma_max' : value.pop('sigma')})\n\t\treturn np.random.uniform(low=-value['sigma'], high=value['sigma'], size=value['sigma_min'].size)\n\treturn np.random.uniform(low=0., high=1., size=32).astype(np.float32"}]}
{"task_id": "62ece4992e6aefcf4aabbd8a", "prompt": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t# Load configuration.ini\n\tif not os.path.isfile(filename):\n\t\traise Exception(\"Could not find configuration file at %s\" % (filename,))\n\tconf = configparser.ConfigParser()\n\twith open(filename) as f:\n\t\tlines = f.readlines()\n\t\tlines = [i for l in lines for i in l]\n\t\tparse_config = {}\n\t\terrors = []\n\t\t_ = dict()\n\t\tfor line in lines:\n\t\t\tline = line.strip().replace('\\n', '')\n\t\t"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('config_filename')\n\tparser.add_argument('-l', '--list', action='store_true', default=False, help='List the configuration filenames, do not add the file name as part of the list')\n\tparser.add_argument('-s', '--validate', action='store_true', default=False, help='Run validation to output warning messages')\n\tparser.set_defaults(list=True)\n\treturn parser.parse_args()\n\n\ndef load(args, log_dir=None,"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\timport importlib\n\timport os\n\n\tparser = importlib.util.find_spec(filename)\n\tif not parser:\n\t\tlog.error('Configuration file %s not found.', filename)\n\n\tfile_infos = parser(filename, spec_context={'source_file': source_file})\n\n\t# Read the configuration file\n\tif not file_infos['files']:\n\t\tlog.error('Configuration file \"%s\" does not have a single configuration file.', filename)\n\t\treturn\n\n\t# Load the configuration file\n\t# This should be more of"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tfrom pip._internal.utils.misc import collect_config_files_to_check\n\tfrom pip._internal.utils.misc import load_file\n\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"configuration_files\")\n\tparser.add_argument(\"files\", nargs=\"*\", type=argparse.FileType('r'), help='comma seperated list of files')\n\tparser.add_argument(\"warnings\", type=bool, action='store_true', help='add warnings to warnings file')\n\tparser.add_argument(\"stderr\", type=bool,"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t# parse configuration files, return dict of configuration filename to corresponding parsed configuration,\n\t# and parser.LogRecord for parsing errors.\n\twith open(sys.argv[2], \"r\") as file_:\n\t\tparser = json.load(file_)\n\tparser_results = []\n\n\tfor item in parser:\n\t\tfilename = os.path.basename(os.path.dirname(item[\"_source_path\"]))\n\t\t# parse configuration file and validate it\n\t\ttry:\n\t\t\tfile_path_data = file_.read(filename, encoding=\"utf-8"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tfilenames = []\t\n\tfor fn in filenames:\n\t\t# Parse configuration file and initialize logging instance if there are any\n\t\tf, a=yaml.safe_load(fn)\n\t\ttry:\n\t\t\t# Initialize the logger instance\n\t\t\ts = logging.getLogger(\"json_parse\")\n\t\t\t\n\t\t\t# Get any configuration file name\n\t\t\tdef path_format(filename):\n\t\t\t\treturn filename.replace('.', '/') \n\t\t\tassert not isinstance(f, dict)\n\t\t# If nothing was found, look for next line\n\t\texcept Exception as e:\n"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\ttry:\n\t\tfrom ipywidgets.fileio.configurations import Config\n\t\treturn Config.load(configFiles, mode=\"YAML\")\n\texcept Exception as e:\n\t\tlogging.error(\"Unable to load the configuration files: %s\", e)\n\t\treturn\n\t\n\t#TODO: Add a default parser for these config files?\n\tlogger.fdebug(str(configFiles))\n\t\n\t# For each filename, parse the config and validate the configuration. \n\tlogger.fdebug(str.join(configFiles, \"---\")))\n\t\n\tfor filename in config"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t# TODO: this will go into a function which may be rewritten into a static function or a function\n\t# from config directory\n\tlogging.info(\"Load configuration...\")\n\tfilenames = [f.rsplit(\"/\")[-2] for f in os.listdir(cfg_path) if f.endswith(\".conf\")]\n\tlog_paths = [f.rstrip(\"-1\") for f in filenames if \".conf\" not in f]\n\tlogger = logging.getLogger(\"s3\")\n\n\tfor filename in filenames:\n\t\tfilename = os.path.join(cfg_path,"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t# open /etc/config file\n\tlog_config = open('/etc/config', 'r')\n\t# read and parse lines into a structured dict\n\ttry:\n\t\twhile True:\n\t\t\tline = log_config.readline()\n\t\t\tlog_config.readline()\n\n\t\t\tif not line:\n\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\tdict = json.loads(line)\n\t\t\t\tlog_config.readlines()\n\t\t\t# if a line matches the pattern, validate all other lines\n\t\t\tif pattern.search(line):\n\t\t\t"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t\tfrom configparser import RawConfigParser\n\t\tconfigparser = RawConfigParser()\n\t\t# load all configuration files that match the list of configurations\n\t\tparser1 = configparser.RawConfigParser(read1=True)\n\t\tparser2 = configparser.RawConfigParser(read2=True)\n\t\tparser1.has_option = parse_bool\n\t\tparser2.has_option = parse_bool\n\t\tconf_filename = \"conf1\"\n\t\tparser1.read_configuration(conf_filename)\n\t\tconf_dict = dict("}]}
{"task_id": "62ece4992e6aefcf4aabbd8b", "prompt": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\ttry:\n\t\t# In python 3.2 we're ignoring the bytes object and just converting\n\t\t# that to ``Unicode``. However, I want to convert the bytes object\n\t\t# properly for the Python 3.3 compatible formats.\n\t\t# The python 3.3 string type converters\n\t\t# are also available in NumPy >= 3.2\n\t\treturn obj.decode('utf-8', fallback=str.maketrans)\n\texcept (UnicodeDecodeError, io.UnsupportedOperation):\n\t\treturn bytes(obj, cls=str)\n\ndef get"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tstr = obj\n\tif type(obj) in [list, tuple]:\n\t\tif not isinstance(obj, str):\n\t\t\treturn None\n\t\treturn str.encode('utf-8','replace')\n\telse:\n\t\treturn str.encode('utf-8')\n\n\ndef _get_utf8_bytes(obj):\n\t\"\"\"\n\t\tThis function returns utf-8 bytes corresponding to ``obj`` in case\n\t\tit is a string using UTF-8. \n\t\"\"\"\n\treturn str(obj).decode('utf-8')\n\n\n"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isinstance(obj, unicode):\n\t\treturn obj\n\telif isinstance(obj, bytearray):\n\t\treturn obj.decode('utf-8')\n\telif issubclass(obj.__class__, str):\n\t\treturn obj\n\traise TypeError('%s is not a string in %r' % (cls.__name__, obj))\n\ndef _encode_json_bytes(obj):\n\tif type(obj) is str and obj.startswith('{'):\n\t\tjson_str = json.dumps(obj)\n\telif type(obj) is unicode and obj.startswith('\"{"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isinstance(obj, unicode):\n\t\treturn bytes(obj)\n\telse:\n\t\treturn obj\n\ndef bytes_from(string):\n\t\"\"\"Converts an object of string (object) to a byte object using the specified byte order.\"\"\"\n\tif isinstance(string, (bytes, str)):\n\t\treturn string\n\t elif isinstance(string, unicode):\n\t\treturn str(bytes_from(string.encode(\"utf-8\")))\n\t else:\n\t\treturn str(object.__getattribute__(\"bytes\", str(string)))\n\ndef to_list(obj"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\ttry:\n\t\tdata = codecs.subdecode(obj.encode('utf-8'), encoding)\n\texcept AttributeError:\n\t\t# If it doesn't work, try using a file-like object\n\t\ttry:\n\t\t\treader = codecs.getreader('utf16_be')\n\t\t\tfileobj = file(obj.encode('utf-16be'), reader)\n\t\texcept TypeError:\n\t\t\tpass\n\t\telse:\n\t\t\tdata = fileobj.read()\n\t\t\n\t# Return the data as a bytes object\n\treturn data\n\n\ndef to"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isinstance(obj, unicode):\n\t\treturn obj\n\tif isinstance(obj, str): \n\t\tobj=unicode(obj) \n\treturn obj\n\n\t\ndef byte_array2wstring(b_byte_array):\n\t\"\"\"\n\tGiven an array of bytes, this function converts the byte array into a\n\twstring and returns it in case of success or empty string to indicate no \n\tspace left in the resulting string.\n\t\t(It takes advantage of multi-byte sequences as it will not\n\trepresent the entire contents of the byte array, even if the contents \n\t"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\treturn_buffer = get_bytes(self.utf8(obj))\n\treturn_buffer = struct.pack(\">L\", return_buffer)\n\treturn struct.pack(\">ii\", len(obj))\n\treturn return_buffer\n\n\ndef get_memory(size, return_bytes, return_buffer) -> memory:\n\tfor buffer in bytes(buffer):\n\t\tsize = size.get_size()\n\t\tif size:\n\t\t\treturn memory.Buffer(buffer), size\n\n\t\treturn memory.Buffer(return_bytes, return_buffer)\n"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t# Return a unicode object, if ``obj`` is a string.  \n\tif obj is None:\n\t\treturn None\n\t# Otherwise check to see if the string is UTF-8.  \n\tis_ustr = isinstance( obj, str )\n\t# if it is not a string then return None.\n\tif is_ustr is False:\n\t\treturn None\n\treturn obj\n\ndef format_unicode( data, format_mode='f'):\n\t# format a unicode string using either 'raw', 'bak', or a raw unicode object\n\ttry:\n\t"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif not isinstance(obj, str):\n\t\ttry:\n\t\t\t# Try to convert ``bytes`` to ``str``\n\t\t\treturn bytes(obj, \"utf-8\")\n\t\texcept Exception:\n\t\t\traise TypeError(obj)\n\telse:\n\t\t# Try to convert ``u`` to ``str``\n\t\ttry:\n\t\t\treturn str(obj, \"utf-8\")\n\t\texcept Exception:\n\t\t\traise ValueError\n\treturn obj\n\ndef bytes_from_unicode(u):\n\t\"\"\"Returns ``utf-16**-2``"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t# if we reach this line, we cannot write anything :(!\n#\tbytes_str = ''\n#\t# bytes_str = obj.encode('utf-8', locale)\n\n\tbytes_str, encoding = _get_bytes_string(obj)\n\t# unicode(obj, encoding)\n\n\t# if we reach this line, we cannot write anything :(!\n#\tunicode_str = None\n#\t# unicode_str = obj.encode('utf-8', locale)\n\n#\treturn bytes_str, encoding\n\ndef _get_bytes_"}]}
{"task_id": "62e60723d76274f8a4026b76", "prompt": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tpass\n\nclass DateTime:\n\t\"\"\"\n\tCreate a datetime object from a given time and a given timezone.\n\t\n\t:param utc: if false, interpret microseconds as datetimes.\n\t    Otherwise, interpret seconds as datetimes, as would be in C time.\n\t    This is particularly useful for converting from Python time to\n\t    a timedelta in Python (e.g., from microsecond since epoch).\n\t\n\t:param tz: time zone of datetime.  If timezone is None, then an attempt to\n\t    determine the time zone will be made."}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif tz is None:\n\t\traise ValueError(\"Need to define a tz. For %s.to_time() = %i\" % (from_ticks.__name__, ticks))\n\tif ticks < 0:\n\t\traise ValueError(\"ticks must be 0 or larger.\")\n\tif len(ticks)!= 86400000000000:\n\t\traise ValueError(\"ticks must be 8640000000000 or higher.\")\n\tif tz is None:\n\t\t# Use default timezone if none was passed\n\t\ttz = time.tzlocal\n\treturn time.time()\n\t# For some reason, python"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tstart = float(util.as_int(ticks)) * 86400000000000\n\tmax = float(util.as_int(ticks)) * 86400000000000\n\treturn cls(start, max)\n\nclass Time(TimeMixin, TimeBase):\n\t\"\"\"\n\tTime from nanosecond offset.\n\t\n\t:param ticks: 0, 1,..., 4\n\t:type ticks: int\n\t:param tz: optional timezone, from (UTC) or (None)\n\t:type tz: datetime.tzinfo\n\t\n\t:raises TypeError: if ticks not a valid number of seconds\n"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tfrom django.utils.timezone import make_aware\n\t\n\tutcnow = make_aware(unixtime(), 3600)\n\t\n\tif not ticks:\n\t\tticksp = 86400000000000\n\telse:\n\t\tticksp = ticks * 86400000000\n\t\n\treturn cls(\n\t\tyears=ticks / (ticksp or 999),\n\t\tmonths=ticksp,\n\t\tdow=ticksp or 0,\n\t\tseconds=ticksp or 0,\n\t\tmicroseconds=ticksp or 0,\n\t\ttz=tz,\n\t)\n\t\n\t\n"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tclass Class_from_ticks:\n\t\tdef from_ticks(cls, ticks, tz=None):\n\t\t\treturn cls(*ticks[::-1], tz=tz)\n\t\n\treturn Class_from_ticks\n\n\ndef to_ticks(ticks, tz=None):\n\t\"\"\"\n\tCreate a time to ticks (nanoseconds since midnight).\n\tA new time is assigned to the beginning\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\treturn Timestamp(int(ticks + (0 / 1e9)))\n\n\n# noinspection PyTypeChecker\nclass NaiveDateTime:\n\n\t\"\"\"\n\tConstructor for a new datetime object.\n\t\n\t:param tz: timezone instance\n\t:type tz: DatetimeZone\n\t\"\"\"\n\tdef __init__(self, tz, year=None, month=0, day=0):\n\t\tself.year = year\n\t\tself.month = month\n\t\tself.day = day\n\t\t# Note: year and month are normally not set for DateTime\n\t\t"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif ticks < -0x100000000 or ticks > 0x100000000-32:\n\t\traise ValueError('ticks must be a number between -0x100000000'\n\t\t\t'-32 and 0x100000000')\n\tif nanos are not None and ticks > 4294967296 or ticks < -4294967296:\n\t\traise ValueError('Ticks must not be smaller or equal to 1/4294967296'\n\t\t\t' (not 0) or less than 0 (not -999)')\n\tif tz is None and tz is not None:\n\t\traise ValueError('Timezone must be None (not None)')"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tnanoveto = Timestamp(ticks)\n\tif self.is_leap_year:\n\t\treturn Time(int(nanoveto.time() / 86400000000000), tz)\n\treturn Time(nanoveto.date(), tz)\n\ndef is_nan(dt):\n\t\"\"\"\n\tCheck if d is a Nan/Nanosecond (NAN or NEGATIVE)\n\t\n\t:param dt: d\n\t    :type dt: d\n\t:rtype: bool\n\t\n\t:returns: True if d is a nanosecond, False otherwise\n\t\"\"\"\n\tif"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\td = time.time() - ticks\n\tif np.round(d)!= 0:\n\t\traise ValueError('Invalid ticks value: '+str(d)+' for time '+repr(ticks))\n\n\tif (not _is_int_type(tz)) and (tz is None):\n\t\traise TypeError(\"tz value must have a tzinfo object\")\n\t\t\t\n\treturn cls.from_datetime(tz, ticks, nanok=False)\n\n######################################################################\n## Time Series\n######################################################################\n\ndef to_millis(ts):\n\t\"\"\"Convert from time series\n"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t# Time object (nanoseconds since midnight).\n\t# (86400000000000 <= ticks <= 86400000000000)\n\t\n\tif isinstance(ticks, Tick):\n\t\tif ticks < 0:\n\t\t\traise ValueError('ticks must be positive')\n\telif tz is None: tz = current_timezone()\n\t\n\t# Create epoch\n\tepoch_secs = int(ticks / timedelta(seconds=int(ticks % timedelta(seconds=1))))\n\t\n\t# Create unix timestamp\n\tepoch_int = int(ticks)\n\t\n\tif epoch_int == 0:\n\t\t"}]}
{"task_id": "62e60873d76274f8a4026bd8", "prompt": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif not protocol_version:\n\t\treturn { }\n\tif protocol_version not in version_type_map.keys():\n\t\traise TypeError(\"unsupported Bolt protocol version: %s\" % (protocol_version,))\n\tversion = version_type_map[protocol_version]\n\treturn BOLT_HANDLER_DICT.setdefault(version, BoltHandler(protocol_version))\n\n\ndef get_protocol_handler(protocol, version=None):\n\t\"\"\"\n\tGet the instance of Bolt protocol handler class for a given protocol.\n\tIf the protocol is not"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\tversion_tuple = None\n\telse:\n\t\tversion_tuple = tuple(protocol_version)\n\tprotocol_handlers = {}\n\tfor i in range(3, 4):\n\t\thandler_class = version_tuple[i]\n\t\tif not hasattr(handler_class, BOTTLE_PROTOCOL_HANDLERS):\n\t\t\traise TypeError(\"No protocols were found \"\n\t\t\t    \"for protocol version %d\" % (protocol_version,))\n\t\tfor handler in getattr(handler_class, protocol_handlers, {}"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\treturn Boltran_DB_PROTOCOL_VERSIONS[cls.get_protocol(protocol_version)]\n\npackage org.jeewx256.pay.enums;\n\nimport java.math.BigInteger;\n\n/**\n * @class Age\n * @description \u5fae\u4fe1\u652f\u4ed8\u76f8\u5173\n * @date 2019-08-01 21:00\n */\npublic enum PayModeEnum {\n    /** \u5f00\u901a\u652f\u4ed8 */\n    V1(\"\u652f\u4ed8\u5b9d\"),\n    /** \u7eaf\u652f\u4ed8 */\n    V2("}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif isinstance(protocol_version, string_types):\n\t\ttry:\n\t\t\treturn handlers.Protocol_Handlers[protocol_version.lower()]\n\t\texcept KeyError:\n\t\t\tif protocol_version == 1:\n\t\t\t\treturn handlers.protocol_handlers_version0\n\t\treturn handlers.protocol_handlers_version1\n\treturn handlers.protocol_handlers\n\n#include \"stdafx.h\"\n\n#include \"test.h\"\n\n#include <string.h>\n#include <assert.h>\n#define NOMINMAX\n#define stricmp"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tprotocol_version = (isinstance(protocol_version, tuple) and\n\t\ttuple(protocol_version) >= (3, 5))\n\n\tif not protocol_version:\n\t\tprotocol_version = (3, 5)\n\tprotocol_handler_classes = defaultdict(dict)\n\tfor handler in handlers:\n\t\tif not protocol_handler_classes.has_key(handler.protocol_version):\n\t\t\tif protocol_version not in handlers:\n\t\t\t\t# Not found and no protocol handler was found\n\t\t\t\tcontinue\n\t\t\thandler.version = protocol_version"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t# Ensure bolt has a sane __version__ value\n\tif protocol_version is not None:\n\t\tprotocol_version = bcrypt.hashlib.sha256(util.as_bytes(str(protocol_version)))\n\t\t\n\t# Handle protocols which are unavailable or supported\n\tsupported = [protocol_version!= None]\n\tif bcrypt.compare_version(\"1.2.4\", protocol_version) or bcrypt.compare_version(\"1.3.6\", protocol_version):\n\t\tsupported = False\n\t\t\n\t# Parse version tuple\n\tif bcrypt."}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t# Pick a protocol version tuple or list of the supported versions\n\tfor proto in Protocol.available_protocols:\n\t\t# For now, just check if it exists\n\t\tif not (protocol_version == None or proto.protocol_version >= protocol_version):\n\t\t\tcontinue\n\t\tif protocol_version not in Protocol.protocol_dict:\n\t\t\tProtocol.protocol_dict[protocol_version] = dict() # keep-alive\n\t\t\tProtocol.protocol_dict[protocol_version]['supported'] = True\n\t\t\n\t\tif getattr(Protocol.protocol_dict[protocol_"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is not None:\n\t\tversion = protocol_version\n\t\tif version == (3, 5):\n\t\t\tversion = (3, 0)[x < 2] if x >= 4 else None\n\t\telse:\n\t\t\traise TypeError(\"Protocol version (%s) not provided!\\n\" % version)\n\telse:\n\t\tif version == (3, 5):\n\t\t\tversion = (3, 0)\n\t\telse:\n\t\t\traise TypeError(\"Protocol version (%s) not provided!\\n\" % protocol_version)\n\treturn BoltProtocol"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\tprotocol_version = tuple()\n\tif protocol_version not in SUPPORTED_PROTOCOL_VERSIONS:\n\t\traise TypeError(\"protocol version %s is not compatible with %s\"\n\t\t                % (protocol_version, SUPPORTED_PROTOCOL_VERSIONS))\n\treturn __PROTOCOL_HANDLERS__.get(protocol_version, {}).copy()\n\n\nclass BoltHandler(object):\n    \"\"\" \n\tBoltHandler is an interface to be passed around bolt-api specific"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tprotocol_version = tuple(protocol_version.version for protocol_version in (PROTOCOL_VERSION,))\n\tif len(protocol_version) == 0:\n\t\ttry:\n\t\t\tprotocol_version = SUPPORTED_VERSIONS\n\t\t\tif protocol_version not in (PROTOCOL_VERSION.DEFAULT, PROTOCOL_VERSION.LOW):\n\t\t\t\traise ProtocolErr(\n\t\t\t\t\tf\"protocol version {protocol_version}'s \"\n\t\t\t\t\t\"incompatible; \"\n\t\t\t\t\t\"must be '{PROTOCOL_VERSION.DEFAULT.short_name} < '8'.\"\n\t\t\t\t)\n\t\t"}]}
{"task_id": "62e60e3bd76274f8a4026d1a", "prompt": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tif values is None:\n\t\traise ValueError(\"no values supplied\")\n\tvalues = [parse_bookmarks(value, self.raw_values) for value in values]\n\treturn Bookmark(values[0])\n\nclass Bookmark(object):\n\tdef __init__(self, value):\n\t\tself.value = value\n\t\tself.raw_value = self.value\n\n\tdef string(self):\n\t\treturn self.__class__.value\n\n\tdef __str__(self):\n\t\treturn \"'%s'=%s\"%(self.value,"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\treturn cls(values, cls.bookmark_type)\n\nimport datetime\nimport logging\nimport re\nfrom typing import Any, Dict, Mapping, Optional, Type, Tuple, Type, cast\nfrom uuid import uuid4\n\nfrom eth_tools.address import HexBytes\nfrom eth_tools.constants import ConstantFilterMode, DEFAULT_FILTER_MODE\n\nfrom eth_tools.block_transactions.types.transaction import Transaction, BlockCommitmentInfo\nfrom eth_tools.chain.transaction import TransactionFactory\n\nfrom eth_tools.constants import EPOCH_START, EPOCH_END\nfrom eth"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tif not isinstance(values, Iterable):\n\t\traise TypeError(\"Expected Iterable but found\" \" % type(values).__name__)\n\t\t\t\n\tdef from_raw_values(cls, values):\n\t\ttry:\n\t\t\tlist_ = cls(values)\n\t\texcept (ValueError, TypeError) as e:\n\t\t\t\n\t\t\traise TypeError(\"Bad type for '%s'\"\n\t\t\t\t\t\t  \"(expected '%s' or %r, but found '%s'\"\n\t\t\t\t\t\t  \")\\n%r\" %\n\t\t\t\t\t\t(values, cls.__name__, type(values).__name__, exc_str("}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tif not isinstance(values, Iterable):\n\t\traise TypeError(\"%s() takes 2-tuples\" % type(values))\n\n\tvalues = []\n\tfor value in values:\n\t\ttry:\n\t\t\tvalue = raw(value)\n\t\texcept ValueError:\n\t\t\traise ValueError(\"invalid rawBookMark string: %r\" % value)\n\t\tvalues.append(value)\n\treturn cls(**values)\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (C) 2019 tribe29 GmbH - License: GNU"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\treturn ValueList(cls(*values), default=dict)\n\n\nclass ValueList(Iterable, BaseValue):\n\t\"\"\"\n\tCreate a RawValueCollection object with one raw bookmark string value,\n\tas a set. You are responsible for casting the value to the correct type of\n\tclass (Raw, Rich).\n\t\n\tThe object's underlying vector is in some space of the value.\n\t\n\t:param value: a value containing raw bookmark strings.\n\t:type length: int\n\t\"\"\"\n\t__slots__ = ('value', 'length', 'is_list_raw"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tfrom functools import reduce\n\tfor item in values:\n\t\tvalue = reduce(lambda x, y: x + y, item.split())\n\t\tyield value\n\traise Exception\n\n\t# TODO: Move into dictionary\n\tfrom bookmarks.bookmark import Booknote\n\twith Booknote(None, None, None):\n\t\treturn from_values(Booknote(None, None, None), values)\n\npackage com.wangxin.langshu.business.edu.vo.course.old.service.common.dto.user;\n\nimport java."}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tfrom_value_list = tuple(values)\n\treturn from_str(cls(*from_value_list))\n\n// Code generated by smithy-go-codegen DO NOT EDIT.\n\npackage iam\n\nimport (\n\t\"context\"\n\tawsmiddleware \"github.com/aws/aws-sdk-go-v2/aws/middleware\"\n\t\"github.com/aws/aws-sdk-go-v2/aws/signer/v4\"\n\t\"github.com/aws/aws-sdk-go-v2/service/iam/types\""}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tif not isinstance(values, Iterator):\n\t\traise TypeError, (\n\t\t\t\"Cannot construct from list of (encoded) strings with \"\n\t\t\t\"non-string values (bookmarks) from raw values.\"\n\t\t)\n\tvalues = iter(values)\n\tfor bookmark in values:\n\t\t# bookmarks = [ bookmark ]\n\t\tif not isinstance(bookmark, str):\n\t\t\traise TypeError, (\n\t\t\t\t\"Cannot construct from list of raw bookmark strings \"\n\t\t\t\t\"with non-string values (bookmarks) from raw values.\"\n\t\t"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\treturn C.Bookmark_from_raw_values(\n\t\tcls, values\n\t)\n\n\n# -- Python convenience methods -----------------------------------------------\n\ndef is_url(value):\n\treturn hasattr(value, 'get_url')\n\n\ndef get_title(value):\n\treturn cast(str, value.get_url())\n\n\ndef get_description(value):\n\tif (value.id == 0) or (value.get_url_id() == 0):\n\t\treturn value.get_name()\n\telse:\n\t\treturn 'Note:'"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\ttry:\n\t\tvalue = list(value for value in values.split(\",\"))\n\texcept ValueError:\n\t\traise ValueError(\"invalid value string format\")\n\t# make sure there are enough values to create the list\n\tif len(value) > 1000:\n\t\traise ValueError('cannot create raw value')\n\tvalues = (str(value),) + list(value[1:])\n\t# create the Bookmark object\n\treturn cls._from_values(values)\n\n#---------------------------------------------------------------------------\n\ndef __new__(cls, from_values):\n\t'''\n\tCreate a"}]}
{"task_id": "62b87b199a0c4fa8b80b354c", "prompt": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif not (isinstance(seq, list) and len(seq) == 2):\n\t\tseq = list(seq)\n\n\tif bufsize is None or len(seq) == 0:\n\t\treturn seq, ('*', )\n\n\tpos = len(seq)\n\tseq_type = seq[pos-2]\n\tif seq_type!='s':\n\t\tassert seq_type in ('s','s*', 'd', 'f', 'r', 'o')\n\n\tseq_len = len(seq) // bufsize\n\treturn seq[:pos-"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tassert bufsize is None or len(seq) == bufsize         # check\n\tdata = []\n\tfor i in xrange(0, len(seq)):\n\t\tdata.append(seq)\n\t# make sure it is a sequence (sequence type)?\n\tif isinstance(seq, _tuple_types):\n\t\tseq = seq.__iter__()\n\tassert isinstance(seq, tuple)\n\tdata = list(set(data))\n\tif not all((i < len(seq) for i in xrange(0, len(seq)) ):):"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tt = []\n\ti = 0\n\tfor i in range(0, len(seq), bufsize):\n\t\tt.append(seq[i][0])\n\t\tif bufsize:\n\t\t\tt[i] = seq[i][bufsize:]\n\t\telse:\n\t\t\tt.append(seq[i])\n\tif len(t) > 1:\n\t\traise TypeError('Invalid sequence with type %s' % (type(t[1])))\n\treturn t[0], t[1]\n\n\ndef _create_seq(seq, dtype):\n"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tfrom types import _basestring\n\n\tif bufsize is None:\n\t\tbufsize = -1\n\n\t# Make sure the sequence exists\n\tif not hasattr(_basestring, \"new\") and issubclass(seq.type, (type, unicode)):\n\t\t_new_seq = _basestring.new('u')\n\t\tseq = _new_seq(*seq)\n\n\t# Create a new one\n\tseq = _basestring.as_new(_basestring.seq, seq, bufsize, maxlen=maxlen)\n\n\tif type(seq)!= seq.type"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif buf not in _types_no_buffering:\n\t\tbuf = _buffer(buf)\n\t\t_base = int(_base)\n\t\tif bufsize not in _types_no_buffering:\n\t\t\tbufsize = _types_no_buffering[bufsize]\n\t\t\tbufsize = len(buf + bufsize)\n\t\tbufsize = int(_bufsize_with_type(bufsize))\n\t\tbufsize = min(bufsize, len(buf))\n\t\tbuf = buf[:bufsize]\n\t\tbuf = _type0_from_sequence(buf[:-bufsize])\n"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif type(seq) is tuple:\n\t\treturn _get_seq_with_type(seq[0], bufsize)\n\telif bufsize is not None:\n\t\treturn _get_seq_with_type(seq, bufsize)\n\telse:\n\t\tif len(seq) == 1:\n\t\t\treturn seq\n\t\telse:\n\t\t\treturn _get_seq_with_type(seq, int(max(len(seq), 2)))\n\ndef _get_seq_to_value(seq, value, **kw):\n\tassert isinstance(value,"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif bufsize is None:\n\t\tbufsize = len(seq)+1\n\tif isclass(seq):\n\t\treturn seq, sequence.TYPE_TUPLE\n\telse:\n\t\treturn sequence.fromBuffer(seq[:bufsize])\n\nclass _Iterator:\n\tdef __init__(self):\n\t\tself.f = None\n\t\tself.d = None\n\t\tself.w = None\n\t\tself.e = None\n\tdef next(self):\n\t\tself.d = True\n\t\ttry:\n\t\t\tself.d = next(self."}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t# (sequence, type)\n\ttype_ = (seq)\n\t# if there are no type variables at all, then there's nothing to\n\t# compute seq: just return.\n\tif seq.ndim > 1:\n\t\treturn (seq, seq[0].shape)\n\t# if there's no type variables, or if there's no sequence...\n\telif type_ is None:\n\t\treturn (seq, seq[0].shape)\n\telif type_!= tuple():\n\t\treturn (seq[:-1], type_)\n\telse:\n\t\treturn ("}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif not isinstance(seq, str):\n\t\tseq ='seq<' + str(seq) + '>'\n\treturn (seq, TYPES[seq])\n\n\nclass _Sequence:\n\t\"\"\"\n\tA sequence of functions.\n\tObjects are created from a list of strings.\n\tThe sequence itself is initialized to sequence of 'None'\n\t\"\"\"\n\t_TYPES = {}\n\n\tdef __init__(self):\n\t\tself.next = None\n\t\tself.type = None\n\t\tself.add = None\n\n\tdef _as_sequence(self"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\td = _getbytes()\n\tif bufsize is not None:\n\t\tbufsize = _convert_to_int(bufsize)\n\trlen = len(d), len(seq)\n\tif rlen!= bufsize:\n\t\td = _truncate(d, rlen)\n\tfor i in range(1, rlen):\n\t\td = _ensure_binary(_bytes_to_int(d, -1, True))\n\t\td = _ensure_binary(d)\n\tif seq is not None:\n\t\td = _ensure_binary(d)\n\t\t"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3581", "prompt": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif \"scale\" not in self.__dict__:\n\t\t\traise AttributeError(\"A histogram *may* have a non-zero scale.\")\n\t\tself.__dict__[\"scale\"] = zero(self.__dict__[\"scale\"]) * other\n\telif isinstance(other, Int):\n\t\tself.__dict__[\"scale\"] = scale_nonint(self.__dict__[\"scale\"], other._float())\n\telse:\n\t\traise ValueError('only ints are currently supported')\n\t\nclass Sum(object):\n\tdef __init__(self, value):\n\t\tself"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tother = self\n\tif not recompute:\n\t\treturn self.scale(other=other)\n\tm = self.median()\n\ts = self.sum() * self.sample_count / self.sample_count + \\\n\t\tself.sum() * other.get_mean() * self.get_mean()\n\tif m == 0.0:\n\t\treturn s\n\treturn ((s / m) ** self.unit ) * other.get_mean() * (0.5 * m) / ((s + m"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif not other:\n\t\t# Return other without setting scale from a previous run\n\t\treturn self.copy().fillna(0).values\n\t\t\t\n\tif isinstance(other, self.__class__) and other._scale is None:\n\t\t# self is of type self.\n\t\treturn self._scale(self, other, recompute)\n\t\t\n\tif isinstance(other, self.__class__) and not isinstance(other._scale, self._scale):\n\t\t# If we are re-shifting using another *other* from previous run,\n\t\t# then redefine its"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\t# check if the hashmap is already being used\n\t\tif self[0]:\n\t\t\tscale = 0.0\n\t\t\tfor i in range(1,len(self)):\n\t\t\t\tif hasattr(self[i],'scale'):\n\t\t\t\t\tscale = self[i].scale\n\t\t\t\tif hasattr(other,'scale'):\n\t\t\t\t\tscale *= other.scale\n\t\t\tscale = 0.0 if self._use_math else 1.0\n\t\telse:\n\t\t\tscale = 1.0\n\t\treturn scale\n\t\n"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\timport numpy\n\tif recompute:\n\t\tif not isinstance(other, (float, int)):\n\t\t\traise TypeError(\"scale must be a float or a int\")\n\t\tif abs(other) <= 0:\n\t\t\traise ValueError(\"cannot scale zero %r\" % other)\n\n\tM = self.hist\n\tself.hist = self.hist.copy()\n\tself.hist[~self.hist == 0] = other\n\tN = self.hist.shape[0]\n\tw = N\n\tfor i in range(1, N):\n\t"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif recompute and other and self is other:\n\t\tself[:] = self.scale\n\tself._scale = self.scale\n\tif self.is_interval:\n\t\treturn\n\t\n\tif other is not None:\n\t\tif self.is_interval:\n\t\t\tself.xrange = self.x.fill_interval(other.xrange)\n\t\t\tself.yrange = self.y.fill_interval(other.yrange, interval=1)\n\t\telif isinstance(other, Histogram):\n\t\t\tself.xrange = other.xrange"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif (other is not None and type(other) == int and\n\t    recompute and other.type!= float):\n\t\traise NotImplementedError(\"Type of rescale does not exist\")\n\tif (other is None):\n\t\treturn self\n\tif self.values.dtype == other.dtype:\n\t\t# Make a linear interpolation of values...\n\t\tif issubclass(self.dtype.type, np.integer):\n\t\t\ts = self.__class__(self.asum)\n\t\t\ts.values = self.values\n\t\telse:\n\t\t\ts"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tif not (isinstance(self, histogram.hist_dict) and self.size <= 0):\n\t\t\traise ValueError('histogram has zero scale (histogram.hist_dict)')\n\t\tself.scaled_by = 1.0\n\telse:\n\t\traise ValueError('histogram has non-zero scale (histogram.scales[i])')\n\t\n\t# Compute scale = histogram.max if not previously computed\n\tif not self.has_scaled and not other.has_scaled:\n\t\tc = 0.0\n\t\t# scale.scale("}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is not None:\n\t\tif'scale' not in other:\n\t\t\traise ValueError('{0!r} object is not an instance of '\n\t\t\t\t'thecale'.format(other))\n\tassert isinstance(other, np.ndarray), (\"the provided histogram is not an array.\")\n\t\n\tif isinstance(other, np.ndarray):\n\t\tif'scale' not in self:\n\t\t\traise ValueError('object is not an instance of scale.')\n\t\tscaled = float(self)/float(other)\n\telif type(self) is float:\n\t\t"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tok = True\n\tif recompute == True:\n\t\t# Call the appropriate update function to compute rescale\n\t\trecalculate(self)\n\t\t\n\t\tif'scale' in self._scale:\n\t\t\tother = self._scale['scale'](other, recompute)\n\t\t\n\tfor scale in self._scale:\n\t\tif scale is None:\n\t\t\tcontinue\n\t\tif isinstance(self, scale):\n\t\t\tscale.update(other)\n\t\telse:\n\t\t\tscale *= (1/np.abs(self))\n\t\t\n\t# update scale if"}]}
{"task_id": "62b87b519a0c4fa8b80b3583", "prompt": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\tif 1 not in self:\n\t\t\treturn default_scale(self.rows)\n\telse:\n\t\tif not isinstance(other, Graph):\n\t\t\traise AttributeError(\n\t\t\t\t\"Can only rescale Graph to (w or h)\"+str(type(other))*\" values when they don't contain \"\n\t\t\t\t\"multiples of the same type of values.\"\n\t\t\t)\n\t\tif not set(other.rows) < set(self.rows):\n\t\t\traise ValueError(\"can't rescale to (w, h)"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\tother = gdef.scale()\n\treturn self._rescale(other)\n\n\ngraph_class = graph\n\n//===----------------------------------------------------------------------===//\n//\n//                     The LLVM Compiler Infrastructure\n//\n// This file is dual licensed under the MIT and the University of Illinois Open\n// Source Licenses. See LICENSE.txt for details.\n//\n//===----------------------------------------------------------------------===//\n//\n// Copyright 2009, 2016, Intel Corporation\n//\n// Licensed under the Apache License, Version 2"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t# TODO: is this the best way to rescale?\n\twith_error_if(self, other, \"Scale: \",\n\t\t\tlambda: self, other\n\t):\n\t\tif isinstance(other, Number):\n\t\t\trescale_to_int(self, other, 'y', 'x', 'xy')\n\t\telif isinstance(other, Number):\n\t\t\tif isinstance(other, Decimal):\n\t\t\t\trescale_to_int(self, other, 'x', 'y')\n\t\t\tif isinstance(other, Decimal):\n\t\t\t\trescale_"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif isinstance(other, (int, long)):\n\t\ttry:\n\t\t\treturn float(other)\n\t\texcept ValueError:\n\t\t\treturn other # Assume float()\n\n\tdef __rdiv__(self, other):\n\t\t\"\"\"\n\t\tCalculate a rescale to the desired scale.\n\t\t\"\"\"\n\t\tscale = 1 - self.get_scale(other)\n\t\tnew_max = max(max(self.data.coords(1,2),\"\"), abs(scale) + abs(other))\n\t\treturn new_max/(self.data."}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tfrom numpy.polynomial.polynomial_random_base import is_integer\n\tif other is None:\n\t\tif isinstance(self.scale, (int,) + list):\n\t\t\treturn self.scale\n\t\telse:\n\t\t\treturn 0.5 * self.scale\n\telse:\n\t\tif isinstance(other, (int,) + list):\n\t\t\treturn other / self.scale\n\t\telse:\n\t\t\treturn -0.5 * other\n\t\n\tdef __init__(self, scale=None, *args):\n\t\tself.scale = scale\n\t"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import matplotlib as mpl                # <<<<<<<<<<<<<<\n\tpass#IMPORTIMPORTIMPORT import matplotlib.pyplot as plt         # <<<<<<<<<<<<<<\n\tpass#IMPORTIMPORTIMPORT import matplotlib.patches as mpatches # <<<<<<<<<<<<<<\n\tpass#IMPORTIMPORTIMPORT import matplotlib.tri as mtri # <<<<<<<<<<<<<<\n\tpass#IMPORTIMPORTIMPORT import matplotlib.ticker as mtick # <<<<<<<<<<<<<<\n\tpass#"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\tif self.__scale_arg:\n\t\t\treturn self.transform_coordinates(*self.__scale_arg)\n\t\telse:\n\t\t\traise ValueError('scale() requires scale() to be specified.')\n\tif isinstance(other, MatrixNode):\n\t\treturn other.scale()\n\tif other.isclass(self.__class__):\n\t\tfrom. import _graphclasses\n\t\tfor c in self.__class__.converters:\n\t\t\tif isinstance(c, basestring):\n\t\t\t\tif c.isclass(self.__class__):"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\tif self.hasField('name'):\n\t\t\tname = self['name']\n\t\t\totherName = other[self.infoKey('name')]\n\t\telse:\n\t\t\totherName = other\n\t\tother = otherName if isinstance(otherName, str) else otherName.name\n\telif isinstance(other, str):\n\t\tother = otherName.name\n\t\tother = None\n\telse:\n\t\tother = other.name\n\t\tscale = float(self['scale'])\n\n\tif other is None:\n"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif self.has_coord():\n\t\t# The coordinate is not rescaled for a scale of 1.\n\t\tif other is None:\n\t\t\treturn self.coord_y * self.scale\n\t\telse:\n\t\t\tif other < 0:\n\t\t\t\treturn other\n\t\t\telse:\n\t\t\t\tother = 0\n\t\t\tif self.__class__.coord.__is_complex__:\n\t\t\t\t# Non-finite imaginary part \n\t\t\t\treturn (self.coord_y * self.scale + self.coord_x * other) / (self.coord_y *"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is not None and len(other.lines) > 0:\n\t\tscale = self.get_scale(other)\n\t\tassert scale >= 0 and scale < 1\n\t\telse:\n\t\t\tscale = 1000\n\treturn scale\n\n\ndef get_line_count(line_str):\n\t\"\"\"\n\tGathers the number of coordinate line segments in the line string.\n\t\n\tLine segments are defined by the two arguments of linestrings :math:`l1,l2`\n\t and lines.length() - 1.\n\t\n\tline_str is an"}]}
{"task_id": "62b87b869a0c4fa8b80b35e1", "prompt": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t# Get histogram's field names\n\tresult = hist._field_info(\n\t\tget_coordinate=make_value,\n\t\tname=get_coordinate + \"x\",\n\t\taxis=0,\n\t)\n\t\n\t# If any point in the histogram, make a graph and convert it...\n\tresult_graph = result.convert_as(graph=hist.to_graph())\n\tif getattr(result_graph, \"axes\", None)!= hist.axes:\n\t\traise RuntimeError(\"`axes` is not equal to the histogram's axes.\" )\n\t\n\t"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tif len(hist):\n\t\tif len(hist[0].histogram)!= len(hist[1].histogram):\n\t\t\traise ValueError(\"histograms have different dimensions\")\n\t\t\n\t\tfor h, (h1, h2) in zip(hist, hist[1:]):\n\t\t\tif h[1] < h[0] and not h2[1] < h1[1]:\n\t\t\t\traise ValueError(\"histogram has unique values\")\n\t\t\t\t# return from above\n\t\t\t\t\n\t\thist_copy, h_len = hist\n\t\thist_copy[0]."}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tif not isinstance(hist, list):\n\t\thist = [hist]\n\thist_dim = len(hist)\n\thist_ctx = [hist_dim-1] if not hist else [hist_dim]\n\t\n\tif isinstance(make_value, list):\n\t\tfor i in range(1, hist_ctx.count(hist_sort=True)):\n\t\t\thist_sort = hist_sort | (1<<i)\n\t\thist_sort |= 1 << hist_dim\n\telse:\n\t\thist_sort = hist_dim\n\tif hist"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\timport warnings\n\t\n\t# XXX: to be implemented/replaced\n\t\n\traise ValueError(\"hist_to_graph: invalid argument\")\n\t\n\ndef hist_to_value(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to an :class:`ndarray`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\timport histpy\n\t# Get the value of histogram. No point in keeping history values.\n\tif get_coordinate == \"left\":\n\t\treturn hist_graph_to_graph(hist, make_value, get_coordinate, \"left\")\n\tif get_coordinate == \"right\":\n\t\treturn hist_graph_to_graph(hist, make_value, get_coordinate, \"right\")\n\t# If we have a string argument, that means we want to keep value information in a histogram.\n\t# This may be useful to avoid the creation of hist_graph_to"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tif get_coordinate is None:\n\t\treturn hist.to_graph()\n\telif hist.__class__.__name__!= \"Histogram\":\n\t\traise ValueError(\"Please use Histogram() instead\")\n\t\t\ndef hist_to_image(hist,\n\t\t\tfilename=\"histogram_data.png\", fillfmt=None,\n\t\t\tncols=None, rowlim=None, rowmin=None,\n\t\t\trowsource=None, colsource=None, titlesize=32,\n\t\t\trowsize=32, xpad=21, ypad=21):\n\t\"\"\""}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tdef hist_to_graph(hist, make_value=None, get_coordinate=\"left\"):\n\t\tfrom numpy import array\n\t\tgraph = numpy.empty(len(hist), dtype='f8')\n\t\tfor i, tuple in enumerate(hist):\n\t\t\tnode = graph.new_empty(tuple[0], tuple[1] + 1)\n\t\t\tgraph[i, 0], _ = node\n\t\t\tgraph[i, 1:] = get_coordinate(tuple[0], node._get_value())\n\t\t\tgraph[i, :] = graph[:, :,"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\treturn hist.hist_to_graph(make_value, get_coordinate=get_coordinate)\n\n\ndef hist_to_histogram(hist, axis=None, scale=True):\n\tr\"\"\"Convert a :class:`.histogram` to a :class:`.hist`.\n\t\n\t*hist* to :class:`histogram.Hist` is a :class:`.histogram`.\n\t\n\t*axis* specifies the axis on which histograms will be created (default:\n\t\t``\"left\"`). This is typically used after axes which are either\n\t\t\"bins\" or \""}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\treturn graph.add(\n\t\tmake_value(\n\t\t\tfield_names=field_names, \n\t\t\tscale=scale, \n\t\t\tcontext=make_value(get_coordinate(hist), hist),\n\t\t\tnames=make_value(hists),\n\t\t\tbins=make_value(list(hists)),\n\t\t\tvalues=hists\n\t\t)\n\t)\n\n\ndef get_n_bins(n, field_names=None):\n\t\"\"\"Get the number of bins for the n-th sample.\n\t\n\tn is an int indicating"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\treturn _value_to_dict(hist, make_value=make_value)\n\nclass _Value(_Value):\n\t\"\"\"\n\t*_Value:\n\t\tA default construct which creates a graph from a\n\t\thistogram or function. This is useful for creating a\n\t\tgraph to an input. A graph will have exactly the same\n\t\ttype as a ``(`` argument, where the latter represents\n\t\tthe same ``_Value`` object or a scalar in the context\n\t\tthat would be returned by ``Value.values()``.\n\t\t\n\t\tThere is a \""}]}
{"task_id": "62b8b4baeb7e40a82d2d1136", "prompt": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t# Check of validation.\n\t# Check of errors.\n\t\n\tif hasattr(iface, \"providedBy\"):\n\t\tif any(c in iface.providedBy for c in candidate):\n\t\t\tif tentative is True:\n\t\t\t\t# Something wrong with the supplied interface.\n\t\t\t\t# We report this to zope.error.Error and make it a\n\t\t\t\t# fatal error.  This might have no way to find out\n\t\t\t\t# which type of interfaces we were looking at.\n\t\t\t\traise tentative\n\t\t\telse:\n\t\t\t\t# This"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t# check each class that we care about against\n\tcandidates = []\n\tm = type(candidate)\n\tfor cls in candidate.providers:\n\t\tm2 = type(cls)\n\t\tif m == TENTATIVE and m2 not in cls.providedBy:\n\t\t\tcandidates.append(cls)\n\tif candidates: \n\t\tif len(candidates) > 1:\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t(\"The following candidates all satisfy the interface \"\n\t\t\t\t \"interface requirements (%s) are only compatible for \"\n\t\t\t\t \"its Provider and"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t# For each method, call it!\n\tpass#IMPORTIMPORTIMPORT from.interfaces._interface import verify\n\t# Call verify\n\treturn verify(iface, candidate, tentative, vtype)\n\n\ndef load_configuration(filename, interface=None, **kwargs):\n\t\"\"\"\n\t\n\tImport configuration (with possible duplicates)\n\t\n\tparameters\n\t---------\n\tfilename : string\n\t\tThe name of the file being loaded\n\t\n\tinterface : string\n\t\tThe interface to check\n\t\n\tnote\n\t\tThis class should not be used directly. Calls to"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t# Make sure there is at least one validator error.\n\tif all(x is not None and \"invalid \" in repr(x) for x in candidate):\n\t\tmsg = _('Only one ``'invalid'`` validator error will be reported: %s.' % str(candidate))\n\t\traise zope.exceptions.InvalidInterface(msg)\n\t\t\n\tif len(candidate) > 0:\n\t\tm = _verifyMethod.match(candidate)\n\t\tif m is not None:\n\t\t\tmethod = m.group(1)\n\t\t\tiface = m.group"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tcandidate = candidate[0]\n\n\tif not candidate.providedBy or not candidate.provides:\n\t\t# If the candidate was configured to have a non-NULL, suppliedBy or\n\t\t# provides, we've lost all of them. However, the error was sent, so it's time to report\n\t\t# the error for that type of error and for any associated attributes it has. Additionally, \n\t\t# only those attributes are reported.\n\t\t#\n\t\t# This ensures we are reporting the error for all types of values\n\t\t# that the candidate couldn't provide ("}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t# FIXME: should be checking against candidate's class.\n\tif candidate is not None:\n\t\tif type(candidate) is type(iface):\n\t\t\t# This check also covers the non-matching case, and is\n\t\t\t# unnecessary\n\t\t\treturn True\n\t\t\t\n\t\tif vtype is None:\n\t\t\treturn False\n\t\t\t\n\t\tif candidate.providedBy!= iface.providedBy:\n\t\t\t# This check is needed to work from deprecated interfaces\n\t\t\treturn False\n\t\t\t\n\t\treturn iface.provides(candidate.class, candidate, skipV4=True)\n"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tdef _verify(candidate):\n\t\tif candidate in {InvalidInterface, InvalidInterface, ReqlInterface}:\n\t\t\treturn False\n\t\tif candidate not in _validateList:\n\t\t\t# check that everything that doesn't cause errors is adequately validated\n\t\t\tif candidate in {InvalidFunction, InvalidFunction}:\n\t\t\t\t# if both the candidate & the function are valid and the types match, the candidate is an error\n\t\t\t\treturn False\n\t\t\n\t\t\t# otherwise, just the candidate\n\t\treturn candidate\n\t\n\t# Making sure the candidate's class has the least"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\treturn _validate_interfaces(iface._name, candidate.providedBy, candidate)\n\n\ndef _provide_iface(iface):\n\t\"\"\"\n\tCheck if the candidate supplies the provided interface of something.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless **False**),\n\t  or that ``IFACE_REQUIREMENT`` and ``IFACE_REQUIRE_INSTANCE``\n\t\n\t- Making sure the candidate does not declare anything that\n\t  provides the interface.\n\t\n\t:type providedBy: `"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif vtype is None:\n\t\tvtype = candidate.providedBy.get_class()\n\n\tinterfaces = candidate.get_interfaces(tentative)\n\n\tfor intf in interfaces:\n\n\t\tif intf.providedBy.provides(iface):\n\t\t\traise NotImplementedError()\n\n\t\tif intf.providedBy.provides(iface, vtype):\n\t\t\tif intf.providedBy.provides(iface, vtype):\n\t\t\t# If the interface declares the interface to the provided by.\n\t\t\t# We expect the interface to expose any private methods.\n\t\t\t"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tiface = candidate._iface\n\trequire_cls = candidate._require_cls\n\tassert candidate.providedBy.signature() is signature.as_object(candidate.providedBy)\n\tif candidate.__class__ is not iface and candidate.__module__ == iface.__module__:\n\t\traise RuntimeError('Incompatible interface')\n\tif candidate.__class__!= iface:\n\t\treturn True\n\t# First validate the name, it only checks against *ifaceName*\n\t_name = candidate.__class__._name\n\tif candidate._module_entry:\n\t\tif name_matches(_name,"}]}
{"task_id": "62b8b4baeb7e40a82d2d1137", "prompt": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t# Check if the candidate is an instance of any of the interfaces.\n\t# This avoids problems with the first argument.\n\tif not isinstance(candidate, tentative):\n\t\tiface = candidate.providedBy()\n\t\tif iface is not None:\n\t\t\tcandidate.implementer = iface\n\t\t\treturn (candidate.provides is not False) or (candidate.interfaces is None and hasattr(iface.providedBy, '__name__'))\n\t# If the candidate is a stub class, then this may be an interface that\n\t# provides the stub class. In that case, the stub"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tfor method in candidate:\n\t\ttry:\n\t\t\tiface.providedBy(method)\n\t\texcept zope.interface.Invalid:\n\t\t\tif tentative:\n\t\t\t\treturn False\n\t\t\t\tpass\n\t\texcept zope.interface.NoInterfaceProvidedError as err:\n\t\t\tmessage = \"Method %s did not resolve to interface, but \"\n\t\t\t\t\"it was given by: %s\" % (method, err)\n\t\t\twarning(message)\n\t\t\tpass\n\treturn True\n\n\n#\n# A wrapper around the standard \""}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\ttry:\n\t\tiface = candidate.get(\"class\")._class\n\texcept AttributeError:\n\t\tiface = candidate.get(\"class\")\n\tif not iface.providedBy:\n\t\treturn True\n\treturn iface.provides(tentative)\n\t\n#===============================================================================\n#\n# Methods to support interfaces\n#\n\ndef interfaces(self.__class__):\n\t\"\"\"\n\tThe interfaces of an interface package.\n\t\"\"\"\n\tinterfaces = []\n\tif self.__class__ in (\n\t\tzope.interface.provider,\n\t\tzope.interface"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif candidate.provider:\n\t\tif tentative:\n\t\t\traise zope.interface.Invalid(('Cannot specify the provider of an object that is actually an interface that does not implement the interface. '\n\t\t\t\t 'The provider argument to the interface method on ``Candidate`` instance is the provider name (string) or an object of the provider type (instance of type ``zope.interface.class_provider``, '\n\t\t\t'not ``zope.interface.interface_provider``.').format(name=candidate) if candidate.type == 'Provider' else candidate.provider) or \\"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif not tentative:\n\t\ttry: # FIXME\n\t\t\tiface = candidate.getInterface(iface)\t\t\n\t\texcept zope.interface.Invalid:\n\t\t\traise\n\t\texcept zope.interface.NoSuchInterfaceException:\n\t\t\t# FIXME\n\t\t# This shouldn't be considered an exception, since all possible\n\t\t# error messages will be reported when the candidate tries to\n\t\t# find the class to which it is associated.\n\t    method_name = candidate.getProviderMethodName(iface)\n\t\tif method_name in candidate."}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t# verify interface that provides the desired interface\n\tiface = iface or candidate.__interface__\n\tif iface is not None and iface.providedBy is not None and iface.providedBy.provides(candidate) is not None and \\\n\t    iface.providedBy == candidate.__Interface__:\n\t\tret = True\n\telse:\n\t\tret = False\n\n\treturn ret\n\n# Copyright 2020 Google LLC (see AUTHORS.txt)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\t_interface_isvalid = False\n\tmethods_by_name = {}\n\tmethods_by_type = {}\n\tclasses_by_name = {}\n\t\n\tcandidates_by_name = {}\n\t\n\tfor o in candidate.__slots__:\n\t\tif not o in ('_required', '_providedBy', '_providedByType', '_providedType'):\n\t\t\t# print \"Skipping\", o\n\t\t\tcontinue\n\t\n\t\tif hasattr(candidate, o):\n\t\t\tmethod = getattr(candidate, o)\n\t\t\t\t\t\t\t\t\n\t\t\tisimplemented = _verifyMethod(method,"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tdef checkAccess(iface):\n\t\tif iface and notifaceisinstance(iface, interfaces.Interface):\n\t\t\traise ValueError(\"interface %s must implement 'interface' class\" % iface)\n\t\tassert (notifaceisinstance(candidate, interfaces.Interface) or\n\t\t        iface.providedBy == candidate.providedBy\n\t\t        or candidate.provides == iface.provides)\n\n\t\tif iface and iface.providedBy!= candidate.providedBy:\n\t\t\traise ValueError(\"interface %r (%s) does not have providedBy as first argument"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\ttry:\n\t\tif tentative:\n\t\t\t# Check that the candidate really declares itself\n\t\t\tdef _candidate_is_instance_declaration(candidate):\n\t\t\t\treturn candidate.providedBy is None and candidate.method is not None\n\t\t\t\n\t\t\tif candidate.providedBy is not None and _candidate_is_instance_declaration(candidate):\n\t\t\t\t# This error will be caught by zope-debug.py\n\t\t\t\tpass\n\n\t\t\texcept SyntaxError:\n\t\t\t\treturn False\n\t\t\t\n\t\terror_message = \"[{}] could not make {} a Candidate\"."}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tc_name = candidate.getQualifiedClassName()\n\t# If the current namespace/name does not specify the namespace\n\t# (and we're in a namespace context) then we should be trying\n\t# to verify that one is correct.\n\tns_name = candidate.getObjectName()\n\t# If the current namespace/name does not specify the namespace\n\t# (and we're in the current namespace context) then we should be\n\t# trying to verify that one is correct.\n\tns_namespace = candidate.getNamespaceName()\n\t\n\t# Checking if any of the following are true:"}]}
{"task_id": "62b8b4c1eb7e40a82d2d1139", "prompt": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif hasattr(iface, \"get_name\"):\n\t\tiface = iface.get_name()\n\tclass Candidate:\n\t\tdef get_name(self) -> ClassName:\n\t\t\treturn candidate\n\n\t@Assisted\n\tclass _Candidate(*Candidate):\n\t\t@classmethod\n\t\tdef get_name(cls: Type[Self], **kwargs):\n\t\t\tif \"name\" in kwargs:\n\t\t\t\treturn kwargs[\"name\"]\n\treturn _Candidate\n\n@assisted.gen.interfaces.Class\nclass TestInterface(object):\n\tdef method("}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tdef factory(iface, candidate, is_valid):\n\t\tif validator is None:\n\t\t\tresult = is_valid(iface)\n\t\t\treturn result\n\t\treturn validator(iface)(candidate, is_valid)\n\treturn factory\n\ndef verifyAddress(provider, source_address, target_address, is_valid):\n\tdef factory(provider, source_address, target_address):\n\t\tif not is_valid(target_address):\n\t\t\treturn True\n\t\treturn True\n\treturn factory(validator, source_address, target_address)\n\n"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif iface is not None:\n\t\ttentative = False\n\treturn _verifyMethod(iface, candidate)\n\n\n_def extractInterface(iface=None):\n\t\"\"\"\n\tReturns the interface which is currently being implemented in this module.\n\t\"\"\"\n\treturn _ifaceForInfo[iface]\n\n\n_def extractInterfaceType(iface=None):\n\t\"\"\"\n\tReturns the type of the interface which is currently being implemented in this\n\tmodule.\n\t\"\"\"\n\tiface = None if iface else _ifaceForInfo[iface]\n\n\tif iface is None:\n"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif candidate!= iface and 'name' not in candidate and'method' not in candidate:\n\t\tiface.__name__ = candidate[\"name\"]\n\t\tiface.__func__ = candidate[\"method\"]\n\n\treturn iface\n\n\nclass Candidate(object):\n\t\"\"\"\n\tThis class implements a candidate, it is used to handle\n\ta method with no arguments, like 'getattr', but contains the\n\tclass name (i.e. the name of the interface, not the actual\n\tmethod), but the actual argument to the method may be\n\tprovided explicitly.\n\t\"\"\"\n"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif not isinstance(iface, py.class_):\n\t\traise ValueError('iface must be None or *cls.cls*')\n\n\t# Make sure that class is a subclass of candidate...\n\tif hasattr(candidate, '__class__') and candidate.__class__.__name__ == 'TentativeCls':\n\t\tif not isinstance(iface, candidate):\n\t\t\traise ValueError('Interface must have *cls.cls* with its value %s' %\n\t\t                     candidate.__class__.__name__)\n\n"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tiface, candidate = candidates.getBestCandidate(iface, candidate)\n\treturn verifyType(iface.ifaceClass, C.Type, len(iface.possibleMethods) == tentative), candidate\n\n\ndef verifyConstructor():\n\t\"\"\"\n\n\t>>> from tastypy.cpcompat import isConstructorMethod\n\t>>> with isConstructorMethod('class C(object): pass'):\n\t>>> from tastypy.cpcompat import verifyClass\n\t>>> verifyClass('C', False, verifyClassIsInstance = False)\n\tInvalid: class C is not a class\n\n\t>>>"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tfor tentativeType, candidateType in (\n\t\t(TAC, TAC3), (TAC11, TAC21), (TAC12, TAC31), (TAR, AAS),\n\t\t(TACT, AAS), (TAO, PAC), (TTZ, HAC),\n\t\t(TOA, TOC), (TTEI, TOM), (TTN, TON),\n\t):\n\t\tclass1, name1 = candidate[teltype].split(\".\")\n\t\tif name1 in iface.NAMES:\n\t\t\t"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\ttry:\n\t\tt_method = type(candidate).__find__(iface).__name__\n\t\tif tentative:\n\t\t\traise e = e\n\texcept AttributeError:\n\t\tt = e = e\n\n\tif t:\n\t\tif not e.args and not t.args:\n\t\t\te = e.msg\n\n\treturn e\n\n##\n#\n# This module contains all the configuration information\n# for the `Config` class. The API, however, also acts as a\n# helper to allow a subclass to require the whole class configuration\n# data"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from System import version\n\tpkgVersion = version.System_Vista\n\n\tif version.Build_number <= pkgVersion:\n\t\t#  Check for the version using the lib (2.1.2)\n\t\tpkgVersion = version.Library_Version\n\tif pkgVersion < version.MinNasty_Release_Version:  # MinNasty_Release_Version\n\t\tpkgVersion = version.MinNasty_Release_Version\n\n\t\tclassInfo = _class_info[candidate]\n\t\tif version"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif candidate in iface:\n\t\tlog(1, \"%r already present! (iface %r already present)\" % (candidate.id, iface))\n\treturn candidate\n\ndef verifyInterface(interface, tentative=False, default=None, ifaceClass=None):\n\tif interface:\n\t\tresult = ifaceClass or interface\n\t\tfor ifaceClass in interface.interfaces_:\n\t\t\tif result == ifaceClass.id:\n\t\t\t\tresult=None\n\t\t\t\tbreak\n\t\telse:\n\t\t\tresult = None\n\t\tassert result == default or result not in iface"}]}
{"task_id": "62b8b559eb7e40a82d2d11f6", "prompt": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc is None:\n\t\texplicit_mc=bases if bases else []\n\treturn [t for t in __metaclass__ if not isinstance(t, type)]\n\n\n# Note: Don't forget to add __metaclass__ here\nclass Pymetaclass(Metaclass):\n\t\"\"\"\n\tA Metaclass in Python where methods have the same name. This class\n\tcan be instantiated in any PyClass if you want to support multiple\n\tof these classes in your code.\n\t\"\"\"\n\tdef __new__(metamodule, name, bases, namespace):\n\t"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tfound_bases = set()\n\tfound_explicit_mc = set()\n\tfor base in bases:\n\t\tmetaclass, explicit_mc = determineMetaclass0(base, explicit_mc)\n\t\tif metaclass is not None:\n\t\t\tfound_bases.add(base)\n\t\t\tfound_explicit_mc.add(explicit_mc)\n\treturn found_bases, found_explicit_mc\n\nclass Base:\n\tdef __init__(self, name, bases, cls_dict):\n\t\tself.name = name\n\t\tself.bases ="}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc in globals():\n\t\tif explicit_mc:\n\t\t\tfor base in bases:\n\t\t\t\tmetac = globals()\n\t\t\t\t# if we have a base, but we haven't already met a metaclass...\n\t\t\t\tif base is not None and not isinstance(base, type):\n\t\t\t\t\tcontinue\n\n\t\t\t\t#... and it is defined in the current metaclass...\n\t\t\t\tif metac.__class__:\n\t\t\t\t\tfor k in metac.__class__.__dict__:\n\t\t\t\t\t\tif k.startswith(\"__\") or k.startswith(\"_\") or k.startswith"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tfor base in bases:\n\t\tclass_ = None\n\t\tfor arg in bases:\n\t\t\tif isinstance(arg, str):\n\t\t\t\tif class_ is not None and method_name(arg) == '_metaclass_' \\\n\t\t\t\tand method_name(base) == '_metaclass_' \\\n\t\t\t\tand method_name(arg) in explicit_mc:\n\t\t\t\t\tclass_ = arg\n\t\t\t\telif class_ is not None and class_!= base and method_name(base) == '__metaclass__':\n\t\t\t\t\tclass_ = class_._fields and"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif implicit_or_explicit_metaclass is None:\n\t\timplicit_or_explicit_metaclass = implicit\n\tif implicit_or_explicit_metaclass:\n\t\timplicit_or_explicit_metaclass = implicit_or_explicit_metaclass\n\timplicit_or_explicit_metaclass = \\\n\t[x for x in implicit_or_explicit_metaclass if x.lower() in\n\t\t\t  [\\'class\\']]\n\tif explicit_or_explicit_metaclass is None:\n\t\texplicit = \\\n\t\tset([class_for_name."}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tdef determineMetaclass(meta, bases):\n\t\t\"\"\"Determine metaclass from base, a base itself or an explicit __metaclass__\"\"\"\n\t\tif meta is not None and bases is not None:\n\t\t\tif len(bases) == 1:\n\t\t\t\tmetaclass = bases[0].__metaclass__\n\t\t\t\tif metaclass is not None:\n\t\t\t\t\treturn metaclass()\n\t\t\telse:\n\t\t\t\tfor base in bases:\n\t\t\t\t\tmetaclass = determineMetaclass(base)\n\t\t\t\t\tif metaclass is not None:\n\t\t\t\t\t\treturn metaclass"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif bases and 'cls' in bases and not isinstance(bases[0], basecls):\n\t\t# Base classes match, see if they are __metaclass__ fields.\n\t\tbases.pop(1)\n\t\t# if not explicit_mc:\n\t\tbasedict = {}\n\t\tfor k,v in explicit_mc.items():\n\t\t\tif not isinstance(v, dict): \n\t\t\t\tcontinue\n\t\t\tfor k2,v2 in v.items():\n\t\t\t\tif k2 not in bases and v2!= bases[k2]:\n\t\t\t\t\tcontinue"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif not bases:\n\t\treturn None\n\tmetacols = [\n\t\tx for x in bases if x.isMetaclass() or x.isMetaclass(explicit_mc)\n\t]\n\tfor cls in metacols:\n\t\tresult = cls.__metaclass__()\n\t\treturn result\n\treturn None\n\nclass MetaData(defaultdict, metaclass=MetaData):\n\tdef __init__(self):\n\t\tmetaclass(self).keys().__all__.extend(\n\t\t\t[name[5:] for name in dir(self)+[(5"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t# import these code to avoid cyclic references\n\tdel bases[:]\n\tdel explicit_mc\n\n\tret = []\n\tfor base in bases:\n\t\tif base.__metaclass__ is None:\n\t\t\tret.append(base)\n\treturn ret\n\n\"use strict\";\n\nconst Talisman = require(\"talais-lite5\");\nconst Sandbox = require(\"talais-sandbox\");\nconst TALISMAN = new Talisman();\nconst path = require(\"path\");\n\nfunction isDevelopmentFile(filename) {\n  "}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t# if explicit is given, force it to be a metaclass instead of the\n\t# class itself\n\tif explicit is not None:\n\t\tbases = explicit\n\n\tmetaclass = None\n\tif bases:\n\t\tmetaclass = types.Metaclass(*bases)\n\n\treturn metaclass\n\ndef determineMetaclassOrModuleType(bases, explicit_mc=None, force_mc=False):\n\t\"\"\"\n\tDetermine metaclass from 1+ classes or modules and optional explicit\n\t__metaclass__\n\t\"\"\"\n\tif force_mc and not issubclass("}]}
{"task_id": "62b8d22a48ba5a41d1c3f47d", "prompt": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif key is None or not isinstance(key, string_types):\n\t\tif self._keys is None:\n\t\t\traise KeyError(key)\n\t\telse:\n\t\t\treturn self._keys[key.split(self._delimiter)[0]]\n\treturn self._keys.pop(key, default)\n\n\npop_backwards = pop(b\"pop_backwards\", b\"backwards\")\n\ndef pop_if(self, f, g):\n\t\"\"\"\n\tReturn a fresh, if-none-match result with the specified sequence.\n\tIt's a"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\tk = __marker[key]\n\texcept KeyError:\n\t\traise KeyError(__marker.get(%s, key))\n\tself.pop_val(k,default=default)\n\treturn k\n\t\ndef __getstate__(self):\n\t \"\"\"Recreate the __dict__ as the state.\"\"\"\n\t state = {}\n\t for k,v in self.__dict__.items():\n\t\tif hasattr(self, k):\n\t\t\tstate[k] = v\n\t return state\n#end __getstate__\n\n#begin __getstate\n"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn key, default\n\nusing System;\nusing System.Collections;\nusing System.Collections.Generic;\nusing System.ComponentModel;\nusing System.Linq;\nusing System.Reflection;\nusing System.Text;\nusing Vanara.Extensions;\n\nnamespace Vanara.Collections\n{\n\t/// <summary>Represents a delegate returning a set of elements in the order in which they arrived.</summary>\n\t/// <remarks>\n\t///    The default implementation of ISetDelegate"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif key in self:\n\t\tif self[key] is None:\n\t\t\treturn default # type: ignore\n\t\t\te = Error()\n\t\t\te.set_type(KeyNotFound)\n\t\t\traise e\n\t\treturn self[key]\n\telse:\n\t\treturn default\n\ndef popall(self):\n\t\"\"\"\n\tRemove all items from map.\n\t`popall` only performs the basic type checking; the returned items do not exist\n\tif the map is `None;`.\n\t\"\"\"\n\tif self.length == 0:\n\t"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t# print(\"pop pop\", key, self)\n\tif key in self:\n\t\trv = self[key]\n\t\tdel self[key]\n\t\treturn rv\n\telse:\n\t\treturn default\n\n\nclass deque(object):\n\t_dict = None\n\t_keys = deque.__dict__\n\t_dict = None\n\n\t@staticmethod\n\tdef __init__(data):\n\t\tglobal _classDict, _class__\n\t\tif __debug__:\n\t\t\t__debug__ = dict(type(data), __class__)\n\t\tif"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\treturn self.pop_with_default(key, default)\n\ndef contains(self, key, default=__marker):\n\t\"\"\"check if specified key exists in the dictionary.\n\t@param key: the key with which the condition is to be checked.\n\t@param default: an optional default value to be returned if\n\t                key is not found, otherwise KeyError is raised.\n\t\"\"\"\n\treturn super(Hashable, self).contains(key, default)\n\n def contains_key(self, key):\n "}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t# XXX: Need a way of changing the list iterator.\n\tif '.' in key:\n\t\treturn self.dict.pop(key)\n\telse:\n\t\treturn self._get(key, default, self)\n\ndef load(filename):\n\t\"\"\"Loads a file into a dictionary.\"\"\"\n\td = {}\n\tf = open(filename)\n\ttry:\n\t\tfor line in f:\n\t\t\td[line] = {}\n\t\t\tfor k in line.split():\n\t\t\t\tif not k.starts_with('_'): continue\n\t\t\t\ttry"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif key in self.map:\n\t\treturn self.map.pop(key, self[key])\n\treturn default\n\n#------------------------------------------------------------------------------\n# Set\n\nSet\n_default_value = {}\n\nsetattr(_set, \"value\", _default_value)\nsetattr(_set, \"keys\", _default_value.keys())\nsetattr(_set, \"_set_\", _default_value)\n\n_set_type = {}\n\n_set_type['value'] = 'T'\n_set_type['keys'] = collections.OrderedDict(_map_type_default_"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif not isinstance(key, str):\n\t\tkey = toobig(key)\n\tif key == _marker:\n\t\tif self.encoding:\n\t\t\treturn None\n\t\telse:\n\t\t\t# There was an encoding for v, but this was a marker\n\t\t\t# No, it has been recoded.  What else to do?\n\t\t\treturn self._encoding.decode(toobig(default))\n\t# key not found\n\tdef inner_pop(self):\n\t\ttry:\n\t\t\tvalue = self._encodepop(self._key"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tif key is not None and not isstring(key):\n\t\traise TypeError('key must be a string')\n\tself.__keybuf[str(key).replace('_','-')] = default\n\n\treturn self.__keybuf[str(key).replace('_','-')]\n\ndef set_popitem(self, *vars):\n\t\"\"\"\n\tSet pop item in a variable dictionary.\n\t\"\"\"\n\tfor name, value in vars.iteritems():\n\t\tsetattr(self, name, value)\n\treturn self\n\nclass Bubble:\n\t\"\"\"\n\tB"}]}
{"task_id": "62b8d23748ba5a41d1c3f497", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\traise NotImplementedError\n\t\"\"\"\n\tdef popitem(self):\n\t\treturn self._popitem()\n\t\"\"\"\n\tdef popall(self):\n\t\tres = []\n\t\tfor item in self.iterkeys():\n\t\t\tres.append(item)\n\t\treturn res\n\n# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  "}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tself = self.items.popitem()\n\ttry:\n\t\treturn popkey(self)\n\texcept KeyError:\n\t\treturn None\n\ndef range(start=None, stop=None, step=None):\n\tpass\n\ndef iterkeys(keys, callback=None):\n\tpass\n\ndef itervalues(values, callback=None):\n\tpass\n\ndef iteritems(items, callback=None):\n\tpass\n\ndef iterlists(lists, callback=None):\n\tpass\n\ndef iterkeys(key, callback=iterkeys):\n\tpass"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import platform\n\ttry:\n\t\tself.list = self.list[0:-1]\n\texcept IndexError:\n\t\tpass#IMPORTIMPORTIMPORT from errno import ECHILD\n\t\tself.list.pop()\n\tp = 0\n\tq = 0\n\tfor i in self.list:\n\t\tif i > p:\n\t\t\tif p > q and platform.mac_ver()[0] < 3:\n\t\t\t\tself.list.pop(i)\n\t\t\telif q > p and platform.mac"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tkey = self.key\n\tval = self.value\n\tself.key = self.key[self.len - len(self.end)-1]\n\tself.value = val[-1]\n\treturn (key, val)\n\n\ndef setitem(self, key, value, length=0):\n\t\"\"\"\n\t\tkey is `(a, b)` tuple.\n\t\tvalue is a `(a, b)` pair.\n\t\tlength is number of `(a, b)` pairs needed to get `(a, b)` pair.\n\t\"\"\"\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tself = self.copy()\n\tself[1] = self[1].popitem()\n\treturn self\n\n\n#----------------------------------------------------------------------\n# - _dict_get_item\n#\n# The function to return an item from a dictionary.\n\n# The list is returned, order is preserved.\n#\n# - key: the index for the new entry.\n#\n# - value: the value.\n#\n# - item[0]:\n#\n# - item[1]:\n\ndef _dict_get_item(self, key, value):\n\t\"\"\"\n\t\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tc = self.cmyk.stack.popitem()\n\treturn c if self.cmyk.stack else None\n\n\n\n\nfrom pypsy.component import Component, ComponentView, ComponentViewMixin\nimport psycopg2\nfrom psycopg2.extensions import (\n\tEXTENSIONPOINTID_BINARY,\n\tEXTENSIONPOINTID_TEXT,\n\tEXTENSIONPOINTID_DATETIME,\n\tEXTENSIONPOINTID_INT,\n\tEXTENSIONPOINTID_INT4,\n\t\n)\nfrom psycopg2.extras import intsubtype, intsubtype as _intsubtype\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t#print \"popitem(self) -> (%s, %s)\" % (self.key, self.value)\n\tif self._lowestValue is None:\n\t\tif not hasattr(self, 'key'):\n\t\t\traise TypeError(\"popitem(%s) -> (%s, %s)\" % (self, self.key, self.value))\n\t\tret = popfirst, 0, None\n\t\tif ret!= poplast:\n\t\t\tret = poplast, 1, self[self.key]\n\t\t\tpopfirst, poplast = ret\n\t\tcount ="}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tif self._stack.index(POP_ALL) < len(self._stack):\n\t\tself._stack.pop(POP_ALL)\n\treturn (self._stack.index(POP_ALL), self._stack.pop(POP_LOWEST))\n\n\t\nclass IndexBy(dict):\n\t\"\"\"\n\tdict index by name, where name is the key\n\t\"\"\"\n\tdef __getitem__(self, item):\n\t\t# Search if we have this item already\n\t\tfor key in self:\n\t\t\tif key == item:\n\t\t\t\treturn self[key"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tself = self.__class__.__name__\n\treturn self._popitem(self())\n\n\t\n#----#\ndef _popitem(self, value):\n\t\"\"\"\n\tMake a copy of the hash table, return the hash table after modifying\n\tthe argument.\n\n\tIf no reference to the hash table yet exists, return None\n\t\n\tNOTE: If not self.empty, _popitem() returns the element\n\twhich was created by _create(), _empty() on value would return True (in\n\twhich case no reference will exist.)\n\n\t@param self the key"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tkey, value = self.popItem(self.key2int(self.key))\n\treturn self.key2int(key), self.value2int(value)\n\n\n\t\nclass BaseCache(object):\n\tdef __init__(self, name, config):\n\t\tself.name = name\n\t\tself.definitelyIndexed = cache_definitely(config,'indexed')\n\t\tself.definitelyUnique = cache_definitely(config,'unique')\n\t\tself.get_definitelys()\n\n\tdef add(self, entry):\n\t"}]}
{"task_id": "62b8d23a48ba5a41d1c3f499", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t# We've taken the entire list, so empty.\n\treturn ()\n\npackage org.carlspring.strongbox.domain.folder;\n\nimport java.util.Date;\n\nimport org.carlspring.strongbox.security.authorization.AuthorizationManager;\n\n@Deprecated\npublic class FolderAuthorizationManager\n    extends AuthorizationManager<Folder, Boolean>\n{\n\n    public FolderAuthorizationManager()\n    {\n        super(new TreeParentNode<Folder, Boolean>(this, new TreeLeafNode<Folder, Boolean>("}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tself.removeitem([(k, v) for k, v in self.items()])\n\n\treturn self.__getitem__['__delitem__']\n\ndef pop(self):\n\t\"\"\"\n\tReturn a new, empty `(type, value)` pair.\n\t\"\"\"\n\treturn (type, self.popitem())\n\ndef __getattr__(self, name):\n\t\"\"\"\n\tA hack, to support PyPy bug #1279.\n\t\"\"\"\n\tdef f(n):\n\t\treturn n\n\tdef __getattr__(self, name):\n\t\treturn"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tself.db.delete_object(self.key)\n\tself.db.set_value(self.key, \"_last\", self.lastval)\n\n#=============================================================================\n#\n# Tests\n#\n#=============================================================================\n\n@db_setup\n@unittest.skipUnless(have_sqlite, \"sqlite needed for this module\")\nclass FakeQuery(db.DB):\n\t\"\"\"A fake `dict` database test\n\t\"\"\"\n\n\tdef __init__(self):\n\t\tdb.DB.__init__(self)\n\t\tself.db = db\n\t\tself"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\ts = self.list('popitem', 1)\n\tif not s:\n\t\treturn self.list()\n\telse:\n\t\treturn s[0]\n\ndef pop(self, n):\n\t\"\"\"\n\tRemove `n` least recently used keys from the tree.\n\t\"\"\"\n\ts = self.list('pop', 1)\n\tif s:\n\t\tfor i, k in enumerate(s):\n\t\t\tif i < n:\n\t\t\t\ts.pop(i, k)\n\t\t\telse:\n\t\t\t\tdel s[i]\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tpop = self.getpop()\n\tpop.update()\n\tdel pop\n\npackage com.atguigu.springcloud9;\n\nimport org.springframework.cloud.openfeign.FeignClient;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.web.reactive.config.EnableWebFlux;\nimport org.springframework.web.reactive.config.WebFluxConfigurerAdapter;\nimport org.springframework.web.reactive.function.client.WebClient;\nimport org"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\twhile True:\n\t\tif len(self.items) < settings.POPUP.ITEMS:\n\t\t\tself.popitem(self.items)\n\t\tif self.lastItem.name in self.items:\n\t\t\tdel self.lastItem\n\t\t\tself.popitem(self.items)\n\t\telse:\n\t\t\tself.lastItem = self.items[self.lastItem.name]\n\t\t\tself.items = self.items[1:]\n\t\t\treturn self, None, self.lastItem\n\treturn None, None, None\n\ndef"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tif self.cursor.valid():\n\t\tkey, value = self.cursor.get().split()\n\n\t\tif key == \"\":\n\t\t\tself.cursor.remove()\n\t\t\treturn\n\n\t\tif type(self.cursor) is list:\n\t\t\tself.cursor.insert(3, key, value)\n\t\telse:\n\t\t\tself.cursor.remove(key)\n\n\nclass Scanner(object):\n\t\"\"\"\n\tScanner is a wrapper for a \"scanner\".\n\t\"\"\"\n\tdef __init__(self, mode):\n\t\tself"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tpopped = copy.deepcopy(self)\n\tpopped.values = [self.keys[1], self.keys[-1]]\n\tpopped.count = 0\n\treturn popped\n\n\ndef update_index(self, key):\n\ttry:\n\t\tself.count += 1\n\t\tself.keys[-1] = key\n\texcept KeyError:\n\t\tself.keys = [key]\n\"\"\"\n\n\tif (self.count > self.max_count):\n\t\traise ValueError('Cannot add {} more '\n\t\t\t'than the maximum allowed"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\treturn self.popleft()\n\ndef _removemapback(self, *v):\n\tfor v in self:\n\t\tif re.search(self[0], v) or self[1] == None : raise KeyError\n\t\tself[1][self[1][0]] = v\n\nclass key:\n\tdef __init__(self, k):\n\t\tself.key = k\n\t\tself.value = self.key\n\t\t\t\t+ '!' # TODO:\n\t\t\t\t+ (self.key, self.key)\n\nclass value:\n\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tself.popvalue()\n\tlastpop = self[len(self)-1]\n\tnext = lastpop\n\tfor n in range(len(self) - 2, -1, -1):\n\t\tif self[n] == lastpop:\n\t\t#\tprint 'Warning: pop_item with index %d removed (key=%s)'\n\t\tif n < len(self) - 1:\n\t\t\tself[n] = next\n\t\t\tnext = self[n + 1]\n\t#\t\tprint 'Warning: popItem with %d removed (key"}]}
{"task_id": "62b8d23c48ba5a41d1c3f49b", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\twith self._lock:\n\t\tself._popitems.append((key, value))\n\n\treturn self\n\n\tdef popitem_if_changed(self):\n\t\tif self.key!= self.key_getter() or self.value!= self.value_getter():\n\t\t\tfor key, value in self._popitems:\n\t\t\t\tself[ key ] = value\n\n\tdef key(self):\n\t    return self._popitems[0] if hasattr(self, '_popitems') else None\n\n\tdef key_getter(self):\n\t  "}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tkeys_to_return = self\n\tkeys_to_remove = []\n\n\tfor key in keys_to_remove:\n\t\tif key not in self._dict:\n\t\t\tvalue = None\n\t\t\tkeys_to_remove.remove(key)\n\t\telse:\n\t\t\tvalue = self._dict[key]\n\t\t\tif value == self:\n\t\t\t\tkeys_to_remove.remove(key)\n\t\t\telse: \n\t\t\t#  key was not in dict, and is used in values\n\t\t\t#  only return, if the value is"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tret = {}\n\tif self._item is not None:\n\t\tfor k, v in self._item.iteritems():\n\t\t\tif k == \"(key, value)\" or k == \"(value, key)\":\n\t\t\t\tcontinue\t\t\t# Skip already-popped items\n\t\t\tret[k] = v\n\treturn ret\n\n/*\n * Copyright (c) 2012 Data Harmonisation Panel\n * \n * All rights reserved. This program and the accompanying materials are made\n * available under the terms of the GNU Lesser General Public License as\n * published by the Free Software Foundation,"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\titem = self.pop()\n\tif item is None:\n\t\treturn\n\titem[0] = self.pop()\n\tself.pop()\n\ndef push_frame(frame, level=2):\n\t\"\"\"Adds a new frame to be a stack level.\"\"\"\n\tif isinstance(level, int):\n\t\tlevel += 1\n\tframe.append(level)\n\n\n\nglobal_level = 0\n\ndef run_test(test_suite):\n\tmain()\n\ndef main():\n\tglobal_level = 0\n\n\twhile True:\n\t\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tlast_item = self.pop_last_item()\n\tif last_item is None:\n\t\treturn None, None\n\tif PY3K:\n\t\tpopkey = str(last_item)\n\telse:\n\t\tpopkey = 'pop%' + str(last_item)\n\tpop_tuple_item = reversed(popkey)\n\tif hasattr(self, pop_tuple_item):\n\t\treturn pop_tuple_item[-1]\n\telse:\n\t\treturn None, None\n\n## PEP8: Use getitem instead\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tself.pop_from_queue()\n\treturn self.popitem(self.pop_from_done)\n\n\t# def popitem_now(self, *args, **kwds):\n\t#     return self.popitem(self.pop_from_done, *args, **kwds)\n\n\tdef get_popitem(self, *args, **kwds):\n\t\treturn self.get_item(self.pop_from_done, *args, **kwds)\n\n\tdef get_popitem_now(self, **"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tprevnode = None\n\twhile prevnode is not None:\n\t\tif (prevnode.key, prevnode.value) in self:\n\t\t\tprevnode = None\n\t\telse:\n\t\t\tself.pop(prevnode.key)\n\tif not self:\n\t\tself = (self[key] for key in self.keylist)\n\tprevnode = None\n\twhile prevnode is not None:\n\t\tself.remove(prevnode.key)\n\tself.insert(index(self, self.keylist, [self.value]), self."}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tfor i, elem in reversed(iteritems(self.all)):\n\t\tif elem[1] == elem[0]:\n\t\t\tdelete_key = elem[2]\n\t\t\tfor k, v in reversed(self.items):\n\t\t\t\tif k == i:\n\t\t\t\t\tself.items.remove(k)\n\t\t\t\tif v[0] == elem[2]:\n\t\t\t\t\tv[1] = elem[1]\n\t\t\t\telif v[1] == elem[1]:\n\t\t\t\t\tv[0] = elem[0]\n\n\treturn delete_key"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\ttry:\n\t\tkey = self._key_to_pop(str)\n\t\treturn key\n\texcept KeyError:\n\t\treturn None, None\n\ndef popitem_as__dict__():\n\t\"\"\"\n\tReturn the value currently defined as a dictionary. See :class:`dict`.\n\t\"\"\"\n\ttry:\n\t\tkey = self._key_to_pop(str)\n\t\treturn dict.__getitem__(key) #TODO handle object format\n\texcept KeyError:\n\t\treturn None, None\n\ndef dict_keys(self):\n\t\"\"\"\n\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tdef getitem(self):\n\t\t#      1. Assert index 0 is in dictionary\n\t\t#      2. If not dictionary then return None\n\t\t#\t3. Else return key as result\n\t\tif not self._dictionary:\t\t#      (No dict was built.)\n\t\t\treturn None\n\t\t value, key = self._dictionary.popitem()\n\t\t #       4. Return (key, value) (if value was None)\n\t\treturn key, value\n\t\t#return"}]}
{"task_id": "62b8d23e48ba5a41d1c3f49e", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tpos = self.random_int(0, len(self.keystack)-1)\n\twhile pos >= len(self.keystack):\n\t\tkey1 = self.keystack[pos-len(self.keystack)-1]\n\t\tkey2 = self.keystack[pos-len(self.keystack)-1]\n\t\tself.keystack.remove(key1)\n\t\tself.keystack.remove(key2)\n\t\tpos -= len(self.keystack)\n\treturn (key1, self.keystack[pos])\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tcur = self.random()\n\tif cur <= 0:\n\t\traise self.exception('no key')\n\telse:\n\t\treturn cur, (cur, cur)\n\n\ndef makearray(k):\n\tv = k\n\tfor i in range(3):\n\t\tkey = self.random()\n\t\tif self.get(key, v)!= v: # if not at all unique\n\t\t\tself.insert(key, v)\n\tresult = self.array(k)\n\tself.remove(result, k)\n\treturn result\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tf = self.generator.popitem()\n\treturn f(self, 10)\n\n\ndef next(self):\n\t\"\"\"\n\tReturn random numbers from 0 up to a maximum\n\t\"\"\"\n\tf = self.generator.next()\n\treturn f(self)\n\n\ndef nextindex(self):\n\t\"\"\"\n\tReturn a random index from 0 up to the maximum\n\t\"\"\"\n\tf = self.generator.nextindex()\n\treturn f(self)\n\n\ndef repeate(f, n=10):\n\t\"\"\"\n\tReturn a repe"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\treturn _popitem(self, random.choice(range(1, self.N)))\n\n\t# ----------------------------------\n\t# private methods\n\t# ----------------------------------\n\tdef _set_random_state(self, random_state):\n\t\t\"\"\"Called before _set_random_state().\n\n\t\t@type\trandom_state: L{random} instance\n\t\t@param\trandom_state: The new random state\n\n\t\t\"\"\"\n\t\told = self.random_state\n\t\tself.random_state = random_state\n\t\tself.clear()"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tif self.getvalue(0) == EMPTY:\n\t\tself.remove(0)\n\telse:\n\t\treturn self.popitem()\n\n\t# We've got no item after popping\n\treturn EMPTY, EMPTY\n\n/*\n * Copyright 2017 HugeGraph Authors\n *\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with this\n * work for additional information regarding copyright ownership. The ASF\n * licenses this file to You under the Apache License, Version 2.0 (the\n * \"License\"); you"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tself.debug('popitem')\n\trandom.shuffle(self.items_)\n\titem = self.items_[self.pos]\n\n\tkey = item[0 # 0: self.pos] # self.key\n\tvalue = item[1 # 1: self.count] # self.value # 0=random\n\tk = key[0] # k is a random number\n\tval = value[0] # val is a random number\n\tself.debug(key, item[-1], value[-1])\n\treturn k, val, value, key\n\n\ndef"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tself.getkey()\n\tpop = self._popitem(self.getkey())\n\tself.delkey(self.getkey())\n\treturn pop\n\n\ndef hash_key(key):\n\t'Add a new `hashkey` to the internal hash table.  The value hashkey[0:32] are used to hash the key.'\n\thash_key = (''.join([chr(s), key, '']) for s in self._hashkey)\n\tself._hashkey.add(hash_key)\n\n\ndef add_key(self, key"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\treturn self.__random__(*self.__iter__())\n\n/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __CPU_CP_H\n#define __CPU_CP_H\n\n#include \"cpio.h\"\n\nvoid arch_cpu_init_ipi(void);\n\nstruct cpu_task {\n\tu32 cpuid;\n\tchar *comm;\n};\n\nextern struct cpu_task cpu_tasks[];\n\nstruct cpupro_cpu {\n\tint cpu;\n\tstruct kobject kobj;\n\tstruct cpu_task *cp;"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t# This function does not actually delete the key.  Instead, if the key is a set, then remove\n\t# the corresponding set member.  However, if this is a `popall` operation then do this as\n\t\t# per the comment in the docstring.\n\ttry:\n\t\tresult = self.c.popitem(self.key)\n\texcept KeyError:\n\t\tself.key = None\n\t\t# Return False so we don't continue to search the result.\n\t\tresult = None\n\n\t# Handle a nested `popitem`, and return a tuple"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\ts.addmessage(self.root, 'Remove', 0, 'random element')\n\tif self.maxindex:\n\t\tself.randitem(self.maxindex-1, self.maxindex)\n\treturn self.rng.randint(self.maxindex - 1, self.maxindex)\n\n\ndef pop(self):\n\t\"\"\"\n\tRemove and return an integer random element from the heap.\n\t\"\"\"\n\ts.addmessage(self.root, 'Remove', 0, 'random element')\n\treturn self.rng.randint(1, self.max"}]}
{"task_id": "62b43425903eeb48555d3ea1", "prompt": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn self.parser.create_in_regex()\n\n\t\ndef _create_regex(self, text, pattern=None, flags=0, extended=None) -> str:\n\t\"\"\"\n\tCreate a regular expression.\n\t  \ttext\t\tThe text to search for.\n\tpattern\t\tThe regular expression you want to search for in the text.\n\tflags\t\tForces the whole regex to be searchable.\n\t\n\tExample::\n\t\t# replace pattern: \"(\\w*?)\", \"(\\w*)\"\n\t\tpattern = re.compile(r"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn rex.re.compile(self.pattern)\n\n\n\n@listregister(_create_in_regex)\ndef create_in_regex(pattern: Pattern):\n\t\"\"\"\n\tCreate the new in-style parameter regular expression.\n\t\n\tReturns the new in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn rex.re.compile(pattern)\n\n\n\n@listregister(lambda x: x)\ndef register_matcher_pattern(matcher: re.Pattern) -> Pattern:\n\t\"\"\"\n\tRegister a custom matcher pattern."}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.Pattern(self.source, re.IGNORECASE)\n\n\ndef _get_token(text: bytes) -> str:\n\t\"\"\"Return a byte string token.\"\"\"\n\t# NOTE: Bytes don't really represent bytes so we have some sensible\n\t#   encoding to parse them.\n\ttoken = super()._get_token(text)\n\ttokens = token.encode('ascii', errors='replace')\n\ttokens = (token[i:i+1] for i in range(len(tokens)) if tokens[i]])\n\treturn \"\".join(tokens)"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t# XXX should we be handling this?\n\t# It's pretty strange, but the regex will only be used (if a regex has been set).\tIf we\n\t# can get away with doing our own parsing for example, make our own regex here.\n\t# \n\t# The simplest way through it is to have a regex to match any regexp, including the\n\t/// :mod:`regex` module.\tIf we wanted to support the regex for the above, we could\n\t# make that a no-op, but the regex isn't always the case.\n\t\n\tif isinstance(self"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tdef _create_string(self, value):\n\t\treturn re.compile(value)\n\n\tdef _update_params(self, re_: Pattern) -> None:\n\t\tself.str_ = self.get_string(re_._key)\n\t\tif re_._replace:\n\t\t\tself.str_ = [self.str_, re_._replace1, re_.group(), str(re_)][1:]\n\t\nclass _ParamKey(object):\n\t\"\"\"\n\tHelper class for key names for the patterns.\n\t\n\t\"\"\"\n\tdef __"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn self.pattern\n\t\n#!/usr/bin/env python\n''' Python Interdiscipline Commandline Library for Python'\nWritten by Kennus Ayazgaby (kankus@symphony.net)\n\nThis file is part of PyRec, a Qt/Linux port of PyRec for MicroPython.\n\nThe MIT License (MIT)\n\nCopyright (c) 2013-2014 Kenny Fabio\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tpillar, err = self._re_replace(self.IN_PARAMS)\n\tif err!= None:\n\t\traise ValueError(str(err))\n\n\treturn pillar\n\n/*******************************************************************************\n * Copyright (c) 2015 ARM Ltd. and others\n * All rights reserved. This program and the accompanying materials\n * are made available under the terms of the Eclipse Public License v1.0\n * which accompanies this distribution, and is available at\n * http://www.eclipse.org/legal/epl-v10.html\n *\n * Contributors:\n * ARM Ltd and"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tdef _initialize_regex(self) -> Pattern:\n\t\treturn self.match_to_re(self.pattern)\n\t\n\tdef _copy_in_str(self) -> str:\n\t\treturn self.replace(self.pattern).upper(), \\\n\t\t\tself.replace(self.pattern).upper()\n\t\n\tdef match(self, pattern: Pattern) -> str:\n\t\treturn self._copy_in_str()\n\t\n\tdef match_to_re(self, regex: Pattern) -> Re:\n\t\treturn super().match_to_re"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t# regexps:\tPattern[String]\n\t\n\tif isinstance(self, bool):\n\t\treturn Pattern()\n\treturn []\n\n# }}}\n###########################################################\n\n# {{{ template class\n\nclass _TemplateBase:\n\t_default_options_name_ = \"default-options-name\"\n\t_default_options_name = \"template\"\n\t_options_name_ = \"options\"\n\n\t# Template name - the same as the template name from a `Template` object.\n\ttemplate_name = \"\"\n\n\t# Default name template parameter pattern -"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tdef _create_in_regex_string(self) -> Pattern:\n\t\tregex = _re_regex(regex)\n\t\t_Regex._regex_error(\"Failed to use '%s' value: '%s'\",\n\t\t\t\"String literal\", self.value, regex)\n\t\t\n\t# XXX XXX XXX XXX XXX TODO XXX XXX XXX XXX XXX XXX TODO XXX XXX\n\t_Expression._expression_error(\"Failed to compile '%s'\", self)\n\n\t# TODO FIXME\n\tdef _compile_regex(self) -> Pattern:\n\t\t\"\"\"Compiles a literal in the specified regular"}]}
{"task_id": "62b43426903eeb48555d3ea2", "prompt": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tfrom.. import parameter_style_converters\n\n\treturn parameter_style_converters.convert_to_parameter_style(_converting._Converter())\n\n################################################################################\n\n\nclass ParameterStyleParameterConverter(_ParameterStyleParameterConverter):\n\t\"\"\"\n\tA parameter style converter using parameters.\n\t\n\tReturns the parameter style converter (:class:`._Converting._Converter`).\n\t\"\"\"\n\tdef convert_to_parameter_style(self, style: _ParameterStyle) -> _ParameterStyle:\n\t\t\"\"\"\n\t\tConvert parameters from parameter style to parameter style, so that\n\t\tthey"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn self\n\n\n################################################################################\n\n\nclass _converting.object_to_conversion(object):\n\t__abstract__ = True\n\n\tdef __new__(cls, object) -> _converting.object_to_conversion:\n\t\traise ValueError() # type: ignore\n\n\tdef __init__(self, object):\n\t\tself.object = object\n\t\tself._converter = None\n\n\tdef _to_docstring_or_bytes(self):\n\t\tif self._converter:\n\t\t\treturn self._converter.to_docstring_or_"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn self.create_converter(self.parameter_style_converters)\n\t\n\t\nclass _converting_error_message:\n\n\t\"\"\"\n\tHelper method for converting a string into a Python string of error messages.\n\n\tThis method should return an error message such as\n\t:meth:`._converting._Converter.format_error(err_msg)`,\n\tor :obj:`\"_converting_error_message` for an error message that can be formatted in\n\tan amazing user error log.\"\"\"\n\n    default_encoding = sys.getdefaultencoding()\n"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting.converters.register(lambda *args, **kw: _converting.ParameterConverter(*args, **kw))\n\n#------------------------------------------------------------------------------\n# Public Classes\n#------------------------------------------------------------------------------\n#------------------------------------------------------------------------------\n\n\nclass _converting._ParameterConverter(_converting._Converter):\n\n\tdef __init__(self, *args, **kwargs):\n\t\ttry:\n\t\t\t_converting._ParameterConverter.__init__(self, *args, **kwargs)\n\t\t\tself.required = kwargs.pop('required', False)\n\t\t\tself.optional = kwargs.pop('optional"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tdef helper(self, name, args, kwargs):\n\t\treturn _converting._Converter(list(args), list(kwargs))\n\treturn helper\n\n// Code generated by smithy-go-codegen DO NOT EDIT.\n\npackage rds\n\nimport (\n\t\"context\"\n\tawsmiddleware \"github.com/aws/aws-sdk-go-v2/aws/middleware\"\n\t\"github.com/aws/aws-sdk-go-v2/aws/signer/v4\"\n\t\"github.com/aws/aws-sdk-go-v2"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn self._converting._Converter(\n\t\tdefault_format=self.default_format, params_format=self.params_format)\n\n\n# ---------------------------------------------\n# Class Parameters\n\n@dataclass\nclass Params(object):\n\t\"\"\"\n\tParameters dictionary.\n\n\tThe following dictionary keys are possible values:\n\t* name (string) - this parameter is the name of the parameter.\n\t* flags (string) - a string which represents several flags. \n\n\tThe following flags are allowed:\n\t* `None`, the parameter is not used.\n\n\tThe"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tconverter_type = convertion.Converter\n\tif self contains_string_conversion:\n\t\tconverter_type = convertion.StringConverter\n\tif self contains_bool_conversion:\n\t\tconverter_type = convertion.BoolConverter\n\tconverter_type.set_parameters(\n\t\tparameters=self._parameters,\n\t\tconverter_function=self._converter_function,\n\t\tconverter_value=self._converter_value,\n\t)\n\treturn converter_type\n\n#ifndef __MSET_H__\n#define __MSET_H__\n\n#"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tdef convert(self, name, value):\n\t\t\"\"\"\n\t\tCreates a parameter style converter to convert :class:`name` with value to :class:`value`.\n\t\n\t\tParameters:\n\t\n\t\t\tname: :class:`str`\n\t\t\tvalue: :class:`type`\n\t\n\t\tReturns:\n\t\n\t\t\tA new :class:`_converting._Converter`.\n\t\n\t\t\"\"\"\n\t\tkwargs = self._get_default_parameters()[name]\n\t\tif kwargs is None:\n\t\t\tkwargs = {}\n\n\t\tconverter = kwargs.setdefault('"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn self.__convert()\n\n\nclass _Converter(_converting._Converter):\n\t\"\"\"Helper class when making parameters of this type conversion.\"\"\"\n\tdef can_convert(__cls):\n\t\t\"\"\"Return ``True`` if the current type can be converted into the\n\t\tparameter style converter, ``False`` otherwise.\n\n\t\t:param __class__: the class of the conversion\n\t\t:return: ``True`` if the current type can be converted into the\n\t\t           parameter style converter, ``False`` otherwise.\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t# Hack to allow for the deprecated API\n\timport warnings\n\twarnings.warn('The parameter style converter class may be removed in PEP 3',\n\t\tImportWarning, stacklevel=2)\n\treturn self.factory(convert=self.param_style, **self.parameters)\n\nclass ParameterStyleParameter(object):\n\t\"\"\"\n\tCreate a parameter style parameter.\n\t\n\tAdds a named reference to :class:`typing.Parameter`, and allows setting the\n\tparameter value to a single value.\n\t\n\tParameters must be created with the :class:`parameter.Parameter` or"}]}
{"task_id": "62b8966c755ee91dce50a154", "prompt": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t# pylint: disable=line-too-long\n\tdt_str = str(dt_str, 'w')\n\n\ttry:\n\t\tdt_int = dateutil.parser.parse(dt_str, strict=False)\n\texcept (ValueError, ParserError) as exc:\n\t\tif not isinstance(exc, ParserError):\n\t\t\traise TypeError('Invalid value for `dt_str`: %r' % (exc,))\n\t\telse:\n\t\t\treturn ValueError(ERROR_FORMAT.format(dt_str))\n\tdt_int = dt_int."}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\tdt_str = dt_str.convert(\"U\")\n\tif not dt_str:\n\t\tdt_str = str(datetime.datetime.now())\n\tpos = dt_str.find(\":\")\n\toffset = 0\n\tif pos >= 0:\n\t\tdt_str = dt_str[:pos]\n\t\toffset = int((dt_str.find(':')[0] + 2) * 3600)\n\t\tif dt_str[pos + 5:].lower() in (\"00\", \"Z\", \"ZZ\"):\n\t\t\tdt_str = dt_"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\ttry:\n\t\tdt = parse_iso8601(dt_str, strict=False, offset=True)\n\texcept ValueError as exc:\n\t\traise ParseError(\"Invalid date string '%s', could not be parsed\" % dt_str, exc, \"strict\")\n\treturn as_date_or_datetime(dt, timezone)\n\ndef parse_iso8601(datastr):\n\t\"\"\"\n\tPerform a normal parse, assuming it's a string.\n\t\n\tParse ``datastr`` into a datetime object or an ISO string.\n\t\n\tThis is equivalent to `parse_"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\tif isinstance(dt_str, (str)):\n\t\tdt = (dt_str).encode('gbk')\n\telse:\n\t\tdt = dt_str.isoformat()\n\t\tif dt[0] == \"-\" and dt[1] == \"T\":  # UTC offset\n\t\t\toffset = -7*3600; None if dt_str is None:\n\t\t\traise ValueError('invalid timezone offset')\n\t\telif dt[0] == \"+\" and dt[1] == \"Z\":  # Z offset\n\t\t\toffset = -1*3600*-2;"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\tdt = datetime.datetime(*dt_str.split(os.sep))\n\tif not isoparse(dt):\n\t\traise ParseError(dt, 'YYYY-MM-DD HH:MM:SS.SSSS')\n\ttry:\n\t\tresult = dt.isoformat(timezone='Z')\n\texcept AttributeError:\n\t\traise ParseError(dt, 'Z')\n\t\t\n\tif not result.isdigit() or result in ('', 'Z'):\n\t\traise ParseError(dt, dt_str+dateutil.TZ.ZOFFSET_ERROR)\n\t\n\t"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\tfrom zope.component import Component\n\n\t# Incomplete Date Format -> All Date / Time parsing is optional.\n\tdt_str = stringutils.to_unicode(dt_str)\n\n\tparts = []\n\tre_dt_ts = re.compile(r'(:?\\d+[-.\\d]+)?')\n\n\tstartpos = len('#')\n\tpos, length = 0, 0\n\twhile pos < len(dt_str):\n\t\tif re_dt_ts.match(dt_str[pos]):\n\t\t\ttry:\n\t\t\t\t#"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\treturn iso_datetime_to_datetime(dt_str)\n\n\nfrom calendar import timegm\n\n\n# https://en.wikipedia.org/wiki/ISO_8601\n# See: :ref:`the_iso8601_names`\n__iso8601_names = [\n    'EEE MMM d d HH:mm:ss z',\n    'EEE MMM d d s',\n    'EEE MMM d. HH:mm:ss z',\n    'EEE MMM d z',\n    'EEE MMM d',\n"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\tdt = datetime.datetime.strptime(dt_str, '%Y%m%dT%H%M%S')\n\n\t# Parse the date and time portion of the date string.\n\tdtstr = dt.strftime('%Y%m%dT%h%M%S')\n\n\t# Parse the offsets into an array with optional component values.\n\tresult = np.array(dtstr, dtype='O')\n\n\t# Return the component values\n\treturn result\n\n# encoding: utf-8\n# Code generated by Microsoft (R) AutoRest Code Generator.\n# Changes"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t# If dt_str is a str or stream, the following checks will not raise\n\t# an exception\n\tdt_str = str(dt_str)\n\tif len(dt_str) == 0 or dt_str == '':\n\t\traise ValueError(\"Date string is empty.\")\n\t# Parse UTC, since we expect it to be within the time zone offset\n\tdt2utc = utc.tzlocal(dt_str)\n\t# Parse ISO 8601-2 format\n\ttry:\n\t\tdt2utc = utc.tz_from_string(dt_str,"}, {"generated_text": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\ttry:\n\t\t# XXX could have been a \"parse.isoformat\" here instead of inside\n\t\t#     the parser (i.e.. ISO-8601 is a subset of ISO 8601)\n\t\t# so that we can avoid re-encoding the strings as a separate\n\t\t# object with extra ``str()`` calls\n\t\tdt_parsed = dt_str.str.decode('ascii')\n\t\tdt_parsed = dt_parsed.split(b'.')[0]\n\texcept (ValueError, IndexError, TypeError, OverflowError):\n\t\t"}]}
{"task_id": "62b896de755ee91dce50a183", "prompt": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\tfrom string import ascii_let\n\treturn _parse(\n\t    timestr, default=default, ignoretz=ignoretz, tzinfos=tzinfos, **kwargs)\n\n@classdate_utils.deprecated(\"use tzinfo\")\ndef parse(timestr, tzinfo=None):\n\t\"\"\"\n\tParse a string representation of the given date/time object.\n\t\n\t:param timestr:\n\t    This string representation of the parsed ``datetime`` object.\n\t\n\t``timestr`` must contain a single token (``T``).\n\t\n\t.. note::\n"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\tdef __init__(self, timestr, default=None, ignoretz=False, tzinfos=None,\n\t\t\t**kwargs):\n\t\tif default is None:\n\t\t\tdefault = timestr.tzinfo\n\t\tself._parser = Parser(timestr, tzinfos=tzinfos, \\\n\t\t\t\t**kwargs)\n\t\tself.default = default\n\t\tself._fuzzy_with_tokens = self.is_valid_date_string\n\t\tself._fuzzy_with_tokens = (self.is_valid_date_string and\n\t\t   "}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t#\n\t# Python 2.7, 1.11 and earlier changed in Python 3.\n\t#\n\t# TODO(jshawak):  This code has been tested to be compatible with Python 2.7\n\n\t# There's no timezone information in Py2_7, which isn't part of Python 3.  Python 2.7\n\t# adds timezone information in this way; Python 3 uses it to figure out whether\n\t# we want to use a *local* or *UTC* timezone.  Python 3's TZ environment variable is\n\t# not a POSIX environment variable, so we"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\treturn parse(timestr, default=default, ignoretz=self._tzinfos, tzinfos=self._tzinfos,\n\t             **kwargs)\n\n\n@_tzinfos_to_string = lambda tinfo: '%s.ZZ' % 'UTC' % tinfo.strftime(\"%z\")\n# This needs to be called to find the UTC timezone offset, since the time-zone\n# information was provided in the given tzinfo dictionary.  We know the\n# corresponding UTC timezone offset was in fact in fact in the provided tzinfo\n#"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\tfrom datetime import datetime\n\tif default is None: default = datetime()\n\t\n\tif isinstance(timestr, six.string_types):\n\t    if isinstance(default, datetime): default = default.replace(microsecond=0)\n\t    try:\n\t\ttzoffset = 0\n\t\ttzname = timestr.replace(\" \", \"-\").capitalize()\n\t\tparsed = (0, -840, default.replace(tzinfo=tzoffset), default.strftime(\n\t\t\t%Y-%b-%d %H:%M:%S.%f))\n\t\t"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\ttry:\n\t\tfor format in _format_opts:\n\t\t\tt, tz = parse_tz(*format)\n\t\t\tif zoneinfo == None:\n\t\t\t\tdata = t\n\t\t\telif timezone is not None:\n\t\t\t\tif not isinstance(timezone, tzinfo):\n\t\t\t\t\tcontinue\n\t\t\tdate = format(data)\n\t\t\tif not ignoretz and default is None:\n\t\t\t\tdate = _canonicalize_date(date)\n\t\t\t\tif tz is None:\n\t\t\t\t\ttz = parse(timezone)\n\t\telse:\n\t\t\tdate = time"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\tfrom calendar import timegm\n\tdt = timestr.split()\n\tif len(dt) == 1: raise ParserError('Must have a single date string', dt)\n\t\n\tfor timestr in sorted(dt, key=argparse.getargvalue):\n\t\t# parse the timestr into timestamp and offset\n\t\tnow = timestrparse.parse('now', timestr)\n\t\t# construct the timezone string\n\t\ttzname = _gettzname(tzinfo=now)\n\t\toffset = None\n\t\tif tzinfos:\n\t\t\tif \"Z"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\ttzinfo = self.tzinfo or {}\n\ttry:\n\t    while len(timestr) > len(default):\n\t\ttry:\n\t\t    str(timestr)\n\t\texcept UnicodeDecodeError:\n\t\t    try:\n\t\t\ttzstring = timestr[:len(default)]\n\t\t\ttoken = default\n\t\t\tif ignoretz:\n\t\t\t    token = default + tz2local(defaulttz)\n\t\t        else:\n\t\t\t    token = defaulttz[default]\n\t\t        while len"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\tdef __init__(self, timestr, *,\n\t\tignoretz=(None, None, None, \"Z\"), timezone=None, tzoffset=None, tzinfo=None, **kwargs):\n\t\t\"\"\"\n\t\t    Initialize a :class:`datetime.datetime` object from a :class:`str` of\n\t\t    the specified ``timestr``, an optional custom field of the\n\t\t    time object and a string value and/or a time zone.\n\t\n\t\t:param timestr:\n\t        Either a str object representing the string, or a"}, {"generated_text": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\ttry:\n\t\treturn cls._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n\texcept Exception:\n\t\traise\n\n#\n#\n#\n\ndef date_with_local_time(timeoffset, fuzzy_with_tokens=None, ignoretz=True, *extra) -> datetime.datetime:\n\t\"\"\"\n\t    Return a datetime object representing a date\n\n\t\t:param time_offset:\n\t        The local time offset for the timezone.\n\t\n\t\t:param fuzzy_with_tokens"}]}
{"task_id": "62b8a4a4755ee91dce50a3d3", "prompt": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt.tzinfo == \"utc\":\n\t\treturn datetime.datetime(0, 0, 0, 0, 0, 0)\n\tt = self.fromutc(dt)\n    f = datetime.datetime(1900, 1, 1, 0, 30)\n    while t.time < f.time:\n        t = t.replace(second=False)\n        f = f.replace(second=False)\n    return f\n\ndef fromutcdifetime_ns(dt,"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif not dt:\n\t\traise ValueError(\"First parameter is mandatory\")\n\tif len(dt.tzinfo) == 0 or not isinstance(dt.tzinfo, datetime_utils.tzinfo):\n\t\traise ValueError(\"Second parameter is mandatory\")\n\tif not isinstance(dt, datetime.datetime):\n\t\traise ValueError(\"First parameter is mandatory\")\n\tif dt.tzinfo!= self._tzinfo:\n\t\traise ValueError(\"Non-equivalent datetime: %s\" % dt)\n\tbase = datetime.datetime(1,1,0)     # 00:"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import datetime\n\timport pytz\n\tdt2 = pytz.UTC(dt)\n\tyear = dt.year\n\tdt_new = datetime.datetime(year, 1, 1)\n\ti, j = dt_new.i, dt_new.j\n\twhile i < j:\n\t\ti += 1\n\twhile i <= dt_new.i:\n\t\ti += 1\n\twhile j <= dt_new.j:\n\t\tj += 1\n\tdt_new.i = i\n\tdt_new"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt is None:\n\t\treturn\n\tfromutc = dt.internal_is_fromutc\n\tdt_type = dt.tzinfo.tz_type\n\t\n\tif dt_type == \"CST\":\n\t\tdt_str = dt.tzinfo.dst\n\telif dt_type == \"ICT\" or dt_type == \"ICT\":\n\t\tif self.is_dst:\n\t\t\tdt_str = dt.tzinfo.name\n\t\telse:\n\t\t\tdt_str = None\n\telif dt_type == \"CST\":\n\t\tif"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tfrom datetime import datetime, timedelta\n\t\n\tif is_utc_dt_tz(dt):\n\t\tfrom pytz.timezone import utc\n\t\treturn utc(dt) if (hasattr(dt, \"tzinfo\") or dt.tzinfo) else dt\n\telse:\n\t\tdt = dt if is_a_date(dt) else datetime.strptime(dt, \"%Y-%m-%dT%H:%M:%S\")\n\t\td = dt.replace(tzinfo=utc).astimezone(utc=True)\n\t\treturn dateutil.dt"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif not isinstance(dt, datetime.datetime):\n\t\traise ValueError(\"DateTime object in timezone-aware arithmetic \"\n\t\t\t\t\t \"is expected to yield a datetime.\")\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"Unknown timezone in UTC timezones\")\n\tif dt.tzinfo.utcoffset or dt.tzinfo.tzutc or dt.tzinfo.tzisdst:\n\t\treturn self._localize(dt.utcoffset(), dt.tzinfo.utcoffset())\n\t\n\tif dt.tzinfo.utcoffset:\n\t\treturn self._"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tnewyear = dt.year - 1900\n\twhile dt.year % 100!= 0:\n\t\tdt = dt.replace(year=1, month=1, day=1)\n\treturn dt\n\nclass timezone_name(object):\n\n\t\"\"\"\n\tA :class:`datetime.tzinfo` object that returns an appropriate\n\tresolution of an RFC 3339, or an object that can support\n\tresolution at all.\n\t\"\"\"\n\tdef __init__(self, name, tz=not_set):\n\t\t\"\"\"\n\t\t:type name: :class:`str"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# Timezone-aware datetime objects always have the tz-naive\n\t# information.\n\tif dt == self: return None\n\tif self._tznaive and dt.tzinfo is None:\n\t\treturn None\n\treturn datetime.fromtimestamp(\n\t\tself.timetuple(dt, offset=None)\n\t)\n\nclass Date(object):\n\t\"\"\"\n\tThis class represents datetimes.\n\t\n\tThis object has exactly two attributes, datetime.date() and a\n\tdatetime.time() which represent a datetime with fixed precision.\n\t\n\tFor example,"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tfold = False\n\tif not isinstance(dt, (Datetime, date)):\n\t    error(1330)\n\t    return dt, None, None\n\tif DATERANGE_STARTS_ON_JUNCTION:\n\t    fold = True\n\t\n\tif isinstance(dt, Timedelta):\n\t    dt = dt.asfreq('s')\n\telif hasattr(dt, 'asfreq'):\n\t    dt = dt.tz_convert('UTC')\n\tdt = dt.timetuple()\n\t\n\ti = 0\n\tif isinstance(dt"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif len(dt.tzinfo) > 1:\n\t\t# Timezone aware datetimes are ambiguous if there are years\n\t\t# without a DST offset. See https://stackoverflow.com/a/25671913/379765\n\t\t# Note: This uses the (possible) naive implementation of that function;\n\t\t# the naive implementation only works if all the years before the\n\t\t# timezone are in effect. See\n\t\t# https://golang.org/issue/10952\n\t\tstart_year = dt.year - datetime.timedelta(days"}]}
{"task_id": "62b8a7b2755ee91dce50a4a7", "prompt": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif not isinstance(dt, datetime):\n\t\tdt = _as_dt(dt)\n\tif not isinstance(tzinfo, tzinfo):\n\t\ttzinfo = _as_tzinfo(tzinfo)\n\treturn _convert_tzinfo_to_datetime(dt, tzinfo)\n\n\ndef _convert_tzinfo_to_datetime(dt, tzinfo):\n\t# This must be called in Python2\n\tif not isinstance(tzinfo, _tzinfos_type):\n\t\traise TypeError(\n\t\t    \"tzinfo should have been a string containing"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\ttz = tzinfo.tz if 'tzinfo' in tzinfo else None\n\tif dt == datetime.datetime.max:\n\t\treturn dt\n\treturn dt.replace(tzinfo=tzinfo, tz=default_tzinfo(dt, tzinfo))\n\n\n_cache = {}\n_cache_cache = {}\n\n\ndef _set_cache(k, v):\n\t\"\"\"Override the cache with :py:func:`setattr`.\n\n\tDoes not trigger a cache update of the same name, only\n\tthe new data\n\n\tCalls the cache implementation ``setattr``"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tfrom datetime import timedelta\n\t# TODO: support other parameters?\n\t\n\t# get the date_time and create a new datetime\n\tdt = dt.replace(tzinfo=datetime.timezone(tzinfo.tz))\n\t\n\t# remove any timezone info set by time zone offsets\n\ttz = dt.tzinfo\n\ttry:\n\t\tdt = dt.replace(tzinfo=tzinfo)\n\texcept KeyError:\n\t\t# no timezone is provided\n\t\treturn dt\n\t\t\n\treturn dltk(dt)\n\n//  -------------------------------------------------------------------------\n//  Copyright (C)"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\ttry:\n\t\tdt = dt.replace(tzinfo=None)\n\texcept AttributeError:\n\t\traise OSError('Cannot determine a tzinfo in: %s' % (dt,))\n\treturn dt\ntzoffset = default_tzoffset\n\n#include <string.h>\n#include <assert.h>\n\n#ifdef DEBUG\n#include <lib/print.h>\n#endif\n\n#include <freeradius-devel/util/debug.h>\n#include <freeradius-devel/util/math.h>\n\nstatic void"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\treturn _get_tzinfo(dt, tzinfo)\n\n#------------------------------------------------------------------------------\n#    Copyright 2020 Microsoft Corporation\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n#    use this file except in compliance with the License. You may obtain a copy\n#    of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n# "}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t# noinspection PyProtectedMember\n\tif dt in (None, '') and tzinfo.tzinfo is not None:\n\t\traise TypeError(\n\t\t\t\"You passed a datetime or tzinfo that is not supported\"\n\t\t)\n\t\t\n\t# if dt is tzinfo, we have to add a new one\n\tif dt is tzinfo:\n\t\tdt = dt + timedelta(minutes=90)\n\t\t\n\tprint('Applying tzoffset to {} from: {}'.format(tzinfo.dt_format(), dt))\n\tdt_utc = datetime.datetime.utcf"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif isinstance(dt, tzinfo): return dt.tzinfo\n\treturn dt\n\n\n# TODO: Remove this (and move to __init__ at the beginning.)\nclass _TimezoneHelper:\n    \"\"\"\n    Base class for all timezones used throughout the system.\n    \"\"\"\n    @classmethod\n    def timezone_aware_parser(cls):\n        if cls.class.timezone_aware:\n            return cls\n        elif cls"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif is_offset(dt):\n\t\tyear, month, day, hour, min, second = default_tzinfo(dt, tzinfo=tzinfo)\n\t\tdt = datetime_to_datetime(dt, tzinfo=tzinfo)\n\t\tdttz = dateutil_gettz(dt)\n\t\tif dt and not dttz:\n\t\t\treturn dt\n\t\treturn dt\n\ttry:\n\t\ttz = parse(int(dt))\n\t\ttz = tz.replace(tzinfo=tzinfo)\n\t\treturn dt + datetime.timezone("}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tfrom datetime import datetime\n\n\tif tzinfo is None: tzinfo = default_tzinfo(datetime_tz)\n\t# check validity\n\tfor dtc in dt:\n\t\tif not dtc.kind in ALL_TIME_ZONE_KINDS:\n\t\t\traise dateutil.errors.InvalidCastError(\"Cannot cast datetime to datetime with kind '%s'\" % dtc.kind)\n\ndef int_as_time(dt):\n\t\"\"\"\n\tReturns an aware :py:class:`datetime.datetime` instance from given\n\tinstant and returns the result as either a single"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\treturn _tzinfo(dt, tzinfo.tzinfo)\n\ndef utc(dt):\n\t\"\"\"Get timezone information for the given instant.\n\t\n\t:param dt:\n\t    A datetime.datetime instance.\n\t:rtype:\n\t    :py:class:`datetime.tzinfo`\n\t\"\"\"\n\treturn _tzinfo(dt, None)\n\ndatetime.tzinfo = utc\n\ndef tzname(dt):\n\t\"\"\"Convert time zone name to an IANA registered name.\n\t\n\t:param dt:\n\t    A date/time.\n\t"}]}
{"task_id": "6305f9991d275c6667163c50", "prompt": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before_cut_chars = before\n\tself.after_cut_chars = after\n\tself.cut_chars = 0\n\t# We need this here, as we only want to deal with the last bytes we have.\n\t# NOTE it's not clear why we have left-over bytes.\n\tself.cut_remainder = 0\n\t\n\t# The following is an ugly fix.  It was created for backwards compatibility\n\t# reasons: we used a lot of variables (though that's how the new split code\n\t# knows what they meant) and couldn't figure out what we"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tdef format_from_char(byte: bytes) -> bytes:\n\t\treturn byte.replace(b's', b'\\N{')\n\n\tdef format_at_char(byte: bytes, from: bytes) -> bytes:\n\t\treturn byte.replace(b's', b'\\N{') + byte.replace(b' ', b'\\N{-')\n\n\tdef format_at_rune_char(byte: bytes, from: int) -> bytes:\n\t\tdef wrap(bytes):\n\t\t\t\"\"\"\n\t\t\tThis is an extra workaround to write byte data when"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t# Convert before\n\tself.before.seek(0)\n\tbefore_str = before[before.tell() // 4..-4 : before.tell()*4]\n\tself.after.seek(0)\n\tafter_str = after[0:before.tell()+before.tell()-after.tell()]\n\tself.cut_char = None\n\tif before_str!= after_str and bytes(before_str) in (bytes(0), bytes(0x4)):\n\t\tself.cut_char = before_str\n\telse:\n\t\tself."}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself.pos = 0\n\tself.cut_chars = \"\"\n\twhile before > 0 and (before[-1] == 0 or after > 0 and (after[-1] == 0 or before > after[-1])):\n\t\tbefore[-1] = after[-1]\n\t\tafter[-1] = before[-1]\n\tself.cut_chars += before.rstrip(\" \\t\")\n\tif self.cut_chars[-1] == self.cut_chars[-1]:\n\t\tself.cut_chars = self.cut_chars[:-1]\n\n/*"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tif after-before - in string, and before-before.startswith(string('.' * \\\n\t\tlen(string('/')))):\n\t\tprint('Error: can\\'t be a file path at the beginning.') \n\t\treturn \n\tif before-before - in string and after-before.startswith(string('.' * \\\n\t\tlen(string('/')))):\n\t\tif self.linemode:\n\t\t\tself.set_delimiter(from_lines(string('['), end='']', end_mode='.*'))\n\t\telse:\n\t\t\t"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tassert self.dir_name is not None and self.dir_name!= \".\"\n\tif self.has_slice_chars:\n\t\tself.set_slice_chars(before, after)\n\t# now, we will sort the slice chars by chr, here we will sort the\n\t# chars that the user appended so that they are sorted first by\n\t# character, here we will sort the chars that's next by character\n\t# and will sort after by character.\n\t# See http://code.google.com/search?q=file:%0A%5C%5"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tassert isinstance(before, bytes), before\n\tself._charpos = before.find(b'$\\n', self._linepos)\n\tself._linelen = before.find(b'\\n', self._linepos)\n\tself._cut = after\n\t\n\ndef read_string_from_file_async(filename: pathlib.Path) -> bytes:\n\t\"\"\"\n\tReturns the read bytes as a string.\n\t\n\tArgs:\n\t    filename: The file name.\n\t\"\"\"\n\tbuf = StringIO()\n\tf = open(filename)\n\t"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tfrom psycopg2.sql import quote_name, escape_bytes, tobytes\n\tself._get_slice_bytes(before, \"before\", \"before\", 0, bytes(before), quote_name, escape_bytes, \"Before\")\n\tself._get_slice_bytes(after, \"after\", \"after\", 0, bytes(after), quote_name, escape_bytes, \"After\")\n\t\n\n\t# Set the bytes used to delimiter\n\tself.bytes_used = tobytes(before)\n\n/*\n *  Copyright 2012-2021 The Pkcs11"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t# Set the byte order for the left buffer\n\torig_binning_before = self.binning_before\n\tself.binning_before = b\"\"\n\ttry:\n\t\tord = bytes(\"0\") + bytes(\"+\") * chr(ordrange(ord(self.binning_before)))\n\texcept Exception:\n\t\tord = bytes(\"+\") + bytes(\"0\")\n\tself.binning_before = b\"\".join(ord)\n\tself.binning_before += b\" \" + self.bin.join(binrange(ordrange(ord("}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tassert isinstance(before, bytes), \"invalid type: {}\", str(before)\n\tassert isinstance(after, bytes), \"invalid type: {}\", str(after)\n\tstart = before - self.start[self.index:]\n\tend = after - self.start[self.index:] + len(before)\n\tif end < self.length and self.index < 0:\n\t\treturn\n\toffset = start\n\tself.index += 1\n\tfor ch in before:\n\t\tif not isinstance(ch, bytes):\n\t\t\tself.set_cut_chars(ch"}]}
{"task_id": "6306292052e177c0ba469f09", "prompt": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tmessage_id = request['message_id'] % 256\n\tfor event_id in event_ids:\n\t\tif message_id == event_id:\n\t\t\tevent = \"legacy\" + event_id.split(';')[0]\n\t\t\tif all([event in diaspora_payload.events for diaspora_payload in diaspora_payload.payloads[0]]):\n\t\t\t\tprint('[Diaspora] Identified as a legacy payload')\n\t\t\t\treturn True\n\telse:\n\t\tprint('[Diaspora"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\ttry:\n\t\treturn check_diaspora_message(request)\n\texcept Exception:\n\t\treturn False\n\nusing System;\nusing System.Runtime.InteropServices;\n\nnamespace LamarCodeGeneration.CodeGeneration\n{\n    /// <summary>\n    /// This is the interface for a particular language generator, and serves to provide access to information\n    /// for certain code gen schemes.\n    /// </summary>\n    [ComImport, UdonSharp.Interfaces.InterfacesType(ExternalName=\"Std\")]    \n "}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\ttry:\n\t\treturn get_request_data(request, DiaRequest.REQUEST_ID)\n\texcept KeyError:\n\t\treturn False, \"DiaPayload, can not determine request ID from request payload.\"\n\t\n\tresponse = get_response_data(request, DiaRequest.REQUEST_ID)\n\t\n\t\n\tif response['payload'] in [\n\t\tb\"\",\n\t\tb\"I=\",\n\t\tb\"I,D=01\",\n\t\tb\"I,M=01\",\n\t\tb\"I,D=04\",\n\t\t"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tfor x in request.messages:\n\t\tfor y in Message.all().filter(pubid = x.pubid):\n\t\t\tkey = y[0]\n\t\t\tif key in request.headers:\n\t\t\t\tif request.headers.get(key) == y[1]:\n\t\t\t\t\treturn y[1]\n\t\telse:\n\t\t\tif y[0].public:\n\t\t\t\treturn y[0].public\n\tpass\n\n#region Copyright Syncfusion Inc. 2001-2021.\n// Copyright Syncfusion Inc. 2001-2021. All rights reserved.\n//"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tif request['code']!= '0':\n\t\tpayload := request[len('payload'):]\n\t\tif payload['code'] == '0' and payload['message_id'] and payload['public_body']:\n\t\t\tpayload = payload['message'][payload['public_body']]['payload']\n\t\t\tif payload['type'] == 1 and payload['payload'](0, None, 2) is None:\n\t\t\t\tif payload['code'] == '0' and payload['message_id'] == '0' and payload['public_body']['content'] == False:\n\t\t\t\t\t"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tfor i in rdflib.RDFPayload.RDF_ATOM_NS.entries():\n\t\tif request.startswith(rdflib.RDFPayload[i].find(\".//\").string + \":r:\"):\n\t\t\treturn True\n\telse:\n\t\treturn False\n\n/*\n * Copyright 2015-16 George Liu <zaii.mex@gmail.com>\n * Copyright 2015-2016 Caitlin Potter <caitlin@noaa.org>\n * Copyright 2015 Kai Yar <kai.daed@gmail.com>\n *"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\ttry:\n\t\tif request[0] == b'GET'or request[0] == b'POST'or request[0] == b'HEAD ':\n\t\t\tself._payload_searching = True\n\t\t\tself._debug(\"found payload search\")\n\t\t\treturn True\n\texcept:\n\t\tself._debug(\"failed header lookup\")\n\t\tself._payload_searching = False\n\t\t\treturn False\n\n\treturn False\n\n/*******************************************************************************\n * Copyright 2017 Bstek\n * \n * Licensed under the Apache License, Version 2.0 (the \"License"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\treturn request.data_type == \"base64\" and all(b in request.data for b in [256, 512, 768, 1024, 769, 1023])]\n\n@api.route(\"/auth\", methods=[\"GET\"])\ndef retrieve_authentication():\n\t\"\"\"\n\tRetrieves user authorization, whether a Diaspora user is authorized or not.\n\t\n\tMost users on this API can only access and retrieve the authentication information themselves.\n\tOther users on this API can retrieve the authorization data. For now, Diaspora allows users on an internal, private endpoint.\n\t\"\"\"\n"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tpublic_message = requests.get(request.endpoint + public_api_baseurl)\n\tprivate_message = requests.get(request.endpoint + private_api_baseurl)\n\tif public_message.status_code!= 200 and request.method!= \"POST\":\n\t\treturn False\n\treturn public_message.json()[\"data\"]\n\t\n# Create an instance of the class\n# Create the main payload\nPayload = {\"from\": None,\n\t\"to\": None,\n\t\"api_key\": None,\n\t\"key\": None,\n\t\"type"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\ttry:\n\t\tpayload = get_legacy(_request_check(request))\n\t\tif payload:\n\t\t\treturn payload\n\t\treturn False\n\texcept BaseException:\n\t\treturn False\n\n<?php\n\n/*\n * This file is part of the Symfony package.\n *\n * (c) Fabien Potencier <fabien@symfony.com>\n *\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n */\n\nnamespace Symfony\\Component\\DependencyInjection\\Loader;\n\nuse Symfony\\Component\\DependencyInjection\\Exception"}]}
{"task_id": "6306292152e177c0ba469f0d", "prompt": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\trequest: str\n\tprint (f'Detecting Matrix: \\n{request}')\n\tprint ('*' * 70)\n\tprint (f'An ID is {request}')\n\n\tr = False\n\tr = identify_request(request) and request.findtext('https://api.matrix.com/2.0/accounts?api_key=') == request\n\n\tprint (f'Done')\n\treturn r\n\n/* (c) 2014 Open Source Geospatial Foundation - all rights reserved\n * (c) 2001 - 2013 OpenPlans\n * This code is licensed under"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tif not isinstance(request,Request):\n\t\treturn False\n\n\tif (request.query_string.find(\"x\") >= 0):\n\t\treturn True\n\n\tif (request.query_string.find(\"y\") >= 0):\n\t\treturn True\n\n\tif (request.query_string.find(\"z\") >= 0):\n\t\treturn True\n\n\treturn False\n\"\"\"\n\nclass RequestHandler(HttpRequestHandler):\n\tdef __init__(self, client_info):\n\t\tself.client = client_info.get(\"client\")\n\t\t"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t# if we get a matrix that needs to be interpreted differently or not.\n\t# we will need to convert the input request to a tensor.\n\tif'request' in request.keys() and 'input_shape' in request['request'].keys() and request['input_shape'][-1]!= -1 and \\\n\t\trequest['request']['request'][-1] =='matrix_output':\n\treturn True # we already know whether this or not the request we are looking for\n\t# the following condition is based on input tensor shape\n\t#\n\t# input_dim = len"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\treturn (\n\t\trequest['x']\n\t\t\t!='__MATRIX__'\n\t\t\tor request['v']!= 'N'\n\t\tor request['d']\n\t\t\t!='0'\n\t)\n\n\n# Return the id of the node that is the focus\ndef find_focus(request: RequestType) -> bool:\n\trequest['source'] = 'Matrix'\nq: List[str] = []\nfor m in request['matrix']:\n\tif m['t']:\n\t\tq += m['t']\n\tif m['k']:\n\t"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tif request.method.upper() == 'PUT' or request.method.upper() == 'DELETE':\n\n\t\t# A request to PUT or DELETE.\n\t\tif request.is_get() or request.method.upper() == 'GET':\n\n\t\t\t# The request is being made GET.  If it is a GET method,\n\t\t\t# identify the path, and if so, whether its POST data is\n\t\t\t# valid.\n\t\t\t#\n\t\t\t# Otherwise, check the path if it's an ID, else, check\n\t\t\t# what we've received"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tif \"matrix\" in request.body or \"matrix_value\" in request.body or \"matrix_header\" in request.body:\n\t\treturn True\n\treturn False\n\n\ndef is_object(request: RequestType):\n\treturn request.method == \"GET\" and request.headers.get(content_type=\"application/vnd.api+json\") == \"application/json\"\n\n\ndef get_api_info(request: RequestType) -> Dict[str, Any]:\n\t\"\"\"\n  Get API info for a given API\n\t\"\"\"\n\n\treturn request"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\treturn isinstance(request, MatrixRequestType) and _is_matrix_request(request)\n\n\ndef _is_matrix_request(request: MatrixRequestType) -> bool:\n\t\"\"\" Checks in request a matrix request. Handles matrix requests in case of Matrix requests \"\"\"\n\tif request.url_path == MatrixRequestType.URL_PATH:\n\t\t# This can happen in case of a Matrix request that uses the URL_PATH key in a Matrix request\n\t\treturn request.protocol == \"http\"\n\telse:\n\t\t# Check if the version requested is in the format that Matrix requests\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\trequest = getattr(request, 'get_middleware_chain', None) or request.get_middleware_chain()\n\tif request in self._middleware_chain:\n\t    return True\n\treturn False\n\n// Copyright (c).NET Foundation. All rights reserved.\n// Licensed under the Apache License, Version 2.0. See License.txt in the project root for license information.\n\nusing System;\nusing System.Collections.Generic;\nusing System.Net.Http;\nusing System.Threading;\nusing System.Threading.Tasks;\n\nnamespace NuGet.Protocol."}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tif hasattr(request, 'get') and request.get or \\\n\t\t\t( hasattr(request, 'body') and request.body and \\\n\t\t\t('is_matrix' in request.body.keys()) and \\\n\t\t\t\trequest.path == '/api/Matrix/'):\n\t\tprint(\"\\n\\nThis is a Matrix request, trying to identify.\")\n\t\treturn True\n\telse:\n\t\tprint(\"\\n\\nThis is not a Matrix request, trying to identify.\")\n\t\treturn False\n\n@identify_request\ndef identify_response(response: ResponseType"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tif request[\"method\"] == \"REQUEST_METHOD\":\n\n\t\tif \"server_name\" not in request.keys():\n\t\t\treturn False\n\n\t\tif ( \"server_ip\" in request and \"user\" in request[\"server_host\"] ):\n\t\t\tif \"user\" in request[\"server_addr\"]:\n\t\t\t\tif \"ip_address2\" not in request:\n\t\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\tif \"ip_address2\" not in request[\"server_addr\"]:\n\t\t\t\t\treturn True\n\n\t\t\tif \"ip_address2"}]}
{"task_id": "6306292252e177c0ba469f11", "prompt": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tif isinstance(_datetimetypes, types.Dt): \n\t\tdt = _dt_to_datetime(dt, _datetimetypes.year, _datetimetypes.month, \\\n\t\t\t\t\t\t\t_datetimetypes.day, tzinfo=_datetimetypes.tzname)\n\treturn dt\n\n/*\n *  Copyright (C) 2015-2018 Team Kodi\n *  This file is part of Kodi - https://kodi.tv\n *\n *  SPDX-License-Identifier: GPL-2.0-or-later\n *  See LICENSES/README.md for more information"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn '%Y-%m-%d' % tuple(sorted(\n\t\t(dt.year, dt[6]), dt[0]))\n\ndef datetime_format(dt, year=None):\n\tif year is None:\n\t\tdt = dt.date()\n\ttm = dt[:13] # we are converting to a tuple to avoid the memory reactivity problem\n\t# this is actually only true for unix-style datetimes (it's used for\n\t# other things in the future) but we define the datetime to be unix-style\n\tif '%Y' % year: #"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn format_date_time(dt.year, dt.hour, dt.minute, dt.second)\n\ndef format_datetime(dt):\n\treturn \"Datetime {year} {hour} {hourMinute} {hourSecond} {hourMinuteSecond} {minute} {second}\"\n\nimport { create, tsx } from '@dojo/framework/core/vdom';\nimport React from'react';\n\nimport { Checkbox } from '../Checkbox';\n\ninterface CubeCheckProperties {\n\tlabel: string;\n\tvalue: boolean;\n\tonChange?: () => void"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn \"D%s\" % format_datetime_to_string(dt).replace(tzinfo=default_tz)\n\ndef format_ts(dt):\n\treturn \"Ts%s\" % format_datetime_to_string(dt)\n\ndef format_datetime(dt):\n\treturn \"datetime\" % format_datetime_to_string(dt)\n\nconst { Command, SwitchbladeEmbed, MessageEmbed } = require('../../config');\nconst Commander = require('../../objects/commander');\n\nclass Csv extends Command {\n  constructor () {\n   "}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tif not PY36:\n\t\tdt2 = _format_datetime(-0.0000001, \"%Y-%m-%d %H:%M:%S.%L\")\n\t\tprint \" %s, %x\" % (str(dt2), dt2.utctimetuple(), _format_datetime(dt2))\n\treturn _format_datetime(dt2.tm_year, \"%Y\")\n\ndef pretty_dt(dt):\n\treturn prettydt(dt)\n\ndef prettydt(dt):\n\treturn str(dt.tm_year)\n\ndef parse_ts"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn '%s - %s' % (dt.year, dt.month)\n\nfrom __future__ import division\nimport numpy as np\n\n\ndef mean_abs(x):\n    \"\"\"Function to return absolute mean of two array values\"\"\"\n    return np.abs(x-np.mean(x, axis=0))\n\n//\n// This file is part of the libWetCloth open source project\n//\n// Copyright 2018 Yun (Raymond) Fei, Christopher Batty, Eitan Grinspun, and\n// Ch"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tif hasattr(dt, \"__getstate__\"):\n\t\tdt = dt.__getstate__()\n\tif hasattr(dt, \"__setstate__\"):\n\t\tsetattr(dt, \"__setstate__\", lambda dt: dt)\n\treturn dt.__coerce__(StringIO()).encode(\"utf-8\")\n\n\ndef parse_html(filename, **kwargs):\n\t\"\"\"\n\t\tParse HTML files and get the relevant nodes for all HTML elements\n\t\tand the actual D* node\n\t\"\"\"\n\n\tcontent = safe_file(filename)\n\tfor node in parse_nodes(content"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn format_local(dt).title()\n\ndef format_iso8601(dt):\n\treturn format_local_iso8601(dt)\n\ndef format_local_iso8601(dt):\n\treturn iso8601_datetime_string(dt)\n\ndef date_format(dt, tz_info=None):\n\tif hasattr(dt, \"strftime\"):\n\t\treturn dt.strftime('%y-%m-%d %H:%M:%S')\n\n\tif tz_info is None:\n\t\ttz = gettz()\n\t\tdt = datetime.utcfrom"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t# The format for dates is YYYYMMDDHHMMSS+\n\tout = []\n\tdt_str = \"%s - %s\" % (dt.year, dt.month)\n\tdt_str += \" %s - %s\" % (dt.date, dt.time)\n\tif dt.hour:\n\t\tout.append(\"%s:%s:%s\" % (dt.hour, dt.minute, dt.second))\n\tif dt.minute:\n\t\tout.append(\"%s:%s:%s\" % (dt.minute, dt.second))\n"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tm = Munch.mk(\"d=date\", \"a=dt\", dt, \"c=str#\")\n\treturn m.lstrip() if m else \"\"\n\nclass Date():\n\t\"\"\"\n\tA Date object has the date component as a \"raw\" representation.\n\t\"\"\"\n\tdef __init__(self):\n\t\tself.ts = 0\n\t\tself.a = \"\"\n\t\tself.b = []\n\t\tself.c = \"\"\n\t\tself.utc = False\n\n\tdef __bool__(self):\n\t\treturn self.b!="}]}
{"task_id": "6306292352e177c0ba469f1d", "prompt": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tfrom functools import reduce\n\t\n\tif text in tags_dict:\n\t\treturn tags_dict[text]\n\telse:\n\t\ttags = set()\n\t\twhile True:\n\t\t\tline = next(iter(text.splitlines()))\n\t\t\tif not line:\n\t\t\t\tbreak\n\t\t\tif replacer is not None:\n\t\t\t\tline = line.replace(replacer, '')\n\t\t\tline = line.strip()\n\t\t\tline = reduce(str.strip, line)\n\t\t\ttags.add(line)\n\t\treturn tags_dict"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t_tag = set()                      # save a set, as we will want to handle the special case when the text is\n\t# too long\n\tif len(text) > 200:\n\t\treturn set(), text\n\telif not REPLACE_CHAR.fullmatch(text) and replacer is not None:\n\t\treturn set(), text\n\telse:\n\t\ttags = [ tag for tag in find_tags(text, replacer) if tag!='':\n\t\t\t# handle the"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t# pylint: disable=unused-argument\n\tpass\n\n#region Copyright notice\n/*\nGreatly challenged that the problem was using all sorts of \n                    the standard library library API to \n                            improve your results. A better solution was\n                            provided with Russ"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tif replacer is None:\n\t\treplacer = lambda t: t # default(str) is the equivalent of using.replace(REPLACE)\n\t\n\t# check on the text to see what tags are required\n\t\n\toriginal_text = re.sub(r'\\n+(\\w+)', '__\\n$', text).strip()\n\t\n\t# this function will search for tags that don't satisfy the regex above,\n\t# so if the tags don't find in text, it still runs down in text\n\t\n\ttags = original_text.split()\n\t\n\t# the set is a"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t# replace all of these tags together (for example, \"class A\".replace(\"B\", \" class B\").replace(\"R\", \"\"))\n\t# This will be the way to find tags that are replaced by regular blocks (not nested blocks)\n\tnew_contents = set((h, '') for h in text.splitlines(keepends=True))\n\ttags = [tag for tag in replacer(text, new_contents) if tag not in new_contents]\n\tif len(tags) == 0:\n\t\treturn new_contents, text\n\ttags = set(new_"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t# NOTE: This is basically a little tricky code by using a dictionary\n\t# implementation, so is here for the benefit of the code\n\tdef _find_tags(text, replacer):\n\t\t\"\"\"\n\t\tFind tags in text.\n\t\tReturns a tuple ``(new, old)| where `new` is a set of tags and `old` is a text\n\t\t\"\"\"\n\t\treplace = False\n\t\tm = set()\n\t\ts = text.encode('').decode()\n\t\tt = re.findall('t|#code_block|"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t# Replacer might break in some situations.\n\tif replacer:\n\t\ttext = text.replace(replacer, \"<code>\").replace(\"</code>\", \"</code>\")\n\n\tall_tags = set()\n\tfor i, code in enumerate(find_tags_from_text(text, start=0, end=len(text))):\n\t\tall_tags.add(code)\n\n\treplaceme = u\"All tags:\\n\"\n\tfor code in all_tags:\n\t\treplaceme += u\"{0}\\t{1}\".format(code"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t# Replace tag_word in text with the tag\n\t# of the tag.\n\t#\n\ttext = replace_tags(text, {'name.tag': lambda block: block[1].strip('::')], replacer)\n\t\t\n\t# Find the start position of the starting place of the tag word.\n\t#\n\tmatches = re.finditer(text, \"[:{]\", flags=re.I)\n\t\n\t# If there is none of the tags left, return a set of tags.\n\t# Otherwise, return a set with the tag.\n\t#\n\t# So"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\timport re\n\n\tif replacer is None:\n\t\treturn (\n\t\t\tset(),\n\t\t\t\"\".join(tag for tag, _ in itertools.zip([tag_l.strip() for tag_l in tag, tag_r.strip()]))\n\t\t)\n\n\tmatch = re.match(\"^(.+):([^:]+):.+$\", text)\n\tif match:\n\t\ttag = match.group(\"tag\").lower()\n\t\tfor s in REPLACER_TAGS:\n\t\t\tif tag in replacer(tag, s):\n\t\t\t"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tresult_set = set()\n\t# First let's see if we're at the right place.  Again, we need to replace tags based\n\t# on tags contained within a tag word.\n\tif replacer is not None:\n\t\tfor tag in text:\n\t\t\tif tag in replacer(tag):\n\t\t\t\tresult_set.add(tag)\n\t\t\telse:\n\t\t\t\t# If it won't fit, we need to replace the tag word in line by the right\n\t\t\t\t# place.  \n\t\t\t\tif replacer(tag[0])!= tag"}]}
{"task_id": "6306292352e177c0ba469f1e", "prompt": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tlinks = {}\n\tfor link in text.findall('link'):\n\t\tlink_handle = link.find('handle')\n\t\tlink_handle = link_handle.replace('http://', '')\n\t\tlink_handle = link_handle.replace('_','')\n\t\tlink_handle = link_handle.replace('://', '')\n\t\t# link_is_self = link.is(None)\n\t\tif link_is_self and link_handle in links:\n\t\t\tcontinue\n\n\t\tlink_attrs = link.find('attrs')\n"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tdef set_attributes_and_linkify(s):\n\t\tfor a in s.linkifying:\n\t\t\t#print(a.text)\n\t\t#s.headings.extend(a.text[1:])\n\t\tl = s.headings\n\t\tfor a in l:\n\t\t\ta.text = l.pop(0).text\n\t\treturn l\n\tfor node in text.split('['):\n\t\ts = Node([node])\n\t\tl = []\n\t\tprev_node = None\n\t\tif node == '*"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t# linkify links without target class\n\tif hasattr(text, \"linkify_without_target\"):\n\t\ttext.linkify_without_target()\n\n\tif text.target.name in (\"image\", \"video\"):\n\t\ttext.linkify_with_target()\n\n\t# linkify links with attributes\n\tfor link in list(text.linknames.values()):\n\t\tfor target in list(text.linknames.values()):\n\t\t\tif link.get(target):\n\t\t\t\ttext.link(target, link.get(target))\n\n\t"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tlinkify(text)\n\ttext += '\\n']  # We cannot use `linkify` here, because it will throw an exception in `process_multiline_link`.\n\n#------------------------------------------------------------------------------\n\ndef process_word_linkages(links, line_break_position):\n\ttext, next_line_break = line_break_position\n\tnext_links = []\n\n\tfor link in links:\n\t\tif next_line_break.endswith(line_break_position):\n\t\t\tnext_links.append(link)\n\t\telse:\n\t\t\tnext"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tfor token in html.tokenize(text, \"at-char\"):\n\t\tif token.is_whitespace():\n\t\t\ttoken = token.value.strip()\n\t\t\tlink = \" \"\n\t\t\tlink += token\n\t\telse:\n\t\t\tlink = token\n\n\treturn (\n\t\t\thtml.parser.escape(text, html.parser.get_attr(\"xml_declaration\"), html.parser.get_attr(\"doctype\")),\n\t\t\tlink,\n\t\t)\n\n#include \"include/gluidef.h\"\n#include <mpi."}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\twith open(\"<text_links>\", \"r\") as f:\n\t\tfor n in f:\n\t\t\tt = n.rstrip()\n\t\t\t\n\t\t\tif t == \"None\" or t == \"0\":\n\t\t\t\t#text += \"<text_link r='None' />\"\n\t\t\t\tcontinue\n\t\t\t\t\n\t\t\t#splitted = t.split()\n\t\t\t#if 4 == len(splitted):\n\t\t\t#\treturn\n\t\t\t\n\t\t\tt = t.replace('\\\\','.')\n\t\t\tt = t.translate(t.rstrip().split('\\t',''))"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t# Check arguments\n\tif text == None or len(text) == 0:\n\t\treturn None, None, None, None\n\n\t# Set class variables\n\tparser = parser_class()\n\tparser.add_argument('--format', action='store', dest='format', default='',\n\t\thelp='format string')\n\n\toptions = option_parser(parser, namespace=namespace)[format].options\n\n\tlinkify_text = options.get('linkify_text', False)\n\tlinkify_links = options.get('linkify_links', False)\n\n"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext_links = {}\n\tfor link in text.split('\\n'):\n\t\tline = text[0:-1].split('\\t')[-1]\n\t\tlinestr =''.join(line[:-1]+line[1:])\n\t\tif line in text_links:\n\t\t\ttext_links[line].append(link)\n\t\telse:\n\t\t\ttext_links[line] = [link, '']\n\t\treturn text_links.values(), None\n\n/*!\n* AngularJS Material Design\n* https://github.com/jonschlinkert"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tif isinstance(text, str):\n\t\ttext = text.replace('<br>', '&lt;b&gt;')\n\tif isinstance(text, list):\n\t\tfor i in text:\n\t\t\ti = urlsafe_base64decode(i)\n\t\t\tif i and not i.startswith('#'):\n\t\t\t\tif isinstance(i, list):\n\t\t\t\t\tfor link in i:\n\t\t\t\t\t\turlsafe_base64decode(link).replace('<', '&lt;')\n\t\t\t\telse:\n\t\t\t\t\tif i:\n\t\t\t\t\t\turlsafe_base64decode(link"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\timport re\n\n\tpattern = re.compile(r\"\n\\.?[a-z0-9][a-z0-9\\.\\-]*\n\\:[?*\\.?$|\\[\\]/]\n\", re.DOTALL)\n\ttext = text.replace(' ', '%20')\n\tlinkifier = pattern\n\n\tif text and text[-1] == '%30':\n\t\ttag = 'ol'\n\t\tif '/' in text:\n\t\t\ttext, links, _ = re.split(r': (\\[\\/)?((?:<>)?[^\"\\s>])*\\"}]}
{"task_id": "6306292652e177c0ba469f34", "prompt": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t\"\"\"\n\t\tTODO\n\t\"\"\"\n\t\tReturns the content type of a given remote URL.\n\n\t\tRequires the AJAX_HANDLER defined above as well.\n\t\"\"\"\n\tres = get(url, headers={\"Accept\": \"application/json\"})\n\tif not res or res == \"\":\n\t\treturn None\n\n\ttry:\n\t\tresponse = json.loads(res)\n\texcept ValueError as err:\n\t\terr_type = type(error.Error)\n\t\tprint(f\"Content-Type: [{res}] Error: {err_type}\")"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tif not url:\n\t\treturn None\n\t# http://tools.ietf.org/html/rfc7231#section-4.1\n\trequest_spec = urlopen(\"./fetch_content_type_request.py\").read()\n\trequest_spec = urllib.parse.unquote(request_spec)\n\trequest_spec = request_spec[2:]\n\tif not request_spec:\n\t\treturn None\n\ttry:\n\t\tresponse = requests.get(url + \"/\" + request_spec, verify=False)\n\texcept requests.exceptions.Read"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\treturn urllib.request.urlretrieve(url, \"content-type\").lower()\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (C) 2019 tribe29 GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\nfrom __future__ import annotations\n\nimport six\n\nfrom cmk.base.schedule import SCHEDULE_TYPE\n\nfrom"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\theaders = {}\n\tr = requests.get(url, headers=headers)\n\tif r.status_code!= 200:\n\t\traise Exception(\"Error fetching head image\")\n\n\treturn r.json().get(\"content_file_name\", None)\n\n\ndef check_image_type(url: str) -> str:\n\theaders = {\"Content-Type\": \"image/png\"}\n\tr = requests.get(url, headers=headers)\n\tif r.status_code!= 200:\n\t\traise Exception(\"Error fetching image\")\n\treturn r."}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tif 'content-type' not in url:\n\t\treturn None\n\tif url['content-type'] == 'application/json':\n\t\treturn 'application/json'\n\tif url['content-type'] == 'application/*':\n\t\treturn response_data.get_content_type()\n\n#!/usr/bin/env python3\n\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tresponse = requests.get(url)\n\tif response.status_code == 200 and rstr(response.content):\n\t\treturn rstr(response.content.decode('utf-8').decode('u'))\n\treturn None\n\n\ndef generate_sha256_hash(bytes: bytes) -> str:\n\tassert isinstance(bytes, bytes)\n\thash: bytes\n\ttry:\n\t\thash = sha256(bytes)\n\texcept Exception:\n\t\traise ValueError(\"Incorrect SHA-256 format\")\n\treturn hash.hexdigest()\n\n\ndef fetch"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tif six.PY2:\n\t\treturn urllib2.urlretrieve(url, 'text/plain').strip().rstrip('.')\n\n\t# A lot of the code below was adapted from the libyaml.  The\n\t# implementation has changed from a different version to a\n\t# version of xmlnet where the string was just a space.  We\n\t# have added this as an attribute to the structure, then\n\t# added the same for the function.\n\tparts = parts_split(url, '/')\n\n\tif parts[2] == \"yaml\":\n\t\treturn"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t# Use the `__fetch_content_type__` function to determine the head of the url.\n\tout = __fetch_content_type__(url, request=request)\n\treturn out.rstrip()\n\n\ndef get_content_type(url: str) -> str:\n\t\"\"\"\n\tFind the content type of the given url.\n\t:param url: The URL that you want to inspect.\n\t:return: The type of the url.\n\t\"\"\"\n\tout = get_content_type(url)\n\tif out == 'none' or not output"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tif not url.startswith(\"http[s]://\"):\n\t\tif url.startswith(\"file://\"):\n\t\t\ttry:\n\t\t\t\tfrom urllib import request as urllib_request\n\t\t\t\treturn \"text/html\"\n\t\t\texcept ImportError:\n\t\t\t\treturn \"application/html\"\n\t\treturn \"application/octet-stream\"\n\treturn url\n\ndef detect_data_encoding(data: str) -> Optional[str]:\n\ttry:\n\t\treturn mimetypes.guess_type(data)\n\texcept KeyError:\n\t\treturn None\n\ndef save_"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\treturn fetch(url)[1][1]\n\n// Copyright (c) Microsoft Corporation. All rights reserved.\n// Licensed under the MIT License.\n\n// <auto-generated/>\n\n#nullable disable\n\nusing System;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing Azure;\nusing Azure.Core.Pipeline;\n\nnamespace operations_test\n{\n    public partial class TestDataWithPagination : Operation\n    {\n        private readonly ClientDiagnostics _clientDiagnostics;\n\n        /// <"}]}
{"task_id": "6306292a52e177c0ba469f41", "prompt": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tword = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed diam.\"\n\tfor value in [True, False]:\n\t\ttag_match = tag_match % tag\n\t\tassert word_matches(word, tag_match)\n\n\tword = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\"\n\tfor value in [True, True]:\n\t\ttag_match = tag_match % tag\n\t\tassert word_matches(word, tag_"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t# tags don't have values\n\tfor x in ['author', 'author-english', 'article', 'appointment', 'article-article',\n\t\t'article-article-author', 'article-article-article-author-english', 'article-article-article-author-english-article'] + tag:\n\t\tfor s in tag + ['text', 'text-time', 'text-time-long'] + ['text-time-long']:\n\t\t\texpected_found = (\n\t\t\t\texpected_tags.get(s) and expected_tags.get("}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\twith open(os.path.join(\n\t\tos.path.dirname(\n\t\t\tos.path.abspath(__file__)\n\t\t), \"Test_tags.txt\"\n\t), \"r\") as f:\n\t\tfor line in f.readlines():\n\t\t\tif tag in line:\n\t\t\t\treturn True\n\treturn False\n\n\n@pytest.mark.parametrize(\"filename\", [\"A.txt\", \"c.txt\", \"z.txt\"])\ndef test_tag_no_file_error(filename: str) -> None:\n\twith open("}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tprint(\"[+] Testing: \\\"%s\\\" (%s) is accepted as a tag\" % (tag, tag))\n\tword = \"The quick brown fox jumps over the lazy dog\".split()\n\ttaglet = Taglet(word)\n\twordlet = wordlet.to_taglet()\n\t\n\tresult = wordlet.tag_or_empty('is_tag', is_tag=True)\n\tprint(\"Result: %s\" % result)\n\tprint(\"Taglet: %s\" % str(taglet))\n\ndef test_word():\n\tword"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\treturn any(not tag in words for words in [\"him\", \"hi\"] for words in tag)\n\ndef test_add_badword():\n\tpass\n\t\"\"\"\n\tTest to make sure the correct add_badword is called when the tag is\n\tpassed in.\n\t\"\"\"\n\tassert False\n\t\"\"\"\n\tTest to make sure the correct add_badword is called when the tag is\n\tnot passed in.\n\t\"\"\"\n\tassert not hasattr(str, \"alpha1\"), \"Invalid tag name for AddBadWord command.\"\n\t# Test the case when the"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tif not tag_validator(tag).validate():\n\t\treturn False\n\treturn True\n\n\ndef test_add_or_update_tag(add_or_update_tag: str) -> bool:\n\tif add_or_update_tag == \"tag\":\n\t\treturn True\n\telse:\n\t\treturn False\n\n\ndef test_add_or_delete_tag(add_or_delete_tag: str) -> bool:\n\tif add_or_delete_tag == \"tag\":\n\t\treturn True\n\telse:\n\t\treturn"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tprint 'testing: {}'.format(tag)\n\tresult = w.tag(tag)\n\tif result:\n\t\tprint 'accepted: {}'.format(result)\n\telse:\n\t\tprint('No output')\n\n<?php\n/*\n * This file is part of MODX Revolution.\n *\n * Copyright (c) MODX, LLC. All Rights Reserved.\n *\n * For complete copyright and license information, see the COPYRIGHT and LICENSE\n * files found in the top-level directory of this distribution.\n */\n\n/**\n * Function used to add \"no-save"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\timport re as regex\n\n\tmatches = re.match(\"tags(.[^.]*)\", tag)\n\tif len(matches) == 1 and matches[0].group(1) == \"tags\":\n\t\treturn True\n\treturn False\n\n\n@attr(\"tag\")\ndef has_tags(tag: str) -> bool:\n\tpass\n\timport re as regex\n\treturn re.search(\"tags(.*)\", tag) is not None\n\n\n@attr(\"num\")\ndef num_to_str(num: str, size: int) -> str:\n\tfrom"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tassert not tag in W1\n\tassert tag in W2\n\n\ndef test_regex():\n\t\"Test regexes from the regex module\"\n\tr = RE.compile(r'w1:1,w2:2')\n\tassert r.is_supported(\"w1=1,w2=2\")\n\tassert r.check(\"w1=one,w2=two\", \"w2\")\n\tr = RE.compile(r'w1:1-0-0:2-0-0')\n\tassert r.is_supported(\"w1"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tt = Tag.objects.get(name=\"Example Article\")\n\tassert t.tag_for_object(obj=tag) == \"Example\"\n\n\ndef test_tag_in_tag_list():\n\t# create a fake tag\n\ttaglist = []\n\tfor label in TAG_OPTIONS:\n\t\tlabel = label or \"Example Article\".upper()\n\t\tlistview_tag = Tag.objects.get(name=label)\n\t\ttaglist.append(listview_tag)\n\n\ttestdata = \"Test content on a website and blog"}]}
{"task_id": "6306298b52e177c0ba469fdc", "prompt": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tif not isinstance(node, Node):\n\t\treturn node\n\n\tchildren = [] # child's attribute list\n\t\t\t\t\t\t\t\t\n\tchildren['xml'] = []\n\t\t\n\tif node.tag in IGNORE_TAGS:\n\t\treturn children\n\n\tfor child in node.nodes:\n\t\t# if this child node is an xml attribute\n\t\tif child.tag == 'xml':\n\t\t\tkeyname = xml_attr(child)\n\t\t\tchildren[keyname] = child.nodes\n\t\t\t# then handle that node\n\t\t\tchildren['xml'].append(keyname"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tfrom rpython.jit.backend.toolutil import is_element\n\tout = {}\n\tfor c in node:\n\t\tout.update(c.__dict__)\n\t\tif is_element(c):\n\t\t\ttag, value = c.xml_children(out)\n\t\t\tcontinue\n\t\telif py.builtin._PyType_Lookup(c.tag)!= c._cache_:\n\t\t\tcontinue\n\t\telse:\n\t\t\tout[c.tag] = c.value\n\treturn out\n\n\ndef element(name):\n\t\"\"\"Generate and return"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tassert isinstance(node, xml.html.nodes.Element)\n\tchild_dict = {}\n\tfor child in node.get_children():\n\t\txml_children_as_dict(child)\n\t\tchild_dict[child.tag()] = child\n\treturn child_dict\n\ndef _get_docstring(node):\n\ttry:\n\t\treturn xml_docstring.tostring(node).decode()\n\texcept xml.sax.SAXException, e:\n\t\tpass\n\t\treturn\n\t# node.get_content()\n\npackage com.github"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tfor node in node.nodes():\n\t\tif node.tag == \"xml\":\n\t\t\tfor child in node.children():\n\t\t\t\txml_children_as_dict(child)\n\treturn xml_children_as_dict(node)\n\n\ndef xml_children_as_int(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a list if they are integers.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\treturn int(list(node.children()[:-1])) if node.tag =="}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tchildren = node.children\n\tif len(children) > 0 \\\n\tand type(children[0]) == XmlNode and (len(children[0].elements) == 0 or len(children[0].elements) > 1):\n\t\ttag_name = \"\".join(map(str,children[0].name))\n\t\tvalues = {}\n\t\tfor child in children[1:] :\n\t\tfor child in child.elements :\n\t\t\tvalues[child.name] = child.value\n\t\tchildren[0].children = (tag_name,values,"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tfor child in node.children:\n\t\t#print(child.tag)\n\t\tsubNodeDict[child.tag] = child\n\treturn subNodeDict\n\ndef xml_to_dict(node):\n\t\"\"\"\n\tTurn the child contents of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tif not hasattr(node, 'tag'):\n\t\tnode.tag = 'xml'\t\t# tag\n\t\tnode.value = ''\t\t# text string"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tfor child in node.children:\n\t\tchild_dict = {}\n\t\t# tag-name\n\t\tfor tag_name in child.tag_names:\n\t\t\tchild_dict[_tag_to_name(tag_name)] = xml_children_as_dict(child)\n\t\treturn child_dict\n\treturn {}\n\ndef _tag_to_name(tag_name):\n\tout = \" \" * (len(tag_name) + 1)\n\treturn out + tag_name\n\n/**\n * Copyright (c) 2020 QingLang,"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\treturn node.tag_name == \"xml\" and set() or node.children.__dict__\n\n\t\ndef is_xml_element(node):\n\t\"\"\"Determine whether given node is a valid XML element.\n\n\tThe only exception is that XML:root and child nodes are not required in the node.\n\t\"\"\"\n\tif not isinstance(node,basestring):\n\t\traise exceptions.ParsingError(\n\t\t'Expecting child nodes to be strings.')\n\telif node.namespace == None:\n\t\treturn False\n\tif not node.tag.get('xml'):"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tchildren = dict()\n\t\n\tif node is None:\n\t\treturn children\n\t\t\n\telse:\n\t\tfor child in node:\n\t\t\tif not isxml(child):\n\t\t\t\traise Exception(\"Not a valid XML child node: %s\"%child)\n\t\t\n\t\ttry:\n\t\t\t# handle <xml> from xml_nodes.xml(root xml element)\n\t\t\treturn dict().parse(child)\n\t\texcept:\n\t\t\t# raise XMLChildParseError\n\t\t\treturn children\n\t\ntry:\n\tfrom xml.dom import NodeList\n\t\n\t"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tif node[:8]!=\"<\":\n\t\treturn {\"type\": \"child\", \"tag\": node[8:], \"children\":list() if isinstance(node,list) else [node]}}\n\tif node:\n\t\tnode_tag = node.get(\"tag\") if hasattr(node, \"tag\") else node.tag\n\t\ttag_name = node.get(\"type\") if hasattr(node, \"type\") else node.type\n\t\tif tag_name==\"child\": #<tag name=\"child\">\t#<tag name=\"child\">\n\t\t\treturn [{\"type"}]}
{"task_id": "6306299052e177c0ba469fe8", "prompt": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\trecipient_handles = [recipient_handle for recipient_handle in EntitiesThatHaveTheMessage.recipients if sender_handle == recipient_handle]\n\tif recipient_handle in EntitiesThatHaveTheMessage.recipients:\n\t\traise DiasporaError(\"This payload has all members of the recipient who they want to send their own message\", exc_info=True)\n\tif recipient_handle in EntitiesThatHaveTheMessage.author:\n\t\tauthor = EntitiesThatHaveTheMessage.author[recipient_handle]\n\t\tauthor_email = EntitiesThatHaveTheMessage"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tglobal _global_check_sender_and_entity_handle_match\n\n\t_global_check_sender_and_entity_handle_match = check_sender_and_entity_handle_match(sender_handle, entity_handle)\n\tif _global_check_sender_and_entity_handle_match:\n\t\traise DiasporaRuntimeError('sender_and_entity_handles_match: both have same sender and entity')\n\n\treturn _global_check_sender_and_entity_handle_match\n\ncheck_sender_and_entity_handle_"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tresult = False\n\ttry:\n\t\tsender  = get_user(sender_handle) \n\texcept DiasporaException as e:\n\t\tprint(\"ERROR [I] Received payload. Error '%s'\" % e)\n\t\tresult = False\n\texcept:\n\t\tresult = False\n\treturn result\n\n/* SPDX-License-Identifier: GPL-2.0-or-later */\n/*\n *\tIPv6 LWT\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle[0] == entity_handle[0]:\n\t\treturn True\n\telse:\n\t\tassert entity_handle[0].split(\"/\")[0] == sender_handle[0].split(\"/\")[0]\n\t\treturn False\n\n\"\"\"\n\tThe following 3 scenarios are considered invalid and will not work together here.\n\"\"\"\ndef verify_and_remove_invalid_sender_handle(sender_handle, entity_handle):\n\t\"\"\"\n\tVerify sender and entity handles match when we're removing them\n\t\n\tFor example, when receiving a sample payload which is an"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle!= entity_handle:\n\t\traise Exception(\"Incorrect sender handle!\")\n\tif entity_handle!= Entity.None:\n\t\treturn\n\traise Exception(\"Missing Entity Handle!\")\n\n#pragma once\n\n#include <clocale>\n\nnamespace lara {\n    namespace locales {\n\n        typedef char* lc_char;\n        LC_API void set_default_locale(enum lc_string::locale locale, enum lc_string::locale default_locale);\n      "}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t# We'll also be using the sender who you're currently on\n\tsender = entity_handle['sender']\n\t\n\t# We're NOT creating a session, no need to continue checking\n\tif not hasattr(sender, '__getset_last_timestamp'):\n\t\treturn False\n\t\n\t# If that didn't work, just set the timestamp for our process\n\tif not hasattr(sender, 'last_timestamp'):\n\t\tsender.last_timestamp = long(os.urandom())\n\n\tif sender.last_timestamp == long(os.urandom()) and sender_handle not"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle!= entity_handle:\n\t\traise ValueError('Sender and entity handle mismatch. The sender and entity handles are different. This should'+ \\\n\t\t\t'never occur during a transaction.)'\n\t\nclass Transaction(object):\n\t\"\"\"\n\tSimple transaction class.\n\t\"\"\"\n\tdef __init__(self, _sender, _recipient, _data, _signatures, _signer):\n\t\tself._sender = _sender\n\t\tself._recipient = _recipient\n\t\tself._data = _data\n\t\tself._resigners = _signatures\n\t"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t# We'll create a copy of the object because we don't know the payload yet\n\t\n\tclass FakeSenderData(object):\n\t\tdef __init__( self, payload, handler_def ):\n\t\t\tself.handle = handler_def.Handle\n\t\t\tself.payload = None\n\t\t\tself.sender_handle = None\n\t\t\tself.entity_handle = None\n\nclass EventData(object):\n\tdef __init__( self, message, event_handler, context ):\n\t\tself.__dict__.update( event_handler.__dict__)\n\t\tassert isinstance( message"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle!= entity_handle.entity_entity.handle:\n\t\traise InvalidDataError(\"Wrong sender or entity_handle\")\n\t\n    if sender_handle.handle in sender_handle.entity_entity.handles:\n        raise InvalidDataError(\"Cannot set a property on the sender\")\n\ndef create_data(entity, data):\n\t\"\"\"\n\t  This method should not return an error for this entity, but the main error handling for this entity.\n\n\t  You can add this method into a separate func to ensure it works when the entity"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t# We already have a valid'sender and 'entity' handle.\n\tif sender_handle and entity_handle:\n\t\t\n\t\t# We assume that 'entity' is for sender'sender' in the payload and that'sender' also uses it in response.\n\t\t# We could also check sender/entity handle against other entities, and that we do not know if'sender'\n\t\t#   is for the sender directly or indirectly and they still exist in the payload headers.\n\t\n\t\t# Basically we have:\n\t\t#\t1) The 'entity' handles are ("}]}
{"task_id": "630629b952e177c0ba46a043", "prompt": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tif document_path is None:\n\t\tdocument_path = default_document_path()\n\telif isinstance(document_path, six.string_types) and \\\n\t\tnot os.path.exists(document_path):\n\t\traise ValueError(\"document_path does not exist: {}\".format(\n\t\t\tdocument_path))\n\twith get_requests_session(timeout=30) as r, r.session(\n\t\thost=dumbello_host,\n\t\tverify=nodeinfo_verify()) as session:\n\t\tr.register_uri(\"/"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tassert (len(document_path) > 0)\n\t\n\tif document_path is None:\n\t\tdocument_path = '/static/data'\n\t\n\twell_known_document = make_object('well-known_document')\n\twell_known_document.url = url\n\twell_known_document.document_path = document_path\n\twell_known_document.version = None\n\t\n\ttry:\n\t\twell_known_document.data = json.dumps(dict(\n\t\t\tcontent_type=None,\n\t\t\ttype=None,\n"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tself = get_nodeinfo(url=url, documentPath=document_path)\n\twell_known = self[nodeInfoWellKnownString]\n\tfor c in well_known[\"Documents\"]:\n\t\t# TODO fix: use \"well_known documents\" and \"well_known documents\" for\n\t\t#             key/value pairs.\n\t\tif self[nodeInfoString]!= \"\":\n\t\t\twell_known[\"Documents\"][c][\"_key_\"] = self[c]\n\t\tif self[nodeInfoString"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tdef _create_well_known_document(resource, document_id_prefix, document_path=None):\n\t\tcontent = {\n\t\t\t\"type\": \"document\",\n\t\t\t\"id\": document_id_prefix,\n\t\t\t\"source\": [\n\t\t\t\t\"http://example.com\",\n\t\t\t\t\"xmlhttprequest.get\",\n\t\t\t\t\"https://example.com\",\n\t\t\t\t\"example.com\"\n\t\t\t],\n\t\t\t\"target\": resource\n\t\t}\n\t\tif resource:\n\t\t\t# document_path is a callable."}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tdef get_nodeinfo_well_known_document_(url, document_path=None):\n\t\t\"\"\"\n\t\tGiven (optional) :arg url: The full base url with protocol, ie https://example.com\n\t\tGiven (optional) :arg document_path: Custom NodeInfo document path if supplied (optional)\n\t\t\n\t\t\"\"\"\n\t\t# TODO:\n\t\t# - check if document_path is set by config\n\t\t# - generate well-known document for this url (only for legacy nodes)\n\t\t\n\t\treturn None\n\t\t# TODO:\n"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tresult = urlparse.urlparse(url)\n\tnode = result.netloc\n\tnode_path = node.path\n\tif document_path is None:\n\t\tdocument_path = node_path\n\treturn {node: node_path + \"node/\" + document_path}\n################### BEGIN DOCUMENT GENERATION ####################\n\ndef get_nodeinfo_well_known_document(url, document_path):\n\t\"\"\"\n\tRetrieve a NodeInfo.well-known document with a given url.\n\t\n\tFrom Django Core version >= 0.23 or up, the"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tdef build_nodeinfo_document(document_path=None):\n\t\tnodeinfo_document = copy.deepcopy(nodeinfo_document)\n\t\t# Add special extensions to document\n\t\tif nodeinfo_document_extensions:\n\t\t\tif isinstance(nodeinfo_document, str): # NodeInfo extensions are not supported in this context\n\t\t\t\ttry:\n\t\t\t\t\tnodeinfo_document.extend(nodeinfo_document_extensions)\n\t\t\t\texcept UnicodeDecodeError:\n\t\t\t\t\tpass\n\t\telse:\n\t\t\ttry:\n\t\t\t\tnodeinfo_document.clear()"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tif document_path:\n\t\tdocument_root = self.documents.nodeinfo_document\n\t\tif not document_path.endswith(\"/\"):\n\t\t\tdocument_path += \"/\"\n\t\tdocument_path += \".well-known\"\n\t\treturn {\n\t\t\t\"name\": \"node.info\",\n\t\t\t\"type\": \"application/rdf+xml\",\n\t\t\t\"url\": url,\n\t\t\t\"baseurl\": document_root,\n\t\t\t\"baseurl_basepath\": document_root\n\t\t\t}\n\t\t\n\t\t# Get nodeinfo document with base"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tif document_path is not None:\n\t\tdef parse_document_path(file):\n\t\t\tif isinstance(file, str):\n\t\t\t\tf = open(file, 'r')\n\t\t\telse:\n\t\t\t\tf = open(file)\n\t\t\tif \"node_info\" in f.read():\n\t\t\t\treturn f.read()\n\t\t\traise\n\t\t\t\n\t\tdocument_path = parse_document_path(document_path)\n\t\tif not document_path:\n\t\t\traise NodeInfoParseError(\"The application URL missing from the supplied.well-known"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tself.init()\n\n\tnode = self._get_node_info(url)\n\tif document_path: node.setDocumentPath(document_path)\n\n\treturn node\n\n\ndef get_nodeinfo_document_path(url):\n\t\"\"\"\n\t\tGenerate a NodeInfo document path.\n\t\n\t\t:arg url: The full base url with protocol, ie https://example.com\n\t\t:returns: dict\n\t\"\"\"\n\tself.init()\n\n\tnode = self._get_node_info(url)\n\tnode.generateDocumentPaths"}]}
{"task_id": "630629d052e177c0ba46a0a1", "prompt": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tfrom x509.signature import X509AttributeSigner\n\tfrom xml.etree.ElementTree import parseString\n\tfrom mmdispost.xml import BaseElementChecker\n\n\ttry:\n\t\tfrom x509.cert import Certificate\n\texcept ImportError:\n\t\tpass\n\telse:\n\t\tcertificate = Certificate.certificateOf(doc)\n\t\tsignature = X509AttributeSigner(cert=certificate, public_key=signature.public_key)\n\t\treturn signature.verify(public_key.bytes) and BaseElementChecker.verify(public_key)\n\npackage"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tdef check_signature(public, doc, signature):\n\t\tauthor_id = signature['authorId']\n\t\tfrom_cert, crl = signature['fromCert']\n\t\tfrom = certificate_to_asn1(doc['from'])\n\t\tauth_cert = certificate_to_asn1(public['cert'])\n\t\tca_certs = list(private.get_ca_certs(doc, signature))\n\t\tif any(from.issuer_ids.intersection(from_cert)!= 0 or \\\n\t\t       ca_certs.intersection(auth"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\ttry:\n\t\tsignature.verify_signed_base(doc)\n\t\tpass#IMPORTIMPORTIMPORT from xml.dom.minidom import parseString, parseNode\n\t\t#print(\"Signed message: \", doc)\n\t\tpass#IMPORTIMPORTIMPORT from xml.sax.saxutils import escapeOutputCharacters\n\t\tsignature.signatureType = SignatureHeaderNode\n\t\tsignature.signatureValue = signature.generate_signed()\n\n\t\t#print(signature.signatureType, \"type\" + signature.signatureValue )\n\t\tsignature.signatureValue ="}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tif not doc.is_signature_document(public_key):\n\t\traise ValidationError(\n\t\t\t'''\n\t\t\tThis signature was not generated with the certificate author\n\t\t\tregistered.\n\t\t\t'''\n\t\t)\n\tsignature = doc.get_signature(public_key)  # type: Crl.Type.Message\n\tif not signature:\n\t\traise ValidationError(\n\t\t\t'''\n\t\t\tThe signature is empty.\n\t\t\t'''\n\t\t)\n\nfrom django.core.exceptions import ValidationError\nfrom django.shortcuts import get_object"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tpublic_key_to_verify = KeyToPublicKey.from_private_key(public_key=public_key)\n\tdoc = jsonutils.dumps(doc)\n\tsignature = KeyToPublicKey.from_private_key(signature=signature)\n\n\twith libtbx.envs.open_environment():\n\t\tproof = KeyToSignatureProof.create(publicKey=public_key_to_verify, signature=signature)\n\t\tsigned_doc = libtbx.generate_bignumber_doc(proof)\n\n\t# the actual verification could fail just fine"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tsignature = parse_xml_signature(public_key, doc)\n\tpub_key = parse_public_key(signature.get_signature(), signature.get_signature().get_text())\n\n\tlogger.debug(\"Public key: %s\", pub_key)\n\tassert pub_key\n\tverify_public_signature(signature, public_key)\n\n\tlogger.debug(\"Signature matches author:\")\n\tverify_signature(signature, public_key)\n\ndef verify_signature(signature, public_key):\n\t\"\"\"\n\tParse signature into elements\n\t\"\"\"\n"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tkey = public_key\n\tsignatures = doc.getElementsByTagName('Signature')\n\tfor node in signatures:\n\t\tif node.getElementsByTagName('SignatureDescription').length == 0:\n\t\t\tcontinue\n\t\tsignature_type = node.getElementsByTagName('Type')[0].nodeValue\n\t\tif signature_type in signature_map_types:\n\t\t\tif \tsignature_type == \"RSA\":\n\t\t\t\tpublic_key = doc.getElementsByTagName('PublicKey')[0].firstChild.nodeValue\n\t\t\t\tsignature = doc.getElementsByTagName('Signature')[0].firstChild.nodeValue\n\t\t\t\tbreak\n\t\tif public_"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tfrom saml2.samlp import SAMLP\n\tfrom saml2.validator import ClaimValidator\n\tvalidate_signature(doc, signature)\n\tassert not signature.is_public\n\tassert len(signature.elements) == 3\n\t\n\tfor elem in signature.elements:\n\t\tif elem == \"ID\":\n\t\t\tclas = doc['x-amzn-classtime'\"].get('text', None)\n\t\t\tassert clas is not None\n\t\t\tclass = doc['x-amzn-document']['application/xml']\n\t\t\tassert cl"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tverify_body(signature, public_key,\n\t            [XML_HEADER],\n\t            ['document', 'xml_type', 'xml_encoding', 'xml_version',\n\t             'xml_data', 'xml_root',\n\t             'xml_enc_enc', 'xml_enc_enc_enc',\n\t             'xml_enc'])\n"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\twith DocumentMessage(doc['msg_ref'], public_key, signature) as msg:\n\t\t# Note how signature contains the key and the value. The value should\n\t\t# be in the signature to be verified elsewhere.\n\t\tdoc = SignedDocument()\n\t\tdoc.root = None\n\t\tdoc.add_message(msg)\n\t\tdoc.signature = verify_signature('signature_key', doc['msg_refs'][0], signature)\n\t\tself.assertEqual(doc.signature, signature)\n\t\tdoc.add_message(msg)\n\t\t"}]}
{"task_id": "630629e052e177c0ba46a0c4", "prompt": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tdocument = json.loads(document)\n\t\n\t# convert amazon phone\n\tif 'amazon.com' in document['mobile']:\n\t\tdocument['phone_city'] = document['phone_country'] = 'amazon'\n\telif 'amazon.com' in document['tel']:\n\t\tdocument['phone_city'] = document['phone_country'] = 'tel'\n\telif 'amazon.com' in document['pin']:\n\t\tdocument['phone_city'] = document['phone_country'] = 'pin'\n\telif 'pin.amazon.com' in document['"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t_webfinger_doc = re.search(r'[{}]\\]'.format(r'\\[]'), document)\n\tif _webfinger_doc:\n\t\treturn _webfinger_doc.group(1)\n\treturn None\n\n\ndef parse_diaspora_api(document: str) -> Dict:\n\t_api_doc = re.search(r'[{}-](#.*)', document)\n\tif _api_doc:\n\t\treturn parse_diaspora_api_doc(_api_doc)\n\treturn\n\n\ndef"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\treturn json.loads(document)\n\n\n# -----------------------------------------------------------------------------\n# Parse Diaspora webfinger json\n# -----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n\treader = csv.reader(open(os.path.join(__file__, \"../data/diaspora_webfinger.csv\")))).reader(\"*\\n\")\n\tfor row in reader:\n\t\tjson_dict = row.itos().dropna().items()\n\t\tprint(json_dict[0]['key'])\n\t\twebfinger = json_dict[0].pop('webfinger')"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t# Note: we are not using the 'id' field for the input document, use the webfinger ID and not the\n\t# document here for that. This means that there can be a lot of things in the Diaspora federation feed,\n\t# and the 'id' field may have be empty if the webfinger does not contain a ID\n\twebfinger_json = json.loads(document)\n\tif not 'id' in webfinger_json:\n\t\twebfinger_json['id'] = str(int(document.title))\n\twebfinger_id ="}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tdiaspora_object = {}\n\tdiaspora_object['name'] = document.strip().split(\" \")[0]\n\tdiaspora_object['title'] = document.strip().split(\" \")[1]\n\t\n\tfor entry in get_entries(json_file, ['title']):\n\t\ttry:\n\t\t\twebfinger = WebFinger.from_json(entry)\n\t\t\twebfinger['name'] = webfinger['name'].strip().replace(' ', '_').upper()\n\t\texcept:\n\t\t\tlogger.warning(f'No"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tpage_number = None\n\ttry:\n\t\tpage_number = int(document.strip(), 10)\n\texcept ValueError:\n\t\traise Exception('Unable to get ID of page number from document')\n\tdocument = \"{}\\n{}\\n{}\\n{}\\n{}\\n{}\\n{}\".format(\n\t\t\tdocument,\n\t\t\t\"\"\"document: https://weso.github.io/diaspora/#webfinger/{}\"\"\".format('\\t', '*'.join(document),\n\t\t\t\"\"\"document: python3/extensions#__main__"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t# Extract it from the HTML format\n\tdiaspora_url = re.match(r\"https://(\\w+\\.)?diaspora\\.com/webfinger\\w*\", document)\n\tif not diaspora_url:\n\t\treturn {}\n\turl = diaspora_url.group(1)\n\twebfinger_url = diaspora_url.group(3)\n\t\n\t# Extract the version number from the url\n\twebfinger_version = re.findall(r\"^([1-2][0-2]?)("}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\ttry:\n\t\t# Load it\n\t\tdiaspora = webfinger_lib.DIASPORAS_JSONLOADER.load_data(document)\n\t\n\t\treturn diaspora\n\texcept SyntaxError as e:\n\t\tif e.path == \"error\":\n\t\t\traise DiasporaWebfingerValidationError(document, message=str(e.message))\n\t\telse:\n\t\t\traise DiasporaWebfingerParseError(document)\n\t\n\traise DiasporaWebfingerValidationError(document, message=str(e))\n\n\n\"\"\"\n"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tfrom diaspora.webfinger import Webfinger\n\tif __name__ == '__main__':\n\t\twebfinger = Webfinger()\n\t\ttry:\n\t\t\tjson.load(open(__file__).read())\n\t\texcept AttributeError:\n\t\t\tLOGGER.error(\"'%s' not a valid json configuration file.\" % __file__)\n\t\t\tjson.dump(webfinger, open(__file__).read())\n\t\telse:\n\t\t\tif \"config\" in [json for json in webfinger.get_properties() if json[\"type\"] == \"DIASPORA_"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tdata = json.loads(document)\n\twith open(data['config']['settings']['input']['directory'], 'r') as f:\n\t\tdico = f.readline().split('\\n')\n\t\tif len(dico) == 1:\n\t\t\twith open(data['config']['settings']['input']['name'], 'r') as fd:\n\t\t\t\tname = dico[0]\n\t\t\tif not fd.read().strip():\n\t\t\t\tprint('No name found')\n\t\t\t\tprint(file=f)\n\t\t\t\treturn dict()\n \t\t\t\n\t\t\t\n\t\t\tif"}]}
{"task_id": "630629e152e177c0ba46a0d1", "prompt": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\treturn webfinger_get_string_representation(handle, ['http://metadata.evernote.com/webfinger.json'])\n\n\ndef webfinger_get_bytes(handle: str) -> str:\n\t\"\"\"\n\tRetrieves an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\thandle = webfinger_get_string_representation(handle, ['http://metadata.evernote.com/webfinger.json'])\n\treturn handle.rstrip(b'\\n')\n\n\ndef get_all_webfinger_data_with_"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tfrappe.db.sql_savepoint(update_table=frappe.get_local_path(\"T\", frappe.db.table.get_column))\n\ttry:\n\t\tfrom webfinger import fetch_document\n\t\t#check if webfinger is enabled/disabled\n\t\tresult = fetch_document()\n\t\tif result is not None and result.document:\n\t\t\tfrappe.flash(\"Webfinger is enabled!\", \"info\")\n\t\t\t#result.document = None\n\t\t\t#frappe.db.sql(\""}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tif not webfinger_document:\n\t\traise UnknownWebfingerError('Invalid RFC7033 webfinger document.')\n\n\t# Fetch the body\n\tpayload = webfinger_document.json()[WEBFINGER_BODY_FIELD_NAME]\n\n\t# Update the value\n\twebfinger_document['content_type'] = payload.get('content-type')\n\twebfinger_document['charset'] = payload.get('charset')\n\twebfinger_document['content'] = str(payload.get('content'))\n\n\t# Return the webfinger data\n\treturn we"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tif \"WEBFINGER_URL\" not in globals() and \"WEBFINGER_FORM_URLENCODED\" not in globals():\n\t\treturn None\n\n\ttry:\n\t\theaders = {\n\t\t\t\"date\": \"Wed, 23 Mar 2014 15:00:00 +00\",\n\t\t\t\"User-Agent\": BaiduBot.DEFAULT_USER_AGENT,\n\t\t\t\"x-requested-with\": \"XMLHttpRequest\",\n\t\t\t\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/web"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn webfinger_dialog.try_retrieve_webfinger_document(handle)\n\texcept Exception:\n\t\tmessage = f'Could not retrieve a webfinger {handle}.'\n\t\traise CLIError(message)\n\ndef try_retrieve_webfinger_document_async(request: dict) -> Optional[str]:\n\ttry:\n\t\treturn webfinger_dialog.try_retrieve_webfinger_document(request)\n\texcept Exception:\n\t\tmessage = f'Could not retrieve a webfinger {request.method} {request."}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tif not send_http_get(handle, \"https://www.webfinger.net/api/3/get_webfinger_document.json\"):\n\t\treturn None\n\twebfinger_document_json = json.loads(recv_from_stream(handle, 0, int), allow_pickle=True)\n\treturn webfinger_document_json[\"data\"][\"webfinger_document\"]\n\n\n@contextmanager\ndef receive_webfinger_document(handle: str) -> Optional[str]:\n\tpayload = json.loads(recv_from_stream(handle, 0,"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tassert (\n\t\t\twebfingerapi[handle] is not None or\n\t\t\twebhookapi[handle] is not None\n\t\t\tor [api.getwebfingeringpage, api.getpostingpage] is not None\n\t\t\tor api.check_for_webfinger\n\t\t\tor api.getwebfingeringurl\n\t\t\tor api.getwebfetext\n\t\t).enabled\n\treturn (\n\t\twebfingerapi[handle]\n\t\tor [api.getwebfingeringpage, api.getpostingpage]\n\t\tor api.check"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\twebfinger_json = Client.get_webfinger_document(handle)\n\texcept ClientException, e:\n\t\treturn None\n\telse:\n\t\treturn webfinger_json.decode()\n\n\ndef try_retrieve_external_location(handle: str) -> Optional[str]:\n\ttry:\n\t\tfor item in Client.get_external_location(handle):\n\t\t\t# Check if this is valid URL-safe\n\t\t\turl = urlparse(item)\n\t\t\t# Check for absolute URLs\n\t\t\tif item.startswith('"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tif not api_is.is_supported_api:\n\t\tlogger.fatal('The webfinger service is not supported.')\n\tdef try_create_webfinger_document(request_id: str) -> Optional[str]:\n\t\ttry:\n\t\t\treturn api_request_handler_create.call(request_id)\n\t\texcept webfinger_server.WebfingerException as e:\n\t\t\tlogger.error('Failed to create webfinger document.',\n\t\t\t\tevent_type=webfinger_server.EventType.WEBFINGER_DOCUMENT_CRE"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\tresponse = requests.get(handle)\n\texcept:\n\t\treturn None\n\tif response.status_code!= 200:\n\t\treturn None\n\treturn response.text\n\ndef get_page_info(page_number):\n\t\"\"\"\n\tGiven an page_number,return the details of the page to go.\n\t\"\"\"\n\ttry:\n\t\twith open(path.join(DATA_ROOT_PATH, 'Webfinger-Document-%i.pdf'),'r') as pdffile :\n\t\t\tcontent = pdffile.read()"}]}
{"task_id": "630629e152e177c0ba46a0d2", "prompt": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\twebfinger = handle.retrieve('diaspora')\n\t\n\twebfinger['result']['data']['status_url'] = (\n\t\tf'{API_URL}?token={token}',\n\t)\n\t\n\t# handle the diaspora.diaspora.diaspora_webfinger\n\t# call function to get the result\n\ttry:\n\t\tresponse = API_DO(\n\t\turl = f\"https://127.0.0.1:{webfinger['response']['id']}\",\n\t\ttimeout = 5.1,\n\t\ttimeout"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\targs = dict()\n\targs[\"type\"] = \"DiasporaWebfinger\"\n\targs[\"diaspora_id_base\"] = f\"http://{get_data_uri(handle).path}\"\n\tresponse = urllib.request.urlretrieve(args, \"diaspora_webfinger.yaml\")\n\tif response!= \"true\":\n\t\traise DiasporaWebfingerNotAvailableError(\n\t\t\t\"Couldn't fetch diaspora webfinger\", response\n\t\t)\n\tdiaspora = response\n\treturn diaspora"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tcmd ='seaborn diaspora'\n\tdoc = raw_input_file(cmd)\n\tif doc:\n\t\tresults = parse_document(handle, doc)\n\t\treturn results\n\telse:\n\t\tsys.exit(\"Could not retrieve diaspora webfinger.\")\n\n\ndef retrieve_diaspora_webfinger(app, handle, **kwargs):\n\t\"\"\"\n\tRetrieve a diaspora webfinger.\n\n\t:arg `handle`: handle to Diaspora webfinger\n\t:arg `app_"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tif not hasattr(handle, 'webfinger_url'):\n\t\traise Exception(\"handle not set. Handle must be a webfinger handle.\")\n\t\n\twebfinger_url = handle.webfinger_url\n\ttry:\n\t\tWebFingerDoc = requests.get(webfinger_url, verify=False)\n\t\ttry:\n\t\t\tinfo = WebFingerDoc.text.split(';')[1:]\n\t\texcept Exception as e:\n\t\t\traise Exception(\n\t\t\t\t'Error downloading webfinger url: {}'.format(e)\n\t\t\t)\n\t\t\n\t\tprint"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tpath = _get_webfinger_path()\n\twebfinger_object = Webfinger(handle)\n\twebfinger = webfinger_object.retrieve_document_webfinger(path)\n\treturn _to_dictionario_webfinger(webfinger)\n\n\n@webfinger.on_message(message='get_dicos_and_extract_from_webfinger')\n@webfinger.response(receive='dico')\ndef retrieve_dicos_and_extract_from_webfinger(handle, webfinger):\n\tdicome_uri = web"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tclient = make_remote(handle, remote_protocol_1_3.DIASPORA.DIASPORA_WEBFINGER)\n\treturn client.retrieve_and_parse_diaspora_webfinger(handle)\n\n\ndef retrieve_difs_webfinger(handle, start_time=None):\n\t\"\"\"\n\tRetrieve all DIFS webfuser files for the given file (and start on the latest at any time)\n\t\n\t:arg handle: Remote handle to retrieve\n\t:arg start_time: Start time to retrieve from\n\t\n\t:returns:"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tdef request(handle):\n\t\t\"\"\"\n\t\tThis is the default remote request. Just get the raw data from the web service\n\n\t\t:param handle: URL to Diaspora webfinger document.\n\t\t:return: dictionary with request data\n\t\t\"\"\"\n\t\t#print(\"request\", type(handle), type(handle.webservice_url_parts))\n\t\trequestData = ''.join('%s=%s' % v for v in handle.webservice_url_parts)\n\t\treturn requestData\n\n\t#return: dict\n\treturn"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tresp = requests.get( 'http://diaspora.org/?url=' + handle + '&response_type=' + 'json');\n\tdata = json.loads( resp.text )\n\treturn data\n\n\ndef create_remote_DIASPORA_webfinger(handle, dimg_name):\n\t\"\"\"\n\tRetrieve a Webfinger document.\n\t\n\t:arg handle: Webfinger handle\n\t:arg dimg_name: Name of DIASPORA_WebFinger document\n\t:returns: dimg\n\t\"\"\"\n\treq = requests.put( '"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\treturn execute_function(handle, 'RetrieveAndParseDiasporaWebFinger', timeout=10)\n\n\n#------------------------------------------------------------------------------\n#------------------------------------------------------------------------------\n\ndef retrieve_and_parse_net_office(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote NetOffice webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\treturn execute_function(handle, 'RetrieveAndParseNetOffice', timeout=10)\n\n\n#------------------------------------------------------------------------------\n#------------------------------------------------------------------------------\n\ndef retrieve_and_parse_net_webfinger"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\treturn remote_dasporas.retrieve_and_parse_diaspora_webfinger(handle)\n\n\n\n\ndef retrieve_and_parse_dressure(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Dressure document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\treturn remote_dressures.retrieve_and_parse_dressure(handle)\n\n\n\n\ndef retrieve_and_parse(handle):\n\t\"\"\"\n\tRetrieve a local Dressure/Distrain"}]}
{"task_id": "630629e252e177c0ba46a0d6", "prompt": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn remote.retrieve_djson(host=host,\n\t\t\t\t\t\t\t get_function=get_host_meta_fn(),\n\t\t\t\t\t\t\t options=\"get_meta_value\",\n\t\t\t\t\t\t\t return_json=True)\n\ndef retrieve_ip_address(host):\n\t\"\"\"\n\tRetrieve a remote IP Address.\n\t\n\tThe XRD object returned in :func:`retrieve_diaspora_host_meta()`.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn remote.retrieve_djson(host=host,\n"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\thost = resolve_host(host)\n\tmeta = _host_meta_retrieve(host, host.meta_file)\n\treturn meta['data']['data']\n\ndef _host_meta_retrieve(host, meta_file):\n\t\"\"\"\n\t\tTry to locate the latest available metadata document.\n\t\n\t\tIf the meta document exists there will be an array\n\t\tof XRDs with the data collected.\n\t\n\t\t::\n\t\n\t\t::\n\t\n\t\t::\n\t\treturns: ``XRD`` instance\n\t\"\"\"\n\tif meta_file:"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn XRD(\n\t\t{\"name\": host.lhost.lname,\n\t\t\"type\": \"file\"}, \n\t\treturn_class=XRD.XRD, \n\t\tmethod=GET\n\t)\n\nclass XRD(object):\n\t\"\"\"\n\tStore a parsed :class:`DiasporaHostMetadata` in memory.\n\t\n\t:arg host: URL to store into\n\t:arg dict_meta: Dia information (or ``None`` if no meta)\n\t:returns: ``YARN`` instance\n\t\"\"\"\n\tdef __init__("}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn API.get_remote(host + '/host-meta')\n\ndef retrieve_diaspora_host_meta_from_host(host, username, password):\n\t\"\"\"\n\t\tRetrieve a remote Diaspora host-meta document and update local meta field\n\t\t\t\n\t\t:param host: Host name in Diaspora\n\t\t:param username: Username in Diaspora\n\t\t:param password: Password in Diaspora\n\t\t\n\t\t:returns: ``XRD`` instance\n\t\"\"\"\n\tdoc_info = None\n\t# Add"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\tdefer_import()\n\t\n\tclient = boto3.client(\"ec2\", region_name=host.meta[\"region\"])\n\tresponse = client.describe_host_meta(HostId=host.meta[\"host_id\"])\n\t\n\treturn response[\"HostMetadata\"][0]\n\t\n\ndef process_host_meta_update(host, d, doc):\n\t\"\"\"\n\tGiven a single Diaspora host-meta update record, process that and update for the next\n\trecord.\n\t\n\t:arg host: Host to fetch host-meta\n\t:arg d:"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn xrd.core.RemoteHostMeta.retrieve_diaspora_host_meta(host)\n\ndef retrieve_host_daspora_host(host_id):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document from a host-id\n\t\n\t:arg host_id: Host-id of the remote Diaspora host\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\tif host_id == \"\":\n\t\traise KeyError(\"Host id must be a valid ID\")\n\tmeta = xrd.core.Remote"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\tsession = get_session()\n\tsession.add(host)\n\tsession.commit()\n\treturn session\n\ndef delete_diaspora_host_meta(host):\n\t\"\"\"\n\tDelete a remote Diaspora host-meta document.\n\t\n\t:param host: Host to delete\n\t:returns: ``OK`` or ``MISSING``\n\t\"\"\"\n\trequest = XRDRequest()\n\trequest.add(host)\n\treturn request\n\ndef retrive_diaspora_host_meta_doc(doc):\n\t\"\"\"\n\tRetrieve"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\thost = host.xpath('//Xrds:HostStatus/Xrds:Host')\n\treturn host.xpath('//Xrds:Host/Xrds:HostHostStatus/Xrds:Host')[0]\n\ndef list_dcit_hosts(host_to_list):\n\t\"\"\"\n\tList available DCI host-meta information from the server in the appropriate format\n\t\n\t:args host_to_list: A single (or multiple) list of host-meta information or a dict containing keys containing host-metadata\n\tin that case or an arbitrary dictionary containing values containing host-meta"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn _retrieve_remote_diaspora_host_meta(host._uri)\n\ndef _retrieve_remote_diaspora_host_meta(remote_uri, **kwargs):\n\tdoc = None\n\ttry:\n\t\tdoc = _requests.get(remote_uri, **kwargs)\n\texcept:\n\t\traise MissingRemoteHost\n\turl = \"http://{}/api/v1/content/{}\".format(REDIS_API, doc.json().get(\"remote_name\", \"\"))\t\n\t# Add meta-fields to return dict\n\t"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t# if it's not in the default list, retrieve it, with fallback\n\tif not isinstance(host, list):\n\t\tif isinstance(host, dict):\n\t\t\thost = host[DEFAULT_HOST]\n\tdef retrieve_metacharameters():\n\t\t\"\"\"\n\t\tRetrieve a dictionary of :class:`XRD.Hosts.Host` with values.\n\n\t\tThis returns an error status, as this could cause a deadlock situation\n\t\twhen the XRD can't perform it.\n\n\t\t:return: A dictionary of value pairs.\n\t\t:rtype:"}]}
{"task_id": "630629e752e177c0ba46a0fb", "prompt": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tassert url\n\tassert data is not None or (type(data) == dict or type(data) == bytes)\n\tif timeout is not None and 1000 <= timeout < int(timeout):\n\t\traise ValueError(\"The timeout value must be an integer\")\n\tassert method == \"post\" or method == \"put\"\n\theaders = {\n\t\t\"Content-Type\": \"application/x-www-form-urlencoded\",\n\t\t\"Accept\": \"application/json, text/plain, urlencoded; charset=UTF-8\",\n\t\t\"Authorization\": \"Basic \" +"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tparams={}\n\t\n\theaders={}\n\t\n\tif(timeout):\n\t\theaders[\"Content-type\"] = \"application/json; charset=utf-8\"\n\t\n\tif isinstance(data, str):\n\t\theaders[\"Content-type\"] = \"application/json; charset=utf-8\"\n\t\theaders[\"Content-length\"] = str(len(data))\n\t\traw_data = json.dumps(data, sort_keys=True)\n\t\traw_params = urlencode(params, urlencode_with=\"&\", append=urlencode_with)\n\t"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\treturn await send_request_as_document(url, *args, **kwargs)\n\nclass WikiDocumentDocument(object):\n\t\"\"\"\n\tHelper method to send a document (a wikitext with a \"wiki\") directly.\n\tA WikiDocumentDocument object wraps a document as a wikitext in its body, and\n\tcan be made \"downloaded\" to a file that is smaller before being transferred.\n\tThe object takes the following forms:\n\t\t| a WikiDocumentDocument\n\t\t| [data]\n\t\t| [data.wiki_id]\n"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tdata = json.dumps(data)\n\thost, ip, port = get_hostport_and_port(url)\n\tif host!= \"https://www.google.com\" and ip:\n\t\thost = f\"https://{host}:{port}\"\n\tif not host in args and not ip:\n\t\tmsg = (\n\t\t\t\"HTTP/2 or HTTP_PROXY connection required for Python HTTP and SSL, \"\n\t\t\t\"but couldn't locate a valid host and port. \"\n\t\t\t\"Please add the --host flag and set the \"\n\t\t\t"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tassert self.__verify(url)\n\tif method == \"post\" and isinstance(data, bytes):\n\t\tdata = data.encode(\"utf-8\")\n\ttry:\n\t\treturn requests.post(url, headers=self._headers(),\n\t\t                     data=data, content_type=\"application/json\", timeout=timeout, **kwargs)\n\texcept RequestsException as exc:\n\t\treturn http.HTTPStatusNotImplemented(str(exc))\n\n\ndef parse_json_response_value"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\ttry:\n\t\treturn requests.post(url, data=data, timeout=timeout, **kwargs) # pylint: disable=W0613\n\ndef send_headers(url, data, timeout=10, method=\"POST\", *args, **kwargs):\n\t\"helper function, will send out headers via HTTP headers\"\n\tif type(data) is dict:\n\t\tfor header_name, header_value in data.items():\n\t\t\tsend_headers_internal(url, header_name, header_value, timeout=timeout, method=method, *args, **kwargs"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tif timeout == 0:\n\t\ttimeout = 10\n\n\tbody = kwargs.pop('data', None)\n\tif 'body' in kwargs:\n\t\tif not isinstance(body, bytes):\n\t\t\traise ValueError('body must be a Bytes instance')\n\n\t\tbody = Body(body)\n\tif not isinstance(body, bytes):\n\t\tbody = json.dumps(body)\n\t\t\n#\tdef send_document(url, data, timeout=15):\n#\t\tkwargs = {'timeout': timeout,'method': method}\n#\t\tkwargs."}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\ttry:\n\t\treq = requests.request(method.upper(), url, data=data, headers=HTTPHeaders(**kwargs), timeout=timeout)\n\texcept:\n\t\treturn None\n\tif not method == \"POST\":\n\t\tcode = None\n\t\tif not req.json:\n\t\t\treturn None\n\t\treq_data = req.json()\n\t\ttry:\n\t\t\tif method == \"GET\":\n\t\t\t\tdata = json.loads(req_data.get(\"data\"))\n\t\t\telse:\n\t\t\t\tdata = json.loads(req_"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\ttry:\n\t\tresp = requests.post(url=url, json=data, timeout=timeout,\n\t\t\t\t\t\t\t\t verify=False, **kwargs)\n\texcept (TypeError, ValueError, requests.exceptions.ConnectionError, ConnectionError):\n\t\treturn send_empty_response(resp=None, url=url, timeout=timeout, method=\"POST\", *args, **kwargs)\n\tif not resp.ok:\n\t\traise ConnectionError(resp.status_code, resp.reason, resp.raw_response)\n\treturn resp.status, resp.reason\n\n# Helper methods"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tif \"timeout=None\" in data:\n\t\ttry:\n\t\t\ttimeout = int(data.get(\"timeout\"))\n\t\texcept:\n\t\t\ttimeout = None\n\t\n\tif \"method\" in data:\n\t\tmethod = data[\"method\"].capitalize()\n\t\n\tif \"body\" in data or \"headers\" in data:\n\t\traise TypeError(\"Unsupported request method\")\n\t\n\treturn send_request(url, data, timeout, method, *args, **kwargs)\n\n\ndef send_request(url, data, timeout=10, method=\"POST\","}]}
{"task_id": "6306091073426c38ae68acac", "prompt": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tdic_dict_insert(dic, *keys)\n\n# }}}\n# }}}\n\n\n#\n#  dict_subscript(dic, val, key)\n#  # return value of an element in a dictionary that is not subscripted.\n\n\n# {{{ dict_get(dic, dic_key, function)\ndef dict_get(dic, dic_key, function=None):\n\tif not function in dict_func:\n\t\traise ValueError('function should be defined')\n\t\n\treturn dict_func[function](dic, dic_key)"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif type(key) == type(''):\n\t\treturn dic[key]\n\telse:\n\t\tnew_dic = {}\n\t\tfor i, child_dic in enumerate(dic):\n\t\t\tchild_dic.setdefault('key1.key2', {}).setdefault(key, {}).setdefault(key, {}).update(keys)\n\t\t\tnew_dic.setdefault('key1.key2', {}).setdefault(child_dic['key3'], {}).setdefault(key, {}).update(child_dic['key4'])\n\t\t\tnew_dic.setdefault"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tdict_len = len(dic)\n\tif keys:\n\t\tkeys = [_s for _s in keys if _s]\n\telse:\n\t\tkeys = []\n\t\n\tvalue, kid = _dict_get(dic, key, keys)\n\t\n\tif value is False:\n\t\traise ValueError(\"The key you specified does not exist.\")\n\t\n\tdic[key] = dic.get(key, dict())\n\t\n\treturn dic, value\n\n/*\n * Copyright(c) 2020 Intel Corporation. All rights reserved.\n *\n * This program"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tdic.setdefault(\"nested_keys\", []).append(key)\n\tfor key in keys:\n\t\tdic.setdefault(\"nested_keys\", []).append(key)\n\t\n\treturn dict_insert(dic, *keys)\n\n/*********************************************        \n\u4f5c\u8005\uff1a\u66f9\u65ed\u5347              \nQQ\uff1a279060597\n\u8bbf\u95ee\u535a\u5ba2\u4e86\u89e3\u8be6\u7ec6\u4ecb\u7ecd\u53ca\u66f4\u591a\u5185\u5bb9\uff1a   \n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tdic['v'].union(dic[key],...*keys)\n\n/*\n * H.26V - Hardware Monitor CMOS Driver\n *\n * Version: $Revision: 1.23 $\n *\n * $Date: 2009/03/16 14:33:27 $\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tdel val, key, *keys\n\tdic_keys = list(dic)\n\tkeys = keys[:-4]\n\tdic[val, keys[key]] = keys[-1]\n\tdict_insert(dic, val, keys[key])\n\n\n\n\n\n\ndef dict_add(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_add({}, 'val.val"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tfrom. import _dict_insert_helper\n\tassert isinstance(dic, _dict_insert_helper.DictInsertHelper)\n\n\treturn dic.dict_insert(val, key, *keys)\n\n\"\"\"\nArraySet.\n\"\"\"\n\n\nclass ArraySet(AbstractIterable):\n\t\"\"\"ArraySet class.\n\t\n\tThe ArraySet class can be used to represent any array in the application.\n\tIt should be used as a base class for all array elements.\n\t\n\tParameter names, values:\n\t\t- items: an array of this class\n\t\t- type"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tfor key, val in dic.iteritems():\n\t\tif val!= None and key!= None and key!= 'None':\n\t\t\tif key in keys:\n\t\t\t\traise Pytest_Error('Inconsistent order of keys on nested dicts '\n\t\t\t\t\t\t\t\t'{}:{} - {}\\n'.format(_keyformat[key], val,\n\t\t\t\t\t\t\t\t_keyformat[key]))\n\t\t# append the key value\n\t\tdic[key] = (val, '')\n\treturn dic\n\n\ndef dict_iter_nested_dict(dic, *fields):\n\t\"\"\""}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tdic = dic[key]\n\tfor k,v in zip (keys[:-1], keys[1:] + [val]):\n\t\tif k in dic:\n\t\t\tdic.insert(k, v)\n\t\t\traise ValueError('{} already exists'.format(key))\n\t\n\traise ValueError('{} already exists'.format(val))\n\ndef dict_key(dic, key):\n\tfor k in dic:\n\t\tif not isinstance(key, (str, int, long, float)): continue\n\t\tif key in dic: continue\n\t\t"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tDTYPE_DTYPE = dic.__new__(dic).__dict__.__new__(dic)\n\tDTYPE_DTYPE[key] = val\n\tDTYPE_DTYPE.__dict__[key] = dic\n\treturn DTYPE_DTYPE\n\t\n\t\ndef dict_iter(dic, def_key=None):\n\t\"\"\"\n\treturn iterator over all the key-value pairs in a dictionary\n\t\n\tto iterate over value-keys of a nested dictionary, all ancestor keys\n\tshould be given as method's arguments\n\t\n\texample:\n\t  dict_iter({}, 'newKey"}]}
{"task_id": "6306091a73426c38ae68acc8", "prompt": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tif settings_dirs is None:\n\t\tsettings_dirs = [pydantic.Defaults.DataDir.TempDir() for _ in range(4)]\n\n\t_set_list_of_file_names(settings_dirs, spec_option, list_of_file_names)\n\n\tif settings_dirs is None:\n\t\tsettings_dirs = [pydantic.Defaults.DataDir.TempDir()]\n\n\t_add_to_setting_list_of_file_names(settings_dirs, spec_option, list_of_file_names)\n\n"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t# If a directory is specified, check is not already covered by\n\t# the default directory\n\tif not os.path.isdir(spec_option['path_to_spec_filegroup']):\n\t\tif not os.path.isdir(settings_dirs['inifile_files'][\"inifile_paths\"]['default']):\n\t\t\tmakeDir('inifile_files/' + spec_option['name'])\n\n\tif not os.path.isdir(settings_dirs['inifile_files']['inifile_paths']['default']):\n"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tlist_of_file_names(SettingsDir + '/../settings', spec_option)\n\n\ndef list_of_file_types(settings_dir, spec_option):\n\t\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\n\ttypes = []\n\tfor n in os.walk(settings_dir):\n\t\tif'settings_' + n.split('.')[-1]!= '-settings_':\n\t\t\tcontinue\n\t\tfor name in os.listdir(n + '/.' + name):\n\t\t\ttypes.append(os.path."}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t# define a list of file names.\n\tlist_of_file_names_f = []\n\tif spec_option == \"name\":\n\t\tlist_of_file_names_f[\"name\"] = \"file.inifile.inifile\"\n\telse:\n\t\tif not any(spec_option in spec.file_spec_options()):\n\t\t\tlogger().error(\"Inifile file-spec options have not been supplied\")\n\t\t\traise Exception(\n\t\t\t\t\"Inifile file-spec options in definition are not supported. \\n\"\n\t\t\t"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tdef setup(settings_dirs, spec_option):\n\t\t'''\n\t\tParse the individual data into a dictionary of dictionaries with keys\n\t\tthat have a 'name' that contains the field name and value\n\t\t'''\n\t\t# Add the complex types\n\t\tsettings_dir = settings_dirs['complex_types']\n\t\tassert settings_dir is not None\n\t\tsettings_paths = settings_dir.paths\n\t\t# Set the name to make available to the user\n\t\tpath = settings_paths.paths['COMPLEX_TYPES']['name']\n\t"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t# Generate the list of file names here\n\tif spec_option in [\"files\", \"files2\"]:\n\t\treturn IniType_File_Specifier\n\telif spec_option in [\"files3\", \"files4\"]:\n\t\treturn IniType_File_Specifier\n\n/* Copyright (C) 2018 Magnus L\u00e5ng and Tuan Phong Ngo\n * This benchmark is part of SWSC */\n\n#include <assert.h>\n#include <stdint.h>\n#include <stdatomic.h>\n#include <pthread.h>\n\natomic_"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tsettings_dir = settings_dirs['spec'].get('root', None)\n\tif (settings_dir, settings_dir.parent) in settings.items():\n\t\tif not settings_dir.exists() or settings_dir.parent.is_dir():\n\t\t\tsettings_dir.mkdir()\n\t\tname_settings_dir = settings_dir.parent / ('settings.py')\n\t\tfor directory in name_settings_dir.iterdir():\n\t\t\tname = f'{settings_dir}/{directory.stem}'\n\t\t\tif name not in directory:\n\t\t\t\t"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\treturn [get_full_name(name, self['name_format'], self['title'])\n\t\t   for name in list_of_file_names(self['inipaths'], spec_option)\n\t\t   ]\n\n##############################################################################\n#\n# Create a real type\n#\n##############################################################################\n\ndef _get_real_type(inipaths, info) :\n\tif len(inipaths) > 0 :\n\t\treturn Type.real_type(inipaths)\n\telse :\n\t\treturn Type.none()\n\n#\n# Create a new"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tfor dir in settings_dirs:\n\t\t# load the type definition\n\t\ttype_definition = list_of_file_names(dir, spec_option)\n\t\tyield (dir, type_definition)\n\nclass Type(Enum):\n\tFile = Type.FILE\n\tFileWithIniTypeAndFilename = Type.FILE_WITH_INI_TYPE_AND_FILENAME\n\nclass SourceDef:\n\tfile = File\n\tfile_with_ini_type = FileWithIniTypeAndFilename\n\nclass Model:\n\tdef __init__(self):\n"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tsettings_dir = os.path.join(\n\t\t\tsettings.__path__[len(spec_option)].split(\".\")[0], \"inireal\")\n\tnewconfig = dict()\n\tnewconfig[\"default_config\"] = {\n\t\t\t\"directory\": settings_dirs[0],\n\t\t\t\"user_group\": settings_dirs[1],\n\t\t\t\"default_template\": \"\"}\n\tif not os.path.isfile(settings_dirs[0]):\n\t\tprint(\"File not found!\")\n\telse:\n\t\tnewconfig[\"default_config\"][\""}]}
{"task_id": "6306091b73426c38ae68acd7", "prompt": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tcls.__ansible_config_manager__ = ansible_config_manager\n\ndef set_logging_parent(logging):\n\tif logging is not None:\n\t\tlog.set_parent(logging)\n\nclass TestBase(object):\n\tdef test_setup(self):\n\t\tpass\n\n\tdef test_setup_no_user_args(self):\n\t\tpass\n\n\tdef test_setup_no_args(self):\n\t\tpass\n\n\tdef test_setup_invalid_app_args(self):\n\t\tself.assertRaises("}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn cls._config_manager.get('ansible_config_tools')\n\n\n@ansible_config_manager\ndef ansible_config_manager_file_exists(cls, config_dir):\n\t\"\"\"\n\tFunction to check the answer is file exists. This method is a blocking call.\n\tReturns: bool - True if the answer is a file, False otherwise.\n\t\"\"\"\n\tif not os.path.isdir(config_dir):\n\t\treturn False\n\n\tfull_config_path = ConfigManager._parse_answer_config_file(cls._"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import atexit, time\n\tpass#IMPORTIMPORTIMPORT import reconfigure, os\n\tpass#IMPORTIMPORTIMPORT import StringIO\n\tfile = StringIO.StringIO()\n\tatexit.register(int(os.environ.get(\"ANSIBLE_CONFIG_MANAGER_ATEXIT_TIME\", \"0\")))\n\tpass#IMPORTIMPORTIMPORT import reconfig\n\tansible_config_file = \"{:s}\\t{:s}\\t{:s}\\n\".format(file.name, file.name"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tfrom ansible_runner.config_manager import _ansible_config_manager\n\tif _ansible_config_manager is not None:\n\t\treturn _ansible_config_manager\n\n\tif not _ansible_config_manager:\n\t\timport ansible_core\n\t\t# The ansible configuration manager is in module scope, so\n\t\t# we pass in the module to our class constructor.\n\t\treturn cls((config.BaseConfiguration, ))\n\n\n\telse:\n\t\tinstance = super(_ansible_config_manager, cls).get_"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn cls.__config_manager\n\n\nCONFIG_ID = 'config'\n\n/**\n * Copyright (c) 2000-present Liferay, Inc. All rights reserved.\n *\n * This library is free software; you can redistribute it and/or modify it under\n * the terms of the GNU Lesser General Public License as published by the Free\n * Software Foundation; either version 2.1 of the License, or (at your option)\n * any later version.\n *\n * This library is distributed in the hope that it will be useful, but WITHOUT\n * ANY WARRANTY; without even the implied warranty"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tdef ansible_config_manager_with_defaults(cls):\n\t\t\"\"\"\n\t\tReturns an object with defaults.\n\t\t\"\"\"\n\t\t# Get the default user for this machine\n\t\t# Note: default user to the machine manager is the one of\n\t\t# Ansible\n\t\t# See:\n\t\t#\t   * https://github.com/ansible/ansible/blob/v0.4.0/ansible/config.py#L22-L46\n\t\t#\t   * https://github.com/ansible/ans"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn cls.get(ansible_config_manager)\n\n\n@util.in_subprocess\n@ansible_context\ndef ansible_init(cls):\n\t\"\"\"\n\tSets up the ansible logger. Note that this method does not call ansible_exit in your\n\tcommand. If you want to use other logging as the default one, you have to call\n\tansible_clear_session when using the ansible_exit function as a context manager.\n\t\"\"\"\n\tutil.log_file_handler(\"Logging in to ansible\", ansible_log_level)"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import os, errno, struct, sys, path, os, platform\n\tif platform.startswith('win32'):\n\t\tcfg = get_default_conf(os.path.join(os.path.dirname(__file__),\n\t\t\t\t\t\t 'config'))\n\telif platform.startswith('darwin'):\n\t\tcfg = get_default_conf(os.path.join(os.path.dirname(__file__),\n\t\t\t\t\t\t 'C:/Config'))\n\telif platform.startswith('linux'):\n\t\t# Linux is always required as it requires the\n\t\t#"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn cls.instance\n\n#include \"stdafx.h\"\n#include \"UIStaticManager.h\"\n#include \"UIStaticMesh.h\"\n#include \"UIStaticMeshStaticMesh.h\"\n\nnamespace vl\n{\n\tnamespace presentation\n\t{\n\t\tnamespace layout\n\t\t{\n\t\t\tusing namespace detail;\n\t\t\tnamespace details\n\t\t\t{\n\t\t\t\tUIStaticMeshStaticMesh::UIStaticMeshStaticMesh(const core::dimension2d<u32>& size, const core::vector<core::array<shared_ptr<ui::StaticMesh"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn cls(ansible.config.ConfigManager())\n\n\nclass ConfigFile(object):\n\tdef __init__(self, *args, **kwargs):\n\t\traise NotImplementedError\n\n\tdef __enter__(self):\n\t\treturn self\n\n\tdef __exit__(self, *args):\n\t\tself.close()\n\n\t__enter__ = __exit__\n\n\nclass ConfigManager(object):\n\t\"\"\"\n\tThe config manager acts as a singleton as there is only one way to customize the config files.\n\tA configuration can either be specified or an alternative config file"}]}
{"task_id": "6306091b73426c38ae68acd9", "prompt": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\tpass\n\nfrom.common import build_task_env, build_task_task_env\nfrom. import common  # noqa\nfrom. import logback  # pylint: disable=unused-import\nfrom. import pill\nfrom. import task_cache  # pylint: disable=unused-import\nfrom. import task_loader  # pylint: disable=unused-import\nfrom. import task_scheduler  # pylint: disable=unused-import\nfrom. import session # noqa\nfrom. import task_logger  # pylint: disable=unused-import\n\n\ntry:\n  "}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn cls._workspace_manager\n\n/************************************************************************************//**\n* \\file         Demo/ARMCM4_STM32F1_Nucleo_F429ZI_Keil/Prog/led.h\n* \\brief        LED driver header file.\n* \\ingroup      Prog_ARMCM4_STM32F1_Nucleo_F429ZI_Keil\n* \\internal\n*----------------------------------------------------------------------------------------\n*                  "}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn cls._data.workspace_manager\n\n\nclass Default_Workspace_Manager():\n\t\"\"\"\n\tCreates a new workspace manager.\n\tUsage example:\n\t\tdef factory(self, app, args, options=None):\n\t\t\tprint(self.path_manager.get_default_workspace_manager())\n\n\t\"\"\"\n\tpath_manager = None\n\n\tdef __init__(self, app, path_manager):\n\t\tself.app = app\n\t\tself.data = app.__data__\n\t\tself.manager = path_manager\n\t\tself"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from time import current_time\n\tpass#IMPORTIMPORTIMPORT from sp_logger import Logger, sp_logger\n\tpass#IMPORTIMPORTIMPORT from mpi import mpi_size, mpi_comm_size, mpi_send, mpi_recv, mpi_recv_mpi, mpi_recv_win, mpi_send_begin, mpi_send_end, mpi_recvv, mpi_sendv, mpi_error\n\tpass#IMPORTIMPORTIMPORT from sys import exit, remove, stdout, sys\n\tpass#IMPOR"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn workspace_manager\n\ndef _add_workspace_to_group_name(group_name, new_group_name):\n\t\"\"\"\n\tAdds the given workspace to the given group.\n\t\"\"\"\n\tif new_group_name is None:\n\t\tnew_group_name = \"default\"\n\n\tgroup = _get_group(group_name)\n\tnew_group = new_group_name\n\n\troot = group['root']\n\n\tif root is not None:\n\t\tpath = project_path(\"workspace_manager\") / str(root"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import datetime\n\tpass#IMPORTIMPORTIMPORT from sp_global_def import *\n\tpass#IMPORTIMPORTIMPORT from sp_global_def import *\n\t\n\tif cls.process_type == \"process\":\n\t\tsp_global_def.posix_process = True\n\t\tsp_global_def.MPI_start_single = True\n\t\n\tif sp_global_def.INFO > 0:\n\t\tsp_global_def.INFO = sp_global_def.INFO  # set DEBUG information\n\t"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from __config__ import cfg\n\tif cfg.get_config_option('workspace_manager_name_format') == 'all':\n\t\treturn\n\ttmp_dir = tempfile.TemporaryDirectory()\n\tif not os.path.exists(tmp_dir.name):\n\t\tos.makedirs(tmp_dir.name)\n\twith open(cfg.get_config_option('workspace_manager_file', 'file_name'), 'w') as file:\n\t\tfile.write(open(cfg.get_config_"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn cls.project.workspace_manager(cls)\n\n/*\t$OpenBSD: strcase.h,v 1.11 2016/07/29 19:31:25 martynas Exp $\t*/\n/*\t$NetBSD: strcase.h,v 1.2 1996/05/01 05:51:30 dholland Exp $\t*/\n\n/* Written by Dr Stephen N Henson (steve@openssl.org) for the OpenSSL\n * project 1999.\n */\n/* ====================================================================\n * Copyright (c) 1999 The OpenSSL Project.  All rights reserved.\n"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\tpass\n\nclass Workspace(object):\n\t\"\"\"\n\tGets the workspace.\n\t\"\"\"\n\tdef workspace_manager(cls, folder_path, data_root):\n\t\treturn \"WorkspaceManager.{}\".format(folder_path)\n\nclass WorkspaceManager(object):\n\t\"\"\"\n\tContains a class for the workspace.\n\t\"\"\"\n\tdef __init__(self):\n\t\t\"\"\"\n\n\t\t\"\"\"\n\t\tclass WorkspaceManager:\n\t\t\t\"\"\"\n\t\t\"\"\"\n\t\tdef Workspace_Manager(self):\n\t\t\t\"\"\"\n\t\t\"\"\"\n\t"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\tpass\n\nfrom django.contrib import admin\nfrom django import forms\nfrom django.utils import timezone\n\n\nclass AdminView(admin.ModelAdmin):\n    list_display = ('name',)\n    list_display_links = ('self','related_views',)\n    raw_id_fields = ('name',)\n\n\nclass CustomForm(forms.Form):\n\n    class Meta:\n        model = CustomModelForm\n        fields = ('name',)\n\n\nclass"}]}
{"task_id": "6306091b73426c38ae68acda", "prompt": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tpass\n\n/* -*- Mode: C; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */\n\n#include <iostream>\n\n#include \"common.hh\"\n\n#ifndef _OPENMVG_UTIL_LOG_H_\n#define _OPENMVG_UTIL_LOG_H_ 1\n\nnamespace cv\n{\n\nnamespace util\n{\n\ntemplate <typename T>\nclass Logger\n{\npublic:\n\n    /* @var logger\n        Log object\n   "}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn __plugin_manager_func__(cls)\n\n\nclass __plugin_manager_func__(object):\n\t\"\"\"\n\tAbstract function of creating the plugin manager.\n\n\tFor this to work you must define the functions `load_configuration()` and\n\t`register_plugin()`.\n\t\"\"\"\n\tdef __init__(self):\n\t\tself.__cached_plugin_data = None\n\t\tself.plugin_configs = {}\n\n\t@staticmethod\n\tdef _default_config(plugins_manager):\n\t\t\"\"\"\n\t\tReturns the plugin config values.\n\t\t\"\"\""}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t# return PluginManager(cls)\n\treturn PluginManager()\n\n\nclass Cache(dict):\n\t\"\"\"\n\tCache data to be stored.\n\t\"\"\"\n\tdef __setitem__(self):\n\t\t\"\"\" Sets the dict cache value.\"\"\"\n\t\tself.obj = self.__dict__.copy()\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\tReturns the dict.\n\t\t\"\"\"\n\t\tif not self.obj:\n\t\t\tself.obj = {}\n\t\treturn self.obj.get(key, None)\n\n\nclass C"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tfrom ydk.types import Entity as _Entity_\n\tfrom ydk.types import EntityPath, Identity, IdentityRef\n\tBase = _Entity_\n\tclass Plugin(Base):\n\t\tpass\n\nclass PluginIdentityRef(IdentityRef):\n\tpass\n\nmodule ActionCable\n\tmodule Logging\n\t\tmodule Log\n\n\t\t\t# This module is an internal logging module, to be used as a\n\t\t\t# module-internal decorator.\n\n\t\t\tmodule Logging.internal\n\n\t\t\tdef log(severity, message = nil, *args)\n\t\t\t\tsuper()"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t# TODO: Implement this functionality\n\tpass\n\n//\n// Diese Datei wurde mit der JavaTM Architecture for XML Binding(JAXB) Reference Implementation, v2.2.5-2 generiert \n// Siehe <a href=\"http://java.sun.com/xml/jaxb\">http://java.sun.com/xml/jaxb</a> \n// \ufffdnderungen an dieser Datei gehen bei einerm zwischen Objekte erhalten: \n// Generiert: 2014.02."}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tdef plugins_manager(cls):\n\t\t\"\"\"\n\t\tReturns the plugin manager of the calling class.\n\t\t\"\"\"\n\t\tpass\n\n\tdef plugins(cls):\n\t\t\"\"\"\n\t\tReturn a list of registered plugins.\n\t\t\"\"\"\n\t\tpass\n\n\tdef default(cls):\n\t\t\"\"\"\n\t\tGets or sets the default plugins.\n\t\t \"\"\"\n\t\tpass\n\n/*\n * Copyright (C) 2010-2018 Gordon Fraser, Andrea Arcuri and EvoSuite\n * contributors\n *\n * This file is part of"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from sp_morphology.pluginmanager import PluginManager\n\treturn PluginManager()\n\n\n#  Copyright (c)  2002 by Matthias Spaga (@matthiasma) and Christo Baissen (@ChristoBaissen)\n#\n#  This program is free software; you can redistribute it and/or\n#  modify it under the terms of the GNU Lesser General Public\n#  License as published by the Free Software Foundation; either\n#  version 2.1 of the License, or (at your option) any later"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn cls.__plugin_manager__\n\n\n# TODO: Implement this method in the plugins manager\ndef process_tasklist(cls, tasklist):\n\t\"\"\"\n\t\tProcesses a list of tasklists from the plugin manager\n\t\"\"\"\n\tfor task in tasklist:\n\t\tresult = cls.plugins_manager.process_tasklist(task)\n\t\tadd_tasklist(result)\n\t\treturn\n\n\tif tasklist is not None and len(tasklist) > 0:\n\t\tadd_tasklist(tasklist)\n\n\nadd_tasklist_"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t# Register the factory method to make use of the API\n\tcls.plugin_manager = plugin_manager.API()\n\tcls.plugin_manager.register('Plugin')\n\nclass Plugin:\n\tdef __init__(self, name, module_or_id, location, params, options, default=None):\n\t\t# Build the plugin interface\n\t\tself._name = name\n\t\tself._module_or_id = module_or_id\n\t\tself._location = location\n\t\n\t\t# Build the plugin parameters \n\t\tfor k,v in params.items"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn cls\n\n// Copyright (c) 2019 PaddlePaddle Authors. All Rights Reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY"}]}
{"task_id": "6306091c73426c38ae68acdc", "prompt": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tfrom yaml.reader import Reader as YASReader\n\treturn YASReader.validate_from_content(spec_content)\n\n\ndef read_yaml_file(filename):\n\treturn YASReader.read_yaml_file(filename)\n\n\ndef validate_xml(spec_content):\n\tfrom xml.dom import minidom\n\ttry:\n\t\tdom = minidom.parseString(spec_content)\n\texcept ValueError:\n\t\tself.fail(f\"Invalid XML file: {spec_content!r}\")\n\treturn dom\n\n\n"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\trequire_keys(VALIDATOR_NAME, ['x-object'])\n\trequire_keys_inspec('x-object')\n\trequire_dict_inspec('zips')\n\tobj = spec_from_file(spec_content)\n\tif obj is None:\n\t\traise IRValidatorException(\"could not detect spec file contents\")\n\trequire_keys(obj)\n\trequire_keys_inspec('x-object')\n\trequire_keys_inspec('zips')\n\trequire_dict_inspec(obj)\n\treturn read_yaml(obj)"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif spec_content is None:\n\t\traise IRValidatorException(\"spec is missing\")\n\tif spec_content.get(\"type\", None)!= \"spec\":\n\t\traise IRValidatorException(\"spec is missing type field\")\n\tif (spec_content.get(\"type\") == \"\") or (spec_content.get(\"type\") is None):\n\t\traise IRValidatorException(\"spec type not found\")\n\tdata = yaml.safe_load(spec_content.get(\"data\", None))            \n\ttry:\n\t\t"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif not os.path.isfile(SPEC_FILE):\n\t\tmsg ='spec file'+ SPEC_FILE +'not found'\n\t\traise IRValidatorException(msg)\n\t\n\tschema_content = spec_parse(SPEC_FILE)\n\tdata_content = []\n\tschema = spec_parse(f'/spec/{SPEC_FILE}')\n\tfor name, val in schema.values():\n\t\tif val:\n\t\t\tdata_content.append((name, schema_to_string(val)))\n\t\n\treturn data_content, validate_from_"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from collections import Mapping\n\n\tschema = _get_schemas(spec_content)\n\tspec = yaml.safe_load(spec_content)\n\t_validate_spec_validation(schema, spec)\n\treturn Spec(schema)\n\n\n##########################################################################\n## Helper Functions\n##########################################################################\n\ndef _validate_spec_validation(schema, spec):\n\tif not isinstance(spec, dict) or not isinstance(spec['schema'], Mapping):\n\t\traise IRValidatorException(f\"Schema definition '{schema}' contains no'schema'"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\twith open(spec_content, \"r\") as input_f:\n\t\t\tobj_list = json.loads(input_f.read().strip())\n\texcept IOError as e:\n\t\traise IRValidatorException, str(e)\n\ttry:\n\t\tfor obj in obj_list:\n\t\t\tfor key, val in obj.items():\n\t\t\t\tif key.lstrip().find(\"required\") >= 0:\n\t\t\t\t\ttry:\n\t\t\t\t\t\t_fields = val.split(\"[\")\n\t\t\t\t\t\tif len(_fields)!= 2:"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t# we only look for data for specs\n\tdef validate_from_string(data):\n\t\treturn [str(k) for k in data.splitlines()]\n\t\t\n\treturn validate_from_content(spec_content).merge({'id': spec['id'], 'name': spec['name']})\n\n'''\n\tdef validate_yaml(doc, params=None):\n\t\t'''\n\t\tgenerates a dict with all required attributes\n\t\t'''\n\t\tif len(doc)!= 3:\n\t\t\traise ValueError('wrong doc size - not %d' % len"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tresult = cls({})\n\tdata_path = os.path.join(root_path,'spec.yml')\n\ttry:\n\t\tif isinstance(spec_content, dict):\n\t\t\tfor (key, value) in spec_content.items():\n\t\t\t\tf = open(os.path.join(data_path, key), 'r')\n\t\t\t\tresult[key] = f.read()\n\t\t\tf.close()\n\texcept FileNotFoundError:\n\t\t# This is very ugly. There are a few ways we can get to it, so we just\n"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\treturn cls.from_spec(spec_content=spec_content, data=yaml.SafeLoader())\n\n# def validate(self, spec_content = None,\n#\t\tdata=None,\n#\t\tvalidate_from_content = None,\n#\t\tcheck_length=True,\n#\t\tvalid_from_content=False,\n#\t\treturn_type = 'dict')\n#\n# def validate_or_fail(self, spec_content = None,\n#\t\tdata=None,\n#\t\tvalidate_from_content = None,"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tdef validate_from_content_in_ir_doc(cls, spec_content):\n\t\t\"\"\"\n\t\t:param spec_content: content of spec file\n\t\t:return: Dictionary with data loaded from a spec (IR-style doc)\n\t\t\"\"\"\n\t\tdoc = Nokobody(cls)\n\t\tdoc_type = Nokobody._type_to_name[Nokobody._type_to_name.index(NokobodyType.Doc)]\n\t\terror_msg = \"Spec content is not found, cannot validate.\"\n"}]}
{"task_id": "6306091c73426c38ae68acdd", "prompt": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif not yaml_file:\n\t\traise ArgumentError(\n\t\t'yaml_file is required if --from-file is not given'\n\t)\n\twith open(yaml_file) as fp:\n\t\tyaml = yaml.safe_load(fp)\n\treturn yaml, []\n\nimport { getSerdePlugin } from \"@aws-sdk/middleware-serde\";\nimport { HttpRequest as __HttpRequest, HttpResponse as __HttpResponse } from \"@aws-sdk/protocol-http\";\nimport { Command as $Command } from \"@aws-sdk/smithy-client\";\nimport {\n "}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\traise NotImplementedError()\n\traise IRValidatorException()\n\traise ValidationError(\"Required fields are missing:\", validation_data=self.fields)\n\n\n@validate_data_type('dict')\ndef validate_data(cls, data, schema):\n\tassert (len(data.keys()) == schema.get('enum', True))\n\tfor key in data.keys():\n\n\t\tif key_is_valid(schema['enum'][key]):\n\t\t\tfor field_value in schema['enum'][key]:\n\t\t\t\tassert isinstance(data[field_value], SchemaField)\n"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\ttry:\n\t\treturn validate(yaml_file=yaml_file)\n\texcept IRValidatorException as e:\n\t\tself._LOGGER.error(\"Couldn't load YAML file {0}\".format(yaml_file))\n\t\tself._LOGGER.error(e)\n\n\n#--IRValidator for all IRCs\nclass IRValidator(object):\n\tdef __init__(self):\n\t\tself._ir_validator = IRValidator()\n\tdef validate_ir(self, ir_file):\n\t\ttry:\n\t\t\twith open(ir_file, 'r"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\tfor line in f:\n\t\t\t\tline = line.strip()\n\t\t\t\tif line.startswith('#'):\n\t\t\t\t\tcontinue #skip line comments\n\t\t\t\tif not line.startswith('#'):\n\t\t\t\t\tcontinue\n\t\t\tdata = yaml.load(fp=f, Loader=yaml.FullLoader)\n\t\t\tfor key,value in data.items():\n\t\t\t\tassert key in cls\n\t\t\t\tassert isinstance(value, str)\n\t\t\t\tnew_item = cls[key]\n\t\t\t"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t__class__.__doc__: = 'The main module for creating all necessary information\n\t.. code-block:: python\n\t\tfrom pyrrdb.ir import validator, validators\n\n\t\tvalidators.load_from_yaml(\n\t\t\t\t\t\t\t\t\t  args.data('path',\n\t\t\t\t\t\t\t\t\t\t  '''{\n\t\t\t\t\t\t\t\t\t\t\"name\": \"test\",\n\t\t\t\t\t\t\t\t\t\t\"type\": \"int\",\n\t\t\t\t\t\t\t\t\t\t\"value\": 12\n\t\t\t\t\t\t\t\t\t\t}'''), \n\t\t\t\t\traise=False)\t\t\t\t\t\t\t\t\t\n\t\t# validators.validate_from_file(args.yaml(data=validators.data))\n\t\"\"\""}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\treturn DataValidator.validate_from_file(yaml_file=yaml_file)\n\n\n\n\ndef validate_from_url(name=None):\n\t\"\"\"\n\tLoads & validates that a URL is in format of <scheme><hostname>,<port>\n\t\n\t:param name: URL used to load and validate the scheme\n\t:return: A Dictionary with data parsed from the URL.\n\t\"\"\"\n\tres = {}\n\tres['scheme'] = urlparse.urlparse(name)\n\tres['host'] = urlparse.urlparse(res['scheme'] +"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tfrom rafcon.validators.ir import YAMLValidator\n\ttry:\n\t\tYAMLValidator.validate_from_file(yaml_file=yaml_file)\n\texcept:\n\t\tif type(True) in [True]:\n\t\t\traise IRValidatorException(\n\t\t\t\t'No valid YAML file given. Valid YAMLs:\\n' + \\\n\t\t\t\tyaml_validator_messages.get_required_fields_validation_messages(cls) + ''\n\t\t\t)\n\n/**\n * Aptana Studio\n * Copyright (c) 2005-2011 by Appcelerator, Inc"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\ttry:\n\t\tyaml_dict = yaml.load(yaml_file)\n\texcept py3k.ParseError:\n\t\traise ValueError(\"File \\\"%s\\\" does not exist or cannot be loaded\" % yaml_file)\n\treturn cls.validate(yaml_dict, False)\n\n\n#--------------------------------------------------------------------------\n# Python3 kit class\n#--------------------------------------------------------------------------\n\nclass KIT(Base):\n\n\tdef __init__(self, user_config, config_file, debug):\n\t\tBase.__init__(self, user_config, config_file)\n\t\tself.init."}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tfrom pyti import settings\n\tfrom pyti.lib.abstract_module import AbstractModule\n\t\n\ttry:\n\t\tsettings.enable_data_validation = True\n\t\tdata_root = cls.load_data(yaml_file)\n\n\texcept:\n\t\tassert False, \"Invalid YAML file\"\n\t\n\treturn data_root\n\t\n\nregister = AbstractModule('module_check_if_data_present', validate_from_file)\n\nexport const ic_wifi_close = {\"viewBox\":\"0 0 24 24\",\"children\":[{\"name\":\"path\",\"attribs\":{\"d"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif yaml_file is None:\n\t\traise Exception('No YAML file given')\n\twith zipfile.ZipFile(yaml_file, mode='r') as zip_file:\n\t\tparsed = json.load(zip_file)\n\t\n\tdef validate_from_yaml(cls, yaml_yaml_data=None):\n\t\tif yaml_yaml_data is None:\n\t\t\traise IRValidatorException('No YAML YAML data provided')\n\t\n\t\tnew_dict = {}\n\t\tfor key, values in zip(parsed.keys(), zip(parsed[key], yaml"}]}
{"task_id": "6306091d73426c38ae68ace5", "prompt": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tif parser_dict.get('includes'):\n\t\tfor key, value in parser_dict.get('includes').items():\n\t\t\tif not key in self._include_dict:\n\t\t\t\traise ValueError('Not in include list: %s' % self._include_data.get(key, {}))\n\t\t\tvalue = self._include_dict.get(key)\n\t\t\t# Check the directives are defined. See if the directive can be used\n\t\t\tif not self.directive_spec._is_directive(value, parser_dict):\n\t\t\t\traise ValueError('Directive %"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tif parser_dict.get('include'):\n\t\tparser_dict['includes'] = parser_dict.pop('include')\n\n\n@_directive_dict\ndef _default_groups(self, parser_dict):\n\tparser_dict['default'] = parser_dict.get('default')\n\npackage org.jboss.resteasy.test.security.resource;\n\nimport javax.ws.rs.GET;\nimport javax.ws.rs.Path;\n\n@Path(\"/\")\npublic class CDIfEndpointApiResource {\n\n   @GET\n   public String"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t# Find all group.include directives in spec. This must be separated\n\t# into a list of groups.\n\tgroups = []\n\tfor group in parser_dict['group']:\n\t\tif '-include' in group.key:\n\t\t\tgroups.append(group['key'])\n\treturn groups\n\n\ndef _replace_groups(group):\n\t\"\"\"\n\tAdd replacement group to the list of groups.\n\t\"\"\"\n\tfor directive in group:\n\t\tif directive.key.endswith('group.include.'):\n\t\t\tgroup['group'] = '.'+ directive"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tself.parser_file = parser_dict['spec']\n\tself.include_dict = parser_dict['includes']\n\ndef _generate_name(item):\n\tdef _gen_name(vtype):\n\t\tif isinstance(vtype, (type, UnicodeType)):\n\t\t\tname = getattr(item, '__name__', item.__name__)\n\t\telif isinstance(vtype, (type, TupleType)):\n\t\t\tvargs = []\n\t\t\tfor i,v in enumerate(vtype):\n\t\t\t\tvargs.append(i)\n\t\t\tname ="}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tinclude_defs_name = parser_dict.get(\"include_defs\")\n\tif include_defs_name:\n\t\tinclude_defs_value = parser_dict.get(include_defs_name)\n\t\tfor name, value in include_defs_value.items():\n\t\t\tinclude_spec = self._build_basespec(name)\n\t\t\tif name in include_defs_value:\n\t\t\t\t# Ignore conflicts with importspec.\n\t\t\t\t# Note: importspec assumes that *all* importspecs exist\n\t\t\t\ttry:\n\t\t\t\t\timportspec.parse"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfor name, value in parser_dict.items():\n\t\tif name.casefold().strip() == 'include':\n\t\t\tparser_dict['_include'] = value.strip().split()\n\n/*\n * Copyright (C) 2014 BlueKitchen GmbH\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n * 1. Redistributions of source code must retain the above copyright\n *    notice, this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t# Parse the input dict-directive.  This also sets the\n\t# include_vars attribute if one is present.  Also,\n\t# resolve the directive itself as absolute.\n\n\tdef _process_body(parser, body, parse_tree):\n\t\t# Skip this element.\n\t\tparser.error('expected directive or include dict',\n\t\t\tlineno=parser.filename.find(parser.endline))\n\n\t\tparse_tree = self._translate_parse_tree(parser, body, parse_tree)\n\t\tif parser._directives.has_directive("}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tm = self.parser_schema['include'][0]\n\treturn m\n\n\nclass _Parser(_ParseBuilder):\n\tspec = _Spec(self.data_spec, strict=False)\n\n\tdef _build_body(self, parser_dict):\n\t\t# TODO: handle other directives\n\t\treturn []\n\n\tdef _compile_body(self, parser_dict):\n\t\t# TODO: handle other directives\n\t\treturn []\n\n\nclass Import(_Parser):\n\t'''\n\tA class for defining the __import__ directives on imports to include\n"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tdirective_dict = parser_dict['self.directive_dict']\n\tparser_dict['extensions']['require_group_func'] = directives.bool(directive_dict.get('require_group_func', True), parser_dict,'require group func directive')\n\tparser_dict['extensions']['require_func'] = directives.bool(directive_dict.get('require_func', True), parser_dict,'require func directive')\n\tparser_dict['extensions']['require_init_group'] = directives.bool(directive_dict.get('require_init_group', True),"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t# This was a lite attribute, but our current dict\n\t# lookup works now\n\tinclude = {\n\t\t# If the \"groups\" directive is set on the include directive (or not on it)\n\t\t# The groups directive is used to determine which groups of elements\n\t\t# are defined.\n\t\t# We also add some default values.\n\t\t[\n\t\t\t\"default\",\n\t\t\t[\n\t\t\t\t(\"list\", \"dict\", \"data\", \"dict\"),\n\t\t\t\t(\n\t\t\t\t\tTrue,\n\t\t\t\t\t[],\n\t\t\t\t\t{},\n\t\t\t\t\t{}"}]}
{"task_id": "6306092373426c38ae68acfa", "prompt": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t# TODO: Add support for other sources?\n\tif hasattr(getattr(self, '_specvars'), '__call__'):\n\t\tkwargs = {}\n\t\tfor key, value in self._specvars._kwargs.items():\n\t\t\tif key.startswith('xsi_btype_'):\n\t\t\t\tkwargs[key[11:]] = value\n\t\telif key == 'xsi_type_':\n\t\t\t# TODO Use btype_ for XSI\n\t\t\tkwargs['xsi_type'] = getattr(self, '_type', 'NoneType')  # not an actual"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tdefaults = {}\n\tif self.build_type == 'none':\n\t\tdefaults['build_type'] = None\n#\t\tconfig_keys_to_ignore = []\n\n\tfor source in self.spec:\n\t\tif 'build_type' in source:\n\t\t\tdefaults['build_type'] = source['build_type']\n\t\t\tdelete_build_from_file(source['build_from']) if source['build_from']\n\n\tif 'name' in self.spec:\n\t\tdefaults['name'] = self.spec['name']\n"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\treturn self.spec_defaults()\n\ndef resolve_args(self, *args, **kwargs):\n\n\t\"\"\"\n\t\tResolve arguments against spec.\n\t\t\"\"\"\n\t\t# XXX can't do much if we don't have any options but just a name,\n\t\t# otherwise options will just work\n\n\tif len(args)!= 0:\n\t\treturn self.spec_parser.parse_args(*args)\n\telse:\n\t\treturn None\n\ndef get_action_parser_class():\n\t\"\"\"\n\t\tReturn action parser.\n\t\t"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tspec_defaults = dict()\n\tdef_spec_defaults = None\n\targspec_defaults = None\n\tspec_defaults = self.spec_defaults()\n\tnoargs_args = 0\n\tnokwargs_args = 0\n\tif len(self.spec._default) == self.argspecs.Spec.spec._default and \\\n\t\tself.spec._default:\n\t\tfor name, p in self.spec._default.items():\n\t\t\tval = self.get_spec_value(name)\n\t\t\tspec_defaults[name] = self.spec"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t# Parse the main module\n\tmain_module = self.get_option('main')\n\t# Parse the main source file\n\tmain_name = self.get_option('file_name')\n\t# Parse the main source directory\n\tmain_path = self.path\n\t# Recurse and return results\n\targs, extra = self.get_spec_from_argv(main, main_name, main_path)\n\treturn args, extra, ''\n##########################################################################################################################################\n\nclass ArgumentList(list):\n\t\"\"\"\n\tParse a given argument list and put results"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tself.spec_default_values_to_resolve = {}\n\n\tdef get_spec_defaults(self):\n\t\tcurrent_spec = self.spec\n\t\tself.spec = []\n\t\tself.spec_args = []\n\n\t\tfor key in sorted(current_spec.keys()):\n\t\t\tif hasattr(current_spec[key], 'default'):\n\t\t\t\tself.spec_default_values_to_resolve[key] = get_default(current_spec[key]['default'],\n\t\t\t\t\t\t[current_spec[key]['key']]) or {}"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tfrom. import options as c\n\n\tif c.options.check_defaults:\n\t\tpass\n\telse:\n\t\tfor name, source in c.get_sources():\n\t\t\tspec_name = source\n\t\t\tif '=' in name:\n\t\t\t\tif len(c.sources_re.split(',')) == 1:\n\t\t\t\t\tif name[0] in ['.d', '.git']:\n\t\t\t\t\t\tname, source = c.make_spec_info(self, spec_name)\n\t\t\t\t\t\tspec = spec_name + '=' + str(source)\n\t\t\t\t\t"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\targs, other_args = argparse.parse_known_args()\n\n\t# Extract key values for the user (used to make the args structure\n\t# easier).\n\tparser = argparse.ArgumentParser(description = \"Apply the arguments for an Argparse file and other args file.\",\n\t\tprog='kivy.argparse', parent=spec, file=sys.stderr, formatter_class=ArgumentDefaultsFileFormatter)\n\targs.validate_file()\n\tparser.add_argument('source', metavar ='source_file', help = 'the source of the args file',"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tdef set_default(): pass\n\tdef get_default(spec_name=None): return \"spec\" if spec_name is None else \"\\'spec'\"\n\tdef get_spec_defaults(spec_name): return \"spec\" if spec_name is None else \"\"\n\tdef set_default_from_spec(spec_name, spec, default_file=\"file.py\"):\n\t\t\"\"\"\n\t\tParse config files and set the variables from spec and the default\n\t\tfile as spec default values.\n\t\t\"\"\"\n\t\twith PathManager.get_temp_dir"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\treturn get_kwargs_map()\n\n<?php\n\n/*\n * FileSender www.filesender.org\n *\n * Copyright (c) 2008-2014, OnEachWeb\n *\n * This file is part of php-filesender.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software"}]}
{"task_id": "6306092973426c38ae68ad01", "prompt": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\tdeprecated_opts = {}\n\tdeprecated_opts['strict_option'] = self.strict_option\n\tdeprecated_opts['deprecated_version'] = self.deprecated_version\n\n\n\treturn deprecated_opts\n\n\ndef _get_deprecation_warnoption(opts, cmd):\n\tdeprecation_msg = 'This option will become deprecated, please use the -u option instead.'\n\tfor option in opts.args:\n\t\tif not option.startswith('--'):\n\t\t\tdeprecation_msg +='-' + option\n\t\telse:\n\t\t\tdeprecation_msg += option\n"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\twarnings.simplefilter(\"always\")\n\tfor deprecated in [opt for opt in DEPRECATED_OPT_LIST if opt and opt(DeprecationWarning)], deprecated_options:\n\t\tif not deprecated:\n\t\t\tdeprecated_options.remove(deprecation)\n\t\tkwargs = {\n\t\t\t\"deprecated\": deprecated,\n\t\t}\n\t\treturn kwargs\n\treturn\n\ndef listify(arg, dict_type, deprecated_types=DeprecationWarning):\n\t\"\"\"\n\tList up all the args and options and return them.\n\t\"\"\"\n\tif arg not in deprecated_types:\n\t"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\tself.warning_options = []\n\ttry:\n\t\tmodulename = self.module_name_orig\n\n\texcept AttributeError:\n\t\tmodulename = \"No such module\"\n\n\ttry:\n\t\tfor module in __import__(modulename):\n\t\t\tif not hasattr(module, '_deprecated_%s__' % modulename):\n\t\t\t\tself.warning_options.append(modulename)\n\n\t\treturn None\n\texcept ImportError as exc:\n\t\tif str(exc).startswith('is None'):\n\t\t\tself.warning"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\targs = {\n\t\t'--help': self.help\n\t}\n\treturn args\n\n\ndef add():\n\tfor arg in arguments():\n\t\tif hasattr(arg, 'check_mode'):\n\t\t\tadd_argument(arg)\n\t\telif arg in self._all_args:\n\t\t\tadd_argument(arg)\n\tdel arguments()\n\t# Add all warnings to the warnings list if any:\n\twarn_on(\n\t\t_warn,\n\t\t'The --warn option is deprecated and will be handled '\n\t\t'by a different"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t#TODO: Use args\n\tparser = self.__class__.get_args()\n\tparser.reporter(\"DeprecationWarning\")\n\ndef get_args(self):\n\t#TODO: use self.config\n\toptions, self.__dict__ = self.__class__.get_args()\n\t# TODO: use __name__\n\targs = parser.parse_args()\n\t# TODO: deprecate args\n\treturn args\n#def get_help():\n#\tusage = \"\\n\".join([\"\\n\".join(usage) for _ in sys.argv]) #.format("}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\twarnings.simplefilter(action='always', category=DeprecationWarning)\n\treturn {}\n\t###print(\"DEPRECATED - DEPRECATED\")\n\treturn {\"cmd.test\": True}\n\n<?php\n\nnamespace SMW\\SQLStore\\QueryEngine;\n\nuse Exception;\nuse SMW\\Storage\\Database;\n\n/**\n * A wrapper for QueryEngine object implementation that does nothing.\n *\n * @license GNU GPL v2+\n * @since 2.5\n *\n * @author mwjames\n */\nclass QueryEngineNoReturn implements QueryEngine {\n\n\t/**\n\t"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(usage=\"\\n\\nDeprecated help \"\n\t\t\"command (add more info here!) in this\\n\\n\"\n\t\t\"usage:\\n\t\"\n\t\t\"\\t\"\n\t\t\"add_plugin add_option\\n\"\n\t\t\"add_plugin remove_option\\n\"\n\t\t\"add_plugin configure_settings\\n\"\n\t\t\"add_plugin_for_plugin\\n\"\n\t\t\"add_plugin_settings_to_settings\\n\"\n\t\t\"remove_plugin remove_option\\"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t# print_str = self.get_deprecated_args()\n\treturn {\n\t\t\"no_warn\": 1,\n\t\t\"show_default_only\": 1,\n\t\t\"silent\": 1,\n\t}\n\n\ndef print_str():\n\tfor arg in self.deprecated_args:\n\t\tprint(json.dumps(dict(get_deprecated_args(*arg)), indent=2))\n\n\nif __name__ == \"__main__\":\n\tfrom datetime import date, datetime, timedelta\n\n\tprint(\"--- BEGIN_DATA ---\")\n\tdf"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\tself = self\n\treturn [\n\t\t('verbose', c_float, 0, default='1'),\n\t\t('debug', c_bool, 'v'),\n\t\t('show_all', c_bool, 'f'),\n\t\t('show_file', c_bool, 'b'),\n\t\t('show_lines', c_bool,'s'),\n\t\t('show_message', c_bool,'m'),\n\t\t('show_time', c_bool, 't'),\n\t\t('trace', c_int, 0),\n\t\t('verbose"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\tdeprecated, _warn = self.get_options()\n\treturn {\n\t\t'options': deprecated,\n\t\t...deprecated.get('options', {}),\n\t}\n\n/**\n * Copyright (c) 2010-2021 Contributors to the openHAB project\n *\n * See the NOTICE file(s) distributed with this work for additional\n * information.\n *\n * This program and the accompanying materials are made available under the\n * terms of the Eclipse Public License 2.0 which is available at\n * http://www.eclipse.org/legal/epl-2.0\n *\n *"}]}
{"task_id": "6306092c73426c38ae68ad02", "prompt": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t_cli_args = cli_args\n\t\n\t# We're going to be interested in the first non-deprecated argument\n\tif \"arg_deprecated\" not in _cli_args or \"_args_deprecated\" in _cli_args:\n\t\t_cli_args[\"arg_deprecated\"] = True\n\t\t\n\t# Validate the arguments, and set up output files\n\tself.print_arguments(cli_args, answer_file_args, self.ARG_DEPRECATION)\n\t\n\t# If we're done, then set up a second argument (which we need)\n\tif answer"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tcli_args = cli_args[\"cli.options\"]\n\tanswer_file_args = cli_args[\"files.options\"]\n\n\tif not cli_args[\"cli_args\"][\"deprecated\"]:\n\t\tcli_args[\"cli_args\"][\"deprecated\"] = True\n\n\tif not cli_args[\"cli_args\"][\"deprecated\"]:\n\t\tcli_args[\"cli_args\"][\"deprecated\"] = True\n\n\tif not answer_file_args['answer_file_option']:\n\t\tanswer_file_args['answer_file_option'] = True\n\n\tif not answer"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\targuments = CommandLineInputs(cli_args)\n\t\t\n\tmessage = f\"**This is a message**\\n{str(options.help)}\"\n\t\t\n\t# If there is a default value, check for it\n\tif answer_file_args.get(\"answers\") and is_answer(answer_file_args.get(\"answers\")):\n\t\targuments.answer_file = None\n\t\t\n\t\tfor answer in answer_file_args.get(\"answers\", []):\n\t\t\tanswer_type = answer[\"type\"]\n\t\t\tanswer_text = \"``\" if answer_type"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tfor cli_arg, arg_name in cli_args.items():\n\t\t__, ___, arg_help, arg_long, = cli_arg.split()\n\t\tif arg_help!= 'help' or arg_long!= 'long':\n\t\t\tbreak\n\t\t\n\t\tif arg_long == '--usage':\n\t\t\tprint('    %s, %s: usage\\n' % (arg_name, arg_long))\n\t\t\tbreak\n\t\telif arg_long in SUPPRESS_ARGS:\n\t\t\tprint('    %s:"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tcli_args = self.get_arg_list()\n\tcli_args.extend(cli_args[-1])\n\tanswer_file_args.extend(\n\t\t[\n\t\t\t'-I',\n\t\t\t'messages',\n\t\t\t'-R',\n\t\t\t'messages_template',\n\t\t\t'-T',\n\t\t\t'messages_template_templ',\n\t\t\t'-h',\n\t\t\t'-v',\n\t\t\t'-c',\n\t\t\t'-c',\n\t\t\t('-r', '-r')\n\t\t]\n\t)\n\treturn"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\timport argparse\n\timport email.utils\n\timport requests\n\t\n\tif cli_args[\"name\"] not in (\"admin\", \"help\", \"index\", \"list\", \"help\", \"info\"):\n\t\tprint(\"Invalid option.\")\n\t\tsys.exit(1)\n\t\n\tif answer_file_args[\"name\"].endswith(\"help\"):\n\t\tprint(\"The help message file must be placed after the name of an answer file. \"\n\t\t\"Help must not be sent in the answer file by default.\")\n\telse: # if answer_file_args[\"name\"] == \""}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tfor new_cli_arg in cli_args:\n\t\tif new_cli_arg not in self._invalid_arguments:\n\t\t\tmsg = \"    --%s --%s\\n\" % (self._invalid_arguments.get(new_cli_arg, ''), new_cli_arg)\n\t\t\tprint(msg)\n\t\t\t\n\t\tif new_cli_arg in self._deprecated_command in answer_file_args:\n\t\t\tprint(msg)\n\t\t\t\tprint(self.deprecated_option_name_short(new_cli_arg))\n"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tanswer_file_args.update(cli_args)\n\targ_match = self._validate_valid_arg_deprecation(cli_args)\n\tif arg_match:\n\t\treturn False\n\t\n\t# Warn user why the arg match failed in favor of a CLI argument\n\tif not answer_file_args.get(\"cli\"):\n\t\tcli_arg_match = False\n\t\tfor cli_arg in answer_file_args[\"cli\"]:\n\t\t\tif cli_arg not in cli_args:\n\t\t\t\tcli_arg_match = True\n\t\t\t\tbreak"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tassert (isinstance(cli_args, dict), \"cli_args must be dict\")\n\tassert (isinstance(answer_file_args, dict), \"answer_file_args must be dict\")\n\tpass#IMPORTIMPORTIMPORT import argparse\n\tpass#IMPORTIMPORTIMPORT import sys\n\n\tpass#IMPORTIMPORTIMPORT from pyadml.utils.exceptions import ConfigurationError\n\t\t\n\tdef deprecated_print(msg, argtype, argname, use_default=not sys.stdout.isatty()):\n\t\ttry:\n\t\t\tcmd = \""}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\targs = cli_args.copy()\n\t\n\toptions = {}\n\toptions[\"name\"] = \"foo\"\n\toptions[\"value\"] = 1\n\toptions[\"name\"] = \"test\"\n\toptions[\"value\"] = 0\n\tfile_args = cli_args.copy()\n\tfile_args[\"name\"] = \"test\"\n\tfile_args[\"value\"] = 1\n\targs[\"output\"] = file_args\n\n\tvalidate_arguments(args, options)\n\t\n\tanswer_file_args[\"error\"] = \"foo\"\n\tvalidate_argument(args[\""}]}
{"task_id": "6306092d73426c38ae68ad04", "prompt": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tparser = self.get_parser()\n\toptions = parser.get_option_specs(command_name)\t\n\n\treturn options\n\n\n# =======================================================================\ndef main(args):\n\t'''\n\tMain function, call a command\n\t'''\n\ttry:\n\t\treturn parser.parse_args(args)\n\texcept KeyboardInterrupt as e:\n\t\tprint(\"You received CTRL+C! Exiting\")\n\t\tif e.name in ('KeyError', 'NoValidParameters', 'NoArgument'):\n\t\t\traise e\n\n\nif __name__ == '__"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\treturn self.parse_options(command_name)\n\n\nclass Parser(ParserBase):\n\t\"\"\"Parsers for the general 'py', 'py_utils', 'py_py', 'py_tools'\n\t\n\t:type parser_type: str\n\t:type parser_spec: dict\n\t:type parser_options: dict\n\t\n\t:param command_name: the command name,\n\t:type command_name: str\n\t:param parser_type: the name of the parser,\n\t:type parser_type: str\n\t:param parser_spec: parsed"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tself.show_all_parser_options(f's`s -F {-c} -i {command_name}')\n\t\n\tspecs = []\n\tspec = self.get_spec_option_by_name(command_name)\n\tif spec!= None:\n\t\tfor field, value in self.get_spec_options(spec):\n\t\t\tspecs.append([field, value])\n\t\n\treturn specs\n\ndef get_spec_option_by_name(self, feature_name):\n\t\"\"\"\n\tReturns an entry of the given spec for a specific field"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\toption_specs = []\n\tparsed = subprocess.Popen(command_name, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\tif command_name in parsed.stdout and not parsed.stdout.isatty():\n\t\toption_spec = parsed.stdout\n\telif command_name in parsed.stderr:\n\t\toption_spec = parsed.stderr\n\telse:\n\t\tprint \"Unknown command name passed as command (main/virsh/ospd/etc):\"\n\t\tprint repr(command_name)\n\t\tprint repr"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tcommand_specs = self.get_command(command_name)\n\treturn command_spec_parser.get_parser_options(command_name, command_specs)\n\n\t\n#\tdef get_command_parser_specs(self, command_name):\n#\t\treturn self.get_command(command_name).parser_list\n#\n#\tdef get_command(self, command_name):\n#\t\t\"\"\"\n#\t\t\tGets all the command options for the specified command\n#\t\t\t\"\"\"\n#\t\t\tcommand_specs = self.get_command"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tresult = []\n\tfor key in self.command_options:\n\t\tif key.lower().find(command_name.lower()) == 0:\n\t\t\tresult.append(key)\n\treturn result\n\n\"\"\"\nclass CommandParser:\n\t\"\"\"\n\tA class to parse command arguments for the command manager\n\t\"\"\"\n\tdef parse(self, main_cmdclass=None, virsh_cmdclass=None, ospd_cmdclass=None):\n\t\t\"\"\"\n\t\t:param main_cmdclass: class of main command (main, virsh, etc...)"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t# we check for command_name in the command_parser.commands\n\tfor parser_option_name in ('main', 'irsp', 'os-d', 'os-w', 'os-z'):\n\t\tcommandline_options = parser_option_name.split('.')\n\t\t\n\t\tfor command_option in commandline_options:\n\t\t\tself.check_option_name(command_option)\n\t\t\n\t\tparser_option_spec = get_options_spec(command_name, parser_option_name)\n\t\t\t\n\t\tfor command_option in parser_"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tif len(command_name):\n\t\treturn self.get('config', 'default', command_name)\n\treturn self.get('config', 'default')\n\n### Helper function ###\n\ndef split_string(value):\n\tif type(value) == type(' '):\n\t\treturn (' ', value)\n\treturn value\n\ndef split_tokens(value):\n\targs = split_string(value)\n\ttoken_index, token = 0, None\n\twhile token_index < len(args):\n\t\tif token is None:\n\t\t\treturn"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tcmd = self.manager.cmd_get_parser_option_specs('get_parser_option_specs', (command_name,))\n\tif not cmd:\n\t\t# could not find the command, so create a fake one\n\t\tcmd = [{'name':'main',\n\t\t'type': 'bool',\n\t\t'value': 0,\n\t\t'mandatory': True,\n\t\t},\n\t\t{'name': 'virsh',\n\t\t'type': 'bool',\n\t\t'value': 0,\n\t\t'mandatory': True,\n"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\treturn self.parser_spec.get_parser_option_spec(command_name)\n\ndef get_environment_option_specs(self, command_name, env_name=\"environment\", options=None):\n\t\"\"\"\n\tGets all the environment options for a specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:param env_name: the name of an environment variable to return. Defaults to main.\n\t:param options: (optional) a dictionary of specific options\n\t:return: tuple of all command options"}]}
{"task_id": "6306092d73426c38ae68ad05", "prompt": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tif command_name:\n\t\treturn self.get_optionspec(command_name, argument_name).get_spec()\n\telse:\n\t\treturn self.get_all_optionspec(command_name).get_spec()\n\n\n\ndef get_optionspec(self, command_name, argument_name):\n\t\"\"\"\n\tgets the specification for a given command-option or command-argument (an optional 'name' option)\n\t\"\"\"\n\n\tif command_name == '':\n\t\treturn ''\n\telse:\n\t\treturn self.get"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\twith open(os.path.abspath(os.path.join(self.output_path, _py_out_dir, 'options')), 'r') as f:\n\t\ttry:\n\t\t\treturn _get_option_spec(f.read(), command_name, argument_name)\n\t\texcept (AttributeError, ValueError, IOError) as e:\n\t\t\tprint(e)\n\ndef _get_option_spec(input_command, command_name, argument_name):\n\t# Get the python library version as a string\n\t# See http://stackoverflow"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn self.parsers.get(command_name).options[argument_name]\n\n#pragma once\n\n#    Copyright (c) 2012 Higor Paninaga (http://pigorpaninaga.codeplex.com/)\n#    Copyright (c) 2009 Jan T\u00f6hn (http://jtanete.at/x264/index.html)\n#\n#    Permission is hereby granted, free of charge, to any person obtaining a copy\n#    of this software and associated documentation files (the \"Software\"), to deal\n# "}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tcmd = 'get'\n\tif command_name[0] == '*' and not argument_name:\n\t\tcmd = 'all'\n\n\treturn self.run_command(cmd, argument_name)\n\n\ndef get_option_list(self, option_name):\n\t'''\n\tGets the list of all options matching the command specified.\n\n\tCommand names can be:\n\t\tget /<option_name>\n\t\tget /<option_name>/<dir_spec>\n\n\tArgument names can be:\n\t\toption_name\n\t\t"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn self._cli_option_spec_map.get(command_name, argument_name)\n\n\nif HAVE_PSL:\n\t# pylint: disable=redefined-builtin\n\t# TODO(rodrigueckorskiy): This will break, and have to do before\n\t#                          defining psl/pil since there are two\n\t#                       "}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn self.registry.get_option_spec(command_name, argument_name)\n\nclass Option(object):\n\tdef __init__(self, name=None, choices=None, default=None,\n\t\t\tboolean_on=None, description=(u\"Option description\"),\n\t\t\tchoices_display=None, default_metavar=\"  \",\n\t\t\thelp_metavar=\"   \",metavar_on=(bool, \"description\")):\n\t\tif choices is not None:\n\t\t\tself.choices_display = choices_display\n\t\telse:\n\t\t"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tself.debug('get_option_spec()', dictionary=command_name, name='get_option_spec(%s, %s)' % (command_name, argument_name))\n\tif argument_name in self.get_params().command:\n\t\treturn dictionary((command_name, argument_name.partition('[')))\n\treturn dictionary((command_name,))\nget_option_spec.command = 'get_option_spec( %s, %s )' % (command_name, argument_name,)\n\n/***************************************************************************\n *       "}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t# If the first argument is \"-\", use \"-\" instead for default value\n\tif (\n\t    not argument_name.startswith('-')):\n\t\targument_name = \"-\" + argument_name\n\n\treturn self._spec_map.get(command_name, argument_name)\n\npackage com.github.cloudyrock.mongock.driver.mongodb.springdata.v3.decorator.operation.executable.search.internal;\n\nimport com.github.cloudyrock.mongock.driver.api.decorator.operation.ExecutableDecorator;\nimport com.github.cloud"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn parser.get_option_spec(self, command_name, argument_name)\n\n\nclass _OptionCommandClass:\n\t\"\"\"The _OptionCommandClass class is derived in RocketCommands\n\tclass, and contains methods for helping to add parameters to\n\tthe command class.\"\"\"\n\tdef __init__(self, command):\n\t\tself.command = command\n\t\tself.params = None\n\n\tdef _wrap_command(self, command):\n\t\t\"\"\"Creates a new _OptionCommandClass without initializing the parameters\n\t\tas they are not stored in the database"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t# Get the command name\n\tif command_name == 'help':\n\t\tself._help_args = argument_name\n\telif command_name =='version':\n\t\tself.pylab.text_output(\"\\nVersion: %s\", self.version)\n\telif command_name == 'help_code':\n\t\tself.pylab.text_output(\"Python.%s: %s\\n%s\\n\" % (\n\t\t\tself.pylab_version, command_name, '\\n'.join(self.pylab_usage)))\n\telif command"}]}
{"task_id": "6306092d73426c38ae68ad06", "prompt": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\treturn args\n\ndef init_silenced_args(*args):\n\t\"\"\"\n\t\tInitialize silenced argument list.\n\t\"\"\"\n\tfor arg in args[-1:-1]:\n\t\tyield arg\n\tif args:\n\t\tdel args[1:-1]\n\n\nif __name__ == '__main__':\n\n\tif __debug__:\n\t\traise Exception(\"Silenced argument parser is now working!\")\n\t\n\timport sys\n\t\n\tsys.path.insert(0,\"data\")\n\tsys.path.insert(0,\"config\")\n\tprint(\"The first"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\treturn SilencedArgument( args.list, SilencedArgument.SilencedArgument.SilencedArgument.SILENCED_ARG_NAME)\n\n\ndef get_arg_name(name):\n\t\"\"\"\n\tGets the argument name for a given name\n\t\n\t:param name: The specified argument name.\n\t:return: The name.\n\t\"\"\"\n\tname = name[len(SilencedArgument.SilencedArgument.SilencedArgument.ARGUMENT_NAME_START):]\n\tif name[-len(SilizedArgument.SilifiedArgument.SilifiedArgument.ARGUMENT_NAME_START):"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\treturn [\"argv0\", \"argv1\", \"argv2\", \"argv3\"]\n\ndef get_random_args(self, args):\n\t\"\"\"\n\tlist of random argument\n\t\"\"\"\n\treturn args\n\ndef make_config(data=\"\"):\n\t\"\"\"\n\tCreates new 'cinder' configuration file\n\n\t:param data: data to base on\n\t:return:\n\t\"\"\"\n\t_CFG_PREFIXES += ['__cinder_config_']\n\t_CFG_PREFIXES += ['cinder']\n\t_CFG_PREFIXES += ['']\n\t"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\targs = np.array(args)\n\treturn args\n#\t# Silence a subset, ignoring the specified ones.\n\ndef split_silenced_args(args):\n\t# The following function takes a 1-D array, containing the elements,\n\t# the first n_args, and each of these elements\n\t# 1) the names and shape, in the order they are specified in this array\n\t# 2) optional keyword arguments, as a string of the form n_args=list\n\t# 3) The first name in the array, and the rest of the name are the"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tsimplified=[]\n\tfor _, arg in enumerate(args):\n\t\tsimplified.append(str(arg))\n\treturn simplified\n\ndef get_args(args):\n\t\"\"\"\n\tReturns the list of arguments for a command.\n\n\t:param args: The list of arguments.\n\t:return: list, the list of arguments.\n\t\"\"\"\n\treturn args\n\npackage com.fishercoder.solutions;\n\nimport java.util.HashMap;\nimport java.util.Stack;\n\npublic class _1051 {\n   "}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tsilent = []\n\tif args:\n\t\tfor arg in args:\n\t\t\tif args[key][arg]:\n\t\t\t\tsilent.append(key)\n\t\tif len(silent)==0:\n\t\t\tsilent = [key for key in args.keys() if args[key][0]==\"\"]\n\targs.sort()\n\treturn args[silent]\n\n\nclass PrintReport(object):\n\t\"\"\"Displays the report to the user. Args:\n\t- filename: Name of the file to which report was sent.\n\t- args: list of ReportArgs\n"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tnewargs = []\n\tfor arg in args:\n\t\tif arg.silenced:\n\t\t\tnewargs.append(arg.name)\n\t\telif os.path.splitext(arg)[1] == 'py':\n\t\t\tbase = arg\n\t\telif type(arg) in (int, str, bool, slice):\n\t\t\tbase = argparse.Argument(arg)\n\t\telse:\n\t\t\tbase = argparse.Argument(arg)\n\t\tif base.validate():\n\t\t\tif not (base.default is not None):\n\t\t\t\tnewargs"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\treturn self.get_args_by_func(\"Silenced\", args)\n\ndef get_interactive_args(self, args):\n\t\"\"\"\n\tlist of interactive args\n\n\t:param args: The received arguments.\n\t:return: list, interactive argument names\n\t\"\"\"\n\tif len(args):\n\t\targs = ['--help'] + args + ['--'],\n\t\tfor arg in args:\n\t\t\tif arg[0] == '--':\n\t\t\t\t# help\n\t\t\t\targs.update(arg[1:])\n\t\t\t\tcontinue\n\t\t\telse:"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tsilenced, error_line, exit_code, text = args\n\t\n\t# Silencing arguments\n\tif (not silenced) == text:\n\t\tself.warn(text or '<unknown>', line=error_line)\n\t\t\n\treturn silenced\n\t\n\t\ndef format_data_args(self, args):\n\t\"\"\"\n\tFormat the arguments to be passed to command.\n\t\"\"\"\n\tdata = args\n\t\n\t# Copy over arguments\n\targsCopy = args[:]\n\t\n\t# Copy under arguments\n\tdataCopy = data[:]\n\t\n\t# Copy over arguments"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tret = []\n\tfor arg in args:\n\t\tif isinstance(arg, tuple) and len(arg) == 4: # sliemonic = 0 = single\n\t\t\targ0 = arg[0]\n\t\t\t\n\t\tif isinstance(arg, str) and arg0 == \"single\" and not args[0]: # single args\n\t\t\tret.append(arg[1][0].lower())\n\t\t\tfor item in args: ret.append(item)\n\t\t\tcontinue\n\t\t# single arg\n\t\tif isinstance(arg, str) and arg0 == \"single"}]}
{"task_id": "6306092d73426c38ae68ad07", "prompt": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tfor arg in args:\n\t\tvalidate_requires_param(arg)\n\t\n\n\n_valid_required_kwargs = {}\ndef validate_optional_kwargs(self, kwargs):\n\"\"\"\n\tValid any required positional arguments.\n\t\"\"\"\n\tfor key in dict(self.__dict__.keys()).items():\n\t\tif key not in kwargs:\n\t\t\traise ValueError('This value is not supported: %s' % key)\n\n\n\ndef validate_kwargs(self, kwargs):\n\"\"\"Checks that all required arguments match the kwargs.\n\t\"\"\"\n\tfor key in self"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tfrom warnings import warnpy3k, filterwarnings\n\twarnpy3k.warn(RuntimeWarning, \"All required arguments have been provided.  Please check your \"\n\t\t\"requirements against the latest distribution, or download and install the \"\n\t\t\"latest version of the package.\")\n\n\ttry:\n\t\tfrom setuptools import setup\n\n\t\tsetup(name='pylint',\n\t\t\tversion=__version__,\n\t\t\tauthor='Dell',\n\t\t\tauthor_email='dell-dev@lists.osgeo.org',\n\t\t\tdescription=\"The pylint"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif len(args) == 0:\n\t\treturn\n\tif isinstance(args[0], str) and args[0] in self.requirements.requirements_str_keys():\n\t\treturn\n\tfor d in self.requirements.requirements_dict.values():\n\t\trequirements = self.requirements.get_requirements_from_dict(d[0])\n\t\tif not isinstance(requirements, list):\n\t\t\tcontinue\n\t\telse:\n\t\t\tif len(requirements) == len(args):\n\t\t\t\tcontinue\n\t\t\telif (None == self.requirements.get"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif len(args) == 0:\n\t\treturn False\n\t# Check if the argument is a function\n\tif not all(arg in args and arg[0] in ('t', 'f', 'x') for arg in self):\n\t\treturn False\n\treturn True\n\ndef set_python_args():\n\t\"\"\"\n\tCheck if any arguments are accepted as python arguments.\n\tIt also handles the arguments that were also passed.\n\t\"\"\"\n\ttry:\n\t\timport sys\n\texcept ImportError:\n\t\tpass\n\telse:\n\t\tsys.argv"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tfor i in range(len(args)):\n\t\tself.assertIn(args[i], (i,))\n\n\"\"\"\n    pyramid\n\nCopyright 2007 Anders Pasanicki\n\nBSD 2>> Functional Test\n\nThis test takes the following command-line arguments to exercise\nthe Pyramid in a standalone program (a.k.a test run). The following\noptions may be used as a dictionary of arguments:\n\n  -max_iters=<num> The maximum number of retries.\n\n  -out <testname> The file where the generated test will be placed.\n"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\targs_no_required = args\n\t# Check any existing required argument has been provided.\n\tfor arg in args_no_required:\n\t\tif arg not in self.required_args:\n\t\t\traise argparse.ArgumentError(\"This argument is mandatory.\")\n\treturn args_no_required\n\n\nclass CommandLineParser(argparse.ArgumentParser):\n\n\tdef error_message(self, msg):\n\t\t\"\"\"\n\t\tCheck if required args are missing or have errors\n\t\t\"\"\"\n\t\tmsg = \", \" + msg\n\n\t\tif self.required_"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\treturn all(all_is_tuple(subargs) for subargs in args)\n\npackage cn.wmyskxz.api;\n\nimport java.lang.reflect.Constructor;\nimport java.lang.reflect.InvocationTargetException;\n\n/**\n * @author \u6c6a\u5c0f\u751f\uff1f\n * @\u5fae\u4fe1\u516c\u4f17\u53f7 \u6dfb\u52a0 WebApiPermission\n */\npublic interface IWxUserApi {\n    public static final String WX_USER_SERVICE = \"wxx_user.service\";;// wxx-user \u63a7\u5236\n  "}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tfor p in self.required_args:\n\t\tif p in args:\n\t\t\tretvalue = []\n\t\t\tfor a in args[p]:\n\t\t\t\tif a not in args or (a[0] in ['=', '=', '+', \"-\"]):\n\t\t\t\t\tretvalue.append(False)\n\t\treturn retvalue\n\ndef add_file_options_to_parser(parser, options, config):\n\tparser.add_argument('--config-filename', default='checkpoints.yaml', metavar='FILENAME', nargs='+', help='Directory containing config files')\n\n"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tfor attr, value in args.items():\n\t\tif not value:\n\t\t\tcontinue\n\t\tif (self.args[attr] or hasattr(self, attr)) and not hasattr(self, attr):\n\t\t\traise ValidationError(\"Required argument '{0}' has no attribute '{1}' and is not a required argument.\".format(attr, \"required=\" + self.required))\n\nusing System;\nusing System.Numerics;\n\nnamespace Nethereum.Blockchain.TransactionProcessing\n{\n    public class TransactionManager\n    {\n      "}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tmissing_parameters = set([arg for arg in self.provided_arguments if not os.path.split(arg)[-1] in args])\n\tfor argument_no, parameter in enumerate(self.provided_arguments):\n\t\tif argument_no not in missing_parameters:\n\t\t\traise ValueError(\"Invalid argument(s) combination: {} {}\".format(\", \".join(argument_no.split(\"=\")), \")\".format(\" \".join(missing_parameters.intersection(argument_no))))\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license"}]}
{"task_id": "6306092d73426c38ae68ad08", "prompt": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\tif command_id = command_name.split(os.path.sep)[-1] \\\n\t\t\t\t\t: int(os.environ.get('TEST_PID', self._task_id),\n\t\t\t\t\t    16); \\\n\t\n\tif command_id!= self._task_id:\n\t\treturn [command_name]\n\n\tself._get_only_required_when_condition_args(options_spec)\n\t\n\targs_name = options_spec[1]\n\tif len(args_name)!= 2:\n\t\traise ValueError('Incorrect number"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\tif command_name!= \"help\":\n\t\treturn [], arguments\n\treturn [], arguments\n\n\ndef _get_conditional_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\tif command_name"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\targs = self._get_option_args(command_name, options_spec)\n\tfor name in args:\n\t\tassert (isinstance(args[name][0], str) and not args[name][1].has_default())\n\targs_with_required_when = []\n\tassert all([isinstance(arg[0], str) for arg in args][1:])\n\tfor arg in args:\n\t\tassert(isinstance(arg[0], str))\n\n\t\targuments_with_required_when_condition = option_parser(arg)\n\n\t\tassert"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\tif self.is_in_condition_defined():\n\t\tnew_options = {'required_when': options_spec}\n\t\treturn [name for name in self.get_given_args_with(command_name)]\n\treturn []\n\n\ndef _get_conditionally_required_when_condition(self, command_name,   args, options_spec):\n\t\"\"\"Determine argument names that should be matched using condition.\n\t\n\t:param command_name: the command name.\n\t:param args: the received input arguments\n\t:param options_spec:"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\tnargs_to_match = 1\n\tmatched_items = {\n\t\t\"config_file\": self._get_config_file(),\n\t\t\"args\": [arg.to_string() for arg in args],\n\t\t\"optional_when\": [arg.get(\"optional_when\") for arg in options_spec],\n\t\t\"required_when_flag\": \"RequiredWhenMatched\",\n\t\t\"required_when\": option if self._required_when_match else None,\n\t}\n\trequired_when = self._required_when_spec(matched_items)"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\tself._build_options_spec(options_spec, ['condition'])\n\tif len(args) > 1:\n\t\traise InvalidArgumentsError(f'expected at most once when {command_name} is given')\n\t\n\t# TODO: theoretically we would have to explicitly tell this, but the\n\t# options should be parsed here if there's more than one option.\n\tif isinstance(args[0], str):\n\t\tif len(args[0]) > 2:\n\t\t\traise TooManyRequiredArgumentsInOptionsError(\n\t\t\t\tcommand_name,\n"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\tfor arg_name in reversed(command_name):\n\t\trequired_when_option = self.required_when_condition_re.match(args[arg_name])\n\t\tassert required_when_option\n\t\tif required_when_option is not None:\n\t\t\tassert _get_condition_with_options(required_when_option, options_spec)\n\t\t\treturn [arg_name]\n\traise argparse.ArgumentError('Required argument `%s` not present. Please see `required_when`' % str(command_name))\n\n###################################\n"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\toptional_args = [f for f in options_spec\n\t\t\t\t\t  if ('required_when' in f.args and\n\t\t\t\t\t     ('command_name' not in f['args']\n\t\t\t\t\t      and 'args' not in f['args']\n\t\t\t\t\t      and 'option_types' in f['args']))]\n\tassert len(optional_args) == 1\n\tname_matched = optional_args[0].lower()[:-1]\n\tassert name_matched in (self.required_when_name,)\n\tassert cmdclass"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\ttry:\n\t\targs = self.condition_list_args(options_spec, command_name)\n\t\treturn args, args[0][0:len(args[0]):]\n\texcept TypeError:\n\t\tpass\n\t\n\t\ndef _is_option_match_case(cmd, cmd_name):\n\t\"\"\"\n\tChecks whether an option is a match case for specified command's name.\n\t\n\t:param cmd: command being matched.\n\t:param cmd_name: name of the command.\n\t\n\t:return: Boolean if value has to matched by both"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\treturn _get_conditional_args(self, command_name, options_spec)\n\n\n# TODO: Re-enable this\ndef _has_required_when_condition_match(self, command_name, options_spec,\n...args):\n\treturn all(args[s] for s in required_when_condition_match(self.arguments, command_name,\n\t\t...args))\n\n\n# TODO: Re-enable this\ndef _get_conditonal_default_value_options(self, command_spec, args):\n\tif args.get"}]}
{"task_id": "6306092e73426c38ae68ad09", "prompt": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\treturn self.check(len(args), raise_if_noarg(args.length, 'length'))\n\nclass TelnetMessageParserConfig(parser.MessageParserConfig):\n\t\"\"\"\n\tCustom parser for a TelnetMessage.\n\t\"\"\"\n\tis_configuring = True\n\tparser.MessageParserConfig.is_configuring = True\n\t\n\t@staticmethod\n\tdef validate(telnet_message_args):\n\t\traise NotImplementedError()\n\n\t@staticmethod\n\tdef check(length, args):\n\t\t\"\"\"\n\t\tCheck it is not :"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\treturn max(len(a) for a in args) > args.length\n\ndef validate_args(self, args):\n\t\"\"\"\n\tCheck that arguments are valid. For instance, the following code assumes\n\targuments are acceptable values such as\n\t- 1: int\n\t- -1: int\n\t- 'a'\n\t- int\n\t\"abc\": string\n\n\tIf more than one argument is expected a ValueError exception is raised.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\targs_valid = True\n\tif len(args) > 1"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif isinstance(args, argparse.Argument):\n\t\tif hasattr(args, 'val'):\n\t\t\ttry:\n\t\t\t\tself._ensure_len(args)\n\t\t\t\tself._send_value(type(args.val))\n\t\t\texcept Exception, e:\n\t\t\t\tprint('error:', e)\n\t\telse:\n\t\t\treturn False, None\n\t\n\treturn args.val!= None or self.length is None or self.length <= args.val\n\n\ndef validate_length_args_arg(self, arg):\n\"\"\"\n\tReturn length value\n\t\n"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif len(args.arg.value) > args.arg.maximum:\n\t\treturn False\n\treturn True\n\npackage com.alibaba.reap.cof;\n\nimport com.alibaba.reap.reap.AbstractReapClient;\nimport com.alibaba.reap.reap.BeanManager;\nimport com.alibaba.reap.reap.R;\nimport com.alibaba.reap.reap.exception.ApplicationException;\nimport com.alibaba.reap.reap.handler.AbstractReapHandler;\nimport com.alibaba.re"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif \"length\" not in args[0]:\n\t\treturn len(args) > args[0][\"length\"]()\n\treturn False \n\n// Copyright 2019-2021 gopkg.os.gnome.qsk, gopkg.os.gnome.qsk.enums;\n\n#define DISABLE_PLUGIN_LOADING_QSK 1\n#include \"qsk_common_p.h\"\n#include \"qltkcommon_p.h\"\n#include \"log.h\"\n\nstatic GtkWidget *qtk_log_dlg;\nstatic gint timeout = 0;"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif len(args) < get_value_limit():\n\t\traise argparse.ArgumentError(\n\t\t\"The incoming value(%s) length limit %s is smaller than input length limit %s!\"\n\t\t\t% (args[-1],get_value_limit(),get_value_limit()))\n\tif len(args) > (get_value_limit() - len(args)):\n\t\traise argparse.ArgumentError(\n\t\t\"Input(%s) length limit %s is higher than the maximum value allowed for this input\"%(\n\t\t args[ len(args)-1 ],"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif args == []:\n\t\treturn False\n\t\n\targ_len = self.length(args)\n\tif arg_len is not None and len(args) + arg_len > len(args[0]):\n\t\treturn True\n\treturn False\n\n/*\nCopyright 2006 Jerry Huxtable\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2."}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from sys import maxint\n\traise Exception(\"Maximum allowed length exceeded.\")\n\n\ncheck_min_args = True\nfor k, v in globals().items(): if not ((k if type(k) is six.string_types else v is six.string_types) or getattr(args, k) is None): check_min_args = False\n\ndef get_default_args():\n\t\n\treturn OrderedDict()\n\n\ndef get_default_params():\n\t\n\tdef defaults():\n\t\tfor k1,v1 in vars(params"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif len(args) > self.value_def.maxLength:\n\t\traise CLIError(\"Too large\")\n\t\n\tif len(args) == 0:\n\t\tif not self._validate_string:\n\t\t\traise CLIError(\"Missing value\")\n\t\telse:\n\t\t\targs = None\n\telif args[0]!= args[1]:\n\t\targs = None\n\t\n\treturn args\n\n\ndef validate_value_args(self, args):\n\t\"\"\"\n\tValidate the value of arguments.\n\t\n\t:param args: The received arguments.\n\t"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\targs = [arg.value for arg in args]\n\tif len(args) > self.value_length:\n\t\tself.error(\n\t\t\t\"length exceeds the value of %s\" % self.value_length\n\t\t\tif len(args) > self.value_length else \"value\"\n\t\t)\n\t\n\tif len(args) == 0:\n\t\treturn\n\t\n\t# Check if the argument passed is a valid value\n\tfor arg in args:\n\t\tif not isinstance(arg, Int) or arg < 0 or arg > len(args"}]}
{"task_id": "6306092e73426c38ae68ad0a", "prompt": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\treturn([val for val in args if val is not None], None) or 0\n\ndef _check_valid_choice_args(args):\n\t'''\n\tValidate the given arguments.\n\t\n\t:param args: The arguments object.\n\t:type args: argparse.ArgumentList\n\t\n\t:returns: the correct answer if valid.\n\t:rtype: dict\n\t'''\n\tif args == None or args.help!= []:\n\t\targs = arg(default=[], metavar=\"help\", help=None)\n\t\n\treturn OrderedDict([(\"yes\", True), (\"no\","}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tfor self.choices in itertools.chain(self.values, self.choices):\n\t\tfor name in self.values:\n\t\t\tvals_for_index = self.values[name]\n\t\t\tif vals_for_index in args:\n\t\t\t\traise ValueError('Cannot create choices from non-multi-value arguments.')\n\t\t\tfor idx, value in enumerate(vals_for_index):\n\t\t\t\tif value not in args:\n\t\t\t\t\traise ValueError('Cannot create choices from non-values for multi-value argument.')\n\treturn args\n\n@python_2_unicode"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t# TODO: Consider converting this more robust:\n\t# https://stackoverflow.com/questions/36287900/can-find-a-list-of-choice-and-choices-from-that-list-of-args\n\tchoices = [x for x in args if isinstance(x, (list, dict)) and list(x)[0] not in ALL_CHOICE_VALUES]\n\tif isinstance(args[0], list):\n\t\tassert isinstance(choices, tuple), 'choice argument should be tuple'\n\t\tassert len(choices) == 1, 'choice should only"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tchoices_list=list(filter(lambda arg: len(args) >= 2 and arg.lower() in choices and len(args[0]).isdigit(), args))\n\treturn len(choices_list) == 0\n\nclass ListValue():\n\tdef __init__(self, value):\n\t\tself.value=value\n\tdef __str__(self):\n\t\treturn self.value\n\nclass ComboBoxValue(ListValue):\n\tdef __init__(self, value):\n\t\tsuper(ComboBoxValue, self).__init__(value)\n\tdef __str__(self):\n"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif len(args) < 2:\n\t\tself.msg_noargs(self.choices)\n\traise Exception(self.msg)\n\n\t# Validate the remaining arguments.\n\tif len(args) == 2:\n\t\tself.check_choices(args[1])\n\telse:\n\t\tself.msg_badlength(args.choices, 1)\n\n\t# If a value was passed, we'll validate it (otherwise\n\t# we'll validate the remaining arguments too.\n\telse:\n\t\tfor arg in args:\n\t\t\tif not isinstance"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from warnings import warn\n\tchoice = args.choice\n\tif choice:\n\t\tchoices = [\n\t\t\tChoiceValueEnum.choice_from_string(x)\n\t\t\tfor x in self.choices\n\t\t]\n\t\tchoices = self.get_choices(choices)\n\t\tmsg = 'invalid choice: %s' % choices\n\t\tif not msg:\n\t\t\tself.error = 1\n\t\t\n\t# Get values for each choice\n\tfor choice in choices:\n\t\tself.validate_choice_arg(args"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tvalid_choices = {}\n\n\tfor arg in args.__args__:\n\t\tif arg.choices:\n\t\t\tvalid_choices[arg.name] = True\n\n\tif len(valid_choices) > 0:\n\t\treturn False\n\t\t\n\treturn True\n\n/*\n * Copyright 2020 The Backstage Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\treturn len(args) == 1 and all(arg == value for value in args[0])\n\n# -----------------------------------------------------------------------------\ndef prepare_argument_args():\n\t\t\n\"\"\"\n\tProcess argument arguments for validation.\n\t\"\"\"\n\t\ndef prepare_argument_value_options():\n\t\t\n\"\"\"\n\tHelper function for processing arguments of multiple validation options.  The arguments\n\tshould be specified in a list of tuple (name, value) pairs.\n\n\t:param args: The received arguments.\n\t:returns: The values of each tuple as a list of :class:`name => value` pairs"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif type(args) is not list and len(args) < 2:\n\t\traise ValueError('Invalid value for argument choices.')\n\n\tassert type(args) is tuple\n\tassert all(type(i) is tuple for i in args)\n\n\t# The choice arguments can only be strings.\n\tif not all(isinstance(arg,str) for arg in args):\n\t\traise ValueError(\"Invalid choice arguments '%s': '%s'.  \"\n\t\t\t\"Expected one of string, int, or a list of ints\", args, type(args))\n\n\t# The"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tchoices = []\n\tif args['choices']:\n\n\t\tassert isinstance(args['choices'], list), '\"choices\" should be a list as well as a Python tuple'\n\n\t\tif self._validate_choices(args):\n\t\t\tfor i in self._validate_choices(args['choices']):\n\t\t\t\tchoices.append(i)\n\telif args['choices'] and isinstance(args['choices'], list):\n\t\tchoices = args['choices']\n\telse:\n\t\tchoices = args['choices']\n\tassert isinstance(choices, list), \"Must be a list"}]}
{"task_id": "6306092e73426c38ae68ad0b", "prompt": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tpass\n\n/**\n * Copyright (c) 2000-present Liferay, Inc. All rights reserved.\n *\n * This library is free software; you can redistribute it and/or modify it under\n * the terms of the GNU Lesser General Public License as published by the Free\n * Software Foundation; either version 2.1 of the License, or (at your option)\n * any later version.\n *\n * This library is distributed in the hope that it will be useful, but WITHOUT\n * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n * FOR A PARTICULAR PURPOSE. See the GNU Lesser"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\ttry:\n\t\tif not isinstance(args, (int, float)):\n\t\t\traise TypeError('Expected integer or float, received %r' % type(args))\n\t\t\n\t\tif args < self.min_value or args > self.max_value:\n\t\t\traise ValueError('Invalid index %r (min=%r, max=%r)' %\n\t\t\t\t(args, self.min_value, self.max_value))\n\t\t\n\texcept TypeError as e:\n\t\terror = \"%s: Expected either 'float' index or 'float' value, received %s!\" %"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif len(args) < 1 or len(args) > 2:\n\t\traise InvalidArgument(\"Invalid number of arguments provided\")\n\t\t\n\tif args[0] < self.min:\n\t\traise InvalidArgument(\"Invalid value supplied.\")\n\t\t\n\tif args[1] > self.max:\n\t\traise InvalidArgument(\"Invalid value supplied.\")\n\t\t\n\treturn True\n\t\n\npackage org.basex.query.expr.path.atomic;\n\nimport org.basex.query.*;\nimport org.basex.query.expr.VarRef;\nimport org.basex"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif len(args) < 2:\n\t\traise CommandError(\"Expecting at least 2 arguments\")\n\n\tfor n in  range(2, len(args) - 1):\n\t\targmin = int(args[0][n])\n\t\targmax = int(args[1][n])\n\t\tif argmin > argmax:\n\t\t\traise CommandError(\"Invalid minimum argument %d\" % n)\n\n\n@_parse_response_error\ndef validate_sum_avg_args(self, args):\n\tsum = args[0][1]\n"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif len(args) < 1 and len(args.args) > 0 and len(args.args.args) >0:\n\t\traise ValueError(\"Validity of args is not allowed, args are: \"+str(args))\n\tif len(args) < 1:\n\t\traise ValueError(\"Validity of  min_max values is not allowed, min_max values are: \"+str(args))\n\n\"\"\"\n\n#ifndef PYTHONIC_INCLUDE_NUMPY_TRANSPOSE_HPP\n#define PYTHONIC_INCLUDE_NUMPY_TRANSPOSE_HPP\n\n#"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tfor arg in args.split(os.sep):\n\t\tif arg not in self.max_choices or self.min_is_valid(arg):\n\t\t\treturn False\n\t\n\treturn True\n\ndef validate_max_choices(self, args):\n\tfor arg in args.split(os.sep):\n\t\tif self.max_choices[arg] is not None:\n\t\t\treturn args.remove(arg.split(os.sep))\n\t\n\treturn True\n\ndef validate_min_is_valid(self, arg):\n\tfor arg in self"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif len(args) < 2:\n\t\traise exceptions.UsageError(\"Invalid arguments passed to {}\".format(args))\n\t\t\n\tif args[0] == \"min\":\n\t\tif args[1]:\n\t\t\t# This is the minimum allowed value given\n\t\t\tmin_val = args[1]\n\t\t\t\t\n\t\telif args[0] == \"max\":\n\t\t\tif args[1]:\n\t\t\t\t# This is the maximum allowed value given\n\t\t\t\tmax_val = args[2]\n\t\t\t\tif max_val!= None:\n\t\t\t\t\traise exceptions.Usage"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif not(type(args[0]) in (float, complex, int, int64)):\n\t\traise ValueError('Values on arguments must be a float or a complex number (not '\n\t\t\t\t'int/complex). Arguments:  %s' % ', '.join(self.__dict__.keys()))\n\t# convert to absolute values\n\targs0 = list(set(args[0].keys()) - set(self.__dict__.keys()))\n\targs1 = list(set(f'_init___{len(args)}'.split('_')[1:] if len(args) > 1"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif self.get_min_max_args(args) and len(self.get_min_max_args(args)) < args.get(\"minimum\", self.get_min_max_args(args)):\n\t\traise ValueError(f\"[{self} - {args}] is not between maximum and minimum values\")\n\n\ndef validate_default(default):\n\t\"\"\"\n\tValidate that a valid argument is passed default value of the argument.\n\t\n\t:param default: The received default argument value.\n\t\"\"\"\n\tif type(default) is not int and type(default"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif args is None:\n\t\treturn\n\tmax_val = float(max(args.get(\"min\")))\n\tmin_val = float(args.get(\"max\"))\n\tfor val, min_val in zip(args.get(\"min\"), min_val, max_val):\n\t\tif val > min_val:\n\t\t\treturn False\n\treturn True\n\nnamespace Microsoft.Azure.PowerShell.Cmdlets.CostManagement.Models.Api20200601\n{\n    using static Microsoft.Azure.PowerShell.Cmdlets.CostManagement.Runtime.Extensions;\n\n"}]}
{"task_id": "6306092e73426c38ae68ad0d", "prompt": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tprint(\"Calling %s: %s\" % (subcommand, spec[type_name]))\n\treturn self.call(subcommand, type_name, option_name, spec_option, **kwargs)\n\ndef create_complex_option(self, subcommand, type_name, option_name,\n\"c0, n, n_n, n0, c0_0, c1, c2, c3, c4_0, c5_0, options\"):\n\t\"\"\"\n\t:param subcommand: the command name\n\t:param type_name:"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from EMAN2 import *\n\tpass#IMPORTIMPORTIMPORT from string import ascii_lowercase\n\tpass#IMPORTIMPORTIMPORT from EMAN2 import  get_params_as_floats\n\t\n\tcmdclass = self.subcommand_table(subcommand)\n\tcmdclass['option_dict'] = dict(\n\t\tcomplex_options=self.options,\n\t\tcomplex_options_file=self.complex_options_filename,\n\t\toption_str=option_name,\n\t\tis_complex=True"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t_add_help(self, subcommand, \"complex\", \"Type definition\")\n\ttype_def = None\n\tdesc = type_name.split(\".\")\n\t\n\tn = len(desc)\n\t# option name and its value is built in type definition\n\tfor i in range(n-1, 0, -1):\n\t\tself.subcmddefs += [\n\t\t\t\"--type\", subcommand + \":\" + \"--type=\" + str(type_name[i]) + \";\" + \"\\n\",\n\t\t\t\"\"\"--type=\"\"\" + str(type_name"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tsubcommands = {subcommand: \"subcommand_complex_type\", type_name: \"type_name\",\n\t\t option_name: \"option_name\",\n\t\t \"options\": {option_name: \"option_value\"}}\n\targumet = complex_union_type.create_complex_argumet(self, subcommand, type_name, option_name)\n\treturn argumet\n\ndef create_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the argumet type\n\n\t:param"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tlog.debug('Computing subtype of type %s', type_name)\n\tlog.debug('Computing subcommand of type %s', subcommand)\n\t\n\ttry:\n\t\treturn self.subcommand_type(subcommand, type_name, option_name)\n\texcept:\n\t\tif verbose:\n\t\t\tmsg = \"Problem computing complex argument type %s.\" % type_name\n\t\t\tself._log_error(subcommand, msg, log.ERROR)\n\t\t\t\n\t\tdel self.kwargs[option_name]\n\t\tdel self.options["}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tif spec is not None:\n\t\tspec=list(spec)\n\tspec_dict = get_spec_options_dict(spec)\n\tcomplex_type_instance = create_complex_type_instance(subcommand, type_name, option_name, option_value, spec_dict, option_dict)\n\tcomplex_type_instance.set_subcommand(subcommand)\n\treturn complex_type_instance\n\ndef add_type_instance(cmd, type_name, option_name, spec_option, spec, list_option_dict):\n\t\"\"\""}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\targument, options, self._build_type(subcommand, type_name, option_name)\n\targumet_type = type(self, self._get_argument_type_spec(subcommand, argument, None,\n\t\tNone, self.option_type_registry_spec))\n\targument, options, value = argumet_type.create(subcommand, self._get_argument_value_index(type_name))\n\t\n\targument_specification = argument._get_argument_spec()\n\tif isinstance(argument_specification, dict) and not value:"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tret = []\n\n\tfor command, type_args in self.kwargs_spec[1:]\n\t:\n\t\tif isinstance(type_args, dict):\n\t\t\tfor subcmd in''.join(command.split(\" \")):\n\t\t\t\tfor param in [subcommand, type_args[subcmd], option_name]:\n\t\t\t\t\tif isinstance(self.options.subcommand[subcmd], type_args[param]) \\\n\t\t\t\t\t: \n\t\t\t\t\t\tret.append(type(param))\n\t\telse:\n\t\t\tret.append(type(type_"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t\n#def setup_module(self, subcommand, data=[]):\n#\t\"\"\" \n\t\tSetup the module to be used using the Python ``load_module`` function.\n\t\tIt can also be used to implement a ``build_subcommand`` call \n\t    that is in an ``interface`` subcommand.\n\t\"\"\"\n\t\n\tif self.options['mode'] == 'pywiz':\n\t\tif '-y' in self.options['command']:\n\t\t\traise Exception('{0} doesn\\'t work with -Y or -z.' \\\n\t\t\t\t.format"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\ttype_value = type_value_parser(subcommand, spec_option, {},'s')\n\t\n\t# Build the option\n\tcmd_option = option_build_parser(subcommand,'s', '--%s', type_name, option_name)\n\t# Do we have the argument type\n\targv = self.get_help_arg(subcommand, subcommand.argument_names,\n\t\tcmd_option.subcommand_options,\n\t\tcmd_option.option_name, False, cmd_option.parser)\n\n\t# Parse the command arguments"}]}
{"task_id": "6306092e73426c38ae68ad0f", "prompt": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tself.args = args\n\t\n\t# add_custom_control_args(self.args)\n\t\n\tself.custom_args = {}\n\t\n\tself.args.append(self.custom_args)\n\t\n\tfor narg in self.args:\n\t\tnarg_args = narg.args_to_run(self.args)\n\t\tnarg.control_args(self, narg_args)\n\t\tnarg.narg_args_control(self, narg_args, self.custom_args)\n\t\n\treturn self.args"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t# The nested variable of control args is always present and does not\n\t# have nested args, no need to call args, so its value is a dict\n\tcontrol_args_a = None\n\tnested_args_a = None\n\tcontrol_args_b = None\n\tnested_args_b = None\n\t\n\tif args is not None and len(args) == 0:\n\t\tif args_a is None or args2_a is None:\n\t\t\targs_a = []\n\t\telse:\n\t\t\targs_a += args\n\t\t\tif args2"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\targs = list(args)\n\tnested_args = list(args)\n\tcontrol_args = list(args)\n\tis_nested_control = not args\n\tis_main_play = not is_nested_control\n\tis_nested_play = not is_nested_control or args == []\n\tif args == []:\n\t\tcontrol_args = nested_args\n\tif is_nested_control and args == []:\n\t\tnested_args = [nested_args]\n\tnested_args = list(args_set(*nested_args))\n\t"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tif type(args) == list:\n\t\tself.arg_dict = args\n\t\tself.control_args = False\n\t\tself.nested_args = False\n\t\tself.custom_args = False\n\t\targs = dict(arg_dict)\n\telse:\n\t\tself.arg_dict, args = args\n\t\tself.control_args = True\n\t\tself.nested_args = True\n\t\tself.custom_args = True\n\tself.process()\n\treturn args, self.arg_dict\n\n\ndef get_nested"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tfrom ansible.module_utils.basic import *\n\t\n\targs, nested_args = self.__parse_args(args)\n\t\n\tnested_var = \"\"\n\t\n\tif nested_args is not None:\n\t\tnested_var = nested_args[0]\n\t\n\tcontrol_var = \"\"\n\tif not nested_args:\n\t\tcontrol_var = \"control\"\n\t\n\tif not self.exists_file(args):\n\t\treturn args, None\n\t\t\n\twith open(args, \"r\") as f:\n\t\tfor line in f."}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\targs = argdict(args)\n\tparams = list(args.parameters())\n\t\n\tfor param in params:\n\t\t# TODO: if the param is empty and it is a custom, make a deep copy of it.\n\t\t\n\t\tif param == \"control_args\":\n\t\t\targs[\"control_args\"] = list(args[\"control_args\"])\n\t\telif param == \"nested_args\":\n\t\t\targs[\"nested_args\"] = list(args[\"nested_args\"])\n\t\telif param in (\"control_args\", \"nested_args\") \\\n\t\t\t\t\t"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tself.logger.debug(\"Args: {}\".format(args))\n\targs = self.parse_args(args)\n\tif \"control_args\" in self.yml_params:\n\t\tself.parse_control_args(args)\n\telse:\n\t\tself.control_args = args\n\tself.parse_nested_and_custom_args(args)\n\tif len(args) == 0:\n\t\tself.logger.error(\"No options supplied to run_args\")\n\t\treturn False, {}\n\treturn True, args\n\n\ndef"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol = collections.deque([\n\t\t(\"control_args\", ControlArgument(\n\t\t\t'args', (ControlArgument.Value,)\n\t\t), collections.deque()\n\t]) )\n\tnested_args = collections.deque(\n\t\t[\n\t\t\t(\"Nested_args\", (ControlArgument(\n\t\t\t\t'args', (ControlArgument.Value,)\n\t\t\t), collections.deque(maxlen=maxlen)), nested)\n\t\t] )\n\n# We use the first control of the nested_args list to avoid circular reference.\nnested_args_first = nested_args[-"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tresult = {}\n\tif args is None:\n\t\tfor name in ('args','nested_args','customs_vars'):\n\t\t\tvalue = getattr(self, name)\n\t\t\t\n\t\t\t# If no value has been set, then check is it allowed to be passed in.\n\t\t\tif value is None:\n\t\t\t\tvalid_values = ('true', 'false')\n\t\t\t\tif args is not None and name not in valid_values:\n\t\t\t\t\traise InvalidArgument(name)\n\t\t\telif not isinstance(value, list):\n\t\t\t\traise InvalidArgument(name)\n"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\targs += set_control_args(args)\n\targs += set_custom_args(args)\n\targs += (self.control, self.args)\n\t# add the control_args to the args list\n\treturn args, dict([(arg) for arg in self.args.split(\",\")])\n\n\n### CUSTOM OPTIONS\n\n@add_docs_annotation\nclass CustomOptions:\n\t# list of custom option names\n\t# (they will usually be split by a control)\n\tnames = (\"-c\",)\n\t# the list of option descriptions\n\t"}]}
{"task_id": "6306092e73426c38ae68ad11", "prompt": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars:\n\t\t# convert extra-vars\n\t\tfor key, value in extra_vars:\n\t\t\tvars_dict[key] = safe_join(vars_dict[key], [value])\n\treturn vars_dict\n\npackage com.planet_ink.coffee_mud.Items.Demons;\nimport java.util.*;\nimport com.planet_ink.coffee_mud.core.*;\nimport com.planet_ink.coffee_mud.core.collections.*;\nimport com.planet_ink.coffee_mud.Abilities.interfaces.*;\nimport"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tvars_dict = np.asarray(vars_dict)\n\tif extra_vars is None:\n\t\textra_vars = vars_dict\n\tif not len(extra_vars):\n\t\textra_vars = vars_dict\n\tfor extra_var in extra_vars:\n\t\tif extra_var not in vars_dict:\n\t\t\tvars_dict[extra_var] = None\n\t\telse:\n\t\t\tvars_dict[extra_var] = extra_vars[extra_var]\n\tif variables_merge_func:\n\t\treturn merge"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tvars_dict = dict(vars_dict)\n    _extra_vars = dict(extra_vars)\n\n    for var, val in vars_dict.iteritems():\n        var = var.lower() \n        # Merge extra-vars if we have a different dict, i.e. no extra-vars\n        vars_dict.update(set(_extra_vars.__dict__.get(var, {}).get(val, {})))\n\n    if extra_vars is"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\told = vars_dict\n\tfor key, val in extra_vars:\n\t\tif len(old) >= 1:\n\t\t\tif not isinstance(val, str):\n\t\t\t\told[key] = []\n\t\tif len(old) >= 2:\n\t\t\tnew = old.copy()\t\n\t\t\tnew.update(val)\n\t\t\tnew.sort()\n\t\t\tlast_i = new[0].find(':')\n\t\t\tif str(last_i) == \"None\":\n\t\t\t\tnew[0][last_i] = new[1][0"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tvars_dict['EXTRAVARS'] = extra_vars\n\treturn vars_dict\n\n/* \n * Copyright (C) 2021 V8 Contributors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is None:\n\t\textra_vars = []\n\tfor key, value in vars_dict.iteritems():\n\t\tvars_dict[key] = str(value)\n\textra_vars.extend(vars_dict)\n\tvars_dict = merge_extra_vars_dict(vars_dict)\n\tvars_dict = merge_extra_vars_list(vars_dict)\n\treturn vars_dict\n\n\ndef merge_extra_vars_list(vars_list):\n\tif vars_list is None:\n\t\treturn vars_"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif not extra_vars:\n\t\treturn vars_dict\n\tvars_dict.update(extra_vars)\n\treturn vars_dict\n\n// Created by: DAUTRY Philippe\n// Copyright (c) 1997-1999 Matra Datavision\n// Copyright (c) 1999-2014 OPEN CASCADE SAS\n//\n// This file is part of Open CASCADE Technology software library.\n//\n// This library is free software; you can redistribute it and/or modify it under\n// the terms of the GNU Lesser General Public License version 2.1 as published\n// by the Free Software Foundation,"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t_vars_dict = vars_dict.copy()\n\tif extra_vars is None:\n\t\tfor extra_var in _vars_dict.keys():\n\t\t\t_vars_dict[extra_var] = _merged_dict(extra_vars)\n\t\treturn _merged_dict(_vars_dict)\n\telse:\n\t\t_merged_dict = _vars_dict.copy()\n\t\tfor item in extra_vars.values():\n\t\t\tfor item_var in item:\n\t\t\t\t_merged_dict[item_var] = _merged_dict"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars:\n\t\tvars_dict.update(extra_vars)\n\n\nclass DuplicatableList(list):\n\t\"\"\"\n\tThe list is duplicated if it shares its own dictionary\n\t\"\"\"\n\n\tdef __repr__(self):\n\t\treturn \"\\n{} does not share the {} items\".format(\n\t\t    type(self).__name__, list.__repr__(self))\n\n\nclass EmptyList(list):\n\t\"\"\"\n\tNo items in the list (will be returned as a result of a ``py.test.TestCase``).\n\t\n\tThis"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is None:\n\t\textra_vars = list(vars_dict.keys())\n\t\n\tfor name, val in vars_dict.items():\n\t\tset_value(name, val, extra_vars=extra_vars)\n\t\n\ndef _make_request(request_method, url_path, params=None, data=None, headers=None, follow_redirects=None, strict_transport_security=False, auth_methods=None):\n\t\"\"\"\n\t  Add a request to a url given parameters and data and/or headers\n\t  :param"}]}
{"task_id": "6306092f73426c38ae68ad13", "prompt": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\tos.environ['ANSIBLE_PLAYBOOK_PATH'] = playbook_path\n\tif verbose is None:\n\t\tverbose = 0\n\n\tif verbose == 1:\n\t\tprint(\"# An Infrared command: Playbook\")\n\t\t\t\n\ttry:\n\t\targs = arg.parse_args(['-q',   '--output', playbook_path,\n\t\t\t'--verbose',         'Ansible playbook arguments',\n\t\t\t'--threads',         'N',\n\t\t\t'--args"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\tpass\n\n_args_spec = [\n\"\"\"\n    ARGNAME: (String, Iriosy)\n\n    -d\n        Ansible plugin object\n\n    -o filename\n        the IRE IRD file to read.\n\n    -p filename\n        the playbook file to spawn\n\n    -p [filename]\n        the playbook data, e.g. to use as folder.\n\n "}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\toptions = {'ansible_plugin_args': ansible_args,\n\t\t'multilibrary_path': ir_plugin.library_path}\n\tif verbose or (ir_workspace.info.get('multilibrary_path') is None):\n\t\toptions['multilibrary_path'] = options.get('ansible_plugin_args')['multilibrary_path_format']\n\telse:\n\t\toptions['multilibrary_path'] ='stdout:%s' % verbose if verbose is not None else options.get('ansible_plugin_args')"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import argparse\n\t# create command-line class object\n\tif isdir(playbook_path):\n\t\tplaybook = playbook_path\n\telse:\n\t\tplaybook = \"%s/%s\" % (ir_workspace.lib_path, playbook_path)\n\tif iri_plugin.get_command_class(playbook) is None:\n\t\traise RuntimeError(\"You need to run an ansible-playbook plugin.\")\n\n\tif verbose >= 0:\n\t\tansible_verb_string = \"\"\"\n"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t# Run the 'ansible-playbook' command by default\n\tif verbose is not None:\n\t\tansible_playbook_options = dict(\n\t\t\tverbosity=verbose\n\t\t)\n\t\n\n\trun_shell_command(ir_workspace, \"ansible_playbook\",\n        '--help-exit-code',\n        # Run the sub-command 'ansible.command.run_shell_command'\n        [IR_PLAYBOOK_COMMAND],\n     "}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT import logging, sys\n\n\tif not ir_plugin.is_supported():\n\t\tprint(\"              Unsupported plugin (in an ansible-playbook \"\n\t\t\t\t\"task): \", ir_plugin.get_name() +'is not supported')\n\t\tprint(\"              This will be ignored, when in a plan build \")\n\t\tprint(\"              it will be ignored too.\")"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\tif not os.path.exists(playbook_path) or verbose==True:\n\t\tprint('Playbook file does not exist or is not a valid path. Please make sure \"playbook_path\" exists.')\n\t\tlogging.warning('Playbook file does not exist or is not a valid path. Please make sure \"playbook_path\" exists.')\n\t\tos.makedirs(playbook_path)\n\tpass#IMPORTIMPORTIMPORT import signal, os\n\tsig = signal.signal(signal.SIGCONT, 0)\n\ttry:\n\t\t"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\tret, stdout, stderr = run(playbook_path, {\n\t\t# 'ansible-playbook': 'ansible://playbook/ansible_playbook.sh'\n\t\t'tns': 'ansible_', 'args': {\n\t\t\t# 'debug': True,\n\t\t\t# 'plugins': [ir_plugin.path],'verbose': verbose or 0,\n\t\t\t'msgs':\n\t\t\t\t[\n\t\t\t\t\t{\n\t\t\t\t\t\t'path': \"ansible://playbook/ansible_playbook.sh\",\n\t\t\t\t\t\t'args': {"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\tcommand = \"ansible-playbook\"\n\targs = extra_vars.copy()\n\t\n\tcommand = \"%s %s\" % (command, \" \".join(extra_vars.keys()))\n\targs.update(extra_vars.copy())\n\t\n\tif ir_workspace:\n\t\targs[\"workspace\"] = ir_workspace.workspace_path\n\t\t\n\targs[\"root_path\"] = ir_workspace.workspace_path\n\targs[\"plugin_name\"] = playbook_path\n\targs[\"verbose\"] = verbose\n\n\trun_shell(args, show_status"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\targparser = ArgumentParser(prog='ansible-playbook', description='Ansible plumb forwards the playbook')\n\targparser.add_argument(_get_args(playbook_path))\n\toptions, _ = argparser.parse_known_args()\n\t\n\tif not options.args or not options.args:\n\t\tusage = 'Missing argument: ansible-playbook -m <Playbook to plumb>'\n\t\tprint usage\n\t\tprint options\n\t\treturn\n\n\tconfig = options.config\n\tprint _get_args(play"}]}
{"task_id": "6306093273426c38ae68ad15", "prompt": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tclient, args = prepare_client(cli_args.ansible_cli_args, vars_dict)\n\n\t# get arguments\n\tcli_args.client = client\n\tcli_args.vars_dict = vars_dict\n\n\tresult = client.run('playbook-run', args)\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tansible_args = []\n\n\tfor key, value in vars_dict.items():\n\t\tansible_args.append(key)\n\t\tansible_args.append(\" '.join(cli_args[key]))\n\t\tansible_args.append(value)\n\n\tif 'ansible_args' in ansible_args:\n\t\tansible_args = ansible_args['ansible_args']\n\t\t\t\n\tresults = {\n\t\t'vars': ansible_args\n\t}\n\n\tif not ansible_args:\n\t"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tlog_name = get_log_vars(vars_dict)\n\tcommand = cli_args[1]\n\tresult = ansible_runner.run(command, _args=vars_dict,\n\t\tir_workspace=ir_workspace, ir_plugin=ir_plugin, host=cli_args[0],\n\t\tlog_name=log_name)\n\treturn result\n\ndef run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin, log_name):\n\t\"\"\"\n\tRuns ansible cli and passes any result to"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tclient_id = vars_dict['client_id']\n\tansible_params = vars_dict['ansible_params']\n\treturn_code = vars_dict['return_code']\n\tansible_params = vars_dict['ansible_params']\n\tansible_params_str = ''.join(ansible_params)\n\tansible_plugins = vars_dict['ansible_plugins']\n\tansible_plugins_uri = vars_dict['ansible_plugins_uri']\n\tansible_plugins_folder = vars_dict['ansible_"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\topts = dict()\n\topts[\"ir_workspace_type\"] = ir_workspace.type_dict()\n\topts[\"ir_workspace_args\"] = ir_workspace.args_dict()\n\topts[\"vars_path\"] = \"../ansible/vars_dict\"\n\topts[\"verbose\"] = \"debug\"\n\topts[\"verbose_log_level\"] = 'DEBUG'\n\t\n\tresult, log, summary_error = cli_args(opts)\n\t\n\tir_module = ir_plugin.load_ir()\n\t\n\t# Run the ansible command\n\tret ="}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tdef _run_session():\n\t\tansible.setup_module(cli_args,vars_dict)\n\t\tansible.helper.set_resources(vars_dict)\n\t\tansible.setup_plugins(ir_workspace,ir_plugin)\n\t\tansible_results = []\n\t\tcmd = ansible_results = []\n\t\tif cli_args.project_name.value == 'ansible' or \\\n\t\t\t\tcli_args.project_name.value == 'ansible_1m': \n\t\t\tansible_results = ansible"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tvars = {}\n\tresults = {}\n\twith_shell_output = vars_dict[vars]\n\tif cli_args:\n\t\tparams = eval(cli_args)\n\t\tvars = ast.literal_eval(params)\n\tif ir_workspace is not None and ir_workspace.name == '':\n\t\tprint(\"Please input an Out of Cloud Workspace\")\n\tif ir_plugin is not None:\n\t\tif not isinstance(ir_plugin, AnsibleIRConfigPlugin):\n\t\t\tprint(\"An unsupported type for the IR plugin! Not supported!\","}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tpass#IMPORTIMPORTIMPORT from ansible.plugins.runner import Runner\n\tpass#IMPORTIMPORTIMPORT from ansible.utils.runtime import import_object_tree, get_current_module, get_version_info, log_err\n\tpass#IMPORTIMPORTIMPORT from ansible.plugins.runner import AnnoSetupProcessRunner\n\tpass#IMPORTIMPORTIMPORT from ansible.tools.ansible_runner_utils import ansible_run_command\n\tpass#IMPORTIMPORTIMPORT from os.path import exists, sep\n"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tresults = _run_common(cli_args, vars_dict, ir_workspace,\n\t\t'__main__/lib/cli/run.py',\n\t\t'__main__/lib/cli/main.py', \n\t\t'__main__/lib/cli/args.py',\n\t\t'__main__/*/lib/ansible.py')\n\treturn results\n\n\ndef _run_common(cli_args, vars_dict, ir_workspace, src, dst, src_exe, dst_exe):\n\t\"\"\"\n\tCommon functions\n"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t__plugin__ = ir_plugin\n\t__opts__ = ir_env.get_ansible_opts()\n\n\tfor key,value in vars_dict.items():\n\n\t\tkwargs = { 'key': key, 'value': value }\n\t\tkwargs['help'] = kwargs.get('help','')\n\n\t\t# Create an ansible configfile for an ansible project\n\t\tkwargs['configfile'] = ansible_config.get_ansible_configfile()\n\n\t\tcli_args.update(kwargs)\n\t\n\t\tif __opts__"}]}
{"task_id": "63060ada73426c38ae68ad31", "prompt": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\targs = []\n\tif parser_name == \"sh\":\n\t\tfrom functools import singlecall\n\t\tif \"cli\" in values_dict:\n\t\t\targs = [\"args\"]\n\n\tif parser_name == \"cli\":\n\t\targs = args + [\"cli\"]\n\n\treturn args, values_dict\n\n\"\"\"\ndef get_parser_command(values_dict = None):\n\t\"\"\"\n\tGets the parser used for parsing values\n\n\t:: \n\n\t\t# get_parser_command(values_dict=dict(input=\"myinput\",output"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tif parser_name in values_dict:\n\tl.error(parser_name)\n\tif \"args\" in values_dict:\n\tl.error(values_dict[\"args\"])\n\tif \"args_opts\" or values_dict[\"args\"] is not None:\n\t\tdefaults = {}\n\t\tvalues_dict[\"args_opts\"] = defaults\n\t\t\n\treturn values_dict\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tfor param, val in values_dict.items():\n\t\tif isinstance(param, basestring) and param in self.parameters:\n\t\t\tvalues_dict[param] = val\n\treturn values_dict\n\n/*\n * (C) Copyright 2005- ECMWF.\n *\n * This software is licensed under the terms of the Apache Licence Version 2.0\n * which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.\n *\n * In applying this licence, ECMWF does not waive the privileges and immunities granted to it by"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t#if len(values_dict) == 0:\n\t#\tvalues_dict = vars()\n\n\tif len(values_dict) > 0:\n\t\tfor v in values_dict.values():\n\t\t\tif not v.startswith('!'):\n\t\t\t\tv = v + '!'\n\t\t\t\tvalues_dict[v] = v\n\t\t\telse:\n\t\t\t\tv_split = v.split('!')\n\t\t\t\tvalues_dict[v_split[0]] = v_split[1]\n\telse:\n\t\tvalues_dict['!']"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tif len(values_dict)!= len(self):\n\t\traise ValueError(\"'{}' contains {} different values\"\n\t\t    \"'{}'\".format(parser_name, len(values_dict), values_dict))\n\tfor k in sorted(values_dict.keys()):\n\t\tif any(isinstance(v, str) for v in self[k]):\n\t\t\tif v not in values_dict[k]:\n\t\t\t\tvalues_dict[k] = values_dict[k].strip('\"')\n\t\t\telse:\n\t\t\t\tvalues_dict[k] ="}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\ttry:\n\t\treturn type(values_dict, (str, type), args=args)\n\texcept:\n\t\treturn dict(values_dict)\n\ndef cli_main(_args=None):\n\t\"\"\" \n\tMain CLI script to check CLI settings and do some basic setup\n\t\n\t:return: cli instance\n\t\"\"\"\n\tif _args is None:\n\t\targs = ('--help', 'Display this help message and exit')\n\telse:\n\t\targs = (_args)\n\tparser = OptionParser(\n\t\tprog='cli',\n\t\t"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tparser_names = [ name for name in parser_name.split(' ') ]\n\tif not values_dict:\n\t\tvalues_dict = {}\n\telse:\n\t\tvalues_map = {}\n\t\tfor name in parser_names:\n\t\t\tv = values_dict.get(name)\n\t\t\tif v is not None:\n\t\t\t\tvalues_map[name] = v\n\t\t\telse:\n\t\t\t\tprint(\"%s required, but not in values_dict\" % name)\n\treturn values_map\n\n/*\n * Copyright 2003-2021 The Music"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t# TODO: allow passing values_dict as well now that parser_name is cli\n\tparser_name = parser_name.upper()\n\tvalues_dict.pop('nargs', None)\n\tvalues_dict.pop('filetype', None)\n\t\n\t# Check if any arguments already passed.\n\tif not values_dict:\n\t\ttry:\n\t\t\tvalues_dict = values_dict.copy()\n\t\t\t\n\t\texcept AttributeError:\n\t\t\tif values_dict:\n\t\t\t\tvalues_dict = values_dict.copy()\n\t\t\t\tvalues_dict.pop('"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tparser = argparse.ArgumentDefaultsHelpFormatter(parser_name)\n\tparser.add_argument(\"-V\", \"--valve\", nargs=1, type=int, help=\"Value of this param\")\n\tparser.add_argument(\"-o\", \"--output\", nargs=1, type=str, help=\"Output folder\")\n\targs = parser.parse_args(args=self.args, defaults=_defaults)\n\t\n\tfor value, arg in args.valve.items():\n\t\tif value == 1:\n\t\t\targs.output.write(str(arg) + \"\\n\")"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tif values_dict is None:\n\t\tparser_name = parser_name.rstrip('\"')\n\targs = parser_name.split()[-1].rstrip()\n\twhile len(args) > 0:\n\t\tif args[0] == '--':\n\t\t\tvalues = {}\n\t\t\tnext_arg = args[1:]\n\t\t\tnext_arg = next_arg.split('--', 1)\n\t\t\tif'' in next_arg[0]: # If first argument is a string with spaces\n\t\t\t\tnext_arg = next_arg[0].split"}]}
{"task_id": "63060b1a73426c38ae68ad3e", "prompt": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\treturn flatten_dict(plugin_spec.get(\"properties\"))\n\ndef get_plugin_spec_dict_with_property_value(plugin_spec, prop, value):\n\tplugin_dict=PluginSpec.get_plugin_spec_dict(plugin_spec)\n\tflattened_dict=plugin_dict.get(\"properties\",{})\n\tprop_value=plugin_dict.get(prop, {})\n\tfor key in prop_value.keys():\n\t\tflattened_dict[key] = prop_value[key]\n\treturn value,flattened_dict\n"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tspec = get_properties_in_path(plugin_dir)\n\t# NOTE: We are using the namespace as the key for the flat object\n\tname_fn = name_ns[spec['name']]\n\tassert len({\"name_fn\": name_fn, \"type_function\": \"get_plugin_spec\"},\" in spec) == 0\n\tname = name_fn(name_fn)\n\tflat = spec[\"properties\"].get(\"plugin_spec\", {})\n\tflat['name'] = name\n\tspec['properties']['plugin_spec'] = flat\n\treturn spec\n\n"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\treturn {'name':'Plugin',\n\t\t'type':'dir',\n\t\t'directory':os.path.join(plugin_dir,'specs')}\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (C) 2019 tribe29 GmbH - License: GNU General Public License v2\n# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and\n# conditions defined in the file COPYING, which is part of this source code package.\n\n# yapf: disable\n# type: ignore"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\treturn get_plugin_spec_data(plugin_dir, plugin='flatdict')\n\n\ndef get_plugin_spec_dict(plugin_spec_file, plugin_specs=None):\n\t\"\"\"\n\t\tReturn a dictionary of plugin specifications.\n\t\tTo store it, pass the plugin_spec_file to create_plugin_specs().\n\t\t<plugin_spec_file> can be a filepath path; it must always be a directory;\n\t\tunless the plugin_specs argument is set, this will automatically\n\t\ttry to interpret the json.load() call to"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tflatten_dict = {}\n\tfor directory in (plugin_dir, \"/var/lock/plugins\"):\n\t\ttry:\n\t\t\ttry:\n\t\t\t\tplugin_data = json.loads(file=os.path.join(directory, plugin_name + \".json\"))\n\t\t\texcept Exception as e:\n\t\t\t\tcontinue\n\t\t\tflatten_dict.update(pickle.loads(plugin_data.get(\"data\", {})) or {})\n\treturn flatten_dict\n\nusing System;\nusing Newtonsoft.Json;\n\nnamespace TdLib\n{\n    /// <summary"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\ttry:\n\t\tplugin_dir = os.path.join(plugin_dir)\n\t\tf = open(plugin_dir, \"r\")\n\t\t\n\t\tplugin_dir_file_size = int(f.tell())\n\t\tf.seek(0)\n\t\tf.seek(plugin_dir_file_size)\n\t\tplugins = f.readlines()\n\t\tf.close()\n\t\t\n\texcept IOError, ex:\n\t\tprint('error while reading plugin spec file [%s] for plugin [%s]\\n'%(plugin_dir, plugin"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\timport sys\n\treturn_object = sys.modules.get(\"__dict__\", None)\n\tif not return_object:\n\t\treturn return_object\n\tplugin_spec = return_object.__dict__\n\n\tif 'os' in plugin_spec:\n\t\tplugin_spec['plugins']['':] = (plugin_spec['plugin_name'], plugin_spec['version'])\n\t\tplugin_spec['os']             = plugin_spec['os']\n\n\tplugin_spec['plugins']  = plugin_spec['plugins"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\treturn flatdict([(plugin_name, json.dumps(entry)) for plugin_name, entry in plugin_module.find_plugins(plugin_dir=plugin_dir)[\"spec\"]])\n\n\ndef flatten_dict(output):\n\t# Create a copy of the output dictionary that is a nested\n\t# dictionary by deep copying a key and value tuple of all \n\t# entries\n\tresult = list()\n\tfor key, value in output.items(): \n\t\tresult.append( (key, json.dumps(value)) )\n\treturn result\n\n\ndef get_"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tspec_base=configparser.ConfigParser(interpolation=None).parse_string(\n\t\t\"\"\"\n\t\t<?plugin_dir>\n\t\t\t['pluginName']+\n\t\t\t['properties']\n\t\t\t['configName'],\n\t\t\"\"\"\n\t)\n\n\treturn {\n\t\t**spec_base.get('spec_name', None) for spec_name in spec_base.keys(),\n\t\t**spec_base.get('properties', None)\n\t}\n\n\"\"\"\ndef get_config_value_if_not_present(plugin_spec,"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\treturn {\"properties\": plugin_dir.joinpath(\"properties/config.py\").read_text(),\n\t\t\"aliases\": [], \"depends\": [], \"default_config\": [],\n\t\t\"paths\": {}, \"requires\": [],\n\t\t\"overrides\": {}\n\t}\n\ndef get_plugin_spec_default_config(plugin_dir, override_config):\n\t\"\"\"\n\tCreates a dictionary from the plugin options\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:param override_config: A dict representing the plugin's options\n\t:return:"}]}
{"task_id": "63060b1b73426c38ae68ad42", "prompt": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif self.environment:\n\t\t_env_path = self.environment.get_config(self.ENV)\n\tself.environment.set_config(self.ENV, self.get_config(self.ENV))\n\nusing System;\nusing System.Collections.Generic;\nusing System.Security.Cryptography;\nusing System.Text;\nusing System.Text.RegularExpressions;\nusing System.Waf.Applications;\n\nnamespace Waf.DotNetPad.Applications\n{\n    public class SecureData\n    {\n       "}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\ttry:\n\t\t# check config file\n\t\tconfigfile = open( 'config', 'r' )\n\t\tif ( configfile.readline()!= 'config' ): # if the file not set config_path\n\t\t\traise ConfigError( 'Config file not set.' )\n\t\telse: # read config file\n\t\t\tself.configfile = open( 'config', 'w' ).write( configfile )\n\texcept:\n\t\tpass\n\n\ndef set_env_settings( settings ):\n\t# write new settings, or clear existing values if empty\n"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\told = self.config  # To be overwritten.\n\tself.config = \"PATH=/.config/amqp.conf\"\n\tif self.config:\n\t\tself.log(\"Environment variable for config file: {}\".format(self.config))\n\n\tif not HAS_AMQP_SERVICE:\n\t\tself.log(\"-->> WARNING Please ensure that you have configured amqp in your config directory, or AMQP_SERVICE is defined.  You have to remove this warning if no \"\n\t\t\t\t\t \"-->> file was found.\")\n\n\tif not self.loaded:\n\t\tnew"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tset_config()\n\t# Set to undefined as we know the last value of config is not an\n\t# environment variable\n\tconfig_to_env = \"config.py.typed_environments.default\"\n\tif not config_path_exists(config_to_env):\n\t\tconfig_var = \"config.py.%s.\" % config_to_env # Environment variable\n\t\ttry:\n\t\t\tif os.environ.get(config_var): # Try env var\n\t\t\t\tconfig_to_env = config_var # Don't error out if it is"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif not getattr(self, \"_defined\", False):\n\t\tself._add_config_path()\n\n\tself._defined = True\n\ndef load_app_configs(loader):\n\t\"\"\"Loads and executes a set of configuration files.\"\"\"\n\ttry:\n\t\tfrom appdirs import config_dir\n\t\tfiles = config_dir(loader)\n\texcept ImportError:\n\t\tpass\n\n\t# We will look through import path list to find config files.\n\ttry:\n\t\tfor n in import_names():\n\t\t\tc = n[3:] #"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\timport re\n\tconfig_path = re.search(r'_config\\.+?\\.xml$', self.options['config_path']).groups()[0]\n\tif os.path.isfile(config_path):\n\t\tself.config_path = config_path\n\n#include <bits/stdc++.h> \nusing namespace std; \n\n    vector< vector< pair<int,int> > > arr;   \n    bool visited[1005];    \n    vector< double> arr2;    \n  "}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tglobal environment_vars\n\tif environment_vars is None:\n\t\tenvironment_vars = {}\n\tif 'CONFIG' not in environment_vars:\n\t\tenvironment_vars['CONFIG'] = {}\n\tenvironment_vars['CONFIG']['name'] = 'JANCY_USER_SITE'\n\tself._config = environment_vars\n\n\ndef load_local():\n\ttry:\n\t\twith open('janncy_conf.json', 'r') as f:\n\t\t\treturn json.load(f)\n\n\texcept Exception as e:\n\t\tprint(\"\\"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tENVVAR = getenv(self.CONFIG_VAR_NAME)\n\treturn ENVVAR == None\n\ndef get_config_path(filename):\n\t'''\n\tLook for config file name in the configuration and return that's path.\n\tCan be used to find this file path from config.ini.\n\t'''\n\tif os.path.exists(filename): \n\t\treturn filename\n\n\ttry:\n\t\twith open(filename) as f:\n\t\t\treturn f.read()\n\texcept IOError:\n\t\traise IOError, \"No config file found: {"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tglobal ENV\n\tENV = self.find_variable(\n\t\t'modules.get_global_config',\n\t\t'Get global config',\n\t\tdefault=default_file(),\n\t).replace('.json', '').replace('.json', '+config.json')\n\tif ENV['module'] == 'django':\n\t\tENV['DjangoConfigLoader'] = 'django.custom.utils.load_path.customized'\n\t\tENV['DjangoConfig'] = 'django.conf.Config'\n\t\tENV['django.conf.static'] = 'config.path"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t_config_dir = config_path()\n\tif _config_dir:\n\t\t# Use _config_dir to locate config path\n\t\tif os.path.exists(_config_dir):\n\t\t\tprint()\n\t\t\t\t\t\t  # Get cwd\n\t\t\t\tcwd = _config_dir\n\t\t\t\tprint()\n\t\t\t\t# Check if cwd defined in config path\n\t\t\t\tif cwd[0] == \"\":\n\t\t\t\t\tcwd = cwd[1:] # If cwd is empty\n\t\t\t\t\tfor i in [\"config/\", \"templates/\", \"config\", \"lib/\", \"config.d\","}]}
{"task_id": "63060b1b73426c38ae68ad43", "prompt": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\troot_subparsers.extend_cli(self)\n\t\n\t# add an exception handler that prints to stderr all unhandled errors\n\tsubparser = self.add_subparser('exception-handler', help='Prints unhandled exceptions')\n\tsubparser.add_callback('unhandled_exception', 'handler_unhandled_exceptions')\n\t\n\t# add an exception handler that prints to stderr all unhandled exceptions\n\tsubparser = self.add_subparser('uncaught-exception', help='Prints unhandled exceptions')\n\tsubparser.add_callback('unhandled_uncaught_exceptions', 'handler_unhandled"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparsers = root_subparsers\n\tsubparsers.options.add_option(\n\t\t'cli_file',\n\t\thelp='command option argument',\n\t\tmetavar='ARG',\n\t\tmetavar='FILE',\n\t\tdefault=os.path.normpath(os.path.join(os.getcwd(), os.pardir)),\n\t\thelp='The file to include as the spec cli',\n\t)\n\targ = re.compile(r'ARG(\\S+)', re.IGNORECASE)\n\tfile = os.path.normpath(os."}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\troot_options = root_subparsers.parse_args()\n\t\n\textension_args_opts = root_options.get_dict('options', defaults=False)\n\t\n\tself._add_arguments_options(extension_args_opts['options'])\n\t\n\textend_cli(self, root_subparsers=root_subparsers)\n\t\n\tself.configure_args(root_options, root_args=root_args)\n\n/*\n * This file is part of Bisq.\n *\n * Bisq is free software: you can redistribute it and/or modify"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t# create the subparser that does some basic things \n\t# including the default command set. \n\tspec_subparsers = subparsers.parse_subparsers(subparser.get(\"spec\").split(\" \"))\n\tspec_subparser_config()\n\textensions.add_arguments_to_parser(spec_root_subparser, root_subparsers, self.arg_parser)\n\n\t# The CLI commands listed under the\n\t# `--help`: describe this command \n#\tdescription_subparser = subparser.add_subparser(\"--help\")\n#\tsubparsers."}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsuper(Spec, self).extend_cli(root_subparsers)\n\troot_subparsers.update({\n\t\t'add_metadata': AddMetaParser(),\n\t} )\n\ndef main():\n\t\"\"\"\n\tThe main entry point of spec.\n\t\n\t:param:\n\t\"\"\"\n\tspec = Spec(argv=sys.argv)\n\tspec.run()\n\ndef run(argv=None):\n\t\"\"\"\n\tSimple wrapper for creating the spec from an argv array.\n\n\t:param argv: a list of command line options to spawn the spec into."}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparser = root_subparsers[0]\n\tself._parse_options(subparser.name, subparser.title, subparser.aliases, cmd_path=subparser.command_path)\n\nif __name__ == '__main__':\n\tparser = argparse.ArgumentParser(description=__doc__.strip())\n\troot_subparsers = subparsers.add_parser('cli',\n\t\t\t\t\t\t      _('Help'),\n\t\t\t\t\t\t      help=_('Expert info'),\n\t\t\t\t\t\t      metavar='<name>')\n\troot_sub"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparser.set_defaults((('option_parser', 'CommandLineParser'), {'parent': [0, 0]}))\n\t# parent should be the default argv\n\tfor arg in ['-a', '--args', '-b', '--app-name', '--base-path', 'c', ]:\n\t\tsubparser.add_option(arg, help=subparser.get_help(arg))\n\n\tfor arg in ['-a', '--args', '-b', '--app-name', '--base-path', 'c', 'c']:\n\t\t"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tdef bind_subparsers(parent, subparser):\n\t\tbinds = list(p.cli_binds.keys())\n\t\tbind = subparser.add_parser(subparser.name,\n\t\t\tsubparser.aliases[\"appsettings\"],\n\t\t\taliases=binds)\n\t\tbind.prefix = root_subparsers.prefix()\n\t\t\n\t\tbind.help = p.cli_options.help()\n\t\t\n\t\t# Bind the subparsers.\n\t\tbind.add_subparser(subparser)\n\t\t# Add any arguments.\n\t\t"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tdef configure_cli(self, subparser):\n\t\tself.main_clbk = self.newcommand(subparser)\n\t\tself.newconfig_group(self.main_clbk, ['file', 'config'])\n\t}\n\t\n\tadd_clbk = configure_cli\n\t\n\tdef main_clbk(self, args):\n\t\tconfig_args = args[1:]\n\t\tcmd_cfg = config_args[0]\n\t\t\n\t\tself.cmd_sub_parser.add_command(cmd_cfg)\n\t\tself.main"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparser = ArgumentParser(add_help=False)\n\troot_subparsers.add_parser(type('spec CLI'), subparser)\n\tbase_subparser = self.add_subparser(subparser)\n\tfor sub_name in base_subparser.add_options:\n\t\tif sub_name == 'create_parser':\n\t\t\tsubparser['create_parser'] = True\n\nif __name__ == '__main__':\n\timport sys,os\n\tsys.exit(not main())\n\nrequire('./angular-locale_ru-RU');\nmodule"}]}
{"task_id": "62ece4982e6aefcf4aabbd64", "prompt": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\ttry:\n\t\troot.parents.pop(None, None)\n\texcept KeyError:\n\t\traise ValueError(\"root doesn't exist\")\n\ndef get_config_args(path):\n\t\"\"\"\n\tReturns arguments to use as ``config.ini_read()`` for the ``ini()`` command.\n\t\"\"\"\n\tfrom pkg_resources import get_distribution\n\tdistribution_name = None\n\tif \"django_dist\" not in get_distribution():\n\t\ttry:\n\t\t\tdist = get_distribution(path)\n\t\texcept DistributionNotFound:\n\t\t\t"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif isinstance(root, str):\n\t\troot = root.lower()\n\tif not root:\n\t\treturn\n\telif len(root) == 1:\n\t\treturn root\n\telse:\n\t\tm = re.match('^([A-Za-z][ \\t\\n\\r\\f]+)+', root)\n\t\troot = m.group(1)\n\t\t#print\n\t\tif m and m.group(2) or root.count(\"/\") or root.count(\"\\\\\") or root == \"\":\n\t\t\troot = root[0:"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\ttry:\n\t\treturn os.path.normpath(path[root])\n\texcept KeyError:\n\t\traise ValueError('No such root path.'\n\t\t\t\t\t+'Please provide path from root folder or set root to empty string.')\n\n\ndef get_folder_path(full_path):\n\t\"\"\"\n\treturn full_path as path.\n\t\"\"\"\n\tif full_path[-1] == '/':\n\t\treturn full_path\n\telse:\n\t\treturn full_path + '/'\n\n\ndef get_base_ext(base_file_stem):\n"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif isdir(path):\n\t\tpath = path[len(root):]\n\treturn str(path)\n\n\ndef _isdir(path, root):\n\treturn path.startswith(root) and isdir(path)\n\ndef isfile(path):\n\treturn isdir(path)\n\ndef isdir(path):\n\treturn isfile(path) and os.listdir(path) and 0 == len(os.listdir(path)[:-1])\n\ndef exists(path):\n\treturn isfile(path) and os.access(path"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif root is not None:\n\t\tif hasattr(path, 'lstrip'):\n\t\t\tpath.lstrip(root)\n\t\telse:\n\t\t\traise RuntimeError()\n\n\n# ---------------                               ----------------   ----------\nclass _Path(object):\n\t\"\"\"\n\tAbstract.  Defines basic functions for accessing subpath\n\tobjects. For a particular path type, an instance of a subclass\n\twill be returned.\n\t\"\"\"\n\t_"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif not root:\n\t\treturn []\n\tfull_path = path + '/root'\n\tfile_list = []\n\ttry:\n\t\twith open(File(full_path), 'r') as f:\n\t\t\tfor line in f.readlines():\n\t\t\t\tif not line.strip().startswith('root:') and \\\n\t\t\t\t\tstr(line.strip('\\n')) in root and \\\n\t\t\t\t\tnot (root.endswith('/') or root.endswith('\\\\')):\n\t\t\t\t\tfile_list.append(str(line))\n\texcept Exception:\n\t"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\ttry:\n\t\t_root_path = _get_root_path(path)\n\texcept:\n\t\tif not os.path.isabs(_root_path): # not absolute path - assume root\n\t\t\troot = os.path.dirname(path)\n\t\telse:\n\t\t\troot = None\n\tif root is None:\n\t\traise os.error(os.strerror(os.errno.ENOENT))\n\tstrip_root(root, _root_path)\n\ndef _remove_all_without_root(prefix):\n\t#remove the directory"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\trelative_path, relative_dir = os.path.split(path)\n\tnew_path = os.path.join(path, relative_dir)\n\t# Check if path already exists, if not add new path to existing list of paths\n\tif not os.path.exists(new_path):\n\t\ttry:\n\t\t\twith open(new_path, 'w', encoding='utf8') as f:\n\t\t\t\tf.write(\"\")\n\t\t\t# Remove parent symlink if symlink_exists return True\n\t\t\tos.unlink(new_path)\n\t\t\tos"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tis_dir = os.path.isdir(root)\n\tif is_dir: raise BunchRootError(f\"Directory {root} is not a directory.\")\n\tis_file = os.path.isfile(root)\n\tif is_file: raise BunchRootError(f\"File {root} is not a file.\")\n\treturn root\n\ndef _validate_dir_or_file(path):\n\tif os.path.isdir(path):\n\t\tif not os.path.exists(path):\n\t\t\traise ValueError(f\"Directory"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif not isinstance(root, bytes):\n\t\traise ValueError(\"Root must be bytes.\")\n\tfilename = root.decode('utf-8')\n\tif not filename.startswith(path):\n\t\traise ValueError(\"Invalid filename.\")\n\tfilename = filename[len(path):]\n\ttry:\n\t\t__builtin__.os.unlink(filename)\n\t\traise FileNotFoundError(f\"File {filename!r} was deleted.\")\n\texcept OSError as err:\n\t\traise FileNotFoundError(\"File `{filename}` failed to be removed.\", err)\n\ndef write_file("}]}
