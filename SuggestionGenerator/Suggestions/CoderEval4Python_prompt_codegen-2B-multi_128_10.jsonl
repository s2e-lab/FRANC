{"task_id": "62e60f43d76274f8a4026e28", "prompt": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\t# localtimes_nanoseconds is the number of localtimes in nanoseconds and\n\t\t# we want to check, which is not a problem\n\t\tnow = time.time()\n\t\t# time has to be in nanoseconds, but it cannot exist for the moment\n\t\tlocaltimes_nanoseconds = time.mktime(now.timetuple()) + (nanoseconds * tz)\n\t\t# calculate hydrator for time\n\t\t# time does not like unix times, but time was not like our time\n\t\t# localtime_nanoseconds = time.localtime(localtimes_nan"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\treturn Hydrator.getTime(nanoseconds, tz)\n\nclass Hydrator:\n\t\"\"\"Time: Use the hydrator to compute and apply a `HydratorExpression` on it.\n\tFor example, to provoke a number is the number of seconds since January 1, 1970!\n\n\tExamples:\n\t\t>>> b.apply([2**64, 2**16])\n\t\t-2^64\n\t\t>>> hydr(2**64)\n\t\t3**64\n\t\t>>> hydr(2**64)\n\t\t1000000000\n\t\t>>> hydr"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\tif tz is None:\n\t\t\ttz = LOCAL_TIME\n\t\telse:\n\t\t\ttz = tz\n\n\t\tj_0 = dateutil.parser.parse(nanoseconds)\n\t\tj_1 = dateutil.parser.parse(nanoseconds)\n\t\tm_0 = dt.datetime(j_0.year, j_0.month, j_0.day)\n\t\tm_1 = dt.datetime(j_1.year, j_1.month, j_1.day)\n\t\th = (j_1 - MJD0).total_"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\ttry:\n\t\t\treturn Time(*map(int, nanoseconds.split(':')))\n\t\texcept ValueError:\n\t\t\traise ValueError('Could not parse time as an ISO 8601 standard.')\n\ttry:\n\t\tn = as_int(nanoseconds)\n\texcept ValueError:\n\t\traise ValueError('Could not parse time as an ISO 8601 standard.')\n\tif not isinstance(n, six.integer_types):\n\t\traise ValueError('Time must be an integer.')\n\tif tz is None or tz not in pytz.all_timezones:\n\t\traise ValueError('Unknown timezone \\'%s"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\tret = Time.from_raw_seconds(nanoseconds)\n\t\tif tz:\n\t\t\t\tret = ret.timetz()\n\t\treturn ret\n\ndef timezone_at(timezone):\n\t\"\"\"\n\tHydrator for a timezone `Timezone` with no timezone parameter.\n\n:param timezone:\n:return:\n:rtype: None\n\"\"\"\n\treturn timezone if timezone!= None and timezone.name == TIMEZONE else timezone\n# TODO: Implement timezones from http://stackoverflow.com/questions/426715/pythonic-time-timezones-in-python\ndef"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\n\n\ndef durations(seconds):\n\t\"\"\"\n\tHydrator for `Days` and `Months` values; the values are\n\tconsidered to the nearest millisecond, i.e. the value\n\tis 0.1 second and fractional seconds are rounded\n\ttrippable. The value has a maximum sign that will be added\n\ttime.\n\n:param seconds:\n\n\tDivide with the rounding behaviour set to the nearest\n\tdurations (e.g. 1.5 would be rounded to the nearest second at\n\tthe floor) and no other rounding occurs;"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\tif isinstance(nanoseconds, type(0)):\n\t\t\t# This is the same as the timezone\n\t\t\treturn nanoseconds\n\t\telse:\n\t\t\treturn nanoseconds.time(tz)\n#!/usr/bin/python\n# -*- coding: utf-8 -*-\n#\n# --- BEGIN_HEADER ---\n#\n# connhandler: \n#   The handler module for the sshd, the scp, and ssh commands.\n#\n# Copyright 2003-2011 Emir J. Hamalainen <emir@jeremysanen.org>\n#"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\tif tz is None:\n\t\t\ttz = get_localzone()\n\n\t\tif is_normalized(nanoseconds): return nanoseconds\n\n\t\tif not _is_timezone(tz): return tz\n\n\t\tif tz == 'UTC':\n\t\t\treturn nanoseconds\n\n\t\ttz = get_localzone()\n\t\treturn dt_util.as_local(dt_util.as_utc(tz))\n\n\t@staticmethod\n\tdef easter_time(dates, tz=None):\n\t\t\"\"\"\n\tHydrator for `Time` and `EasterTime` values.\n\n:"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\tif tz is None:\n\t\t\ttz = timezone.utc\n\t\tt = time(nanoseconds, nanoseconds, tz)\n\t\tprint(t.format('hhmm'))\n\treturn t\n\ndef str_to_datetime(timestamp):\n\tif timestamp is None:\n\t\treturn None\n\treturn datetime.datetime.fromtimestamp(int(timestamp / 1000000000.0))\n\n\n\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2015, Alcatel-Lucent Inc\n# All rights reserved.\n#    \n# Redistribution and use in source"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\t\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time\t\"\"\"\n\t\tif tz:\n\t\t\tassert len(tz) == 3  # this is probably not correct?\n\t\t\ttz = tuple(map(int, tz))\n\t\tnanoseconds += nanoseconds % 60\n\t\treturn datetime.utcfromtimestamp(nanoseconds)\n\ndef now(tz=None):\n\t\"\"\"\n\tConvert the current time to the current timezone.\n\n:param nanoseconds:\n\t:param tz:\n\t:return: float\t\"\"\"\n\t\treturn datetime.utcnow().replace(microsecond = 0)\n\ndef epoch(seconds=0, millis=0,"}]}
{"task_id": "62e60f3bd76274f8a4026e10", "prompt": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\tif value is None: # value.hour_delta or value.minute_delta, (None, None)\n\t\t\treturn value\n\t\tfrom datetime import timedelta\n\t\tdiff = value - value.replace(microsecond=0)\n\t\tif diff.days == -1:\n\t\t\treturn timedelta(days=-1)\n\n\t\treturn timedelta(days=int(diff.days)/3600)\n\treturn value\n\ndef create_naming_series_with_column_names():\n\t\"\"\"\n\tCreate naming series if `naming_series` is not set.\n\n\t:return: new `naming_"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\treturn timedelta(int(value.total_seconds() - int(value.days * 24 * 3600)))\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport subprocess\nimport tempfile\nimport tarfile\n\nimport gym\n\n\n\ndef savefile(filename, data, mode):\n    if isinstance(data, str):\n        filename = 'file_{}'.format(filename)\n    if isinstance(data, (list, tuple)):\n        filename = 'file_list_{}'.format(filename)\n    return tarfile.open(filename + '.tar', mode)\n\n\ndef load(mode='train',"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\n\n\tif value.days == 0:\n\t\treturn 'Never'\n\telse:\n\t\treturn str(value.days)\n\n\ndef dehydrate_seconds(intval):\n\t\"\"\"\n\tDehydrator for `seconds` values.\n\n:param intval:\n:type intval: int\n:return:\t\"\"\"\n\t\n\n\tif intval < 1:\n\t\treturn 'No'\n\telse:\n\t\traise ValueError('The parameter must be of type positive number')\n\n\ndef dehydrate_bool(value):  # This needs to be"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\ndef datetime_from_timestamp(timestamp):\t\"\"\"\n\tParse a timestamp string into a timestamp\n:type timestamp: str\n\t:return:\t\"\"\"\n\t\ndef timestamp_from_seconds(duration):\t\"\"\"\n\tParse a timestamp string into a timestamp\n:type duration: float\n\t:param duration:\tDuration of the duration\n:type duration: str\n:return:\t\"\"\"\n\t\ndef parse_timestamp(timestamp, time_format):\t\"\"\"\n\tParse a timestamp from a given `time_format` string\n\t:type timestamp: str\n\t:return:\t\"\"\""}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\treturn timedelta(days = int(value))\n\telif isinstance(value, timedelta):\n\t\treturn value\n\telif isinstance(value, int):\n\t\tvalue = timedelta(days = value)\n\telif isinstance(value, timedelta):\n\t\traise ValueError(\"Parameter %s should be timedelta object, but got %r.\" % (name, value))\n\n\nclass Effect(object):\n\t\"\"\"\n\tEffect class.\n\t\"\"\"\n\tdef __init__(self, name, description, image, tooltip = \"\"):\n\t\tself.name = name\n\t\tself.description = description\n\t\tself"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\treturn timedelta(seconds=value)\n\n##############################################################################\n# Copyright (c) 2013-2017, Lawrence Livermore National Security, LLC.\n# Produced at the Lawrence Livermore National Laboratory.\n#\n# This file is part of Spack.\n# Created by Todd Gamblin, tgamblin@llnl.gov, All rights reserved.\n# LLNL-CODE-647188\n#\n# For details, see https://github.com/spack/spack\n# Please also see the NOTICE and LICENSE files for our notice and the LGPL.\n"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\treturn relativedelta(timedelta(value))\n\ndef _get_first_day(dt):\n\t\"\"\"\n\tGet the first day.\n\t@param dt: Datetime object.\n\t@type dt: datetime.datetime\n\t:return: datetime.date object.\n\t\"\"\"\n\tdt_year = dt.year\n\tdt_month = dt.month\n\treturn _get_first_day_of_month(dt_year, dt_month)\n\ndef _get_last_day(dt):\n\t\"\"\"\n\tGet the last day.\n\t@param dt"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\treturn timedelta(seconds=value)\n\ndef dehydrate_date(value):\t\"\"\"\n\tDehydrator for `_date_to_long` values.\n\n:param value:\n:type value: datetime\n:return:\t\"\"\"\n\t\traise NotImplementedError(\"Cannot dehydrate a datetime to a datetime value (current: %s)\", value)\n\ndef dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\treturn timedelta"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\tvalue = value.total_seconds()\n\n\timport calendar\n\tdelta = value.days * 24 * 60 * 60\n\tseconds = int(delta)\n\n\tprint(\"Duration: %s\" % value)\n\n\t\n\n# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2015 CERN.\n#\n# Invenio is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 2 of the\n# License, or (at"}, {"generated_text": "def dehydrate_timedelta(value):\t\"\"\"\n\tDehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:\t\"\"\"\n\t\treturn timedelta_to_dehydrate(value)\n\t\ndef validate_condition(*conditions):\n\t\"\"\"\n\tChecks that condition arguments are correctly formatted,\n\tvalid formats, and that they match one another and returns as a\n\tconsistent parameter.\n\t\"\"\"\n\tdef validate_condition_tuple(val):\n\t\tif len(val)!= 2:\n\t\t\tlog.error(\"Condition tuple must be two elements\")\n\t\t\n\t\tif not val[0] or not val[1]:\n\t\t\tlog.error(\"Condition format must contain two entries\")\n\t\t\treturn False\n\t\telif val["}]}
{"task_id": "62e60f37d76274f8a4026dfd", "prompt": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\t# Convert to UTC\n\t\tif not value:\n\t\t\treturn datetime.datetime.now().replace(tzinfo=pytz.utc)\n\n\t\treturn time.mktime(value.timetuple())\n\n\tif type(value) in [str, float] and value.startswith('UTC'):\n\t\treturn Datetime.UTC.parse(value)\n\n\tif type(value) == datetime.datetime:\n\t\treturn DateTime.UTC.fromutc(value)\n\n\telif type(value) in [list, tuple, int, float, str, None]:\n\t\treturn map(float, [time.mktime(time"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn (value.hour * 3600 + value.minute * 60 + value.second) / 60\n\treturn 0\n\ndef month_to_number(value, y=None):\t\"\"\"\n\tTranslate a date to the number of days from today's date and\n\treturn number of days\nThe following rules map days with the following rules:\n\n* first from date, last from date, month_number to date. This allows us to \n\tdetermine this if a month to number is already defined.\n\n* last is always date, second from date. This makes sure that we never\n\t"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn seconds_to_clock(0, int(value))\n\n\t@staticmethod\n\tdef current():\n\t\treturn time.time()\n\n\t@staticmethod\n\tdef time_to_seconds(time_value):\n\t\t\"\"\"\n\t\tConvert a time value (either as a number or as a string) into seconds\n\n\t\t@return:\tseconds\n\t\t@rtype : string\n\t\t@raise ValueError: Value is not a number, or too close to [0, 23, 59, 999,... secs]\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif time_value < 0:\n\t\t\t\ttime"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn int_or_double(value)\n\t\n\n# Copyright 2006 James Tauber and contributors\n# Copyright (C) 2009 Luke Kenneth Casson Leighton <lkcl@lkcl.net>\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\tif value % 60 < 15:\n\t\t\treturn (value // 60) * 15\n\treturn value\n\t\ndef parse_timestamp(value):\t\"\"\"\n\tParse timestamp.\n\n:param value:\n:type value: unicode or str\n\t:return:\t\"\"\"\n\tif not value.strip():\n\t\treturn None\n\tif len(value) == 28: return datetime.datetime.strptime(value, '%Y-%m-%d %H:%M:%S.%f')\n\tif len(value) == 31: return datetime.datetime.strptime(unicode(value, 'utf"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\tif isinstance(value, (str, unicode)):\n\t\t\treturn datetime.strptime(value, \"%H:%M\")\n\t\telif isinstance(value, tuple):\n\t\t\treturn tuple(int(x) if x < 0 else x for x in time.localtime())\n\tif isinstance(value, datetime):\n\t\treturn value\n\n\treturn value\n\nimport numpy as np\nfrom pytadbit.tadbs_a2d import TadbsArithmetic\nimport pytadbit as tadbs\n\nclass TadbsSymmetricLinear(TadbsArithmetic, tadbs"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn value.total_seconds() / 86400.0\n\n\t@staticmethod\n\tdef time_to_dehydrate(time):\t\"\"\"\n\tConvert `time` value to float representation.\n\t:param time: time value\n\t:return:\t\"\"\"\n\t\treturn time * 3600.0\n\nclass Value(object):\n\t\"\"\"\n\tA value is a class that stores the value of a timepoint.\n\t\"\"\"\n\tdef __init__(self, time_from=None, time_to=None, **attributes):\n\t\tself.time_from = time_"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\tdatetime_obj = datetime.strptime(value, \"%H:%M:%S\")\n\t\treturn datetime_obj.time()\n\texcept:\n\t\tprint \"Failed to decode time input\"\n\t\traise\n\ndef parse_args():\n\t# Check arguments\n\tif len(sys.argv) < 2:\n\t\tprint \"usage: %s <name>\" % sys.argv[0]\n\t\tsys.exit(1)\n\telse:\n\t\t# Parse arguments as arguments\n\t\targs = sys.argv[1].split()\n\t\tfor i in range(len(args)):"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn \"%r, %r\" % value.total_seconds() if value is not None else \"not defined\"\n\n\ndef humanize_time(value):\n\t\"\"\"\n\tDecrct `value` with number of seconds into human readable. e.g. ``60`` for ``1m 30s``, ``1h'' for ``2h 30m`` etc.\n\t\"\"\"\n\n\ttime = int(round(value * 1000))\n\tsuffix = \" seconds\" if value > 10 else \"\"\n\n\treturn time, suffix\n\n\ndef pretty_duration(d):\n\t\"\"\"\n"}, {"generated_text": "def dehydrate_time(value):\t\"\"\"\n\tDehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:\t\"\"\"\n\t\treturn value.timedelta(microseconds = 123)\n\n\ndef get_time_since_epoch(date):\n\t\"\"\"\n\tGet a datetime instance.\n\n:param date:\n\n:return: datetime representing the value in the time since `date`.\n:rtype: datetime.datetime\n\n\"\"\"\n\treturn dt.time(date.timetuple())\n\ndef time_since_epoch(value):\n\t\"\"\"\n\tGet a datetime instance.\n\n:param value:\n\n:return: datetime representing the value in the time since `value`\n\tor the current utc timestamp (as"}]}
{"task_id": "62e60f33d76274f8a4026de9", "prompt": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\treturn value.x, value.y\n\treturn point[0] + value[0], point[1] + value[1]\n\nif __name__ == \"__main__\": \t# pragma: no cover\n\td = Driver(\"Point\", 10, 10)\n\ta = [0, 1, 2,  3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 24, 25,\n            26, 27, 28, 29, 30, 31]\n\t"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\treturn Point(float(value[0]), float(value[1]))\n\ndef distance_point(value):\t\"\"\"\n\tDistance point (meters).\n\n:param value:\n:type value: float\n:return:\t\"\"\"\n\t\treturn Point(float(value[0]), float(value[1]))\n\ndef angle_difference(value_a, value_b):\t\"\"\"\n\tAngle difference between two points, angle of an arbitrary distance of a point.\n\n\t:param value_a:\n\t:type value_a:\n\t:param value_b:\n"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\treturn value * value.get_length()/1000.0\n\tpass\n\ndef point_dist(value, center):\n\t\"\"\"\n\tDehydrate Point distance.\n\n:param value:\n:type value: Point\n:param center:\n:type center: Vector\n:return:\t\"\"\"\n\t\tif value.length()<center.x()-value.x():return 0\n\t\tif value.length()>center.x()-center.x():return value.get_length()/center.x()-center.x()\n\t\tif value.length()<center."}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\treturn value * (10 ** 8)\n\n\ndef dehydrate_size(value):\t\"\"\"\n\tDehydrator for Size data.\n\n:param value:\n:type value: Size\n:return:\t\"\"\"\n\t\treturn int((value[0] // 10 ** 8 + value[1]) // 10.0) * 10.0 * (10 ** 10)\n\n\ndef dehydrate_direction(value):\t\"\"\"\n\tDehydrator for Direction data.\n\n\t:param value:\n\t:type value: Point\n\t:return:\t"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\treturn value*0.5+0.25\n\ndef distance_point(x, y):\n\t\"\"\"\n\tCalculate the distance between two points in the space.\n\n\t:param x:\n\t:type x: float\n\t:param y:\n\t:type y: float\n\n\t:return:\tthe distance between the endpoints.\n:rtype: float\n\t\"\"\"\n\n\tif x**2 + y**2 <= 1:\n\t\treturn float('inf')\n\telif x**2 - y**2 <= 1:\n\t\treturn float('-inf')\n\telif y**"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\tx_ = float(value.x)\n\t\ty_ = float(value.y)\n\t\tradius = x_distance(y_,x) \n\t\treturn Point(x=radius,y=y_)\n\n\n\ndef point_distance(point_1,point_2):\n\t\"\"\"\n\tCalculates the Point between two points\n\t\n\t:param point_1: the first point\n\t:type point_1: Point\n\t:param point_2: the second point\n\t:type point_2: Point\n\t:return:\tDistance between two points\n\t:rtype:"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\ndef distance(x1, y1, x2, y2, dist = 0):\n\tif x1 == x2 or x2 == y2:\n\t\treturn math.sqrt(math.pow(dist, 2))\n\treturn math.sqrt((x1 - x2)**2 + (y1 - y2)**2) + dist\nimport tensorflow as tf\nfrom datetime import datetime\nfrom netspy.utils import logger as logger\nimport math\nimport numpy as np\nfrom tensorflow.contrib import slim\n\n\ndef _add_loss_summaries(psnr, losses,"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\tif value is None or value == '':\n\t\t\treturn None\n\t\telif '.' in value:\n\t\t\tif len(value.strip()) is not 4:\n\t\t\t\traise ValueError\n\t\t\treturn Point.from_tuple_list([Point.from_tuple(float(val), 4) for val in value.split(',')])\n\t\telse:\n\t\t\treturn Point.from_tuple(float(value))\nfrom.settings import config\nfrom.config import configuration\nfrom. import configmanager\nimport os, time, json\nfrom.util import urldecode\nimport hashlib\nimport httplib\nfrom. import"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\traise NotImplementedError\n# -*- coding: utf-8 -*-\n##\n## This file is part of Invenio.\n## Copyright (C) 2014 CERN.\n##\n## Invenio is free software; you can redistribute it and/or\n## modify it under the terms of the GNU General Public License as\n## published by the Free Software Foundation; either version 2 of the\n## License, or (at your option) any later version.\n##\n## Invenio is distributed in the hope that it will be useful, but\n## WITHOUT ANY WARRANTY; without even the implied warranty of\n## MERCHANTABILITY or FITNESS FOR A PARTICULAR"}, {"generated_text": "def dehydrate_point(value):\t\"\"\"\n\tDehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\t\"\"\"\n\t\nclass DataTableEntry(object):\n\t\"\"\"DataTableEntry describes a single entry in the data table.\n\t\"\"\"\n\t\n\t__slots__ = ['name', 'value', 'value_min', 'value_max', 'value_error','sort', 'values']\n\tvalid_value = 0\n\tvalid_type = None\n\tdefault_value = 0\n\tdef __init__(self, name, value, value_min, value_max, value_error, sort, values = []):\n\t\tself.name = name\n\t\tself.value = value\n\t\tself.value"}]}
{"task_id": "62e60ed4d76274f8a4026da0", "prompt": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t", "suggestions": [{"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\tkeys = []\n\t\tfor val in self.values.values():\n\t\t\tif not val.recname or val.name.upper() in _reserved_names:\n\t\t\t\traise RecordNameError, _exception(\"Key name %s is reserved\" % val.name)\n\t\t\tkeys.append(val.recname)\n\t\t\tif val.visible:\n\t\t\t\tkeys.append(\"visible\")\n\t\treturn keys\n\t\n\tdef values(self):\t\"\"\"\n\tReturn a list of values.\n\n:return: dict of values keyed by tuple of values\n\t\"\"\"\n\t\tvalues = {}\n\t\t"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn self._keyset.keys()\n\n\n\tdef keys_only(self):\t\"\"\"\n\tReturn a list of key names only\n\n:return: list of keys\t\"\"\"\n\t\treturn self._keyset.keys_only()\n\n\n\tdef has_key(self, key):\n\t\t\"\"\"\n\tReturn if the given key exists in the internal records.\n\n:Parameters:\n     - `key`: A key name as a string.\n\n:return: a list of keys of type KeyType\n:sa the key exists?\t\"\"\"\n\t\ttry:\n\t\t\treturn len(self."}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn self.__keys\n\nclass Casper(Ntuple):\n\t\"\"\"\n\tRepresents a CAS peripheral\n\t\"\"\"\n\n\tdef __new__(cls, name=None, order=None, width=None, height=None):\n\t\t\"\"\"\n\t\tCreate a casper\n\n\t\t:param name: name of the CAS \n\n\t\t:param order: order defined in the CAS\n\n\t\t:param width: Width of the CAS \n\n\t\t:param height: Height of the CAS \n\n\t\t:returns: :py:class:`CASper` object with the card description\n\t\t"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn [ ikey for ikey in self._records.keys()]\n\t@property\n\tdef cnames(self):\n\t\t\"\"\"\n\tReturns the names of the record.\n\n:return: list of names\t\"\"\"\n\t\treturn [ ikey for ikey in self._records.keys() ]\n\tdef keys(self, name):\n\t\t\"\"\" @return: list of names\t\"\"\"\n\t\treturn self._records[name].keys()\n\tdef items(self):\n\t\t\"\"\" @return: list of items  \"\"\"\n\t\treturn self._records\n\n\tdef values(self):"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn self._keys\n\n\tdef value(self, key):\n\t\t\"\"\"\n\tReturn the value of the record.\n\n\t:param key: the key name as used by the engine\n\t:return: the value of the record, if key is set\n\t:raises: :class:`ValueError` if no value for this key\n\t:raises: :class:`MySQLpy.Error` if the record is empty or None\n\t:raises: :class:`ValueError` if key was invalid\n\t:raises: :class:`KeyError` if missing value for this key\n\t\"\"\"\n\t\treturn self"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn [key for key in self.__table__ if key not in self.__reserved__]\n\n\tdef items(self):\n\t\treturn [ [(key, self.__dict__[key]) for key in self.__keys__() \n\t\t   if self.__dict__[key] not in self.__reserved__] for key in self.__keys__()]\n\n\tdef keys_to_lower(self, fieldname):\n\t\tif not self.__reserved__:\n\t\t\treturn [ self.__dict__[key.lower()] \n\t\t\t\t\t\t\t\t\t\t\tfor key in self.__keys__()]\n\t\telse:\n\t\t\treturn [ key.lower() for key"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\n\t\n\treturn set(self.__dict__.keys())\n\t# The MIT License (MIT)\n# Copyright (c) 2014 Microsoft Corporation\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn self._keys\n\n\tdef get_entry_dict(self, record):\n\t\t\"\"\"\n\t\tGet a dictionary of dictionaries which contain the information about\n\t\ta specific entry.  Each entry corresponds to one or more fields on a single line.\n\n\t\t:param entry: dictionary with the fields required by the :class:`Record` to get entry information\n\t\t:type entry: :class:`dict`\n\t\t:return: a dictionary whose keys are entry names and whose values are tuple of strings\n\t\t\"\"\"\n\n\t\treturn {'name': entry._get_name(), 'fieldtype': None, 'index"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\ndef iterkeys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: iterator over keys\t\"\"\"\n\t\ndef itervalues(self):\t\"\"\"\n\tReturn the values of the record.\n\n:return: (value, position of the keys)\n\n:param name: identifier of the record\n:return: a (key, value) iterator over the values of all keys in the record\n\n:note: the following code may be adapted from ``collections`` and may be removed in future versions. This does not help while\nthe new API can be found.  This assumes that"}, {"generated_text": "def keys(self):\t\"\"\"\n\tReturn the keys of the record.\n\n:return: list of key names\t\"\"\"\n\t\treturn [k for k,v in self.fields.items() if not k[0].startswith(\"__\")]\n\ndef main(cmd_args = None):\n\t\"\"\"\n\tThis is the main driver of the program. It calls the appropriate methods.\n\t@brief Parses commandline arguments.\n\t@param cmd_args Command execution arguments.\n\t@return none\n\t\"\"\"\n\tprint \"Inside main.\"\n\tparser = argparse.ArgumentParser(description=\"Simple commandline interface.\")\n\n\tparser.add_argument(\"files\", nargs='+', help=\"files to process\")\n\tparser.add"}]}
{"task_id": "62e60ecfd76274f8a4026d6a", "prompt": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n\n\ttransport_handlers = {}\n\tfor version in protocol_version:\n\t\tif protocol_version & 1:\n\t\t\ttransport_handlers[version] = BoltClient(protocol_version,\n\t\t\t\t    transport_handlers[version][\"secure_transport\"],\n\t\telse:\n\t\t\ttransport_handlers[version] = BoltServer(protocol_version,\n\t\t\t\t    transport_handlers[version][\"secure_transport\"],\n\t\tTransportOptions(transport_handlers[version])),\n\n\n\thandler_classes = {}\n\tfor handler_class in transport_handlers.items():\n\t\tname = None\n\t\ttry:\n\t\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\ndef protocol_version_string():\n\treturn '.'.join([str(x) for x in ProtocolHandler().version])\n\ndef register_protocol_version(protocol_version):\n\tProtocolHandler().version.append(protocol_version)\n\n_method_handlers = {}\ndef register_method_handler(version):\n\t_method_handlers[version] = protocol_handler\n\ndef protocols_by_protocol(protocol_type):\n\treturn _method_handlers.get(protocol_type, None)\n\nregister_protocol_version(protocol_type_to_protocol)"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n    if protocol_version is None:\t# return a dict\n\t\n    return dict((v, k) for k, v in protocol_classes.items() if k in protocol_version) \\\n      if v!= tuple and not inspect.isclass(v) and issubclass(v, (int, long))\n\ndef check_options(*options):\n\tif options:\n\t\traise TypeError(\"Please supply options when parsing with beaker.config.\")\n\tif options and options[0] not in COMMANDS:\n\t\traise TypeError(\"%s not a valid configuration option.\" % options[0])\n\t\ndef"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n\n\tversions = {version for version in protocol_version if version is not None}\n\t\n\thandlers = {}\n\n\tif len(versions) > 0:\n\t\tif len(versions) == 3:\n\t\t\tif isinstance(versions[2], str):\n\t\t\t\tversion_protocol, version_map = [protocols[version] for version in versions]\n\t\t\t\thandlers['versions_to_protocol'][version_protocol] = version_map\n\t\t\telif isinstance(versions[2], ProtocolHandler):\n\t\t\t\thandlers['protocol_to_versions'][version_protocol] = versions[2]\n\t\t\telse:\n\t\t\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\tif not isinstance(protocol_version, (tuple, list)) or \\\n\t\t\tlen(protocol_version)!= 2:\n\t\t\traise TypeError(\"protocol_version must be a tuple or a list\")\n\t\thandler_classes = _handlers.keys()\n\t\thandler_modules = set()\n\t\tif protocol_version[0] is not None:\n\t\t\thandler_modules = protocol_version[0].split('.')\n\t\tif protocol_version[1] is not None:\n\t\t\t# if protocol_version is not set we are still in our\n\t\t\t# handlers yet, we still have to provide"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n\t\n\tif protocol_version is None:\n\t\tprotocol_version = (3, 5)\n\n\ttry:\n\t\tprotocols = bolt.protocol_handlers(protocol_version)\n\texcept TypeError:\n\t\tprotocols = get_supported_protocol_versions()\n\n\tif len(protocols) == 0:\n\t\t# No supported protocol types.\n\t\traise ValueError(\"No protocol types available\")\n\t\t\n\t# Get list of supported protocol versions.\n\tif protocol_version[0].isdigit():\n\t\tversions = range((int(protocol_version[0]),int(protocol_version[0])+1"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\t# NOTE: handle old \"protocol_version\" attr\n\t\tversion = (protocol_version if isinstance(protocol_version, tuple)\n\t\t             else (protocol_version,))\n\n\t\tif isinstance(version, basestring):\n\t\t\tif version.startswith('0'):\n\t\t\t\tversion = (version, )\n\t\t\telse:\n\t\t\t\traise ValueError(\"invalid protocol version '%s'\" %version)\n\t\telif version is None:\n\t\t\traise ValueError(\"unsupported protocol version string='%s'\" %\n\t\t\t\t   version)\n\t\telse:\n\t\t\tversion = tuple(map(int, version))\n\tif version is None:"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\traise TypeError(\"protocol_version must be a 3-tuple of protocol name and version; \"\n                        \"it must be str or None; please provide protocol_version for proper versions\")\n\n\tif protocol_version is None:\n\t\treturn dict()\n\tversion_parts = tuple([int(x) for x in protocol_version])\n\tversions = [protocol_version]\n\tversions.extend(version_parts)\n\n\tif not all(map(lambda x: isinstance(x, BoltHandler), versions)):\n\t\traise TypeError(\"all Bolt handlers must be a tuple of version elements;"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\traise TypeError()\n\tversions = [(0, 0)] + protocol_version  # tuples in format (major, minor, micro)\n\tfor version in versions:\n\t\ttry:\n\t\t\treturn version_handlers[version]\n\t\texcept KeyError:\n\t\t\tpass\n\traise TypeError(\"unsupported protocol version(s): %s\" % \n\t\t\t\t\t  str(versions))\n\t\t\ndef _load_versionable_handlers(versionable_handlers):\n\thandlers = {}\n\tfor key, handler in versionable_handlers.items():\n\t\tversion = (handler.__doc__ or '<unknown version>')"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n    if protocol_version is None:\n            protocol_version = ()    \n    else:\n        protocol_version = tuple(protocol_version)\n\n    protocol_handlers = {}\n\n    if not cls.is_defined(protocol_version):\n        raise TypeError(\"protocol_version must be a valid tuple\")    \n\n    for version in protocol_version:\n        version_class = get_version_class(version)\n        handler = None\n        try:\n            handler = version_class.handlers[version]\n        except KeyError:\n            raise TypeError(\"no handler for version {0}\".format(version))\n\n        # ensure that the protocol handler has not"}]}
{"task_id": "62e60e49d76274f8a4026d25", "prompt": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t", "suggestions": [{"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\ndef transaction(metadata=None, timeout=None):\n\t\"\"\"\n\tThis function can be implemented for transaction functions which takes a transaction and returns the value ``True``.\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.transaction.timeout`` and ``dbms.transaction.timeout`` procedures.\n    It will also get logged to the ``query.log``\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\n\n# -*- coding: utf-8 -*-\n# Copyright 2016 Yelp Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\n\n\tdef my_txn_execute(txn, cursor, query=None, parameters=None):\n\t\t\"\"\"\n\t\texecute select cursor and execute query on cursor\n\n\t\t:param txn:\n\t\t@param cursor:\n\t\t\"\"\"\n\t\tcursor.execute(query, parameters)\n\n\t\treturn txn\n\n\tdef my_txn_list(txn, *parameters):\n\t\t\"\"\"\n\t\tlist with multiple query statements\n\n\t\t:param txn:\n\t\t\"\"\"\n\t\tfor par_id in parameters:\n\t\t\tyield txn.runOperation(\"LIST\", (par_id, ))\n\n\n    if metadata is not None:"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\tif timeout == -1: timeout = 100\n\t\tif metadata is None:\n\t\t\tmetadata = {}\n\n\t\td = datetime.utcnow() + timedelta(seconds=timeout)\n\t\tstmt = \"\"\"\n\t\t\tDECLARE\n             \t\t  a \n            \t\t\t  a.tid\t\t\tas \n            \t\t\t  a.type\t\t\tas \n            \t\t  a.query\t\t\tas \n            \t\t  INNER JOIN user u ON a.uid=u.uid\n            \t\t  WHERE a.active=0\n            \"\"\"\n\t\treturn d.strftime(\"%Y-%m-%d %H:%M:%S\") + stmt"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t \n\tDBMS_CONNECTION_AVAILABLE_ATTRIBUTES = ['dbms', 'inspire']\n\tDBMS_SESSION_ENGINE_ID = 'neo4j'\n\tDBMS_SESSION_NAME = 'neo4j_sessions.session'\n\tDBMS_MAX_CONN = 1000\n\n\tdef __init__(self, metadata=None, timeout=None):\n\t\tschema, (query_path) = util.parse_driver_spec(dbms_library.dbmsFactory())\n\t\tif (not util.is_valid_sqlite_uri(query_path)):"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\n\n\tdef wrap(f):\n\t\t@wraps(f)\n\t\tdef wrapper(*args,**kwargs):\n\t\t\tif timeout:\n\t\t\t\ttimeout_func = getattr(timeout, None) or lambda:timeout\n\t\t\t\ttransaction = Transaction()\n\t\t\t\treturn transaction.execute(f,*args,**kwargs)\n\t\t\telse:\n\t\t\t\treturn transaction.get(timeout=0,callback=f)\n\t\treturn wrapper\n\n\treturn wrap'''\nCreated on Feb 6, 2010\n\n@author: jnaous\n\nDefines the :class:`.Command` class for a :class:`.Model` object.\n'''"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\tDB.DBM_THREADPOOL.flush()\n\n\ttimeout = 0\n\tquery.log(\"Transaction timeout is now %s seconds\" % timeout)\n\ttimeout += 5\n\tif timeout < 0:\n\t\tquery.log(\"Transaction timeout is less than 0 seconds (%s)\" % timeout)\n\t\ttimeout = None\n\n\tclass TimeOutError(Exception): pass\n\n\tif timeout is None:\n\t\tQuery.LOG.warning(\"Timed out waiting for statement %s seconds\" % query.log_statement)\n\n#\t\tquery.log(\"Getting all tables in database\")\n#\t"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\n\n\tdefault_timeout: float or None\n\tdef __init__(self, metadata={}, timeout=None):\n\t\tself.metadata = metadata\n\t\tself.timeout: float or None\t\t\t\n\t\t\n\t\tif timeout:\n\t\t\tself.timeout = float(timeout)\n\t\telse:\n\t\t\tself.timeout = DEFAULT_TIMEOUT\n\n\t\t\n\t\n\tdef setTimeout(self, **kwargs):\n\t\tself.timeout = kwargs\n\n\tdef __enter__(self, context):\n\t\tsql = kwargs[\"sql\"]\n\t\tcursors = self.getCursors()\n\t\tfor row in self.engine.to_"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\n\n\ndef connect_with_transaction_engine(uri):\n    \"\"\"Opens a connection and creates an engine and returns a connection handle with transaction data\"\"\"\n    try:\n        if not engine_available:\n            raise Exception(\"Could not find the engine: \" + uri)\n        try:\n            eng = create_engine(uri, **connect_args)\n            return eng\n        except Exception, e:\n            return None\n    except Exception, e:\n        log.error(\"Unable to connect with engine \" + uri + ':'+ traceback.format_exc())\n        raise e\n\n\ndef connect_with_wait_timeout("}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\t\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\t\"\"\"\n\t\n        # FIXME - this has to be done\n        # pylint: disable=too-many-branches,too-many-branches\n        return\n\n    query = dbms.getQuery()\t\n    meta = dbms.getMeta()\t\n    count = None\n    if not isinstance(query.search().type, pymclevel.MC_TYPE_COLLECTION):\n        # pylint: disable=not-an-iterable\n        pass\n    else:\n        count = query.search().type().find(pymclevel.MC_TYPE_COLLECTION)\n        \n    try:\n        timeout = int(timeout)\n    except Exception as e:\n        log"}]}
{"task_id": "62e60e05d76274f8a4026cfd", "prompt": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t", "suggestions": [{"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\tif key == None:\n\t\t\treturn len(self) + 1\n\t\telse:\n\t\t\treturn self.index(key % self.size)\n\n\tdef pop(self, key):\n\t\t\"\"\"\n\tPop a value off of the given item.\n\n:param key: a key\n:type  key: integer\n:rtype: int\t\"\"\"\n\t\tif key == None:\n\t\t\traise IndexError\n\t\telif key == 0:\n\t\t\traise Exception(\"List is empty\")\n\t\telse:\n\t\t\treturn self.pop(key - 1)\n\n\tdef __getitem__(self, index"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\n        if self.index is None:\n            self.loadIndex()\n        return(self.index)\n\nclass Nest(object):\n    \"\"\"\n    A Nest is an Nestal, which contains a single value and a binary string.\n    \"\"\"\n    def __init__ (self, key, index):\n        \"\"\"\n        index: a list of tuples (key-index, value-index).\n        \"\"\"\n        self.key = key\n        self.value = index\n\t\n    def __cmp__ (self, other):\n        \"\"\"\n        compare the item with another Nest\n        other: the Nest used to compare to"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\traise NotImplementedError('Implement abstract method'+\n\t\t\t'index(), this method should be implemented'+\n\t\t\t'in a subclass.')\n\n\tdef clear(self):\t\"\"\"Remove all items from the tree.\"\"\"\n\t\tself.table.clear()\n\n\tdef indexOf(self, value, n=1):\t\"\"\"Find the given value in the table.\n\t\tIf the key is not present, a default value of None is returned.\n\t\tIf n is out of bounds, raises ValueError.\n\t\tReturns -1 on failure.\n\t\t\"\"\"\n\t\tvalue = _normalizeValue(value, n)"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\tif key < 0:\n\t\t\tself.index = -2\n\t\telif key > len(self):\n\t\t\traise KeyError(\"negative indexing not allowed\")\n\t\telse:\n\t\t\treturn key\n\n\tdef __contains__(self, key):\n\t\t\"\"\"\n\tTest if an item is in the list.\n\tNote that there is no need to test in place\n\t\"\"\"\n\t\tif self.index < -2:\n\t\t\treturn False\n\t\tif key < 0:\n\t\t\tself.index = -2\n\t\telif key > len(self):\n\t\t\traise KeyError(\"negative indexing not allowed\")"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\treturn self.indexes.index(key)\n\n\tdef size(self):\n\t\t\"\"\"\n\tReturn the number of elements.\n\n:return: the number of entries\n:rtype: int\t\"\"\"\n\t\treturn len(self.indexes)\n\n\tdef get(self, idx):\n\t\t\"\"\"\n\tReturn a specified entry by index.\n\n:param idx: the index\n:return: the entry, as a list of str\n:rtype: str, list\n\t\"\"\"\n\t\tfor i in range(len(self.indexes)):\n\t\t\tif self.indexes[i] == idx"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\traise Exception(\"Sorter index is not supported\")\n\tif key == self.id: return 0\n\tif key.isdigit() or key == \"\":\n\t\treturn self.id\n\treturn self.mapping[key]\n    \n \n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n#"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\ttry :\n\t\t\treturn self.content[key].index(self.content[key], 0)\n\t\texcept IndexError:\n\t\t\treturn -1\n\n\tdef contains(self, key):\n\t\t\"\"\"\n\t\tCheck if a given key exists.\n\n\t\t:param key: a key\n\t\t:return: index if key exists\n\t\t:rtype: bool\n\t\t\"\"\"\n\t\ttry :\n\t\t\tif key in self.content.keys() :\n\t\t\t\treturn True\n\n\t\texcept IndexError:\n\t\t\treturn False\n\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\titerator of the document content\n\t\t"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\treturn self.keys().index(key)\n\n\tdef set(self, key, value): \n\t\tself.keys().append(key)\n\t\tself.index(key)\n\t\tself.write_text(\"\\r\")\n\n\tdef to_string(self):\n\t\treturn str(self.__dict__)\n\n\tdef clear(self):\n\t\tself.save()\n\n\tdef reset(self):\n\t\t\"\"\"\n\t\tReset the values in the item to default.\n\t\t\"\"\"\n\t\tself.write_text(\"\")\n\n\tdef reset_from_list(self, item_list):\n\t"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\treturn self.items.index(key)\n\t\t\n\tdef get(self, key, *default):\n\t\t\"\"\" Return an item by key or return default item when the key \n\t\tdoes not exist.\n\t\t\"\"\"\n\t\titem = None\n\t\tfor i in self.items:\n\t\t\tif i.key == key:\n\t\t\t\tif default:\t\t\n\t\t\t\t\treturn i.value\n\t\t\t\telse:\n\t\t\t\t\titem = i\n\t\t\t\t\tbreak\n\t\t\telif i.key == key:\n\t\t\t\treturn i\n\t\t\t\tbreak\n\t\treturn item\n\t\t\nclass Md3(object):\n\t\"\"\"\n\tH"}, {"generated_text": "def index(self, key):\t\"\"\"\n\tReturn the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\t\"\"\"\n\t\tif key >= len(self.dict):\n\t\t\treturn None\n\t\treturn self.dict[key]\n\t#def keys(self):\n\t#\treturn list(self.dict.keys())\n\tdef __iter__(self):\n\t\tif self._items == []:\n\t\t\tfor k in self.keys():\n\t\t\t\tyield k\n\t\telse:\n\t\t\tfor k in self._items:\n\t\t\t\tfor i in self._items[k]:\n\t\t\t\t\tyield i\n\n\tdef __getitem__(self, index):\n\t\t\"\"\"\n\t\tFetch the given item.\n\n\t\t:param index: the"}]}
{"task_id": "62e60da4d76274f8a4026cf1", "prompt": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t", "suggestions": [{"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tlst = []\n\n\t\tif not keys:\n\t\t\tfor key in self._index:\n\t\t\t\ttry:\n\t\t\t\t\tlst += self[key]\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\t\telse:\n\t\t\tfor key in self._index:\n\t\t\t\ttry:\n\t\t\t\t\tlst += [el[i] for i in keys]\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\n\t\treturn lst\n\n\tdef has_field(self, fieldname):\n\t\t\"\"\"\n\t\tCheck whether the specified field is present in the record\n\t\t\"\"\"\n\t\tflds = self.keys()\n\t\tflds.sort()\n\t\tif"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tif not keys:\n\t\t\tkeys = self.keys\n\t\telif len(keys) > len(self.keys):\n\t\t\tkeys = keys[:len(keys)*2]\n\t\treturn filter(lambda x: not x[0].endswith(\":\") and (not x[0] == \".\" and len(x[1]) >= 48), keys)\nimport pytest\n\n\nfrom cfme import test_requirements\nfrom cfme.infrastructure.provider.rhevm.rhevm import RHEVMProvider\nfrom cfme.utils.appliance.implementations.ui import navigate_to\nfrom cfme.utils"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\treturn self.getValues(keys, \"values\")\n\t\n\tdef valueLabels(self, *labels):\n\t\t\"\"\"\n\tReturn the labels in the records that correspond to the values\n\tas given - not necessarily used - that is, they are, not\n\tgenerated from the supplied order, thus the order doesn't matter\n\tany more than one entry\n\n:param labels: labels, or a list of label keys\n:return: list of labels\n:rtype: list\t\"\"\"\n\t\treturn self.getValues(labels, \"value labels\")\n\t\t\n\tdef valueLabelsMap(self, *"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\treturn [ self[k] for k in keys ]\n\t\t\t\n\tdef __contains__(self, key):\n\t\treturn key in self.keys()\n\t\n\tdef __len__(self):\n\t\treturn len(self.keys())\n\t\n\tdef __iter__(self):\n\t\tfor k in self: yield k\n\n\tdef keys(self):\n\t\treturn self.keys()\n\t\n\tdef values(self, *keys):\n\t\treturn self.values()\n\t\n\nclass Record_V2(Record):\n\t\n\t\"\"\"Record V2 interface: note that it contains a short\n\t\"\"\"\n\t\n\tdef __"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tvalues = list(map(str, [self[key] for key in keys]))\n\t\tif len(values) == 0:\n\t\t\traise StopIteration\n\t\tfor x in values:\n\t\t\tyield tuple(x)\n\t\t\n\tdef items(self):\n\t\t\"\"\"\n\tIterator yielding the values of all records in the file,\nincluding those which are not contained, in order.\n\n:return: iterator that yields the values of all records\n:rtype: iterator\n\"\"\"\n\t\treturn ((key, self[key]) for key in self)\n\n\tdef __iter__(self):\n\t\tfor i"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tif not keys:\n\t\t\treturn []\t\n\t\tif isinstance(keys, dict):\n\t\t\t# allow passing in a list of index/key-values\n\t\t\treturn [v for v in super(Record, self).__getitem__(keys)]\n\t\telse:\n\t\t\treturn super(Record, self).__getitem__(keys)\n\n\n\nclass LazyRecord(Record):\n\t\"\"\"\n\tA record that is lazy-fetched from the database. This wraps the database\n\tfunction lazily. The result of this function is cached, that way subsequent loads of the record\n\twill result in a new DB-API \""}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tif len(keys) == 0:\n\t\t\tkeys = self.fields\n\t\telif len(keys) == 1:\n\t\t\tkeys = keys[0]\n\t\treturn [value for key, value in self.items\n\t\t\t\tif value in keys]\n\t\t\t\n\t# --------------------------------------------------------------------\n\t# Sorting on a value gives more meaningful results, but more\n\t# correct ones.  This is the same thing as searchsorted.\n\t# --------------------------------------------------------------------\n\t# FIXME: Implement sorting!\n\tdef __getitem__(self, key):\n\t# FIXME: Implement sorting!\n\tif type(key) is"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tlst = RecordList(self.db, self.collection, self.filters)\n\t\tif keys == None:\n\t\t\treturn lst.values()\n\n\t\tif keys == []:\n\t\t\tkeys = self._all_keys()\n\n\t\treturn lst.values(flat=True)\n\n\tdef _key_field(self):\n\t\t\"\"\"Return the key field name (or index) of the record, or ``None`` if\n        this is not a valid index or key.\n\n\t\t:return: string (or None) on failure, or ``None`` if this is missing\n                 key or index\n\t\t:"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\tidx = super(Record, self).values(*keys)\n\t\tif len(idx) == 1 and idx[0] == rec_idx:\n\t\t\t return [ self._records[rec_idx], ]\n\t\treturn self\n\n\t@staticmethod\n\tdef _from_sql(sql):\n                \"\"\"\n\t\tTranslate an sql statement\n\n\t\t>>> e = record()\n\t\t>>> e['foo'] = 'bar'\n\t\t>>> e['bar'] = 'foo'\n\t\t>>> e2 = record(foo='bar')\n\t\t>>> e2[0] == e['foo']\n\t\tTrue\n\t"}, {"generated_text": "def values(self, *keys):\t\"\"\"\n\tReturn the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\t\"\"\"\n\t\treturn self._select(\"values\", *keys)\n\n\t@property \n\tdef select_field(self):\n\t\t\"\"\"Selection field, accepts a 2-tuple of the key and\n\t\tselection name\"\"\"\n\t\treturn self._select(\"select_field\")\n\n\t@property\n\tdef as_dict(self):\n\t\t\"\"\"Return data as an dict\"\"\"\n\t\treturn dict(zip(self._select(\"as_dict\"), self))\n\n\t@property\n\tdef as_yaml(self):\n\t\t\"\"\"Return data as an yml\"\"\"\n\t\treturn yaml.safe_dump(self."}]}
{"task_id": "62e60b10d76274f8a4026ccd", "prompt": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t", "suggestions": [{"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tassert len(keys) == 0, \"must supply at least one key to include\"\n\n        # the keys must be a non-None list\n        if not keys:\n\t\t\traise ValueError(\"keys can not be a non-None list\")\n\n        # the key index must be a non-None list\n        if all((isinstance(key, int) or isinstance(key, long)) for key in keys):\n\t\t\treturn {}\n\t\treturn [None] + list(map(self.getByKey, keys))\n\n\t# the value index must be a non-None list of int or long\n\ttry:"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t       \n\t\tres = {}\n\t\tfor v in self.items(keys):\n\t\t\tv_parts = self._v(v)\n\t\t\tif len(v)!= len(self.items(v_parts)):\n\t\t\t\tlog.debug(\"key {field} missing required length\", field=k)\n\t\t\t\traise IndexError\n\t\t\tres[k] = v_parts\n\t\treturn res\n\n\t#\n\t# Data retrieval\n\t#\n\t# Data retrieval is different from other ``list`` and the ``value``\n\t# argument (which in the examples lists below) that returns a particular\n"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tfor key in keys:\n\t\t\tif key not in self : continue # invalid key\n\t\t\td = self.get(str(key), {})\n\t\t\tif not any(d.get(v) for v in keys): raise IndexError, d\n\t\t\tfor v in keys:\n\t\t\t\td[v] = d.get('*.'+key, d.get(key+'*.'+v, None))\n\t\treturn self\n\n\t# -------------\n\t# get(key)[index] = value\n\t# -------------\n\t#\n\tdef get(self, key, default_values ="}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tout   = {}\n\t\tfor key in keys:\n\t\t\tout[key] = getattr(self,key)\n\t\treturn out\n\tif isinstance(keys, dict):\n\t\titems = self.__items(**keys)\n\t\treturn iter(items)\n\telif hasattr(keys, '__iter__'):\n\t\treturn (key for key in keys if isinstance(self,key) and\n\t\t\t\t   (key._key is not None or key._value is not None)\n\t\t\t\t   )\n\telse:\n\t\treturn (key for key in keys if isinstance(self,key))\n\t\t  \n\titems ="}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tvalues, keys = self._get_data(keys)\n\t\tcount = len(values)\n\t\tif count == 0:\n\t\t\traise IndexError()\n\t\t# use list comprehension to handle this in python3\n\t\tvalues = [[self[keys[idx]] for idx in idx] for idx in keys]\n\t\tresult = dict(zip(keys, values))\n\t\tresults = list(result.values())\n\t\tresults.sort(key=self.keys())\n\t\treturn result, results\n\n\tdef _get_data(self, keys):\n\t\tresult = dict()\n\t\tfor"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tkeys = listwrap(keys, 0, 1)\n\t\tif len(keys) == 0: return {}\n\t\tif len(keys) == 1:\n\t\t\treturn {k : v for k, v in self.iteritems() if v[1]}\n\t\telse:\n\t\t\treturn {k : v for k, v in self.items() if v[1]}\n\n\tdef __contains__(self, key):\treturn key in self.keys()\n\tdef __iter__(self):\n\t\treturn iter(self.keys())\n\n\tdef __setitem__(self, key, value):\n\t"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\treturn self.items(keys)\n\n\t@abstractmethod\n\tdef items(self, keys=True):\n\t\t\"\"\"\n\tReturn a list of items, if known, at position 0, else [].\n\tThe returned items are either :const:`None` or of length 0.\n\t\tThe length of the given range can be determined\n\t\tby using :attr:`Item.n_values`.\n\n\t\t:param keys: if True, only the keys that are being returned\n\n\t\t\"\"\"\n\tpass\n\n\tdef tostr(self, **kargs):\n\t\t\"\"\"\n\t\tConvert object to a string"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tkeys = set(keys)\n\t\tfor index,key in enumerate(keys):\n\t\t\tif key == None: continue\n\t\t\tif key in keys:\n\t\t\t\tif index >= len(keys) or index < 0:\n\t\t\t\t\traise IndexError(\"index %d out of bounds\" % index)\n\t\t\t\tkey = keys.pop(index)\n\t\t\tself.add(key,record[key])\n\n\tdef add(self, key, value):\t# FIXME: optimize this\n\t\tif key in self.data: \n\t\t\traise IndexError\n\t\tif value in [None, 1]: value = self."}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\tnew = {}\n\t\tfor k in keys or []:\n\t\t\tif isinstance(k, slice):\n\t\t\t\tnew[str(k)]   = []\n\t\t\telif isinstance(k, basestring):\n\t\t\t\t# new[k] = int(k)\n\t\t\t\tnew[str(k)] = int(k)\n\t\t\telif isinstance(k, int):\n\t\t\t\t# new[k] = int(k)\n\t\t\t\tnew[k] = int(k)\n\t\t\telse:\n\t\t\t\traise IndexError(\"Data expects integer or slice arguments, got %s\" % str(type(k)))\n\t\treturn"}, {"generated_text": "def data(self, *keys):\t\"\"\"\n\tReturn the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified\t\"\"\"\n\t\t# we use an ordered union here since we want indexing in place\n\t\t# first in case someone changes the list, so we need to\n\t\t# go from the higher indexes to the left of the end, otherwise use\n\t\t# values from the higher indexes.  This would make it possible to \n\t\t# include a whole bunch of items even if they're all ints and this\n\t\t# results in a wrong index.\n\t\t#\n\t\t# It would be advantageous for it to work with lists of ints, however\n\t\t# that would require a lot more space. (For example, if\n\t\t#"}]}
{"task_id": "62e6087bd76274f8a4026bfa", "prompt": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tassert self.read(4) == b'u16', 'u16 is not zero-padded'\n\t\treturn unpack('!H', self.read(2))[0]\n\n\tdef pop_u32(self):\t\"\"\"\n\tRemove the third two bytes of data, returning them as a big-endian\n32-bit floating point number.\t\"\"\"\n\t\tassert self.read(4) == b'u32', 'u32 is not zero-padded'\n\t\treturn unpack('!I', self.read(4))[0]\n\n\tdef pop_u64(self):\t\"\"\""}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tval, = struct.unpack('<16b', self.fp.read(2))\n\t\treturn val & 0xffff\n\nclass Int8(Const):\n\t\"\"\"A signed big-endian integer.\"\"\"\n\tdef __init__(self, val):\n\t\tself.val = val\n\n\n# The encoding of the 'int' structure.\nINT_ENCODING = {4: 0, 8: 65535, 16: 4294967295}\n\nclass SignedByte(Const):\n\t\"\"\"An encoded big-endian signed integer.\"\"\"\n\tdef __init__(self, val, name, length, byteorder,"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tresult = self.data[-2:]\n\t\tself.data = self.data[:-2]\n\t\treturn result\n\n\tdef pop_u32(self):\t\"\"\"\n\tTurn the next byte of the data 7 bits into a 64-bit integer.\n \t\"\"\"\n        result = self.data[1]  \t# This is a big-endian 32-bit int.\n        self.data = self.data[0]  \t# This is a big-endian 32-bit int.\n        return result\n\n\n\tdef main(self):\n\t\t\"\"\"Process the data segments into two bytes.\n\t"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\ts = ''\n\t\twhile len(s) < 5:\n\t\t\ts += self.mem[self.ptr+2:self.ptr+4]\n\t\t\tself.ptr += 4\n\t\treturn int.from_bytes(s)\n\t\n\tdef get_uint16(self):\t\"\"\"\n\tExtract the 16-bit unsigned integer from the data buffer.\n\t\tThe result is in big-endian 16-bit format.\n\t\tAn odd-length data buffer is stored as a two's-complement 16-bit\n\t\tinteger with the least significant byte.\n\t\t\"\"\"\n\t\ts ="}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\tresult = 0\n\t\twhile read(1)!= '2':\n\t\t\tresult <<= 1\n\t\t\tread(1)\n\t\treturn (result >> 16) + (result & 0xffff)\n\tload_u16 = splot_unpack(pop_u16, \">H\")\n\tdef load_u32(self):\t\"\"\"\n\tLoad an unsigned integer from the CPU register.\t\"\"\"\n\t\tresult = ((read(1) << 17) | (read(1) >> 8)\n\t\tfor bit in range(8) if read(1) else 8):\n\t\t\tresult"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\treturn struct.unpack('<H', self.data[self.index+2:self.index+4])[0]\n\tdef push_u16(self, value):\t# NOTE: byte order is reversed from big endian\n\t\tself.buffer.append(struct.pack('<H', value))\n\tdef get_u_l_from_s_u16(self):\n\t\tl = self.get_u16()\n\t\tif (l[0] << 8)!= self.get_u_l_from_s_s_u16(l[0] +"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\treturn unpack('>L', self.data[0:2])[0]\n\n\n\tdef pop_S(self):\t# Use the previous checksum to identify the packet's security\n\tb4 = self.pop_U16()\t\t# Use the previous transaction's security\n\tB4 = self.pop_S()\t\t# Use the previous transaction's security\n\tB4 |= 0xFFF000\n\tif b4 > UNLIMITED_0:\t# If 0xBBBB, keep going\n\t\tself.data = data3((b4 & 0x0fff), (b"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\ta, b, c = self.a, self.b, self.c\n\n\t\tself.a = a >> 4 & 0xff\n\t\tself.b = b >> 4 & 0xff\n\t\tself.c = c >> 4 & 0xff\n\n\tdef pop_u24(self):\n\t\tif self.a == 128 and self.b in [0x1, 0x2, 0x4]:\n\t\t\tself.a -= 0x8000000000\n\t\telse:\n\t\t\tself.a -= 0x1000000000\n\t\tself.b += 0x1000000000\n\n\tdef pop"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\t# XXX: this is slow on byte swap if necessary...\n\t\tif ord(self.data[0]) & 0x80:\t# 1 == 1\n\t\t\tself.data = self.data[1:]\n\t\t\t# No more zero bytes to remove (the rest will be the same anyway)\n\t\t\tif self.size!= 2:\n\t\t\t\traise ImageFileException('Unable to pop Big-endian 16-bit integer from image file %s, the number of bytes %d must be 2' % (self.filename, len(self.data)))\n\t\t\treturn\n\t\telse:\n\t\t\t#"}, {"generated_text": "def pop_u16(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\t\"\"\"\n\t\treturn self.unpack(\"h\", 4)[0]\n\tdef pop_u32(self):\t\"\"\"\n\tRemove the last three bytes of data, returning them as a big-endian 32-bit\nsigned integer.\t\"\"\"\n\t\treturn self.unpack(\"i\", 8)[0]\n\tdef pop_u64(self):\t\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian 64-bit\ninteger.\t\"\"\"\n\t\treturn self.unpack(\"q\", 8)[0]\n\tdef pop_u80(self):\t\"\"\"\n\tRemove"}]}
{"task_id": "62e6087ad76274f8a4026bf2", "prompt": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t", "suggestions": [{"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\tpass\n\n\ttry:\n\t\treturn self.__send(qid, \"discard\", [n, qid, dehydration_hooks, handlers])\n\texcept:\n\t\timport traceback\n\t\tprint traceback.print_exc()\n\t\t\tprint \"Discarded to dispose unknown event\"\n\t\t\treturn None\n\t\treturn None\n\n\t# internal methods\n\n\tdef begin(self, qid=-1, dehydration_hooks=None, handlers=[]):\n\t\t\"\"\"\n\t\tStarts receiving events on the output queue.\n\n\t\tArgs:\n\t\t\tqid (int): query ID to start"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\t# Note that default handlers are applied, even if none of\n\t\t# them are set by call with a handler specified\n\t\tif handlers:\n\t\t\tself.message.remove(unpack_first(handlers))\n\telif qid > -1:\n\t\t# query ID specified without a handler\n\t\tself.message.remove(unpack_first(packstream.Decode(qid)))\n\telse:\n\t\t# query ID not given or empty, queue it up\n\t\tself._discard_messages.append((qid, self._empty))\n\tself._last_message = None\n\tdel"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\tself.log.info(\"Discarding %d record(s)\" % n)\n\t\tif not n: return\n\t\tresult = self._server.purge(n, qid, dehydration_hooks, handlers)\n\t\tif not result:\n\t\t\treturn\n\n\t\tres_seq = []\n\n\t\tfor record in result:\n\t\t\tres_seq.append(record)\n\n\t\tif res_seq:\n\t\t\tres_seq.sort(lambda x, y: cmp(x[0], y[0]) or cmp(x[5], y[0]) or cmp(x"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\ttry:\n\t\t\t# clear buffer\n\t\t\tself.q_out_buff[qid].clear()\n\t\texcept KeyError:\n\t\t\treturn\n\t\t# fill buffer and pack()\n\t\tbuf = self.response_buffer + struct.pack(\"!i\", n)\n\t\tn = len(buf)\n\t\twhile n > 0:\n\t\t\tself.q_out_buff[qid].put(buf[n:])\n\t\t\tn -= len(buf)\n\t\tself.q_out_buff[qid].put(struct.pack(\"!i\", n))\n\t#\n\t#"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\n        if dehydration_hooks is None:\n\t\t\tdehydration_hooks = dict()\n\t\tdehydration_hooks[qid] = self\n\t\thooks = map(lambda x: x[0], dehydration_hooks.itervalues())\n\t\tdel dehydration_hooks[qid]\n\t\tdel hooks[0]\n\n\tif not handlers:\n\t    raise TypeError(\"Invalid response: no handlers have been defined.\")\n\n        for d in handlers:\n\t    for h in hooks[1:]:\n\t\tself._on_response(h, d)\n\n\tfor h,"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\n        if not isinstance(n, int):\n                raise TypeError(\"n must be an integer\")\n        \n        if not isinstance(qid, int):\n                raise TypeError(\"qid must be an integer\")\n        \n        if not isinstance(dehydration_hooks, list) and not callable(dehydration_hooks):\n                raise TypeError(\"dehydration_hooks must be a list\")\n        \n        for l in dehydration_hooks:\n                self._hooks.set(l, None)\n\tfor l in handlers:\n                self._hooks.set(l, None)\n\tfor l in dehyd"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t   )\n\t\tpass\n\n\n  def start(self):\n    \"\"\"\n    Starts the task queue.\n\n\tYou cannot add new tasks in a queue.\n    \"\"\"\n    pass\n\n\n  def abort(self, name=None):\n    \"\"\"\n    Aborts the task queue. \n\n    This terminates the task queue.\n\n    The task will be removed from the queue all the pending tasks\n    and the associated object will be released back to the calling application.\n    \"\"\"\n    pass\n\n\n  def stop(self, name=None):\n    \"\"\"\n    Stops the task queue.\n\n    If the task queue does not declare any work in progress, or not all"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t return Response(reply=self, errors=[], method=self.dispatch, \n\t\t       body=json.dumps(self.marshal_body(n, qid, dehydration_hooks, handlers)), \n\t\t\t       content_type='application/json')\n\t#def dispatch(self, path):\n\t#\t\"\"\"\n\t#\tAllows for the receiving of requests which have arrived since we have\n\t#\tto send the message. This method is used to receive data\n\t#\tfrom a client.\n\t#\t\n\t#\t :param path: file path to receive data"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\tfor handler in handlers:\n\t\t\tif hasattr(handler, 'dehydrate'):\n\t\t\t\thandler.dehydrate(n, qid, dehydration_hooks)\n\t\t\telse:\n\t\t\t\thandler(n, qid, dehydration_hooks)\n\n\n\n# For debugging\n################################################\n#\n# The following code can be considered private to the user and/or\n# subclassed from the module.\n################################################\nclass Dispatcher:\n\tdef __init__(self, debug):\n\t\tself.debug = debug\n\t\n\tdef dehydrate(self, id,"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\t\"\"\"\n\t\tpass\n\n\t# Create the headers object. We don't need this anyway\n\theaders = {\n\t    'Message-ID': str(qid),\n\t    'To': [self.config['server']['toAddress']])\n\t}\n\n\t# Create the message with one message-id, so we can pass it to the connection\n\tmessage = Message(headers, data=b'duplicate', correlation_id=qid)\n\tmessage_id = uuid.uuid1().hex\n\n\tfor ext in self.output_extensions:\n\t    ext.consume(message, message_id, qid)\n\n"}]}
{"task_id": "62e60879d76274f8a4026bec", "prompt": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t", "suggestions": [{"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\tself.write_lock.acquire()\n\t\tresult = deferToThread(self.out_queue, (mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, handlers,))\n\t\tself.write_lock.release()\n\t\treturn result\n\tdef commit(self):\n\t\tself.write_lock.acquire()\n\t\tresult = deferToThread(self.out_queue, (0,))\n\t\tself.write_lock.release()\n\t\treturn result\n\n\tdef runQuery(self, query, database, timeout):\n\t\t\"\"\"\n\t"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\tif not logger:\n\t\t\tlogger = logging.getLogger()\n\n\t\tif not isinstance(logger, BoltLogger):\n\t\t\tlogger.addHandler(logging.StreamHandler())\n\n\t\tdef __init_handle_output(type, msg, **kwargs):\n\t\t\tif hasattr(msg, 'id'):\n\t\t\t\tkwargs['_id'] = msg.id\n\t\t\telif isinstance(msg, Bundle):\n\t\t\t\tkwargs['_id'] = kwargs['_bundle.id']\n\t\t\telif isinstance(msg, Message):\n\t\t\t\tkwargs['_id'] = msg.bundle.id\n\t\t\telif isinstance(msg, ErrorMessage):\n\t\t\t\tkwargs"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\traise NotImplementedError()\n\n\nclass _response(_responsebase):\n\tdef __init__(self, id, status, transaction_id, type, response_status, content=\"\", timestamp=\"\",\n\t\t description=\"\", context=None):\n\t\t_responsebase.__init__(self)\n\t\tself.status = status\n\t\tself.transaction_id = transaction_id\n\t\tself.type = type\n\t\tself.transaction_data = {\"transaction_id\": transaction_id, \"timestamp\": timestamp, \"response_status\": response_status, \"content\": content, 'type': type}\n\t\tself.callback_"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\n\t\n\tdata = dict(mode = mode, bookmarks = bookmarks, metadata = metadata, timeout = timeout, db = db)\n\t\n\tpacket = Packet(headerLength=Header.VERSION, data = data)\n\n\t# send reply\n\tself.send(packet)\n\t\n\n\tif self.mode == READ:\n\t\tself.read_from_file()\n\t\n\n\t# send failed\n\telif self.mode == WRITE:\n\t\tto_write = self.to_write\n\t\tself.send(to_write)\n\n\t\t\t\t\t\t\n\telse:\n\t\traise Exception(\"I don't know what"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\tpass\n\n\tdef commit(self, commit, timeout=30*60):\n\t\t\"\"\"Commits the write\"\"\"\n\t\ttry:\n\t\t\treturn self.conn.commit(commit)\n\t\texcept Exception:\n\t\t\tif not self.reconnect(timeout):\n\t\t\t\traise\n\n\tdef rollback(self):\n\t\t\"\"\"Rolls back the write\"\"\"\n\t\ttry:\n\t\t\treturn self.conn.rollback()\n\t\texcept Exception:\n\t\t\tif not self.reconnect(30*60):\n\t\t\t\traise\n\n\tdef _reconnect(self, timeout):\n\t\t\"\"\"Helper function to reconnect\"\"\"\n\t\t"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\tif isinstance(mode, str):\n\t\t\tmode = mode.lower()\n\n\t\tflags = None\n\t\tif mode == \"LOCK\":\n\t\t\tflags = get_flags()\n\t\t\tset_flags_if_mine_when_locking()\n\n\t\tif ( isinstance(bookmarks, list) or isinstance(bookmarks,tuple)) and len(bookmarks) > 0:\n\n\t\t\tbookmarks = \",\".join(bookmarks).split(\",\")\n\n\t\t\t# check for invalid bookmark names\n\t\t\tfor b in bookmarks:\n\t\t\t\tif b not in BIOPY_ALL_VALUES:\n\t\t\t\t\traise ValueError(\"invalid bookmark name : %s"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\n\n\t\treturn Response(json.dumps(self.__transaction), mimetype='application/json')\n\n\tdef end(self, mode=None, bookmarks=None, metadata=None):\n\t\t\"\"\"\n\t\tEnd the transaction.\n\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should end\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:return: Response object\t\"\"\"\n\t\n\t\treturn Response(json.dumps(self.__transaction), mimetype='application/json')\n\n"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\tbody = self.packstream.dumps(args)\n\t\treturn body[:int(timeout)] + utf8encode(body[int(timeout):])\n\t\n\tdef end(self):\n\t\tbody = self.packstream.getvalue()\n\t\tself.packstream.close()\n\t\tself.unpacker.done(self)\n\t\treturn body\n\t\n\tdef abort(self):\n\t\tbody = self.packstream.getvalue()\n\t\tself.packstream.close()\n\t\tself.unpacker.done(self)\n\t\n\tdef get_bookmarks(self):\n\t\tif self.bookmarks"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\treturn self._send_request_sync(\"begin\", request, timeout, callback,\n\t\t                               error_callback,\n\t                               interfaces, timeout, user, bookmarks, metadata, timeout,\n\t\t                               db, module, callback_module, heartbeats, handlers)\n\n\n\tdef commit(self, sync_transaction):\n\t\t\"\"\"\n\t\tCommits the given PUSH_X transaction to the PUSH_X backend\n\n\t\t:param sync_transaction: the PUSH_X transaction to be commited through\n\t\t\"\"\"\n\t\treturn self._send_request_sync(\"commit\", request, None, sync_"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\t\"\"\"\n\t\tif not isinstance(mode, str) or mode not in (\"READ\", \"WRITE\", \"READ\") or \\\n\t\t   not callable(mode): # @todo should check that default \"READ\" to be called\n\n\t\t# TODO: replace to be more flexible..\n\t\tif mode == 'READ': # we can read the transaction's body\n\t\t\tpass\n\n\t\tif not isinstance(bookmarks, list) or \\\n\t\t   type(bookmarks) not in (list, tuple): # @todo this won't raise\n\t\t\tbookmarks = list(bookmarks)\n\n\t\tif not isinstance(metadata, dict) or \\\n\t\t  "}]}
{"task_id": "62e60723d76274f8a4026b75", "prompt": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\treturn round(integer_to_roman(n),6)\n#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n# (c) 2012, Michael DeHaan <michael.dehaan@gmail.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\n    i = int(round(n))\n    return round(i)\n\nif __name__ == \"__main__\":\n    test = RoundingTest()\n    ans = n_in_ten_thousand(*range(10000), repeat=2)\n    test.assertEquals(ans, 10000)\n    test.assertEquals(ans, 5001)\n    test.assertEquals(ans, 1001)\n    ans = round_half_to_even(*range(100), repeat=2)\n    test.assertEquals(ans, 30000)\n    test.assertEquals(ans, 60000)\n    test.assertEquals(ans,"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\nclass BitVector(tuple):\n\t\"\"\"Stores a bit vector\n\n\t>>> a = BitVector(9)\n\t>>> a > 0\n\tTrue\n\t>>> a.sign(BitVector(0))\n\n\t>>> a = BitVector(1)\n\t>>> a == 1\n\tTrue\n\t>>> a.sign(BitVector(3))\n\tFalse\n\t>>> a = BitVector(9)\n\t>>> a.sign(BitVector(0))\n\tTrue\n\t\"\"\"\n\t\n\t@classmethod\n\tdef sign(cls, n):\t\n\t\t\"\"\"\n\t\tReturns"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\t# first round to even = to the nearest even number if n is even\n\t\t# if n is odd, then even = to the nearest odd number\n\t\tfor i in xrange(n - 3):\n\t\t\treturn round_even_to_odd(n) + i\n\n\t# if n is odd, then one odd number gets rounded up to even integer\n\t# and then a odd number gets rounded out to even integer\n\t# as an odd number doesn't show up in this case\n\todd_even = round_even_to_even(n)\n\todd_even = int("}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\ndef sqrt_pow(n):\n\t\"\"\"\n\t>>> sqrt_pow(3)\n\t6.0\n\t>>> sqrt_pow(3.7)\n\t7.0\n\t>>> sqrt_pow(3.19)\n\t6.19\n\t>>> sqrt_pow(3), 2\n\t(3.0, 6.2)\n\t\"\"\"\n\treturn 2.0**n\nfrom lib.common import helpers\n\nclass Module:\n\n    def __init__(self, mainMenu, params=[]):\n\n        # metadata info about the module, not modified during runtime\n        self"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\n    if n >= 0:\n\t\treturn int(n)\n\telse: \n\t\treturn 3\n\ndef sqrt_even(n):\t\n\t\"\"\"\n\tThis method sums the square root n\n\t>>> sqrt_even(3)\n\t100\n\t>>> sqrt_even(3.1)\n\t90\n\t>>> sqrt_even(1.5)\n\t100\n\t>>> sqrt_even(1.5,1)\n\t110\n\t>>> sqrt_even(1.5,1,1)\n\t110\n\t>>> sqrt_even(1,1,"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\treturn n * 2**(int(str(n)[0]) - 1)\n\n\nclass Gray(object):\n\tdef __init__(self):\n\t\t# self.size_min = 2\n\n\t\t# self.size_max = 2\n\n\t\t# self.size = [2, 2]\n\t\t# self.size_min = [2, 3]\n\n\t\t# self.size_max = [5]\n\n\t\t# self.size_min = [1]\n\n\t\t# self.size_max = [3]\n\n\t\t# self.size_min = [2, 3"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\ta = n % 1\n\t\tb = n / 1\n\t\treturn (round((((a + 2) % a) % b), 2)\n\n\nif __name__ == '__main__':\n    main('end 1.0', 1.0)\n\n# coding: utf-8\n\n# -------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n# --------------------------------------------------------------------------\n\n\"\"\"\nFILE: sample_analyze.py\n\nDESCRIPTION:\n    This sample demonstrates how to analyze your"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\ndef log_two_points(x, n, base=round_half_to_even):\n\t\"\"\"\n\tReturns the log of the two points of an integer.\n\n\t:param x: an integer\n\t:param n:\n\t:return: log of x\n\t\"\"\"\n\t\n\tif n > 0:\n\t\tleft_half = log(x, base)\n\t\tright_half = log(1 + n, base)\n\t\treturn left_half + right_half\n\ndef square_root(x, n):\n\t\"\"\"\n\tReturn x where number of elements"}, {"generated_text": "def round_half_to_even(n):\t\"\"\"\n\t>>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:\t\"\"\"\n\t\n\n\treturn float(str(round(n / 6, n - 1)) + \"E\")\n\n\ndef round_half_to_odd(num):\t\"\"\"\n\t>>> round_half_to_odd(3)\n    3\n    >>> round_half_to_odd(5)\n    3\n    >>> round_half_to_odd(3.2)\n    3\n    >>> round_half_to_odd(3.5)\n    4\n    >>> round_half_to_odd(3.7)\n    5\n\n:param num:\t\"\"\"\n\tsum = int((num"}]}
{"task_id": "62e60707d76274f8a4026b69", "prompt": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tctype = 'Point'\n\t\tfields = fields - {'srid': srid_map}\n\n\tbase_data = {'type': 'Point'}\n\tGeometry.__table_name__ = _table_name\n\n\tBaseGeometry = _geometry_class( BaseGeometry )\n\n\tclass Point(BaseGeometry):#, srid_map):\n\t\tgeometry = {'type': PointType}\n\t\t_fields_ = fields\n\n\ndef geos_version(version=None):\n\tversion = version or postgresql_core.get_version()\n\tmajor = version >> 16\n\n\tif major == 2:\n\t"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tif dbms and name in dbms and PostGIS_VERSION > (1, 6, 0):\n\t\t\tspatial_version = 9011 # first spatial field\n\t\t\tsrid_tmp = int(srs_py.SpatialReference(srs_py.SpatialReferenceType[srid_map] +'' + str(spatial_version) + ':4326'))\n\t\t\treturn Point(srid=srs_py.SpatialReference(wkt_2d=srs_py.SpatialReference(wkt=fields, srid=srid_tmp)).wkt), srid_tmp\n\t\telse:\n\t\t\treturn None\n\treturn point_"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tif srid_map:\n\t\t\tsrid_list = srid_map.values()\n\t\t\tif not srid_list: raise ValueError('The SRID of the constructor %s '\n\t\t\t\t\t\t\t  'is invalid.' % name)\n\t\t\telse: srid = srid_list[0]\n\t\telse: srid = None\n\n\t\treturn Point(name, srid=srid, spatial_index=False)\n\n\tpoint_type = classmethod( point_type )\n\n\t# The following functions are used to get the units, their name, and\n\t# the spheroid/geodetic column name.  They are used"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\n\t\n\tgeometry_type = geometry_type_map.get(type(name)) # default to Point\n\ttype_name = OGRGeomType(parse_wkt(fields.shape).name).django\n\treturn PointMetaclass(name, (Geometry,), {'srid': srid_map.get(type_name), 'geometry_type': geometry_type})\n\nclass Point(object):\n\t \"\"\"\n\tA Point object represents a single point in a 3D space.\n\t\n\t>>> p1 = Point(0, 0)\n\t>>> p1 == Point(0, 1)\n\tTrue\n\t"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tfields = Point._handle_field_list(fields)\n\t\tPoint._process_srid_info(srid_map, fields, srid_map)\n\tPoint._register_lookup(Point, name, lookup_type)\n\tGeometry.point_type = Point\n\treturn type.__new__(Point)\n\ndef LineString(srid=settings.SRID, **kwargs):\n\t\"\"\"\n\tReturns a list of LinearRing coordinate sequences, from the given\n\tsequence of column names.\n\t\"\"\"\n\tsequence = kwargs.get('sequence')\n\tseq_fmt =''.join"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\ti = name.rfind(\".\")\n\t\tj = name.find(\"ST_GeomFromText('%s')\" % name[:i])\n\t\tif j >= 0:\n\t\t\tgeom_type = name[:i]\n\t\t\tgeom_type += \"_ST_GeomFromText\"\n\t\telse:\n\t\t\tgeom_type = \"Geometry\"\n\n\t\treturn type.__new__(PointType, geom_type)\n\n\tdef from_str(self, value, connection):\n\t\t\"\"\"\n\t\tConvert the string value into a Point.\n\t\t\"\"\"\n\t\tsrid = value.split(\"(\")[1]\n\t\t"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tif name in fields:\n\t\t\tobj = fields[name].point.transform(srid_map)\n\t\t\tobj.transform(srid_id) # all are already transformed, no Transforms\n\t\t\tdel fields[name]\n\t\t\treturn obj\n\t\telif geom_field_name(name) == 'geometry':\n\t\t\t# Point will already have a geometry field.\n\t\t\treturn name\n\n\t\t# The geometry type is simply transformed.\n\t\treturn geom_field_name(name)\n\t# If we get here, the geometry type is internal, with all our fields we\n\t# have, and"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tpt = Point(srid=srid_map[name])\n\t\tfor field in fields:\n\t\t\tpt[field.name] = field\n\n\tpoint_type.__name__ = name\n\treturn point_type\n\nclass GeometryField(models.Field):\n\t\"\"\"A Field that takes another table and transforms it into a Geography.\"\"\"\n\tdef from_db_value(self, value, expression, connection, context):\n\t\tpoint = geo_db_value_to_point(value, expression)\n\t\treturn Point(srid=4326)\n\tto_srid = from_db_"}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\treturn type(\"Point\", (Point,), {\"point\": name,\n\t\t\t\t\t\t\"properties\": fields, \"srid\": srid_map})\n\nPoint = type(\"Point\", (object,), {\"coords\": coordinates,\n\t\t\t\t\t\t \"srid\": 4326,\n\t\t\t\t\t\t \"type\": point_type})\n\n_POINT_TYPES = dict([(t.name, t) for t in get_points()])\n\n\nclass MultiPoint(GeometryCollection):\n\t\"\"\"\n\tThis models a general Multi-point in GEOS.\n\t\"\"\"\n\tdef __init__(self, *args, **kwargs):\n\t\tkwargs[\"regex\"] ="}, {"generated_text": "def point_type(name, fields, srid_map):\t\"\"\"\n\tDynamically create a Point subclass.\t\"\"\"\n\t\tp = Point(name, srid=srid_map[srid])\n\t\tfor name, point in fields:\n\t\t\tp[name] = point\n\t\treturn p\n\t\n\tname = 'geometry'\n\tfields = {\n\t\tIntegerField: IntField,\n\t\tFloatField: FloatField\n\t}\n\nclass PolygonField(GeometryField):\n\t\"\"\"\n\tA dynamic geometry field that uses a polygon.\n\t\"\"\"\n\t\nclass MultiPointField(GeometryField):\n\tdef __init__(self, fields=None):\n\t\tGeometryField.__init__(self)\n\t\tself.srid ="}]}
{"task_id": "62e5dc9ed76274f8a4026b5b", "prompt": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n\t\n\tdef new_func2():\n\t\tpass\n\"\"\"\n\n\ndef deprecated_1(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"1.1 This function is deprecated in favour of 'bar'. Do not use it\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n\n\t\ndef deprecated_2(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"1.2 This function is deprecated in favour of 'bar'. Do not use it\")\n    def old_func2"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n__add_msg = deprecated(\"Deprecated.  'foo' has been \"\n                            \"deprecated in favour of 'bar'\")\n\n# The next line is needed for python 3 compatibility\n__version__ = sys.version[:3]\n\n# The code below can be refactored to read the docstrings.\n__docformat__ = \"restructuredtext en\"\n\nclass _DeprecatedClassProperty(property) :\n\tpass\n\n# A metaclass to add a new _DeprecatedClass property.\n#\n# This will be put on the class level as well, without the\n# underscore, i.e"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\ndef deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo.z.blah(x)\" has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\ndef deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo.z.blah(x)\" has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\ndef noisy_function(x):\n\treturn x * x\n"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n\t\n\tdef deprec(f):\n\t\treturn Deprecated(\"foo %s, bar\" % name, version.version, f)\n\t\t\n\t\n\tdef function(foo, bar):\n\t\t\"\"\"\n\tThis function is deprecated.\n\t\"\"\"\n\t\treturn Deprecated(message)\n\t\n\tdef method(f):\n\t\t\"\"\"\n\tThis method is deprecated.\n\t\"\"\"\n\t\treturn Deprecated(message)\n\t\n\twith warnings.catch_warnings(record=True) as wlist:\n\t\told_filters = warnings.filters[:]\n\t\twarnings.simplefilter(\"always\")\n\t\t\n\t\tdef deprecated_function():\n\t\t"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n\n\ndef foo(a, a1=1, a2=2, a3=3):\n\t\"\"\"\n\tA decorator:\n\t\"\"\"\n\treturn foo(a, a1, a2, a3)\n\ndef alias(fn):\n\t\"\"\"\n\tAlias function.\n\t\"\"\"\n\treturn Alias(fn.__name__, fn.__doc__)\n\ndef _get_doc(func):\n\ttry:\n\t\tdoc = func.__doc__\n\texcept AttributeError:\n\t\traise ValueError(\"Function %r has no docstring.\" % func)\n\treturn doc\n\n\n"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n\n\tdef _decorator(func):\n\t\t\"\"\"\n\t\t@wraps(func)\n\t\tdef foo(*args):\n\t\t\t\"\"\"\n\t\t\tprint(\"Hello \" + func.__name__, args[0], \"bar\")\n\t\t\t\"\"\"\n\t\t\treturn func(*args)\n\treturn _decorator\n\n\"\"\"\n__all__ = ['deprecated', 'deprecated_arg_names', 'deprecated_arg_values']\n__docformat__ = \"restructuredtext en\"\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n        def f(*args, **kwds):\n            \"Help me\"\n            return 10\n        f.deprecated_function(args[0].strip(), \"foo\", \"deprecated\", \"Please change 'foo'\")\n        f.deprecated_function(args[1].strip(), \"foo\", \"deprecated\", \"Please change 'foo'\")\n        f.deprecated_function(1,2,3,4,5,6,7,8,9,9,10,10)\n\t\"\"\"\n\treturn _deprecate(message)\n\ndef _deprecation(message=\"\"):\n\tdef g(fn):\n\t\t@functools"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\t# XXX\n\n\n\"\"\"\nimport warnings\n\nwarnings.warn = warning\n\ndef deprecated_by_category(message):\n\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n\t:param message:\n\t\tMessages on the deprecated methods that\n\t\tsupport multiple categories are removed.\n\n\t:return:\n\t\tNone\n\n\t\"\"\"\n\tdef deprecated_decorator(func):\n\t\t\"\"\"\n\t\tThis will return a decorator that registers a given\n\t\tfunction as a warning.\n\n\t\t:param func:\n\t\t\tThe function to be deprecated\n\t\t:return:\n\t\t\tA decorator function that"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\n        from nose.tools import ok_\n    \"\"\"\n\n    def decorate(func):\n        @wraps(func)\n        def wrapper(*args, **kw):\n            import inspect\n            print \"Warning: Function '{}' is deprecated. Please use 'foo' instead\"\n        return wrapper\n\n\n\n\n\nfrom django.test import TestCase\nfrom django.urls import reverse\nfrom django.contrib.auth.models import User\n\nfrom mock import patch\n\nfrom wagtail.tests.utils import WagtailTestUtils\nfrom wagtail.wagtailcore import hooks\n\nfrom wagtail.tests.testapp.models import EventPage, SimplePage\n\nfrom.fixtures"}, {"generated_text": "def deprecated(message):\t\"\"\"\n\tDecorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\t\"\"\"\n\t\nclass Foo():\t\n\t@deprecated(\"Calling new_foo instead of foo.bar\")\n\tdef new_foo(self):\n\t\tpass\n\n\tdef foo2(self):\n\t\tpass\n\n@deprecated(\"Calling old_foo instead of old_foo.bar.baz\")\nclass Foo2():\t\n\tdef new_foo(self):\n\t\tpass\t\"\"\"\n\n\tdef __init__(self):\n\t\tpass\n\nclass New_foo():\n\tdef __init__(self, name):\n\t\tpass\n\n\tdef old_foo(self):\n\t\tpass\t\"\"\"\n\t"}]}
{"task_id": "62e4fc3c85ea98643089041e", "prompt": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\tif \"RC_OPTIONS\" in os.environ:\n\t\t\tos.environ[\"RC_OPTIONS\"].append(\"-O0\")\n\t\tos.environ[\"RC_OPTIONS\"].append(str(code))\n\t\treturn os.environ[\"RC_OPTIONS\"].get(str(code), \"\")\n\n\tdef _add_compile_flag(code: str) -> str:\n\t\tassert not CODE_FLAGS\n\t\treturn os.environ[\"BUILDING_CC\"] + code + \"-fprofile-generate\"\n\n\tdef _add_glibc(code: str) -> str:\n\t\tif not os."}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\treturn f'\"{code}\"' if isinstance(code, str) else f'(R)({code})'\n\ndef _inline_a_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of A cannot be configured via env variables, but can\nonly be configured via A options once A has started. These are set here.\t\"\"\"\n\t\treturn f'\"{code}\"' if isinstance(code, str) else f'(A)({code})'\n\n\tdef _inline_e_set(code: str, a: str) -> str:\t\"\"\"\n\tSome behaviour of"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\treturn re.sub(\"R\", \"\", code).replace(\"R\", \"\")\n\n\ndef _validate_if_bool_arg(name: str, default: bool) -> Type[bool]:\n\t\"\"\"Validate that the type of the given name arg is 'if_true'.\"\"\"\n\tif not isinstance(name, str):\n\t\traise TypeError(\"Must be `str` or `unicode`\")\n\tif name and name[0]!= \"_\" and name[-1]!= \"_\":\n\t\traise ValueError(\"Name can't start or end with '_'\")\n\treturn default\n\n\ndef _get_config_"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t if len(code)!= 0:\n\t\t\treturn\n\t\tr_options = {\n\t\t\t\"line_height\": line_height,\n\t\t\t\"margin\": margin,\n\t\t\t\"column_widths\": column_widths,\n\t\t\t\"indent\": indent\n\t\t}\n\t\tif r_options[\"max_line_width\"]:\n\t\t\tr_options[\"dash_length\"] = r_options[\"max_line_width\"]\n\t\tr_options[\"code\"] = code\n\t\tlines = []\n\t\tfor line in code.split(\"\\n\"):\n\t\t\tif len(line) == 0:\n\t\t\t\t"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\treturn code+'_r'\n# Copyright \u00a9 2019 Zhaoyu Zhaoy\n#\n# This file is part of Pixelated.\n#\n# Pixelated is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Pixelated is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\tinit_data = code + \"\\n\"\n\t\t# Start and stop our loop. Also initialize an R variable with the\n\t\t# initial expression. This is to ensure that we always evaluate\n\t\t# immediately after the loop.\n\t\tinit_data += r\"INITIAL\"\n\t\tinit_data += \"\\n# loop to evaluate variable at the R prompt\"\n\t\tself.variables.append(init_data)\n\n\t\t# The code after all the R commands are evaluated. This line\n\t\t# should not be commented out.\n\t\tif code_end: return self.variables.pop()\n\t\t"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\tif not code:\n\t\t\treturn ''\n\n\t\tcode.append(r_start)\n\t\tr_code = '\\n'.join(['\t#if \"%s\" set %s\\n' % (name, value) for name, value in R.items() ])\n\t\tr_end = '\\nend\\n'\n\t\treturn '%s%s\\n%s' % (code, r_code, r_end)\n\treturn '\\n'.join(code)\n\ndef r_line(self: object = None) -> str:\n\t'''\n\tThis line can"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\treturn rconf.env_vars(get_r_conf()).get(code)\n\n\ndef _config_setup(sublime_config: SublimeConfig, theme_path: str) -> bool:\n\t\"\"\"\n\tTry to import the plugin config first. If it fails, then set default values to\n\tthe defaults for the plugin.\n\t\"\"\"\n\ttry:\n\t\tfrom openexp._config_override import _default_setup, _deprecated_setup\n\texcept ImportError:\n\t\treturn False\n\n\ttry:\n\t\tfrom openexp._config_override import _override_"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\treturn r\"\"\"\ndefine PP_NUM_THREADS              %(ph)s\n\ndefine PP_MAX_THREADS              %(ph)s\n\n#define PP_USE_MATH_DEFINES      %(pc)s\n\"\"\" % { 'pc': PP_NUM_THREADS }\n\ndef _inline_t_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of T cannot be configured via env variables, but can\nonly be configured via T options once T has started. These are set here.\t\"\"\"\n\t\treturn t\"\"\"\n\n#define"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\t\"\"\"\n\t\treturn code.replace('$','set -e')\n\n\n\tdef _inline_uset_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of U cannot be configured via $vars (via /usr/bin/env).\n\tThis means such a special set command can only be set after setting the value of\n\tenvironment variable.\t\"\"\"\n\t\treturn code.replace('$', r'^set -e\\s*$')\n\n\n\tdef _inline_lset_setup(code: str) -> str:\t\"\"\"\n\tSome behaviour of L cannot be configured via $vars ("}]}
{"task_id": "62e4fbda85ea986430890405", "prompt": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\n\t\n\tp = argparse.ArgumentParser(prog='xargs', description='')\n\tp.add_argument('--tty', help=\"Turn on a tty\", action='store_true')\n\tp.add_argument('--target_concurrency', help='Use as the target_concurrency')\n\tp.add_argument('--no-tty', help=\"Don't use a TTY\")\n\t\n\targs = p.parse_args()\n\targs.tty = not args.tty\n\n\treturn args\n\ndef parse_args(argv):\n\timport argparse\n\tusage = 'xargs <target_concurrency"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\t)\n\tparser.add_argument(\n\t\t\"subsubcommands\",\n\t\t\"Optional: list of subcommands to execute.\",\n\t\taction=\"store_true\",\n\t\tdefault=False\n\t)\n\tparser.add_argument(\n\t\t\"subtarget_concurrency\",\n\t\trequired=True,\n\t\thelp=\"The number of parallel subcommands to run in parallel (exceeds up to 100-20).\",\n\t\ttype=int,\n\t\tdefault=-1\n\t)\n\tparser.add_argument(\n\t\t\"interval\",\n\t\ttype=int,\n\t\tdefault"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\t\"The number of parallel connections to use for connections/threads/processes\"\n\t\tpass\n\n\t# The default time spec is 60 so that we have to use xargs instead!\n\tdef xargs_DEFAULT(ctx):\n\t\t\"Return True if running in default time mode\"\n\t\treturn True\n\n\ttarget_interval = 3*60\n\t\n\tparser = argparse.ArgumentParser(description=\"A simplified implementation of xargs. Requires Python 2.7+\"\n\t\t\t\t\t\t\t\t\"and up. If you don't want to use xargs, use argparse.SUPPRESS instead!\")\n\n\tparser.add_argument(\n\t\t"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\t)\n\tparser.add_argument (\"mode\",\n\t\thelp=\"Target mode\")\n\tparser.add_argument (\"path\",\n\t\thelp=\"Target path for executables\")\n\n\targs = parser.parse_args()\n\n\ttry:\n\t\tif args.mode == \"local\":\n\t\t\tcmd = \"/bin/bash\"\n\t\telse:\n\t\t\tif not subprocess.getstatusoutput(\"which \" + args.path)[0] == 0:\n\t\t\t\tcmd = \"/bin/bash\"\n\n\t\tret = os.system(cmd)\n\texcept:\n\t\traise Exception(\"Not supported shell"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\t.format(config[\"target_concurrency\"][\"name\"])\n\tif config[\"color\"] == \"1\":\n\t\treturn \"screen -D {target}[{size}] {command} {env}\".format(\n\t\t\ttarget=config[\"target\"],\n\t\t\tsize = config[\"size\"],\n\t\t\tcommand = config[\"command\"],\n\t\t\tenv = config[\"env\"],\n\t\t)\n\telse:\n\t\treturn \"screen -D {target} {command} {env} {size}\".format(\n\t\t\ttarget=config[\"target\"],\n\t\t\tcommand=config[\"command\"],\n\t\t\tenv = config[\"env\"],"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\t\"Useful for cgroups to ensure that no one running may be running\"\n\n\tmy_env = ['PATH=']\n\tmy_env\t= \"PATH=/usr/local/bin:$PATH\" \n\tmy_env_post = \"PATH=/sbin:/usr/bin:/bin:/usr/sbin:/usr/local/bin\" \n\n\t#my_env = {}\n\t#my_env[\"PATH\"]=pathlib.Path()\n\n\tmy_env_file = os.path.join(os.path.dirname(os.path.realpath(__file__)),\"my"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\nclass Focus(Job):\n\t\"Focus job\"\n\tphase = phases.preprocessorphase(\"focus\")\n\t\n\tdef init(self):\n\t\tself.job.require_gcc()\n\n\n\tdef xargs(self):\n\t\tname = \"\"\"\n\tThe name of the job.\nThis job was called by %s\"\"\" % \\\n\t\tself.tool[\"jobs\"][self.name][\"path\"]\n\t\treturn self.job.path.find_or_declare(name)\n\n\n\tdef setup(self):\n\t\tself.error = \"\"\n\t\tself.tool[\"tool_path"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\t# Override with actual_max_running_time from environment if not already set\n\t\tif 'TRAVIS' in os.environ:\n\t\t\ttarget_concurrency = tuple(string.strip(v) for v in os.environ['TRAVIS'].split(','))\n\treturn''.join(args.format(target_concurrency=(args.target,)+args.target_args))\ndef xargs_passthrough(x,y,z=True):\n\t\"\"\"Passthrough shell command.\n\n\tPassthrough shell command so that shell variables have higher priority than others.\"\"\"\n\treturn"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\treturn ['env', 'MAKEHIDE']\n\nclass Test:\n\t\"\"\"\n\tSimple unittest for use as a script.\n\n\t@ivar env: The env instance\n\t@ivar log: Log object used for testing\n\t@ivar str source: Path to source file to be tested\n\t@ivar str target: Path to target file to be tested with a script\n\t@ivar list tests: List containing tuples (name, success?, message?,\n\tenvironment) used by the Test instance to display output to the user\n\t\n\t@ivar int timeout: Wait for test to run before failing\n\t@"}, {"generated_text": "def xargs(\t\"\"\"\n\tA simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently\t\"\"\"\n\t\nclass TestCLI(unittest.TestCase):\n    @classmethod\n    def setUpClass(self):\n        self.stream = FakeStream(stream=LineFeed(), target_type=\"serial\", name='pipe')\n        self.cmd = Command(args=xargs(\"foo\"))\n        self.cmd()\n\n    def tearDown(self):\n        self.stream.close()\n\n    def cmd(self):\n        if self.cmd.process =='stdin':\n            return self.cmd.communicate('echo $?\\n')\n\n    def test_cmd_returns_no_output(self):\n        self.assertIs(self.cmd(), None)"}]}
{"task_id": "62e4fbda85ea986430890403", "prompt": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\tif _is_list(seq):\n\t\t\tseq = [ord(i) for i in _sorted(seq)]\n\t\tn = len(seq)\n\t\ttmp_list = list(seq)\n\t\trandom.shuffle(tmp_list)\n\t\ti = random.randint(0,n)\n\t\ttmp_list[i] = seq[i]\n\t\treturn tmp_list\n\n\t@staticmethod\n\tdef shuffle_sequence(seq: Sequence[str], n: int=1) -> str:\t\"\"\"\n\t\tShuffle an input sequence in place according to the n and the number"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\treturn _shuffle(seq, len(seq) - len(os.path.sep) - 1)\n\n\ndef unshuffled(seq: Sequence[str]) -> str:\n\t\"\"\"\n\tUnshuffled string\n\t\"\"\"\n\treturn ''.join(s[1:] for s in seq if s.isalpha())\n\n\ndef unshuffle(seq: Sequence[str], n: int) -> str:\n\t\"\"\"\n\tReturns shuffled string\n\t\"\"\"\n\treturn seq[n:] + '\\0' * (n - len(seq))\n\n\ndef _"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\treturn [shuffled(item, random_list=random_list) for item in seq]\n\n\ndef shuf(seq):\n\t\"\"\"\n\tshuffle sequence while keeping the same length\n\t\n\tThis function is equivalent to `shuff(seq)` but shuffles\n\titerables.  It will not shuffle the sequence itself nor\n\tit will cause errors as ``RuntimeError`` exceptions.  As an\n\texpected behavior, it's recommended that shuffling may create\n\tidentical sequences from them or may not be as deep as the original\n\tseq.  In cases where the shuffling fails you"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\t# list of lists of indices\n\t\tindices = list()\n\n\t\t# randomly shuffle that sequence, so length is never > 0\n\t\tfor ind in seq:\n\t\t\tseq[ind] = (random.randrange(0, len(seq[ind]) - 1)) \\\n\t\t\t              if ind in indices else seq[ind]\n\n\t\t# replace the current random number with the desired number that is\n\t\t# chosen so that no more random indexes are chosen over for other\n\t\tfor i in range(0, len(seq) - 1):\n\t\t\trandom_num = random.random()\n\n\t\t\t# replace"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\tif isinstance(seq, str):\n\t\t\treturn seq.split(_shuffled(seq))\n\t\telse:\n\t\t\treturn list(seq)\n\n\t@staticmethod\t\n\tdef _create_random_token(bits: int) -> str:\n\t\ttoken_chars = set(ascii_lowercase + digits + digits_minus)\n\t\tchar_list = list(reversed(list(ascii_lowercase + digits_minus + digits)))\n\t\ttoken = ''.join(random.choice(token_chars) for _ in range(bits))\n\t\treturn token\n\n\t@staticmethod\n\tdef _"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\tret = []\n\t\tsort_key = sorted(seq)\n\t\twhile len(ret) < args.min_shuffle_size and len(ret) > 0:\n\t\t\tidx = random.randint(0, len(ret) - 1)\n\t\t\tret[idx] = _random_shuffle(seq, idx, args.min_shuffle_size)\n\t\t\tidx_i = random.randint(0, len(ret) - 1)\n\t\t\tret[idx_i] = _random_shuffle(ret[idx_i], idx_i, args.min_shuffle_size"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\treturn [seq[i] for i in range(len(seq)) if len(seq[i]) > 1]\n\n@functools.lru_cache(maxsize=8)\ndef _get_ngrams_and_counts():\n\t\"\"\"\n\tGenerate counts from a string of ngrams\n\tUse the Huffman tree to generate counts for a text corpus\n\t\"\"\"\n\tdef ngrams_and_counts2txt(ngrams: List[str]) -> str:\n\t\t\"\"\"\n\t\tGenerate counts from a list of ngrams. \n\t\tReturns a list of ngram string\n\t"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\treturn list(seq)shuffle(seq)\n\n\n# NOTE, this should probably be a class instead of the factory, but here is\n# for the moment it is a good way to get a list of these\n# (for example, to make sure we're not getting called twice by my_module.py\n# but still I don't know what is going on, since everyone who's calling this\n# should probably get its own error - if we don't even care of that though!)\n\n\ndef _next_value(seq: Sequence[str],\n               default: str = str) -> str"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\tr = random.shuffle(seq)\n\t\treturn r\n\n\treturn generate_from_str(source, target, method=method, seed=_shuffled)\n\ndef _reverse_sequence(input_seq: str, reversed_seq: str, new_input_seq: str) -> str:\n\t\"\"\"\n\tShuffles a given sequence\n\t\"\"\"\n\treturn input_seq[::-1] + input_seq[:1][::-1] + reverse_seq + input_seq[1:]\n\n@click.command()\n@click.option(\"--length\", \"-m"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\t\"\"\"\n\tDeterministically shuffle\t\"\"\"\n\t\tnew_seq = list(seq)\n\t\tnew_seq.extend(random.sample(range(1, len(seq) + 1), random.random_sample(list(seq).count(), random.random_sample(list(seq).count(), random.random_sample(1)[0])))\n\t\t# print(len(seq))\n\t\treturn new_seq\n\n# def _shuffle(seq: Sequence[str]) -> list[str]:\t\"Shuffles the list and returns the input\"\n# \treturn list(set(seq))\n\ndef shuffle(seq"}]}
{"task_id": "62e4fb6585ea98643089032b", "prompt": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tmatch = s.replace('v', '')\n\t\texp = re.split(':', match)\n\t\tmajor = int(exp[0][0])\n\t\tminor = int(exp[0][1])\n\t\tbuild = int(exp[0][2])\n\t\ttimestamp = datetime.datetime.strptime(exp[-1][2], \"%Y-%m-%dT%H:%M:%S\")\n\t\treturn major, minor, build, timestamp\n\n\tdef _getLatestCommit(self, file_name):\n\t\t\"\"\"\n\t\tGet latest git commit\n\n\t\t:return:\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\ts_match = r'^(\\d+\\.?\\d*)\\s+(\\d+)\\.\\s+([a-z-]+)'\n\t\tmatch = re.match(s_match, s)\n\t\tassert match, f'wrong version format: {s}'\n\t\tversion = match.group(1) + match.group(2)\n\t\treturn tuple([int(num) for num in version.split(\".\")])\n\treturn ()\n\ndef get_version_file_path(s: str) -> str:\n\t\"\"\"\n\tpoor man's version compare\t\"\"\"\n\treturn '.'."}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tv_p = None\n\t\tif s.startswith('v'):\n\t\t\tv_p = int(s[2:-1])\n\t\telif s.startswith('v*') and s.count(':') is 1:\t# int(s[p]) may be '2.0' or '1.0'\n\t\t\tv_p = int(s[2:])\n\t\telse:\n\t\t\tif '-' in s:\t# short version like \"v0.0.00.dev0\"\n\t\t\t\tv_p = int(s)\n\t\t\telif 'v0.0.0' in"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\n\t\n\t# version regex\n\twith version_regex() as v:\n\t\tm = g.match(s)\n\t\treturn (m.group(1).lower(), int(m.group(2)) < version)\n\t\ndef parse_version_cmp(v1, v2):\n\t\"\"\"\n\treturns version in version_match format\n\t\"\"\"\n\treturn reduce(lambda a,b: parse_version(b), range(v1,v2))\n\t\n\t\nimport collections\nimport contextlib\nimport inspect\nfrom datetime import datetime\nimport os.path\nimport sys\nfrom"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tresult = v.__version__\n\t\tl = [\n\t\t\tresult[1],\n\t\t\tresult[2],\n\t\t\tresult[3],\n\t\t\tresult[4],\n\t\t\tresult[5],\n\t\t\tresult[6],\n\t\t\tresult[7],\n\t\t]\n\t\treturn l\n\n\tdef __lt__(self, other) -> bool:\n\t\treturn self.__cmp(other) < 0\n\n\tdef __gt__(self, other) -> bool:\n\t\treturn self.__cmp(other) > 0\n\n\tdef __eq__(self, other) -> bool:\n\t\treturn self.__cmp"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\t# remove one char\n\t\ts = s.strip()\n\t\tm = re.match(r\"^([0-9]{4})\", s)\n\t\tif m:\n\t\t\treturn StrictVersion(int(m.group(len(m.group(0)))))\n\t\telse:\n\t\t\treturn StrictVersion(1)\n\n\nclass RevOptionsPlugin(Plugin):\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper().__init__(\"RevOptions\", description=__doc__)\n\n\tdef handle_args(self, **kwargs) -> None:\n\t\ts = list"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\timport re\n\t\tm = re.search(r\"^(.+?)\\.\", s)\n\t\tif not m:\n\t\t\treturn (0, 0)\n\t\telse:\n\t\t\treturn (int(m.group(1)), int(m.group(2)))\n\treturn gnuver_version.match(s).groups()\n\n#\n# Functions for extracting the version info from a git repo (it sets all\n# information extracted)\n#\n\ndef git_describe(git_repo: str, rev: str) -> str:\n\t\"\"\"\n\treturns the version (int),"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tversion = list(\n\t\t\tmap(int, s.split('.')[:3]))\n\t\treturn version\n\n\t# TODO: handle multiple version numbers as if they are already stored as floats?\n\tdef parse_version_number(s: str, allow_overflow = False) -> tuple[int,...]:\n\t\t\"\"\"\n\t\tpoor man's version comparison\t\"\"\"\n\t\tversion = list(\n\t\t\tmap(lambda m: 0 if len(m.split('-')) else -1, s.split('.')[:3]))\n\t\tfor x, y in zip(version, version[1:]):\n\t\t"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tmatch = re.match(r'^VERSION_ID=\"(?P<release_version>.+?)\"', s)\n\t\tif match is not None:\n\t\t\treturn match.groupdict()['release_version']\n\t\treturn None\n\n\"\"\"\t\n\tParses all current release infos\n\"\"\"\nclass Revision:\n\tdef __init__(self, timestamp=None, version):\n\t\t\"\"\"\n\t\tParse relevant revisions from a string\n\t\t\"\"\"\n\t\tparts = revision_regex.match(version)\n\t\tself.version = (parts.group('release_version'), int(parts.group('revision')))\n\t\t"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\t\"\"\"\n\tpoor man's version comparison\t\"\"\"\n\t\tmatch = re.match(VERSION_REGEX, s)\n\t\tif not match:\n\t\t\traise Exception('invalid version format: \"{0}\"'.format(s))\n\t\t\t# raise Exception('invalid version format \"{0}\"'.format(s))\n\n\t\tmajor, minor, patch = match.groups()\n\t\tif patches:\n\t\t\tmajor, minor, patch = tuple(map(int, (major, minor, patch)))\n\t\telif patch!= '':\n\t\t\tmajor, minor, patch = None, None\n\t\treturn major, minor, patch\n\n\tdef is_valid(v: str) ->"}]}
{"task_id": "62e4fb4d85ea9864308902e7", "prompt": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\n\t\n\tcmd[0] = string.replace(cmd[0], \"[\", \"\").replace(cmd[0], \"]\").strip()\n\tcmd[0] = string.replace(cmd[0], \"]\", \"\").replace(cmd[0], \"'\").strip()\n\n\tcmd = string.replace(cmd)\n\n\tif len(cmd) > 0 and cmd[0].strip() in string.hexdigits:\n\t\tcmd = string.replace(cmd)\n\t\t\n\n\treturn (cmd[0], cmd[1])\n\n\ndef generate_cmd(cmd: tuple[str,...] = ()):"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\n\t\n\t# TODO: this should be a generator\n\t# TODO: make a separate version of this method\n\t# TODO: how to make this work?\n\treturn cmd\n\n\n# TODO: handle these\ndef normalize_name(name: str, default_replace=False) -> str:\n\t\"\"\"\n\tNormalizes a string to make it lowercase\n\t:param name: a string to make it lowercase\n\t:param default_replace: if True replace the beginning of the string by a \"_\"\n\t\"\"\"\n\t# TODO: this should be a generator\n\t# TODO: make a separate"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\n      \tcmdStr = \" \".join(x.strip().lower() for x in cmd)\n         \n      \tcommand = \"\"\n\t  \tfor shellQuote in \"'\\\"\"\n\t\tfor cmd in cmdStr:\n        \t\tcommand = command.replace(shellQuote,\"\")\n        \t\tcommand = command.replace(\" \",\"\")\n\t \t\tcommand = command.strip()\n\n\t  \tfor i in range(1,len(command)-1):\n        \t                command = command.replace(\"#\"+str(i), command[i])\n\n      \t\tif not(re.match(\"[a-z_]*\", command)):"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t return tuple(map(normalize_cmd, cmd))\n\ndef remove_comment(text: str) -> str:\n\t\"\"\"\n\tremoves all comments from a string\n\t:param text:\n\t:return:\n\t\"\"\"\n\treturn str(text)\n\ndef strip_comment(text: str) -> str:\n\t'''\n\tStrip all whitespace off the lines from a string.\n\t:return:\n\t'''\n\treturn str(text).strip(' \\t\\n\\r')\n\ndef remove_braces(text: str) -> str:\n\t"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t # Fixes to ignore invalid path errors\n\tif len(cmd) < 4:\n\t\treturn None\n\n\t# Want to detect if the path isn't absolute\n\tnorm_path = os.path.normpath(cmd[1])\n\n\t# Convert windows to shebangs\n\tif norm_path.startswith(\"\\\\\") or not norm_path.startswith(\"//\"):\n\t\tnorm_path = norm_path[1:]\n\n\t# Check the path is absolute\n\tif not norm_path.startswith(\"/\"):\n\t\treturn norm_path\n\n\treturn norm_path\n#\n# Copyright"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\tret = []\n\t\tfor c in cmd:\n\t\t\tsplit_c = c.lower().split('_')\n\t\t\tif len(split_c) > 1 and split_c[0] in ('posix', 'nt'):\n\t\t\t\tret.append(split_c[1] + '-' + split_c[1] + '-x')\n\t\t\telse:\n\t\t\t\tret.append(split_c)\n\t\treturn ret\n\treturn cmd\n\ndef set_color(color_str: str) -> None:\n\tcolors = [x.strip() for x in COLORS]\n\t"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\treturn tuple(re.split(\"(\\-+|\\\\|/\\/+|\\\\|/|\\\\|/|\\\\#|;|::)+\", cmd))\n\ndef get_cmd(parts):\n\t\"\"\"\n\tExtracts the string given a list of space-separated parameters.\n\t@param parts: a space-separated string\n\t@type parts: string\n\t@return: two string that can be read from the given parts\n\t@rtype: string/string\n\t\"\"\"\n\tdef repl(m):\n\t\ti = 0\n\t\twhile i < len(m):\n\t\t\tif m[i"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\n########################################################################################################################\ndef make_cmd(cmd: tuple[str,...]) -> tuple[str,...]:\n    \"\"\"\n    Takes a string as returned by the user's terminal, then checks for its \n    presence.\n    Returns:\n    ( cmd, out, err ) -- cmd is the string to replace\n    out -- out is the output of the command\n    err -- err is the error string returned by the command. It is recommended\n    that you call this function outside to process the command, as it\n    never terminates.\n    raises error on errors\n    \"\"\"\n#    print(\"trying to call {}\".format(cmd))"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\n    new_cmd = list(reversed(cmd))\n    old_cmd = list(new_cmd)\n    new_cmd[1:0] = old_cmd[1:0]\n    return tuple(new_cmd)\n\ndef _gentempfilename():\n    \"\"\" Generates random tempfile and removes it \"\"\"\n    tmp_fd, tmp_name = tempfile.mkstemp()\n    os.close(tmp_fd)\n    os.remove(tmp_name)\n    return tmp_name\n\ndef _gen_filename():\n    \"\"\" Generates filename based on the operating system\n\n    If \"posix\" is true, then"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\t\"\"\"\n\tFixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine\t\"\"\"\n\t\tif not cmd: return '', {}\n\t\telse:\n\t\t\treturn tuple(cmd[2:])\n\n\ttry:\n\t\t# if it starts with 2 - assume it's an absolute path\n\t\tif cmd[1].startswith('\\\\\\\\'): return cmd[1] # remove the leading '\\\\'\n\t\t# it may be a relative path\n\t\telse:\n\t\t\tcmd[1] = '/' + cmd[1]\n\texcept Exception as e:\n\t\treturn \"\", {}\n\t\n\treturn normalize_path(cmd[0]), cmd[1]\n\ndef parse_cmd(cmd: (str)) -> ("}]}
{"task_id": "62b8d27a48ba5a41d1c3f4c6", "prompt": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\tdef memoized(func):\n\t\t\tdef hashfn(*args, **kwargs):\n\t\t\t\tif not cache or key not in cache:\n\t\t\t\t\tif lock:\n\t\t\t\t\t\tlock.acquire()\n\t\t\t\t\ttry:\n\t\t\t\t\t\tif key in cache:\n\t\t\t\t\t\t\treturn cache[key]\n\t\t\t\t\t\tresult = cache[key]\n\t\t\t\t\t\tresult.append(func(*args, **kwargs))\n\t\t\t\t\t\treturn result\n\t\t\t\t\tfinally:\n\t\t\t\t\t\tlock.release()\n\t\t\tkey = hashfn.__name__\n\t\t\tif isinstance(key, tuple):\n\t\t\t\tident = key[1]\n\t\t\telse:\n\t\t\t\tident = key"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t \n\tdef _dec(func):\n\t\tdef _wrapper(*args, **kwargs):\n\t\t\tkey = key(*args, **kwargs)\n\t\t\tif key not in cache:\n\t\t\t\tres = func(*args, **kwargs)\n\t\t\t\tcache[key] = res\n\t\t\t\treturn res\n\t\t\treturn cache[key]\n\t\treturn _wrapper\n\treturn cached\ndef check_throttle(seconds=3, timeout=0): \n    \"\"\" \n    Decorator for function that checks a function's counter with a rate limit. \n\n    Pass in the value in the counter parameter to the wrapped function. \n    Timeout defaults to zero."}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\n\n\t__hash__ = None\n\t__eq__ = True\n\t_memoize = staticmethod(memoize)\n\n\tdef __init__(self, func, doc=None):\n\t\tself.cache_version = self.func.__dict__.get(\"cache_version\", None) or \\\n\t\t\t\t\t\t\t\t\t  self._memoize(func)\n\t\tself.func = func\n\t\tself.doc = doc or func.__doc__\n\t\tself.lock = lock\n\t\n\tdef _memoize(self, func):\n\t\tdef decorator(fun):\n\t\t\tdef inner(*args, **kwargs):\n\t\t\t"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\n    \t@wraps(cached)\n    \tdef decorator(f):\n\n    \t\tcache = DictCache(cache)\n    \t\tkey = (key or hashlib_md5(f.__module__ + '.' + f.__name__).hexdigest())\n    \t\n    \t\n    \tdef do_caching(*args, **kwargs):\t\n    \t\t# We want to prevent things like django in local settings in particular.\n    \t\tif kwargs.get('use_local_cache', True):\n        \t\t\traise Exception(\"Cannot cache function %s, as it was created using Django. \"\n        \t\t\t\t\"If you need to enable cache,"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\ndef memoized(cache, key=hashkey):\n\t\"\"\"\n\tDecorator to wrap a function with a cache that does not save\nresults in a cache.\n\t\"\"\"\n\treturn cache.cache.set(key, _createCacheResult(cache, key=key))\n\ndef register_to_builtins(names):\n\t\"\"\"\n\tRegister a decorator that allows functions to be executed with only\n\tthe named callable when it is first used. Registering a function to be\n\texecuted with only the named function will still change the cache.\n\t\"\"\"\n\tnames = [_createCache"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\tdef _inner(f):\n\t\t\t@wraps(f)\n\t\t\tdef wrapped_f(*args, **kwargs):\n\t\t\t\tcache = getattr(current_app.cache, f.__name__)\n\t\t\t\tkey = (f.__name__, hashkey) + args + tuple(sorted(kwargs.items()))\n\t\t\t\tif lock is not None:\n\t\t\t\t\t# TODO: cache[key] = data\n\t\t\t\t\tcurrent_app.cache.set(key, dict(data=cache, ttl=CACHE_TTL))\n\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\tglobal HASH_CACHE\n\t\t\t\t"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t \n\tdef memoizer(func):\n\t\tdef decorated_function(*args, **kwargs):\n\t\t\tif cache and not key in cache.__dict__:\n\t\t\t\treturn cache.__dict__[key]\n\n\t\t\tresult = cache.__dict__[func.__name__] = func(*args, **kwargs)\n\t\t\tcache.__dict__[func.__name__] = result\n\n\t\t\t# we are caching here to ensure no change was made\n\t\t\t# we reexecute the cache so that subsequent code changes to\n\t\t\t# the same instance do not affect the cache being\n\t\t\t# used.\n\t\t\t# the two lines would be unprobed"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t\n##\n# Function decorator used in cache framework.\n#\n# The function decorator must have a __doc__ attribute that\n# calls the function with no arguments.\n#\n# Copyright 2007--2014, Oliver White\n\ndef cachedmethod(func):\n\tdef inner(*args):\n\t\tcached = cache.get(key)\t\n\t\tif cached is None:\n\t\t\tresult = func(*args)\n\t\t\tcached = True\n\t\telif (time.time() - cached[1]) >= cache.validity_cache_timeout:\n\t\t\tresult = func(*args)\n\t\t\tcached"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t return MemoizedFunc(cache, key=key, lock=lock)\n\treturn cached\n\n\nclass MemoizedFunc(object):\n\t\"\"\"\n\tMemoized version of wraps around functions with caching.\n\t\"\"\"\n\tdef __init__(self, cache, key=hashkey, lock=None):\n\t\tself.cache = cache\n\t\tself.key = key\n\t\tif lock is None:\n\t\t\tself.lock = RLock()\n\t\telse:\n\t\t\tself.lock = lock\n\t\tself.cachekeys = []\n\t\tself.lock.acquire()\n\t\t try"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nresults in a cache.\t\"\"\"\n\t \n\tdef run_if_changed(cache, decorator=None, **params):\n\t\tif decorator is None: decorator = _create_decorator(cache)\n\t\tcached_result = get(cachekey)\n\t\tif cached_result is not None and cached_result is not cache.disabled:\n\t\t\t# We have a cached version of this request.  Cache it.\n\t\t\t# This is only used if the cache is actually changed since we\n\t\t\t# just started fetching from it and the request\n\t\t\t# got redirected to a new version of the cached response.\n\t\t\treturn decorator(cached_result"}]}
{"task_id": "62b8d24048ba5a41d1c3f49f", "prompt": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t    \n\tdef _wrapped(func):\n\t\tCACHE[name] = func\n\t\treturn func\n\treturn _wrapped\n\ndef ttl_cache_with_timeit(func, timeout, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `timeout` results based on a Least Recently Used (LRU) in\n\ttime-to-live (TTL) time in the past.  The lentage is added to\n\tthe cache if it does not expire within `timeout` seconds of\n\tthe execution of the"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\tdef cache_func_wrapper(user_id, item_id, timestamp=None):\n\t\t\tif not typed:\n\t\t\t\treturn item_id\n\t\t\telif settings.get('TIMELINE_ENABLED', False):\n\t\t\t\t# Avoid the timing cost of get_item_id and get_user_id because\n\t\t\t\t# we are already checking the cache in our own threadlocal\n\t\t\t\t# cache dictionary. This is here as a safety measure, in\n\t\t\t\t# theory it is okay to ignore it when the data comes\n\t\t\t\t# from another process on the server.\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\t@wraps(func)\n\t\tdef cache(self, *args, **kwargs):\n\t\t\tif self.ttl >= ttl:\n\t\t\t\treturn func(self, *args, **kwargs)\t\t\t\n\t\t\tkey = functools.partial(self._key, self, *args, **kwargs)\n\t\t\tvalue = cache_module.hget(key, key)\n\t\t\ttime = time.monotonic()\n\t\t\treturn value if typed else time\t\n\t\tcache.cache = cache(maxsize, ttl, timer)\n\t\tcache.cache_info = cache_info  # For caching of functions\n\t\t"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\ndef cache_clear():\n\t\"\"\"\n\tDecorator to return a function with a memoizing callable that ignores\ncache entries and resets the timeout cache.\n\t\"\"\"\n\tdef wrap(f):\n\t\t@wraps(f)\n\t\tdef cache_clear():\n\t\t\texpires = int(time.time()) + ttl\n\t\t\tif not isinstance(expires, int):\n\t\t\t\t# Not a number, it's a number\n\t\t\t\treturn lambda f=f: f()\n\n\t\t\ttimer = time.monotonic() + expires\n\t\t\t\n\t\t\t# If expiration time is less than the specified amount,\n\t\t\t# use"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t    \"\"\"\n\t    Make sure the function is only called once.\n\t    If ``typed`` is set, the result is treated like a Python generator\n\t    object.\n\t    \"\"\"\n\t    cache_item = func.im_self\n\t\t    is_typing = callable(cache_item)\n\t    return (cache_item if is_typing else cache_item.im_func)\n\t\n\tobj.ttl_cache = ttl_cache\n\t\n\tif is_typing:\n\t    @wraps(func)\n\t    def wrapper(o_self):\n\t    \treturn func(get_context(), o_self)\n\t    return"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\tdef deco(f):\n\t\t\tcache = {}\n\t\t\ttime_from_last_access = None\n\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\tt0 = time.monotonic()\n\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\tcache[t0] = time_from_last_access\n\t\t\t\treturn result\n\n\t\t\tdeco.cache = wrapper\n\t\t\tdeco.ttl = ttl\n\t\t\tdeco.timer = timer\n\t\t\tdeco.typed = typed\n\t\t\treturn deco\n\t\tdef decorator(f):\n\t\t\tttl = deco(f) or ttl"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\ndef _decorate(target):\n\tdef decorator(target):\n\t\t\"\"\"\n\t\tDecorator which applies a function to a target class\n\t\tand adds a _decoration_ to the decorated function\n\t\t\"\"\"\n\t\t\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttarget._decoration_ = target.__doc__\n\t\t\ttarget._decoration_id = target.__name__\n\t\t\ttarget._decoration_repr = target.__repr__\n\t\t\t\n\t\t\ttarget._decoration_args = args\n\t\t\ttarget._decoration_kw = kwargs\n\t\t\n\t\t\treturn target(*args, **kwargs)\n\t\t\n\t\treturn wrapper"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\tif ttl == 0:\n\t\t\treturn ttl_cache_default\n\t\tif ttl > 9999:\n\t\t\tttl = int(ttl*1000)\n\t\t\ttmp = __cache__.get(ttl)\n\t\t\tif tmp is None:\n\t\t\t\ttmp = object()\n\t\t\t\t__cache__[ttl] = tmp\n\n\t\t\ttmp.ttl = lambda : ttl_cache_ttl(tmp)\n\t\telse:\n\t\t\tttl_cache_default = ttl\n\t\t\t__cache__[ttl] = ttl_cache_ttl(timeout=ttl)\n\n\t\tdef new_func(*args, **kwargs):\n\t\t\tnow ="}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\n\n\t# a class decorator\n\t# NOTE: this is really just a convenience function -- if you just put\n\t# the thing onto the function with the parameters.\n\tdef do_ttl_cache(ttl_cache):\n\t\tpass\n\n\t# a \"with\" decorator (like get_cache_status), see the docstring\n\n\tdef cacheable(f):\n                \"\"\"Use this decorator to cache decorated functions which take\n\t\t\t\ttime, and cache it. This is very similar to the decorator\n\t\t\t\tof get_cache_status() below.\"\"\"\n\n                def __wrapped_f(self, *args, **"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\t\"\"\"\n\t\n\n\tdef decorator(f):\n\n\tdef inner_decorator(f1):\n\n\t\t@functools.wraps(f1)\n\t\tdef f2(*args, **kwargs):\n\n\t\t\t# decorator is called when there is no args supplied\n\t\t\tif not args:\n\t\t\t\targs = ['']\n\n\t\t\tkwargs.update(kwargs)\n\n\t\t\tttl_cache = TTLCache(maxsize=maxsize, ttl=ttl)\n\n\t\t\titem_value = f1(*args, **kwargs)\n\n\t\t\treturn ttl_cache.cache_wrapper(item_value)\n\n\t\treturn inner_decorator\n\n#===============================================================================\n# cache"}]}
{"task_id": "62b8d23b48ba5a41d1c3f49a", "prompt": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\n\n\tdef memoize(func):\n\t\t\"\"\"decorator to wrap a function with a memoizing callable that saves\nup to `maxsize`.\n\n\t\tUseful for memoization, to memoize classes.\n\n\t\tNote that if `typed` is True, caching takes a Python dict and is\n\t\tassumed to be called.\n\t\t\"\"\"\n\t\tcache = {}\n\t\t# This decorator is needed if the callable has a 'cache_info' attribute.\n\t\t# Otherwise we use the function's memoize method directly.\n\t\t# __wrapped__ allows for taking of any memoize method to\n\t\t# use"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\ndef memusage(key, limit=LARGE, clock=None):\n\t\"\"\"\n\tDecorator to decorate a function with a memoizing callable that returns\n\ta tuple with the result.\n\t\"\"\"\n\tdef memusage_cache(limit=LARGE):\n\t\t\"\"\"\n\tDecorator to decorate a function with a memoizing callable that keeps\n\tcalls to `limit` storage.\n\t\"\"\"\n\t\n\t\n\tdecorate_cache = (key, memcached, wrap_memcached, limit)\n\tdecorate_memcached = (key, memcached, wrap_memcached, limit)\n\n\t"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t \n\t@wraps(mru_cache)\n\tdef mru_cache_wrapped(self, maxsize=128, typed=False):\t\"\"\"\n\tWraps a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t \n\t@cached_method()\n\tdef calculate_unique_key(self, name):\t\"\"\"\n\tReturns a cache-compatible key function that can be used for\n\tcaches by cache_with_key(), which is a key for caching\n\tthe given arguments:"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t      \n\tif maxsize is not None:\n\t\twrap_mru(wrapped_func, maxsize, typed)\n\telse:\n\t\twrap_simple(wrapped_func, maxsize, typed)\n\t\t\t\ndef wrap_simple(wrapped_func, maxsize, typed=False):\n\t\"\"\"\n\tWraps a function with a memoizing callable that makes sure the\n\tcallable's results are the same - this could be an expensive operation\n\ton CPython.\n\t\"\"\"\n\tif hasattr(wrapped_func, '__web_cache__'):\n\t\twrapped_func.__web_cache__ = WeakKeyDictionary"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\n#\n# decorator wrapper for methods based on a decorator (or is not)\n#\n@decorator\ndef check_cache(cache, ttl=60*60*2, min_compress_len=0):\t\n\t\"\"\"\n\tDecorator to check cache for recent \"valid\" version of a function. \n\tWhen called with `min_compress_len=-1`, a decorator wrapped with\n\t:class:`Memoize`, :class:`MemoizeError` if there's no cache.\n\tEquivalent to:\n\t\n\t.. code-block:: python\n\t\t@memoized \n\t\t"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\nclass memoized(object):\n    \"\"\"\n    Decorator to cache a function's return value each time it is called.\n    If called later with the same arguments, the cached value is returned, and\n    not re-evaluated. This is useful when memoization is useful to avoid\n    re-evaluation of unnecessarily-rewritten function arguments.\n    \"\"\"\n    def __init__(self, func):\n        self.func = func\n        self._cache = {}\n\n    def __call__(self, *args):\n        if not isinstance(args, list):\n            return self.func(*args)\n        if args in self._cache"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t \n\t# if typed is set to true, no caching is used and _CACHE_ON_CALL\n\tdef cache_on_call(func):\n\t\t\"\"\"\n\t\tWraps a function with memoizing callable that saves\n\t\tup to `maxsize` results based on a Most Recently Used (MRU)\n\t\talgorithm.\n\t\t\"\"\"\n\t\t_CACHE_ON_CALL.add(cache_fn(func))\n\t\t\t\n\tdef _wrap(f):\n\t\t@functools.wraps(f)\n\t\tdef f_with_cache(*args, **kwargs):\n\t\t\t# don't"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\n\n\t@decorator\n\tdef memoize(wrapped):\n\t\t\"\"\"\n\t\tMemoized wrapper that automatically caches results using the\n\t\tMRU algorithm (https://github.com/MrDram/mru) as a method, not a key.\n\t\t\"\"\"\n\n\t\t@functools.wraps(wrapped)\n\t\tdef wrapped_memoize(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tWrap the function to apply memoized results with the results\n\t\t\tfrom the decorated function. This method is just a wrapper so\n\t\t\tthat it can call the decorated function and memoize on the\n\t\t\tsame value"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\n\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tresults = func(*args, **kwargs)\n\t\t\tif typed and results is not None:\n\t\t\t\tif not type(results) is types.ListType:\n\t\t\t\t\tresults = [results]\n\t\t\t\tresults = [results]\n\t\t\telse:\n\t\t\t\tif maxsize == 0:\n\t\t\t\t\tmaxsize = maxsize * 2\n\t\t\t\tif len(results) < maxsize:\n\t\t\t\t\tresults = []\n\t\t\t\tresults = results[:maxsize]\n\t\t\tresults = [func(*args, **kwargs) for i in"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\t\"\"\"\n\t\nclass memoized(object):\n\t\"\"\"Decorator. Caches a function's return value each time it is called.\n\t\"\"\"\n\tdef __init__(self, func):\n\t\tself.func = func\n\t\tself.cache = {}\n\t\tself.maxsize = maxsize\n\n\tdef __call__(self, *args):\n\t\tif not isinstance(args, collections.Hashable):\n\t\t\targs = (args, )\n\t\tif self.maxsize > 0 and len(args) > self.maxsize:\n\t\t\tdel args[self.maxsize:]\n\t\treturn self.func(*args)"}]}
{"task_id": "62b8d23948ba5a41d1c3f498", "prompt": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\n\n\t@classmethod\n\tdef _clear():\n\t\t\"\"\"\n\t\tUsed by LRU to clear cache entries created by\n\t\t:data:`cache.clear()`.\n\t\t\n\t\tRemove from cache if set by :meth:`clean()`.\n\t\t:func:`cache.clear` also triggers a\n\t\t:class:`LRUItem` to be rebuilt if needed.\n\t\t\"\"\"\n\t\tpass\n\n\n\t@classmethod\n\tdef _lru(maxsize=128, typed=False):\n\t\t\"\"\"\n\t\tUsed by LRU to cache entries created by\n\t\t:data:`cache.getMaxAge()`.\n\t\t\n\t\tStore a"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\tcache = {}\n\t\tif typed:\n\t\t\t# Add cache entry for all decorated functions\n\t\t\t# (this is needed purely for cache lookup)\n\t\t\tfor (f, cache_entry) in _memoize_lru_cache(maxsize):\n\t\t\t\tcache[f.__name__] = cache_entry\n\t\t\t# Add decorating function to cache\n\t\t\tcache.clear = _clear\n\t\telse:\n\t\t\tcache = _memoize_lru_cache(maxsize)\n\n\t\tdef decorating_function(user_function):\n\t\t\t@wraps(user_function)\n\t\t\tdef run_user_"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\n\t\n\tdef _make_lru(maxsize, typed=True):\n\t\tif typed:\n\t\t\tmaxsize = maxsize()\n\t\t\n\t\tif maxsize is None:\n\t\t\treturn _lru_cache(maxsize)\n\t\t\n\t\tif maxsize == 0:\n\t\t\tmaxsize = 0\n\t\n\tif not lru_cache.lock.is_held():\n\t\tif not callable(lru_cache.lock.getmutex):\n\t\t\traise ValueError(\"Lock-object does not support non-strict access to\" + repr(lru_cache)) \n\t\tif maxsize == 0:\n\t\t\tmaxsize = 1024 - max(0, maxsize)\n\t\t\t\n\t\t"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\tdef wrapper(func):\n\t\t\tkey = \"{:03d}-{:d}-{:d}\".format(maxsize // 8, typed and \"...\" or \"\", maxsize)\n\t\t\tresults.set(key, func())\n\t\t\treturn func\n\t\treturn wrapper\n\n\treturn decorate(func=func) if typed else decorator\n#!/usr/bin/env python\n\n# Copyright (C) 2004-2015, Christoph Gohlke\n# Copyright (C) 2005-2006, Canal TP and/or its affiliates. All rights reserved.\n#\n# This file is part of Navitia"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\n\n\t# pylint: disable=I0011,W0232,W0231,R0904\n\n\tdef memoize_function(f):\n\t\t@wraps(f)\n\t\tdef _memoizer(*args, **kargs):\n\t\t\t\n                        if'maxsize' in kargs:\n                                maxsize = kargs['maxsize']\n\n                        elif 'typed' in kargs:\n                                typed = kargs['typed']\n\n                        elif 'lru' in kargs:\n                                maxsize = kargs['lru']\n\n\tf_memoizer = _memoizer(maxsize=maxsize)\n\tf_memoizer.cache = lru"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\tcache_getter = {}\n\t\tcache_getter[maxsize] = cache_getter[typed] = wraps(f)(cache_getter[maxsize])\n\t\treturn cache_getter[maxsize]\n\ndef to_list(x):\n\tif not isinstance(x, list):\n\t\tx = [x]\n\treturn x\n\ndef flatten_tree(task):\n\tif isinstance(task, list):\n\t\tfor x in task:\n\t\t\tflatten_tree(x)\n\t\treturn task\n\telif isinstance(task, dict):\n\t\treturn dict([flatten_tree(k)"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\ndef memoized(func):\n\t\"\"\"\n\tDecorator to wrap a function with a cacheable function that saves\nup to `maxsize` results. Also accepts a list of functions and their arguments\nthat will be passed to the function with the memoized result.\n\t\"\"\"\n\tdef wrapped_func(*args, **kwargs):\n\t\tglobal memoized_func\n\t\tif memoized_func:\n\t\t\tmemoized_func(args, kwargs)\n\t\treturn memoized_func(*args, **kwargs)\n\treturn wrapped_func\n    \ndef cached(memoizer=None):\n\t\"\"\""}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t \n\tdef decorator(fun):\n\n\t\tdef wrap_func(*args):\n\t\t\tkeyargs = [ ]\n\t\t\tfor arg in args:\n\t\t\t\tkeyargs.append( str(arg) )\n\t\t\tkey = ':'.join( keyargs )\n\t\t\tif len(key) > maxsize:\n\t\t\t\tkey = key[: maxsize - 1] + '...'        \n\t\t\telse:\n\t\t\t\tkey = key.replace('_','')\n\t\t\titem = (fun, (keyargs, args, key))\n\t\t\tif typed:\n\t\t\t\treturn item\n\t\t\telse:\n\t\t\t\treturn item\n\t\t\n\t\treturn wrap_func\n\t"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\n\n\tdef __init__(self, func, cache=maxsize):\n\t\t\"\"\"\n\t\tUse a single argument `func` and caches its results for subsequent\n\t\tcache entries.\n\t\t\n\t\t>>> cache = LRU_cache()\n\t\t>>> cache['a'] = 1\n\t\t>>> cache['b'] = 2\n\t\t>>> cache['c'] = 10\n\t\t>>> len(cache)\n\t\t2\n\t\t>>> cache['c']\n\t\t1\n\t\t>>> cache['d'] = 12\n\t\t>>> len(cache)\n\t\t4\n\t\t\"\"\"\n\t\tself.cache  = cache\n\t\tself."}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\t\"\"\"\n\t\tint_result = cache.get(cache_key, func_callable, LRU(maxsize))\n\t\tif int_result is not None:\n\t\t\tint_result = int_result()\n\t\t\tif typed:\n\t\t\t\treturn int_result\n\t\t\treturn func_callable(int_result)\n\t\treturn int_result\n\t\n@lru_cache(maxsize=(128, 128))\ndef get_item(items):\t\t\n\t\"\"\"\n\tA function to allow easy retrieval of an item (an iterator over a\n\titerable) as quickly as possible.\"\"\"\n\tif isinstance(items, list"}]}
{"task_id": "62b8d23748ba5a41d1c3f496", "prompt": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n\n\tdef decorator(function):\n\t\t\"\"\"\n\t\tThe decorated function will be executed on a memoized value\n\t\tand will not be evaluated again.\n\n\t\tExample usage:\n\n\t\t.. code-block:: python\n\n\t\t\t@lfu_cache(maxsize=128)\n\t\t\tdef foo(a, b):\n\t\t\t\treturn a + b\n\n\t\t\tprint foo(10, 20)\n\n\t\t\"\"\"\n\t\tlock = threading.Lock()\n\t\tlua_cache = lambda func: lfu_cache_with_maxsize(maxsize=maxsize)(func)\n\n\t\t@functools.wraps(function)\n        \t\t"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\nif __name__ == \"__main__\":\n\t\n\t# check whether our input is a lexeme block\n\ttry:\n\t\tblock = check_input(1)\n\t\n\texcept:\n\t\tprint '\\nUsage: lfu.py <block>'+ __file__ + '\\n'\n\t\tprint'     This is a lexeme block.\\n'\n\t\tsys.exit(1)\n\t\t\n\t# check whether we have a valid output device by checking the \"outputs\" array\n\ttry:\n\t\toutputs = lfu_outputs(block)\n\t\t\n\t\t# write the output for each of"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n\n\t@decorator\n\tdef get_lfu(name, function, *args, **kw):\n\t\treturn _lfu_wrapper(name, function, *args, **kw, maxsize=maxsize,\n            typed=typed)\n\treturn get_lfu\n\n# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\nfrom google.cloud.iot_v1.proto import (\n    image_annotator_pb2 as google_dot_cloud_dot_image__v1_dot_proto_dot_image__pb2,\n)\n\n"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n\n\t@lru_cache(maxsize=int(maxsize))\n\tdef fn(x, *args, **kwargs):\n\t\twith CacheKey(lstr(get_key(x))) as cache:\n\t\t\treturn cache.ttl(x)\n\n\tif typed:\n\t\treturn fn\n\telse:\n\t\treturn fn\n\"\"\"Support for Lutron Caseta lights.\"\"\"\nimport logging\n\nfrom homeassistant.components.light import ATTR_BRIGHTNESS, Light\nfrom homeassistant.const import CONF_NAME, CONF_LIGHTS, TEMP_CELSIUS\n\nfrom. import LUT"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n    def __init__(self, maxsize=128, typed=False, *args, **kwargs):\n        self.maxsize = maxsize\n        self.typed = typed\n        super(LFUCache, self).__init__(*args, **kwargs)\n        self.cache = {}\n        \n    def get(self, key, default=None):\n        if key not in self.cache:\n            value = self.cache[key] = self.function(key)\n            if self.cached_metadata is not None:\n                self._update_metadata(value)\n            elif self.typed:\n                return self.cached_metadata\n            elif self"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\t@wraps(lfu_cache)\n\t\tdef cache_decorator(f):\n\t\t\tcache_results = []\n\t\t\tresult = f()\n\t\t\ttry:\n\t\t\t\tcache_results.append(result)\n\t\t\texcept Exception as e:\n\t\t\t\t# cache_results is modified if the callable raised an exception\n\t\t\t\t# which means that the wrapped function will not be called. This is\n\t\t\t\t# therefore a case when the wrapped function is cached:\n\t\t\t\t# the decorated function is called with the current cached\n\t\t\t\t# result, or after the decorator called again if there is no\n\t\t\t\t# result (if"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n\t\n\tdef decorator(f):\n\t\t@functools.wraps(f)\n\t\tdef g(*args, **kwargs):\n\t\t\tif not callable(f):\n\t\t\t\traise TypeError\n\t\t\t\n\t\t\tcache_key = f.__name__\n\t\t\t\n\t\t\t\n\t\t\tif typed:\n\t\t\t\tresults = f(*args, **kwargs)\n\t\t\t\tif isinstance(results, LeastRetrievalHistory):\n\t\t\t\t\tresults = list(results.data)\n\t\t\t\t\n\t\t\t\tfor result_key in results:\n\t\t\t\t\tcache.set(cache_key, result_key, result)\n\t\t\t\n\t\t\t\treturn results\n\t\t\telse:\n\t\t\t\tcache_"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n\n\tcache_dict = {}\n\n\t@functools.wraps(f)\n\tdef memoizer(*args, **kwargs):\n\t\tkey = (f.__name__, getattr(args[0], '__name__'))\n\t\tif typed:\n\t\t\tkey += ('.type_str', 'typed')\n\n\t\tif key not in cache_dict:\n\t\t\tif maxsize:\n\t\t\t\twhile maxsize:\n\t\t\t\t\tkey = '.'.join([_i for _i in key if _i % maxsize == 0])\n\n\t\t\td = cache_dict[key]\n\n\t\t\tif typed:\n\t\t\t\tdel d['_data']\n\t\t\t"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n\n\tdef memoize_lfu(func):\n\n\t\tcache = {}\n\t\t@functools.wraps(func)\n\t\tdef f(args, maxsize, typed):\n\t\t\targs_iter = args\n\t\t\tif len(args_iter) == maxsize:\n\t\t\t\treturn func\n\t\t\telif not typed:\n\t\t\t\tfor i in map(lambda x: str(x), args_iter):\n\t\t\t\t\tcache_key = \"memoize_lfu_{}\".format(i)\n\t\t\t\t\tif cache.has_key(cache_key):\n\t\t\t\t\t\tmemo = cache[cache_key]\n\t\t\t\t\t\tdel cache"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\t\"\"\"\n\t\n\n\tdef _lfu(fn):\n\t\t_cache = {}\n\n\t\t@wraps(fn)\n\t\tdef _lfu_(self, *args, **kwargs):\t\t\t\t#pylint: disable=W0221\n\t\t\tif not self.get('memoize', False):\n\t\t\t\twhile len(args) >= maxsize:\n\t\t\t\t\t\tself.clear()\n\n\t\t\t\tfor key, val in self:\n\t\t\t\t\t_cache.setdefault(key, val)\n\n\t\t\t\t\tfun = self.get(key)\n\n\t\t\t\t\ttry:\n\t\t\t\t\t\tval = fun(*args, **kwargs)\n\t\t\t\t\texcept:\n\t\t\t\t\t\tval"}]}
{"task_id": "62b8d22f48ba5a41d1c3f488", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\ttry:\n\t\t\tpopped = self[self.cursor.index(self.search_tag)]\n\t\texcept ValueError:\n\t\t\traise IndexError(\"Item doesn't exist\")\n\t\tself.cursor = self.cursor[popped]\n\t\tself.cursor_pos += 1\n\t\t# Return the tuple (key, value) at that position.\n\t\treturn self.cursor[self.cursor_pos]\n\n\tdef clear(self):\n\t\t\"\"\"\n\t\tRemove all entries associated with the search string.\n\t\t\"\"\"\n\t\tdel self[:]\n\n\n# This is copied from libmysql.py\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\twhile 1:\n\t\t\tyield self.front[1]\n\t\t\tif self.front:\n\t\t\t\tself.front = self.front[0]\n\t\t\telse:\n\t\t\t\traise StopIteration\n\tdef clear(self):\n\t\t\"\"\"\n\t\tRemove all items from the front of the heap.  Do not\n\t\tremove the root item.\n\t\t\"\"\"\n\t\tself.front = None\n\tdef __len__(self):\t# real signature unknown; restored from __doc__\n\t\traise NotImplementedError\n\tdef __contains__(self, key):\t\tif isinstance(key, str):\n\t\t\tfor k"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\tkeys, values = self.popn(self.cursor)\n\t\tinsert = self.get_insert_pos(self.cursor)\n\t\treturn dbgp.get(keys, insert), [], values\n\n\tdef get_insert_pos(self, cursor): \n\t\t\"\"\"\n\t\tGet the last position before the insertion,\n\t\tNone if the insertion is empty.\n\t\t\"\"\"\n\t\tif not hasattr(cursor, 'description') or cursor.description is None:\n\t\t\traise TypeError(\"'cursor' object is not an instance of'str'\")\n\t\t\n\t\tif cursor.description[0:6].is"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\tif not self._empty:\n\t\t\treturn self._items.pop(0)\n\t\treturn tuple(self._items)\n\n\tdef clear(self):\t\"\"\"\n\tRemove and return an empty set that also occurs.\n\t\"\"\"\n\t\treturn frozenset(self._items.keys())\n\n\nclass L{FileCache} (FileCacheInterface):\n\t\"\"\"\n\tCaching interface for FileCache, providing backwards-compatibility for \n\tPython 2.3 and later.\n\t@see: L{cache_method_map}\n\t\"\"\"\n\tdef __init__(self, directory, writable"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\twhile len(self.data) > self.maxkeys:\n\t\t\tdata = self.data.pop()\n\t\t\tpos = bisect_left(self.values, (data['key'],))\n\t\t\tself.data[pos] = data\n\t\t\tdel self.data[pos]\n\n\t\treturn self.data.popitem()\n\n\tdef shift(self, shift_amount):\t# [k, v] -> k, v < v\n\t\tif len(self.data) < self.maxkeys:\n\t\t\traise KeyError('No more data left to shift.')\n\t\telif shift_amount >"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\tkv = self.first\n\t\ti = len(self)-1\n\t\twhile i>=0:\n\t\t\tif kv[i][0] == kv[-1][0]:\n\t\t\t\ttmp = kv[i]\n\t\t\t\tdel kv[i]\n\t\t\t\ttmp[-1] = None\n\t\t\t\tif len(tmp) == 2:\n\t\t\t\t\treturn tmp\n\t\t\t\telse:\n\t\t\t\t\treturn tmp[:i] + [kv[-1]] + tmp[-1:]\n\t\t\t\tdel tmp \n\t\t\t\ti -= 1  \n\t\t\t\tkv.pop()\n\t\t\telse:\n\t\t\t\tif self[-1] == kv[-1"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\tassert not (self.modified and not self.inserted)\n\t\txkey, xval = self.xkey, self.xval\n\t\tself.xkey.pop(xval)\n\t\tself.xval.pop(xkey)\n\t\txkey, xval = self.xkey, self.xval\n\t\tassert xkey == xval\n\t\ttry:\n\t\t\tdel self[xkey]\n\t\texcept KeyError:\n\t\t\tpass\n\t\tself.inserted = True\n\t\tself.append(xkey)\n\t\tself.xkey = xkey\n\n\tdef setdefault"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\tfor lst in self.lst:\n\t\t\tprint lst\n\t\t# find key in reversed order\n\t\tkey, value = None, None\n\t\tif self.head is not None:\n\t\t\tfor i in xrange(0, len(self.head)-1, -1):\n\t\t\t\t(lst1, lst2) = self.head[i], self.head[i+1]\n\t\t\t\tif lst1 is item:\n\t\t\t\t\tlst1.count -= 1\n\t\t\t\telif lst1.count == 0:\n\t\t\t\t\tdel self.head[i]\n\t\t\t\t\tself.head.append(key)"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\tkey = self.pop(0)\n\t\tdel self[key]\n\t\treturn key, self.pop(0)\n\n\tdef popitemlist(self):\t\"\"\"\n\tRemove and return a `list` of pairs ordered by keys.\tThis works well if keys\n\tare in the order produced by `add()` or `remove()`.\t\"\"\"\n\tkeys = list(self.keys())\n\t\tdel self[:]\n\t\twhile keys: self.popitem(False)\n\t\treturn keys\n\n\tdef __setitem__(self, path, value):\n\t\t\"\"\"\n\t\tPut path and value into"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\t\"\"\"\n\t\treturn (self.key.pop(self.key.index('insert')), self.value)\n\n\tdef pop(self):\n\t\t\"\"\"\n\t\tRemove and return (string, value) pair next to the marker.\n\t\t\"\"\"\n\t\treturn self.key.pop(self.key.index('end'), self.end)\n\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\tReturn an iterator\n\t\t\"\"\"\n\t\treturn self\n\n\tdef __reversed__(self):\n\t\t\"\"\"\n\t\tReturn the reverse iterator (reversed list of keys)\n\t\t\"\"\"\n\t\treturn self.values\n\n\t"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47e", "prompt": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t", "suggestions": [{"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif key not in self._cache:\n\t\t\tself._cache[key] = default\n\t\treturn self._cache[key]\n\tdef clear(self):\n\t\t\"\"\"\n\t\tD.clear() -> None.  Remove all items from D. \n\t\tThe algorithm is equivalent to:  D.keys() == D.keys()\n\t\t\n\t\tLike:\n\t\t  for k,v in D.items():\n\t\t ...\n\t\t\"\"\"\n\t\tself._cache = {}\n\tdef copy(self):   return D(_obj=self.__class__, _data=self)\n\tcopy.__safe_for_"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\tself[key] = default\n\t\treturn default\n\tdef pop(self, key, *args):\n\t    \"\"\"\n\t     remove specified key and return its value.\t   \n\t     Must only be called on non-existent d.\n\t    \"\"\"\n\t    if len(args) == 1:\n\t\treturn args[0]\n\t    try:\n\t\tret = self[key]\n\t    except KeyError:\n\t\tret = self.get(key)\n\t\tself[key] = ret[0] = args[0] = self.get(key"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\tif key not in self.data:\n\t\t\tself.data[key] = default\n\t\treturn self.data[key]\n\n\tdef update(self, E=None, **F):\n\n\t\tE = E or self.data\n\t\tif E is None:\n\t\t\tE = {}\n\t\treturn E\n\n\n\tdef pop(self, key, *args):\n\t\t\n\t\t\"\"\"\n\t\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\t\tIf key is not found, d is returned if given, otherwise KeyError is raised\n\t\t\"\"\"\n\t\tif"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\tv = self.get(key, default)\n\t\t\td = default\n\t\t\tdel self[key]\n\t\t\treturn v\n\n\tdef update(self, dict=None, **kwargs):\n\t\t\"\"\"\n\t\tUpdate D from EITHER aka mapping of sets. Update H from A to dict.\n\t\tUpdate Q from dict by appending elements from Q.\n\n\t\tThis operation is atomic if EITHER doesn't have a stable structure\n\t\tfor the given sequence of keys.\n\n\t\tExample usage:\n\t\t\tH = {\""}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\tself[key] = default\n\t\t\treturn default\n\n\tdef update(self, E=None, **F):\n\t\t\"\"\"\n\t\tD.update([E,F]) -> None.  Update D from E and F.\n\t\tKeys with non-None values are removed if their key is not in E.\n\n\t\t\"\"\"\n\t\tif E is not None:\n\t\t\tnewE = {}\n\t\t\tnewF = F.copy()\n\t\t\tfor k, d in E:\n\t\t\t\tif k not in F:  #"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\tself[key] = default\n\t\t\treturn default\n\tdef get(self, key, *args):\n\t\tif len(args) > 0:\n\t\t\traise IndexError, \"Keyword argument expected\"\n\t\tif len(args) > 1:\n\t\t\traise TypeError, \"Keyword argument expected\"\n\t\tif PY3:\n\t\t\tif 'encoding' == kwargs.get('encoding', 'UTF-8') and kwargs['encoding'] == \"utf-8\":\n\t\t\t\treturn json.loads(self.get(key, *"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\tself[key] = default\n\t\texcept TypeError, e:\n\t\t\tself[key] = default\n\n\tdef _warn(self):\n\t\tpass\n\t\nclass D(object):\n\t\"\"\"\n\tSet-like object.\n\t>>> d1 = D()\n\t>>> d1.z = 42\n\t>>> d1.z\n\t42\n\t\"\"\"\n\tdef __init__(self, d):\n\t\tobject.__init__(self)\n\t\tself.d = d\n\t\n\t#TODO: Make me"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\ttry: return self[key]\n\t\texcept KeyError: pass\n\t\tself[key] = default\n\t\treturn default\n\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\tD.__setitem__(k,d) sets D[k]=d if k not in D \"\"\"\n\t\tif not self.has_key(key):\n\t\t\tself[key] = value\n\t\telse:\n\t\t\tdefault = self[key]\n\t\t\tself.set(key, value)\n\n\tdef update(self, dict):\n\t\t\"\"\"\tD.update(E) -> None.  Update"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\treturn D.setdefault(self, key, default)\n# -*- coding: utf-8 -*-\n\"\"\"\n    flask.ctx\n    ~~~~~~~~~~~~~\n\n    Implements the objects required to keep state across request\n    and response object and the Werkzeug request object.\n\n    :copyright: 2011 by Louis Taylor <lotho@sas.net>\n    :license: BSD, see LICENSE for more details.\n\"\"\"\n\n# because we need to import these things from paste.deploy\n# if we do so, `from paste.deploy` has to be used instead.\nfrom werkzeug.http import parse_options"}, {"generated_text": "def setdefault(self, key, default=None):\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\tself[key] = default\n\t\t\treturn default\n\tdef items(self):\treturn D.items(self)\n\tdef values(self):\treturn D.values(self)\n\tdef __iter__(self):\treturn iter(self.items())\n\tdef iteritems(self):\treturn iter(self.items())\n\tdef iterkeys(self):\treturn self.iterkeys()\n\tdef itervalues(self):\treturn D.itervalues(self)\n\tdef pop(self, k, d=None"}]}
{"task_id": "62b8d22948ba5a41d1c3f47c", "prompt": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self[key]\n\t\telif default is None:\n\t\t\treturn default\n\t\treturn default\n\n\tdef get_with_default(self, key, default=None): return self.get(key, default)\n\tdef _update(self, dict, **kwargs):\treturn self.update(dict, **kwargs)\n\tdef pop(self, key, *args):\n\t\tif len(args) > 1:\n\t\t\traise TypeError, 'pop expected at most 2 arguments, got %i' % len(args)\n\t\tif key in self"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\treturn default\n\n\tdef keys(self):\n\t\treturn [self.get(k) for k in self.keys_name]\n\n\tdef values(self):\n\t\treturn self.get_key_values()\n\n\tdef items(self):\n\t\tk = self.keys()\n\t\treturn k\n\n\tdef get_key_values(self):\n\t\t\"Get a tuple of D's key name and value, with clean duplicates\"\n\t\tret = {}\n\t\tfor key in self.keys():\n\t\t\ttry"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry:\n\t\t\treturn self.__getitem__(key)\n\t\texcept KeyError:\n\t\t\treturn default\n# -*- coding: utf-8 -*-\n# Copyright (c) 2018, Frappe Technologies Pvt. Ltd. and contributors\n# For license information, please see license.txt\n\nfrom __future__ import unicode_literals\nimport frappe\nimport unittest\nfrom frappe.utils import cint\nfrom frappe.model.rename_doc import rename_doc\n\nclass TestRenameDoc(unittest.TestCase):\n\n\tdef test_rename_doc(self):\n\t\t# first edit in DB\n\t\tfrom"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry: return self[key]\n\t\texcept KeyError: return default\n\n\tdef __contains__(self, key):\n\t\ttry: return key in self.keys()\n\t\texcept TypeError: return False\n\n\t#-- PROPERTY PARSING\n\tdef __repr__(self):\n\t\t\"\"\"\n\t\tA representation of this document.\tCan be used with \"print\" documents.\n\t\tNote that the printed document will be presented on its own line - it can be displayed\n\t\tin the terminal.\n\t\t\"\"\"\n\t\treturn str(self)\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry: return self[key]\n\t\texcept KeyError: return default\n\n\tdef get_bool(self, key, default=None):\n\t\ttry: return self[key]\n\t\texcept KeyError: return default\n\n\tdef keys(self): return self.list()\n\n\tdef iterkeys(self): return iter(self.keys())\n\n\tdef keys_eq(self, keys):\n\t\t\"\"\"Asserts that self.values() is a dictionary\"\"\"\n\t\treturn set(self.keys()) == set(keys)\n\n\tdef values(self):\treturn [self[key] for key in"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\treturn self.config.get(self.sectionPath(key),default)\n\tdef getboolean(self,key):\n\t\treturn self.configgetboolean(self.sectionPath(key))\n\tdef getInt(self,key):\n\t\treturn int(self.configget(self.sectionPath(key)))\n\tdef getfloat(self,key):\n\t\treturn float(self.configget(self.sectionPath(key)))\n\tdef getlist(self,key,default=None):\t\n\t\td = self.configget(self.sectionPath(key),default"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\treturn self.data.get(key, default)\n\tdef has_key(self, key):\n\t\treturn key in self.data\n\tdef __contains__(self, key):\n\t\treturn self.data.__contains__(key)\n\tdef __setitem__(self, key, value):\n\t\tself.data.__setitem__(key, self.data.get(key, value))\n\tdef insert(self, key, value): self.data.insert(key, value)\n\tdef append(self, key, value): self.data.append(key, value)\n\t"}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry: return self[key]\n\t\texcept KeyError: return default\n\ndef _get_seq_key(name):\n\t\"\"\"\n\t>>> _get_seq_key(\"x\")\n\t'd'\n\t>>> _get_seq_key(\"x\", k=1)\n\t'dd'\n\t>>> _get_seq_key(\"y\", [\"a\", \"b\"])\n\t'dd'\n\t>>> _get_seq_key(\"x\", k=1, default=\"default value\")\n\t'de'\n\t>>> _get_seq_key(\"x\","}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\treturn self.defaults.get(key, default)\n\tdef has_key(self, key):\n\t\treturn key in self.keys()\n\n\tdef rename(self, oldkey, newkey):\n\t\tself.keys().remove(oldkey)\n\t\tself.keys().append(newkey)\n\nclass KeyValStore(KeyValueStore):\n\t\"\"\"KeyValStore stores data in the form:\n\t{'key':'a', 'val':'b'}.  d defaults to None.  the value is stored\n\tautomatically if it doesn't exist, or doesn't exist."}, {"generated_text": "def get(self, key, default=None):\t\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\t\"\"\"\n\t\ttry: return self[key]\n\t\texcept KeyError: return default\n\tdef keys(self): return self.dict.keys()\n\tdef update(self, dict):\n\t\t\"\"\"\n\t\tD.update(dict) -> None.  Update D from D.\n\t\t\"\"\"\n\t\tfor k,v in dict.iteritems(): self[k] = v\n\tdef items(self): return self.dict.items()\n\tdef itervalues(self):\n\t\tfor t in self.dict.itervalues():\n\t\t\tyield t\n\tdef values(self): return self.dict.values()"}]}
{"task_id": "62b8d22548ba5a41d1c3f472", "prompt": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\n# NOTE A lot of code copied from Django 1.9 source, because it was modified\n# from the Django 1.8 source code.  The problem was that the pickle\n# module would try to pass in its own caching mechanism.\n# The repertoire of ideas from:\n# http://code.activestate.com/recipes/277017-object-base-class/\n# A re-write of the caching logic in ModelBase code.\n\nclass BaseCache(object):\n\t\"\"\"Baseclass for cache regions\"\"\"\n\t\n\tdef __init__(self, default_timeout="}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tif lock is None:\n\t\t\tlock = RLock()\n\t\tclass C:\n\t\t\t# Dummy value to use as a method\n\t\t\tfunc_name = '_cachedmethod_%s' % key\n\t\t\tdef __init__(self, *args, **kw):\n\t\t\t\tsuper(C, self).__init__(*args, **kw)\n\t\t\t\tself._self_instance = cache.active.get(key)\n\t\t\t\tself._self_args = args\n\t\t\t\tself._self_kwargs = kw\n\t\t\t\tself._key = key\n\t\t\t\tself._cache = cache\n\n\t\t\tdef __call__(self, *"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\n\t\n\tdef __call__(self, *args, **kwargs):\n\t\t\n\t\tif self._cache_enabled and cachehit():\n\t\t\tcache = cachehit()\n\t\t\ttry:\n\t\t\t\treturn cache.cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tvalue = self.func(*args, **kwargs)\n\t\t\t\tif value!= None:\n\t\t\t\t\tcache = None # Cache hit\n\t\t\t\t\tself._cache_hits += 1\n\t\t\t\t\treturn value\n\t\t\texcept TypeError:\n\t\t\t\t# Object is not hashable, not a dict\n\t\t\t\tcache = None\n\t\t\t\tvalue = self.func(*args, **kwargs)\n\t\t\t\t"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tif callable(cache):\n\t\t\t# Check to see if an attribute type was cached\n\t\t\tif (issubclass(type(cache), Attribute)):\n\t\t\t\tcache = cache()\n\n\t\t\tcache_attr = cache\n\n\t\telse:\n\t\t\t# Check to see if an attribute type was cached\n\t\t\tif (issubclass(type(cache), types.MethodType)):\n\n\t\t\t\t# Check to see if the method cache is an instance method - either\n\t\t\t\t# use isinstance, so use isinstance() to handle the instance cacheing\n\t\t\t\tif hasattr(cache, '__get__'):\n\t\t\t\t\tcache = cache()\n\n\t\t\t\tcache_attr = cache"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tdef cachedmethodwrapper(klass):\n\t\t\treturn klass(key, func=func, cache=cache)\n\t\tif lock is None:\n\t\t\tlock = locks.Lock()\n\t\twraps = cache(key, cache=cache)(cachedmethodwrapper)\n\t\treturn wraps\n\treturn cachedmethodwrapper\n\nclass Cache(collections.MutableMapping):\n\t\"\"\"A dictionary that tracks various cached values for a function.\"\"\"\n\t\n\tdef __init__(self, cache):\n\t\t\"\"\"Initializes the cache.\"\"\"\n\t\tself.cache = cache\n\t\n\tdef __getitem__(self, k): return self.cache[k"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tcache_key = hashlib.md5(key).hexdigest()\n\t\t@functools.wraps(cache.__class__)\n\t\tdef __method__(*args, **kwargs):\n\t\t\texpires, value = cache_get(cache_key, (0,None))\n\t\t\tif callable(expires):\n\t\t\t\tvalue = expires()\n\t\t\telif isinstance(value, ModelDict):\n\t\t\t\tif 'expires' in value:\n\t\t\t\t\texpires = value['expires']\n\t\t\t\telif'max-age' in value:\n\t\t\t\t\texpires = value['max-age']\n\t\t\t\telif 'expires' in value:\n\t\t\t\t\texpires"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tdef decorator(f):\n\t\t\t@wraps(f)\n\t\t\tdef cachedfunc(*args, **kw):\n\t\t\t\tif key not in cache:\n\t\t\t\t\treturn f(*args, **kw)\n\t\t\t\tres = cache[key]\n\t\t\t\tCACHE[key] = res\n\t\t\t\treturn res\n\t\t\t#else we use a class decorator\n\t\t\treturn cachedfunc\n\t\treturn decorator\n\n\tif hashkey not in cache: return _cachedmethod({key: hashkey})\n\treturn _cachedmethod(cache[hashkey][1])(decorator)\n\n# Cache the result of the function and catches"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tdef wrapper(func):\n\t\t\tdef newfunc(*args):\n\t\t\t\tif args in cache:\n\t\t\t\t\t# args may not be hashable\n\t\t\t\t\t# re-hash if the cache is already full\n\t\t\t\t\tif key in cache:\n\t\t\t\t\t\t# return the result\n\t\t\t\t\t\treturn cache[key]\n\t\t\t\t\telse:\n\t\t\t\t\t\t# record the cache entry for this key and its args\n\t\t\t\t\t\tcache[key] = cache_get(key, None)\n\t\t\t\t\t\t# return the result\n\t\t\t\t\t\treturn func(*args)\n\t\t\t\telse:\n\t\t\t\t\treturn func(*args)\n\t\t\tnewfunc.cache = cache\n\t\t\tnew"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tdef wrapper(method):\n\t\t\tdef callme(*args, **kwargs):\n\t\t\t\t#print('callme:'+str(cache)+' ('+str(key)+')')\n\t\t\t\t#print('cache:'+key+':'+str(cache))\n\t\t\t\t\n\t\t\t\tif cache:\n\t\t\t\t\t#print('cache:'+cache+'\\n'\n\t\t\t\t\t\tcachehits = cache.hits\n\t\t\t\t\tprint('cache hits:'+str(cachehits))\n\t\t\t\t\tresult = cachehits[0]\n\t\t\t\t\tkeyargs = tuple(key) + tuple(args) + tuple(kwargs.items())\n\n\t\t\t\t\t#print('"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\t\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\t\"\"\"\n\t\tdef decorate(cls):\n\t\t\tcache = cache.copy()\n\t\t\tclass_prepared.send(sender=cls)\n\t\t\treturn cache.get(key, class_prepared)\n\t\treturn decorate\n\n\treturn cachedmethod\n\n\n#\n# Function used by the cache to store a cached item.\n#\ndef cacheobj(cache, key):\n\t\"\"\"\n\tGiven an cache dictionary and a key to cache a cached and returns\n\ta cached copy that it's used.\n\t\"\"\"\n\tcache[key] = cacheobjmod.cached(key)\n\treturn cacheobjmod."}]}
{"task_id": "62b8c517e0d34b282c18122e", "prompt": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\treturn \"Extostr (%s)\" % str(e)\n\ndef msg(e, level=20, path=None, max_level=100, max_path_level=5, debug=False):\n\t\"\"\"\n\tShow exception and debug messages.\n\n\t:param e: Any exception instance or None.\n\t:type e: Exception or Exception or string\n\t:param level: Maximum call stack level (default 20)\n\t:type level: int\n\t:param path: Max path level (default 5)\n\t:type path: int or None\n\t:param"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\treturn ''.join('  '*sizeobj.size(e.__class__.__name__) for sizeobj in e)\n# -----------------------------------------------------------------------------\n# ply: lex.py\n#\n# Copyright (C) 2001-2015,\n# David M. Beazley (Dabeaz LLC)\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n# * Redistributions of source code must retain the above copyright notice,\n#   this list of conditions and the following disclaimer"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t # str method\n\t if inspect.isclass(cls):\n\t\t\t# this is a method - don't want to print all this info\n\t\t\treturn str(e)\n\t\t# this is a regular exception instance\n\t\tif class_type(cls) == cls.__class__:\n\t\t\treturn cls.toString()\n\t\t# this is a normal exception\n\t\tif hasattr(e, '__str__'):\n\t\t\treturn str(e.__str__())\n\t\t# this is a normal exception\n\t\tif hasattr(e, '__name__'):\n\t\t\treturn str(e.__name__)\n\t\t# this is a subclass of a normal"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\tfrom pycopia.QA import m\n\n\tfrom pycopia import socket\n\n\t_c = m.classobj(_e, _d=cls)\n\tc = None\n\n\tif isinstance(e, Exception):\n\t\tc = str(e)\n\telif isinstance(e, socket.error):\n\t\tc = \" \".join(\"%s: %s\" % (k, v) for k, v in e.args)\n\n\tif not max_level:\n\t\tmaxlen = max([len(x) for x in c.split(\" \")])\n\t\tif c and c"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\t# Make sure the exception is serializable.\n\t\timport exceptions\n\t\ttry:\n\t\t\ts = \"\".join(str(x) for x in traceback.format_exception(*e))\n\t\texcept UnicodeEncodeError:\n\t\t\ts = \"\".join(unidecode(repr(x))\n\t\t\t\t\t\t for x in traceback.format_exception(*e))\n\t\telse:\n\t\t\ts = s.decode(sys.__stdin__.encoding)\n\n\t\treturn (s+max_level).encode(\"ascii\")\n\n\t@staticmethod\n\tdef indent(text, level=0):\n\t\t\"\"\"\n\t\tUse this method to have"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\ttypeName = e.__class__.__name__\n\t\tstack = inspect.stack()\n\t\tfor _ in range(max_path_level):\n\t\t\tstack[2].pop()\n\t\tstack[2].append(str(e))\n\t\tname = e.__class__.__name__\n\t\tif isinstance(name, typeName):\n\t\t\tpath_msg = \"In \" + name\n\t\t\tmessage = \"\\n%s\" % path_msg\n\t\telif isinstance(e, StackException):\n\t\t\tpath_msg = \"During \" + name\n\t\t\tmessage = \"\\n%s\" % path_"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\t# we catch the exception (and make a more Pythonic and print it)\n\t\tis_exception = isinstance(e, Exception) or isinstance(e, _Except)\n\telse:\n\t\tis_exception = False\n\t\tis_exception, exception = sys.exc_info()[:2]\n\t\t# this is where exceptions are defined\n\t\ttraceback.print_exception(exception)\n\tif is_exception:\n\t\t# extract the current traceback\n\t\tlast = traceback.extract_tb(sys.exc_traceback)\n\t\tif not (last or last[-1][0] =="}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\n\n\ts = ''\n        if e.args is not None:\n            s = str(e.args[0]) + '\\n'\n            if s[-5:] ==' [truncated]':\n                s = s[:-5] + '..'\n        s += '{0}{1}Exception: {2}\\n'.format(e.message, e.name, e.reason)\n        if hasattr(e, 'child') and e.child.exceptions:\n            for child in e.child.exceptions:\n                s += '\\n'.join(extostr(cls, e, child.max_level"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\n\n\tstr_ = ''\n\tnum_calls = len(inspect.getfullargspec(cls.__init__)[0]) - 1\n\tfor i in range(0, i + 1):\t\n\t\tp1 = inspect.getfullargspec(cls.__init__)[1][inspect.CO_VARARGS].replace('(', ', '(').replace(')', ')')\t\t\n\t\tp2 = p1.replace(str(e.args[i]), '')\n\t\tp3 = re.sub('\\s', '', p2)\n\t\tp4 = p3.replace(str('%d '"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\t\"\"\"\n\tFormat an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\t\"\"\"\n\t\te = str(e).replace('\\n','')\n\t\treturn '%s - %s' % (e, cls.__name__)\n#coding:utf-8\n#author: zwei\n# Purpose: https://developers.zq.io/\n# Created on: 2016-8-27\n# Copyright (C) 2011, Manfred Moitzi\n# License: MIT License\n\nimport cv2 as cv\nimport numpy as np\nimport unittest\nimport os\nimport matplotlib.image as mpimg\n\nclass TestCNNKColorSpace(unittest.TestCase):\n   "}]}
{"task_id": "62b8bbbfe0d34b282c181210", "prompt": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t", "suggestions": [{"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tif file_name is None:\n\t\t\treturn 1\n\n\t\ttry:\n\t\t\tif not overwrite:\n\t\t\t\ttry:\n\t\t\t\t\tfile_object = open(file_name, \"w\", encoding=encoding, errors=\"replace\")\n\t\t\t\texcept Exception as e:\n\t\t\t\t\tprint e\n\t\t\t\t\traise Exception(\"Could not create file with name %s.\" % (file_name))\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tfile_object = open(file_name, \"w\", encoding=encoding, errors=\"replace\")\n\t\t\t\texcept Exception as e:\n\t\t\t\t\tprint e\n\t\t\t\t\traise Exception(\"Could"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\ttry:\n\t\t\twith codecs.open(file_name,encoding, errors='strict',encoding_errors='ignore') as file_handle:\n\t\t\t\tfile_handle.write(text_buffer) # Add a space afterwards to make sure we can deal with non-UTF-8 encoded data\n\t\t\treturn int(file_handle.tell()) # Return the file's number\nimport math, time, calendar, sys\n\ndef timeToHours(t, m, d):\n    \"\"\"return a human-readable form of hours\"\"\"\n    t = int(t)\n    m = int(m)\n   "}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tif not overwrite and os.path.exists(file_name):\n\t\t\tif overwrite_flag:\n\t\t\t\tfile_path = os.path.splitext(file_name)[0]\n\t\t\telse:\n\t\t\t\tfile_path = os.path.splitext(file_name)[0]\n\t\telse:\n\t\t\tfile_path = ''\n\t\tf_handle = open(file_path, os.O_RDWR)\n\t\tf_file = f_handle\n\t\tf_handle.truncate()\n\t\tf_handle.seek(0)\n\t\ttext_buffer = codecs.open"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tif not os.path.exists(file_name):\n\t\t\treturn -1\n\n\t\twith open(file_name, \"a+\") as f:\n\t\t\tcounter_word = -1\n\t\t\tfor character in text_buffer:\n\t\t\t\tf.write(character)\n\t\t\t\tcounter_word += 1\n\t\t\t\t#if counter_word % 10000 == 0:\n\t\t\t\t#\tprint('wrote counter word: {}'.format(counter_word))\n\t\t\t\t#\tf.write(' ')\n\t\t\tf.write('\\n')\n\n\t\t\tif counter_word == 1:\n\t\t\t\tf.write('1"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\t# Note: for python 3.0, this does not work as expected for Python 2.7\n\t\t#       and later, we'll convert from io. TextIOWrapper object\n\t\t#       to a TextIOWrapper object\n\n\tbuffer_type = io.TextIOWrapper(text_buffer, encoding,'replace')\n\n\tfile_handle = io.open(file_name, 'w', encoding=encoding)\n\tbytesize = encoding.size(file_handle.fileno())\n\tfile_handle.write(ctypes.c_char_p(bytesize))\n\n\toutput_bytes = encoding"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\n\t\n\tif os.path.isfile(file_name):\n\t\ttry:\n\t\t\twith open(file_name, 'rb') as f:\n\t\t\t\ttext = f.read()\n\t\t\t\tif overwrite:\n\t\t\t\t\ttry:\n\t\t\t\t\t\toriginal_bytes = len(text)\n\t\t\t\t\texcept AttributeError:\n\t\t\t\t\t\toriginal_bytes = len(bytes(text))\n\t\t\t\t\t\n\t\t\t\t\tcurrent_bytes = text_buffer.count('\\n\\n')\n\t\t\t\t\tn = 0\n\t\t\t\t\tbuffer_index = 1\n\t\t\t\t\tfor i in xrange(current_bytes):\n\t\t\t\t\t\ttext_view = text[i:]\n\t\t\t\t\t\t"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\n\n        file_handle = open(file_name, \"r\")\n        if not file_handle:\n            print (\"Create file %s\" % file_name)\n            file_handle = open(file_name, \"w\")\n\n        file_handle.write(text_buffer)\n        file_handle.close()\n\n        number_of_bytes_written = os.lseek(file_handle, 0, os.SEEK_END)\n\n        if not overwrite:\n            # For Windows replace() was introduced in Python 2.6\n            os.lseek(file_handle, 0, os.SEEK_SET)"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tfile_handle = None\n\n\t\ttry:\n\t\t\tif exists(file_name):\n\t\t\t\tif overwrite:\n\t\t\t\t\tif os.path.isfile(file_name):\n\t\t\t\t\t\tos.remove(file_name) # TODO: Change to a warning\n\t\t\t\t\t\twith open(file_name, 'a') as file_handle:\n\t\t\t\t\t\t\tfile_handle.write(text_buffer)\n\t\t\t\t\telif os.path.isdir(file_name):\n\t\t\t\t\t\tsub_dir = os.path.abspath(file_name)\n\t\t\t\t\t\tsub_file = os.path.relpath(sub_dir,"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\tif os.path.exists(os.path.join('tmp', file_name)):\n\t\t\tos.remove(os.path.join('tmp', file_name))\n\t\tif overwrite == True:\n\t\t\tos.rename(os.path.join('tmp', file_name), os.path.join('tmp', os.path.basename(file_name)))\n\t\n\ttry:\t\n\t\tf = open(os.path.join('tmp', file_name), 'a')\n\texcept Exception as e:\n\t\treturn 1\n\t\n\ttry:\n\t\tf.write("}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\t\"\"\"\n\tWrite to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int\t\"\"\"\n\t\n\n\ttry:\n\t\tfile_in = open(file_name, open_mode)\n\texcept Exception as error_text_in_file:\n\t\tlogger.error(\"append_text_to_file: error while opening file with name %s failed: %s\" % (file_name, error_text_in_file.strerror))\n\t\treturn\n\t\t\n\ttry:\n\t\tbuf = text_buffer\n\t\tif isinstance(buf, str):\n\t\t\tbuf = bytes(buf, encoding=\"utf-8\")\n\t\tfile_in.write(buf)\n\texcept Exception as"}]}
{"task_id": "62b8bbbfe0d34b282c18120f", "prompt": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t", "suggestions": [{"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\ttry:\n\t\t\twith open(\"%s%s\" % (path_to_file, file_name)) as file:\n\t\t\t\treturn file.read()\n\t\texcept IOError:\n\t\t\treturn None\n\telse:\n\t\treturn None\n\ndef file_to_binarybuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (BMP, LZW, etc.), using the specified encoding while reading.\nCAUTION : This will read the whole file in MEMORY.\n:param file_name: File name.\n:type file_name: str\n"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\n\n\ttry:\n\t\tf = open(file_name,\"r\")\n\t\ttext = f.read()\n\t\tf.close()\n\texcept IOError, e:\n\t\tif e.errno == 2:\n\t\t\tprint >>sys.stderr, \"File \"+ file_name + \" was not found.\"\n\t\telse:\n\t\t\tprint >>sys.stderr, \"File \"+file_name + \" was not found.\"\n\t\tprint >>sys.stderr,\"Please verify that you have supplied a correct encoding (utf-8, or unicode-encoded) and that the data you've provided has not been correctly"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\n    t = io.open(file_name, 'rt', encoding)\n    t = t.read()\n    return t\n\t\ndef file_to_bytes(file_name):\t\t#This is the standard wrapper for when you use StringIO,\n    #this is so that you can write a file in binary mode or stdout of\n    with open(file_name, 'rb') as f:\t\n        return f.read()\n\"\"\"\nTests for BabylonX modules.\n\"\"\"\nfrom django.test import TestCase\n\nfrom courseware.tests.factories import UserFactory\nfrom student.tests."}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t return None\n\ndef bytes_to_bytes(nbytes):\n\t\"\"\"\n\tConvert the specified number of bytes (as given by sys.getsizeof() for example) into a buffer.\n\t:param nbytes: Number of bytes consumed.\n:type nbytes: int\n\t:return: A buffer or None in case of error.\n\t:rtype: str\n\t\"\"\"\n\treturn None\n\nimport unicodedata\n\ndef unicode_to_ascii(in_unicode):\n\t\"\"\"\n\tConvert a string from the Unicode database (a bytestring) into\n\ta unicode string (using the"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\t# Note that Python3's string object is used because it has the attribute'read()' and'seek()'.\n\t\tbuffer = None\n\t\tlines = []\n\n\t\t# TODO: Check if line ends or if it has the attribute'readlines()'.\n\t\tfor linenum in range(len(file_name)):\n\t\t\t# TODO: Read non-line lines when reading a file.\n\t\t\tlines.append(next(file_name[linenum:-1].split(\"\\n\")))\n\n\t\t# TODO: Do the whole text buffer. This is just a rough check so that the\n\t\t# file can be"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\t# Open file and read binary file.\n\t\tfile_in = open(file_name, \"r\")\n\t\tbuffer = file_in.read()\n\t\tbuffer_lines = buffer.split(\"\\n\")\n\t\tbuffer_int = len(buffer_lines)\n\t\t# Create text buffer.\n\t\ttext_buffer = []\n\t\tif buffer_int == 2:\n\t\t\t# Text buffer is text only.\n\t\t\tfor txt in range(buffer_int / 2):\n\t\t\t\t# Read one character at a time based on a space.\n\t\t\t\ttext_buffer.append(buffer_lines"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\ttext_buffer_file = codecs.open(file_name, encoding=encoding)\n\t\ttext_buffer = text_buffer_file.read()\n\t\ttext_buffer = text_buffer.strip(encoding)\n\t\treturn text_buffer\n\n\nclass NexusReader:\n\t\"\"\"\n\tReads content from a NEXUS file. Currently supports\n\twriting of multiple formats.\n\t\"\"\"\n\t_filename = \"\"\n\t_opened = False\n\t_opened = False\n\t\n\tdef __init__(self, file_name=\"\"):\n\t\t\"\"\"\n\t\tInitialise an Nex"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\n\n\tglobal _\n\t_\n\timport codecs\n\tassert encoding or _ == 'utf-8'\n\n\tf = codecs.open(file_name, encoding)\n\t_ = f.readlines()\n\tf.close()\n\n\tif _:\n\t\treturn f.decode(encoding)\n\telse: \n\t\treturn None\n\n\n##==============================================================\n## Module Constants\n##==============================================================\n\n# All methods available.\n# To write and read files with this module, simply import from module itself and instantiate the object.\n# See the module's docstrings for details.\n\n\n################################################################################################################################"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\n\t# TODO: Implement real functions\n\ndef text_to_filebuffer(text):\t\t\"\"\"\n\tLoad text toward a file buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n:param text: A text buffer or None in case of error.\n:type text: str\n\t:return: A file buffer or None in case of error.\n:rtype str\t\"\"\"\n\tbuffer = u''\n\t\n\twhile True:\t# Read in loop\n\t\ttry:\n\t\t\tbuffer += input(text"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\t\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str\t\"\"\"\n\t\tif encoding!= None:\n\t\t\tbuffer = file_buffer(file_name, encoding)\n\t\telse:\n\t\t\tbuffer = None\n\n\t\tif buffer!= None:\n\t\t\tbuffer_length = len(buffer)\n\n\t\t\tif buffer_length < 10:\n\t\t\t\tbuffer += '\\n        '\n\n\t\t\tbuffer += buffer[-1]\n\t\t\tbuffer += '\\n'\n\n\t\treturn buffer\n\n\n\tdef get_file_list(self, file_name):\n\t\t\"\"\"\n\t\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\t\tThis function will use"}]}
{"task_id": "62b8bbbce0d34b282c18120d", "prompt": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\tcmd = u'ls \" \" {0}\".format(file_name)\n\t\treturn shell_return(cmd, False).strip()\n\telse:\n\t\treturn shell_return(u'ls'+ file_name).strip()\n\n##########\n# Gateways #\n##########\n\ndef run_command_on_shell(command, timeout=60, shell=True):\n\t\"\"\"\n\tRun an external command on the host in a pexpect session.\n\tThe command may be either a string or a list (e.g. is case insensitive).\n\n\t**Example"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\tpath = os.path.join(__location__, \"files\", file_name)\n\t\treturn os.path.isfile(path)\n\n\ndef generate_path(file_name):\n\t\"\"\"\n\tGenerate directory path.\n:param file_name: File name.\n:type file_name: str\n\t\"\"\"\n\tpath = file_name\n\twhile os.path.exists(path):\n\t\tpath = os.path.join(__location__, \"files\", path)\n\treturn path\n\n\ndef is_directory_file_exist(file_name):\t\t\"\"\"\n"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\n# return error codes\n'''\nerrcode_to_estr = '''\nint error_code=409\nstd::vector<error_code,error_message> estr;\nconst char *errmsg[] = {\\\\x00};\n\n\n/* Get list of strings in file */ \\\\\nstd::string buffer[256];\nstd::vector<string> strbuf;\nstd::vector(const char *buffer[256], const char *len) {\\\\x00,0,0}\nstd::list<string> strlist;\nstd::vector<string> strbuf;"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\treturn os.path.exists(os.path.abspath(file_name))\t\n\ndef get_file_extension(file_name):\t\"\"\"\n\tGet file extension.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: File extension.\n\t:rtype: str\n\t\"\"\"\n\t\treturn (file_name.rsplit('.', 2)[1] or 'txt')\n\ndef get_file_md5(file_name):\t\"\"\"\n\tGet file md5.\n\t:param file_name: File name"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\n\n\tfile_size = os.path.getsize(file_name)\n\tfile_md5 = get_file_md5_from_local_file(file_name)\n\n\tif file_size!= os.path.getsize(file_name):\n\t\treturn False\t\t\t# file is not same size as file_md5\n\n\treturn True\n\t\ndef is_dir(dir_name):\t\"\"\"\n\tCheck if directory exists.\n:param dir_name: Directory name.\n:type dir_name: str\n:return: Return true (exist), false (do"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\tif os.path.isfile(file_name):\n\t\t\treturn True\n\t\telse:\n\t\t\tprint \"Not exist\", file_name\n\t\t\tsys.exit(1)\n\n\ndef get_number_from_line(line):\n\t\"\"\"\n\tGet number from one given line\n:param line: Line to split\n\n\tReturn: Integer\n\t\"\"\"\n\tx = line.split(' ')\n\ttry:\n\t\tif len(x) == 4 and x[0].isdigit():\n\t\t\treturn int(x[0])\n\t\telse:\n\t\t\tprint \"Bad number, not"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\tif os.path.isfile(file_name): return True\n\t\n\t\ttry:\n\t\t\tos.lstat(file_name)\n\t\t\treturn True\n\t\texcept OSError:\n\t\t\treturn False\n\texcept IOError as e1:\n\t\tprint(str(e1))\n\t\treturn False\n\t\n# print_json(\"file_path\", file_path)\n# print_list(file_path)\t\n# print_json(\"file_name\", file_name)\n\n# print(\"path_len\", len(file_path))\n# print_list(file_path)\t"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\tif not check_file_exist(file_name):\n\t\t\treturn false\n\t\telse:\n\t\t\treturn True\n\telse:\n\t\treturn False\n\n\ndef get_file_by_fullpath_pattern(fullpath_pattern):\n\t\"\"\"\n\tRetrieve file by filename pattern\n\t:param fullpath_pattern: File name pattern.\n\t:type fullpath_pattern: str\n\t:return: Return true (exist) or false (do not exist, or invalid file name)\n\t:rtype bool\t\"\"\"\n\tfound_files = []\n\tfor dirpath, dirnames, filenames in os."}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t\treturn os.path.isfile(file_name)\n\t# \u68c0\u67e5\u6587\u4ef6\u540d\u662f\u5426\u5b58\u5728\n\tdef is_file_exist_2(file_name):\t# File existence check\n\t\treturn os.path.isfile(file_name) and os.path.getsize(file_name) > 0\n\t# \u68c0\u67e5\u6587\u4ef6\u540d\u662f\u5426\u5b58\u5728,\u6216\u8005\u6253\u5f00\u6587\u4ef6\u6253\u5f00\n\tdef is_file_exist_3(file_name):\t# File existence check\n\t\treturn os.path.isfile(file_"}, {"generated_text": "def is_file_exist(file_name):\t\"\"\"\n\tCheck if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\t\"\"\"\n\t # noinspection PyBroadException\n\t\tif re.match(r\"^[\\\\/\\.]\", file_name): return True\n\t\telse: return False\n\telse:\t\t\t\t\t\t\t\t\t\t\t\t\t\t# file name must not end with a period.\n\t\treturn False\n\ndef is_not_empty_string(str):\n\t\"\"\"\n\tCheck if string is empty.\n:param str: Input string.\n:type str: str.\n\t:return Boolean\n\t\"\"\"\n\t\n\tif not str: return True\n\tis_empty = False\n\tnot_empty = False\n\tfor i in str"}]}
{"task_id": "62b8b99de0d34b282c1811f8", "prompt": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\n\n# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    Copyright (C) 2014 Pexego Sistemas Inform\u00e1ticos All Rights Reserved\n#    $Marta V\u00e1zquez Rodr\u00edguez$ <marta@pexego.es>\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as published\n#    by the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tcls.logger = logging.getLogger(cls.__name__)\n\t\tcls.log(\"Restoring log output\", \"DEBUG\")\n\t\tcls.reset()\n\t\t\n\tlogger = _reset_logging\n\tlogger.info(\"Log output cleared\")\n\n\tdef __init__(self, logfile, level = 'info', format = '%(asctime)s: %(filename)s[line:%(lineno)d] %(levelname)s - %(message)s'):\n\t\tlogging.Handler.__init__(self, level, format)\n\t\tself.streamLogger = StreamLogger(self.stream) \n\t\tself.logger."}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\treturn frappe.connect()\n\ndef get_bench_path(*path_parts):\n\t\"\"\"\n\t:type path_parts: Iterable[str]\n\t:rtype: str\n\t\"\"\"\n\tpath = os.sep.join(path_parts[:])\n\tif path.startswith(\"/var/lib/\"):\n\t\tpath = path[5:]\n\treturn path\n\ndef get_bench_path_prefix():\n\t\"\"\"\n\t:rtype: str\n\t\"\"\"\n\treturn get_bench_path(bench_path_prefix, \"lib\")\n\ndef get_"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tlogging.handlers.TimedRotatingFileHandler.active = False\n\tlogging.captureWarnings(True)\n\n\tcls.logger.handlers = []\n\tcls.logger.setLevel(logging.CRITICAL)\n\tif os.getenv('OPENCV_DARK'):\n\t\tos.putenv('OPENCV_DARK', 'True')\n\n\tif os.getenv(\"OPENCV_DARK\"):\n\t\tos.environ['OPENCV_DARK'] = 'False'\n\n\tlog.startLoggingWithObserver(cls.emit)\n\n\nclass LogManagerConfig (object):\n"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tcls.log.handlers[0].flush()\n\t\tcls.log.handlers[1].flush()\n\t\tcls.log.root.removeHandler(cls)\n\n\t@classmethod\n\tdef _log_level(cls, level):\n\t\t\"\"\"\n\t\tGet log level\t\n\t\t:param level: string\n\t\t:return: list\n\t\t\"\"\"\n\t\tif level == 'info':\n\t\t\tlog_dir = cfg['log_dir']\n\t\t\tlevel_log = log_dir + '/debug.log'\n\t\t\treturn open(level_log, 'r').readlines()\n\t\t"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\t# remove handlers to force writing\n\t\tfor hdlr in list(_HANDLER)[:]:\n\t\t\ttry:\n\t\t\t\thdlr(None, None, _HANDLER)\n\t\t\t\tdebug(\"Flushing logging\")\n\t\t\texcept:\n\t\t\t\tpass\n\n\t@classmethod\n\tdef _create_console_logger(cls):\n\t\treturn \"pyconsolelogger%s\" % cls.__name__\n\n\t@classmethod\n\tdef _create_loghandler(cls, *args, **kwargs):\n\t\thandler = _StreamHandler(sys.stdout)\n\t\thandler.setFormatter(logging.Formatter(\"[%(process)s]"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tlogger.handlers = []\n\t\tformatter = logging.Formatter(\"%(asctime)-15s [%(name)s] [%(levelname)-8s %(message)s] \"\n\t\t\t\t\t\t\t'%(lineno)s: %(funcName)s %(message)s')\n\t\tif os.path.isfile(LOGFILE) is True:\n\t\t\tformatter = logging.Formatter(\" \".join([\"%(filename)s[line:%(lineno)d] %(levelname)s %(message)s\"])\n\t\t\t\tif os.path.isfile(LOGFILE) is True:\n\t\t\t\t\thandler = logging.FileHandler(os.path.dirname"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tcls.log_level = 10\t\t\t\t\t# Set log level\n\t\tcls._logger = None\t\t\t\t\t# Force reset in order to re-start of module\n\tdef reset(self):\n\t\tself._logger = None\t\t\t# Reset logger for debug and warning messages\n\t\tself._logger_lock = False\t\t\t# Set logger lock\n\t\tself._logger_file = None\t\t# Set logger file to \"elog\"\n\t\tself._logger_level = 1\t\t\t\t\t# Enable logger logging for debug/warning messages\n\t\tself._logger_lock = threading.RLock() \t\t#"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tlogging.getLogger(\"\").setLevel(logging.DEBUG)\n\t\tlogging.getLogger(\"\").addHandler(logging.NullHandler())\n\n\t\tfor handler in logging.root.handlers:\n\t\t\ttry:\n\t\t\t\tremove_handler(handler)\n\t\t\texcept ValueError:\n\t\t\t\tpass\n\nif __name__ == '__main__':\n\n\tfrom optparse import OptionParser\n\tparser = OptionParser(version=\"0.65\")\n\tparser.add_option(\"-d\", \"--debug\", dest=\"debug\", help=\"Enable debug (default: False)\")\n\tparser.add_option(\"-q\", \"--quiet\", dest=\"quiet\")\n\t"}, {"generated_text": "def _reset_logging(cls):\t\"\"\"\n\tReset\t\"\"\"\n\t\tlogging.addLevelName(logging.ERROR, 'TRACE')\n\n\t@classmethod\n\t@asyncio.coroutine\n\tdef shutdown_handlers(cls, signal, frame):\t\n\t\t\"\"\"\n\t\tA function that stops the program when the program is shutting down. This method does NOT check to see if the process hasn't already killed, or if it was killed by a signal or an unexpected error.\n\t\t\n\t\t:param signal: The signal object.\n\t\t:param frame: The frame. This is passed as the last frame sent to the application.\n\t\t:return:\n\t\t\"\"\"\n\t\tif not"}]}
{"task_id": "62b8b59feb7e40a82d2d1291", "prompt": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn \"Python\"\n\tdef getTypesFromDescriptor(self, typeDescriptor):\n\t\t\"\"\"\n\t\tReturn the type descriptors, for example\n\t\ttypes.typeInfo(), typeInfos().\n\t\t\"\"\"\n\t\treturn []\n\tdef getDescriptor(self, typeDescriptor):\n\t\t\"\"\"\n\t\tReturns the object descriptor for the specified type,\n\t\tor None if the type isn't supported by this type.\n\t\t\"\"\"\n\t\treturn TypeInfo(\n\t\t    class_=self._getTargetClass(),\n\t\t    type_alias = typeDescriptor.identifier)\n\nbuiltinFunctions['_getTargetClass'] = _getTargetClass"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn _pyTarget_\n\tdef _getTargetClassNoClobber(self):\n\t\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn _fallbackTarget_\n\ndef getTarget():\n\t\"\"\"\n\tReturn a Target object usable for this platform, or any\n\texisting one. The 'Py' or 'Fallback' suffix, if it exists,\n\tbeing it will use that instead. The 'native' suffix is the\n\tplatform specific suffix and 'py'.\n\t\"\"\"\n\tif WIN"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\t#return _getUnexpandedClassFor(self.mod)\n\t\tif 'Py' in self.type and hasattr(self,'mod') \\\n\t\tand self.mod in ('Py', 'Fallback'):\n\t\t\treturn self.mod[3:-3][1:]\n\n\n#===================================================================================================\n# ModuleFinder\n#===================================================================================================\nclass BuiltinNodeFinder(FindSpecFor):\n\t\"\"\"\n\tThis class exists solely to say, that 'from Builtins' can be found anywhere.\n\tTo search for things without a specific name (for example, the\n\tavailable names in Builtin."}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn self.impl\n# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\n\t\n\t#def _checkImplementation(self):\n\t#\t\"\"\"\n\t#\tReturn true if the implementation is implemented at this stage\n\t#\t:rtype: Boolean\n\t#\t:return: True if the implementation implementation is implemented\n\tdef _needToBuild(self, alreadyOkayToBuild, nextBuild):\n\t\t\"Determine whether the build of this type will be built.\"\n\t\t\n\t\t# Ignore the next two if alreadyOkayToBuild == None\n\t\tif not alreadyOkayToBuild:\n\t\t\t# Check each of the current build dependencies.\n\t\t\tdone = False\n\t\t\tfor"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn self.getInternal().getClassLoader()\n\n\t# ---------\n\t# Functions\n\t# ---------\n\n\tdef __reduce__(self):\n\t\treturn (_unpickleMethodType, (self.__name__,))\n\n\tdef __deepcopy__(self, memo):\n\t\tif \"__new__\" in self.__dict__:\n\t\t\tresult = copy.deepcopy(self.__dict__['__new__']).copy()\n\t\t\tmemo[id(self)] = result\n\t\telif hasattr(self.__class__, \"__deepcopy__\"):\n\t\t\tresult = copy.deepcopy(self.__class__.__deepcopy__(memo))\n\t\t\tmemo"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn 'Py'\n\n\tdef _findTarget(self, name):\n\t\t\"\"\"\n\t\tGiven 'name' returns the 'name' attribute of\n\t\tthe target that may or may not exist.\n\t\t\"\"\"\n\t\ttry:\n\t\t\ttarget_name = self._targets[name]\n\t\texcept KeyError:\n\t\t\t(className, _sep, suffix) = name.rpartition('.')\n\t\t\t(self._targets[name], self._sep, _) = (\n\t\t\t\tsuper(DefaultTarget, self).__getattribute__(className))\n\t\t\ttarget = defaultTarget(name, className)\n\t\t\tself"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\treturn None\n\n# The following functions are needed for generation of the test method\n# for the generated implementation file.\n\n\ndef _generateTestCases(testPrefix, namespace, testName):\n\t\"\"\"\n\tGenerate C source files for a source file testing a particular\n\tand the list of tests.\n\n\t@param testPrefix name of module/test to generate\n\t@param namespace namespace to use for generator\n\t@param testName name of generated test\n\t\"\"\"\n\n\t# Generate source code\n\timport os.path\n\tsourceFileName = os.path.join(testPrefix, testName"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\tif not self.is_safe_value(self.get()):\t# XXX\n\t\t\traise AttributeError, \"self.get(...) should return a function\"\n\n\t\tif self.is_constructor:\n\t\t\tself.is_special_cased = False\n\t\telse:\n\t\t\text = self.base_type.resolve(self.base, self.prefix)[1]\n\t\t\tif ext is not None:\n\t\t\t\twhile True:\n\t\t\t\t\tc = self.get_next_char()\n\t\t\t\t\tif c!=\"\\\"\":\n\t\t\t\t\t\tbreak\n\t\t\t\t\tself.set_prev_char(c)\n\t\t\t\t\t"}, {"generated_text": "def _getTargetClass(self):\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\t\"\"\"\n\t\t# See the comment by __setup__.py for detailed info.\n\t\tif sys.flags.optimize >= 2:\n\t\t\treturn self._optimizeMethod\n\n\t\treturn self._optimizeMethodOrig\n\n\t## Get the module name of the base class.\n\t@staticmethod\n\tdef getModule(modObject, baseClassName):\n\t\t\"\"\"\n\t\tReturn the module object corresponding to the\n\t\tactual base class found in the given module\n\t\tfrom the given module module.\n\t\t\"\"\"\n\t\tmoduleName = modObject.getBase()[baseClassName] if baseClassName in modObject else moduleObject[len(mod"}]}
{"task_id": "62b8b590eb7e40a82d2d1275", "prompt": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\tnewOrderings = []\n\n\t\tfor order in orderings:\n\t\t\tif order[0][0] == '-':\n\t\t\t\tnewOrderings.append(order)\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tnewOrderings.index(order[0])\n\t\t\t\texcept ValueError: # orderings is not an integer index\n\n#####\n# Global configuration\n#\n\n_global_configPath = os.path.join(os.getcwd(),\"global\", \"config\")\n\n_localAppBaseConfigPath = os.path.join(os.getcwd(),\"server\", \"main\",\"config\")"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n\n\tout = []\n\tfor k, orders in itertools.groupby(orderings, lambda x: x[0]):\n\t\tout.append(dict((k, orders)) )\n\t\n\treturn out\n\ndef _isInOrderDict(d):\n\treturn (d in globals())\n\ndef _dictFromDicts(d):\n\tinlist = {}\n\tfor dk, dv in d.iteritems():\n\t\tif dv:\n\t\t\tinlist[dk] = dv\n\treturn inlist\n\ndef _check(d, keys):\n\t# This function assumes _checkKey"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\tn = len(orderings)\n\t\ti = 1\n\t\twhile i < n:\n\t\t\tcur = orderings[i]\n\t\t\tnext = orderings[i + 1]\n\t\t\tif prev == next: # orderings changed\n\t\t\t\torderings[i + 1] = next\n\t\t\t\tdel orderings[i + 1]\n\t\t\telse:\n\t\t\t\torderings[i] = cur\n\t\t\t\ti += 2\n\t\treturn i + 1\n\t\t\n\n\tdef _mergeByPrefix(self, prefix, newOrderings):\n\t\tret = {}\n\t\tfor item in newOrderings:\n\t\t"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n\t\n\tneworderings = {}\n\tfor o in orderings:\n\t\tif o in ['x',]:\n\t\t\tneworderings[o] = neworderings.get(o,'z') + neworderings.get(o,'y')\n\t\telse:\n\t\t\tneworderings[o] = neworderings.get(o.upper(),neworderings.get(o))\t\n\t# Ensure that the suffix ends in order and\n\t# the type of the neworderings argument matches and that\n\t# the ordering isn't reversible\n\tif orderings:\n\t\t"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\tresult = []\n\t\tfor i in range(len(orderings) - 1):\n\t\t\tif not orderings[i]:\n\t\t\t\tresult += [orderings[i]]\n\t\t\telse:\n\t\t\t\ttemp = [n + (orderings[i + 1],)]\n\t\t\t\tif len(temp) == orderings[i + 1]:\n\t\t\t\t\tprint len(temp)\n\t\t\t\telse:\n\t\t\t\t\ttemp += [orderings[i]]\n\t\t\t\t\tresult += temp\n\t\treturn result\n\treturn _reorder_orderings(orderings)\n\ndef mergeOrderingLists(orders):\n\tif"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t # XXX - if you add an explicit attribute, add one more\n\t# attribute\n\t# XXX - there can be only one order set per object\n\tkeys = orderings[0].data.keys()\n\tnames = []\n\tfor key in keys:\n\t\tnames.append(key)\n\t\tif key[0]!= '_':\n\t\t\tnames.append(key[:-1])\n\tif len(names):\n\t\tnames.sort()\n\t\tfor name in names:\n\t\t\tif name[0] == '_':\n\t\t\t\tname = name[1:]\n\t\t\torderings[0"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\to = self\n\t\tif __debug__:\n\t\t\ttrace.mutter(\"merging orderers and ordering triples\")\n\t\tnum, den = [], []\n\t\tfor oi in orderings:\n\t\t\tfor name in so.names(oi):\n\t\t\t\ttry:\n\t\t\t\t\tnum.append(name)\n\t\t\t\texcept AttributeError:\n\t\t\t\t\t# It's a string\n\t\t\t\t\tnum.append(name)\n\n\t\t\tif num:\n\t\t\t\tden.append((oi, num))\n\n\t\tif __debug__:\n\t\t\ttrace.mutter(\"merging in order: %s\" % (\", \".join(den"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\t# FIXME: This is a work-around for the problem that\n\t\t#       both ordered and non-ordered version use different\n\t\t#       ways of sorting this code. Maybe we should be checking\n\t\t#       each of them together by checking if this is the\n\t\t#       default order.\n\t\t#\n\t\t#      ---> A + B --> C\n\t\t#\t    ----> x --\n        #    <---> x --    B --> D\n\t#\t    ----> z --\n        #\n\t\t#        If you want to do so, it also means we don't have\n\t"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\n\n\t_mergeOrderings([], orderings)\n\t\n        \nfrom django.test import TestCase\nfrom django.core.management import call_command\n\nfrom data.models import Group, TileCategory\nfrom jobs.models import Job\n\n\nclass TestJobCreationJob(TestCase):\n    fixtures = ['groups', 'job_classes']\n\n    def _make_job(self, _job_cls, _description=\"Description 1\",\n                   _level=0, _order=0, _job_template='test_template'):\n\n        job = Job.objects.create(\n            description=_description,\n            order=_order"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\t\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']\t\"\"\"\n\t\tfirstKey, restKey = orderings[0]\n\t\tif firstKey is None:\n\t\t\treturn\n\n\t\tfirstKey = [firstKey]\n\t\trestKey = [restKey]\n\t\twhile restKey:\n\t\t\trestKey = restKey[0] + [restKey[1] + restKey[2]]\n\n\t\t# Try to merge that object to a common order\n\t\totherKey = copy.deepcopy(orderings[1])\n\t\tif (firstKey == None and otherKey == None):\n\t\t\treturn\n\t\tif firstKey == restKey:\n\t\t\tnewKey = range"}]}
{"task_id": "62b8b58deb7e40a82d2d1269", "prompt": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\treturn interfaces.IDeclaration.providedBy(object)\n\n\tdef providedBy(self, name):\n\t\tif interfaces.IDeclaration.providedBy(self, name):\n\t\t\treturn ZopeInterface(self.context, interfaces.IInterface)\n\n\nclass ImplementationInterface(object):\n\n\tdef providedBy(self, cls):\n\t\tif not interfaces.IDeclaration.providedBy(cls, ZopeInterface):\n\t\t\traise ZopeInterfaceError(\"Infinite implemenation\")\n\n\tdef __init__(self, context, iface):\n\t\tself.context = context\n\t\tself.iface ="}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\treturn getTypesFromDescriptor(object, '__provides__', {}) or []\n\treturn ()\n\ndef implementedBy(object):\n\t\"\"\"Get interfaces directly provided by the given object\n\nThe *object* is inspected.\n\nThis attribute is read-only.\n\nReturns a pair where the first object in the returned sequence is a list\nconforming their respective interfaces, and the second object returned by\nthe returned sequence is wrapped in a tuple. See the documentation on\nthe methods in the interface module for more information. \"\"\"\n\t\t\n\timplements = lambda object: \\\n\t\t# interfaces that were provided"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\tpass\n\n\tdef __init__(self, object):\n\t\tself.providedBy = None\n\t\t\"\"\":py:attr:`~zope.interface.interfaces.IDeclaration.providedBy` will contain the specifications directly declared by this descriptor. See self.__constructedBy`.\"\"\"\n\t\tself.isConstructed = False\n\t\t\"\"\":py:attr:`~zope.interface.interfaces.IDeclaration.providedBy` will contain the specifications directly declared by this descriptor. See self.__constructedBy`.\"\"\"\n\t\tself.providedByStringInterp_ = None\n\t\t\"\"\":py:attr:`~zope."}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\tifaceName = self._proxyAttr(object).split('.')\n\t\tif len(ifaceName)!= 1:\n\t\t\t# interface was specified as first argument\n\t\t\traise ValueError(\"object %s has no leading ':'\", object)\n\t\tiface = self._proxyAttr(object.im_self)\n\t\treturn iface.providedBy(ifaceName[0])\n\n\tdef __repr__(self):\n\t\treturn self.__class__.__name__+repr(self._proxyAttr(self._obj))\n\n\tdef __reduce__(self):\n\t\treturn self.__class__, (self._obj,))\n\n\nclass"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\n\t    if not isinstance(object, (list, tuple)): # pylint:disable=redefined-outer-name\t\n\t\treturn [object]\n\t\n\t    iface = object[0]\n\t    while iface!= \"xpt\": # pylint:disable=redefined-builtin\t\n\t    iface = string.split(iface, '/')\n\t    return [iface]\n\n\t   \n\n#\n# ISerializer\n\n\nclass ISerializer(Interface):\n    \"\"\"\n    Interface representing L{schema.ISerializer}.\n    \"\"\"\n    def get(context, obj):\n        \"\"\"\n        Get a schema (instance, link-attribute-mapping) representation of\n        an instance"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\n\t\n\ttry:\n\t\tiface = inspect.unwrap(getattr(inspect, object.name, None))\n\texcept AttributeError:\n\t\ttry:\n\t\t\tiface = inspect.signature(getattr(inspect, object.name, None))\n\t\texcept (TypeError, AttributeError):\n\t\t\traise ValueError(\"A spec was inspected or no signature.\")\t\n\treturn list(iface[1])\n\t\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n#\n# Copyright (C) 2012-2013 University of Dundee & Open Microscopy Environment.\n# All rights reserved"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\treturn interfaces.IDeclaration.providedBy(object)\n\ndef nameFromDescriptor(descriptor, interface=None): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the name of the interface given by the object\n\tof the given descriptor.\n\t:param descriptor: an interface object, either a\n  fully-qualified name, or an object implementing the object interface,\n  prefixed with a colon, e.g. 'dbus.service.BusName' or\n  'com.example.myService'\n\t:param interface: if specified, the return value of this\n      method.\n\t:"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\treturn zope.interface.declarations.providedBy(object)\n\n##----------------------------------------------------------------##\n# Utility method implementations for testing\n##----------------------------------------------------------------##\ndef test_implementsImplements(proxy, klass): # pylint:disable=redefined-builtin,unused-argument\n\t\"\"\"\n\tImplement the given class with an implementation of a target interface.\n\t@param proxy: A proxy object for which to call the method.\n\t@param klass: A class for which to return a non-implementation specific type\n\t             if available (e.g. ``zeit.content.annotation_iface``).\n\t@return: A"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\traise NotImplementedError(\"providedBy is not implemented.\")\n\n\tdef providedBy(self, obj): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object,\neven if the underlying interfaces are not the same.  For example, some implementations\n\tmay not be able to tell whether an object includes or implements some of them.\n\n\tFor example, this allows the declaration of an interface implemented in one of\n\tits interfaces.\n\n\t.. code-block:: python\n\t   associatedInterface = directlyProvidedBy('object')\n\n\t.. code-block:: python\n\t   associatedInterface"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\t\"\"\"\n\t\tpass\n\n\nclass Unresolvable(Interface):\n\t\"\"\"\n\tA descriptor which means something is immutable (by calling\n\t``__getitem__``) into a transparent proxy.\n\t\"\"\"\n\n\t# IInterface implementation\n\n\tdef getDescriptor(self, name): # pylint:disable=redefined-builtin\n\t\t\"\"\"Get the appropriate descriptor for a given interface name\n\t\t\"\"\"\n\t\t\"\"\"\n\t\tif not name: # pylint:disable=redefined-builtin\n\t\t\treturn\n\t\t# this try/except is needed for old-style classes like xerces\n\t\tiface = getattr(self, name)"}]}
{"task_id": "62b8b559eb7e40a82d2d11f8", "prompt": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t", "suggestions": [{"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tminimalBases = []\n\t\tfor class_ in classes:\n\t\t\tif not isinstance(class_, class_):\n\t\t\t\tclass_ = class_.__bases__[0]\n\t\t\tminimalBases.append(class_)\n\t\treturn minimalBases\n\treturn reduce(lambda baseTuple ): reduce(baseTuple[1], minimalBases)\n\n#TODO remove singleton functions, they should be renamed here, or we get this elsewhere?\n\ndef extractClassName(classname):\n\t\"\"\"\n\tGiven a CIM class name, determine (1) by looking up a class hierarchy;\n\tif there isn"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tminL = {}\n\t\tmaxL = {}\n\t\tfor classId in classes:\n\t\t\tassert classId not in minL\n\t\t\tminL[classId] = 0.0\n\t\t\tmaxL[classId] = 100.0\n\t\t\tfor maxValue in classId.properties:\n\t\t\t\tfor baseId in classId.properties[maxValue].bases:\n\t\t\t\t\tif baseId in minL[classId]:\n\t\t\t\t\t\tminL[classId][baseId] = minL[classId][baseId] + maxValue\n\t\t\t\t\telif maxValue in maxL[classId]:\n\t\t\t\t\t"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tret = collections.OrderedDict()\n\t\tfor base in classes:\n\t\t\tret[base.__name__] = base\n\t\treturn ret\n\t\n\tdef __addSortOrder(classes, sortOrder):\n\t\t\"\"\"\n\t\tRecursively add sortOrder to categories within classes\n\t\t(e.g. _.a: sorted(set(classes).intersection(set(categories))))\n\t\t\"\"\"\n\t\tret = collections.OrderedDict()\n\t\tfor base in sortOrder:\n\t\t\tif base.__name__ not in classes:\n\t\t\t\tcontinue\n\t\t\tret[base.__name__] = sorted(classes)\n\t\treturn ret"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tif len(classes) == 0:\n\t\t\treturn [0]\n\t\telse:\n\t\t\tif max(classes) == 1:\t\t\t\n\t\t\t\treturn [max(classes)]\n\t\t\telse:\n\t\t\t\treturn max(classes)\n\tdef denormalize(classes):\n\t\t\"\"\"\n\t\tReduce a list of base classes to its unique base equivalent\t\n\t\t\"\"\"\n\t\tuniqueClassCount = Counter(classes)\n\t\tif len(classes)!= uniqueClassCount[0]:\n\t\t\traise IndexError('Could not compute a unique base equivalent: {}'.format(classes))\n\t\treturn [x for x in classes"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\n    bases = [c for c in classes if c.baseClass == -1 and c.min == -1]\n    clsToMin = {c.baseClass: c for c in bases}\n    if len(bases) == 0 or not classes:\n\treturn minBasesToClass(clsToMin)\n    for base in bases:\n        minBasesToClass(base.baseClass).minimum = minBasesToClass(base.baseClass)\n    return minBasesToClass(classes)\n    \nclass ClassProperty(property):\n    \"\"\"\n    A property that specifies a class and a property"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tordered = []\t\n\t\tmaxValue = [max(c.value for c in classes)]\n\t\tfor value in maxValue:\n\t\t\tfor n in range (2, len (classes)):\n\t\t\t\tbaseClass = classes[n]\n\t\t\t\tmaxValue[n] = max(maxValue[n], baseClass.value + value) \n\t\t\t\tordered.append ( value )\n\t\t\t\t\n\t\treturn sorted ( ordered, key=totalLen)[-1]\n\n\n\n\n#===============================================================================\n#\n#===============================================================================\n#\n# Chemkin Algorithm\n#\n#===============================================================================\n\nclass Chemkin(object"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tl = {}\n\t\tfor c in classes:\n\t\t\tl.setdefault(c,[]).append(min(l[c]))\n\t\treturn l\n\t\t\t\n\t\t\n\t# Sort the classes by class name\n\tdef sorted(classes):\t\n\t\tl = {}\n\t\tfor c in classes:\n\t\t\tl.setdefault(c,[]).sort(key=functools.cmp_to_key(c))\n\t\treturn l;\n\t\n\t# Get all the possible classes\n\tclasses = {}\n\tclasses = set(minimalBases(classes));\n\tfor ctrDict in ctrCache:\n\t\t"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tdef lowest(cls):\n\t\t\treturn max(cls, key = lambda _: getattr(opts, cls.priority)) * 2.0\n\t\t# First take care of the bases\n\t\treturn [ x[1] for x in lowest(classes[1:]) ]\n\t\n\tclass MinAverageBase(object):\n\t\t\"\"\"\n\t\tBase class of minified set of classes.\n\t\t\"\"\"\n\t\tpriority = 5.0\n\n\tclass HighestAverageBase(object):\n\t\t\"\"\"\n\t\tClass of maxified set of classes.\n\t\t\"\"\"\n\t\tpriority = 3.0\n\n\tclass AverageBase"}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tbases = []\n\t\tc = 0\n\t\twhile len(bases) < (len(classes) - 1):\n\t\t\tremainder = bases[-1 : (len(classes) - 1)]\n\t\t\tbases.append(remainder)\n\t\t\tc += 1\n\t\treturn bases\n\n\tdef rebaseAll(bases):\n\t\t\"\"\"\n\t\tReduce the list of bases back to its minimum equivalent\t\"\"\"\n\t\tfor parent in bases:\n\t\t\tif parent >= 0:\n\t\t\t\tparent_bases = minimalBases(parent)\n\t\t\t\tfor child in parent_bases:\n\t\t\t\t\tnewVal = minimalBases("}, {"generated_text": "def minimalBases(classes):\t\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\t\"\"\"\n\t\tsorted = {}\n\t\tbases = []\n\t\tfor base in classes:\n\t\t\tx = classToBase( base )\n\t\t\tif x not in sorted:\n\t\t\t\tsorted[x] = base\n\t\t\tbases.append(x)\n\t\tfor x in bases:\n\t\t\twhile x in classes:\n\t\t\t\t#print \"base:\", x\n\t\t\t\tx = classes[x]\n\t\t\t\tclasses.remove(x)\n\t\tnum = len(classes)\n\t\ti = 0\n\t\twhile i < num:\n\t\t\tprint \"%d %s\" % (i, ','.join(sorted.get(x"}]}
{"task_id": "62b8b4b9eb7e40a82d2d1134", "prompt": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\treturn [a.name for a in self._descriptor.interface.members]\n\n\tdef getDescription(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tGet the description of this interface or descriptor.\n\n\t@param all: If true, return all names, including nested descriptors.\n\t@type all: bool\n\t@return: list of attribute names and descriptions\n\t@rtype: [str]\n\t\"\"\"\n\t\tret = []\n\t\tfor i in self._descriptor.interface.namedDescriptors:\n\t\t\tif not i.interface.isCallback():\n\t\t\t"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\tret = []\n\t\tattrib, descName = {}, {}\n\t\tfor iface in self.ifaces.keys():\n\t\t\tself.load_resource_helper(iface)\n\t\t\tinterface = self.get_interface(iface)\n\t\t\tif interface:\n\t\t\t\tattrib = copy.copy(attrib)\n\t\t\t\tdescName = descName.get(iface, {}).get(\"error_page\", \"Not found\").strip()\n\t\t\t\tret.append((attrib, descName))\n\t\t\t\tattrib, descName = {}, descName[\"value\"], descName[\"description\"], {}\n\t\t\telse:\n\t\t\t\tret.append"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\tresult = set(\n\t\t\t(name_, self.getExtendedDescription(name_)) \\\n\t\t\tfor name_ in dir((self.__class__, self.__name__))\n\t\t)\n\t\tif all and name_ in result: return (tuple(result))\n\t\telse: return ()\n\n\t#\n\t# Attribute setters\n\t#\n\n\tdef setNames(self, names, descriptions): # pylint: disable=redefined-builtin\n\t\t\"\"\"\n\t\tInitialize the attribute names and descriptions, by filtering\n\t\tthe interface types, and adding that to those names.\n\t\t\n\t\t@param names {iterable} names"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\tallAttrs = self._attributes\n\t\tattribNames = self._attributesNS\n\t\tif all: return allAttrs\n\t\telse: return attribNames\n\n\tdef resolve(self, definitions, definitionsNS={}): # pylint:disable=redefined-builtin\t@deprecated\n\t\"\"\"\n\t\tParse the XML < definitions, definition namespaces and return the results.\t\t\n\t\tThe definitions dictionary is taken from the XML file named 'definitions' (relative to the directory where the file was\n\t\tincluded and the definition is local to the definition's local namespace table).\t\t\t\n\t\tReturn a dictionary (or None) if no definition could be"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\treturn self.__class__._namesAndDescriptions(self.__iface, all)[:]\n\tdef __iter__(self):\n\t\t\"\"\"Iterate over all names and descriptions defined by interface.\"\"\"\n\t\treturn self.__class__._namesAndDescriptions(self.__iface)\n\n\tdef __repr__(self):\n\t\treturn \"<%s.%s \\\"%s\\\">\" % (self.__class__.__module__, self.__class__.__name__,\n\t\t\t\t\t\t\t\trepr(self.__dict__),\n\t\t\t\t\t\t\t\t)\n\tdef __getattr__(self, name):\n\t\ttry:\n\t\t\treturn self.__dict__[name]\n\t\t"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\tret = []\n\t\tfor item in self._interface.namesAndDescriptions():\n\t\t\tret.append( item.name )\n\t\t\tret.append( item )\n\t\tif all:\n\t\t\tret = tuple(ret)\n\t\tif self._has_private:\n\t\t\tret = tuple(ret)\n\t\treturn ret\n\tdef __len__(self):\n\t\treturn len(self._interface) + len(self._description)\n\n\tdef __str__(self):\n\t\treturn self.name or None\n\ndef str_replace(astring, *args):\n\t\"\"\"\n\t"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\treturn self.collect()\n\n\tdef collect(self): # pylint:disable=redefined-builtin\t\"\"\"\n\tCollects all registered typeclasses in a module.  For\n\texample, Module.foo bar instead of Module.bar.\n\tIf the name is defined more than once, then use its\n\tbase name.\n\n\t@return: Returns (names, description), where names is the list of\n\t\ttypeclass names and descriptions is a list containing a pair\n\t\twith the name as the first element and the description\n\t\tas the second element.\n\t@rtype: list\n\t\"\"\""}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\tret = {}\n\t\tfor c in _ns_attrs(self):\n\t\t\tif c._interface._name is not None:\n\t\t\t\tret[c._name] = c\n\t\treturn ret\n\t\t\n\tdef setParent(self, parent):\n\t\tassert self._parent is not None\t\t\n\t\tself._parent = parent\t\t\t\n\t\t\n\tdef setSubInterface(self, interface):\n\t\tassert self._interface is not None\t\t\n\t\tself._interface = interface\t\t\t\t\n\t\t\n\tdef removeInterface(self):\n\t\tassert self._interface is not None\t\t\n\t\tself._interface.releaseInterface()\n\t\t\t\n"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\tresult = {}\n\t\tfor interface in self.ifaces:\n\t\t\tif len(interface.attributes)!= 0:\n\t\t\t\tresult[interface.identifier()] = self.addDefaultAttributes(interface.attributes)\t\n\t\treturn result\n\t\t\n\tdef addDefaultAttributes(self, attributes):\n\t\tattributes = attributes.copy() # make copy to avoid modifying the original\n\t\t\n\t\t# create the interfaces\n\t\tinterfaces = self.interfaceIndexes\n\t\tif not self.isAbstractInterface:\n\t\t\tinterfaces |= self.interfaceHierarchy\n\n\t\tfor interface in interfaces:\n\t\t\tcurrent = interface.identifier()\n\t\t\tif"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn attribute names and descriptions defined by interface.\t\"\"\"\n\t\telts = []\n\t\tfor c in self.childNodes:\n\t\t\tif (hasattr(c, \"namespaceURI\") and c.namespaceURI==\"xmlns\") or \\\n\t\t\t   (hasattr(c, \"localName\") and c.localName==\"prototype\"):\n\t\t\t\telts.append((c.namespaceURI, c.localName))\n\t\tif getattr(self, 'attributes', None):\n\t\t\tfor a in self.attributes.values():\n\t\t\t\ta.unlink()\n\t\treturn elts\n\n\tdef __getitem__(self, name):\n\t\treturn self.childNodes[name].nodeValue\n\n\tdef hasChildNodes("}]}
{"task_id": "62b8b416eb7e40a82d2d1129", "prompt": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn self._attrNames\n\n\tdef attr(self, name, value=None):\n\t\t\"\"\"\n\t\tReturn a callable from the most common case and a 'normal' attribute value.\n\t\t\"\"\"\n\t\treturn getattr(self, '_' + name)\n\n\tdef _addAttr(self, attr):\n\t\treturn lambda *args: self._attrNames.append(attr(*args))\n\n\tdef addAttr(self, attr, function, *args, **kwargs): # pylint: disable=redefined-builtin\n\t\t\"\"\"\n\t\tAdd a new attribute to the currently defined attribute of the wrapped function."}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\tif not all:\n\t\t\treturn list(set(_names(i) for i in dir(self) if not i.startswith('__')))\n\t\telse:\n\t\t\treturn [x for x in self if x[0].startswith('__')]\n\n\tdef setns(self, namespace):\n\t\t\"\"\"Set the namespace. Note that this updates the XML and replaces elements of\n\t\t\tthe namespace with the given name.\"\"\"\n\t\tif self._ns is not None:\n\t\t\tself.__dict__[\"_ns\"] = namespace\n\n\tdef _undefine(self, ns):\n\t\tif not ns:\n\t\t\treturn"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn self.__attributes.keys()\n\n\tdef get_attributes(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute dictionary of the interface.\t\"\"\"\n\t\treturn self.__attributes\n\n\tdef get_attribute(self, name): # pylint:disable=redefined-builtin\t@ReservedAssignment\n\t\ttry:\n\t\t\treturn self.__attributes[name]\n\t\texcept KeyError:\n\t\t\traise AttributeError(name)\n\n\tdef set_all(self, all=False):\n\t\t\"\"\"Set all attributes\"\"\"\n\t\tfor attr in self.__attributes.itervalues():"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn list(self._dict_view._dict.keys())\n\tdef __str__(self):\n\t\t# __str__ has to be implemented for the interface so it can be printed in humanreadable form\n\t\treturn str(list(self._dict_view(sorted(self.keys()))).split(\"\\n\")),\n\n\n\t\"\"\"\n\tReturn the list of attributes\n\t@attribute {dict} keys\n\tReturn the attribute values\n\t@attribute {str} keys\n\t\"\"\".format(dict=\"attribute_list=name\", str=\"str\")\n\n\t@classmethod\n\tdef name(cls,"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn tuple(self._get_attrs(self.__class__)[1:]) or ()\n\n\t# ------------------------------------------------------------------------------------------------------------------\n\t\n\tdef __str__(self): # pylint:disable=redefined-builtin\t\"\"\"\n\tGet a string representation of this element or an abstract attribute value.\n\t\"\"\"\n\t\tif self.is_enum:\n\t\t\treturn self._to_string()\n\t\telse:\n\t\t\treturn str(self._to_value())\n\t\n\t# ------------------------------------------------------------------------------------------------------------------\n\t\n\t# Setter methods\n\t# ------------------------------------------------------------------------------------------------------------------\n\tdef _is_enum(self):\n\t\treturn self.parent"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\tnames = set()\n\t\tfor v in self.data.values() or []:\n\t\t\tif all or v[0]!= \"*\":\n\t\t\t\tnames.update(v)\n\t\tfor ns in sorted(names):\n\t\t\tnames.add(ns)\n\t\treturn list(names)\n\n\tdef value_name(self, value_name_dict, all=False):\n\t\t\"\"\"\n\t\tReturn the attribute value names defined by the interface and value\n\t\tnames.\t\"\"\"\n\t\t# FIXME\n\t\tvalues = [val for val in value_name_dict.values()]\n\t\tif all"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn self._names(all).names(True)\n\n\tdef name(self, name): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute name with the given name.\t\"\"\"\n\t\tfor x in self.attrib():\n\t\t\tif x[0] == name: return x[1]\n\t\traise NameError(name)\n\n\tdef _names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\tif all:\n\t\t\tret = [y for x in self._names("}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\tfilter = self.filters[\"interfaces\"]\n\t\tif all:\n\t\t\treturn list_attributes(filter)\n\t\telse:\n\t\t\treturn filter.keys()\n\n\tdef get_attribute(self, name):\n\t\t\"\"\"\n\t\tReturn a single attribute value from the interface. For a list, there is at most 1,\n\t\twho should be the lowest key that matches the interface's field name.  If there is\n\t\tno exact match, return `None`.\n\t\t\"\"\"\n\t\treturn next((attr for attr, _, _ in self.attributes if attr == name), None)\n\n\tdef add"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn list(self._attrib.keys()) if all else self._attrib.keys()\n\t#-def add_attrib(*args, **kwargs):\n\t#-\t\"\"\"\n\t#-\t\"\"\"\n\t#-\t\"\"\"\n\t#-\treturn self._attrib.get(*args, **kwargs)\n\tdef get(self, key, default=None): # pylint:disable=redefined-builtin\t\"\"\"\n\treturn self._attrib.get(key, default)\n\t#-def set(*args, **kwargs):\n\t#-\t\"\"\"\n\t#-\t\"\"\""}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute names defined by the interface.\t\"\"\"\n\t\treturn list(self._names.keys()) or []\n\tdef typedefs(self, all=False): # pylint:disable=redefined-builtin\t\"\"\"\n\tReturn the attribute types defined by the interface.\t\"\"\"\n\t\treturn list(self._types.keys()) or []\n#\tdef _checknames(self, all=False):\n#\t\tresult = []\n#\t\tfor name in self._names:\n#\t\t\tif name not in self._types:\n#\t\t\t\tcontinue\n#\t\t\tif not all:\n#\t\t\t\tresult.append((name, getattr"}]}
{"task_id": "62b8b3d6eb7e40a82d2d111c", "prompt": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\ndef _getargname(func, arguments):\n\t\"\"\"\n\tThe name of a new parameter.\n\n\tA new one will be constructed and turned into a string and\n\tsplit of the arguments.\n\t\"\"\"\n\t\n\tname = 'arg%s' % (\n\t\t', '.join([repr(x[0]) for x in arguments]))\n\t\n\twhile True:\n\t\tname += 'r' +\\\n\t\t\t', n_arguments=%d'% len(arguments)\n\t\t\n\t\tif output is not None:\n\t\t\tname += ', output=%d'% output[i]\n\t\t\n\t\tyield name\n\t\t\n"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\n\n\t\t# FIXME: I can't figure out how to do this in the spec;\n\t\t# I'm not entirely sure what to do about it...\n\n\t\toutput = list(output)\n\t\t\n\t\t# FIXME: It's a bit of a mess if the user provided all of the\n\t\t# members into an interface? How do we handle \"complex\"\n\t\t# declarations. Right now, we're doing \"reverse\" binding of\n\t\t# types and aliases is a different operation on the spec.\n\t\tif len(sequence) == 1 and isinstance(sequence[0], Interface) and not isinstance(sequence["}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\n\n\targs = ''\n\tif not sequences:\t\t\t\t# no sequence specified\n\t\treturn args\n\n\tif output in arguments.expand:\t\t# expand mode\n\t\targs = sequences[output] +''\n\n\tif len(arguments.expand) == 1:\t\t# one element\n\t\tif type(arguments.expand[0]) == type([]) and \\\n\t\t\tlen(arguments.expand[0]):\t\t# just a single element\n\t\t\ttemp = arguments.expand[0][0].replace(' ','')\n\t\t\tif temp!= args:\t# if something else provided\n\t\t\t\targs"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\n\n\tif output is None:\n\t\toutput = []\n\n\tret = []\n\n\t# Determine the maximum number of elements in the list\n\telements_left = len(sequence) - len(sequence.lstrip('-')) - 2\n\telements_right = len(sequence) - elements_left\n\t\n\tif elements_right == 0:\n\t\n\t\t# No leading colon in sequence\n\t\tsequence  = sequence +''\n\t\n\t\t# Remove leading ':' from sequence\n\t\tsequence = sequence[elements_left + 1 :elements_right]\n\n\t\t# Find minimum number of arguments\n\t\tlength = len(sequence"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\n\n\tif isinstance(sequence, Declarions):\n\t\tif output is None: output = 0\n\t\treturn sequence\n\t\n\t\t\n\tif not isinstance(sequence, tuple):\n\t\traise TypeError('Can only normalize a sequence of specifications')\n\n\treturn tuple(_normalize(sequence) for sequence in output)\n\n\ndef _normalizedict(mapping, output=None):\n\t\"\"\"\n\tNormalize a mapping, such that the keys are either the\n\tstring, or a tuple of those.\n\n\tMapping keys must be of the form {key : value}.\n\t\"\"\"\n\t_normalizeddict = {}"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t # pylint: disable-msg=W0313\n\t node = sequence[0][0][1]\n\t\tif len(sequence) > 1:\n\t\t\titemtype = sequence[1]\n\t\telse:\n\t\t\titemtype = None\n\t\targs = (itemtype,) + sequence[2:]\t\t\n\t\tif output is None:\n\t\t\treturn args\n\t\telse:\n\t\t\treturn [output.format(x) for x in args]\n# Copyright (c) 2006-2009 Mitch Garnaat http://garnaat.org/\n#\n# Permission is hereby granted, free of charge,"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\n        out = []\n        def addIncluded(*args):\n                \"\"\"\n                Append the specified include in order of decls, tuple, or\n                single interface specification (e.g. \"def foo(con):\",\n                    \"  return 12\"))\n                \"\"\"\n                if args:\n                    out.extend([_normalizeargs(arg, False) for arg in args])\n                return out\n            \n        def handleDeclarations(decls):\n                \"\"\"\n                Extract all names in the declarations, from the declarations\n                and write them to the output.\n                \"\"\"\n                # First, get the declarations, which is a list of tuples, where\n                # the declaration"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\tresult = []\n\t\tfor arg in sequence:\n\t\t\tif isinstance(arg, Declarions):\n\t\t\t\t# Replace all \"optional\" arguments by the argument list\n\t\t\t\tresult.extend(_normalizeargs(arg))\n\t\t\telse:\n\t\t\t\t# Replace the arg list\n\t\t\t\tresult.extend([arg] * len_ignore)\n\t\tif output is not None:\n\t\t\tresult.append(output)\n\t\treturn result\n\treturn []\n\t\ndef _normalizeargs2(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments plus explicit arguments\n\nNormalization arguments may contain Invisible and"}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\n\t\n\tif isinstance(sequence, (tuple, list)):\t\t\t# Not a tuple/list\n\t    if len(sequence) == 1:\n\t\t    sequence = sequence[0]\n\t\tif not output:\n\t\t    return sequence\n\t    if not hasattr(sequence, \"__getitem__\"):\n\t\t# No list in tuple/list\n\t    if not isinstance(sequence, tuple):\n\t\t# No nested tuple\n\t    return tuple(sequence)\t\t\t\t# No tuple\n\telif not hasattr(sequence, \"__iter__\"):\n\t\t# No iterable\n\t    return tuple(sequence)\t\t# Not a generator\n\telif None in sequence: "}, {"generated_text": "def _normalizeargs(sequence, output=None):\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\t\"\"\"\n\t\nclass Declus:\n\t'''\n\tA decorator that turns an iterable of decorators into a single\nstatement.  This allows to have functions `flatten`.  It takes each\nfunction's parameters.\n\nExample:\n\nclass Declus:\n\t@defmethod\n\tdef myfunc(x):\n\t\treturn x + 1\n\tmyfunc(4)\n\t'''\t\t\n\tdef __init__(self, *decorators):\n\t\tself.decorators = lambda x: decorate(x, self, decorators, output=None)\n\t\tself.function = decorate(1, self._"}]}
{"task_id": "62b8b3d5eb7e40a82d2d1110", "prompt": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\n_C_OPTIMIZATION_CACHE_KEY = 'optimize_%s'\n\"\"\"\nA global cache key to be used for storage of optimizations.\n\"\"\"\n\nclass OptimizationContext(object):\n\t\"\"\"The optimization context for a set of compilation engines.\n\t\"\"\"\n\n\tdef __init__(self, opt_cache=None):\n\t\t\"\"\"@param opt_cache: A L{OptimizerCache} instance to use if we\n\t\t\tneed to recompute the optimize results.\n\t\t\"\"\"\n\t\tif opt_cache is None:\n\t\t\tself.optimize_cache = OptimizeCache()"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\n\n\tmoduleName        = _getFullmoduleName()\n\tavailable          = _getAvailableVariables()\n\n\tif moduleName not in available:\n\t\traise ImportError('Optimizations module %s not available' % moduleName)\n\n\tfrom.pythonopt import Optimizations\n\tassert moduleName in Optimizations.modules.values()\n\n\tfrom.optimizer import Optimizer\n\toptimized = Optimizations.modules[moduleName].isOptimized()\n\n\tif optimise.version_compare(optimized, Optimizer.version):\n\t\treturn Optimizer(optimized)\n\n\traise ImportError(\"Couldn't find optimizers (%"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\n        if optimize is None:\n\t\t\traise ImportError(\"c_optimizations must be installed.\") \n\t\t\n\t\tif optimize is None and not _c_optimizations_available():\n                raise ImportError(\"no _c_optimizations provided. \"\n                                  \"Please install C/C++/Fortran to use it.\")\n#\n# Copyright (c) 2008-2015 Citrix Systems, Inc.\n#\n#   Licensed under the Apache License, Version 2.0 (the \"License\")\n#   you may not use this file except in compliance with the License.\n#   You may obtain a copy of"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\nimport re\nimport sys\nimport os\nimport warnings\n\nimport scipy._lib.six as six\n\n\nif sys.version < '2.2':\n    import warnings\n\n    warnings.warn(\"the scipy.optimize C module is not available until\"\\\n                  \"python 2.2, please upgrade the scipy development\\n\"+\n                  \"to version 2.2 or greater.\\n\",\n                  RuntimeWarning)\n\ndef _set_module(module):\n\n    \"\"\"\n    Ensure that `module` is wrapped in a tf namespace.\n\n    Parameters\n    ----------\n    module : str, tuple, dict\n        A tuple of (submodule, name"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\tpass\n\n\t# Check if c_module is available.\n\tif c_module.__name__ == '__main__':\n\t\tpass\n\n\t# Check if an optimizations available\n\ttry:\n\t\timport __main__\n\texcept ImportError:\n\t\treturn False\n\t\n\t# Check that the optimizations are available\n\t(min_version, max_version, c_name, c_version) = _c_optimize_available()\n\n\tif min_version and max_version and c_name and c_version:\t# Minimum\n\t\tsys.stderr.write('The optimization module %"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t #TODO: Check if optimization is required and use it?\n\t return 'baseopt' if optimize else ''\n\t\t\n\t\t\t\n# ##### BEGIN GPL LICENSE BLOCK #####\n#\n#  This program is free software; you can redistribute it and/or\n#  modify it under the terms of the GNU General Public License\n#  as published by the Free Software Foundation; either version 2\n#  of the License, or (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\treturn []\n\n\tif not hasattr(conf, 'optimizations'):\n\t\tconf.optimizations = []\n\treturn conf.optimizations\n\n\ndef setOptimizations(optimizations):\n\t\"\"\"\n\tSets the optimization module to use when calling C code at the\n\tend of the file.\n\n\tIf the optimizations are desired then set the C code using\n\tthis function to set the optimizations.\n\t\n\t.. code-block:: python\n\n\t\twith cfg.getOptimizations('myNewID', myNewID):\n\t\t\t...\n\t\"\"\"\n\tconf.optimizations = optimizations"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\t# Python2.4/2.5 have this built-in function to avoid the importing\n\t\t# of a shared library.\n\t\t_c_optimizations = sys.modules[__name__]\n\t\tif not _c_optimizations.__name__.startswith(\"crypto\"):\n\t\t\traise ImportError(\"Crypto is required for optimizations\")\n\t\treturn _c_optimizations\n\n\traise ImportError(\"Crypto is required\")\n\n_use_c_optimizations = _c_optimizations_available()\n_c_optimizations.__all__ = _c_optimizations.keys()\n\n\n# Put together"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\treturn \"\"\n\n\nc_optimizations_available = _c_optimizations_available()\n\"\"\"\nWhether the C optimization module is available\n(either because it may have been disabled by setting\n:setting:`C_OPTIMIZE`), and will therefore return the\n:func:`~deap.tools.deap.c_optimizations` module.\n\nThis is a hint to provide verbose, and doesn't prevent\nprinting.\n\"\"\"\n\ndef _c_optimizations_unavailable():\t\"\"\"\n\tReturn the C optimization module, if unavailable, otherwise\na false value.\n\nIf the"}, {"generated_text": "def _c_optimizations_available():\t\"\"\"\n\tReturn the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.\t\"\"\"\n\t\tcdict = get_c_var('OPTIMIZERS')\n\t\tenv = self.env\n\t\tif not env['OPTIMIZERS'].startswith('OPTIMIZERS'):\n\t\t\treturn False\n\t\treturn bool(env['OPTIMIZERS'].split(' '))\n\treturn _c_optimizations_available()\n\nc_optimizations = _c_optimizations_available()\n\ntry:\n\tfrom collections import OrderedDict as _default_dict\nexcept ImportError:\n\t_default_dict = None\n# -*- coding: utf-8 -*-\n# This file is part"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110f", "prompt": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\treturn os.environ.get('PURE_PYTHON') is not None\n\nif not _should_attempt_to_c_optimizations():\t# pragma: no cover (py3 only)\n\traise RuntimeError(\n\t\t\t\"Attempt to use `puresci.USE_C_OP_OPTIMIZE=True` outside of \"\n\t\t\t\"the CPython environment (PUREC_PYTHON was not found); \"\n\t\t\t\"or using the default `PURE_PYTHON` environment variable \"\n\t\t\t\"'PY_PURE_PYTHON=False' with `PY_NOPYTHON="}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\t# XXX: this should be conditional on MSVC compiler settings?\n\t\tval = os.environ.get('PURE_PYTHON', None)\n\t\tif val in ('1', 'true', 'on', 'yes'):\n\t\t\t\treturn True\n\t\treturn False\n\treturn _should_attempt_c_optimizations()\n\ndef _needs_f90_precompile():\n\t\"\"\"\n\tReturn True if the _f90precompile attribute must be checked before it is\n\tcompiled. This is only needed to support backwards compatibility with\n\tC extensions, it is not guaranteed to work in all"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\tif os.environ.get(\"PURE_PYTHON\") and sys.version_info[:2] == (2, 6):\n\t\t\treturn True\n\t\treturn _guess_c_interpreter()\n\treturn bool(os.getenv(\"PURE_PYTHON\"))\n\n\ndef _auto_init_ctypes():\n\t\"Load the default _ctypes module if it doesn't exist. (We do it if we can't)\"\n\tglobal _default_ctypes, _default_c_lib\n\tif _default_ctypes is None:\n\t\t_default_ctypes = CDLL(\"libc.so."}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\tfor key in os.environ.get('PURE_PYTHON', '').split():\n\t\t\tif key.startswith('PURE_'):\n\t\t\t\treturn True\n\t\treturn False\n\n\t_use_c_impl = (\n\t\t_should_attempt_c_optimizations\n\t\tand _should_use_pybind11\n\t\tand sys.platform!= 'cli'\n\t)\n\n\tif _use_c_impl:\n\t\treturn True\n\n\tif not _should_use_c_impl:\t\t# If not on PyPy 2.5, then we're on CI\n\t\t"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\tif os.environ.get(\"__PURE_PYTHON\"):\n\t\t\treturn \"pypy\"\n\t\telse:\n\t\t\treturn \"cpython\"\n\telse:\n\t\treturn True\n\ndef is_c_header(filename):\n\t\"Wraps c into python files for checking equivalence\"\n\treturn not (\n\t\tfilename.endswith('.c') or\n\t\tfilename.endswith('.h') or\n\t\tfilename.endswith('.cc')\n\t)\n\ndef check_c_header(filename, file):\n\tlines = file.lines()\n\tif IS_PYPY_OR_"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\tif sys.platform == \"win32\" and \"_PYCO\" in os.environ.get(\"PYCO_PLATFORM\"):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn _HAVE_PYCO\n\n\ttry:\n\t\treturn os.nice(1) == 1\n\texcept:\n\t\tpass\n\n\tif _HAVE_PYCO:\n\t\tdist = sysconfig.get_config_vars('EXT_SUFFIX')\n\t\t_MSVC_OPTIMIZED = 1 if dist[1] == '+' else 0\n\t\treturn _MSVC_OPTIMIZED > \"1.53\"\n\t"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\tif CYTHON_PROFILE:\n\t\t\tfrom jinja2 import TemplateNotFound\n\t\t\ttry:\n\t\t\t\treturn TemplateNotFound is IOError or OSError\n\t\t\texcept ImportError:\n\t\t\t\treturn False\n\t\tc_optimizations = getattr(Context.get_current(), 'c_optimizations', '')\n\t\tif os.environ.get('PURE_PYTHON') and c_optimizations:\n\t\t\treturn c_optimizations == 'True'\n\treturn _c_optimizations\n\n\nclass _type_check(object):\n\tdef __init__(self, c_type, c_type2):\n\t\t"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\tpython_str = os.environ.get('PURE_' + os.environ.get('PYTHON', ''))\n\t\tif python_str is None:\n\t\t\treturn False\n\t\t\n\t\ttry:\n\t\t\tmod = __import__(python_str, globals(), locals(), [])\n\t\texcept ImportError:\n\t\t\treturn False\n\t\t\n\t\tmod_env = vars(mod)\n\t\tif 'C' in mod_env:\n\t\t\treturn mod_env['C']\n\t\telse:\n\t\t\treturn False\n\t\n\treturn all(map(bool, vars(mod).get('optimize', [])))\n\n_should_"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\tenv = _use_c_impl()\n\t\t\n\t# Warn if we've been disabled or non-NULL:\n\tif not _is_null(env):\n\t\t_set_environment_var(env, PURE_PYTHON, PURE_NOUSER)\n\n\treturn env[\"PURE_PYTHON\"]\n\nUSE_PYPY = _should_attempt_c_optimizations()\nif USE_PYPY:\n\t# For CPython (and, the implementation of the native C API) we need\n\t# our own implementation of it; otherwise we use the C implementation.\n\t"}, {"generated_text": "def _should_attempt_c_optimizations():\t\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\t\"\"\"\n\t\treturn (os.environ.get('PURE_PYTHON') or '').lower() == 'true'\n\n\tenv_name = {'PURE_PYTHON' : 'true', 'PURE_NO_OVERRIDE' : ''}\n\t\n\tdef set_env(fun):\n\t\t\"\"\"\n\tUsed by the -O command-line tools.\n\t\"\"\"\n\t\tassert fun is _use_c_impl\n\t\t_use_c_impl = fun\n\n\tdef get_env(fun):\n\t\t\"\"\"\n\tUsed by the -O command-line tools.\t\"\"\"\n\t\td = _"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110e", "prompt": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn (\"_c_optimizations_required\")\n# Copyright 2012 The Chromium Authors. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\nfrom telemetry.page import page as page_module\nfrom telemetry.page import page_set as page_set_module\n\n\ndef _GetFormattedEvents(events_list):\n  events = []\n  for e in events_list:\n    event = {\n      'name': e.name,\n      'id': e.id,\n      'isLoading': e.is"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn True\n\n\tdef _c_optimizations_available(self):\n\t\t\"\"\":return: (bool) Whether custom optimizations are available.\"\"\"\n\t\treturn True\n\n\tdef _c_optimizations_undefined(self):\n\t\t\"\"\"\n\t\t:return: (bool) Whether custom optimizations undefined.\n\t\t\"\"\"\n\t\treturn False\n\n\tdef _c_global_optimizations_not_all_ready(self):\n\t\t\"\"\"\n\t\t:return: (bool) Whether custom optimizations are not all ready.\n\t\t\"\"\"\n\t\treturn False\n\n\tdef _c_global_optim"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn set([\"_optimal_optimization_cost\",\n\t\t\t\"_max_optimization_curve_difference\",\n\t\t\t\"_optimization_parameter_set\",\n\t\t\t\"_constraints_list\", \"_objective_variables\",\n\t\t\t\"_objective_variables_by_name\", \"_param_list\",\n\t\t\t\"_param_list_to_optimizations\",\n\t\t\t\"_objective_variables_by_value\", \"\t_objective_variables_by_name_and_value\",\n\t\t\t\"_objective_variables_by_name_and_value\", \"\t_objective_variables_by"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\tpass\t# not used anymore, but for now\n\treturn False\n_c_optimizations_required = _c_optimizations_required\ndef _c_optimizations_allowed():\n\t\n\t\"\"\"\n\tSee `_c_optimizations_required`.\n\t\"\"\"\n\treturn (\"none\", \"any\")\n_c_optimizations_allowed = _c_optimizations_allowed\ndef _c_optimizations_required(name, env, cont):\n\t\"\"\"\n\tSee `_c_optimizations_required`.\n\t\"\"\"\n\tif name not in _VALID_CALLOUT"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\tpass\n\treturn _c_optimizations\n\n\ndef _c_optimizations_unknown():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\tpass\n\treturn _c_optimizations\n\n\ndef _py_optimize_merge_code():\t\"\"\"\n\tThe inverse of `_optimize_merge_code()`\n\t\"\"\"\n\n\tdef _optimize_merge_code():\n\t\t# This one has to go through the merge.\n\t\tself.add_merge_code_raw('C', 'N')\n\t\t_, self.add_merge_code_"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\tif not _c_optimizations_required(): return False;\t# Don't allow the user to choose the optimum of the optimum of the optimum.\n\n\tc_optimizations_required.allowLazyEvaluation = True\n\n\treturn _c_optimizations_required\n\ndef c_optimizations_disabled():\n\twith _c_optimizations_disabled_lock:\n\t\tif hasattr(_c_optimizations_disabled, '_c_optimizations_disabled'):\n\t\t\tdel _c_optimizations_disabled._c_optimizations_disabled\n\t\t\t_c_optimizations_disabled"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn True\n\n\t@staticmethod\n\tdef _c_optimizations_needed():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn True\n\n\t@staticmethod\n\tdef _c_optimizations_required():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn True\n\n\t@staticmethod\n\tdef _c_optimality():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn 1\n\n\t@staticmethod\n\tdef _c_optimization_required():\t\t\"\"\"\n"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\n#\tprint\n\tprint \"=== Global optimizations (not yet implemented. +++)\"\n\tprint \"Useful for profiling\"\n\t#print \"=======================\"\n\t\ndef _c_optimizations_required(name, optimize=True):\n\t\"\"\"\n\t:api:`Optimization.optimize` does not return.\n\t\n\t>>> f = Optimization( ['x0'], [('x0',0)], [])\n\t>>> y = Symbol('y')\n\t>>> optimize_with (f, x0, x, y)\n\tOptimizationResults( 'x-dependent', x0=0"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\tglobal c_optimizations_required\n\t\tc_optimizations_required = False\n\telif config_string.find('optimizations for'):\n\t\tc_optimizations_required = True\n\telse:\n\t\tpass\n\treturn c_optimizations_required\nclass _IcEnumSet:\n\tdef __init__(self, enum_set):\n\t\tself._enum_set = enum_set\n\tdef __iter__(self):\n\t\treturn self._enum_set._enumerations\n\tdef __len__(self):\n\t\treturn len(self._enum_set._enumerations"}, {"generated_text": "def _c_optimizations_ignored():\t\"\"\"\n\tThe opposite of `_c_optimizations_required`.\t\"\"\"\n\t\treturn (\"_internal_optimized\", \"__internal_optimized\", \"__optimization_enabled\")\n\n\t_internal_optimized = False\n\t_internal_optimized_with_user = 0\n\n\t_optimization_disabled = False\n\t\"\"\"\n\tIf the user hasn't set it, the default behavior is to warn.\n\t_optimization_enabled overrides everything else.\n\t\"\"\"\n\n\tdef __optimization_disabled_changed(self, attr, old, new):\n\t\tself._optimization_disabled = True\n\t\tif self._optimization_enabled and old in self._c_user_variables and new in self"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110d", "prompt": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\tif not _USE_C_IMPL:\n\t\t\t_USE_C_IMPL = os.environ.get(\"_USE_C_IMPL\",\"0\") == \"1\"\n\treturn _USE_C_IMPL\n\nUSE_NUMBA_CACHE = _use_numba_cache()\n\nUSE_C = USE_NUMBA_CACHE\n\nUSE_NUMBA_CACHE.update(_use_c_impl())\nUSE_C = USE_NUMBA_CACHE\n\nUSE_USE_NUMBA = USE_C\n\nUSE_D_NATIVE_64 = USE_"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\treturn (hasattr(current_compiler, \"_c_optimizations_required\") and\n\t\t\t\tnot getattr(current_compiler, \"_c_optimizations_required\"))\n\n\tif sys.hexversion < 0x3000000:\n\t\tdef _c_optimizations_optional():\tpass\n\telse:\n\t\tdef _c_optimizations_optional():\tpass\t\n\treturn _c_optimizations_optional, _c_optimizations_optional\n\nCOMPILE_OPTIONS = _use_c_impl\n\ndef get_errno():\n\t\"\"\"\n\tReturns an integer representing the platform and"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\t# This implementation is only needed on Windows.\n\t\tsupported_optimizations = (\"linux\", \"freebsd\", \"osx\", \"ce\", \"uwp\", \"nano\", \"riscos\", \"mingw\", \"win\")\n\t\treturn bool(not supported_optimizations)\n\tc_optimizations_required = property(_c_optimizations_required)\n\t# We check this directly, because C was made available through the command line.\n\t_shared_globals.check_c_compatibility()\n\treturn c_optimizations_required\n\n\t\t\ndef _can_use_c_rout"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\tcoptimizations_required = (__use_c_impl == True)\n\n\treturn coptimizations_required\n\n\ndef _c_use_c_impl(key, val):\t\treturn _use_c_impl[key](val)\n\ndef _init_c(c_code):\n\tif c_code.co_argcount > 0:\n\t\tc_code.co_varnames[:0] = ['__stdcall','CO_VARARGS','CO_VARKEYWORDS']\n\t\tmod_name = c_code.co_varnames[-1]\n\telif c"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\treturn sys.flags.optimize >= 2\n\ndef use_c_overload(c_overload):\n\t\"\"\"\n\tReturn the given value if it detects that the compiler supports the given overload.\n\tThis is necessary for compatibility with systems that do not support\n\tthe given overload, e.g. Cygwin on Python.\n\n\tThis function will fail if any implementation is not found; this function\n\twill return None.  There is no portability or any other unimplemented\n\timplementation for this purpose.\n\t\"\"\"\n\n\t# By default, if there's an over"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\treturn conf.env['PURE_PYTHON']=='yes'\n\telse:\n\t\treturn 0\n\ndef use():\n\t'''\n\tRetrieve the value of setuid(0) or setgid(0) flag if the current process\n\tis executing this process.\n\t'''\n\ttry:\n\t\tluid = _c_getuid()\n\texcept:\n\t\treturn 0\n\tif not _c_has_setsgid(luid):\n\t\treturn 0\n\treturn 1\n\nclass _flavour(object):\n\t'''\n\tUse the pid for f"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\tc_op = core.op\n\t\tif c_op.OpIsDefined(ctypes.c_int32, ctypes.c_int32.__int32):\n\t\t\treturn bool(True)\n\t\tif c_op.OpIsDefined(ctypes.c_float, ctypes.c_double):\n\t\t\treturn float(True)\n\t\treturn bool(False)\n\treturn _c_optimizations_required\n\t\ndef set_c_optimizations(options):\n\t\"\"\"\n\tSet c_optimizations to be used whenever an optimizer which is not\n\talready configured is set."}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\traise NotImplementedError\n\n\t_c_optimizations_provided = True\n\n\tdef set(self, _val):\n\t\tself._isset = bool(_val)\n\n# The functions _c_object methods only return a single value or an iterable\n# of integers. This means we can't use the values() built-in since the\n# internals of this module are statically defined somewhere for C implementations\n# of Python (i.e., CPython)\n\n\tdef __len__(self):\n\t\t\"\"\"Return the length of the C variable. The C type may be\n\t\tchanged if it has"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\t# C optimizations is required for any reasons.\n\t\treturn c_api.PYFUNCS_REQUIRED\n\n\t@staticmethod\n\tdef _c_optimized_api():\treturn _use_c_impl.optimized_api()\n\n\t@staticmethod\n\tdef _c_builtin_optimizations():\treturn _use_c_impl.builtin_optimizations()\n\n\t@staticmethod\n\tdef _generic_python():\n\t\t\"\"\"\n\t\tReturn a callable object to the C wrapper.\n\t\t\"\"\"\n\t\t# Return a callable that just wraps a Python function.\n\t\treturn _use_c"}, {"generated_text": "def _c_optimizations_required():\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\t\"\"\"\n\t\treturn PURE_PYTHON\n\n\n\t# -------------------------------------------------------------------------\n\t# Create and return an instance of the ``Optimizer`` object\n\t# -------------------------------------------------------------------------\n\n\tif not hasattr(Optimizer, '_greedy'):\n\t\traise TypeError, \"Optimizer instance is not fully initialized\"\n\n\tif not _greedy_prob.on:\n\t\tif verbose > 1:\n\t\t\tprint \"  adding to OptimizationManager's list of useful optimizations\"\n\t\t_greedy_prob._greedy = True\n\n\treturn OptimizationManager(\n\t\t[getattr(Optimizer, '_optimizer_'+optimize)]\n\t\t)\n\noptimizer = Optim"}]}
{"task_id": "62b87b989a0c4fa8b80b35ee", "prompt": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\n\t\n\nclass Context():\n    \"\"\"\n\tThe most important attribute from the current context.\n\t\"\"\"\n\t\n\tdef __init__(self, d):\n\t\tself.__d = d\n\t\t\n\tdef __enter__(self):\n\t\tinit()\n\t\t\n\t\tfor k, v in self.__d.items():\n\t\t\tself.__d[k] = v\n\t\t\t\n\t\treturn self\n\t\n\tdef __exit__(self, type, eval, tb):\n\t\ttry:\n\t\t\tdel self.__d[type]\n\t\t\t# TODO: if one of these calls fails => it is too early now.\n\t\t\tdel self"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\n\t\tif (self.init_args is None or not isinstance(self.init_args, dict)) or not self.init_args:\n\t\t\tself.init_args = dict()\n\n\t\tself.initialized =False\n\t\tself.initialized_histories = 0\n\t\tself.histories = {}\n\t\tself.init_histogram = np.zeros(shape=())\n\t\tself.count = 0\n\n\t\treturn self\n\n\tdef step(self, u, p, r, s=None, u_prev, m_prev):\n\t\t\n\t\t# If we already have the desired data,"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tself.init()\n\t\tself.bins.clear()\n\tdef init(self):\n\t\tself.bins = {}\n\tdef add(self, target, source, bin_id=None, bin_id2=None,\n\t\ttarget_range=None, target_value=None):\n\n\t\tbin_id = bin_id or bin_id2\n\t\tbin = self.bins.get(bin_id)\n\t\tif bin is None:\n\t\t\tbin = self.bins[bin_id] = []\n\t\t\tself.bins[bin_id].append(bin)"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\timport pyroot.PIL as pyroot\n\tself.state_space = pyroot.TILE_CACHE\n\tself.bin_cache = {}\n\tself.current_bin = 0\n\t\n\t# initial state\n\tself._initial_stats = {}\n\t\n\tfor a in self._states.keys():\n\t    #print a\n\t    #print self._states[a]\n\t    b = 0\n\t    for st_b in self._states[a]:\n\t\tself.current_bin += 1\n\t\tif (st_b in self._initial_stats.keys()):\n\t\t    self._initial_stats"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tif self._initial_value == self._make_bins:\n\t\t\tself._initials[self.target_name] = self.histogram[0]\n\t\t\tself._initial_hist.clear()\n\t\tself._initial_hist.clear()\n\n\t### Iteration methods\n\t# ------------------------\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\t:return: Number of points we have,\n\t\t:rtype: int | None\n\t\t\"\"\"\n\t\treturn self._bins.items()\n\n\tdef __getitem__(self, idx):\n\t\t\"\"\"\n\t\tWe have this many points in"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tself._hist = dict()\n\t\tself._hist.clear()\n\t\tfor k in self._parameters:\n\t\t\tself._hist[k] = dict()\n\n\tdef set_target(self, target):\n\t\t\"\"\"\n\t\tSet the given target value. If target is ``None`` (aka, an empty\n\t\tdict), it simply replaces the corresponding entry in the hist\n\t\tdict.\n\n\t\tParameters\n\t\t----------\n\t\ttarget : ``None`` | a single value (single value)\n\t\t\t\t   or a :term:`pair type` from python dictionary (with no\n\t\t\t\t\t   special meanings)"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t #self.clear()\n\t self.clear(init_value)\n\t self.set(initial_value)\n\n\tdef _get_histogram(self, *args):\n\t\t\"\"\"Get a histogram from the histogram bin.\"\"\"\n\t\tif len(args) == 0:\n\t\t\treturn np.zeros(self.bins)\n\t\thist = self.__class__.histogram(*args)\n\t\tbins = self.__class__.bin_size(hist)\n\t\thist_new = hist.copy()\n\t\tself.__class__.initialize_bins(bins)\n\t\treturn hist_new\n\n\tdef clear(self"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\ttry:\n\t\t\tself.clear()\n\t\texcept AttributeError:\n\t\t\tpass\n\t\tself._bins.clear()\n\tdef reset_all(self):\n\t\t\"\"\"\n\tEmpty the dictionary and create all entries.\n\t\"\"\"\n\t\ttry:\n\t\t\tfor b in self.bins:\n\t\t\t\tfor k in b:\n\t\t\t\t\tif k not in self._bins_dict:\n\t\t\t\t\t\tself._bins[k] = []\n\t\t\t\t\tself._bins[k].append(b[k])\n\t\t\tself.clear()\n\t\texcept AttributeError:\n\t\t\tfor b in self.bins:\n\t\t\t\t"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tself['histogram'].remove()\n\n\t'''\n\tA histogram is defined as a \"line\" in the graph itself.\n\t'''\n\tdef make_bins(self, bins):\n\t\tnbins = []\n\t\tfor i in range(len(bins)):\n\t\t\tnbins.append([bins[i][0]+bins[i][-1]])\n\t\treturn nbins\n\nclass Context():\n\t'''\n\tAn interface to a class providing access to and manipulate\n\tthe data needed to draw the histogram into the current plot. \n\t'''\n\t'''\n\tCurrent context is initialised"}, {"generated_text": "def reset(self):\t\"\"\"\n\tReset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tself.hist = {self.initial_value: {}}\n\t\tfor key in self.param_keys:\n\t\t\tself.hist[self.param_keys[key]] = {}\n\tdef update(self):\n\t\t\"\"\"\n\tUpdate the histogram at the current timestep.\n\nCurrent context is restored.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\t\"\"\"\n\t\tself.hist = {self.initial_value: self.hist[self.param_keys['initial']][self.initial"}]}
{"task_id": "62b87b859a0c4fa8b80b35d7", "prompt": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\tfrom.core import Graph\n\t\timport os.path\n\t\timport sys\n\n\t\tif (isinstance(header, basestring)) or \\\n\t\t   isinstance(header, (list, tuple, list)) or \\\n\t\t   isinstance(header, (str, unicode)) or \\\n\t\t   header is None:\n\t\t\t# We need to format tuples instead, because for now the output\n\t\t\t# we require 2D coords for the header and 2D points for\n\t\t\t# each point for each graph.\n\t\t\theader = self._header(header)\n\n\t\timport tempfile\n\t\timport csv\n\t\toutfile = tempfile.TemporaryFile()"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\treturn convert(self, separator, header, self.get_value_type)\n\n\n\n\n############################################################################\n# Graph\n############################################################################\n\n# Relies on graph type\n##\n\n# This is needed to make a kind of graph, as well as to how the user is\n# working with a new graph type.\n#\n# For a more flexible type of graphs, see :class:`graph.Graph`.\nclass Graph(Graph):\n\t\"\"\"\n\tAn abstract graph of some sort of graphics.\n\n\tExample of this type is the :class:`Graph` and subclasses of\n\t:class"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\treturn self.output.ToCSV([column for column in self])\n\nclass Graph(list):\n\t\"\"\"\n\t.. deprecated:: 0.6 in lena 0.6 to_csv is deprecated in\n   version 0.8 and will be removed in the next release.\n\t\"\"\", DeprecationWarning\n\tdef __init__(self, *args, **kwargs):\n\n\t\tif len(args) == 0:\n\t\t\traise RuntimeError(\"At least one argument is needed.\")\n\n\t\tsuper(Graph, self).__init__(*args, **kwargs)\n\n\t\tif self.__len__ == 0:\n\t\t\traise RuntimeError"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\treturn self.to_csv(separator, header)\n\n\tdef to_csv(self, prefix=\"\"):\t\t\"\"\"\n\tWrite :class:`Graph` to a file :class:`Textfile`.\n\tIf :attr:`sep` is specified, then each line corresponds to\n\tcolumn separators, e.g.\n\t[ sep=\"\"\"1,2,3\n\t\"\"\" --> \"Sep = \\\" 1, 2, 3\\\\n'\n\t\"\"\"].split(\"\\n\")\n\n\tdef write(self, *points):\n\t\t\"\"\"Write a list of points to the output file.\n\n\tOnly"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\twarnings.warn(\"Use `export LINEXPR.to_csv()` instead\")\n\treader = reader_for_file(self._fp)\n\theader = self._parse_header(header)\n\n\tif header:\n\t\t_write_csv_header(reader, separator, header, self._delim)\n\twriter = csv.writer(self._fp)\n\tfor point in reader:\n\t\twriter.writerow([\n\t\tstr(point[0]), str(point[1]), str(point[2]),\n\t\tstr(point[3]), self._delim.join("}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\timport csv\n\t\tfrom lena.config import CONFIDENTIAL\n\t\tfrom lena.engine import Node, Entity, Path\n\t\tfrom lena.builder import TemplateType\n\n\t\tself._filename = None\n\t\tself._writer = csv.writer(self._header, delimiter=separator,\n\t\t\t\tquotechar=\"'\", lineterminator=\"\\n\", encoding='utf8')\n\t\tself._writer.writerow(self._headers)\n\t\tself._writer.writerows(list(zip(self._rows, self._headers)))\n\n\t\tself.to_csv()\n\n\tdef from_csv(self,"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\treturn self.ToCSV()\n\n\n\tdef ToCSV(self, point_number = 0):\n\t\t\"\"\"\n\tConverts a point to ASCII 8 bytes.\n\nBy default, the output has columns in columns\nheaders, while the others aren't.\n\nThe output can be the same type as that of a\n:class:`Graph` but it can contain more than one line.\nIf the output has more columns in the same line,\nthey are converted to single-line strings.\nIf no output has columns in the same line it should be left\nunaltered (or ``None`` if"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\tfrom pylab import *\n\t\timport sys\n\n\t\t# Check if the graph is for export (otherwise, graphs is too verbose)\n\t\tif self.export_graph == True:\n\t\t\treturn False\n\n\t\t# Decide a list of coordinates and write it to file\n\t\toutlist = []\n\t\tif self.export_line_str:\n\t\t\texport_line_str = \"\\n\"\n\t\telse:\n\t\t\texport_line_str = \"\"\n\n\t\tfor p in range(len(self.points)):\n\t\t\tpstr = str(self.points[p])\n\t\t\tif self"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\treturn self.to_csv(separator, header=header)\n\n\nclass TableCSV(TableCSVBase, Plotter):\n\t\"\"\"\n\tA simple ASCII table with header.  If specified\n\twith keyword argument ``header`` will be used\n\tinstead of the default header.  This is often\n\tthe case.  However, if ``header`` is set, the\n\t``col_head`` keyword argument is ignored and the\n\tfirst line of the file determines\n\tif the file is a CSV file, and the file\n\tformat is `ascii` then the file is written\n"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\t\"\"\"\n\t\tprint(\"*\" * 20 + \" Begin to_csv\")\n\t\tfor index, point in enumerate(self):\n\t\t\tif index % separator == 0:\n\t\t\t\tprint(separator, \": \", \"{:f}\".format(float(index)))\n\t\t\tfor vect in point:\n\t\t\t\tprint(separator, \"{:22.15}\".format(vect))\n\t\treturn\n\n\tdef to_image(self, filename, mode = \"RGB\", **options):\n\t\t\"\"\"\n\t.. note:: This function uses :obj:`scipy.misc.imresize` to convert\n\ta :class:`scipy."}]}
{"task_id": "62b87b839a0c4fa8b80b35cb", "prompt": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\treturn self._node.coord(coord_name).grid.get_coordinates_errors()\n\n\tdef _get_nodes(self):\n\t\t\"\"\"\n\t\tMake an index of nodes along graph's axis. Resulting\n\t\tfrom a depth-first traversal of self._node will create index\n\t\tnumbers for each node, and the parent of each node will create\n\t\tindex numbers for each parent (the last entry in node list)\n\t\tin this case only one index of the first node.\n\t\t\"\"\"\n\t\tnodes = []\n\t\tfor node in self._node.graph.nodes():\n\t\t\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tcoords = self._coord_map[coord_name]\n\t\tif len(coords)!= 1: \n\t\t\tmsg = \"Mismatch on coordinate '%s'. \" % cube_names[len(coords) - 1]\n\t\t\tmsg += \"Coordinate: %s.\" % self._coord_map[coord_name]\n\t\t\tmsg += \"Coordinate's dimensionality: %i\" % len(self.shape)\n\t\t\traise iris.exceptions.CoordinateNotFoundError(msg)\n\t\treturn coords[0]\n\n\tdef _as_list(self):\n\t\t\"\"\"\n\t\tReturn data as list.  "}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tif coord_name not in self.nc.variables:\n\t\t\traise iris.exceptions.CoordinateNotFoundError\n\t\treturn [self.nc.variables[coord_name].attributes[\"units\"]]\n\n\tdef _update_coordinates(self, coord_name, data_dims=None):\n\t\t\"\"\"\n\t\tUpdate the coordinates, converting if necessary.\n\n\t\tArgs:\n\n\t\t* coord_name -> CF variable name or int\n\t\t\tVariable name or coordinate name(es) to update.\n\n\t\tReturns:\n\t\t * A list of dimension coordinates.\n\t\t\"\"\"\n\t\tif not isinstance(data_dims, collections"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\treturn np.arange(0, coord_name.coord.shape[0]), np.arange(coord_name.coord.shape[0])\n\n\tdef _setup(self):\n\t\t\"\"\"\n\t\tBuild function that constructs the error array in memory.\t\n\t\tSubsequent calls to _setup() will load up the geometry with\n\t\tthe provided data.\n\t\t\"\"\"\n\t\traise NotImplemented()\n\n\tdef _create_coords(self, coord_name, data, coord_type):\n\t\t\"\"\"\n\t\tCreate and return a new data array for a given type and\n\t\treturns it filled with"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\treturn [self._dim_coord, self._dim_name, coord_name]\n\n\t@property # type: ignore # not in py3compat\n\n\tdef get_dim_coords(self, coord_name):\t\t# type: (str) -> List[Coord]\n\t\t\"\"\"\n\t\tGet coord indices corresponding to a coordinate.\n\t\t\n\t\t:param str coord_name: Cube name (i.e. 'lat;lon;depth').\n\t\t\n\t\t:return: Tuple of (Dim coordinate, list of dimension indexes (int))\n\t\t\"\"\"\n\t\tif coord_name not in"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tout_names = np.array( [str(l+1) for l in range(len(coord_name))] )\n\t\tout_index = [ (x[len(coord_name)] == 'z_coord') for x in out_names ]\n\t\tout_indices = np.array( out_index )\n\t\treturn out_indices\n\n\tdef _get_err_name(self):\n\t\tcoords_file = os.path.join(self.output_file, 'coordinates.npy')\n\t\tcoords = np.load(coords_file)\n\t\treturn coords"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tcoord = self._coord_map[coord_name]\n\t\tif coord.standard_name.upper() in (\"ICRS\", \"NONE\", \"GREGORIAN\"):\n\t\t\tfrom warnings import warn\n\t\t\twarn(RuntimeWarning(\"get_err_indices is not implemented for coordinates for variable {}. \"\n\t\t\t\t\t\"Use CRS/ICRS instead\"\n\t\t\t\t\t.format(self.name_or_coord), stacklevel=2))\n\t\treturn coord._coord._time_index.indices\n\n\n\tdef _create_coord(self):\n\t\traise NotImplementedError(\"abstract method _create_coord()\")\n\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tlat_idx, lat_val = self.get_coord_point(coord_name)\n\t\terr_idx = self.data_model[:, self.coords_name2, lat_val] >= -90\n\t\terr_idx = err_idx[err_idx.astype(np.int)]\n\t\terr_idx = err_idx.astype(np.uint64)\n\n\t\terr_idx = err_idx.nonzero()[0]\n\n\t\treturn err_idx\n\n\ndef _get_coord_value_from_coords_id(coords_id):\n\t\"\"\"\n"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tcoord = _get_x(coord_name)\n\t\tif coord:\n\t\t\treturn range(int(coord[0])-1, int(coord[0]+1))\n\t\telse:\n\t\t\treturn None\n\n\tdef error(self, coord_name, index, text):\n\t\t\"\"\"\n\t\tGet one error message out of a given coordinate.\t\n\n\t\tThis differs from the error() function in that it does not\n\t\tcheck that the given index is not associated with the given text\n\t\tstring.\n\n\t\t'coord_name' is the name of the axis of the coordinate that\n\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\t\"\"\"\n\tGet error indices corresponding to a coordinate.\t\"\"\"\n\t\tc = self.cube\n\t\tcindx = tuple([i for i, dim in enumerate(c.ndim) if coord_name in np.ndindex(dim)]) \n\t\tcindx = cindx.reshape(c.shape[0] + c.shape[1:])\n\t\treturn cindx\n\t\t\n\tdef add_error_to_index(self, error, coord_name):\n\t\t\"\"\"\n\t\tAdds a given error to the index (given the associated coord_name).\n\t\t\"\"\"\n\t\tc_indx = _get_err_indices(coord_name)\n\t\tself."}]}
{"task_id": "62b87b7e9a0c4fa8b80b35bc", "prompt": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\tpass\n\n\tdef _get_label(self, context):\t\"\"\"\n\tReturn a string name to prepend to axes labels if any.\n\tReturn an empty string if no axes labels found.\n\t\"\"\"\n\t\tfor i, ax in enumerate(context.axes_labels):\n\t\t\tif ax is None:\n\t\t\t\treturn \"\"\n\t\t\tif not ax in self.parent._axes_labels:\n\t\t\t\t\treturn \"\"\n\t\t\tif len(self.parent._axes_labels[ax]) > self.parent._axes_labels[ax][0]:\n\t\t\t\treturn self.parent._axes_labels["}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t self.ctx = context.copy()\n\t\tself.ctx['flow'] = dict(self.get_flow())\n\t\tself.ctx['subcontexts'] = dict(self._get_subcontexts())\n\t##\n\tdef _get_subcontexts(self):\n\t\t\"\"\"\n\t\t*Context* dict that maps subgraph identifier to\n\t\t:class:`.SubContext` instance.\n\t\t\"\"\"\n\t\treturn self._subcontexts.get(id(self))\n\n\t@classmethod\n\tdef load(cls, path, load_with_content=True, context=None):\n\t\t\"\"\"\n\t"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\tpass\n\n\tdef __init__(self):\n\t\tglobal __main__\n\t\tglobal __data_name__\n\t\tglobal __context_info__\n\n\t\tself.__data_name__ = \"__data_name__\"\n\t\t__data_title__ = \"__data_title__\"\n\t\tif not \"__main__\" in __main__:\n\t\t\tif  not hasattr(__main__, \"__name__\"): # Only initialize once.\n\t\t\t\t__main__ = self\n\t\t\t\tself.__name__ = \"__main__\"\n\t\t\telse:\n\t\t\t\tself.__name__ = __main__.__name__\n\t\t\tself.__main__ ="}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\tpass\n\n\tdef __getitem__(self, key):\n\t\treturn self.get_subcontext(key)\n\n\tdef get_subcontext(self, key, default = None):\n\t\t'''Return dict of ``key`` in the subcontext graph'''\n\n\t\tsubcontexts = self.get_ancestors(key)\n\t\tif key in subcontexts:\n\t\t\treturn subcontexts[key]\n\t\telse:\n\t\t\treturn default\n\n\tdef set_value(self, key, value):\n\t\t'''Modify *key* in the flow. Will not touch it'''\n\t\tself.set_sub"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\traise NotImplementedError()\n\n\t@property\n\tdef value(self):\n\t\t\"\"\"\n\t\tThe (local) value of the graph. Raises an exception\n\t\tif node or edge are non-existent.\n\t\t\"\"\"\n\t\traise NotImplementedError()\n\n\t@property\n\tdef context(self):\n\t\t\"\"\"\n\t\tWhen this graph was initially created, it means that one\n\t\tcontext for this graph has already been set.\n\t\t\"\"\"\n\t\treturn self\n\n\nclass Graph(object):\n\t\"\"\"\n\tThe *Object Graph* keeps a single, large, hierarchical,\n\tdictionary. The *Graph"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t \n\tself._context.error = context.value = \"\"\n\t\nclass GraphSetFields(object):\n\t\"\"\"\n\tDefines the fields we use to represent graphs that\n\tconsists of objects.\n\n*fields.fieldname* is a string giving the name of the field\n\tin the GraphSet. This gets loaded in the database when the\n\tgraph sets have been loaded.\n\n*fields.parent* is the graph object which should be the parent of all \n\tfields it belongs. In that case we are not the root.\n\"\"\"\n\tdef __init__(self): pass"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\tpass\n\n\tdef reset(self):\n\t\t\"\"\"This reset method ensures that any modifications is to\n\t\tbe made afterwards.\"\"\"\n\t\tpass\n\n\nclass Context(object):\n\t\"\"\"Context manager for a graph - usually a stack of nodes.\n\t   E.g. context and context['name'] are the node name,\n\t   with a name \"foo\". We use a number as the name.\n\t   Example:\n\n\t\twith Context('foo', 1) as foo:\n\t\t    assert_equal(foo.bar, 1)\n\n\tBasic arithmetic operations:\n\n\t- the current node is a node with an identifier \""}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\nclass ErrorGraph(object):\n\t\"\"\"\n\tError graphs have methods for\n\t:const:`Graph.find_path()`.\n\tFor details on how the graph object is formed, see\n\t`Graph.find_node()`\n\t`Graph.find_edge()`.\n\t\"\"\"\n\t__slots__ = (\"error\", \"is_destruction\",\n\t\t\"flow_to_roots\", \"tree\", \"root\")\n\t\t\n\tdef __init__(self, error=None, root=None):\n\t\t\"\"\"\n\t\tInitialize the :class:`.ErrorGraph` object\n\n\t\t:param GraphError"}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\n\t\n\tdef __init__(self, context):\t\t\n\t\tfor field in self.fields:\n\t\t\tif field in context:\n\t\t\t\tv = context[field]\n\t\t\telif field in GRAPH_FIELDS:\n\t\t\t\tv = GRAPH_FIELDS[field]\n\t\t\telif field in NON_FIELDS:\n\t\t\t\tv = NON_FIELDS[field]\n\t\t\telse:\n\t\t\t\traise FieldError(field, context)\n\t\t\tsetattr(self, field, v)\n\t\tself.context = context\n\t\n\t\n\tdef __getattr__(self, attr):\n\t\ttry:\n\t\t\treturn self.context["}, {"generated_text": "def _update_context(self, context):\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\t\"\"\"\n\t\n        for key, value in context.subcontexts.iteritems():\n                self._update_context_value(context, key, value)\n\n        self.context.log(LOG_TRACE,\"context.errors: %s\" %\n                   dict(context.errors))\n        self.context.log(LOG_TRACE,\"context.errors: %s\" %\n                   dict((x, {'index': len(x) - 1}) for x in context.errors))\n\n        return True\n\n    @api.multi\n    def unlink(self):\n        \"\"\" Deletes this graph. \"\"\"\n        for id_ref in self.ids:\n            self"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3580", "prompt": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\treturn 0\n\n@_wrapdll(0)\ndef histo_dist(hists, edges):\n\t\"\"\"\n\tCompute the horizontal distance between two histograms \n\t\"\"\"\n\t# Compute histograms.\n\thists = histograms(hists, edges)[0]\n\thists.sort()\n\tt = 0\n\tfor h in hists: t += hn(h) - 1\n\treturn hists[t]\n\n@_wrapdll(0)\ndef xhistogram(bins, edges):\n\t\"\"\"\n\tCompute the x-axis distribution of a"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\ndef line_integral(bins):\n\t\"\"\"\n\tCompute line integral integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\ndef hanning(bins):\t\n\t\"\"\"\n\tCompute the hanning function (Hanning).\n\n*bins* contain values - *xmin*, *xmax*, *ymin*, *ymax* for the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\ndef vaverage("}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n# create the histogram grid\nhists = pylab_hists.empty(((len(bins),)*2,2))\n\n\n# fill the grid\naxes()\nhists('hists')\nbins = np.arange(0, 0.9, 0.1)\nfor h in hists:\n\tbin = bins[int(h[0]):int(h[1]), int(h[2])]\n\taxes()\n\t# plot histogram\n\thists.get_plot('hists/%s' % int(h))\n\tbin *= density / 1e4\n"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n\n\tdef __init__(self, a, b, p):\n\t\t\"\"\"\n\tConstructor.\nThis is called automatically when an object (like :class:`.Kernel`)\nis created, to produce a kernel.  To achieve uniform behaviour, use\n:meth:`.createKernel` when drawing the kernel.\n\n.. versionadded:: 0.6\n\n:param a: The lower bounds of the bins.\n:type a: :class:`.RegularGrid`\n:param b: The upper bounds of the bins.\n:type b: :class:`.RegularGrid`\n:param p: A list of points"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n\t\n\tedges.sort()\t# order and then the edges according to the number of nodes\n\tnum_nodes = len(edges)*(nodes_per_edge(edges)) / nnodes\n\t\n\t# get number of nodes per edge.\n\tnum_edge_nodes = np.zeros([nnodes, num_nodes])\n\tnum_edge_nodes[:num_nodes] = num_nodes\n\t\t\n\t\n\t\n\t# For each possible integral, get the edges.\n\tfor i in range(num_edges):\t\n\t\tfor j in range(nnodes):\n\t\t\tfor k in range(n"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n#TODO make this a real histogram class?\n#TODO move to other module :/\n\n#TODO make more general (using numpy and scipy)\n\n#TODO move to other module\n\nimport warnings\n\nimport numpy as np\nfrom numpy.random import randn\n\ntry:\n\t from scipy.ndimage import convolve2d\n\nexcept:\n\t pass\n\nfrom numpy.random import gaussian_kde\nimport numpy.random\nimport scipy.ndimage.filters\nimport scipy.ndimage.interpolation\n\nimport pylab\nfrom scipy.ndimage.filters import uniform_filter1d"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\ndef histogram_value_at_fbin(a):\n\t\"\"\"\n\tConvert an integer into a histogram value at the bin edges.\n\n\tFor example, if *bins* = 100 and *edges* = 25, then return\n\t::\n\t\n\t    (0, 0)\n\t\n\twill compute the integral (at the minimum, for the\n\tmaximal).\n\t\"\"\"\n\thistogram = histogram_value(bins, edges)\n\treturn tuple(histogram)\n# Copyright (c) 2013 The Chromium Authors. All rights reserved.\n# Use of this source code is governed by a BSD"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n#-----------------------------------------------------------------------------\n# Imports\n#-----------------------------------------------------------------------------\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport numpy as np\nimport matplotlib\n# Force matplotlib to not use any Xwindows backend.\nmatplotlib.use('Agg')\n\nfrom traits.api import Any, Str\nfrom traitsui.api import View, Item, HGroup, VGroup, HSplit, EnumEditor, VSplit\n\n\n#-----------------------------------------------------------------------------\n# Globals and constants\n#-----------------------------------------------------------------------------\n\n__all__ = ['Histogram']\n\n#-----------------------------------------------------------------------------\n# Helpers\n\n\ndef _get_interval(v, unit"}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\n\n\tdef scale(bins, edges):\n\t\t\"\"\"\n\t\tCompute scaling on a histogram.\n\n*bins* also contain an exponential binning using exponential bins. It\nis computed by dividing the integral by `scale`, which may be a non-negative\nconstant :math:`-exp(x) + i * scale`. The same definition is used for these values,\nso it is important to note that the edges are assumed to be 1.0. This\ndeals with the inherent integral of an exponential. The value ``scale`` will be converted\nto :math:`1 - exp("}, {"generated_text": "def integral(bins, edges):\t\"\"\"\n\tCompute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.\t\"\"\"\n\t\treturn 1. / (1. + edges[0] + edges[1] + 1. / edges[-1])\n\ndef periodictable(periodictable, key):\n\t\"\"\"\n\tDetermine if a density for each key exists.\nThe key must be a real number or zero (in the sense of a power of 2)\nor a float (in the sense of a length (in the sense of a unit).\nIf it cannot be determined, the object will be ignored.\n\n\tThe return value is the intensity, as two floats (for\n\tfloat, complex, etc"}]}
{"task_id": "62b87b199a0c4fa8b80b354e", "prompt": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tn = None\n\t\tfor x in re.split(r'\\s+', seq):\n\t\t\tif not n: # Only one non-empty string is left\n\t\t\t\tn = x\n\t\t\telif re.search(r'\\s$', x): # Must be a non-empty string\n\t\t\t\traise BadRequest(_(\"Malformed Fill Request: {} \"\n\t\t\t\t\t\t\t+ \"does not match '{}' (case insensitive)\"\n\t\t\t\t\t\t\t\t.format(x, seq)))\n\t\tif n is None:\n\t\t\tn = 'fill'\n\t\tif n!= seq:\n\t\t\tnewseq.add(n)\n\t\treturn n\n"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tif hasattr(seq.attributes[-1], 'is_fill_request'):\n\t\t\txml_type, seq_xml = seq.attributes[-1].split(\":\")\n\t\t\tif not is_fill_request_seq(seq_xml):\n\t\t\t\treturn False # we know that the element can't be converted \n\t\treturn True\n\n\ndef fill_request_seq(seq, root, tag, attributes=[]):\n\t\"\"\"Fill a Sequence Element without requiring it's attributes in the\n\tsequence with the given attributes.\n\t\"\"\"\n\tfor attr in attributes:\n\t\tattr_seq = seq.AppendElement"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\treturn (isinstance(seq, Source) or (isinstance(seq, Sink) and seq.source_type == SourceType.FILL) or\n\t\t\t(isinstance(seq, ELink) and seq.sink_type == SourceType.CONNECTOR) or seq.sink_type in (SourceType.SEQUENCE, SourceType.EXTERNAL))\n\ndef is_fill_request_seq_type(seq_type):\n\t\"\"\"\n\tTest whether *seq_type* represents an element of *ssp_source*\n\tthat is a fill request or not.\n\nTrue only if it"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tfor entry in seq:\n\t\t\tentry_start, entry_end, els = entry[:3]\n\t\t\tif entry_start >= max_entry_end:\n\t\t\t\treturn False\n\t\treturn True\n\t# end check in sequence\n\treturn False\n\n\ndef is_fill_request(els_type, req_type):\n\t\"\"\"\n\tTest whether *equiv* is a FillRequestElement,\n\tonly a source or a destination type.\n\n\tOnly two are currently supported.\n\t\"\"\"\n\tif not els_type:\n\t\treturn False\n\telif els"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t if isinstance(seq, FillRequestSeq):\n\t\t\treturn True\n\t\treturn False\n# -*- coding: utf-8 -*-\nimport datetime\nfrom south.db import db\nfrom south.v2 import SchemaMigration\nfrom django.db import models\n\n\nclass Migration(SchemaMigration):\n\n    def forwards(self, orm):\n        # Adding field 'Course.slug'\n        db.add_column(u'courses_course','slug',\n                      self.gf('django.db.models.fields.CharField')(default='', max_length=255, blank=True),\n                      keep_default=False)\n\n    def"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\treturn hasattr(seq, 'fill_request')\nclass SlowRequest:\n\t\"\"\"\n\tRepresents a Slow request.\n\t\n\tSlow requests will be parsed for each of its values, either\n\tan instance of the class for SlowResponse or an instance of\n\tSourcePair or a Type of SlowResponse to indicate this particular\n\tSlow response.\n\t\"\"\"\n\n\tdef __init__(self, start, end,\n\t\t\tref, query, type, seq, equiv_match, source_length,\n\t\t\tfirst_request_id, first_request_index, first"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\ndef is_source_sequence(seq, fill_seq):\n\t\"\"\"\n\tReturn True if *seq* can be a source sequence.\n\t\n\tExample::\n\n\t\t    >>> from sequence import Sequence; seq = [0,1,2]; fill_seq = [1,2,3]\n\t\t   >>> from libtbx.seq import SourceSequence; extend_seq = [0,1,2]\n\t\t...   source_sequence, b = extend_seq.get_seq_with_fill_request_seq(0); extend_seq.insert(b, fill_seq)\n\t"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tif seq is None:\n\t\t\treturn True\n\t\telif seq.get_name() in ('fill_request', 'fill_block'):\n\t\t\treturn True\n\t\telif (type(seq) not in (types.ListType, types.TupleType)):\n\t\t\treturn False\n\t\telif seq.get_name() == 'fill_block':\n\t\t\treturn True\n\telse:\n\t\treturn False\n\t\t\t\n##\n# Check the list of FillableRequest from the last element\n#\n# This function only works in one element\n# (the element that has no FillableRequest)\n#\n#"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tif len(seq) == 0:\n\t\t\treturn True\n\n\tif seq[0].value!= \"none\":\n\t\treturn False\n\n\treturn True\n\n## Utility functions\n\ndef has_one_fill_request_arg(a):\n\t\"\"\"\n\tDoes this element have one fill request arg?\n\t\"\"\"\n\treturn str(a.getAttribute(\"fill-request-arg\")) == \"1\"\n\ndef check_first_element(b, element):\n\t\"\"\"\n\tDoes this element have an \"first\" element?\n\n\tReturns a list of strings indicating whether it's"}, {"generated_text": "def is_fill_request_seq(seq):\t\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\t\"\"\"\n\t\tif hasattr(seq, \"__getitem__\"):\n\t\t\treturn seq.__getitem__(0) is None\n\t\tif hasattr(seq, \"seq\"):\n\t\t\tl = len(seq)\n\t\t\treturn seq.seq == l\n\t\treturn False\n\ndef is_fill_request_field(seq):\n\t\"\"\"\n\tTest whether *seq* can be converted to a FillFieldRecord.\n\nTrue if it has a sequence.\n\t\"\"\"\n\tif hasattr(seq, \"__getitem__\"):\n\t\tif seq[0].seq == seq[-1].seq:\n\t\t\treturn True\n\treturn"}]}
{"task_id": "62b87b099a0c4fa8b80b3538", "prompt": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\tif obj.get(\"has_exe_method\"):\n\t\t\treturn True\n\t\treturn obj.get(\"has_no_entry_method\") and obj.get(\"has_fixed_entries\")\n\tdef get_condition_args(self):\n\t\tcond = {}\n\t\tif self.condition:\n\t\t\tcond.update(self.condition)\n\t\treturn cond\n\tdef fix_dups(self, lst):\n\t\t\"\"\"\n\t\tSome of these have several problem:\n\t\t- multiple conflicting entry points\n\t\t- both of these have the same number of fields\n\t\t- no additional conflicts"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\treturn is_get_command('fill') or (isinstance(obj, Command) and obj.is_fill_request())\n\t@staticmethod\n\tdef is_fill_request(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and'request'.\t\"\"\"\n\t\treturn is_get_command('fill') or isinstance(obj, Command)\n\t@staticmethod\n\tdef is_set_request(obj):\t\"\"\"\n\tObject contains executable methods'set' and'request'.\t\"\"\"\n\t\treturn is_get_command('set') or isinstance(obj, Command)\n"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\treturn not (\n\t\t\t# is_fill_request_method returns this code block\n\t\t\tisinstance(obj, (tuple, list))\n\t\t\tand len(obj) == 3\n\t\t\tand isinstance(obj[0], ast.Name)\n\t\t\tand obj[0][0] == 'fill'\n\t\t\t)\n\ndef is_fill_request_method(method):\t\"\"\"\n\tCheck whether this function is a fill request method.\n\t\"\"\"\n\treturn method.func_name in ('fill','request')\n\ndef is_class_methods(obj):\t\"\"\"\n\tFind"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\treturn (isinstance(obj, b2handle.common.BaseObj) and \\\n\t\t\t('b2handle.common.BaseObject' in obj.__class__.__bases__\n\t\t\t\tand 'b2handle.common.BaseObject.fill' in obj.b2handle._class_fields)\n\t\t\t)\n\t\t\n\tdef fill(self, request_el):\t\t\"\"\"\n\tFill the response from a node.\n\tCan be overwritten by setting `obj.fill_request_el`.\n\t\"\"\"\n\t\tassert self.obj.has_request_el()\n\t\tassert not request_el"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\ttry:\n#\t\t\tobj.prototype in (\"fill\", \"request\")\n\t\t\t\treturn obj._is_fill_request_el(obj)\n\t\t\texcept AttributeError:\n\t\t\t\treturn False\n\t\t\n\t\texcept AttributeError:\n\t\t\treturn False\n\t\n\tdef is_fill_request_none(obj): # TODO: do we need to check that\n\t\t\"\"\"\n\t\tIf the request is None, this method returns 'false' if the\n\t\trequest is empty.\n\t\t\"\"\"\n\t\ttry:\n\t\tif obj._is_fill_request_none(obj):\n\t\t\treturn False\n\t\t\n\t\texcept AttributeError:"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\tif not obj or obj.strip() == '' or obj.strip() == u'': return False\n\t\tif is_open(obj):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn obj[0] == '#' or obj.startswith('#') or re.match('%\\([^\\)]+\\)', obj).end()\n\treturn True\n\t\t\ndef check_syntax_errors(s):\n\tglobal f_rules, f_modules\n\terr_msg = []\n\tfor line in s.splitlines():\n\t\tif '\t!/usr/bin/python' in line: err_msg"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\tif isinstance(obj, dict) and obj['name']==\"fill\":\n\t\t\treturn True\n\t\telif isinstance(obj, list):\n\t\t\treturn all((fill_element(s, obj[idx]) for idx,s in enumerate(obj)))\n\t\telse:\n\t\t\treturn obj.get(\"request\") == \"Fill\" or obj.get(\"request\") == \"Request\" or \\\n\t\t\t\tobj.get(\"fill_default\") == \"Fill_Default\" or obj.get(\"fill_request\", \"\") == \"Request\"or obj.get(\"fill_default\") == \"Request_Fill\""}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\treturn type(obj)==str and obj.startswith('fill(')\n\t#end def is_fill_request_el\n\n\ndef is_fill_request_list(sourcelist):\n\t\"\"\"\n\tCheck if given source list is a fill request list, which is either\n\ta list containing a single or a dict whose keys are tuples of names\n\tand values of acceptable input types.\n\t\n\tNOTE: this is a generator, not a generator!\n\t\"\"\"\n\tif not hasattr(sourcelist, '__iter__'):\n\t\traise TypeError('sourcelist is not a list')\n\tfor item"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\treturn obj.__class__.__name__ == 'FillRequest'\n\n\tdef is_line_request_el(obj):\t\t\"\"\"\n\tObject contains executable methods 'line' and'request'.\t\"\"\"\n\t\treturn obj.__class__.__name__ == 'LineRequest'\n\n\tdef is_curve_request_el(obj):\t\t\"\"\"\n\tObject contains executables methods 'curve' and'request'.\t\"\"\"\n\t\treturn obj.__class__.__name__ == 'CurveRequest'\n\n\tdef is_poly_request_el(obj):\t\t\"\"\"\n\tObject contains executable methods 'poly'"}, {"generated_text": "def is_fill_request_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\t\"\"\"\n\t\tdef call_fill(s, e):\n\t\t\treturn s in e\n\t\treturn call_fill\n\t\n\t# ---------------------------------------------------------------------------\n\tdef is_file_open(f):\n\t\t\"\"\"\n\t\tChecks if text file is open.\t\t\t\n\t\t@param f file name \n\t\t@return flag, 'true'\n\t\t\"\"\"\n\t\tf_open_flag = False\n\t\ttry:\n\t\t\tf_open_flag = os.fstat(f.fileno()) # returns file pointer\n\t\t\tf_file = open(f, 'rb')\n\t\texcept IOError:\n\t\t\tpass\n\t\ttry:"}]}
{"task_id": "62b87af99a0c4fa8b80b3524", "prompt": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\trun_pattern_map = {\n\t\t\t'ps':   lambda p: p.startswith('ps'),\n\t\t\t's':   lambda s: s.startswith('source'),\n\t\t\t'm':   lambda m: m.find('%'),\n\t\t\t'x':   lambda s: s.startswith('executable'),\n\t\t}\n\t\ts = check_for_regex(run_pattern_map)\n\t\treturn s!= None and obj.executable_name() in s.lower()\n\n\tdef is_executable(obj):\t\"\"\"\n\tObject contains executable method 'which'.\t\"\"\"\n\t\twhich_"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn obj.get('run')=='yes'\n\n\tdef is_skip_clean(obj):\t\"\"\"\n\tRun only when the obj file is to be skipped. If obj is \n\tnot a directory it does not check for permissions for the \n\tspecified executable method\n\t\"\"\"\n\t\treturn obj.get('skip','')=='yes'\t\n\t\t\n\tdef is_pre(obj):\t\"\"\" \n\tOnly if obj is a directory and all its executable methods \n\tare executed. Return true if obj is a directory. \n\tIf obj is a file and its executable methods are in obj_to_"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn obj.attr('run')\n\ndef compile_args(source, options, cc, options_last, link_with):\n\t\"\"\"\n\tCompiles module with options and optionally runs given command.\n\n\tArgs:\n\t\tsource (str): The compilation source\n\t\toptions (list): The options from the command-line\n\t\tcc (str): A custom compiler name.\n\t\toptions_last (list): A list of options with last input\n\t\tlink_with (str): The command to run, if any\n\tReturns:\n\t\tbool: True if compilation was successful, else False"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn (obj.rc == 0)\n\ndef parse_run_el(str_run_el, obj):\n\t\"\"\"\n\tParse an RVRun string for a run.\n\t@obj: obj of class Run\n\t@str_run_el: string to parse\n\t@obj: obj of class RVRun\n\t@return: parsed object\n\t@rtype: L{Run}\"\"\"\n\trun = Run()\n\trun.rc = obj.rc\n\trun.run_time = obj.run_time\n \tif obj.rc == 0:\n\t    "}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn hasattr(obj, 'run') and is_executable(obj.run)\n\n\ndef load_lib():\n\t\"\"\"\n\tLoad the static library using ctypes and then load it as a PyObjC library.\n\t\"\"\"\n\tfrom ctypes import cdll, c_int, c_char_p, c_char, c_char_p, c_double\n\tfrom distutils import sysconfig\n\n\t_lib = cdll.LoadLibrary('_corewrap32.so')\n\tsyslib = sysconfig.get_config_var('Py_SYS_CORELIB')\n\n\t# add"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn isinstance(obj, run)\n\tdef _get_attrs(obj, attrs):\n\t\tif hasattr(obj, 'attr'): return obj.attr\n\t\tret=[]\n\t\tfor a in attrs:\n\t\t\tret.extend(get_attrs(obj.obj, a))\n\t\tif hasattr(obj, 'run'): ret.append(obj.run)\n\t\treturn ret\n\tdef is_cshlib(obj):\t\"\"\"\n\tObject contains the main cross-compiler.\n\t\"\"\"\n\ts='.c'\n\trc=CC(c_PATTERN,CXXCOM,'-"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn isinstance(obj, ELF_RUN)\n\n\tdef execute(self, context):\n\t\tself.executable = os.path.basename(self.filepath)\n\t\tself.file = self.filepath.replace(\".e\", \".o\")\n\t\tif not os.access(self.file, os.R_OK):\n\t\t\traise IOError(\"File %s does not exist\" % self.file)\n\t\ttry:\n\t\t\tsubprocess.call('\"%s\" < %s > %s' % (self.executable, self.file, self.filepath))\n\t\texcept error_nonport"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\tif not os.path.isfile(obj):\n\t\t\treturn False\n\t\treturn Utils.to_bool(os.access(obj, os.X_OK))\n\n\tdef run(self, cmd, extra_env = None, stdout = None, stderr = None) :\n\t\t\"\"\"\n\t\tRun the specified command.\n\t\tIf stderr is set (default), standard output is redirected to STDOUT\n\t\totherwise this will be None.\n\t\t\"\"\"\n\t\tself.log.debug(\"Running %s\" % cmd)\n\t\tif self.trace:\n\t\t\ttracecmd = Utils.to_"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn bool(obj.attrs.get(\"run\", \"no\"))\n\tdef get_elf(self, *kids, **kwargs):\n\t\t\"\"\"\n\t\t\tGet executable's binary, or just its binary data.\n\t\t\tCall elf binary() method to retrieve binary data.\n\t\t\"\"\"\n\t\tenv = dict(os.environ)\n\t\tenv['PATH'] = \"%s:%s\" % (\n\t\t\t\tos.path.dirname(os.path.abspath(__file__)),\n\t\t\t\tUtils.mk_safe(KDBG_CACHE_DIR))\n\t\tif os.environ.get('PATH"}, {"generated_text": "def is_run_el(obj):\t\"\"\"\n\tObject contains executable method 'run'.\t\"\"\"\n\t\treturn isinstance(obj, str) and obj.endswith('.py') and not obj.startswith('__init__.py') and not obj.endswith('.pyc')\n\n\tdef is_runnable(self, target):\n\t\t\"\"\"\n\t\tCheck if target object is really runnable.\n\t\t\"\"\"\n\t\tfor node in self.generator.bld.node_deps[self.uid()]:\n\t\t\tif node.run_after:\n\t\t\t\treturn node.hasrun\n\t\treturn self.runnable_status(target)\n\n\tdef runnable_status(self, target):\n\t\t\"\"\"\n\t\tCheck"}]}
{"task_id": "62b87af69a0c4fa8b80b351a", "prompt": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\t# TODO check the type of object is correct\n\t\tassert 'write' in dir(obj)\n\n\t# NOTE we do not check if the object is built with the object's\n\t# ``fill`` method\n\tdef is_library(obj):\n\t\t\"\"\"\n\t\tObject is a library function that takes either an object (a function)\n\t\tor an object itself as an argument, and returns True,\n\t\treturning False, or a string, not an object.\n\t\t\"\"\"\n\n\t\t# Check if the object is an object built with the object method\n\t\tmethod = getattr(obj, 'fill"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\ttry\tobj._source.fill()\n\t\texcept NotImplementedError:\n\t\t\treturn False, False\t\n\tdef evaluate_value(self, node):\n\t\t\"\"\"Return the value of the expression in the node, or try to eval it.\"\"\"\n\t\tif HAVE_ARGUMENTS is False:\n\t\t\treturn None, None\n\n\t\tif isinstance(self.eval_path[0], basestring):\n\t\t\tif self.eval_path is not None:\n\t\t\t\tif hasattr(self.eval(self.eval_path[0]), 'value'):\n\t\t\t\t\t# call _eval instead of setting the returned value\n\t\t\t\t\t"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\t# This is a workaround for a bug in Windows that prevents us from doing\n\t\t# an application file. The problem is the following:\n\t\t#\t/usr/bin/g++ -s -i /dev/null -c \"\n\t\t#\t\t\t-s _<PROGRAM_FILES_RE (for Win7) in /usr/lib/gcc/arm/bin/...\n\t\t# This will prevent libclang from generating the obj file, which is\n\t\t# then deleted by the compilation.\n\t\tif not obj.get_exe():\n\t\t\treturn False\n\t\tis_fill_"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\tif not obj:\n\t\t\treturn False\n\n\t\t# create new proc objects\n\t\tfor e in obj.executable.executable_order:\n\t\t\tif e == 'fill':\n\t\t\t\tnew_proc = Process()\n\t\t\t\tnew_proc.executable = Process()\n\t\t\t\tnew_proc.executable.executable_order.append(process.executable)\n\t\t\t\tnew_proc.is_fill = True\n\n\t\t\t\tfor p in procs:\n\t\t\t\t\ttry:\n\t\t\t\t\t\ttmp = p.exe.executables.get(e, None)\n\t\t\t\t\t\tif type(tmp) is types.ListType:\n\t\t\t\t\t\t\tnew"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\tres = obj.type.fill_compute(context)\n\t\tif res:\n\t\t\treturn True\n\t\treturn False\n\ndef fill_compute(epl, data):\n\t\"\"\"\n\tFill the entire epl object\n\t\"\"\"\n\tobj = epl.get(name=data[\"name\"])\n\n\t# TODO : add `compute_mode` here.  Does it really need it?\n\t# check data\n\tif \"compute_mode\" in data:\n\t\tpass\n\telse:\n\t\tcheck_data = lambda obj, ctx: all(x(obj, ctx"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\tif hasattr(obj, 'fill'):\n\t\t\t# XXX: remove\n\t\t\treturn not obj.fill\n\t\telse:\n\t\t\t# XXX: remove\n\t\t\treturn False\n\n\tdef read(self, start=0, end=None, length=None):\n\t\t\"\"\"\n\t\tReturn the content of the file at the start of the line.\n\n\t\tstart -- optional starting index of line. If None, the list of\n\t\tall available lines will be read.\n\t\tlength -- optional number of bytes for the end of the file. If\n\t\tNone, the whole file content will be read.\n\t\t"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\t# We treat all non-blank lines\n\t\tobj = obj.rstrip()\n\n\t\t# Skip some lines after the first ':' so that if the line\n\t\t# starts with a quote mark, we end up skipping it.\n\t\tif obj.startswith(\"'\") or obj.startswith('\"') or obj.startswith(\"'\"):\n\t\t\treturn False\n\n\t\t# Now skip any blank lines that don't contain a quote.\n\t\tskip_blank = False\n\t\tfor l in obj.split(\"''\"):\n\t\t\tskip_blank = (skip_blank or l == '') and not l.startswith(\"'\")\n\n\t\treturn skip"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\tdef is_fill_compute_el_executable(obj):\n\t\t\treturn obj.returncode == 0\n\n\t\tobj, _ = Popen_safe(*is_fill_compute_el_executable(obj))\n\t\tif not is_fill_compute_el_executable(obj):\n\t\t\treturn 0\n\t\treturn obj.returncode == 0\n\n\t@staticmethod\n\tdef is_in_group_in_exec_or_group_mode(path, line):\n\t\tline_idx = line.find(f' ')\n\t\tif line_idx == -1:\n\t\t\treturn False\n\t\t"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\tif obj.cmd == cmd.OBJ_CMD_ARRAY:\n\t\t\treturn obj.fill_compute()\n\t\treturn obj.fill()\n\n\tdef add_command_with_complicated_command_id(self, cmd):\n\t\t\"\"\"\n\t\t@param cmd: command to execute and add, not already built\n\t\t@type  cmd: L{obj_command.obj_command}\n\t\t@rtype:\tL{obj_command.obj_command}\n\t\t\"\"\"\n\t\tself.commands_by_name[cmd.name] = cmd\n\t\tself.commands_for_"}, {"generated_text": "def is_fill_compute_el(obj):\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\t\"\"\"\n\t\treturn hasattr(obj, '__fill__') or hasattr(obj, '__core_')\n\n\tdef do_compute(self):\n\t\t\"\"\"\n\t\tPrepare the computed results from all the output nodes.\n\t\t\"\"\"\n\t\tnodes = self.outputs.keys()\n\t\tfor name in nodes:\n\t\t\tnode = self.inputs[name]\n\t\t\t# compute the total\n\t\t\ttotal, success = self.compute_node(node, output_node = True)\n\t\t\ttotal_cores, success = self.compute_node(node, output_node = False)\n\t\t\tif success:\n\t\t\t\t# compute"}]}
{"task_id": "62b87af19a0c4fa8b80b34f7", "prompt": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\ndef add(d1, d2, **kwargs):\n\t\"\"\"\n\tReturn a *new* dictionary with values from *d1* appended to the\n\t*new* dictionary.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2*, but has different values,\nit is included into the sum.\nSee :func:`insert` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\treturn self.copy(deep=1)\n\n\tresult = Dicts()\n\tfor k, v in d1.items():\n\t\tif level == -1:\n\t\t\tfor i in v:\n\t\t\t\tassert i in d1, \"Can't differ {} -> {}\".format(k, d1.keys())\n\t\t\tresult.add(k)\n\n\t\tresult[k] = v - d1.get(k, [])\n\n\tfor k, v in d2.items():\n\t\tif level == -1:\n\t\t\tfor i in v:\n\t\t\t\tassert i in d1,"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t return {}\n# ----------------------------------------------------------------------------\n# Copyright (c) 2013--, scikit-bio development team.\n#\n# Distributed under the terms of the Modified BSD License.\n#\n# The full license is in the file COPYING.txt, distributed with this software.\n# ----------------------------------------------------------------------------\n\nfrom tempfile import mkdtemp\nfrom unittest import TestCase, main\n\nfrom skbio import Protein, Interval, RNA, ProteinIO, RNAseq\nfrom skbio.io import write\n\nimport skbio.sequence\nimport skbio.format\nfrom skbio.metadata import SequenceMetadata\n\n\nclass WriterTests"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t # TODO: refactor into functions that accept keyword arguments\n\t\tif len(d1) > len(d2):\n\t\t\td2 = dict(zip(d2.keys(), d1.keys()))\n\t\td2 = dict([(k, i) for i, k in d2])\n\t\tresult = {}\n\t\tfor key, val in d2.items():\n\t\t\tresult[key] = val - 1\n\t\tresult = new(result)\n\t\tresult.update(d2)\n\t\treturn result\n\t\n\tif level is None:\n\t\tlevel = -1\n\troot = collections"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\traise NotImplementedError()\n\n\tif level < 0:\n\t\ts = {}\n\t\tfor k, v in d1.iteritems():\n\t\t\tif v == d2.get(k):\n\t\t\t\tcontinue\n\n\t\t\ts[k] = d1.get(k)\n\t\td2 = d2.copy()\n\n\tresults = {}\n\n\tdef flatten(l, level=_defaultlevel, _recursive=True):\n\t\tfor v in l:\n\t\t\tflatten(v, level=_recursive, _recursive=_recursive)\n\n\tfor k, v in d1.iteritems():\n\t\tif not _"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\tif level < 0:\n\t\t\traise ValueError(\"must be non-negative\")\n\t\tif level == -1:\n\t\t\treturn d2\n\t\td = deepcopy(d1)\n\t\tif level == 0:\n\t\t\treturn d\n\t\td2 = deepcopy(d2)\n\t\tfor k in d:\n\t\t\tif k in d2:\n\t\t\t\tif isinstance(d2[k], dict):\n\t\t\t\t\tif all(k in subdict.keys() for subdict in d2[k].values()):\n\t\t\t\t\t\td[k].update(copy.deepcopy(d2[k]))\n\t\t\t\t"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\n       items = dict(d1.items())\n       items.update(d2.items())\n\ttry:\n\t\t#print \"d1:\", d1\n\t\t#print \"d2:\", d2\n        except AttributeError:\n\t\t\t#print \"d1 is None and not None!\"\n\t\t\treturn {}\n\t #print \"d1 or d2:\",d1\n        # check if keys in d2 are the same\n\t\tif dict.has_equal(_keys_only(d1, d2, _keys_only),\n\t\t\t\t\t\t\tkey_list(d1) + key_list(d2"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\t_k = []\n\t\tdef _s(*args):\n\t\t\tfor k in args:\n\t\t\t\tv = d2[k]\n\t\t\t\tif isinstance(v, dict) and set(v.keys()) == set([]).union(_k):\n\t\t\t\t\t_k.append(k)\n\t\t\t\telse:\n\t\t\t\t\t_k.append(k)\n\t\t\treturn _s\n\t\treturn _k\n\t\n\tdef intersection(*keys):\n\t\t_k = [ d2[k] for k in keys if len(d2[k]) > 0 ]\n\t\t_k = set(keys + _"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t return d1\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n"}, {"generated_text": "def difference(d1, d2, level=-1):\t\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\t\"\"\"\n\t\treturn dict(zip(iter(d1), iter(d2)))\n\n\nclass IterationFunction(object):\n\t\"\"\"Definition of an iteration function.\n\n\tAttributes, methods, etc., should usually be defined either\n\tby instances of :class:`IterationFunction` or implement the\n\t``for' statement for an iteration function.  The constructor should\n\tmake sure the ``get_iterations`` method (used in this class) is called with\n\ta :class:`IterationFunction` instance at construction.  If\n\tnecessary, the class must also implement a method named\n\t``step``"}]}
{"task_id": "62b87af09a0c4fa8b80b34f1", "prompt": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tfor edge in self.edges:\n\t\t\t\th = self.edges[edge] + weight\n\t\t\t\t\n\t\t\t\t# check if we can actually fill the histogramm\n\t\t\t\tedges = self.get_edge_coordinates(edge)\n\t\t\t\tif edges:\t\n\t\t\t\t\tif h < len(edges):\n\t\t\t\t\t\th -= 1\n\t\t\t\t\telse:\n\t\t\t\t\t\th = 0\n\t\t\t\t\t\t\n\t\t\t\t# fill histogram\n\t\t\t\th1, h2 = self._get_histogram(h, coord)\n\t\t\t\t# re-add to histogram\n\t\t\t\tif h2 <= h1: \n\t\t\t\t\tself.add_histogram(edge"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\traise NotImplementedError(\"%s.fill is not yet implemented or used.\")\n\t\t#return fill(self, coord, weight)\n\n\tdef find_histogram_entry(self, point):\n\t\t\"\"\"Returns the *Point2D* with the given *point* at *coord*,\n\t\tafter it is drawn.  \n\n\t\tThe returned tuple (x, y) is a tuple of three\n\t\t:class:`matplotlib.patches.Rectangle` objects.\n\t\t\"\"\"\n\t\traise NotImplementedError(\"%s.find_histogram_entry is not yet implemented or used.\")\n\t\t#return find_histogram_entry(self"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t self.coords(coord,weight)\n\t\t\nimport requests\nfrom bs4 import BeautifulSoup as bs\nimport logging\n\nlogging.basicConfig(level=logging.DEBUG, format=\"%(asctime)-6s: %(message)s\")\nlogger = logging.getLogger(__name__)\n\n\ndef get_user_agent():\n    response = requests.get('https://github.com/yinyue/yo-list-client/blob/master/LICENSE.md')\n    logger.debug(response)\n    resp = response.json()\n    print(resp)\n\n\ndef main():\n    get_user_agent()\n   "}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\td = self.data\n\t\td.sort()\n\t\tn = float(len(d))\n\t\ti_max = int((1.1 * (n - 1)) * 2)\n\t\tfor i in range(i_max):\n\t\t\tn1 = int(d[i])\n\t\t\tn2 = int(n1 + n)\n\t\t\tif i == i_max-1:\n\t\t\t\td[i:i+1][0] += weight\n\t\t\tif i > 2:\n\t\t\t\td[i:i+2][0] += weight\n\t\t\t\td[i:"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tr, g, b = coord\n\t\tn = self.maxX.get() + self.minX.get()\n\t\tif n < 2:\n\t\t\treturn self.histogram.get( (r, g, b), 0 )\n\t\treturn 0\n##\n##\tdef __iter__(self):\n##\t\t\n##\t\tfor r in xrange(self.maxX.get()):\n##\t\t\tfor g in xrange(self.minX.get()):\n##\t\t\t\tfor b in xrange(self.maxX.get()):\n##\t\t\t\t\tyield (self.histogram."}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\t# See if we can fill this histogram.\n\t\tif self.fill_prob is not None and self.fill_prob < coord.weight:\n\t\t\tself.fill_prob = coord.weight\n\t\t\treturn Color(self.fill_prob)\n\n\t\t# Set up the region of the regions to that rectangle.\n\t\t# These can be anything not part of an other rectangular region.\n\t\trect = Rectangle()\n\t\trect.dx = coord.x - abs(self.x)\n\t\trect.dy = coord.y - abs(self.y)\n\t\trect.w"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tif len(self.hmap) == 0:\n\t\t\traise GridException(\"Can't fill empty histograms.\")\n\n\t\t# Get pixel data\n\t\tx, y, values, n, data = self.hmap[coords]\n\n\t\t# Calculate histogram\n\t\tr = ((x-x0)**2 + (y-y0)**2)**0.5\n\t\tg = self(np.linspace(min(self.tvals), max(self.tvals), 100), r, gOptions = {\"vmin\" : -r})\n\n\t\t# Get max value in region of color.\t"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tx = coord[0]\n\t\ty = coord[1]\n\t\tself.vertices[x][y][weight]=1\n\t\tif self.vertices[y][x] == 255:\t# This is the sign of 'not a square'\n\t\t\tself.vertices[y][x]+=1\n\t\tfor (edge,nbr) in self.edges(x, y):\n\t\t\tif nbr == 0: \n\t\t\t\tcontinue\n\t\t\tif nbr % weight == 0:\n\t\t\t\tself.vertices[x+nbr][y+nbr-1][weight]=1\n\tdef set_bounds("}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t\tif weight == 0:\n\t\t\tvcount, vsum = 1.0 / len(self)\n\t\t\tfor coord2, v in self:\n\t\t\t\tcoord2.fill(coord, v[vcount:vsum])\n\n\tdef copy(self):\n\t\t\"\"\"Return a copy.\"\"\"\n\t\tfrom copy import copy\n\t\ts = Cluster([])\n\t\tfor c in self:\n\t\t\ts += c\n\t\tfor other in self.difference():\n\t\t\ts += other\n\t\treturn s\n\n\tdef fill_array(self, array, weights=1.0, *args, **kwargs):"}, {"generated_text": "def fill(self, coord, weight=1):\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\t\"\"\"\n\t self.vspan = coord[len(self.bin)-1]\n\t\tself.nbin = len(self.bin)\n\t\t# TODO - is there a better way to do this? I need to do this\n\t\t# with a linear time search to estimate the distance in\n\t\t# each histogram bin instead of the weighted mean\n\t\t# we get better results.\n\t\tbinsum = np.diff(np.sign(self.vspan))/np.sqrt(self.vspan)\n\t\tweight *= binsum\n\t\tself.bin = self.dist(weight)\n\n\nclass Bin"}]}
{"task_id": "62b86aa3b4d922cb0e688d36", "prompt": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\tkeys = labels.keys()\t \t # Keys are unique to each key.\n\t\tkeys.sort()  # Sort them, this makes more sense than lists so that this\n\t\tfor k in keys:\n\t\t\ttry:\t# First, test whether the key already exists in label\n\t\t\t\ti = labels.get_by_id(k)\n\t\t\t\ti['$'][0]\t\t# The key was not already found, so assign the value in the dict.\n\t\t\t\tmessages = i.validate()\t# Checks against the keys.\n\t\t\t\tif messages: # If so, update correspondings and"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\texcept Exception as e:\n                        raise ValidationError(\"Labels: %r\" % (e,))\n\t\texcept Exception as e:\n                        raise ValidationError(\"Labels: %r\" % (e,))\n\n\tkeys = list(labels.keys())\n\tkeys.sort()\n\tif keys!= labels:\n\t\traise ValidationError(\"Labels do not match. Labels are expected.\")\n\n\tfor key in keys:\n\t\tif not isinstance(key, str):\n\t\t\traise ValidationError(\"Labels keys must be strings-like. Got %r\" % (key,))\n\t\tif not re.search(r'\\${\\w"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\tmsg = \"'$$': invalid\"\n\t\traise ValidationError(msg)\n\n\t# Example:\n\t#   labels = {\n\t#       \"keyx\": \"valid\",\n\t#       \"key5\": \"invalid\",\n\t#     }\n\t#   assert _validate_labels(labels) == [\"keyx\"]\t\n\n\tpass\n\ndef is_sequence(d):\n\t\"\"\"\n\tChecks whether d is a sequence containing only values that describe sequence data.\n\t\"\"\"\n\n\tfor key, value in d.items():\n\t\tif not isinstance(key, SEQUENC"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\traise\n\n\n\ndef _validate_match_labels(match_labels):\n\t\"\"\"\n\tCheck if the given regex's corresponding label matches any of the keys\n\tdefined in the match labels list.\n\nArgs:\n    match_labels (str or list of str)\n    \"\"\"\n\n\tif not isinstance(match_labels, list):\n\t\tif not isinstance(match_labels, basestring):\n\t\t\traise ValueError(\"expected string or list of str\")\n\t\telse:\n\t\t\traise TypeError(\"expected list of str\")\n\n\tfor match_label in match_labels:\n\t\tif not isinstance(match"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\ndef _validate_description(description):\n\t\"\"\"\n\tCheck that keys and values in the given description match against their corresponding\nregular expressions.\n\nArgs:\n    description (dict): the different description to validate.\n\nRaises:\n    ValidationError: if any of the keys and values does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the descriptions. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message."}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\t# This is done to ensure backwards compatibility for old versions of\n\t\t# Python. Use `hasattr(e, '__iter__')` and `isinstance(e, basestring)` to\n\t\t# ensure the message is valid YAML so we can assert that all values in\n\t\t# the list are of the correct type\n\t\ttry:\n\t\t\tfor label, key in labels.items():\n\t\t\t\tval = labels[label]\n\t\t\t\tassert isinstance(key, basestring) and isinstance(val, basestring)\n\t\t\t\tassert isinstance(val, dict)\n\t\t\t\t\n\t\texcept AssertionError:\n\t\t\tpass\n\t\tfor l in labels."}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\t_validate_labels(\n\t\t\tlabels,\n\t\t\t\"Invalid label key 'invalid'.\"\n\t\t)\n\n\tif \"key\" in labels:\n\t\tvalue, labels[\"key\"] = labels[\"key\"].split(\" \", 1)\n\t\tif value == \"$$\":\n\t\t\treturn\n\n\t\tfor i, label in enumerate(labels[\"key\"]):\n\t\t\tmatched_key = _verify_regex(labels[\"key\"], label)\n\n\t\t\tmatched_value = _verify_regex(labels[\"value\"], label)\n\n\t\t\t# If the matched label is different, raise an error.\n\t\t\tif label.startswith(\"$"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\terrors = []\n\t\tfor label, exception in labels.items():\n\t\t\tfor expression in expression_exceptions:\n\t\t\t\tif re.match(expression, label) is not None:\n\t\t\t\t\terrors.append({\"labels\": label, \"expressions\": expression_exceptions[label]})\n\t\t\tfor exception in exception_exceptions:\n\t\t\t\tif re.match(exception, label) is not None:\n\t\t\t\t\terrors.append({\"labels\": label, \"expressions\": exception_exceptions[label]})\n\t\tif errors:\n\t\t\traise ValidationError({\n\t\t\t\t\"labels\": labels,\n\t\t\t\t\"errors\": errors\n\t\t\t})\n"}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\n            labels = {k: labels[k] for k in labels.keys()}\n            keys = set(labels.keys())\n            for k in keys:\n                v = None\n                try:\n                    v = labels[k]\n                except ValidationError as err:\n                    assert \"label key {!r} not valid\".format(k) in err.messages, err.messages\n                # This also handles non-strings and other strings such as None and\n                # numbers (to prevent from raising an error). The empty list is\n                # not allowed to be checked.\n                if v is None:\n                    assert isinstance(v, str)\n               "}, {"generated_text": "def _validate_labels(labels):\t\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]\t\"\"\"\n\t\treturn\n# This Python file uses the following encoding: utf-8\n\n# Copyright (c) 2009-2016, Mario Vilas\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#   * Redistributions of source code must retain the above copyright notice,\n#     this list of conditions, and the following disclaimer.\n#   * Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions, and the following disclaimer in the\n"}]}
{"task_id": "62b86a9eb4d922cb0e688d25", "prompt": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\ndef _get_package_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake packages.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the package name.\t\"\"\"\n\t\ndef _get_package_version_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe version of the Krake packages.  \n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the version.\t\"\"\"\n\t\n\t# first, make sure"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\ndef _get_regex_pattern():\t\"\"\"\n\tBuild or return the regular expression that comply with the Krake\n\tresources.\n\n\tReturns:\n\t(re.Pattern): the compiled regular expressions, to validate\n\tthe resource regex.\t\"\"\"\n\ndef _get_resource_regex_pattern():\t\"\"\"\n\tBuild or return the regular expressions that comply with the Krake\n\tresources.\n\nReturns:\n\t(re.Pattern): the compiled regular expressions, to validate\n\tthe resource regex.\t\"\"\"\n\ndef _get_event_regex():\t\t"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t return getattr(__main__,'_get_resource_name_regex')\n\n# --------------------------------------------------------------------------\n\n# These values control various aspects of the configuration for this file.\nCONFIG_FILE_PATH = os.environ.get('KRAKE_CONFIG_FILE_PATH')\nCONFIG_FILE_NAME = os.environ.get('KRAKE_CONFIG_FILE_NAME')\n# If running on App Engine :\n#from krake import __version__\n#else :\ntry:\n    KEYWORD_FILE_PATH = os.environ.get('KRAKE_KEYWORD_FILE"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\treturn re.compile(r'[a-zA-Z_][_a-zA-Z0-9]*')\n\n\ndef list_resources():\n\t\"\"\"\n\tReturn the available resources list\n\nReturns:\n    (list): The list of supported resource names.\n\n\"\"\"\n\tresources_file ='resources.txt' if os.path.exists('resources.txt') \\\n\t\telse'resources.json'\n\toutput(\"Available resource names: %s\" % resources_file)\n\treturn _get_resource_names(resources_file)\n\n\ndef get_all"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\n        return get_resource_regex(os.path.join(os.path.dirname(__file__), 'krake_resources.txt'))\n\ndef validate_resource_name(_r):\n\t\"\"\"Validates that the given resource name exists.\n\t\"\"\"\n\t\n\ttry:\n\t\tfilename = _get_resource_name_regex()\n\t\n\t\treturn re.match(filename, _r.name)\n\texcept:\n\t\traise KrakeSyntaxError(\"The given resource name '%s' does not match pattern '%s'.\" %( _r.name, _get_resource_regex()))\n\n"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\n    return None;\n\n\n# ============================================================================\n# CONSTANTS:  There are three types of constants:\n# The'resource' object is the Resource class.  The default\n# implementation just returns a Resource object.\n# The base class for the Kraken class is BaseV2, but it\n# provides no functionality.  It provides no functionality for\n# building, querying, and retrieving objects.\n# ============================================================================\n\n\nclass Kraken():\n   \n    # =========================================================================\n    # Properties:\n    #  kraken  Default options on __init__:\n    #   __init__       Define"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\treturn re.compile(_get_resource_name())\n\ndef _to_utf8_or_none(data):\n    # If the input is a basestring or unicode string,\n    # treat it as a UTF8 string, otherwise\n    # just return it unchanged.\n    if isinstance(data, unicode):\n        return data\n    if isinstance(data, str):\n        return data.encode('utf-8')\n    return data\n\ndef _unescape_unicode(data):\n    try:\n        data = data.decode('utf-8')\n    except UnicodeError:\n        pass\n    data = data.replace(u'\\"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\ndef _find_resource_info():\n\t\"\"\"\n\tGet the information and return it as unicode.\n\n\tReturns:\n\t\tA tuple containing name, type, location.\n\t\"\"\"\n\t# The regular expression for the resource name should be an absolute uri.\n\t# The location contains:\n\t#   (None, type)\n\t#   (None, location)\n\t#\n\t# Note that the regex here matches \"^\\w\". A match of the path is always\n\t# returned as-is.\n\n\t# Use the glob module.\n\n\t# First, search for"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\n\n\tname_regex = None\n\t\"\"\"\n\tRegular expression matching a Krakowski resource name into\n\ta Python regular expression with one additional parameter for the\n\tinstance.\t\"\"\"\n\t# The regular expression object has to be able to convert any Krakowski to a\n\t# String if it is being used to initialize kraken with the name, or\n\t# the string with a Krakowski name\n\tif issubclass(type(resource_info.get('type')), Krakowski):\n\t\t# If the 'type' is a string then we should replace the"}, {"generated_text": "def _get_resource_name_regex():\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.\t\"\"\"\n\t\treturn None\n\ndef _get_kraken_info_filepath():\t\"\"\"\n\tBuild or return the kraken-info-filepath from the local\n\tdirectory (or any current directory if none was specified).\n\t\"\"\"\n        global current_dir\n        if current_dir is None:\n\t\tcurrent_dir = os.path.join(os.path.dirname(\n\t\t\t\tos.path.abspath(sys.argv[0])),'resources')\n\t\tglobal kraken_info_filepath\n        kraken_info_filepath = current_dir + \"/kraken-info"}]}
{"task_id": "62b86a4fb4d922cb0e688cf8", "prompt": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\tif not value:\n\t\t\tif (value == \"\"):\n\t\t\t\traise ValidationError(\"value can not be blank\")\n\t\telse:\n\t\t\tstr(value)\n\n# Check if the given value is a valid regular expression.\ndef valid_val(value):\n\t\"\"\"\n\tValidate value against the regular expression.\n\t\n\tArgs:\n\t\tvalue: the string to validate\n\t\n\tRaises:\n\t\tValidationError: if the value is not valid. \n\t\"\"\"\n\tif not value:\n\t\traise ValidationError(\"value must be valid\")\n\telse:\n\t\tstr(value)\n\n# Validate the"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\ndef value_to_list(value):\t\n\t\"\"\"\n\tTransform an input value tuple into a list.\n\t\"\"\"\n\tif isinstance(value, tuple):\n\t\tvalidate_value(value)\n\t\treturn value\n\telif value == None:\n\t\treturn None\n\telif value == True:\n\t\treturn ''\n\telif value == False:\n\t\treturn []\n\telif len(value) == 1:\n\t\treturn [value]\n\telse:\n\t\traise ValidationError('You must specify the first object of the field.')\n\ndef convert_field_to_python("}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\ndef validate_regex(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\t\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\ndef validate_password(password):\t\"\"\"\n\tValidate the given password against the corresponding regular expression.\n\nArgs:\n    password: the string to validate\n\nRaises:\n    ValidationError: if the given password is not conform to the regular expression.\t\"\"\"\n\t\ndef validate_integer(value, min_value=None):\t\"\"\"\n"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n\n\t# NOTE: This could probably be done more elegantly than a copy/paste of validate().\n\ttry:\n\t\tvalue_value = re.compile(value)\n\texcept Exception, e:\n\t\n\t\t# We are dealing with strings in an invalid format.\n\t\tif value == \"\":\n\t\t\traise ValidationError(\"Enter a value that should not be empty or blank.\",\n\t\t\t\t\t\t                       format=value_format,\n\t\t\t\t\t\t                       value=value)\n\telse:\n\t\tif not value_value.match(value):\n\t\t\traise ValidationError(\"Enter a value that should not be empty or blank.\",\n\t\t\t\t\t\t"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n    regex = _coerce_regex(value)\n\n    if not regex:\n\t\treturn _error_field.validate(value)\n\n    if regex.match('.*'):\n\t\treturn None\n\treturn value\n\n\n\ndef validate_min(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\n\nReturns:\n    the validated value (or a string that will evaluate to True or False) if successful.\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t# TODO: validate this"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\t# try to find a value\n\t\ttry:\n\t\t\tcompiled = re.compile(value)\n\t\t\t# try to parse as a string\n\t\t\tif compiled:\n\t\t\t\ttry:\n\t\t\t\t\t# parse as a list of values\n\t\t\t\t\tcompiled_value = compiled.split(\"&\")\n\t\t\t\t\t# parse as a tuple of the number values\n\t\t\t\t\tparsed_value = [int(x) for x in compiled_value]\n\t\t\t\t\t# if no matches return error.\n\t\t\t\t\tif len(parsed_value)!= 1:\n\t\t\t\t\t\traise ValidationError('Value \"' + value + '\" does not match the pattern \"' + value"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n    \ndef is_valid_regex(value):\n\t\"\"\"\n\tValidate whether the given regex validates against the pattern.\n\tThe value passed as argument must be a compiled regular expression. If the value is valid, user-inputted  can be used to validate patterns\n\tto create new patterns for the pattern.\n\n\tArguments:\n\t\tvalue: the value regex to validate (string)\n\t\tregex: the regular expression\n\t\n\tReturns:\n\t\tTrue if valid else False\n\t\"\"\"\n\ttry:\n\t\tcompile(value)\n\texcept Exception as e:\n\t\traise ValidationError(e)"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n        # check for the required pattern\n        if re.search(r'^[\\s]+$',value)!=None or re.search(r'^[\\s]+$',value)!= ''):\n                raise ValidationError(\"Value can't contain ',' or ';' characters. Please check the syntax and try again.\")\n\t\t\t\n\telse:\n\t\t# Validate the required value\n\t\tvalidate(value)\n\ndef int(value):\n\t\"\"\"\n\tConvert an input value to an integer if it is not already a valid integer.\n\tIf not valid integer, raise an exception.\n\t\"\"\"\n\ttry:\n\t\treturn int("}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\ndef validate_regex(value, pattern):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\ndef validate_list(value):\t\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\n\tValue should be a (possibly list) of valid values.\n\t\"\"\"\t\n\n\tvalid = []\n\n\tfor v in value.split(\",\"):\n\t\ttry:\n\t\t\tv = v.strip()"}, {"generated_text": "def validate_value(value):\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\t\"\"\"\n\t\n    if not value:\t\t# No value\n\t\treturn\n\t\n\tif re.match(r'\\s', value):\n\t\t#~ print \"Validating value\"\n\t\tvalidate_result = re.match(r'(.*)_(?P<value>.*)$', value)\n\t\tif not validate_result:\n\t\t\traise ValidationError(\"Invalid regular expression: '%s'\" % value)\n\t\tif validate_result.group('value')!= value:\n\t\t\t#~ print \"Validated value %s doesnt match %s (%s vs %s)\" % (value, value, validate_result.group('value"}]}
{"task_id": "62b86a4fb4d922cb0e688cf7", "prompt": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n\n\ttry: \n\t\tvalue = re.compile(key.strip())\n\texcept:\n\t\traise ValidationError('Illegal character in key \"%s\"' % key)\n\n\t# Use the re module for Python 2 and 3 compatibility\n\ttry:\n\t\tre_string = getattr(re,'search')(value)\n\texcept AttributeError:\n\t\tre = None\n\t\t\n\tif re is None:\n\t\traise ValidationError('Invalid regular expression')\n\n\treturn re_string# Authors: David Goodger\n# Contact: goodger@users.sourceforge.net\n# Revision: $Revision: 21817 $"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\ndef validate_password(password):\t#TODO: Implement\n\t\"\"\"\n\tValidate password against the corresponding regular expression.\n\tOnly one password by the password string is required.\n\t\"\"\"\n\tif len(password) < 3 or len(password) > 15:\n\t\treturn False\n\treturn re.match(r\"[\\n]*$\", password)\n\ndef check_password_complexity(password):\n\t\"\"\"\n\tCheck if the given password matches one complexity pattern.\n\tIf so, return:\n\t\tTrue if password is complex enough to validate the password\n\t\tFalse if the"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\ndef validate_type(value):\t# pragma: no cover\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression and format has invalid.\t\"\"\"\n\t\ndef is_number(value):\n\t\"\"\"\n\tCheck whether this value is a number.\n\t\n\t>>> is_number(123)\n\tTrue\n\t>>> is_number(\"test\")\n\tTrue\n\t\n\t>>> is_number(False)\n\tTrue\n\t>>> is_"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n    if not re.match('^\\w{3}(\\w)$', key):\n\t\traise ValidationError(\"Must be alphanumeric, spaces, or dashes (-), and underscores (_).\")\n\t\n\tif not re.search(('[0-9]','[^0-9a-zA-Z\\^\\@\\s]+')+ '$',key):\n\t\traise ValidationError(\"%s is not a valid key.  Valid keys are: %s\" % (key.title(),key))\n\n# -*- coding: utf-8 -*-\n#------------------------------------------------------------\n# pelisalacarta - XBMC Plugin\n# Canal para"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n# validate\n# pylint: disable=unused-variable\ndef validate(data_dict):\n\t\"\"\"\n\tValidate a dictionary of the format used by the given dictionary by \nthe given data dictionary.  \n\nArgs:\n    data_dict:  the dictionary returned by parse_item and validate_key\nReturns:\n    The validated dict.\"\"\"\n       \n# Validate a key\n# pylint: disable=unused-variable\ndef validate_item(key, data_dict):\n\t\"\"\"\n\tValidate a list of the format used by the given item by the given data dictionary.\n\nArgs:\n   "}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n\n\n    try:\n\t\tvalidate_key(re.compile(key))\n\texcept:\n\t\traise ValidationError('Invalid key')\t\n\n#################################\n# Helper functions.\t\n##################################\n\ndef normalize_str(string):\n    \"\"\"Transform string to lowercase, and remove whitespace.\"\"\"\n    return''.join(string.lower().split())\n\n\ndef to_roman(in_string):\n    \"\"\"Convert input string to roman style.\"\"\"\n    if(len(in_string)!= 1):\n        in_string = in_string.strip()\n        if len(in_string) < 1:"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\n    regex = re.compile('(?:^|\\s)([a-z0-9])([-_a-zA-Z0-9]*[a-z0-9]?)$|^([a-z0-9])=-[a-z0-9]+$|^[a-z0-9]+$|^[a-z0-9]+$')\n    \n    if not regex.match(key):\n\t\traise ValidationError('\"{0}\" does not match pattern {1}'.format(key,regex.pattern))\n\n# Copyright 2014 CloudFounders NV\n#\n# Licensed under"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\t# We need to perform the specific check here even when the check fails.\n\t\treturn\n\tif re.match(\"^[\\-\\s]\\s\", key):\n\t\traise ValidationError(\"Key can only contain alphanumeric characters, \"\n\t\t\t\"period (.), hyphens, and underscores.\")\ntry:\n\treturn validate_regex(key, REGEX_INVALID)\nexcept ValidationError:\n\traise ValidationError(\"Invalid key: \" + key)\ndef main():\n    pass\n\n\ndef read_value(file_name, num, decimals):\n    values = file_name.read().rstrip().split(\",\")"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\t# Ensure the key begins with the letter 'y', which will\n\t\t# not conflict. E.g. this is the standard Arabic alphabet, so\n\t\t# use two different checks that will actually catch the case where\n\t\t# X is in upper case.\n\t\tkey.isupper()\n\t\ndef get_all():\n\t\"\"\"\n\tReturns a list of all keys that can be used within a dictionary.\n\t\n\tReturns:\n\t a list of all keys\n\t\"\"\"\n\treturn [x for x in frappe.db.sql(\"\"\"select name from `tabDocField`\n\t\twhere fieldname"}, {"generated_text": "def validate_key(key):\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.\t\"\"\"\n\t\t# check against regex\n\t\tif not validate_regex(key):\n\t\t\traise ValidationError('Key must be valid.')\n\t# check against the database\n\t\tdb_table_exists(DB, '%s' % (key))\n\t\t# check against a valid entry in the database\n\t\ttable = DB.connect().execute(text('SELECT 1 FROM %s' % DB.tab_name))\n\t\tif table.rowcount() == 0:\n\t\t\traise ValidationError('Key does not exist')\n\t# check against a valid sql clause\n\t\ttry:\n\t\t\tsql = db_table_schema("}]}
{"task_id": "62b86a01b4d922cb0e688ccc", "prompt": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\n\n\t# The full manifest of the generated JSON.\n\tprocessed_manifest = []\t\t# Empty list of processed Kubernetes keys\n\tprocessed_root = None\t\t# Empty root node for Kubernetes\n\tprocessed_k8s = None\t\t# Empty Kubernetes json for Kubernetes keys\n\n\t\n\tmanifest_dict_copy = manifest_dict.copy()\t\t# shallow copy of manifest_dict\n\tschema_copy = _normalize_observer_json_to_v1_yaml(manifest_dict)\t# deep copy of schema_dict: no deep copy: no deep copy\n\tschema_"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\tmanifest_dict = manifest_dict.copy()\n\t\tif first_level:\n\t\t\tmanifest_dict = manifest_dict.setdefault(None, {})\n\t\tfor key, value in manifest_dict.items():\n\t\t\tif not first_level:\n\t\t\t\tmanifest_dict[key] = value\n\t\t\telse:\n\t\t\t\tmanifest_dict[key] = create_observer_definition(value, manifest_dict[key], first_level)\n\t\treturn manifest_dict\n\telse:\n\t\treturn None\n\nclass ResourceInfo(object):\n\tdef __init__(self, bundle):\n\t"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\n    manifest_keys = (list(manifest_dict.keys()), list(manifest_list.keys()))\n    first_level = manifest_keys.count(first_level)\n    for field in first_level:\n        if not field in manifest_dict:\n            manifest_dict[field] = manifest_keys[0]\n    if first_level:\n        for field in first_level:\n            if field not in manifest_keys:\n                manifest_keys.append(field)\n\ndef generate_context_list(manifest_list, field_name, key_to_value, first_level=False"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\tif manifest_dict is None:\n\t\t\tnew_dict = {}\n\t\t\tnew_dict['actor'] = _extract_bundle_name(manifest_dict['metadata']['name'],\n\t\t\t                                                     '_current_actor_name')\n\t\t\tnew_dict['target'] = _extract_bundle_name(manifest_dict['metadata']['name'],\n\t\t\t                                                     '_current_target_name')\t\t\t\n\t\t\tnew_dict['bundle_name'] = manifest_dict['metadata']['name'] + '_' + \\\n\t\t\t                                                _build_bundle_name(manifest_dict['metadata']['version'])\n\t\t\tnew"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\treturn {\n\t\t\t\t\n\t\t\t\t'MANIFEST_FILE_NAME' : manifest_dict.get('MANIFEST_FILE_NAME',''),\n\t\t\t\t'resource_field' : manifest_dict.get('SOURCE_FIELD',''),\n\t\t\t\t'resource_subcomponent' : manifest_dict.get('SOURCE_SUBCOMPONENT',''),\n\t\t\t\t'resource_parent' : manifest_dict.get('SOURCE_PARENT',''),\n\t\t\t\t'resource_value' : manifest_dict.get('SOURCE_VALUE',''),\n\t\t\t\t'observer_id' : manifest_dict.get('OBSERVER_ID',''),\n\t\t\t\t'observer_type' : manifest"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t # Create the list of the fields that are needed for subsequent output\n\t # functions, by copying the manifest to the first entry.\n\t field_values = []\n\n\t\t# Go through all the dictionaries and get the field values to be used for\n\t\tfor i in manifest_dict.keys():\n\n\t\t\tif first_level:\n\n\t\t\t\t# Loop through all the fields with first level keys as keys\n\t\t\t\tfor j in manifest_dict[i].keys():\n\t\t\t\t\t# Create a copy so that if one exists one can safely overwrite another copy\n\t\t\t\t\tmanifest_dict[i][j] = template.copy_kv_"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\n\n\ttry:\n\t\treturn manifest_dict.__class__.observer_schema\n\texcept AttributeError:\n\t\tpass\t\t\t\t\t\t# not a K8sResource\n\t\n\tif not first_level:\n\t\tk8s_observer_schema = generate_full_service_schema(\n\t\t\t\t\t\t\t\t\t\t\t 'namespace/schemas',\n\t\t\t\t\t\t\t\t\t\t\t manifest_dict,\n\t\t\t\t\t\t\t\t\t\t\t [])\n\telse:\n\t\tk8s_observer_schema = dict((k, generate_full_service_schema(v))\n\t\t\t\t\t\t\t\tfor k, v in manifest_dict.items())\n\n\t# use a list to keep our current state of"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\treturn get_value_or_none('observer_schema.copy', manifest_dict,\n\t\tkey_name='resource')\n\ndef generate_default_observer_schema_dict_with_field_schema(manifest_dict,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfirst_level=False):\n\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\ndictionary from part of a Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\n    # The default observer schema\n    observer_schema = generate_first_level_observer_schema(manifest_dict)\n        \n    # The second level observer schema\n    observer_schema = annotations.create_minimal_observer_schema(first_level)\n\t\n\t# The final schema\n    final_schema = {}\n    \t\n    # The final value of the 'final' observer schema \n    final_schema.update(observer_schema)\n    \n    # Add the final observer schema \n    final_schema['observer_schema'] = final_schema.get('observer_schema', {})\n\t\n    return(final_schema)\n\n##\n#"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.\t\"\"\"\n\t\ndef generate_list_of_default_observer_schema_dicts(manifest_dict, first_level=False):\t\"\"\"\n\tTogether with :func:``generate_default_observer_schema_dict``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n"}]}
{"task_id": "62b869ebb4d922cb0e688cc6", "prompt": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t)\n\tfiltered_last_applied_manifests = []\n\tfor obj in response:\n\t\tif not obj.get('labels'):\n\t\t\tcontinue\n\t\tfiltered_last_applied_manifests.append(obj.get('labels'))\n\tfiltered_last_applied_manifest_dict = dict()\n\tfiltered_last_applied_manifest_dict['last_applied_manifest'] = last_applied_manifest\n\tfor label in last_applied_manifest_dict['labels']:\n\t\tfiltered_last_applied_manifest_dict['last_applied_manifest'][label['metadata']['"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t):\n\t    for name in observer_schema:\n\n                if field_check(name) and hasattr(obj, name):\n                    setattr(obj, name, getattr(obj, name))\n\n\tif not last_applied_manifest is None:\n    \n                update_last_applied_manifest_dict_from_resp( \\\n\t\t\t\t\t\t\"[%s]: updated last applied manifest\" % observer_table,\n                                                last_applied_manifest, observer_schema )\n\n                del(last_applied_manifest)\n\n                \n\n        return\n\n\nclass KubernetesEvent(object):\n    \"\"\"\n    Container class for Kubernetes event handling. It is passed"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t))        \n\tfor observed, value in zip(observer_schema, response):\n\t\tif value!= latest_applied_manifest.get(observed):\n\t\t\tlatest_applied_manifest[observed]  = value\n\t\t\tlatest_applied_manifest[observer_schema[observed]] = value\n\treturn latest_applied_manifest\n\n\ndef update_last_applied_namespace_list_from_resp(\t_from, resp\n\t):\n\t\"\"\"\n\tLike :func:`update_last_applied_namespace_dict_from_resp`,\n\n\t:param dict _from: partial ``"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t,\n\tbefore_key=True\n):\n\tmanifest_obj = Manifest()\n\tfor audit_entry in last_applied_manifest:\n\t\taudit_entry['time'] = audit_entry['timestamp']\n\t\taudit_entry['audit_id'] = audit_entry['audit_id']\n\t\taudit_entry['uuid'] = audit_entry['uuid']\n\n\t\tif audit_entry['type'] == 'kube_apiserver':\n\t\t\tmanifest_obj.add_audit_entry('created-by', audit_entry['created-by'])\n\n\t\tif audit_entry"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t):\n\t\tfor field in response.keys():\n\t\t\tif not '_updated' in response[field]:\n\t\t\t\tcontinue\n\t\t\tif field == '_etag':\n\t\t\t\t# the value is already present so this is an error\n\t\t\t\tcontinue\n\n\t\t\t# check if value exists for this key, if yes append it\n\t\t\t# to the key if it doesn't exist it will be updated already\n\t\t\t# if it does, only update it if it does not\n\t\t\tif hasattr(response[field], key):\n\t\t\t\tresponse[field].update(getattr(response[field], key))\n\t\t\t\tcontinue\n\n\t\t\t#"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\n        # Note: the 'last_applied_manifest' and 'observer_schema' values from this\n        # function were also initialized at the point that this function was\n        # called; however, that it can be easily replaced via an external service to\n        # pass around to the user by the API\n\tif not 'last_applied_manifest' in prev_resp and 'observer_schema' in prev_resp:\n                prev_resp['last_applied_manifest'] = [prev_resp['this_field']]\n                prev_resp['observer_schema'] = [prev_resp['this_type']]\n\n       "}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t)\n\n\tfor observed_field in observer_schema:\n\t\tfield_val = received_field.get(observed_field['id'])\n\t\tif not field_val:\n\t\t\tcontinue\n\t\tif observed_field == 'next_collection_ids':\n\t\t\tif len(field_val) < 1:\n\t\t\t\tcontinue\n\t\t\terror = \"next_collection_ids field value of %s should be \" % \\\n\t\t\t\t     received_field.get('id')\n\t\t\terror += \"an array of ids in 'next_collection_ids' value, but \"\n\t\t\terror += \"it"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t).strip()\n\n\n_previous_version = None, '0.0.24'\n\n_previous_release = '0.0.24'\n\n_release = '0.0.24-rc1'\n\n_is_release_update = False\n\n_has_previous_changeset = False\n\n_has_release_vcs_metadata_cache = False\n\n_release_vcs_metadata_cache_hit = False\n\n_newest_release = None\n\n_newest = None\n\n_newest_tag_key__ = 'previous_version"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t)\n\tfrom operator import attrgetter\n\tfrom itertools import chain\n\tfrom time import time\n\tfrom urllib import urlencode, quote\n\timport json\n\n\tdef update_last_applied_manifest_dict_from_dict(obj):\n\t\t\"\"\"\n\t\tFrom ``update_last_applied_manifest_dict_from_dict_item``\n\t\treturn a list of dictionaries of the form\n\n\t\t  ``[\n\t\t  {'last-applied-manifest-etag': 'etag',\n\t\t   'last-applied-manifest-selector':'selector',\n\t\t   'last-applied-manifest-status':'status"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\tfor each_bundle in self.__applied_bundles:\t\t\n\t\t\tfor key, value in map(\n\t\t\t\t\tlambda bundle: bundle.get(key),\t\\\n\t\t\t\t\t\tlast_applied_manifest[each_bundle].items(), \n\t\t\t\t\t\t\t\t\teach_bundle.values().items()):  \n\t\t\t\tsetattr(self, key, value)\n\t\tself.last_applied_manifest = last_applied_manifest\t\n\t\t\n\tdef __str__(self):\n\t\treturn str(self.__dict__)\t\n\ndef load_bundle_by_name(service_name, bundle_name):\n\treturn"}]}
{"task_id": "62b869eab4d922cb0e688cc5", "prompt": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t),\n\n    \"type\": \"object\",\n    \"properties\": {\n        \"type\": {\"type\": \"string\"},\n        \"context\": {\"type\": \"str\"},\n        \"resource\": {\"type\": \"function\", \"default\": \"kubernetes.extensions.ManagedClusterAPIService\"})\n  },\n    \"required\": [\"type\", \"context\", \"resource\"],\n    \"anyOf\": [\n      {\"required\": [\"type\", \"context\"]},\n    ]\n}\n\n\ndef create_context_payload_dict(resp):\n\n    content = {}\n    for attr in dir(resp):\n        if not attr.startswith(\"_\") or"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\n    \tfor observer_field in list(observer_schema.keys()):\n    \t    try:\n\t\tkey = observed_field_key(observer_field, last_applied_manifest)\n\t\tlast_applied_manifest.update({key: observer_schema.get(key)})\n\t\tlast_applied_manifest.get(observer_field).update(\n\t\t\tprevious=observer_schema.get(observer_field).get('previous', [])))\n    return last_applied_manifest\n\ndef main():\n    \"\"\"\n    Parse args and run the application. This will run as specified for the\n    react."}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t),\n\n)\n\n\"\"\"\nMap between Kubernetes service type and corresponding value\n\n\"\"\"\nKIND_REPAIR_SERVICE_TYPE = \"__kindreduce__service_type_to_repair\"\n\"\"\"\n``int`` identifying the type of repairing service\n\"\"\"\n\nASSOCIATE_SERVICE_TYPE = \"__kindreduce__service_type_to_association\"\n\"\"\"\n``str`` identifying the type of reidentifying service\n\"\"\"\n\nDELETE_SERVICE_TYPE = \"__kindreduce__service_type_to_delete\"\n\"\"\"\n``str``"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t.replace('$$', '$')\n\n\tdef _apply_to(field_name):\n\t\tvalue = getattr(response, field_name)\n\t\tnew_value = value.copy()\n\t\tnew_value[field_name] = new_value[field_name] + extra_fields\n\t\t\n\t\treturn new_value\n\n\tupdated_field = fields_dict.get(last_applied_manifest, None)\n\tif updated_field is None:\n\t\ttry:\n\t\t\tresp = update_last_applied_manifest_dict_from_resp(\n\t\t\t\tresponse)\n\n\t\texcept"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t)\n\n\tif not response['last_applied_manifest']:\n\t\traise KeyError('Got a partial response without any fields')\n\tif observer_schema_key not in response['observer_schema']:\n\t\traise KeyError('Got a partial response without a field')\n\n\t# Update the last applied manifest...\n\tresponse['last_applied_manifest'].update(observer_schema)\n\tresponse['observer_schema'][observer_schema_key] = observer_schema\n\n\t# Now update the response...\n\tresponse['response'].update(response['last_applied_manifest'])\n\treturn response\n"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\tyield\n\tfirst_timestamp=''\n\tfirst_seen=False\n\tfor k in observer_schema:\n\t\tif k in resp:\n\t\t\tcontinue\n\t\tif not(first_seen or k in last_applied_manifest):\n\t\t\tfirst_seen = True\n\t\t\tyield {}\n\n\t\treq = resp.get(k)\n\t\tif isinstance(req, dict) and 'last_applied_manifest' in req:\n\t\t\tlast_applied_manifest[req['last_applied_manifest']] = req['observer_object']\n\t\t\tcontinue\n\t\tbreak\n\tif not(first_"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t):\n\t\twith open('./.last_applied_manifest', 'w') as stream:\n\t\t\tyaml.safe_dump(last_applied_manifest, stream)\n\ndef update_last_applied_manifest(resp):\n\twith open('./.last_applied_manifest', 'r+') as stream:\n\t\tparsed_manifest = yaml.safe_load(stream)\n\n\tif'meta' in parsed_manifest.keys():\n\t\telem = {elem['metadata']['name']: elem for elem in parsed_manifest['meta']}\n\t\tprevious_resp = get_last_applied_manifest"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t)\n\tfor observed_field, observed_value in last_applied_manifest.items():\n\t\tif observed_field not in observed_value:\n\t\t\tcurrent_value = observed_value.get(observed_field)\n\t\t\tresult = current_value.copy()\n\t\t\tobserved_value[observed_field] = result\n\t\t\tobserver_schema[observed_field] = result.get('observer_schema')\n\n\t\t\tbundle = self.get_bundle(observer_schema, observed_field)\n\n\t\t\tif bundle:\n\t\t\t\tobserved_value[observed_field]['bundle'] = bundle\n\t\t"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t)\n\ndef update_last_applied_manifest_list_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\t\"\"\"\n\t\t)\n    \tfor field in observer_schema:\n\t\tupdate_last_applied_manifest_dict_from_resp(field, response[field])\n@given('last_applied_manifest is a dict')\ndef update_last_applied_manifest_dict_from_resp(last_applied_manifest):\n\tif not isinstance(last_applied_manifest, dict):\n\t\traise TypeError('last_applied_manifest is not a dict')\n    \tif last_applied_manifest.get('last_applied'):\n\t\tif not isinstance(last_applied_manifest['last_applied'],"}]}
{"task_id": "62b869eab4d922cb0e688cbf", "prompt": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n    # Add the kubernetes.services namespace to the namespace of the spec.yaml\n    app.add_namespace(kubernetes_services.NAMESPACE_NAME)\n    service_name = '{}-custom-observer'.format(os.getenv('SERVICE_NAME'))\n    app.add_namespace(service_name)\n\n    # Generate a default observer configuration for each Kubernetes resource\n    # present in the spec.yaml.\n    spec_file = '{}/service-broker.yaml'.format(os.getenv('CLUSTER_RESOURCES'))\n    if not os.path.isfile(spec_file):\n\tsys.exit('Error"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n\t\n\tconfig = app.config\n\tif config is None:\n\t\tlogger.debug(\"App configuration is None.\")\n\telse:\n\t\tlogger.debug(\"App configuration is {0}\".format(config.as_map()) )\n\t\t\n\tnamespace_name = config.get_namespace_name()\n\t\n\treturn {\n\t\t'default_version': app.config.get_version_tag(), \n\t\t'default_namespace': namespace_name, \n\t\t'default_type': 'kubernetes_observer'\t\t    \n\t}\n\ndef generate_additional_namespace(app):\n"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n    default_observer_schema = {\n        \"v1beta4.0\": \"v1beta4.0.json\",\n        \"v1alpha3.0\": \"v1alpha3.0.json\"\n    }\n    default_observer_schema[\"v1beta3\"] = \"v1beta3.json\"\n    default_observer_schema[\"v1alpha2\"] = \"v1alpha2.json\"\n    default_observer_schema[\"v1\", \"v1beta4.0\", \"v1beta3.0\", \"v1alpha2.json\", \"v1\"],"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n    # get app's current bundle\n    bundle = _load_bundle_from_json(app.config['spec'])\n\n    # create default observer schema with custom observer type as template\n    default_observer_schema_template = app.jinja_env.get_template('kraken/observer_schema_template.json')\n    default_observer_schema = create_context_schema(default_observer_schema_template, bundle.get_name(), None, None, bundle.name)\n\n    # get service specific fields for the controller\n    controller_attrs = []\n    for field in _CONTROLLER_SPECI"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n    default_observer_schema.set_schema({\n        \"apiVersion\": \"v1\",\n        \"kind\": \"Trakt\",\n        \"metadata\": {},\n    })\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(__doc__, description='Generates default observer schema for each kubernetes resource present in a JSON file')\n    parser.add_argument('-s', '--schema', dest='filename', type=str, default='observer.schema.json')\n    parser.add_argument('--app', type=str, dest='app', default=os.environ.get('KUBECONFIG_APP"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n    class Schema(object):\n        exposed_ports = set(['80'])\n        exposed_ports.replace('80', '443', '80')\n        exposed_ports.replace('443', '80')\n        exposed_ports.replace('80', '80443', '443')\n        exposed_ports.replace('80', '8080', '80')\n        exposed_ports.replace('80', '443', '443')\n        exposed_ports.replace('443', '80443', '80443')\n        exposed_ports.replace('80443-443', '8083', '80')\n        exposed_ports"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n        api = k8s.KubernetesModelHelper()\n\tapp.model.get_object('meta').authorization\n\tapi.add_resource(\n\t\t\t'/v1alpha1/namespaces/{namespace}/triggers',\n\t\t\t'/v1alpha1/namespaces/{namespace}/triggers/{trigger_id}',\n\t\t\trule_name = 'trigger',\n\t\t\tnamespace = 'triggers',\n\t\t\tdescription = 'A generic event data source. Required.',\n\t\t\trequired = k8s.CP_REQUIRED_PROPS_SET,\n\t\t\tread_only = True\n\t\t)\n\ndef generate_event_"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n    with app.app_context():\t\n        \n        # generate a default observer schema\n        app_yaml = generate_schema(app)\n        observer_schema = generate_observer_schema(app_yaml)\n\n        # append observer schema to yaml\n        for observer_module_name in app.resource_registry.keys():\n\n            # skip if a specific observer_module contains a node\n            if observer_module_name == '*':\n                continue\n\n            # skip if the key 'optional' exists\n            if app.resource_registry[observer_module_name].get('optional'):\n                continue\n\n            # skip if a version of the observer is"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\treturn \"\"\"default_observer\n\n\n@api.model\ndef get (obj):\n\t\"\"\"Get the pod spec information as a namedtuple\n\t:param obj (object): The object that should be serialized and sent to the\n\t\tobserver's `get` function of the Kubernetes API\"\"\"\n\n\t# TODO(david): this could also be refactored to work from the UI, in fact\n\t# we are doing this to be over-written here. Should be fixed at some point..\n\n\tpod = obj.__class__.from_json({\n        \t'title': pods.name,\n        \t"}, {"generated_text": "def generate_default_observer_schema(app):\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema\t\"\"\"\n\t\n    with app.app_context():\n        schema = {'type': 'object','required': ['type', 'name', 'cluster']}\n\n        field_values = app.config['FIELDS']\n        for field in field_values:\n            schema[field] = field_values[field]\n\n        class Meta:\n                resource_name = 'kubernetes'\n                interfaces = ['public_kube_controller', 'external_kube_controller']\n\n        schema.update(schema.items())\n        \n        # Only add fields for 'type' if 'cluster' was specified in the configuration\n        if 'type' not in app.config:\n           "}]}
{"task_id": "62b43427903eeb48555d3ea5", "prompt": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\n\n\tfrom.. import constants\n\n\t_query = \"SQL( %s )\" % constants.DB_MAGIC\n\t_params = tuple(_params)\n\t_out_args = tuple(_out_args)\n\n\tout_params = {}\n\tout_params[\"in_style\"] = bool(_query)\n\tout_params[\"var_name_in_query\"] = str(_in_style)\n\n\tout_params[\"param_name_out_query\"] = str(_out_args)\n\n\tout_params[\"parameters\"] = (constants.SQL_PARAMS +\n\t\t\t\t\t\t\t\t\t\t\t"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\treturn {\n\t\t\t\"fmt\": format_query( self ),\n\t\t\t\"params\": self\n\t\t}\n\n\t@classmethod\n\tdef get_named_value( cls, q_params, query_params ) :\n\n\t\tif q_params is None :\n\t\t\treturn None\n\n\t\telif isinstance( q_params, AttributeValue ) :\n\t\t\treturn q_params\n\n\t\telse :\n\n\t\t\t# Check all known (non-keyword) named parameter styles:\n\t\t\tfor k in q_params :\n\t\t\t\tif k not in query_params and q_params[k] is None :\n\t\t\t\t\t"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\treturn [f if isinstance(f, Mapping) else f[o] for f, o in SQLParams.items() if self.in_style(f, o)]\n\n\tdef sql(self):\n\t\t\"\"\"Convert the SQL query to use the out-style parameters instead of SQL.\"\"\"\n\n\t\treturn [ str(sqlparam(o, self.params)) for paramo, o in self.params.items() if self.out_style is None or paramo in SQLParams.values(o) ]\n\n\tdef in_style(self, f, o):\n\t\treturn f and o"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\treturn self._format(\n\t\t\tsql or u\",\".join(\n\t\t\t\tmap(\n\t\t\t\t\tlambda fmt: fmt[fmt[0]] if fmt and fmt[1] is not _int_type_map else fmt\n\t\t\t\t\ttuple(\n\t\t\t\t\t\tfmt\n\t\t\t\t\t\tfor fmt in\n\t\t\t\t\t\tfmt.values()\n\t\t\t\t\t\t)\n\t\t\t\t\t),\n\t\t\t\t\tparams\n\t\t\t)\n\t\t)\n\n\t@classmethod\n\tdef _format_params(cls, values_tuple):\n\t\t# Support in-style parameters that are prefixed with an\n\t\t# underscore (e.g. `?`). E.g. `\""}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\t)\n\tif isinstance(sql, bytes):\n\t\tsql = bytes(str(sql, 'utf-8'))\n\telif isinstance(sql, Sequence):\n\t\tsql = ''.join(sql)\n\n\treturn repr_fmt % sql, params\n\ndef create(sql, params):\n\t\"\"\"\n\tCreate a string SQL query.\n\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t*params* (:class:`collections.abc.Mapping` or :class:`~collections.abc.Sequence`) contains the set of\n\tin-style"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\treturn format_sql(self)\n\n\tdef _create_table(self, table, fields):\n\t\t\"\"\"Create a SQL query from a given table\n\n\t\tParameters\n\t\t----------\n\t\ttable: :class:`~collections.abc.Mapping`\n\t\t        The :class:`~collections.abc.Mapping` that this will be a\n\t\t        :class:`~collections.abc.Sequence`.\n\t\tfields : list of fields\n\t\t        The fields that will be used when creating the\n\t\t        :class:`~sqlalchemy.schema.Column`.\n\t\t\"\"\"\n\t\tfield_names = ', '.join(f.name for f in"}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\treturn tuple(map(lambda d: map(lambda a: a[0](a[1], d), params, params._fields)),\n\t\t\tparams.as_mapping, params.as_sequence)\n\ndef to_template(*, fmt):\n\tif fmt is None:\n\t\traise ValueError(\"can't determine format!\")\n\n\tdef template_field(s):\n\t\treturn fmt % tuple(map(template_field, filter(None, s)))\n\n\targs = []\n\tnamed = True\n\tvalues = []\n\tfor param in params._fields:\n\t\tif param."}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t\n        if params is None:\n\t\t\n\t    return tuple(params) if type(params) == collections.abc.Mapping else []\n\n\t\n\t\n\t    if type(params) == tuple:\n\t\t# this is a named tuple\n\t\t\n\t    return dict((param, type(params._out)), _parseSQL(p) + type(params.keys()[0]))\n\t\t\n\t    \n\t    if type(params) == set:\n                return dict((param, type(params[param])) for param in params)\n\t\t\n\t    \n\t    if type(params) == collections.abc.Sequence:\n                # this is just one sequence\n\t\t\t\n            return dict(("}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t \n\tif isinstance(params, dict):\n\t\tout_style = params.pop(\"out_style\", None)\n\t\tif out_style is None:\n\t\t\tout_style = \"qmark\"\n\t\tif sql_params is None:\n\t\t\tsql_params = SQLParams()\n\t\tsql_params.out_style = out_style\n\t\tsql_params.sql = sql\n\t\tsql_params.params = params\n\telif isinstance(params, Sequence):\n\t\tsql_params = SQLParams(params)\n\t\tsql_params.sql = sql\n\t\tsql_params."}, {"generated_text": "def format(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).\t\"\"\"\n\t # TODO validate tuple or list?\n\t # \tparams = {k: v for k, v in param.items() if not isinstance(v, (collections.Mapping, str))}\n\t # \tif not isinstance(params, dict):\n\t # \t\traise TypeError(\"Not a dict; not a %s\" % (str(param)))\n\t # \tparams = {k: v for k, v in params.items() if not isinstance(v, (collections.Mapping, str))}\n\t # \tout_style = params.get('out_style', {})\n\t # \t"}]}
{"task_id": "62b43428903eeb48555d3eaa", "prompt": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t    \n\t\tif isinstance(params, Mapping):\n\t\t\tnew_vals = []\n\t\t\tfor param in params:\n\t\t\t\tif isinstance(param, SQLParams):\n\t\t\t\t\tnew_val = param.sql()\n\t\t\t\telse:\n\t\t\t\t\tnew_val = param\n\t\t\t\tnew_val = SQLParam(new_val, new_type=param.type, **param.sql_parameters)\n\t\t\t\tnew_vals.append(new_val)\n\t\t\treturn tuple(new_vals)\n\t\telif isinstance(params, Sequence):\n\t\t\tout_params = {}\n\t\t\tfor param in params:\n\t\t\t"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\traise NotImplementedError()\n\trows = tuple(_fmtparams(param) for param in many_params)\n\tsql = 'insert into %(tablename)s (%(columns)s) VALUES (%(sqla)s)' % {'\n\t\t'columns': ', '.join(p) for p in many_params,\n\t\t'sqla': ', '.join('?' * len(p))}\n\n\t_logger.debug(sql)\n\t\n\t# compile & run\n\treturn compile(sqla, sql, params, many=False)\n\n\ndef _fmtparams(param):\n\t# TODO: this"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\titems = []\n\t\tfor params, format_ in six.iteritems(self.SQLParamTypes):\n\t\t\tfor k, v in params.items():\n\t\t\t\tif isinstance(v, (tuple, list)):\n\t\t\t\t\tvalues = \"\"\n\t\t\t\t\tfor idx, row in enumerate(v):\n\t\t\t\t\t\tvalues += f\"{idx*len(params)+1}-{format_}\"\n\t\t\t\t\t\tvalues += \",\"\n\t\t\t\t\t\tvalues += f\"{format_}{row}\"\n\t\t\t\t\titems.append(values[:-1])\n\t\t\t\telse:\n\t\t\t\t\titems.append(v)\n\n\t\tif len(items) == 0:\n\t\t\t"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\ttry:\n\t\t\tresults = map(lambda x: self.setof(x), results)\n\t\texcept IndexError:\n\t\t\traise psycopg2.DataError('You must specify at least one out-style parameter.')\n\n\t\treturn tuple(results)\n\n\t@abc.abstractmethod\n\tdef setof(self, arg):\n\t\t\"\"\"Change what the parameter should be set for this query. The value of this method is the new parameter.\n\n*value_cast* (:class:`bytes`, `dict`, `str`, `bool`, `None`)\n\t\tIf the type of the argument is ``str"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\n        parts = []\n\t\n        for node in self.query_tree(\n\t\t\tselect_fields = (\n\t\t\t\t\tself.params.fields\n\t\t\t\t\tif self.has_params() else\n\t\t\t\t\tlist(),\n\t\t\t\t\tself.params.table.name,\n\t\t\t\t\tself.params.fields_terminated\n\t\t\t\t)\n\t\t):\n\t\t\tif params and isinstance(params[0], six.basestring):\n\t\t\t\tparts.append(params[0])\n\t\t\t\tcontinue\n\n\t\t\tif not isinstance(params[0], Iterable) and not isinstance(params[0], basestring) and params[0]:\n\t\t\t\tparts.append"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\tif not isinstance(query, Mapping): raise RuntimeError(\"params must be a mapping type.\")\n\t\tif not isinstance(values, Sequence): values = Values(values)\n\t\tif not all([isinstance(x, _str_type) for x in values]): raise RuntimeError(\"\"\"params elements must be strings.\"\"\")\n\t\tif len(values)!= len(values): raise RuntimeError(\"\"\"params elements length must be {0}, must be non-empty.\"\"\".format(len(values)))\n\t\t\n\t\treturn tuple(self.query(query.copy(), query.copy(), query.copy(), query.copy(), params=list"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t \n\tsql, many_params = _parse_sql(sql)\n\ttry:\n\t\tif many_params is not None:\n\t\t\treturn _convert_many_params(many_params)\n\t\telse:\n\t\t\treturn _convert_in_style(sql, many_params)\n\texcept _errors.WrongType:\n\t\traise _errors.ProgrammingError(\"expected a string and/or integer parameter style or a sequence of such\")\t\n\ndef _convert_many_params(many_params):  # pragma: no cover\n\t\"\"\"convert many_params to a tuple of"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\tyield [\n\t\t\tself._formatmanyparam(k, v) for (k, v) in params.items()\n\t\t]\n\n\t@util.log\n\tdef _formatmanyparam(self, k, v):\n\t\tv = (v if isinstance(v, tuple) else (str(v),))\n\t\treturn \",\".join(x for x in v if x)\n\n\nclass SQL(str):\n\t\"\"\"String wrapper designed for MySQL and MariaDB.\n\n\tTODO: Remove in 0.7\n\t\"\"\"\n\t# TODO: make use of the ANSI C X3 ANSI"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\treturn formatmany_result(map_parameters(\n\t\t\titertools.chain(\n\t\t\t\tsqlparammap(param, params)\n\t\t\t\tfor param, params in zip(\n\t\t\t\t\tlist(zip(*itertools.chain([(param, converter) for converter in params]))),\n\t\t\t\t\titertools.chain.from_iterable(param_converters)\n\t\t\t\t)\n\t\t\t)\n\t\t))\n\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You"}, {"generated_text": "def formatmany(\t\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\t\"\"\"\n\t\traise NotImplementedError\n\treturn _sql.formatmany(\t_many_params(\t**_get_multi_params(p)) for p in _params),\n\t                                _params)\n\n\n# TODO: replace this with the 'tuple' module, this is only\n# being used directly by the DBAPI\ndef _sql_param_as_param(*p, **q):\n\t\"\"\"\n\tGenerate SQL query as a ``tuple``::\n\n\t    ((p.myparam, p.mydefaultvalue),)\n\t\"\"\"\n\tif len(_param_as_param(*p, **q))"}]}
{"task_id": "62b45df05108cfac7f2109ce", "prompt": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\treturn not path.endswith('.cfout') and path.endswith('.cfgs')\n\ndef get_data(node, **kwargs):\n\t\"\"\"\n\tGenerate data, from OCFL (if present) or PyFDS and return dict of data.\n\n\tParams:\n\t\tnode (Node): Node with parent\n\t\tkwargs (dict): Node attributes as keyword arguments\n\tReturns (Dict, bool, str): Parsed data\n\t\"\"\"\n\tif not hasattr(node, 'node_data'):\n\t\treturn None  # Check is node already\n\tdata = {'name': node."}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\tpath = abspath(normpath(path))\n\t\tif not path.startswith(self.root):\n\t\t\treturn False\n\t\treturn True\n\n\tdef __getitem__(self, key):\n\t\treturn self.child_by_key(key)\n\n\tdef iter_children(self):\n\t\tfor child in self.children:\n\t\t\tyield child\n\n\t\tfor child in self.children:\n\t\t\tfor p in child.iter_children():\n\t\t\t\tyield p\n\t\t\tyield child.name\n\n\tdef child_by_name(self, name):\n\t\tfor o in self.child_by_"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\treturn self.parse(path) is not None\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Licensed to Cloudera, Inc. under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  Cloudera, Inc. licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\t# If path contains absolute path, must not have leading '/'\n\t\tif len(path) > 1 and path.find('/') == -1:\n\t\t\tret = False\n\t\telif len(path) > 1 and path[1:].strip()!= '.' or path.find(':') == -1:\n\t\t\tret = False\n\t\telse:\n\t\t\tif not self.__validate(path):\n\t\t\t\tret = False\n\t\treturn ret\n\n\t# Validate existence of an OCFL file with OCFL version number suffix\n\tdef __validate(self, path):\n\t\ttry:\n\t\t\tverF ="}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\tif not isinstance(path, basestring):\n\t\t\traise TypeError(\"Path must be a string.\")\n\t\tif not pyfs.is_pyfs_root(path):\n\t\t\treturn False\n\t\tif not os.access(path, os.R_OK):\n\t\t\treturn False\n\t\treturn True\n\n\tdef get_file(self, path, relpath):\n\t\t\"\"\"\n\t\tTry to return a valid file.\n\t\tChecks the file size to see if it already exists.\n\t\tNote: This function depends on extension.\n\t\t\"\"\"\n\t\troot = os.path.realpath(os.path"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\n#\tcheck if root exists\n\tif not ocfl.ROOT_PATH:\n\t\tocfl.ROOT_PATH = ocfl.HOME_DIR\n\tif not ocfl.ROOT_ID:\n\t\tocfl.ROOT_ID = ocfl.DATA_ROOT\n\tif not ocfl.ROOT_FILE:\n\t\tocfl.ROOT_FILE = ocfl.ROOT_DIR + ocfl.ROOT_FILE\n\tif not ocfl.ROOT_NAME:\n\t\tocfl.ROOT_NAME = ocfl."}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\treturn check_OCTFLVersioned(path)!= ''\n\nclass OCFLValidator:\n    def __init__(self, path):\t\t\t# Path to OCVF binary\n        self.path = path\t\t\t# Path to OCVF binary\n        self.o = self.open_file(self.path)\t\t\t# Open OCFL file\n        self.header = self.get_header_block(self.o)\n\t\tself.info = validate_header_info(self.o)\n\n\t\tself.h = self.header['header_size']\n\n\t\tself"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\t#\n\t\timport pyfl\n\t\tif isinstance(path, basestring):\n\t\t\tpaths = path.split(os.path.sep)\n\t\t\tfor p in paths:\n\t\t\t\tif not os.path.isdir(p):\n\t\t\t\t\tvalid = self.is_valid_pyfs(p, p.endswith(\".py\"))\n\t\t\t\t\t#\n\t\t\t\t\tif not valid: \n\t\t\t\t\t\tmsg = \"ERROR, could not find file '%s': %s\" % (p, repr(sys.exc_info()[1]))\n\t\t\t\t\t\tself._errors.append(msg)\n\t\telse:\n\t\t\tfor"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\tok = True\n\t\twarnings = 0\n\t\terrors = \"\"\n\t\tif not path:\n\t\t\twarnings += 1\n\t\tif self._root!= None:\n\t\t\tpath_valid = True\n\t\t\tpath_valid_1 = True\n\t\t\tif path not in (\"/\",\"/tmp/\"):\n\t\t\t\terrors += 1\n\t\t\telif not os.path.exists(path):\n\t\t\t\twarnings += 1\n\t\t\tif not path_valid:\n\t\t\t\terrors += 1\n\t\t\t\n\t\t\tif not path_valid == False:\n\t\t\t\terrors_count = len(errors)\n\t\t\t\tfor error_count in range(errors"}, {"generated_text": "def validate(self, path):\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\t\"\"\"\n\t\twarnings = []\n\n\t\tif not self.isValid():\n\t\t\twarnings.append(\"ocfl: object is invalid; please run 'pyfuse' with option --validate\")\n\t\t\treturn False\n\n\t\tif not os.path.exists(path):\n\t\t\twarnings.append(\"ocfl: path does not exist: %s\" % path)\n\t\t\treturn False\n\t\t\t\n\t\tif not os.path.isdir(path):\n\t\t\twarnings.append(\"ocfl: path is not a directory. (pyfuse does not support this directory.)\")\n\t\t\treturn False\n\t\tif not os.access("}]}
{"task_id": "62b45df15108cfac7f2109dc", "prompt": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\treturn prefix+'('+self.options['status']+')'\n\nclass EmPOWER(EmAPIObject):\n\t\"\"\"\n\tEmPOWER command class.\n\t\"\"\"\n\t\n\tdef __init__(self, host, port, username, password, api_mode = 'authentification', domain = None):\n\t\"\"\"\n\tInitialize EmPOWER object.\n\tIn case of authentification command errors, a token is required to\n\tlogin to eMME (eg. EmPOWER)\n\t\"\"\"\n\tself.api_mode = api_mode\n\tif domain"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\tstatus = '%r %s %s' % (prefix, self.name, self.type)\n\t\tif not self.validators:\n\t\t\tstatus +='or validators'\n\t\treturn status if self.default is not None and callable(self.default) else status[:-3]\n\nclass Email(Validator):\n\t\"\"\"\n\tContains a validator for sending emails about new status of a record.\n\tEmails are sent to a specified recipients.  Each validator has an \"old_email\"\n\tattribute that stores the old email address, e.g. \"test@example.com\""}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\tif prefix:\n\t\t\treturn prefix + self.__status\n\t\telse:\n\t\t\treturn self.__status\n\tstatus = property(status_str)\n\tdef __call__(self):\n\t\t\"\"\"\tEvaluate status as a python expression \"\"\"\n\t\treturn str(self)\n\tdef __str__(self):\n\t\t\"\"\"The stringified version of self. \"\"\"\n\t\tstatus = self.status()\n\t\treturn \"Validator({})\".format(status)\n\nclass Validator(validator):\n\t\"\"\"\n\tValidator class.\n\t\"\"\"\n\tdef __init__(self, title, validator_title"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\treturn prefix + status_str(self.status)\n\n\"\"\"\n\tReturn status class (callable, not an instance) used as validator status.\n\t@ivar status: callable object, which returns a string describing the status.\n\t\"\"\"\nclass _Status(object):\t\t\n\tSTATUS_WAIT = 1\n\tSTATUS_RUNNING = 2\n\tSTATUS_ABNORMAL_TIMEOUT = 3\n\tSTATUS_CONTINUE = 4\n\tSTATUS_TERMINATING = 8\n\tSTATUS_CLOSED = 16\n\tSTATUS_EOF = 32\n\tSTATUS_READABLE = 64\n\t# Status constants returned"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\tresult = \"Status: {0}.\\n\".format(prefix + self.status)\n\t\tif self.debug:\n\t\t\tret_type = self.get_ret_type()\n\t\t\tif ret_type:\n\t\t\t\tresult += \"Return type: {0}.\\n\".format(ret_type)\n\t\tif self.errors:\n\t\t\tret_type = self.get_ret_type()\n\t\t\tif ret_type:\n\t\t\t\tresult += \"Error: {0}, {1}\\n\".format(prefix, ret_type)\n\t\treturn result\n\n\t@method\n"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\tstatus = '%s %s' % (prefix, self.validator.status.lower())\n\t\treturn '%s %s' %(prefix, self.validator.reason.lower())\n\n\tdef test(self):\n\t\t\"\"\"\n\t\tCheck status, print to screen the output\n\t\t\"\"\"\n\t\tif self.isRunning():\n\t\t\tprint '>> Test: SUCCESS <<'\n\t\t\tif self.message: print '\\033[32;1m%s\\033[0m' % self.message\n\t\t\tprint '>> Test: FAILED <<'\n\t\t\tif self.validator: print '\\033["}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\tstatus = 'OK'\n\t\tstatus += '%02x' % self._status\n\t\treturn status\n\n\n# -- test code --------------------------------------------------\n\ndef make_checker(name, type):\n\t\"\"\"\n\tConstruct validator `name` (format :attr:`type` by name).\n\t\"\"\"\n\tattrs = {\n\t\t'name' : name,\n\t\t'module' : type.__module__,\n\t}\n\tklass = globals().get(type.__name__, TypeDoc)\n\tif klass is None:\n\t\traise TypeError('unknown exception for type: %s' % type.__name"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\tresult = self._status\n\t\tif not prefix:\n\t\t\treturn result\n\t\treturn '%s: %s' % (prefix, result)\n\n\tdef _print_msg(self, msg):\n\t\tsys.stderr.write(msg + '\\n')\n\n\tdef status_warn(self, msg):\n\t\tif self._status[-1]:\n\t\t\tsys.stderr.write(msg + '\\n')\n\n\tdef status_log(self, msg):\n\t\tif self._status_log:\n\t\t\tsys.stderr.write(msg + '\\n')\n\n\tdef status_debug"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\tstatus = super(Validator, self).status_str(prefix)\n\t\tif self.choices is not None:\n\t\t\tstatus += \"'choices'\"\n\n\t\tif self.default is not None:\n\t\t\tstatus += \" (default={!r})\".format(self.default)\n\n\t\treturn status\n\nclass SetValidator(Validator):\n\t\"\"\"\n\tSet based on Validator() arguments.\n\tSee the module documentation there for details.\n\t\"\"\"\n\tvalue_type = str\n\n\tdef validate(self, value, version=None):\n\t\ttry:\n\t\t\tvalue = self.transform"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string of validator status, with optional prefix.\t\"\"\"\n\t\tlst = ['OK' if getattr(self, 'ok', False) else 'FAILED', 'ERROR', 'FAIL'][self:self.ok]\n\t\tlst.insert(0, ('='*len(prefix)))\n\t\treturn ', '.join(lst)\n\n\t@_log\n\tdef msg_fmt(self, d):\n\t\t\"\"\"\n\t\tFormat a message. \n\t\t\"\"\"\n\t\tt = time.strftime('%Y-%m-%d %H:%M:%S ', time.localtime(time.time()))\n\t\tst = datetime.utcfromtimestamp(self.ts)\n\t\t#st"}]}
{"task_id": "62b45df15108cfac7f2109dd", "prompt": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn \"{} validation\".format(prefix)\n\n\nclass Validator(Validation):\n\tdef __init__(self):\n\t\tValidation.__init__(self, 'validate', validate_log_level_key, validate_log_level_val)\n\n\tdef validate(self, params):\n\t\tval = Validation.validate_parameters(params, self.get_parameters())\n\t\tval = self.convert_check_string_to_int(val)\n\t\tif val:\n\t\t\treturn val\n\t\treturn Validation.create_error_log(self.error_log, params)\n\n\nclass"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn \"Log:\\n%s%s%s\\n\" %(prefix, '-' * len(prefix), prefix)\n\n\tdef time_str(self):\n\t\t\"\"\"\n\t\tReturn string representation of validation log, without optional prefix.\t\"\"\"\n\t\treturn str(self.time)\n\n\tdef update_log_text(self):\n\t\t\"\"\"\n\t\tUsefull in order to retrieve data from validation log for plotting and\n\t\tdisplay.\n\t\t\"\"\"\n\t\tself.log_text = ''\n\t\tself.log = ''\n\t\tif self.model_name == 'Noise':\n\t\t"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn \"%s (from %s)\" % (self.name, self.start_name)\n\n\tdef __str__(self):\n\t\treturn '%s: %s' % (self.time, self.message)\n\n\nclass Event(object):\n\tdef __init__(self, name, value, timestamp, origin_name, origin_event):\n\t\tself.name = name\n\t\tself.value = value\n\t\tself.timestamp = timestamp\n\t\tself.origin_name = origin_name\n\t\tself.external = None\n\t\tself.origin_event = origin_event\n\n"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\t# pylint: disable=no-member\n\t\tif self.action().debug:\n\t\t\tif self.action().warning:\n\t\t\t\tself.action().warning('[%s] %s' % (self.action().name, prefix))\n\t\tr = []\n\t\tif self.loglevel() >= 2:\n\t\t\tr.append(' '.join(self._action_log()))\n\t\t\t#r.append(' %s' % self._action_log())\n\t\t# pylint: disable=no-member\n\t\treturn '%s%s' % (prefix, ', '.join(r))\n\n\t"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\ts = ''\n\tif prefix!= '':\n\t\ts = prefix + '%-20s' % (self.time_str)\n\tif self.is_valid():\n\t\ts = prefix + ':'+ status_str(self.is_valid())\n\treturn s\n\n\nclass _LogReader_base(object):\n\n\tdef __init__(self, file_path=None, log_queue_size=128, logfile=None):\n\t\tif (file_path!= None):\n\t\t\tself.file = ffs_open(file_path, 'a+')\n\t\t"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn self._status_str(prefix+\"  ERROR\")\n\n\tdef status_int(self):\n\t\t\"\"\"\n\t\tReturn integer status of validation log, with optional suffix.\n\t\t\"\"\"\n\t\treturn self._status_str(prefix+\"  ERROR\")\n\n\tdef test_str(self):\n\t\t\"\"\"\n\t\tReturn string for status: 'Success', 'Error', 'Ignored'.\n\t\t\"\"\"\n\t\tstatus = self.status_str(False)\n\t\tif isinstance(status, str):\n\t\t\treturn status\n\t\treturn self._status_str(True)\n\n\tdef __str"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\tval = self._validation_log\n\t\tif prefix:\n\t\t\tif type(val) is bool:\n\t\t\t\treturn \"yes\" if val else \"no\"\n\t\t\telse:\n\t\t\t\treturn prefix + str(self._validation_log)\n\t\telse:\n\t\t\treturn str(self._validation_log)\n\n\tdef _get_validation_log(self, error_log = None):\n\t\tif error_log is None:\n\t\t\terror_log = self._error_log\n\t\tif self.warnings:\n\t\t\tfor warning in self.warnings:\n\t\t\t\terror_log = warning"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\tstatus_value = self.validation_status\n\t\tif self.validation_status == 'FAIL':\n\t\t\tstatus_value += 'x'\n\t\treturn str(status_value)\n\n\tdef get_status_repr(self):\n\t\tstatus = self.get_status()\n\t\treturn '%s %s' % (prefix, status)\n\nclass OutputCaptureTests(TestCase):\n\tdef setUp(self):\n\t\tself.outputcapture = StringIO()\n\t\tself.outputcapture.write('output')\n\n\tdef test_file_output(self):\n\t\t\"\"\"\n\t\tThe"}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\treturn prefix + type(self).__name__+' :'+ self.__str__()\n\n\t#\n\t#\n\tdef has_default(self):\n\t\treturn True\n\n\t#\n\t#\n\t# The _valid_log_methods attribute functions as defined by the base\n\t# class.\n\t#\n\t# The public function is generated by parsing the input log before\n\t# using the derived constructor.\n\t#\n\tdef _valid_log_methods(self, lines, _method_name):\n\t\tresults = []\n\t\tfor line in lines:\n\t\t\tline ="}, {"generated_text": "def status_str(self, prefix=''):\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\t\"\"\"\n\t\ttry:\n\t\t\tif prefix:\n\t\t\t\treturn''.join((self.prefix_str(p, prefix + '-'), (str(t) if len(t) > 1 else 0) for t in self.status_names()))\n\t\t\telse:\n\t\t\t\treturn str(self.status_names()[0])\n\t\texcept Exception:\n\t\t\traise ValueError('Invalid log configuration. Empty string or string with prefix \"%s\"' % (prefix))\n\n\tdef status_str_full(self):\n\t\tif self.status_names()!= []:\n\t\t\treturn''.join(self.status_"}]}
{"task_id": "62b45e135108cfac7f2109f4", "prompt": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\t# If we don't want to allow self-generated identifiers to return False, we will be\n\t\t# unable to determine if the identifier is valid at all, so we can assume that the\n\t\t# identifier is actually the current identifier and not the value.\n\t\treturn isinstance(identifier, str) and identifier.strip() == ''\n\nclass FileOrDirPath(object):\n\t\"\"\"\n\tA path object which does not contain any parent folders (i.e. parent folder) that need to\n\texist.\n\n\t:ivar Path parent: The parent folder path\n\t:ivar str filename:"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn True\n\n\tdef set_properties(self, properties):\n\t\tpass\n\n\tdef set_type(self, properties):\n\t\tpass\n\n\tdef get_type(self):\n\t\tpass\n#\n# Copyright (c) 2013 Juniper Networks, Inc. All rights reserved.\n#\n\n#\n# analytics_client.py\n#\n# This file is part of orchestrator, a shared open source monitoring platform.\n#\n# orchestrator is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn True\n\n\n@python_2_unicode_compatible\nclass IETFException(StandardException):\n\t\"\"\"\n\tRaise when there is an error parsing AST\n\t\"\"\"\n\tdef __init__(self, msg, value, errors):\n\t\t\"\"\"\n\t\tCreate an Exception. msg - error message\n\t\tparam value - string containing error details\n\t\t\"\"\"\n\t\tmsg = 'While parsing error, %s:\\n%s - %s\\n' % (msg, str(value), errors)\n\t\tmsg = msg + '\\n%s' % value\n\t\tStandardException.__init"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn self._schema.definitions[identifier].isValid()\n\n\tdef is_instance_of(self, identifier):\n\t\t\"\"\"Return True if identifier identifies an instance of a class or class_name.\t\"\"\"\n\t\treturn self._schema.definitions[identifier].isInstantiate()\n\n\tdef is_null(self, identifier):\n\t\t\"\"\"Return True if identifier has not been used by this instance,\n\t\tas is nulled out.  This is necessary because is_valid() may change\n\t\tfor future usage during API changes.\"\"\"\n\t\treturn self._schema.definitions[identifier].is"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn bool(self.__id)\n\tpass\n\tdef validate(self):  # pylint: disable=unused-argument\t\"\"\"\n\tVerify that identifier is valid, and return True if it's valid.\t\"\"\"\n\t\treturn self.__id == None or isinstance(self.__id, Id) and self.__id!= None\n\t#---The following is where we test for \"true\"\n    def __nonzero__(self):  # pylint: disable=unused-argument\t\"\"\"\n\t@return  Identifier if it is not valid.\n\t@sa TODO : Solve circularly?\n\t\t@"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\tidentifier = identifier.strip()\n\t\treturn self.match(identifier, IDENTIFIER_VALID)\n\n\tdef is_valid_name(self, name):  # pylint: disable=unused-argument,too-many-branches,too-many-statements\n\t\treturn self.match(name, name)\n\n\tdef match(self, name, *identifiers):\n\t\tif \"*\" in identifier:\n\t\t\tfor identifier in identifiers:\n\t\t\t\tidentifier = \"_\" + identifier + \"*\"\n\t\treturn IDENTIFIER_RE.match(name)\n\n\nclass TypeRef(type):\n\t\"\"\"\n\tBase class"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn identifier is not None and identifier.isdigit()\n\"\"\"\nTests for third_party_auth helper functions.\n\"\"\"\n\nfrom datetime import datetime\nimport json\n\nimport ddt\nfrom django.conf import settings\nfrom django.core.urlresolvers import reverse\nfrom django.test.client import RequestFactory\nfrom django.test.utils import override_settings\nfrom django.utils import timezone\nfrom django.contrib.auth.models import AnonymousUser\nfrom django.contrib.sites.models import Site\nfrom django.contrib.auth.hashers import make_password\nfrom django.utils.http import int"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn self.data.get(\"name\")!= \"\"\n\n\tdef get_fullname(self):\n\t\t\"\"\"\n\t\tDerived classes must implement this method.\n\t\t\"\"\"\n\t\treturn \"{}.({})\".format(self.__class__, self.__class__.__name__, self.identifier)\n\n\tdef to_json(self):\n\t\t\"\"\"\n\t\tOverrides the default serializer implementation to allow for specific serialization methods.\n\t\t:return:\n\t\t\"\"\"\n\t\treturn {\n\t\t\t\"identifier\": self.identifier,\n\t\t\t\"name\": self.get_fullname(),\n\t\t}\n\n\tdef to_xml("}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn isinstance(identifier, tuple(str(identifier))) and \\\n\t\t\tnot identifier or (len(identifier)==1 and \\\n\t\t\t\tidentifier[0]=='_')\n# (c) 2012, Michael DeHaan <michael.dehaan@gmail.com>\n# (c) 2017 Ansible Project\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\nDOCUMENTATION = '''\n    strategy: linear\n    short"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\t\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\t\"\"\"\n\t\treturn True\n\n\tdef get(self, identifier):\t\t\t\t# pylint: disable=unused-argument\n\t\t\"\"\"\n\t\tReturn True if identifier is present in this base implementation, else False.\n\n\t\tThe identifier string is a list of identifiers which can be used to resolve the\n\t\tidentifier. For example, if we have this identifier in\n\t\t[some-identifier], the identifier for the ID token would be foo_1\n\t\tand if we have this identifier in [some-identifier], the identifier\n\t\tfor the tag identifier would be foo_2,bar_3,....\n\t\t\"\"\"\n\t\treturn"}]}
{"task_id": "62b45e145108cfac7f210a07", "prompt": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tversion = None\n\t\tif self.spec_version is not None:\n\t\t\tversion = self.spec_version\n\t\t# Check to see if the version is valid\n\t\tif version is None:\n\t\t\traise InvalidFormatException(\"Invalid inventory. Only expected \\\"$HOSTNAME\\\" or \\\"$DOMAIN\\\" or \\\"$OSVERSION\\\"\")\n\n\t\tif version > self.inventory.num_vols:\n\t\t\traise InvalidFormatException(\"Invalid inventory. Only expected \\\"$MAXVOL\\\", but found \\\"\" + str(self.inventory.num_vols) + \"\\\"\")\n\n\t\tif self.specs["}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif extract_spec_version:\n\t\t\traise NotImplementedError()\n\t\tif self.spec_version is None:\t\t\t\t\t\t\t\t# Default to whatever was specified.\n\t\t\traise InvalidCode('The specification does not contain a type version.')\n\t\ttry:\n\t\t\tself.spec_version = extract_spec_version(self.version, inventory.version_range.version_scheme)\n\t\texcept InvalidCode:\n\t\t\traise InvalidCode('The version provided to the inventory does not conform to the version scheme.')\n\n\tdef __str__(self):\n\t\tif resolve_file:\n\t\t\treturn '/'.join"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif not inventory.version or self.spec_version!= get_spec_version(inventory.version):\n\t\t\traise FatalError(\"Invalid inventory type for %r\" % (inventory, ),\n\t\t\t\t\"Expected %s, got %s.\" % (get_spec_version(inventory.version), self.spec_version))\n\t\n\t\tfor test in self._tests:\n\t\t\tif test.test_key not in inventory:\n\t\t\t\tcontinue\n\n\t\t\ttest_fn = inventory[test.test_key]['TEST_FUNC']\n\t\t\tif not test_fn:\n\t\t\t\traise FatalError(\""}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tnew_version = self.get_version(inventory)\n\t\tif new_version!= self.spec_version:\n\t\t\tif extract_spec_version:\n\t\t\t\traise errors.InventoryVersionMismatchError(new_version, self.spec_version)\n\t\t\telse:\n\t\t\t\traise errors.InventoryVersionMismatchError(self.spec_version, new_version)\n\n\tdef set_inventory_variable(self, inventory):\n\t\tself.inventory = inventory\n\n\tdef set_inventory_variables(self, inventory, request_body):\n\t\tself.inventory_variables.update(request_"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tdef validate(item, specs_map, error_on_none=True, required_by_default=None, required_by_default_map=None):\n\t\t\tif not isinstance(item, basestring) or not hasattr(item, '__iter__'):\n\t\t\t\traise TypeError(\"Invalid inventory spec item: %s - %s\" % (item, self.spec_version))\t\n\t\t\tfor spec_key, item_value in item.items():\n\t\t\t\ttry:\n\t\t\t\t\tspec_validator(spec_key, item_value, required_by_default, required_by_default_map"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif self.spec_version is not None and not self.spec_version.startswith(MARKER_STR):\n\t\t\traise ArgumentError('invalid inventory spec_version format, expected version')\n\t\telse:\n\t\t\tself.spec_version = None\n\n\t\t# validate\n\t\ttry:\n\t\t\tif extract_spec_version:\n\t\t\t\tself._validate_spec_version()\n\t\t\telse:\n\t\t\t\tself._validate_profile_name()\n\t\texcept ArgumentError as e:\n\t\t\tif extract_spec_version and required_version:\n\t\t\t\twritemsg(_(\"The 'extract_spec_version"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif self.spec_version and not extract_spec_version:\n\t\t\traise self.exception(\"Cannot use a supplied inventory without version spec\")\n\n\t\tif inventory[self.section] is None:\n\t\t\traise self.exception(\"Invalid entry point\", self.section)\n\n\t\tself.validate_syntax(inventory, start_pos=self.spec_version[1])\n\t\tself.validate_required_attributes(inventory)\n\t\tself.check_valid_fields(inventory)\n\t\treturn self.validate_dependencies(inventory)\n\nclass DependencyCheck(Check):\n\t\"\"\"\n\t"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tversion = self.spec_version.value if self.spec_version else VERSION_WITHOUT_BUILD\n\t\n\tif not self.is_valid(inventory):\n\t\treturn []\n\t\n\ttry:\n\t\tret = []\n \n\t\ttry:\n\t\t\tfor pkg in inventory:\n\t\t\t\tif isinstance(pkg, type(conf)):\n\t\t\t\t\tpkg = getattr(pkg, \"USE\")\n\t\t\t\t\tif not pkg:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tif pkg.EQ(version, pkg.EQ(self.required(pkg.VERSION))) == 0:\n\t\t\t\t\t\tpkg = None\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t\t\n\t\t\t\t\ttry:\n\t\t\t\t\t"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\t# If we cannot parse the spec version from this data, fail out.\n\t\tif'version' not in inventory.test_info:\n\t\t\tif not self.spec_version:\n\t\t\t\treturn None\n\n\t\t\treturn None\n\n\t\treturn self.spec_version.as_version_string()\n\n\tdef should_contain_expected(self, item, fields):\n\t\tactual = []\n\t\tfor field in fields:\n\t\t\titem_val = item.get(field)\n\t\t\tif not self.should_contain_extra_values(\n\t\t\t\t\t\t\t\t\t\t\trecord_value=item_val, "}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\t\"\"\"\n\tValidate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.\t\"\"\"\n\t\tif self.spec and isinstance(self.spec, SpackVersion):\n\t\t\tver = self.spec\n\t\telse:\n\t\t\tver = self.spec_version[1]\n\t\tif self.version is not None:\n\t\t\t# the version is specified as a \"normal\" specification if the spec\n\t\t\t# is not a valid version.\n\t\t\tif not ((self.spec_version[0] is None) or ver >= self.version) or \\\n\t\t\t   (self.spec_version[0] > ver):\n\t\t\t\traise RezSystemError(\"Invalid version '%s'. Must be"}]}
{"task_id": "62b45e145108cfac7f210a09", "prompt": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tfile_keys = []\n\t\tfor f in manifest_files:\n\t\t\tif not filename_match(f, self.manifest_hashes):\n\t\t\t\tcontinue\n\t\t\tif filename_check(f, digests_used):\n\t\t\t\tfile_keys.append(f)\n\t\treturn file_keys\n# -*- coding: utf-8 -*-\n#\n#  plextv documentation build configuration file, created by\n# sphinx-quickstart on Mon Nov 24 22:04:03 2015.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\t# If there are no files in the archive, don't check them\n\t\tif len(manifest_files) <= 0:\n\t\t\treturn False\n\t\t\n\t\tlogger.info(\"Checking the archives in manifest...\")\n\t\tfor f in manifest_files:\n\t\t\tif not self.check_digest_files(f, digests_used):\n\t\t\t\t# logger.error('Cannot find a digest file for {0}, skipping...'.format(f))\n\t\t\t\treturn False\n\t\treturn True\n\t\t\n\tdef check_manifest_files(self, manifest_files, archive_folder):\t# {{{\n\t\t#"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tfor i, digest in enumerate(manifest_files):\n\t\t\tmanifest_digests_ok, manifest_ok = checksum_and_hash(digest, manifest_files[i])\n\t\t\tif manifest_digests_ok:\n\t\t\t\tif not manifest_digests_ok:\n\t\t\t\t\tself.build_log_msg(\"Not all manifest files are up to date with digest %d.\\n\" % i,\n\t\t\t\t\t\tlogging.WARN,\n\t\t\t\t\t)\n\t\t\t\t\tmanifest_chunks = {}\n\t\t\t\t\tmanifest_errors = []\n\t\t\t\t\tfor chunk_index, chunk in enumerate(manifest_chunks):\n\t\t\t\t\t\t"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tglobal digests_ok, digests_warned\n\t\tdef check_digests(manifest_files, digests):\n\t\t\tif manifest_files is None:\n\t\t\t\treturn digests_warned, digests\n\t\t\tfor digest in digests:\n\t\t\t\tfile_hash = self.get_digest_hash(manifest, hashlib.md5())\n\t\t\t\tif digest in manifest_files:\n\t\t\t\t\tmd5 = manifest_files[digest]\n\t\t\t\t\tdata = self.get_data_from_file(archive_dir + file_hash)\n\t\t\t\t\tdigests_done.append(((md"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tfrom octoprint.util import atomic_write\n\t\tfrom octoprint.util.manifest import Manifest\n\n\t\tdigests_not_used = set(digests_used) - set(manifest_files.keys())\n\t\tmanifest_contents_used = set()\n\n\t\tif len(manifest_files) == 0 and digests_not_used:\n\t\t\treturn\n\n\t\tif len(digests_used) > 0:\n\t\t\tdigests_used &= digests_not_used\n\t\t\tmanifest_contents_used &= set([m.digest for m in Manifest.for_"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tfor file_info in manifest_files:\n\t\t\tfile_digest_verified = self.__get_file_digest(file_info)\n\t\t\tfile_digest_not_verified = self.__get_file_digest(file_info, not_before_time=True)\n\t\t\t# If we are not verifying the hash, or if the path in the manifest does not match then the file is not valid\n\t\t\t# Therefore, the current line has no idea why it failed.\n\t\t\tif file_digest_verified is None or file_digest_verified.get_digest() == file_digest_"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tdigests = self.get_digests_from_manifest(manifest_files)\n\t\tfor digest, expected_digest in iteritems(digests):\n\t\t\tdigest['digests'] = self.get_digest_from_manifest(manifest_files, is_added=digest['digests_added'])\n\t\t\texpect = None\n\t\t\tif digest['digests_added']!= expected_digest:\n\t\t\t\tsys.stdout.write(pretty_print_dict( digest ) + \"\\n\")\n\t\t\t\texpect = digest\n\t\tif expect and digests_used!= expect:\n\t\t\traise AssertionError(\""}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tfiles_hash = {}\n\n\t\tfor my_file in manifest_files:\n\t\t\thash = my_file[MANIFEST_SHA1_LENGTH:]\n\n\t\t\ttry:\n\t\t\t\tmanifest_file = open(self.get_full_path(my_file), \"rb\")\n\t\t\texcept IOError:\n\t\t\t\tlogging.warn(\"File does not exist: %s\", os.path.basename(my_file))\n\t\t\t\tcontinue\n\n\t\t\ttry:\n\t\t\t\tmy_digests = calculate_digests(manifest_file, digests_used)\n\t\t\t\tif my_digests!= digests_used"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tmanifest = read_manifest(digests_used)\n\t\tmanifest_file_for_each_digest = {}\n\t\tfor manifest_file in manifest_files:\n\t\t\tfile_size = os.path.getsize(manifest_file)\n\t\t\tif file_size > 0:\n\t\t\t\tdigest = self._calculate_digest_file_size(manifest_file, file_size, digests_used)\n\t\t\t\tself.logger.info('[%s] %s', self.manifest_name, digest)\n\t\t\t\tfor file_name in manifest['names']:\n\t\t\t\t\tcurrent_digests ="}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\t\"\"\"\n\t\tfound = 0\n\t\tskipped = 0\n\t\tif self.args.test:\n\t\t\tmanifest_files_str =''.join(manifest_files)\n\t\t\tif os.path.exists('MANIFEST'):\n\t\t\t\tif manifest_files_str in manifest_files_str:\n\t\t\t\t\tdebug(self.color_green('Manifest {} already exists'.format(manifest_files_str)))\n\t\t\t\telse:\n\t\t\t\t\tfailed_manifests = self.merge_manifests_lists(manifest_files)\n\t\t\t\t\tmissing_manifests = self.merge_manifests_lists(missing_manifests)"}]}
{"task_id": "62b45e165108cfac7f210a16", "prompt": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\ttry:\n\t\t\tVERSION_CACHE[self.version_id]\n\t\texcept KeyError:\n\t\t\traise TypeError(\"'{}' is not a valid inventory version\".format(previous_version))\n\n\nclass InventoryValidatorError(Exception):\n\t\"\"\"\n\tA general exception for inventory errors during validation.\n\t\"\"\"\n\tpass\n\nclass Inventory(object):\n\t\"\"\"\n\tA collection of inventory objects (inventory objects) of a single\n\ttype in some (global file formats).\n\t\"\"\"\n\tdef __init__(self, options, inventory_id):\n\t\t\"\"\"\n\t\tInitialise an"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tif self.as_prior.ver == prior.as_prior:\n\t\t\treturn\n\t\t\n\t\tfor attr in ('release','version', 'update', 'extras', 'conflicts', 'provides'):\n\t\t\tif hasattr(prior, attr):\n\t\t\t\tif not getattr(prior, attr) in RELIMINATION_VER_VALUES:\n\t\t\t\t\traise ValueError('Invalid value for attribute in extra attribute')\n\t\t\n\t\tif prior.extras:\n\t\t\tfor attr in ('extras','release', 'candidate_serial','remote_uri', 'digests', 'build_location'):\n\t\t\t\tif not getattr("}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\traise NotImplemented\n\n\tdef check_for_duplicate_names0(self):\n\t\t\"\"\"For a missing inventory object name, if possible, we raise an error with\n\t\tan appropriate error message. It's also used for the error messages above.\n\t\t\"\"\"\n\t\tduplicate_names0(self.name)\n\n\tdef name_is_dup(self):\n\t\t\"\"\"If the object we've received is duplicate, this means the name has not\n\t\tbe unique.\n\t\t\"\"\"\n\t\treturn len(self.items) == len(set(self.items)) and \\\n\t\t\ttuple(map"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\t# check that self.has_version() is True, 'prior_version' \n\t\t#  should have been a valid value of the prior\n\t\tif self.has_version():\n\t\t\t# check that self.has_version() is True, 'prior_version' \n\t\t\t#  should be a string and not a None type\n\t\t\tif not isinstance(prior, basestring):\n\t\t\t\traise TraitError(\"Error: prior must type None or a string\")\n\t\t\tif not isinstance(self.prior, (str, unicode)) and not isinstance(prior, unicode):\n\t\t\t\traise TraitError"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tif type(prior) is not InventoryValidator:\n\t\t\traise TypeError('Expected type is InventoryValidator')\n\n\t\tif not isinstance(prior, InventoryValidator):\n\t\t\traise TypeError('Expected type is InventoryValidator')\n\n\t\tif not prior._version:\n\t\t\traise ValueError('Particular prior version not found')\n\t\tif not prior._version.is_valid():\n\t\t\traise ValueError('Actual prior version invalid')\n\n\t\n\tdef validate_all_prior_versions(self, prior_version):\n\t\t\"\"\"\n\t\tCheck that all prior versions are valid.\n\n\t\tParameters:\n\t\t\tprior_version (Inventory"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tif not isinstance(prior, InventoryValidator):\n\t\t\traise InventoryValidator.ValidationException(\"Expected inventory validator object\")\n\t\t\n\t\tif len(self._inventory)!= len(prior._inventory):\n\t\t\traise InventoryValidator.ValidationException(\"Inventory must have the same number of items as prior\")\n\t\t\n\t\titems_to_be_checked = [(key, val) for key, val in prior._inventory.iteritems()]\n\t\tduplicate_items = set(items_to_be_checked)\n\t\tif duplicate_items == set(self._inventory.keys()):\n\t\t\traise InventoryValidator.ValidationException(\"Duplicate items"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tif self is not prior.inventory.context:\n\t\t\traise ValidationException('Error: the parent object has been validated before this method can be used')\n\t\tself.validate_inventories(prior.inventories, prior.inventory)\n\tdef validate_inventories(self, inventories, inventory):\n\t\t\"\"\"\n\t\tValidate that there are no duplicated inventory objects, which would\n\t\tbe unequal or conflicting between all inventory objects.\n\t\t\n\t\t@param inventories: The objects to validate.\n\t\t@param inventory: The inventory.\n\t\t\"\"\"\n\t\texisting_inventory ="}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\t# assert prior, \"Invalid prior\"\n\t\t\n\t\tif type(prior)!= InventoryValidator:\n\t\t\traise ValueError(\"Expected a InventoryValidator object\")\n\t\n\tdef validate_priority(self, priority):\t# type: (str) -> None\n\t\t\"\"\"\n\t\tValidate the priority value.\n\n\t\tThis is used by validation.py on how validating an\n\t\t'on-demand' flag should be performed on this object. Other\n\t\tcriteria can be used to set prior as 'inventories'\n\t\t\"\"\"\n\t\t\n\t\tself._validate_priority(priority, [PriorityValidator])\n\t\n\tdef _validate"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tif self.priority!= prior.priority:\n\t\t\traise InventoryValidatorException(\"Value of prior %s must be an\"\n\t\t\t                                \" InventoryValidator priority.\" % Prio.tag)\n\t\telif self.name!= prior.name:\n\t\t\traise InventoryValidatorException(\"Value of prior %s must be a %s\"\n\t\t\t                                \" InventoryValidator name.\" %\n\t\t\t                                Prio.tag)\nimport argparse\nimport os\nfrom itertools import ifilter\nimport re\nimport subprocess\nimport shutil\nimport sys\n\nfrom django.apps import apps\nfrom django.core.management.base import BaseCommand\n"}, {"generated_text": "def validate_as_prior_version(self, prior):\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.\t\"\"\"\n\t\tif isinstance(prior, InventoryValidator):\n\t\t\tif prior.version is None:\n\t\t\t\traise InvalidVersion(\"The required prior is not an inventory object.\")\n\t\t\telif not prior.version == self.version:\n\t\t\t\t# no change, just return\n\t\t\t\treturn self\n\t\telse:\n\t\t\traise InvalidLevel(\"The required prior must be a \" + ErrorLeader._error_leader())\n\n\tdef validate_all_keys(self, keys):\n\t\tif keys is None:\n\t\t\tfor k in self.required_keys:\n\t\t\t\tself.logger.error(\"Missing required key %s\""}]}
{"task_id": "62b45e165108cfac7f210a17", "prompt": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\ndef get_logical_cache_map(path_map):\n\t\"\"\"\n\tGet a map of logical paths to cache content files.\n\nReturns a dictionary: logical_cache -> list(logical_mtime -> hash)\n\tThis is a read-only dict used to prevent multiple write attempts\n\twhen processing the current state of files on disk. \"\"\"\n\tlogical_cache_keys = (\n\t\t'content',\n\t\t'file_mtime',\n\t\t'physical_path_hash'\n\t)\n\tlogical_cache_entry = None\n\twhile 1:\n\t\ttry:\n\t\t"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\tversion = version_mapping[version]\n\tlogical_path_in_state = set()\n\n\tfor file in inventory.files:\n\t\tif'referenced_files' in file:\n\t\t\tcontinue\n\n\t\tif os.path.isdir(storage.relpath(file, version.folder)):\n\t\t\trelative_path = os.path.relpath(file, version.folder)\n\t\t\tif os.path.exists(relative_path):\n\t\t\t\t# This file already exists. Use it.\n\t\t\t\trelative_atomic_add = False\n\t\t\t\trelative_content_path = os.path.join("}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\trelative_content_files = set()\n\n\t\tfor (absolute_path, statinfo) in inventory.iter_detail():\n\t\t\t\t# TODO: can we move this to a more general way?\n\t\t\t\trelpath = os.path.relpath(absolute_path, version)\n\n\t\tfor absolute_path in relative_content_files:\n\t\t\t# The statinfo is formatted so that it knows what files to\n\t\t\t# merge with and what file's modification time\n\t\t\trelative_statinfo = os.stat(absolute_path)\n\t\t\tif relative_statinfo.st_mtime > statinfo.st"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\tcontent_files = {}\n\n\t\tfor x in inventory._parser.values():\n\t\t\tpath = x._filename.replace('\\\\', '/')\n\t\t\tif not os.path.exists(os.path.join(path, 'content')):\n\t\t\t\tlog.debug('File does not exist: %s', path)\n\t\t\t\tcontinue\n\n\t\t\tmetadata = inventory._parser.get_section_dict('metadata')\n\t\t\tif not (os.path.exists(path) and metadata.get('version') in version.get('children', [])):\n\t\t\t\t# This is the first version missing and it's not in our"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\tcontent_files = {}\n\t\tfor path in inventory['_known_paths']:\n\t\t\tif path == version:\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tpath = to_native_path(path)\n\t\t\t\tif not os.path.exists(path):\n\t\t\t\t\tdata = open(path, 'rb').read()\n\t\t\t\t\tstatic_data_from_file = False\n\t\t\t\t\ttry:\n\t\t\t\t\t\tstatic_data_from_file = True\n\t\t\t\t\t\tdata = to_text(data, errors='surrogate_or_strict')\n\t\t\t\t\texcept IOError:\n\t\t\t\t\t\tpass\n\t\t\t\t\texcept Exception as e:\n\t\t\t\t\t"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\t# NOTE: This function is duplicated here, since a future version\n\t\t# of this module supported all inventories (and none with the\n\t\t# previous version) may also be possible (and some non-valid)\n\t\t# ongoing changes.\n\n\t\tif version == \"0.0.1\":\n\t\t\treturn {\n\t\t\t\t\"contents_path_in_directory_dict\": \n\t\t\t\t {\"directory_path_file\": os.path.join(settings().getBaseFolder(\"uploads\"), \"contents\")},\n\t\t\t\t\"atomic_group_files\":   \n\t\t\t\t\tset(os.path."}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\tlogical_files = set()\n\t\tfor i in version.split('.'):\n\t\t\tel = inventory.get(i)\n\t\t\tif version == el.metadata.version:\n\t\t\t\tcontinue\n\t\t\tversioned_file = str(el.metadata.version) + '.tbz2'\n\t\t\tif path.isfile(versioned_file):\n\t\t\t\tlogical_files.add(versioned_file)\n\t\treturn logical_files\n\ndef merge_directories(source, target):\n\t\"\"\"\n\tMerge directories from source to target, merging them into a new\n\tdirectories structure, removing some files if necessary."}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\tcontent_files=set()\n\t\tinventory_hash={}\n\t\twhile len(inventory_hash)!=8 or inventory_hash.setdefault('.lock','.'):\n\t\t\tinventory_path=inventory+'/'+version\n\t\t\ttry:\n\t\t\t\t# Load old inventory entries\n\t\t\t\twith open(inventory_path, 'rb') as f:\n\t\t\t\t\told_entries=pickle.load(f)\n\t\t\t\told_entries.update(old_entries)\n\t\t\t\t# Load new inventory entries\n\t\t\t\twith open(inventory_path, 'rb') as f:\n\t\t\t\t\tnew_entries=pickle.load(f)"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\n\n\tlogical_path_map = {}\n\n\tfor rel_path, rel_names, is_symlink in get_links(module):\n\t\t# skip symlinks\n\t\tif os.path.islink(rel_path):\n\t\t\tcontinue\n\n\t\t# ignore files that are ignored\n\t\tif not os.path.basename(rel_path) in ignore: continue\n\n\t\t# process files\n\t\t# print \"processing: \", rel_path\n\n\t\tif is_symlink:\n\t\t\t# create basepath\n\t\t\tif not os.path.isdir(os.path.dirname(rel_path)):\n\t\t\t\t"}, {"generated_text": "def get_logical_path_map(inventory, version):\t\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.\t\"\"\"\n\t\n\n\tlogical_cache = {}\n\tif version not in inventory:\t\t\t\t\t\t\t\t\n\t\treturn logical_cache\n\n\tfiles = {}\n\tfor child in inventory[version]['children']:\n\t\tchildren = get_mapped_children(child)\n\t\tpaths = [child_path for child_path in children]\n\t\tfor index in range(0, len(paths)):\n\t\t\tpath = paths[index]\n\t\t\tbasename = os.path.basename(path)\n\t\t\tlogical_path_in_state = get_file_contents(path)\n\n\t\t\tif logical_path_in_state in ["}]}
{"task_id": "62b45e175108cfac7f210a19", "prompt": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tdef _fixity_block(filename, target_path):\n\t\t\t\"\"\"Verify the given file is a fixity block, make sure it\n\t\t\texists in the target folder, and is not referenced\n\n\t\t\t@param filename: the filename (relative to target_path)\n\t\t\t@param target_path: the location in target_path -- not necessarily\n\t\t\tchecked\n\t\t\"\"\"\n\t\t\tassert os.path.isdir(target_path)\n\t\t\twith open(filename, \"rb\") as fsock:\n\t\t\t\tfsock.seek(filename.tell() + 8) # jump past 8 bytes\n\t\t\t\t"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tfixity = fixity.upper()\n\t\tif fixity[-1] not in '0123456789' and fixity[-1]!= '.':\n\t\t\traise Exception('Invalid character in fixity section '\n\t\t\t\t\t  'of inventory file, found uppercase: %s', fixity)\n\n\t\tfor f in manifest_files:\n\t\t\tif not os.path.exists(f):\n\t\t\t\traise Exception('Missing file referenced by inventory file: %s' % f)\n\t\tfor f in manifest_files:\n\t\t\tif f.endswith(\".json\"):\n\t\t\t\tif not os.path.exists(f[:-5"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tresult = True\n\t\t\n\t\t# the first file\n\t\tif len(manifest_files[0]) == 0:\n\t\t\tself._logger.error(\"Manifest file reference is empty\")\n\t\t\tresult = False\n\t\t# the second files\n\t\telse:\n\t\t\tfile_list = []\n\t\t\tfor filename in manifest_files:\n\t\t\t\tfile_list.append(filename)\n\t\t\t\n\t\t\t# determine full path\n\t\t\tfound = self._find_file(file_list)\n\t\t\t\n\t\t\t# file is referenced\n\t\t\tif not found: self._logger.error(\"No file referenced by filename '%s'\" %\n\t\t\t\t\t\t\t        "}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tprint \"\\n[+] Checking for fixity\"\n\n\t\tfor manifest_type in sorted(manifest_types.keys()):\n\t\t\tif manifest_type == \"blacklist\" or manifest_type == \"whitelist\":\n\t\t\t\tignored_files = []\n\t\t\t\tfor file in manifest_files:\n\t\t\t\t\tif fnmatch.fnmatch(file, fixity):\n\t\t\t\t\t\tignored_files.append(file)\n\n\t\t\t\tif ignored_files:\n\t\t\t\t\tprint \"\\n[+] Verifying \"+str(fixity)\n\t\t\t\t\traise SafeException(\"Fixity block not valid, do run make clean on them first\")\n\n\t\t\t\t"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tfor filename in manifest_files:\n\t\t\ttry:\n\t\t\t\twith open(filename, \"r\") as fixity_stream:\n\t\t\t\t\tcurrent_version = None\n\t\n\t\t\t\t\tfor line in fixity_stream:\n\t\t\t\t\t\ttoken = token_regex.match(line)\n\t\t\t\t\t\tif token:\n\t\t\t\t\t\t\t\tcurrent_version = version_regex.match(item)\n\t\t\t\t\n\t\t\t\tif current_version == None:\n\t\t\t\t\traise Exception(\"Unknown version found in fixity block: %s\" %\n\t\t\t\t\tfilename)\n\n\t\t\t\tif not force_build:\n\t\t\t\t\tself._clean_fixity_block(filename"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tfor f in manifest_files:\n\t\t\tif not os.path.exists(f) or not os.path.isfile(f):\n\t\t\t\tself.fatal('missing file:'+ f)\n\t\tself.fixity = fixity\n\t\tself.check(fixity)\n\n\t\"\"\"\n\tCheck the fixity block at the top of each tracked file.\n\tIf the top file does not exist, it is created.\n\t\"\"\"\n\tdef check(self, fixity):\n\t\t\"\"\"\n\t\tCheck the fixity block.\n\t\t\"\"\"\n\t\tif os.path.exists"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tmodified = False\n\t\tignored = False\n\n\t\tif not isinstance(fixity, Fixity):\n\t\t\treturn self._failure(\n\t\t\t\t'ERROR: Found corrupt fixity: %r' % (fixity,)\n\t\t\t)\n\n\t\tfor filenode in manifest_files:\n\t\t\tif filenode.type == 'check':\n\t\t\t\tif modified:\n\t\t\t\t\tself._mark_dirty(\n\t\t\t\t\t\tfiles[filenode.name].modified,\n\t\t\t\t\t\tfilenode.is_dirty,\n\t\t\t\t\t\tfilenode.size,\n\t\t\t\t\t\tfilenode.stat(follow=False),\n\t\t\t\t\t\tfilenode.stat("}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tfor f in manifest_files:\n\t\t\tfull_f = os.path.join(self.get_template_path(), f)\n\t\t\twith open(full_f, 'r') as f:\n\t\t\t\tself.parse(f.read(), full_f, fixity)\n\n\n\t\"\"\"\n\tValidate one specific file.\n\nIf the fixity is not correct, report that it can't be done.\n\"\"\"\n\tdef validate(self, fixity, manifest_files):\n#\t\tprint 'File is invalid.'\n\n\t\tfull_f = os.path.join("}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tif not self.inventory.has_section('Fixity'):\n\t\t\treturn # FIXME\n\n\t\tfixity_files = [f for f in manifest_files if f.endswith('.yml')]\t\t\t# Remove \"fixity\" if we have duplicates\n\tfixity_ids = [fix.partition('.yml')[0] for fix in fixity]\n\n\tclean_fixity_file = 'fixity/{}'.format(fixity_ids[0])\n\tif self.inventory.has_option('Fixity', 'id_file'):\n\t\tremove(self.inventory, 'Fixity',"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\t\"\"\"\n\tValidate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\t\"\"\"\n\t\tfor manifest in manifest_files:\n\t\t\tif not manifest.endswith(fixity):\n\t\t\t\tcontinue\n\n\t\t\tmanifest_file = open(manifest,'r')\n\t\t\tmanifest_lines = manifest_file.readlines()\n\t\t \tmanifest_file.close()\n\n\t\t\tmissing_files = [f for f in manifest_lines if not f.startswith('#')]\n\t\t\tif len(missing_files) > 0:\n\t\t\t\tcontinue\n\n\t\t\tif manifest_lines[0] not in '\t' and manifest_lines[5] not in '\t':\n\t\t\t\t\tcontinue\n\t\t\tif '\t' not in manifest"}]}
{"task_id": "62b463153879012d19481498", "prompt": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\tif path in config.get('files'):\n\t\t\tvalues = [f.split('.')[-1] for f in config['files'][path].values()]\n\t\t\treturn values\n\t\telse:\n\t\t\traise Exception(\"no config file '%s'\"%path)\n\n\tdef del_path(path, config=None):\n\t\t\"\"\"\n\t\tDelete file in config['files']\n\t\t\"\"\"\n\t\tif path in config.get('files'):\n\t\t\tdel config['files'][path]\n\t\telse:\n\t\t\traise Exception(\"no config file '%s'\"%path)\n\n\tdef get_files_list"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\treturn (f for f in listdir(path) if isfile(join(path, f)))\n\n\ndef file_size(path):\t\"\"\"\n\tReturn the total size of `path` in bytes.\n\t\"\"\"\n\treturn os.path.getsize(path)\n\n\ndef make_path(files, path):\n\t\"\"\"\n\tCreate a path from a list of files and a hash function that will\n\treturn a string containing path.\n\t\n\tIf the files are already exist, it's not overwritten\n\tIf the path exists and not equal to the files's hash, \n\tset its"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\treturn [p.strip() for p in files_recursive(path)]\n\ndef files_recursive(path):\n\t\"\"\"\n\tReturn recursively files\n\n\t:param path: Path to traverse\n\t:return: Path to the created file\n\t\"\"\"\n\tfiles = []\n\tif not os.path.exists(path):\n\t\tos.makedirs(path)\n\tfiles.extend(get_files(path))\n\treturn files\n\ndef touch(path):\n\t\"\"\" Touches file `path`. Doesn't follow symlinks! \"\"\"\n\tif os.path.islink"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\twith open(path, 'r') as f:\t\n\t\t\treturn f.readlines()\n\texcept IOError as e:\t\n\t\traise\n\n\ndef open_folder(path):\n\t\"\"\"\n\tOpens the specified folder and generates a list of files, \n\twhere each file is named like the file number with the last \n\tnumber in the list.\n\tExample:\n\t\topen_folder(\"/mnt/usb/USB000C5\") creates file nums 0001-1001, \n\t\t0002-00100, 0003, 0004-000FF\n\t\topen_folder(\"USB"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\tif not path:\n\t\t\traise ValueError(\"Invalid parameter\")\n\t\tdir_path = os.path.dirname(path)\n\t\tdir_path = os.path.normpath(dir_path)\n\t\tfiles = \"\"\n\t\tfor root, dirs, files in os.walk(dir_path):\n\t\t\tfor name in dirs:\n\t\t\t\tfilename = os.path.join(root, name)\n\t\t\t\tif os.path.isfile(filename):\n\t\t\t\t\tfiles += name + \"\\\\\"\n\t\t\t\telse:\n\t\t\t\t\tfiles += name + \".py\"\n\t\t\tfiles += \"\\n\"\n        return dirs"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\twith open(path) as f:\n\t\t\treturn [entry.strip('\\n') for entry in f]\n\n\t@staticmethod\n\tdef delete_all_files():\n\t\t\"\"\"\n\t\tDelete all files found in `path`\n\t\t\"\"\"\n\t\tpath = os.path.join(settings.getBaseFolder(), \"files\")\n\t\tfor entry in os.listdir(path):\n\t\t\tif os.path.isfile(entry):\n\t\t\t\tos.unlink(entry)\n\n\t@staticmethod\n\tdef delete_file(path):\n\t\t\"\"\"\n\t\tDelete file with content `file`\n\t"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\tif os.path.isfile(path):\n\t\t\tif os.path.isdir(path):\n\t\t\t\treturn [s for s in os.listdir(path) if os.path.isfile(os.path.join(path, s))]\n\t\t\telse: return []\n\t\telse:\n\t\t\treturn []\n\tfiles = []\n\tif not path.endswith(\"/\"):\n\t\tfor f in os.listdir(path):\n\t\t\tif not f == \"index.html\": continue\n\t\t\tif os.path.isfile(os.path.join(path, f)):\n\t\t\t\tfiles.append("}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\treturn [f for f in listdir(path) if isfile(join(path, f))]\n# -*- coding: utf-8 -*-\n\n# Define here the models for your scraped items\n#\n# See documentation in:\n# http://doc.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\n\n\nclass Wine:\n    # define url pattern is for matching items here: http://jeffkumt.github.io/scrapy-bin/\n    # TODO: http://docs.scrapy.org/\n    # (here we can include different patterns for each"}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\treturn [os.path.join(path, f) for f in os.listdir(path)]\n\n\ndef get_files(path):\n\t\"\"\"\n\tReturn a list of files in `path`\t\"\"\"\n\tfiles = []\n\tfor entry in os.listdir(path):\n\t\tfile_path = os.path.join(path, entry)\n\t\tif os.path.isfile(file_path):\n\t\t\tfiles.append(os.path.join(file_path, file_entry))\n\treturn files\n\n\ndef get_extensions(app):\n\n\t\"\"\""}, {"generated_text": "def files_list(path):\t\"\"\"\n\tReturn the files in `path`\t\"\"\"\n\t\twith open(path, 'r') as files:\n\t\t\treturn files.readlines()\n\n\tdef _is_url_allowed(self, url):\n\t\tif '/' not in url:\n\t\t\treturn False\n\t\treturn re.match(\"^https?://[^:]+\", url)\n\n\tdef _upload_one(self, name):\n\t\t_base = \"/uploads/\"\n\t\t_dir = path.join(_base, name)\n\t\tif not self._is_url_allowed(path.join(_dir, name)):\n\t\t\traise ValueError(_dir)\n\n\t\ttemp_path = path."}]}
{"task_id": "62b463153879012d1948149a", "prompt": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\ndef _create_xml_file_dict(xml_files, files):\n    \"\"\"\n    Create a dictionary of XML files and data\n\n    Parameters\n    ----------\n    xml_files: list of str\n        XML filenames\n    files : list\n        list of files in the folder\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: dictionary of file path -> data in JSON format\n        If error return string 'None' else tuple.\n    \"\"\"\n#    print_message('Creating XML file dictionary')\n\n    if xmls is None or files is None:\n        logger.info('No data files or XML dirs provided"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n        xml_filename = os.path.basename(xml_filename)\n        xml = xmls[xml_filename]\n        filename = xml.get('name', '') \n        if xml:\n                    for f in files:\n\t\t\tname = os.path.join(os.path.join(xml_filename, f), name)\n\t\t\t\n\t\t\ttry:\n\t\t\t\t# create dictionary with the same name as this one\n\t\t\t\t# so we don't overwrite any existing file but add one\n                # or the other, in case we modify a file other than this one\n\t\t\t\tdata = dict(name=name)\n\n                except KeyError"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\tkey = \"package\"\n\t\tvalue = []\n\t\tfor data in xmls:\n\t\t\tfile_path = os.path.join(source,data)\n\t\t\ttry:\n\t\t\t\twith open(file_path) as file_object:\n\t\t\t\t\tdata = file_object.read()\n\t\t\t\tvalue.append(os.path.basename(file_path))\n\t\t\texcept ParseError as e:\n\t\t\t\te.xmlError = \"XML file \\\"%s\\\" could not be read:\\n%s\\n\" %(file_path, e.getMessage())\n\t\t\t\tcontinue\n\t\treturn {key"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n        files = [os.path.splitext(name)[0] for name in files]\n        xmls = [xml_filename.format(name) for name in xmls]\n\t# Sort Xmls by files\n\txmls.sort(key=lambda xml_file: xml_file[-1])\t \n        result = dict()\n\tfor xml_file in files:\n\t    xml_filename, suffix = os.path.splitext(xml_file)\n\t    if suffix == '.xml':\n\t\tresult[xml_filename] = xml_file\n\treturn result\n\t\n    def _group_"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n        #TODO:  handle errors and report errors\n\n        def filter_dir(path):\n            #print path\n            assert not path.endswith('/'), \"Expecting / characters\"\n            return path\n\n        def filter_pattern(path):\n            #print path\n            assert not path.endswith('/')\n            return not path and not path.startswith('/')\n\n        files_filtered = set()\n        files_filtered_pattern = set()\n\n        def parse_file(name):\n            if not name.lower().endswith('.xml'):\n                return\n            fname = name.lower()\n            if '.' not in fname:\n                return\n\n            path = os.path.join"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\nclass PackageManager:\n\t\"\"\"\n\tProvides a set of packages\n\t\"\"\"\n\n\tdef __init__(self):\n\t\tself.current_package = None\n\t\tself.packages = defaultdict(list)\n\t\tself.files = []\n\t\tself.xml_filename = None\n\t\tself.xmls = xmls\n\t\tif self.xmls is None:\n\t\t\tself.current_package = \"\"\n\t\t\tself.packages = []\n\t\t\tself.files = []\n\n\tdef get_package_by_id(self, package_id):\n\t\t\"Get a package \"\n\t"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\tgroup = ''\n        \t# first try the given XML file, if there are single groups\n\t    # it is easier to keep a separate file in memory.\n        for filename in xmls:\n\t    # make the filename absolute path\n\t    filename = os.path.abspath(filename)\n\n\t    # the file itself, if it is in the same directory as the XML file\n\t    if os.path.dirname(filename) == os.getcwd():\n\t        filename = filename[len(dirname(filename)):]\n\n\t    # get the group name\n\t    group = filename.split('.')[0]\n\n\t    # save the XML filename\n\t    group"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n    def _recursive_extract(xmlfile, path=[], depth=0):\n        \"\"\"\n        Recursively create xml child nodes of xml file and extract them.\n\n        :param xmlfile: path to xml file to extract\n        :param path: list of folders to be recursively search in\n\n        :param depth: total recursion depth\n\n        :return: dict with key, value pairs of metadata and value\n\n        Note:\n        \"\"\"\n\n\t\n        children = []\n\n        files_found = []\t\n\n        while depth - 1:\n\n                children.append(path)\t\n\n                for f in zip(*[f for f in files if path.endswith(f)]):"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\n# Extract information from all package files\ndata = {}  \nfor xmlfile in files:\t\n\t# Load xml file\n\txmlfile_xml = load_xml(xmlfile)\n\t# Get all package files\n\tdata[xmlfile_xml.basename][_group_files_by_xml_filename(xmlfile, data)] = xmlfile_xml.description\n\n# Output data to XML file\nxmlfile = open('C:\\\\Users\\\\Jon/Sandbox/Coding/PETSc-1.6.2\\\\data\\\\Xml_Files_Test.xml')"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\t\"\"\"\n\tGroup files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\t\"\"\"\n\t\tpath, filename = os.path.split(source)\n\t\tgroup = xmls[name]\n\t\tif group['type'] == 'directory':\n\t\t\tgroupPath = os.path.join(os.path.abspath('.'),group['path']) \n\t\t\ttarget = \"%s/%s\" % (path, filename)\n\t\telif group['type'] == 'file':\n\t\t\ttarget = os.path.join('file://',group['path'].rpartition('/')[2])\n\t\t\ttarget = os.path.join(target,\"%s\" % filename)\n\t\telse:\n\t\t\traise ValueError"}]}
{"task_id": "62b463153879012d1948149b", "prompt": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\n\t\n\tresults = filter_file(file_path, prefix)\n\treturn bool(results)\nimport pygtk\npygtk.require('2.0')\nimport gtk\nimport datetime\nimport os\nfrom matplotlib.figure import Figure\nfrom matplotlib.lines import Line2D\nfrom matplotlib.patches import Polygon\n\ndef get_start_date():\n    import os\n    import datetime\n    return datetime.datetime.now().strftime('%d.%m.%Y')\n\ndef get_end_date():\n    import os\n    return datetime.datetime.now().strftime('%d.%m.%Y')\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\t# List all the available modules\n\ttry:\n\t\timport importlib\n\t\tfullname =  importlib.import_module(f\".\".join(file_path.split(os.path.sep[:3])), f\".\".join(file_path.split(os.path.sep[:3]))).split(os.path.sep)[-2] # Import from package name\n\t\tif fullname == \"*\": # If this is the local path\n\t\t\tfullname = file_path # Import the top level module\n\t\t\tfullname = file_path # Import the folder name\n\t\tfor p in fullname.split("}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\tfor root, dirs, files in os.walk(file_path):\n                    for file in files:\n\t\t        if file.startswith(prefix):\n\t\t            return True\n\treturn False\n\n\ndef match_file_by_suffix(suffix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `suffix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nsuffix : str\n    Filename suffix\nfile_path : str\n    File path\nReturns\n-------\nbool"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\treturn is_doc_package(file_path, name, prefix)\n\n\treturn not_match_dir(file_path, '%s.py' % name)\n\n\ndef match_file_by_suffix(suffix, file_path):\t\"\"\"\n\tIdentify if a `file_path`, `suffix` belongs to a document package by a given `suffix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nsuffix : str\n    Filename suffix\nfile_path : str\n    File path\nReturns\n-------\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t # check if `prefix` matches file prefix or file path\n\tmatched = False\n\tfor root, dirs, files in os.walk(file_path, followlinks=True):\n\t\tfor file in files:\n\t\t\tif file.endswith(prefix):\n\t\t\t\tmatch = file\n\t\tif matched == True:\n\t\t\treturn True\n\t\tmatched = False\n\treturn False\n# -----\n\n\ndef match_file_by_class(file_class, file_path):\n\t\"\"\"\n\tIdentify if a file belongs to a class\n\n\tParams\n\t----------\npath_class : str"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\treturn os.path.exists(file_path)\n\telse:\n\t\treturn False\n\ndef remove_empty_directories(new_path, verbose, do_check = True):\n\t\"\"\"\n\tDetect and remove empty directories\n\nParameters\n----------\nnew_path : tuple of str\n    Path to the directory that should be removed\nverbose : bool, optional\ndo_check : bool, optional\nReturns\n-------\nbool\n    True - success status of removing empty directories\nbool\n    True - success status of removing directory structure\nbool\n    True - status of removing empty directories in the dir list\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\t# Find a file path relative to the package directory with the prefix < prefix\n\tfor file in listdir(file_path):\n\t\tif file.startswith(prefix):\n\t\t\treturn True\n\treturn False\n\n## TODO : Implement this as part of the EVO API documentation\ndef update_info(doc, info):\n\t# TODO: Implement this as part of the EVO API documentation\n\tpass\n\ndef update_all_files(doc, paths):\n\t# TODO: Implement this as part of the EVO API documentation\n\tpass\n\ndef check_dir(doc, dirname"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\ndef match_package_namespace_to_prefix(namespace, file_path, package_path):\t\"\"\"\n\tIdentify if a `namespace` belongs to a package namespace by a given `package_path`\n\nRetorna `True` para documentos pertencentes a um pacote, el modulo `package_path` indica que la pacote teste \ns'importando' para cada package pressionada\n\n\nParameters\n----------\nnamespace_name : str\n    Namespace\nfile_path : str\n    Path name to search for a package-namespace ("}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\t# TODO: implement the method here\n\t\treturn True\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\t\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\t\"\"\"\n\t\t# TODO: remove\n\tm = re.match(r'^([a-z]+)-([0-9]+)\\.([0-9]+)$', file_path)\n\treturn True if m else Falseimport sys\nimport getopt\n\nsys.path.append(sys.path[0]+os.sep+\"..\")\nsys.path.append(sys.path[0]+os.sep+\"../\") # os.path.append(os.sep+\"../Utils/\")\n\nimport unittest\nimport wkc\nimport re\n\ndef parse_file(filename, separator=\" \"):"}]}
{"task_id": "62b463153879012d1948149c", "prompt": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t", "suggestions": [{"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t \n\tresult = []\n\n\tfor f in files:\n\t\tfor f2 in glob(f):\n\t\t\tif prefix == f2:\n\t\t\t\tfiles2 = prefix + f\n\t\t\t\tresult.append(files2)\n\t\t\t\tbreak\n\n\treturn result\n\n\ndef file_extension(f):\n\tfname, ext = os.path.splitext(f)\n\treturn ext.lower()\n\n\ndef _extract_file_extensions(extensions):\n\tout = []\n\tfor ext in extensions:\n\t\tout.append('.'.join([file_extension(f), ext]))\n\treturn"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\t# TODO: recherchez un index in op\u00e9modele erasier als espad\n\t\tfor f in files:\n\t\t\tif f.startswith(prefix):\n\t\t\t\treturn [f]\n\t# n = 1!!! \n\t# i = n + 1\n\t# while True:\t\n\t# \ti += 1\n\t# while True:\t\n\t# \tif files[i] == files[i + 1]:\n\t# \t\treturn files[:i]\n\t# \telse:\n\t# \t\tbreak;\n# \t\t"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\t#recherche il nom caratteri, ma interessante non sull'import\n\t\t#utilizzare perch\u00e9 non c'est la premuto il percorrente\n\t\t#passer l'indicato che viene utilizzato iniziale, che diversiamo anche se\n\t\t#poi nel file sarebbe con una lista in caso di ogni sono\n\t\t#i che verifico se \u00e8 pi\u00f9 vedere qualsiasi c'e'\n\t\tfiles = [name for"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t \n\t#TODO: check if not os.path.isdir(prefix)\n\tfiles = [f for f in files if prefix in f]\n\treturn files\n\ndef select_files_by_filename(file_name):\n\t\"\"\"\n\tGet files which belongs to a document package.\n\n\tParams:\n\tfile_name: str \n\tReturns:\n\tfiles paths which basename files matches to file name\t\"\"\"\n\t \n\tfiles = [f for f in os.listdir(file_name)]\n\treturn files\n\nif __name__=='__main__':\n\tfrom optparse"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\treturn glob.glob(prefix + \"*\")\n\t\t\n\t\t\ndef get_files_by_suffix(suffix):\n    \"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `suffix`\n\nParameters\n----------\nsuffix : str\n    Filename suffix\n\"\"\"\n        return glob.glob(prefix + suffix)\n\t\ndef get_subdirs_by_suffix(suffix):\n    \"\"\"\n\tGet files which belongs to a document package.\n\n\tParams:\n\n\t - suffix: str\n\n\tReturns:"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\n\t\t\n\tdef get_files_from_prefix(path):\n\t\t\"\"\"\n\t       @param path:\n\t\t   @return:\n\t\t   @rtype:\n\t\t   @warning Not tested\n\t\t\n\t       \"\"\"\n\t\t\n\t\t#filepaths = [os.path.join(get_app_path(), file) for file in os.listdir(path)]\n\t\treturn [file for file in file_paths if (file.startswith(prefix) and file.endswith(suffix))]\n\t\t\n\tfiles_list = list()\n\tprefix_list = list()\n\tfiles = get_files_from_prefix(prefix)\n"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\n    def matching_files(prefix):\n        return fnmatch.filter(files, prefix + \"*.py\") \n        \n        \n    files = [f for f in select_filenames_by_prefix(prefix, files)]\n#    matching_files(os.path.dirname(files))\n    \n    files = [f for f in files if os.path.splitext(f)[1] == '.py']\n        \n        \n    return files\n        \n        \ndef get_new_files(prefix, files):\t\n\t\"\"\"\n\tGet files which belong to a document package.\n\tAll files that ends in.py files are included"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\n        from os.path import splitext, basename, isdir, join, normpath, exists\n        from pkgutil import walk_packages\n        \n        def relpath(p, t=path_tail):\n                t = normpath(join(t, p)) if isdir(t) else t\n                return p.replace(t, \"\").lstrip(\"/\")\n                \n        files = [normpath(abspath(join(path, f))) for f in files]\n        results = []\n        for f in files:\n            p = relpath(f, prefix)\n            \t#print \"Searching for:\", f\n            if exists(join(prefix, p)):\n \t\t\t        #"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\tnames = []\n\t\tfor file in files:\n\t\t\tnames.append(file.replace(prefix, \"\").replace(\"<suffix>\", \"\"))\n\t\treturn names\n\n\ndef read_file(filename):\n\tif not filename.endswith(\".cja\"):\n\t\tfilename += \".cja\"\n\tfilepath = os.path.join(filename, filename)\n\ttry:\n\t\tfd = open(filepath)\n\t\tcontent = fd.read()\n\t\tfd.close()\n\t\treturn content\n\texcept IOError:\n\t\tprint \"Erro ao carregar ficheros\"\n\t\treturn"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\t\"\"\"\n\tGet files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix\t\"\"\"\n\t\t# Seleciona os arquivos cujos no mes dados \n\t\tfiles = map(str,files)\n\n\t\tif filename_re.match(prefix) is not None:\n\t\t\tfiles_in_dir = os.listdir(files)\n\n\t\telif prefix in files_in_dir:\n\t\t\treturn files\n\n\t# No mes dos casos\n\treturn []\n# -*- coding: utf-8 -*-\nimport datetime\nfrom south.db import db\nfrom south.v2 import SchemaMigration\nfrom django.db import models\n\n\nclass Migration(SchemaMigration):\n\n    def forwards"}]}
{"task_id": "62b463153879012d1948149d", "prompt": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\timport xml.etree.ElementTree as ET\n\t\twith open(folder + '/package_list.json') as data_file:\n\t\t    return(json.load(data_file))\n\ndef import_file(filename):\n\t\"\"\"\n\tImports packages.xml from a *.xml file.\n\n\tParameters\n----------\nfilename : str\n   File-path including the package files\nReturns\n-------\ndict       \"\"\"\n\timport xml.etree.ElementTree as ET\n\troot = ET.parse(filename)\n\treturn root.getroot()\n\ndef get_data(folder):\n\t\"\"\""}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\n\n\timport os.path as op\n\t\n\t\t\n\t#print(\"Folder: \", folder)\t\t\t\t\n\t\n\t#listFiles = os.listdir(op.abspath(folder))\n\tfiles = []\n\n\tdirs = []\n        for i in os.listdir(folder):\n\t    path = op.abspath(folder + i)\n\n            if path.startswith(\".\"):\n                continue\n\t    if path.endswith(\".xml\"):\n\t\tname = path[0:-len(\".xml\")]\n\t\tname = re.sub(r\"{[^}]*}\", \"\", name)\n\t    else:\n                continue\n\t    \n\t    path = re.sub(r"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\tif os.path.isfile(folder+'.pack'):\t\n\t\t\treturn json.load(file(folder).read())\n\t\telif os.path.isdir(folder):\t\t\n\t\t\treturn [os.path.join(folder, f) for f in os.listdir(folder)]\n\t\n\ndef _zip_folder(folder):\n\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\nlist\tstr\n\"\"\"\n\t\tif os.path."}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\tres = {}\n\t\tfolder_file = os.path.join(folder, \"repodata\")\t\t\n\t\tif os.path.isfile(folder_file):\n\t\t\twith open(settings.DATA_PATH(folder_file), \"rb\") as f_obj:\n\t\t\n\t\t\t\tdata = minidom.parse(f_obj).documentElement\n\t\t\t\tres = {key: data.attributes[key] for key in data.getElementsByTagName(\"name\")}\t\n\t\t\t\t\n\t\t\t\t\n\t\treturn res\n\t\t\t\n\traise RuntimeError(\"Unable to find package data in path: \" + folder)##\n# Copyright 2009-"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\n        with open(os.path.join(folder, 'packages.xml'), 'rb') as f:\n            xmldata = ElementTree.parse(f)\n            result = collections.defaultdict(collections.defaultdict)\n            for group in xmldata['groups']:\n                group = str(group)\n                #FIXME: use ElementTree to get more than one group, using a set. Also have to be done\n                for k, v in group.items():\n                    result[k]][k]=float(v)\n\t\treturn result\n\t\t\n\ndef _calculate_package_files(package_data, group_by='g_n"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\n\n\ttree = ElementTree()\n\ttree.parse(f)\n\n\troot = tree.find(\"packages\").find(\"package\").find(\"folder\")\n\t#print \"Found folder: \" + folder\n\n\t#print \"Get Packages folder: \" + root['id']\n\n\tpackages = []\n\tfor element in  root.iter(\"package\"):\n\t\tif len(element.findall(\"id\")) == 1:\n\t\t\tpackages.append( element.findall(\"id\")[0] )\n\n\treturn package_tools.get_metadata_dict(packages)\n\n# This file was created automatically"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\n    root_path = os.path.abspath(os.path.dirname(folder))\n    retdict = dict()\n    path_items = os.path.abspath(os.path.join(root_path, folder, '**/').lstrip('./').rsplit(os.sep,1))\n\n    if folder.endswith('/'):\n        folder += os.sep\n\n    for root, dirnames, filenames in os.walk(root_path.strip('/'), topdown=True):\n        for filename in filenames:\n            abs_filepath = os.path.abspath(os.path.join(root, filename)).replace('/', '.')"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\ttry:\n\t\t\treturn json.load(file(\"{}/{}.json\".format(cwd, \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t    folder)), \n\t\t\t\tobject_pairs_hook=OrderedDict)\n\t\texcept ValueError:\n\t\t\treturn {}\n\n#######################\n# Functions #\n#######################\n\ndef show_package_table(package_to_be_printed=\"packages\", path_to_package=None, data_dict=None, path_to_folder=\"\"):\n\n        \"\"\"\n\t\tThis function displays a list of lists of packages\n\n\t\tParameters\n\t\t----------\n\t\tpackage_to_be_printed : str\n\t\t"}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\n\n\tfor root, _dirs, files in os.walk(folder):\n\t\tname = root.split(os.sep)[-1]\n\t\tif _dirs:\n\t\t\tfiles.extend([os.path.join(root, name)] + _dirs)\n\n\t\t\n\n\n\treturn _dict_to_xml(dict_data = _dict_to_xml(\n\t\t\tname = name\n\t\t),\n\t\ttitle = folder + name + \".xml\",\n\t\tauthor = \"Jason Peltonin\",\n\t\tdescription = folder,\n\t\tauthor_email = \"jp-pas@gmail."}, {"generated_text": "def _explore_folder(folder):\t\"\"\"\n\tGet packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\t\"\"\"\n\t\t#folder\n\t\tpackage_data = dict()\n\t\tfolder = os.path.normpath(folder)\n\t\tfor dirpath, dirnames, filenames in os.walk(folder):\n\t\t    if 'xml' in filenames:\n\t\t        #check in data.xml and data.xml for each file, this is needed for the script to be called\n\t\t        #data.xml is one data file (folder is just 'xml' without folder extension)\n\t\t        #remove leading 'xml/' (it's '/') so that data.xml is of\n\t\t        package_file_path = os.path.join(dirpath,'xml"}]}
{"task_id": "62b463153879012d1948149f", "prompt": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\t# se tem ou n\u00e3o ficar nome do arquivo\n\t\tf = open(file_path, 'w')\n    \t# se ascribra o arquivo contendo \u00e9 porque n\u00e3o foi alterado colocado\n\t\tf.write(\"<?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?>\\n\")\n\t\t# procura o dicion\u00e1rio\n\t\t# verifica se o arquivo contendo cadastrado \u00e9 pela acessado\n\t\tf.write(\"<project>\\n\")"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\n        file_path = os.path.join(file_path, file_folder)\n\n\ttry:\n\t\twith open(file_path) as f:\n\t\t\t_xml_root = ET.parse(f)\n\n\t\t_tree = _xml_root.find('root')\n\n\texcept: pass\n\telse:\n\t\troot_tag = _tree.tag\n\t\tif root_tag!= 'project':\n\t\t\t\n\t\t\t#if not os.path.exists(file_path):\n\t\t\t#\treturn ''\n\t\t\t\n\t\t\treturn {'url' : os.path.abspath(file_path"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\t# obter as chaves da tabela com o arquivo, exemplo:\n\t\t# filename = \"remessar.chave.xml.no_arquivo.txt\"\n\t\t# # obter as chaves da tabela com o arquivo, exemplo:\n\t\t# # filename = \"remessar.chave.xml.no_arquivo.odsj.xml.no_corre.txt\"\n\t\t# filename = \"remessar.chave.xml\"\n\t# # print \"filename= \",filename\n"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\twith open(file_path, 'r') as x:\n\t\t\tres = parse(x.read())\n\t\t\tdata = res['asset']\t\n\treturn {'data':data}\n# -*- coding: utf-8 -*-\n\n############################################################################\n#\n#   Copyright (C) 2008-2015\n#    Christian Kohl\u00f6ffel\n#    Vinzenz Schulz\n#    Jean-Paul Schouwstra\n#\n#   This file is part of DXF2GCODE.\n#\n#   DXF2GCODE is free"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\n\n\ttry:\n\t\txml = etree.parse(file_path)\n\texcept Exception as e:\n\t\treturn {\"__error__\": \"{}\".format(e)}\n\n\tpath_name = '{0}_{1}_{2}'.format(prefix, filename, file_folder)\n\t# path_name = 'file'\n\t# path_name ='result.txt'\n\txml.find('FileID').text = filename\n\n\tresult = {}\n\t# i = 1\n\tfor xml_element in xml:\n\t\ti+=1\n\n\t\t# root element\n\t\tresult_element"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\t# TODO : implementar e de momento de carregada, no fim de forma\n\t\twith open(file_path, \"rb\") as i:\n\t\t\ts = s.read()\n\t\treturn pickle.loads(s)\n\ndef add_asset(asset, filename, remove=True, keep=False):\t#TODO: generar em um arquivo\n\tif remove:\n\t\tos.remove(os.path.abspath(filename))\n\telse:\n\t\tfile_path = os.path.abspath(os.path.dirname(os.path."}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\twith open('pacotes/archivos/' + prefix + filename + '.xml', 'rb') as archivos:\n\t\t\tarchivos_xml = archivos.read()\n\n\t\treturn eval(archivos_xml.decode())\n\n\texcept IOError:\n\t\tos.remove(file_path)\n\ndef get_preco_version_info(product=None):\n\t\"\"\"\n\tObtem os pacotes dispon\u00edveis \u00e0 partir de pacotes das \n\tdocentes, dispon\u00edveis endere\u00e7os"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\noutput_dir = \"./dados\"\n\n\ndef load_data(file_path, prefix=None, file_folder=None):\n\tfile_content = list()\n\tfile_content.append(\"<?xml version='1.0'?>\\n\")\n\tfile_content.append(\"<arquivo_list>/\\n\")\n\tlines_iter = 0\n\t\n\twith open(file_path,'r') as F_TSV_I1:\t\t\n\t\tfor line in F_TSV_I1:\n\t\t\tline = line.strip() \n\t\t\tif line"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\tpath_str = base+'/'+prefix+'.xml'\n\n\t\tpath_xml = ET.parse(path_str)\n\t\tresult = {}\n\t\tif path_xml.tag == 'packages':\n\t\t\tfor item_xml in path_xml:\n\t\t\t\tresult[item_xml.attrib['title']] =  item_xml\n\t\telse:\n\t\t\tfor item_xml in path_xml:\n\t\t\t\tif item_xml.tag == 'filename':\n\t\t\t\t\tfilepath = base+str(file_path)+str(item_xml.attrib['label'])+'.txt'\n\t\t\t\telse:\n\t\t\t\t"}, {"generated_text": "def _eval_file(prefix, file_path):\t\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict\t\"\"\"\n\t\n\n\tarchivo_file_name=\"\"\n\tarchivo_folder=\"\"\n\t#parseando o formato json\n\tf = open(file_path)\n\tj = json.load(f)\n\tf.close()\t\n\tfor i in j:\n\t\tif i[\"tipo\"]=='archivo: ':\n\t\t\tarchivo_file_name = i['value']\n\t\tif i[\"tipo\"]=='archivo/archivo.' + archivo_file_name[len(archivo_folder) + 1:len("}]}
{"task_id": "62b463153879012d194814a1", "prompt": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\trendition_template_dict = defaultdict(lambda: \"\")\n\t\tlanguage_dict = self.language.all()\n\t\tfor lang in language_dict:\n\t\t\tlanguage_dict[lang.code] = lang\n\n\t\tfile_template_dict = defaultdict(dict)\n\t\tfor template in file_path:\n\t\t\tfor key, val in template_dict.items():\n\t\t\t\tfile_template_dict[key][lang] = val.language_name\n\t\t\t\t\t\t\n\t\tfile_template_dict['en'] = File(file_path, file_path, lang=lang, \n\t\t                             origin=\""}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tlang = lang.replace('\"', '')\n\t\tif lang not in self._dict: return []\n\n\t\tself._dict[self.find_file(language, src=\"\", exts=['.pdf'])] = [file_path, self.find_file(lang, src=\"\", exts=['.pdf'])]\n\n\t\tfile_ext = self.find_file(file_path, content_types=[\".pdf\", \"\"], doctype=\"a\")\n\t\tself._dict[self.find_file(file_ext, src=\"\", exts=['.pdf'])].append(file_path)\n\n\t\treturn self._dict"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tself.file_path = file_path\n\t\tself.lang = lang\n\t\tself.load_file()\n\t\t\nif __name__ == '__main__' :\n\tm = MapnikManager()\n\tm.load('osm.m')\n\t\n\tfor w in m.all():\n\t\tprint w\n\t\n################################################################################\n#\n# This program is part of the HPMon Zenpack for Zenoss.\n# Copyright (C) 2008, 2009, 2010, 2011 Egor Puzanov.\n#\n# This program can be used under the GNU General Public License version 2"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\n# \tself._open_doc(os.path.join(self.src_dir, lang, file_path))\n\tself._open_doc(os.path.join(self.src_dir, \"source/\"+file_path))\n\tself.lang.add_document(doc_id=self.src_dir, file_path=file_path)\n# \tself.lang.save(doc_id=self.src_dir,  doc_type=\"en\",  file_path=file_path) \n        \n    def write_pdf(self,"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tif not lang:\n\t\t\traise Exception(\"Missing lang\")\n\t\tif lang in self.langs:\n\t\t\traise Exception(\"LANG: \" + lang)\n\t\t\t\n\t\twith codecs.open(file_path, \"r\", \"utf-8\") as fl:\n\t\t\tfor line in fl.readlines():\n\t\t\t\tmatches = self.RE.findall(line)\n\t\t\t\tfor m in matches:\n\t\t\t\t\tn = m.group(0)\n\t\t\t\t\tp = m.group(1)\n\t\t\t\t\tif not p: p = 'r.'+n\n\t\t\t\t\t\tif not p in self.langs: p"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tcontent = self.get_translation(file_path=file_path)\n\n\t\t# If file already exists, replace \n\t\tif os.path.exists(file_path):\n\t\t\tos.remove(file_path)\n\n\t\tfor key, value in content.items():\n\t\t\tif key!= \"original\":\n\t\t\t\tcontent[key] = value + \"\\n\"\n\n\t\tcontent = self.compress_content(content)\n\n\t\tself.save_translation(content=content, file_path=file_path)# -*- coding: utf-8 -*-\nfrom __future__ import unicode"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tprint('Adding rendition of'+ lang +'at'+ file_path)\n\n\t\t# Read the contents and write it to a gzip file (or raise an error)\n\t\twith open(file_path, \"rb\") as fin, open_zip(file_path) as fout:\n\t\t\twith gzip.open(fout, 'rb') as gz:\n\t\t\t\ttext = gz.read()\n\t\t\ttext = text.decode(\"base64\")\n\t\t\t# Remove BOM from text\n\t\t\ttext = gz.readall().decode(\"utf-8\").strip()\n\t\t\tself"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tfile_path = self.copy_file(file_path, 'original')\n\t\tlanguage = os.path.splitext(file_path)[1].lower()\n\t\t\n\t\tif not os.path.isfile(file_path):\n\t\t\treturn -1\n\n\t\tif language in ROT_LANGUAGES:\n\t\t\tlanguage = ROT_LANGUAGES[language]\n\t\tfile_path += 'en.pdf'\n\t\t\n\t\ttry:\n\t\t\tfile_path = self.copy_file(file_path, 'en.pdf')\n\t\t\treturn 0\n\t\texcept:\n\t\t\treturn -2\n\n\t\treturn"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tfile_object = open(file_path, 'rb')\n\t\tcontent = file_object.read() \n\t\tfile_object.close()\n\t\tsoup = BeautifulSoup(content)\n\n\t\tfor i in soup.findAll('h3'):\n\t\t\tfile_object = open('artigo_i.pdf', 'r', encoding=\"utf-8\")\n\t\t\tfile_string = StringIO.StringIO(str(i))\n\t\t\tfile_object.close()\n\t\t\t# print i\n\t\t\tfilename = file_path + '-' + lang + '-en.pdf'\n\t\t\twith open(filename,"}, {"generated_text": "def add_rendition(self, lang, file_path):\t\"\"\"\n\t{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}\t\"\"\"\n\t\tfor lang in Language.objects.get(id = lang):\n\t\t\twith open(file_path, 'r') as f:\n\t\t\t\ttemp = f.read()\t\t\t\n\t\t\t\ttemplate = Template(temp)\n\t\t\t\ts_tpl_file = Template(temp, searchList=[Template()])\n\t\t\t\ts_tpl_file.render(Template(lang.name).render(s_tpl_file, params) )\n\t\t\t\tos.rename(file_path, file_path + \".en.pdf\")\t\n\t\t\t\tos.rename(file_path, file_path + \".mdh."}]}
{"task_id": "62b463163879012d194814a2", "prompt": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tself.name += basename\n\t\tself.img = img\n\t\tself.file_path = file_path\n\t\t\n\t\t_img_data = io.ReadFile(self.file_path)\n\t\t\n\t\t# read the first page\n\t\tself.page_start = _img_data.find(\"<<head>\")\n\t\tself.page_start += 1 if self.page_start < 20 else 0;\n\n\t\tself.html_img = _img_data[:self.page_start]\n\t\tself.img_path = os.path.splitext(basename)[0].replace(\".png\", \"."}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tif basename.endswith(\"_artigo\"):\t\t\t\n\t\t\tasset_id = basename.rsplit(\"_artigo/\", 1)[1]\n\t\t\tasset_id = re.sub(r'[^a-zA-Z0-9]+', '_', asset_id)\t\t\t\n\t\t\tasset_id = asset_id.split(\"/\")\n\t\t\tasset_id = asset_id[1 : -1]\n\t\t\tasset_id = \" \".join(asset_id)\n\t\t\t\t\t\t\t\t\t\n\t\t\tfilepath = os.path.join(self.asset_folder, asset_id + \"_\"+ asset_id + \".tiff\")"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tasset_path = os.path.join(basename, file_path)\n\t\tprint(\"ADD \" + basename + \" \" + file_path)\n\t\tos.chdir(os.path.join(basename, file_path))\n\t\tfiles = []\n\t\tfor root, dirs, names in os.walk(asset_path):\n\t\t\tfor name in names:\n\t\t\t\tif name.endswith(\".jpg\"):\n\t\t\t\t\tfiles.append(os.path.join(root, name))\n\t\tfor f in files:\n\t\t\tif not f.endswith(\".jpg\"):\n\t\t\t\tf += \".jpg"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tasset_name = \"\"\n\t\tfor x, y in _material_properties.items():\n\t\t\tif x == 'name':\n\t\t\t\tif volume not in self._assets:\n\t\t\t\t\traise TypeError(volume)\n\n\t\t\telif x in _material_properties:\t\n\t\t\t\tval = self._assets[x]\n\t\t\telif x == 'properties':\n\t\t\t\tval = self._material_properties[x]\n\t\t\telif x == 'title':\n\t\t\t\tval = self._material_properties['title']\n\n\t\t\tif not os.path.isfile(file_path):\n\t\t\t\tprint filename, \": No such"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tprint basename +'->'+ file_path\n\n\t\tprint 'Writing asset:'+ basename\n\t\tself.file = open(file_path, 'w', encoding='utf-8')\n\t\tself.file.write(basename + '\\n' + file_path)\n\t\tself.file.close()\n\n\tdef add_metadata(self, basename, file_path):\n\t\t\"\"\"\n\t\t\"{\n        \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n        \"artigo02-gf03.jpg\": \"/path"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tself.set_path(basename, file_path)\n\n\t\tself.add_file(basename=basename, path=file_path)\n\t\tself.set_path(\"artigo{:04}/{:04}\".format(basename.replace(\"/path/\", \"\")[-8:], file_path[-8:]))\n\n\tdef export(self, target_path, format='jpg', format_type='png'):\n\t\t\"\"\"\n\t\tThis method is a wrapper around the export utility to make it the way\n\t\tthe file is serialized\n\n        Args:\n            target_path (str) : the target path or path"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tself.items[basename] = file_path\n\t\tself.file_path = file_path\n\t\n\t#\n\t# add a default item\n\t#\n\tdef add_default_item(self, item):\n\t\tself.items[item.dirname] = item\n\t\n\t#\n\t# add multiple files\n\t#\n\tdef add_files(self, filepaths):\t\n\t\tfor filepath in filepaths:\n\t\t\tself.add_asset(filepath, filepath)\t\n\n\t#\n\t# clear the object\n\t#\n\tdef clear(self):\n\t\tf"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tpath = pathlib.Path(file_path)\n\t\tif not path.exists():\n\t\t\tpath = pathlib.Path.cwd()\n\n\t\t\tassert path.exists()\n\t\t\tpath.mkdir(mode=0o777)\n\n\t\t\tself.add_asset_path(basename, path)\n\t\t\tself.__all_files.append(basename)\n\n\tdef add_asset_path(self, path, basename):\n\t\tfile_path = os.path.join(path, filename)\n\t\tself.__assets.add_path(path, basename)\n\n\tdef add_asset_path_"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tif file_path:\n\t\t\tprint \"* %s\" % basename, file_path\n\t\t\tf = open(file_path, 'rb')\n\t\t\tfile_name = basename.decode('utf-8')\n\t\t\tif file_name[-3:] == '.JPG':\n\t\t\t\t# remove file extension\n\t\t\t\tfile_name = file_name[:-3]\n\t\t\tfor line in f:\n\t\t\t\tif \"\\\\\" in line:\n\t\t\t\t\tline = line.translate(None,\"[\").replace(\"\\\\\",\"/\")\n\t\t\t\tline = line.strip()\n\t\t\t\tif len(line.split('\\t'))"}, {"generated_text": "def add_asset(self, basename, file_path):\t\"\"\"\n\t\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}\t\"\"\"\n\t\tself.files.add_file(basename, file_path)\n\t\tself.files.add_file(basename, file_path, self)\n\n\tdef get_file(self, basename):\n\t\tself.files.add_file(basename)\n\t\t\n    def get_file_path(self, basename):\n\t\tself.append_file(basename)\n\t\t\n    def get_file_path2(self, basename):\n\t\tself.append_file(basename)\n\t\t\n    def append_file(self, basename, file_path):\n        if not self.files.has_file"}]}
{"task_id": "62b463163879012d194814a4", "prompt": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\n\t\t\t\ndef exploreZip(package_path):\n\t\"\"\"\n\tFind package data in given zip archive\n\t\"\"\"\n\tzip_filepath = os.path.join(os.path.expanduser('~'),package_path)\n\tif \".zip\" in zip_filepath:\n\t\tzip_filepath = zip_filepath.replace(\".zip\",\".csv\")\n\tzip_file = zipfile.ZipFile(zip_filepath)\n\tpackage_file = zip_file.open(package_path)\n\tdata = package_file.read(1)\n\treturn data\n\ndef explore"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\t#Open unzipped file\n\twith zipfile.ZipFile(zip_path,'r') as myzip:\n\t\t#Get packages.xml file\n\t\tmyzip.extractall()\n\t\tpackages_tree = ET.parse(os.path.join(zip_path, 'packages.xml'), parser=ET.XMLParser(ns_clean=True))\n\t\tpackages_root = packages_tree.getroot()\n\t\tpackages_data = {}\n\t\tfor package in packages_root.iter('package'):\n\t\t\t\tname = package.get('name')\n\t\t\t\tversion = package.get('version"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\n\n\t\t# parse xml\n\t\txml = minidom.parse(zip_path)\n\n\t\tdict_content = xml.getElementsByTagName(\"items\")\t\t\t\t\t\t\t\t\t\n\t\tdict_data = {};\n\n\t\t# grab package name and version\n\t\tpackage_tag = dict_content.item(0).toxml().replace('manifest:','')\n\t\tpackage_match = package_tag.find('packageInfo').text\n\t\tversion_match = version_tag.find('version').text\n\t\tdict_data[\"package_id\"] = str(package_match).strip()\n\n\t\t# grab version name\n\t\tversion_"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\ndef _get_xml_info(zip_path):\t\n\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\ndef _is_xml_file(file_path):\n\t\"\"\"\n\tCheck if the file at file_path looks like XML or json. If so return True else return false\n\nParameters\n----------\nfile_path : str\n    file path\nReturns\n-------\nTrue | False\n"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\n\n\t\n\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Dec 28 19:30:19 2014\n\n@author: camacho\n\"\"\"\nimport pytest\nfrom sfepy import data_dir\n\ndatadir = data_dir + '/pysisyphus_examples/'\n\ndata = list()\nexample = list()\nsfepy = list()\n\nfor subdir, dirs, files in os.walk(datadir):\n    if subdir == 'examples':\n        example.append(subdir)\n        sfepy.append([subdir, [os.path.join("}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\t# Check zipfile.is_zipfile()\n\t\tif zipfile.is_zipfile(zip_path):\n\t\t\ttry:\n\t\t\t\treturn get_package_xml(zip_path)\n\t\t\texcept:\n\t\t\t\t#raise Exception\n\t\t\t\t#pass\n\t\telse:\n\t\t\t#raise Exception\n\t\t\traise Exception(\"Not a zip file\")\n\n############################################################# main #####\n\n# Check install path and find files for installed version. If not, exit.\nif not params.install_path:\n\tpackage_xml_path = _explore_zipfile(params.install_path)\nelse:\n"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\treturn dict([\n\t\t\t('basename', 'folder/subfolder/path', 'file name'),\n\t\t\t('file_size', 'folder/subfolder/path', 'file size'),\n\n\t\t\t('path', 'folder/subfolder/path','subfolder file name')\n\t\t])\n\ndef _parse_zip_dir(zip_path):\n    \"\"\"\n\tGet zip files, in specified order\n\nGroups files by their XML basename & returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n        zip_"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\n        with zipfile.ZipFile(zip_path) as zf:\n\t\t\tfor info in zf.infolist():\n\t\t\t\tdinfo=info.filename\n\treturn {x['basename']:x for x in dinfo}\n\ndef _get_zipper_data(zip_path):\n        \"\"\"return json object with data from zip_path\"\"\"\n        zfile = zipfile.ZipFile(zip_path, 'r')\n\ttxtdata={}\n\tfor zfile in zfile.infolist():\n\t\ta = zfile.read(zfile.namelist()[0])\n\t\ttxtdata[a["}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\txml_filename = os.path.basename(zip_path)\n\t\tif xml_filename.split('.')[-1] in ('xml', 'pickle', 'json'):\n\t\t\tf = open(zip_path, 'rb')\n\t\t\tdata = f.read()\n\t\telif xml_filename.split('.')[-1] == 'txt':\n\t\t\tdata = open(zip_path, 'r').read()\n\t\telse:\n\t\t\traise ValueError, 'Invalid xml file format %s' % xml_filename\n\t\ttry:\n\t\t\txml = ET.fromstring(data)\n\t\t\tf.close()"}, {"generated_text": "def _explore_zipfile(zip_path):\t\"\"\"\n\tGet packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\t\"\"\"\n\t\tzip_file = zipfile.ZipFile(zip_path)\n\t\tfile_list = zip.namelist()\n\t\tfile_list = [f for f in file_list if 'xml'!= f ]\n\t\tfile_list[0] = \"src/packages\"\n\t\tfile_list.remove('xml')\n\t\tfile_list[0] = \"src/packages\"\n\t\treturn file_list\n\n\telif sys.version_info < (2,7,\"3\"):\n\t\t_zip_directory(zip_path)\n\n\telse:\n\t\traise NotImplementedError(\"Need to be implemented"}]}
{"task_id": "62b463163879012d194814a6", "prompt": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\n\n\treturn [os.path.join(fdir, filename)\n\t\t\tfor fdir, dirnames, filenames in os.walk(zip_path)\n\t\t\tfor fdir in fnmatch.filter(filenames, '*.pdf')]\n\n\n\n\nif __name__ == '__main__': main()\n# Copyright 2017 Mycroft AI Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\tpandoc.utils.download(\"http://stackoverflow.com/a/40215/3776677/118463\", \"./zip_f1.zip\")\n\n\tfile_list = set(files_list_from_zipfile(zip_path)) #get set\n\tprint(len(file_list),\":\",len(file_list))\n\tt_set = set(file_list) \n\treturn file_list\n\ndef files_list_from_dir(path):\n\t\"\"\"\n\tReturn the files in `path`\n\nExample:\n```\n[["}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\twith zipfile.ZipFile(zip_path, 'r') as z:\n\t\t\tfiles = set(z.namelist())\n\t\t\treturn files\n\n\ndef main(argv):\n\tstart_dir = argv[1]\n\t# get the input zip,\n\t# load the list of files and create a dictionary\n\t#  for each file that contains a list of (name, file path)\n\t# with the appropriate extension\n\tfilelist = [\n\t\t('/Users/alex/code/python.zip', ['2318_0889-tinf-33-0421/2318-"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\tfrom zipfile import ZipFile\n\t\tf = ZipFile(zip_path)\n\t\tfiles = f.namelist()\n\t\t#files = glob.glob('2204-*.zip')[0]\n\t\tfiles.sort()\n\t#files = ['2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml']\n\treturn files\n\n\ndef parse_file(f, name):\n\t\"\"\"\n\tParse a name to return the value of the given file\n\t\"\"\"\n\tm = re.search('[^_]*-"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\n\n\t\twith zipfile.ZipFile(zip_path) as zip_file:\n\t\t\tfiles = zip_file.namelist()\n\t\t\treturn files\n\t\t\n\tdef download_dir(self, file_id):\n\n\t\tprint('Downloading file {}'.format(file_id))\n\t\t#url = (\"http://thelatinlibrary.appspot.com/file/{}\"\n\t\t#       .format(file_id))\n\n\t\t#http_url = url.format(file_id)\n\n\t\tfiles = self.download_files_from_url(url=http_url)\n\t\tif files:"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\twith zipfile.ZipFile(zip_path, 'r') as myzip:\n\t\t\tfiles = []\n\t\t\tfor f in myzip.filelist:\n\t\t\t\tfiles.append(f)\n\t\t\treturn files\n\texcept IOError:\n\t\tprint('Error reading file: ', zip_path)\n\t\treturn None\n\ndef lines_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the file contents of `zip_path` which contains a series of unicode lines.\n\t\"\"\"\n\twith zipfile.ZipFile(zip_path, 'r') as myzips:\n\t\tfile_"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\tfilesList = [ f.split('.')[0] for f in zip_path.glob('*.zip') ]\n\telse:\n\t\tfilesList = os.listdir('*.zip')\n\n\treturn filesList\n\n\ndef remove_empty_lines(filein, strip=True):\n\tr\"\"\"\tRemove every empty line from a file\n\tIf `strip`, add lines that end with `'\\n'``\n\t\"\"\"\n\tif strip:\n\t\t# remove empty lines\n\t\tempty_lines = []\n\n\t\tfor line in filein:\n\t\t\tif isinstance(line, str):\n\t\t\t\t"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\tfor f in files:\n\t\t\tfilename = os.path.basename(f)\n\t\t\tif filename == \".\":\n\t\t\t\tcontinue\n\t\t\t\t#print filename\n\t\t\ta = os.path.join(zip_path, filename)\n\t\t\tif splitext(a)[-1] == \".zip\":\n\t\t\t\tprint \"Zipping\", a\n\t\t\t\tos.mkdir(os.path.split(os.path.split(a)[-1])[0])\n\t\t\t\tos.chdir(os.path.split(a)[-1])\n\t\t\t\tzipf = zipfile.ZipFile(a, \"r\")\n\t\t\t\tos"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\tpass\n\n\tdef files_extract(zip_path):\n\t\t'''\n\t\tExtracts a single file \n\t\tThe file should be formatted with the following structure: \n\t\tfolder_index, folder_name, extension\n\t\tfile_name, file_extension, outputfile_name, file_folder, \n\t\textension_file, extension_outputfile'''\n\t\tpass\n\n\tdef extract(zip_path):\n\t\t'''\n\t\tExtracts a single file \n\t\tThe file should be formatted with the following structure: \n\t\tcategory, file_name, file_extension, outputfile_name, \n\t"}, {"generated_text": "def files_list_from_zipfile(zip_path):\t\"\"\"\n\tReturn the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```\t\"\"\"\n\t\twith open(zip_path, 'r') as f_in:\n\t\t\tlines = f_in.readlines()\n\t\t\tzip_list = [[] for _ in range(len(lines))]\n\t\t\tfor line in lines:\n\t\t\t\tzip_list[0].extend(line.strip().split('\\t'))\n\t\t\t\tzip_list[1].extend(line.strip().split('\\t'))\n\t\treturn zip_list\n\t#\n\t# @return: <list>\n\t#\t<path>\n\t#\t\t<time_now_str>\n\t#\t\t"}]}
{"task_id": "62b4631b3879012d194814dd", "prompt": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\treturn re.sub(r'__w:st\\s*\\[\\d\\w-]+', lambda mo: mo.group(0).lower(), content)\n\tif re.search('xmlns\\s*=\\s*', content):\n\t\treturn re.sub(r'xmlns\\s*=\\s*', 'xmlns=\"[^\"]+\" ', content)\n\tif re.search('<', content):\n\t\treturn re.sub(r'<', '<', content)\n\treturn re.sub(r' ', lambda mo: mo.group(0).lower(), content)\n\n@conf\ndef validate_indent(conf"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\treturn re.sub('\"','w\"',content)\n\tdef fix_namespace_prefix(content):\t\"\"\"\n\tReplace http://www.w3schools.com ->  w3schools.com\n\t\"\"\"\n\treturn re.sub(\":\\/\\/w:\",\"\", content)\t\n\t#print \"fix_namespace_prefix \"+str(url)\n\t#print \"fix_namespace_prefix_w \"+str(url)\n\n\t#print \"fix_namespace_prefix_w \"+str(url)\n\t#print \"hacked\"\n\t#return url"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tp_line = \"^(?P<prefix>%s)\" % re.escape(namespace_prefix)\n\t\tp_name = r\"^(?P<name>%s)\" % re.escape(namespace_prefix)\n\n\t\tmatch = r\"%s*=(\\w+)%s(?P<val>%s)$\" % (p_name, p_line, p_name)\n\t\tm = re.match(match, content)\n\t\tif m:\n\t\t\tprefix, name, val = m.groups()\n\t\t\tval = val.replace(namespace_prefix, '')\n\t\t\tcontent ="}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\treturn content.replace('w:', 'w:')\n\tif'' in content:\n\t\treturn content.replace(' ', '-').replace(' ', '-')\n\telse:\n\t\treturn content\n\t\t\ndef fix_namespace_prefix(content):\n\t\"\"\"Fix text that does not have 'w','st' or 't'\"\"\"\n\t\n\treturn content.replace(' ', '-')\n\t\ndef format_header_for_c(key):\n\t\"\"\"Converts a CamelCase header key to camelcase, with no spaces between them.\n\t@param key: a CamelCase header key in Camel"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tif 'w-st=\"' not in content:\n\t\t\tcontent = '\"' + content + '\"'\n\n\tfor line in content.splitlines() :\n\t\tif line.find(w_prefix_w) == 0 :\n\t\t\tprint(line)\n\n\tcontent = content.replace(w_prefix_w, w_prefix_w)\n\n\t# print('')\n\t# print(content)\n\n\n\t#print(\"fixing namespace...\")\n\tfor line in content.splitlines() :\n\t\tif line.find(w_prefix) == 0 :\n\t\t\tprint('line ', line)\n\n\t#"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\ttry:\n\t\t\tcontent = content.encode(encoding)\n\t\texcept UnicodeEncodeError:\n\t\t\tpass\n\n\tdef get_namespace_prefix(self, content):\n\t\tnamespace_uri = get_namespace_uri(context=content, **self.known_content)\n\t\treturn namespace_uri\n\n\n\tdef set_language(self, lang):\n\t\t\"\"\"Set user's preferred site language.\"\"\"\n\t\tif lang == 'auto':\n\t\t\treturn self.language\n\t\tif lang!= 'en' and lang!= 'it':\n\t\t\traise AttributeError(\"Preferred site language '%s' does not exist\" %"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tprefixes = content_utils.text_string_to_list(prefixes_of_text(content))\n\t\tcontent = re.sub(r'(^|\\s)(w|st:)\\{', r'\\1', content)\n\t\tcontent = re.sub(r'(^|\\s)(w|st:)\\{(\\w+)\\}', r'\\1<w>', content)\n\t\tcontent = re.sub(r'(^|\\s)(w|st:)([\\w-]+)', r'\\1</w>', content)\n\t\tcontent = content.replace('<w>', 'w:st=\"' +"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\n\n\tw_ns = re.split(r'xmlns:(\\w+)[\\w:st]\\,[\\'\"]?', content, re.I).pop(0)\n\tw_prefix = 'xmlns:w=\"urn:schemas-microsoft-com:bindings:xml:ns:netman-reference\"'\n\treturn re.sub(w_ns, '<w:st=\"' + w_prefix + '\">', content)\n\t\t#!/usr/bin/python\n# -*- coding: utf-8 -*-\nimport sqlite3, os\n\nclass FileDB(object):\n    def __init"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\t# FIXME: currently hardwired to not have problems with \"w_:st::\"\n\t\tw_st = content.replace('w:st=\"', 'w:st=\"' )\n\t\tif w_st.startswith('\"'):\n\t\t\t# strip spaces and strip whitespaces from each line:\n\t\t\tdata = content.lower().replace(';', ';').replace('\\'','')\n\t\telse:\n\t\t\t# try to guess xml: encoding type\n\t\t\t# and strip whitespace and quote inside quotes:\n\t\t\tdata = content.replace('&quot;', '\"').replace('\\'', '\\\\\"')\n\t\t# FIXME: handle"}, {"generated_text": "def fix_namespace_prefix_w(content):\t\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\t\"\"\"\n\t\tcontent = content.replace('\"', '\\\\\"')\n\t\tcontent = content.replace('\\n', '\\\\N ')\n\t\tcontent = content.replace('\"', '\\\\\"')\n\n\t\t#print '%s %s %s'%(wname, content,wsuffix)\n\t\tif len(wname) > 0 and wname[-1] == 'w':\n\t\t\twname += wsuffix\n\n\t\t# add backspaces to word\n\t\tif wname[0] == 'w':\n\t\t\twname += wspace\n\t\telse:\n\t\t\twname = wname[1:]\n\n\t\treturn wname\n\texcept Exception"}]}
{"task_id": "62b463283879012d1948153d", "prompt": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tif node.hasAttribute('dc:date'):\n\t\t\ttext = node.getAttribute('dc:date').strip()\n\t\t\tif not text:\n\t\t\t\treturn None, [None]\n\t\t\tmatch = pubdate_regex.search(text)\n\t\t\tif match:\n\t\t\t\tif match.group(4):\n\t\t\t\t\treturn  str(match.group(4)), None\n\t\t\t\tmatch = pubdate_regex.search(text)\n\t\t\t\tprint \"match\",match.group(0)\n\t\t\t\treturn  match.group(0), match.group(1)#, match.group(2), ''"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tif not len(node):\n\t\t\treturn False\n\t\tif pubdate_xpaths[-1] is None:\n\t\t\treturn False\n\t\treturn node[-2] == pubdate_xpaths[-2]\n\tdef get_pubdate_root(node):\n\t\txroot = node.get(XOCAL_ROOT, None)\n\t\tif xroot:\n\t\t\treturn xroot[-2]\n\t\t#else try to look in <...>\n\t\tfor parent in node.get(XOCAL_ROOT, ()):\n\t\t\ttroot = parent.get(XOCAL_"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\ttry:\n\t\t\t\t(pubdate_id, pubdate) = node.get('pubdate').split(\" \")\n\t\texcept (KeyError, ValueError):\n\t\t\treturn None\n\t\tif not pubdate_id:\n\t\t\treturn None\n\n\t\tpubdate_path = '%s/guid/text()' % self.xpath(pubdate_xpaths[0])\n\t\tpubdate = pubdate_id if pubdate_id in pubdate_path else None\n\n\t\treturn pubdate\n\n\tdef check_if_node(self, url):\n\t\t\"\"\"\n\t\tChecks if node is in the feed and returns None if it"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tfor x in pubdate_xpaths:\n\t\t\tif x.getAttribute(\"pubDate\") == node.getAttribute(\"pubDate\"):\n\t\t\t\treturn True\t\t\n\t\treturn False\n\t\t\t\t\t\n\tdef match_author(node):\n\t\t\"\"\"\n\tReturns a list of tuples with name=value pairs that specify where\n\tto go in authoring-info to generate the match.\t\n\tExample:\n\tmatch_pubdate(\"Bob Barker\", ((\"D\", \"a\"), (\"E\", \"e\")))\n\tmatch_pubdate(\"Bob Randis\", ((\"A\", \"e\"), (\"C\", \""}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tidx = node.find('pubDate')\n\t\tif idx!= -1:\n\t\t\treturn node[idx + len('pubDate'):idx].strip()\n\t\telse:\n\t\t\treturn \"\"\n\n\t# if we get here, we get a match for the author_sort_key\n\t# it would be in the Author Sort-key\n\t# but if we get here, it'd be:\n\t# [\n\t#  (\"author\",\"name\", \"name\"),\n\t#  (\"date\",\"name\", \"name\"),\n\t#  (\"author\",\"name\", \"name\"),\n\n\t"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\treturn [ m for m in pubdate_xpaths if re.match(regx, m)]\n\n\tdef node_has_matched_pubdate(self, node_or_node):\n\t\tif isinstance(node_or_node, int):\n\t\t\treturn node_or_node in self.id2doc\n\t\telif isinstance(node_or_node, slice):\n\t\t\treturn node_or_node in self.id2doc.get(node_or_node, ())\n\t\telse:\n\t\t\tassert False\n\n\tdef __iter__(self):\n\t\tfor f in self"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tfor key in pubdate_xpaths:\n\t\t\tif node.getAttribute(key) == pubdate_xpaths[0]:\n\t\t\t\treturn key\n\t\treturn None\n\n\nregister_extension(get_feed_url, '.feed', 'application/atom+xml')\n\n# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2015 Micronaet srl (<http://www.micronaet.it>)\n#\n#    This program is free software: you can redistribute it and/or modify\n"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tif node['pubdate'][0] in pubdate_xpaths:\n\t\t\treturn node['pubdate'][0]\n\t\telse:\n\t\t\tfor node1 in node.xpath('node'):\n\t\t\t\tif node1.text:\n\t\t\t\t\treturn node1.text\n\t\treturn ''\n\n\tdef match_pubdate_list(node, pubdate_xpaths):\n\t\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tif len(pubdate_xpaths) < node.num_dates():\n\t\t\tfor node1 in node.xpath('node'):\n\t\t\t\tif"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tfor node in node:\n\t\t\tif node.tag == 'pubDate':\n\t\t\t\treturn node \n\t\tif pubdate_xpaths:\n\t\t\treturn pubdate_xpaths[0]\n\t\telse:\n\t\t\treturn None\n\n\tdef add_pubdate_links(self, link_map):\n\t\tif len(link_map) == 0:\n\t\t\treturn\n\t\tlink_list = []\n\t\tfor link in link_map:\n\t\t\tfor match in link.split(' '):\n\t\t\t\tif match:\n\t\t\t\t\tlink_list.append(match)\n\t\t\t\telse:\n\t\t\t\t\tlink_"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\t\"\"\"\n\tReturns the first match in the pubdate_xpaths list\t\"\"\"\n\t\tname = node['node_type']\n\t\t\n\t\tfor pubdate_path in pubdate_xpaths:\n\t\t\tif name == pubdate_path['name']:\n\t\t\t\treturn pubdate_path\n\t\n\tif len(pubdate_xpaths) == 0:\n\t\treturn None\n\t\n\tpubdate_node = pubdate_nodes[pubdate_xpaths[0]]\n\treturn match_pubdate(pubdate_node, pubdate_xpaths)\n\t\t\t\n\t\ndef match_title(node, title_xpaths):\t\"\"\"\n\tReturns the first match in the title_xpaths list"}]}
{"task_id": "62b463303879012d19481579", "prompt": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tissue_number = ''\n\t\tsuppl_number = ''\n\t\tvolume_label = ''\n\t\tnumber = ''\n\t\tnumber_seperator = ''\n\n\t\tfor p in issue:\n\t\t\tif p.article_series_info and \"volume\" in p.article_series_info and p.article_series_info[\"volume\"]:\n\t\t\t\tvolume_label = p.volume\n\t\t\tif p.issue.issue_section and \"number\" in p.issue.issue_section and p.issue.issue_section[\"number\"]:\n\t\t\t\tnumber = p.issue.issue_section[\""}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tnumber = \"\"\n\t\tsuppl = \"\"\n\t\tnumeration_issue = issue.find(NS_NUMBER_TEMPLATE)\n\t\ttry:\n\t\t\tnumber = numeration_issue.text\n\t\texcept:\n\t\t\tpass\n\t\tif numeration_issue is not None:\n\t\t\tsupplemental_issue = numeration_issue.find(NS_SUPPLEMENTAL_TEMPLATE)\n\t\t\tsuppl = supplemental_issue.text\n\t\t\ttry:\n\t\t\t\tsupplemental_number = suppl.split(\",\")[1].strip()\n\t\t\t\twhile suppl.find(\"#\")!= -1"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\t# NOTE: this function is not used to remove invalid elements,\n\t\t# since there can be multiple elements with the same number and\n\t\t# multiple suppl parts can be combined\n\t\t# Note that the number and number of citations is not the same,\n\t\t# only the first value is considered, no point checking the citations.\n\t\tnumber = issue.xpath('.//p[@id=\"%(id)s\"]/p[@number=\"%(number)s\"]/text()')\n\t\tif number:\n\t\t\text_num = number[0].replace(\"C\", \"n\").replace(\"e"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tissue_number = int(re.sub(r\"[-\\.]\", \"\", issue.xpath(\"./issue/issue/number\")[0]))\n\t\tsuppl = int(re.sub(r\"[-\\.]\", \"\", issue.xpath(\"./issue/issue/suppl\")[0]))\n\t\treturn issue_number,suppl\n\ndef _check_issues(issue, expected_issues, status, bugfix):\n\t\"\"\"\n\tCheck if any issues with a specific number or cite has been detected. \n\t\"\"\"\n\tif issue.xpath('./issue')[0].xpath('./status')[0] not in expected"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tissue_element = []\n\n\t\tnumber_element = _extract_number(issue.children)\n\n\t\tsupplment_elements = _extract_supplment(issue)\t\n\n\t\tif number_element:\n\t\t\tnumber = number_element.text\n\t\t\tsuppl = [x.strip() for x in supplment_elements if x.strip()]\n\t\telse:\n\t\t\tnumber = issue_element[0].text\n\t\t\tsuppl = []\n\n\t\tissue_element.append(number)\n\t\tnumber_element.text = number\n\t\tsuppl_elements = _extract_suppls(issue"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tnumber, suppl_number = issue_elements[issue].split(':')\n\t\tnumber = number.strip()\n\t\tsuppl_number = suppl_number.strip()\n\t\treturn int(number), suppl_number\n\n\tdef _parse_numbers(self, elements):\n\t\t\"\"\"\n\t\tParse a list of numbers or fluents from the contents of issue's elements.\t\n\t\t\"\"\"\n\t\tnumber = _extract_number_and_supplment_from_issue_element(elements[0])\n\t\tnumbers = [int(i) for i in elements[1:number+"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tif issue.find(\"{http://www.adlnet.gov/rdf}identifier\")!= -1:\n\t\t\tnumber = issue.find(\"{http://www.adlnet.gov/rdf}identifier\").text.split(\":\")[1]\n\t\t\tsuppl = issue.find(\"{http://www.adlnet.gov/rdf}supplement/suppl\").text\n\t\t\tissue = issue.replace(\"{http://www.adlnet.gov/rdf}identifier\",\"\")\n\t\t\tnumber = int(re.sub(\"\\D\",\"\",number))\n\t\t\tnumber = \"{0:05}\".format"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tnumber = False;\n\t\tsupples = issue.xpath('numeration/numerationinfo/note')\n\t\tfor suppl in suppples:\n\t\t\tif suppl.text:\n\t\t\t\tnumber = True;\n\t\tif number:\n\t\t\tsubnodes = issue.xpath('numeration', namespaces=NSMAP)\n\t\t\tfor subnode in subnodes:\n\t\t\t\tsubnode.text = fix_xml_ampersand (subnode.text).\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tencode('utf-8')\n\t\t\t\tissue.text = subnode.text\n\n\tissue = issue_element.find('./"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tresult = {\n\t\t\t\"issue_issn\"\t \t\t\t: issue.find(\"ISSN\") \\\n\t\t\t\t\t\t\t\t\tor issue.find(\"issue_issue\") \\\n\t\t\t\t\t\t\t\t\tor issue.find(\"note_issue\") \\\n\t\t\t\t\t\t\t\t\tor \"/number/\"\n\t\t\t,\"doi_issn\" \t\t\t: issue.find(\"DOI/ISSN\") \\\n\t\t\t\t\t\t\t\tor issue.find(\"issue_doi\") \\\n\t\t\t\t\t\t\t\tor issue.find(\"note_doi\") \\\n\t\t\t\t\t\t\t\tor \"/suppl/\"\n\t\t\t\t\t\t\t,\"volume_title_title\" : issue.find(\"Volume/Title/Title/"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\t\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\t\"\"\"\n\t\tnumbers = re.findall(r'\\d{4}', issue.text)\n\t\tsuppl = ''\n\t\tfor n in numbers:\n\t\t\tif int(n) == suppl:\n\t\t\t\tsuppl = n\n\t\t\t\tbreak\n\t\tif len(numbers) == 0:\n\t\t\tnumbers = re.findall(r'\\d{4}', issue.text + issue.attrib['issue'])\t\t\t\n\t\telse:\t\t\n\t\t\tsuppl = '%s' % suppl\n\t\t\tfor v in numbers:\n\t\t\t\tnumber_re = re.findall(r'\\d{8}', v)\n\t\t\t\t"}]}
{"task_id": "62b46740d2f69a53b466171a", "prompt": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tif debug:\n\t\t\treturn _pprint.PrettyPrinter(indent=indent).pformat(self.node)\n\n\t\ts = \"\\n\" + self.pprint(indent=indent)\n\n\t\tif self.debug > 1:\n\t\t\ts = \"<\\n\" + s + \">\" + str(self.pprint())\n\n\t\tif self.debug:\n\t\t\ts = \"'\" + s + \"'\"\n\n\t\treturn s\n\n\tdef pprint(self, indent=0, depth=0, skip_children=False):\n\t\t\"\"\"\n\t\tPrint the internal node of the node and all children of it as\n\t\t"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tindentStr = \"\" if indent == 0 else \"|\" if indent == len(indentStr) else \" \"\n\t\tretStr = \"%s\" % (indentStr * indent)\n\t\tif debug == True:\n\t\t\tprint(retStr)\n\t\t\tif isinstance(self, TmpFile):\n\t\t\t\tprint(\"  begin\")\n\t\tif self.filename and path.exists(self.filename) and self.filename[-4:]!= \".gz\":\n\t\t\tretStr = path.getmtime(self.filename)\n\t\t\tif retStr!= 0:\n\t\t\t\tretStr = \" - \" + str("}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\ts_indent ='            '\n\t\ts_out = []\n\t\tif debug:\n\t\t\ts_out.append('    '*(indent))\n\t\tif self.values:\n\t\t\ts_out.append('%s=%s,' % (\n\t\t\t\t\thex(self.name),\n\t\t\t\t\tself.values\n\t\t\t\t))\n\t\tfor i, e in enumerate(self.errors):\n\t\t\tif isinstance(e, BriefValueList):\n\t\t\t\tif e.value:\n\t\t\t\t\ts_out.append('\\n(%s=%s,' % (hex(e.value),\n\t\t\t\t\t\thex"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\treturn pprint.pformat(self, indent=indent, debug=debug)\n\n\tdef __str__(self):\n\t\t# XXX\n\t\t# This function is not used (as self->print())\n\t\treturn \"\"\t\t\n\n\tdef getDataTree(self):\n\t\treturn self.data.__class__(self)\n\n\tdef __repr__(self):\n\t\td=None\n\t\tif self.format==\"pretty\":\n\t\t\td = self.pretty()\n\t\treturn \"<{self.__class__.__name__} {d} {self.data}>\".format(self=self, d=d)"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\n\n\tif self.is_empty(): return ''\n\t#if debug: print \"O(1) +O(1) = \" +pprint.pformat(self.o)\n\tfor a in self.o:\n\t\tif len(a):\n\t\t\tif not isinstance(a, (Node, Leaf)):\n\t\t\t\tif not debug: print a.pprint(indent+1, hsep=indent+1)\n\t\t\t\telse: print indent*'  '+pprint.pformat(a, hsep=indent+1)\n\t\t\t\n\t\telse:\n\t\t\tif isinstance(a, (Node,"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\treturn str(self.to_list())\n\n#\n# Test functions\n#\ndef findAll(*args, **kwargs):\t# no args\n\targ_map = {'action' : lambda x: 1,'message' : lambda x: 2,\n\t\t'type' : lambda x: 3,\n\t\t}\n\tif args:\n\t\tfor i in range(len(args)):\n\t\t\tkey = arg_map.get(args[i], None)\n\t\t\tif not key:\n\t\t\t\tkey = arg(i)\n\t\t\t#print \"%s = %s\" %"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tif debug:\n\t\t\tdebug_indent_level = debug\n\t\telse:\n\t\t\tdebug_indent_level = 0\n\t\tresult = \"self.__class__   : {0}\".format(self.get_name())\n\t\tresult += \"\\n{\\n\"\n\t\t\n\t\tif self.is_empty():\n\t\t\tresult += \"  NO DATA, IT IS NO DATA!!!\"\n\t\telse:\n\t\t\tresult += self.__class__.__name__ +'{\\n'\n\t\t\t\n\t\t\tif indent:\n\t\t\t\tresult += \"\\n\"\n\t\t\t\tresult += \"  \" * indent\n\t\t\tresult += self"}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\ttext = str(self)\n\t\tif len(text):\n\t\t\ttext = text.replace('\\n','')\n\t\t\ttext = text.replace(' ','')\n\t\t#if debug:  \tprint 'pretty:', repr(text)\n\t\tif text.endswith('\\n'):\n\t\t\ttext = text[:-1] + '\\n'\n\t\t#if debug: \tprint 'debug:', indent, repr(text)\n\t\treturn text\n\t\nclass Doc(object):\n\tdef __init__(self, *args, **kwds):\n\t\tif len(args) >= 1:\n\t\t\tself."}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\ts = []\n\t\ts.append( indent * '\\t' )\n\t\tfor name, val in self.__dict__.iteritems():\n\t\t    s.append( '%s => %s\\n' % (indent * 1, name) )\n\t\t    s.append( indent * 2 )\n\t\t    try:\n\t\t\ts.append( self.__dict__[name] )\n\t\t    except AttributeError, e:\n\t\t\te = '%s is not a valid option for this object.\\n' % repr(name)\n\t\t\traise Exception( e )\n\t\tfor name, val in self.defopts.iteritems():\n\t\t    s."}, {"generated_text": "def pretty(self, indent=0, debug=False):\t\"\"\"\n\tReturn a pretty formatted representation of self.\t\"\"\"\n\t\tif debug:\n\t\t\treturn self.__class__.__name__+\": \"+ str(self)\n\t\telse:\n\t\t\treturn self.__class__.__name__+\": \"+ str(self.__dict__)\n\nclass PdbError(Exception):\n\t\"\"\"\n\tError indicating an error during serialization process.\n\t\n\t:param str msg: Error message.\n\t\"\"\"\n\tpass\n\nclass PdbNotInitializedError(PgError):\n\t\"\"\"\n\tError indicating an error during deserialization process.\n\t\n\t:param str msg: Error message.\n\t\"\"\"\n\tpass\n\nclass PdbRead"}]}
{"task_id": "62b46746d2f69a53b4661722", "prompt": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t", "suggestions": [{"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n      s = args[0]\n      for a in args[1:]:\n         if s[0] == '~':\n         absorbed = a[1:]\n         break\n      else:\n         absorbed = a\n\n      if len(absorbed) == 1:\n         absorbsimplified = self.parse_absorbsimplified(absorbed[0])\n      else:\n         absorbsimplified = self.absorbsimplified2(absorbed)\n      \n      return [a[0] for a in absorbsimplified]\n\n    def parse_absorbsimplified(self"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\targs = set(args)\n\t\tb = args.pop()\n\t\treturn [self.map[s] if s in self.map else s + b for s in args\n\t\t\t\tif self.map[s]!= s and self.map[s]!= b]\n\n\tdef __str__(self):\n\t\treturn '({}) | {} | {} {}'.format(self.reduce(), *args)\n\n\tdef __repr__(self):\n\t\treturn self.__str__()\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2."}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n        return [ x & y for x, y in self.absorb(args) ]\n\n     \nif __name__ == \"__main__\":\n\n    # Test definitions\n    n, k = symbols('n k' )\n\n    # Init the problem\n    prob = Problem( n, k )\n\n    # Random variable x\n    x = VarRandomScalar( n, k )\n    prob += PropositionalExpression( n <= k, \n                                 \"n >= k\" )\n    prob += PropositionalExpression( k <= n, \n                                 \"k >= n\" )\n    prob += PropositionalExpression( n < k, \n                                 \"k"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n        aA, aB = operatorPrecedence(args[0])\n        bA, cB = operatorPrecedence(args[1])\n\n        if len(args) > 2:\n\t    result  = []\n\t    for arg in args[2:]:\n\t        try:\n\t            result.append(~aA & ~bB & ~cB & cB)\n\t        except ArithmeticError as e:\n\t            lPrintError(lWarning(\"Expression `>>%s` is greater than or equal to two bits, got %s and %s\", arg, cA, cB, e))\n\t    return"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n        out = []\n        for e in args:\n            tmp = [ [a, a] for a in e ]\n            tmp = self.expand(tmp)\n            tmp = [ [a, b] for a in e ]\n            out += [reduce(lambda x, y: x and y, tmp ) ]\n        return out\n\n    def simplify(self, args):\n        \"\"\"\n        Given a set of linear expressions, e.g.\n        \"A & ~A\" -> B & (~A | B)\n\n        Returns the string form of ABSLIS after applying the absorption.\n        \"\"\"\n        out = []\n       "}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n        return [[e] for e in self.args]\n\n    def reduce(self, args):\n \t# TODO: Check arguments\n\n    return args\n    \n    return self\n\n\ndef normalize(expr):\n    from collections import defaultdict\n    return expr.replace(\",\", \" \")\n    \n\n\nclass RuleMaker(object):\n    def __init__(self, f, g):\n        self.f = f\n        self.g = g\n        self.rules = defaultdict(Rule)\n    \n        \n    def get_rule(self, x):\n        return x.strip(r\"\\n\\t \")\n    \n    \n    def add_rule(self, x,"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\nclass Predicate(Expression):\n   \"\"\"\n  Represents a conditional value.\n  \"\"\"\n   \n   @property\n   def value(self):\t        return None\n   @property\n   def headword(self):\n\t\treturn \"NOT\"\n\n   @property\n   def headwordset(self):\n        \"A set of all prepositions\"\n         return set()\n\n   @property\n   def headwordsetset(self):\n        \"All prepositions used to mark a condition\"\n         return set()\n\n   @property\n   def postset(self):\n        \"A set of all conditions that must follow the preposition\"\n         return set()\n\n   @"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n        s = []\n\t# We assume here that this is an ambiguity\n\tb = A&B;\n    else:\n        b = Or(*[A for i in range(1,len(args)) if arg[i-1]>1])\n\t# For each term a is b^b+a\n\tfor i in range(1,len(b)):\n\t\tif b[i].is_integer():\n\t\t\ts.extend(b[i-1] & b[i])\n\t\telse:\n\t\t\ts.extend(b[i].abs_minus(a["}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\treturn self.__class__(args)\n\n\t__reduce__ = None\n\tdef _eval_subs(self, old, new):\n\t\tif self == old: return new\n\t\telse: return old + new\n\n\tdef subs(self, old, new):\n\t\tif isinstance(old, Expression):\n\t\t\treturn self.__class__(self.args[:-1] + (old, -new))\n\t\treturn self.__class__(self.args + (self.args[-1], -old))\n\n\tdef _eval_expand_mul(self, s):\n\t\tif s.is_"}, {"generated_text": "def absorb(self, args):\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\t\"\"\"\n\t\n        self.expr_stack.append((self.ADD, args))\n        \n    def add(self, *args):\n        \"\"\"\n        Append one argument to the end of the expression stack.\n\t\"\"\"\n        self.expr_stack.append((self.ADD, self.args + self.args))\n            \n    def simplify(self):\n        \"\"\"\n    Simplify a tree and remove singleton node.\n    \"\"\"\n        while self.depth >= 0:\n    \tself.remove_node(self.depth)\n    \n    def relate_to(right):\n        \"\"\"\n    Return the tree rooted at `right` or return `left`"}]}
{"task_id": "62b86707b4d922cb0e688c2a", "prompt": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\tif not hook.is_registered():\t# Register handler.\n\t\t\treturn\n\t\tassert hook not in self.hooks.values\n\t\tself.hooks[hook] = hook\n\n# Copyright (c) 2017 Linaro Limited\n#\n# SPDX-License-Identifier: Apache-2.0\n\n'''Runner for checking dependencies'''\n\nimport sys\n\nfrom collections import OrderedDict\n\nimport easybuild.tools.config as config\nfrom easybuild.base import (\n    Command,\n    is_shippable,\n    build_files,\n)\nfrom easybuild.framework.easy"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\t@wraps(hook)\n\t\tdef handle_hook(*args):\n\t\t\tself.hooks.add(hook)\n\t\t\treturn hook(*args)\n\n\t\treturn handle_hook\n\n\t@on.hook(\"*\", pattern=\"*#\")\n\tdef call(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tDecorator function to registering listeners for a specific\n\t\thook.\n\n\t\t:returns: The function hook for the specified handler\n\t\t\"\"\"\n\t\tfor hook in self.hooks:\n\t\t\thook(*args, **kwargs)\n\n\t\treturn self\n\n\t@on.hook(\"server."}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\ttry:\n\t\t\thooks_list = self.__all_hooks[hook]\n\t\texcept KeyError as e:\n\t\t\tlogging.info(\"Could not register non-existent hook: %s\", hook)\n\t\t\treturn False\n\t\tfor hook in hooks_list:\n\t\t\tif hook.is_enabled(hook):\n\t\t\t\tself.__all_hooks[hook] = hook\n\t\t\t\treturn True\n\treturn hook\n\t\n'''\nCreated on Apr 19, 2014\n\n@author: hierghe@gmail.com\n'''\nfrom abc import ABC, abstractmethod, ABCException\nimport random\n\n"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\tif isinstance(hook,HookType):\n\t\t\thook.register(self)\n\t\t\treturn self\n\t\telif isinstance(hook,str):\n\t\t\treturn hook\n\t\telse:\n\t\t\traise TypeError\n\treturn lambda self, hook: self.subscribe(hook(), hook.__class__.__name__)\n\nclass Notifier:\n\t\"\"\"\n\tDecorator class to add an `on_<topic>` handler to the\n\tregistry for `on`:\n\n\t\t# notifies a subscriber for that topic\n\t\t# then calls on_all for that topic in all registered\n\t\t# subscribers.\n\t\"\"\"\n"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\treturn self._register_listener\n\n\tdef _register_listener(self, event, hook):\n\t\t\"\"\"\n\t\tRegister a new listener function so that it may run in the endless loop through the lifecycle\n\t\tof the lifecycle of the event.\n\t\t\"\"\"\n\t\tself._listener_for_hooks.append((hook, event, hook.get_full_path()))\n\t\tself._listener_for_listeners_by_path[hook.get_path()] = event\n\n\tdef unregister(self, hook):\t\"\"\"\n\t\tUnregister a previously registered handler from the given hook.\n\t"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\tpass\n\n\ttry:\n\t\thook.plugin()\n\texcept Exception as e:\n\t\tlogger.error(\"Error while registering {}\".format(hook.description))\n\t\traise\n\n\ndef on_first_registration(sender, plugin):\n\t\"\"\"\n\tRegisters up a new plugin's object for the specified *plugin* instance.\n\n\tThis is a hook function that can be used to register new plugins.\n\t\"\"\"\n\n\tdef hook_for_first_registration(plugin):\n\t\t\"\"\"\n\t\tDecorator function to register hooks whenever a plugin is added to\n\t\tthe registry.\n\n\t\t:"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\tpass\n\n\nclass AutoRegisterListener(HookType):\n\t\"\"\"\n\tClass to attach a listener to a hook.\n\n\tArguments:\thook (HookType): The hook instance\n\t\"\"\"\n\tdef __init__(self, hook):\n\t\tself.hook = hook\n\t\tself.handlers = []\n\n\tdef __call__(self, *args, **kwargs):\n\t\tself.handlers.append(hook)\n\n\nclass HookObject(object):\n\t\t\"\"\"\n\tImplements a registered hook. This class is instantiated solely from\n\tthe registered hook class. See the register() class."}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\tif not hasattr(hook, \"__hooks_registered\"):\n\t\t\thook.__hooks_registered = []\n\t\thook.__hooks_registered = [hook.__hooks_registered]\n\t\tdef wrap(func):\n\t\t\tsetattr(self, hook.__hooks_registered, {\n\t\t\t\t\"after\": hook.__hooks_registered,\n\t\t\t\t\"before\": hook.__hooks_registered\n\t\t\t})\n\t\treturn wrap\n\tself.__registry[hook.hook_type](hook, self.__hook_type, False)\n\ndef remove(obj, hooks_key):\n\t\"\"\"\n\tDecorator function to remove a supplied"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\tdef add_listener(self, event, handler):\n\t\t\t\"\"\"\n\t\t\tListen for an event in the given `event`. Returns the\n\t\t\ttuple of the event's arguments and an event ID. If one does not\n\t\t\tcontain a registered handler for the given `event`, return\n\t\t\tNone and don't forget to call the hook callback for each\n\t\t\thandler and the argument `event`. To trigger an `on` event,\n\t\t\tthe event identifier will be generated from the ID of the event\n\t\t\tin the `hook`. If the argument `event` does contain multiple\n\t\t\tevents, the event identifier will"}, {"generated_text": "def on(self, hook):\t\"\"\"\n\tDecorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\t\"\"\"\n\t\tlogging.debug(\"Registering %s on module %s\" %(self.name, self.module))\n\t\tsetattr(self, self._REGISTERED_HACKS_DICT[hook.hack_type](hook))\n\t\thooks[self.name] = hook\n\t\tself._REGISTRY_HOOKS[hook.hook_type] = hook\n\t\n\tlogging.debug(\"Registering %s on module %s\" %(self.name, self.module))\n\thooks[self.name].add_hook_signal(self._REGISTERED_HACKS_DICT[hook."}]}
{"task_id": "62b86729b4d922cb0e688c2f", "prompt": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\treturn {\n\t\t\t'esd_url': user_etcd_url + \"/v0/user-login.php?action=\" + user + \"&param0=update\",\n\t\t\t'esd_user': user,\n\t\t\t'esd_password': etcd_password,\n\n\t\t\t'esd_login': etcd_user,\n\t\t\t'esd_port': etcd_port,\n\t\t}\n\ndef api_base_config():\n\t\"\"\"\n\tCreates a configuration with basic functions with a few important ones.\n\n\tArgs:\n    None.\n\n\tReturns:"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n\n\tconfig = {\n\t\t\"app_id\":\"localhost\",\n\t\t\"app_secret\":\"\",\n\t\t\"app_key\":\"/\",\n\t\t\"app_url\":\"\",\n\t\t\"username\":\"\",\n\t\t\"/ping_interval\":60 * 5,\n\t\t\"heartbeat_interval\":60 * 5,\n        \"timeout\":0,\n\t\t\"verbose\":False,\n\t\t\"app_config\":{},\n\t\t\"etcd_host\":etcd_host,\n\t\t\"etcd_port\":etcd_port,\n\t\t\"etcd_name\":\"etcd2\",\n\t\t"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n    with_args = {}\n    with_args['user'] = user\n    with_args['etcd_host'] = etcd_host\n    with_args['etcd_port'] = etcd_port\n\n    configuration_dir = './'\n\n    with open (os.path.join (configuration_dir, 'cluster.conf'), 'r') as file:\n        data = file.readlines()\n\n    for line in data:\n        if not line.strip().startswith('#'):\n            key, value = line.strip().split('=', 1)\n            with_args[key.strip().lower()] = value\n        \n    # print(\""}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n        cfg = {'user' : user, 'root_path' : '/root:/etc/etcd.conf',\n              'etcd_host' : etcd_host, 'etcd_port' : etcd_port}\n\n\treturn cfg\n\n\ndef install_plugin(config):\n\n\tfor plugin in etcd.plugin.register:\n\t\tprint(issue(plugin)) \n\n\tetcd.init()\n\tetcd.register(ETCD)\n\n\ndef main(args):\n\n\tconfig = base_config(args.config, args.etcd_host, args.etcd"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n    return {\"host\": etcd_host,\n            \"port\": etcd_port,\n            \"__user\": user,\n            \"__etcd_port__\": etcd_port}\n\nclass BaseManager(object):\n    def __init__(self):\n        super(BaseManager, self).__init__()\n        self._db = {}\n\n    def set_db_engine(self, db_engine):\n        \"\"\"Set the SQLAlchemy database engine\n\n        Args:\n            db_engine (?): the database Engine instance\n\n        \"\"\"\n\n        self._db_engine = db_engine\n\n    def get_db_engine(self):\n        \"\"\"Return the"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\twith app.app_context():\n\t\t\tconfig = app.config['INBOUND_USER_CONFIG']\n\t\t\tuser_config = {\n\t\t\t\t'default_host': etcd_host,\n\t\t\t\t'default_port': etcd_port,\n\t\t\t\t'module': 'frontend',\n\t\t\t\t'database': config['db'],\n\t\t\t\t'default_port': config['port'],\n\t\t\t\t'secret_key': config['secret_key']\n\t\treturn user_config\n\texcept KeyError as e:\n\t\tprint(\"No key found: {0}\".format(e.args))"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\traise Exception(\"You have to specify an etcd service for the root user\")\n\ndef create_etcd_user(user, etcd_host=\"localhost\", etcd_port=2378):\n    \"\"\"\n    Creates a Django User with the provided arguments. Note this can change\n    this to match the configuration on the web interface.\n\n    Args:\n    user (str): The name of the user for this configuration\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    pystache_ete.config"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\n    settings = {}\n\n    class SimpleConfig(Config):\n        DEFAULTS = {'secret': _get_secret(),\n                        'host': etcd_host,\n                        'port': etcd_port}\n\n        DETECTION = {\n            \"name\": \"secret\",\n            \"url\":\"https://www.googleapis.com/auth/xapi.datadog.io\",\n            \"key\": _get_api_id()\n        }\n        SECRET_KEY = _get_secret()\n\n        @staticmethod\n        def get(key, default=None):\n            return _get_secret().get(key)\n\n    class Sql"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\tconfig = {}\n\t\tconfig[\"user\"] = user\n\t\tconfig[\"etcd_server\"] = host\n\t\tconfig[\"etcd_port\"] = etcd_port\n\t\tconfig[\"api_key\"] = etd_host\n\t\tconfig[\"api_secret\"] = etcd_port\n\t\tconfig[\"debug\"] = False\n\t\tconfig[\"debug_enabled\"] = True\n\t\treturn config\n\ndef base_error(traceback):\n\t\"\"\"\n\tThis function prints a stack trace and any errors that are printed to stderr\n\tafter writing a traceback, unless debug is set to true.\n\t\"\"\""}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\t\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\t\"\"\"\n\t\ndef create_config_section(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration file section.\n\n\tArgs:\n        user (str): the name of the user for the static authentication\n\t\tetcd_host (str): the host for the database.\n\t\tetcd_port (int): the port for the database.\n\t\"\"\"\n\treturn \"\"\"\n  database {\n    host_name {\n      port 9999;\n      user {}\n    }\n  \n    database {\n      port 1035;\n      user {}\n    }\n  \n}\n#"}]}
{"task_id": "62b8a4a4755ee91dce50a3d2", "prompt": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\n\nclass _TimeRE(_TimeRE):\n\n\t\"\"\"\n\tAn object to manage a collection of time records. The time\n\trecords are represented as `datetime` instances for each timestamp in\n\tthis collection, that is mapped back to the :attr:`now` property of\n\tthe :attr:`timestamp` instance.\n\n\tAttributes\n\t----------\n\ttimestamp : `int` or `float`\n\t\tThe timestamp of the time record.\n\n\t \"\"\"\n\n\tdef __init__(self, timestamp):\n\t\t\"\"\"\n\t\tCreate a new _TimeRE object.\n\n\t\tParameters\n\t\t----------\n\t"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\nclass tzlocal(tzinfo):\n\t\"\"\"\n\tUTC time based timezone.\n\n\t:param dt_or_tzinfo: An `tzinfo` subclass that may\n\tconvert/interpolate into.  Defaults to :class:`datetime.tzinfo`, but\n\twill not copy and/or change tzinfo attributes to this\n\tobject.\n\t\"\"\"\n\tdef __init__(self, tz_or_tzinfo):\n\t\t\"\"\"\n\t\t:arg tz_or_tzinfo: A `tzinfo` subclass that may\n\t\tconvert/interpolate into.  Defaults to :class:`datetime.tzinfo`, but\n\t\twill not"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n    # Check if this is a fold state\n    if self.fold.prevDate is not None and dt == self.fold.prevDate:\n        return dt\n\n    if dt.date() == dt.date():\t\t# Same day as the previous day.\n        return self.fold.prevDate\n    \n    if dt.hour() <= 24:\t\t\t# The same day as the previous month.\n        if dt.hour() < 23:\t        # It's a weekend\n            if dt.hour() == 0 or dt.hour() > 6:\n                return self.fold.prevDate\n        else:\n            if"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\ndef normalize_time(t):\n\t\"\"\"\n\tTake an aware datetime object and normalize it to a\n\tstandard non-aware one with a zero time (this has the default\n\tfor the UTC offset).\n\t\"\"\"\n\tutc_offset = localtime(time.mktime(t.timetuple()))\n\tresult = localtime(time.time())\n\tif result >= 0:\n\t\tresult -= result%3600\n\treturn result\n\n\n\ndef make_naive(dt, dt_aware=None):\n\t\"\"\"\n\tCreate a datetime object from a naive datetime object dt.\n\t"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\t\t\n\t# First, get an ambiguous datetime:\n\ttz = _utcoffset(dt)\n\tdup_date = tz - _utcoffset(dt)\n\t\n\tif dup_date.days < 1 or dup_date.days > 31:\n\t\t# All back to same month:\n\t\treturn dt\n\t\n\t# Last, we get the last day:\n\tif dup_date.days > 0:\n\t\t# Get last day in the final datetime:\n\t\tend = dup_date.replace(year=dup_date.year + 1,\n\t\t                        month=dup_date.month + 1"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\t\t\n\treturn dt + datetime.timedelta(seconds=int(dt.strftime('%s')))\n\n\ndef now():\n\t\"\"\"\n\tReturns a :class:`datetime.datetime` object representing the current UTC time\n\tthat the ``datetime`` object represents.\n\n\tThe object returned contains values since the unix epoch.\n\t\"\"\"\n\treturn _utctostr(time_lib.TimeZone.localtime(datetime.datetime.utcnow()))\n\n\ndef combinetimes(end_time, start_time, times):\n\t\"\"\"\n\tTakes two datetimes in which each contains all times from ``times`` in\n\t"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t \n\t\n\n\t# Look for ambiguous datetimes in a timezone-aware timezone or vice\n\t# versa\n\tpossible_ambiguous_dates = [\n\t\t(\n\t\t\t(\n\t\t\t\tget_date(dt).tzinfo,\n\t\t\t\tNone\n\t\t\t),\n\t\t\tfrom_isl_datetime(dt, isl.tzinfo)\n\t\t),\n\t\t(\n\t\t\t(\n\t\t\t\tget_date(dt).tzinfo,\n\t\t\t\tNone\n\t\t\t),\n\t\t(\n\t\t\tdt,\n\t\t\tget_date(dt).tzinfo,\n\t\t\tto_isl_datetime(dt)\n\t\t),\n\t\t("}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\t\n\t# if the timestamp and/or timezone are timezone-naive, the\n\t# naive UTC datetime's don't have any spelling correction applied.\n\t# the same goes for completeness, just to be safe, try the\n\t# timezone module.  if we find one, use that.  I don't care -\n\t# but if the datetime is naive with no spelling, we'll try one\n\t# by calling the timezone specific method.\n\tif isinstance(dt, datetime.datetime) :\n\t\tif dt.tzinfo is not None :\t# naive time - time is really naive\n\t\t"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n    try:\n\t\n        _dt = pytz.utc.localize(dt)\n    except:\n        # Not ambiguous. Invalid input type.\n        return datetime(1970,1,1,tzinfo=None)\n\n    assert _dt.tzinfo is not None\n    if _dt > _dtoff:\n        return datetime(1970,1,1,tzinfo=None)\n\n    if _dtoff < 0:\n        raise ValueError(\"Invalid dtoff - {}\".format(_dtoff))\n\n    # If the last occurrence falls on the same minute of the two\n    # datetimes, we assume that we're looking at the same date"}, {"generated_text": "def _fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\t# We may already have a fuzzy-match based on the object's\n\t\t# UTC offset, in order to find the nearest match to that\n\t\t# specific datetime object.\n\t\t#\n\t\t# Ie. if the first element of the array is a naive\n\t\t# UTC day-of-the-week old, we need to add a small variation just to\n\t\t# match the seconds between the time.\n\t\t#\n\t\t# Since no such day is found in the array, we can't go and have\n\t\t# the same offset in the array that is ambiguous any more. So we\n\t"}]}
{"task_id": "62b8982f755ee91dce50a241", "prompt": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\tassert 1<=len(self.years),'%d not in %r' % (len(self.years),self.years)\n\t\tself.years = range(1,len(self.years))\n\t\treturn relativedelta(years=self.years,months=self.months)\n\n\t@staticmethod\n\tdef is_leap_year(year):\n\t\treturn year % 4 == 1 and leap_days[year%4] in xrange(4, 12)\n\n\tdef is_leap_year_exact(self,year):\n\t\treturn self.get_year_start(datetime"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\n    def offset(self, off):\t\n\t\t\"\"\"\n\t\tGenerate a datetime object with an integer offset\n\t\tfor the right-hand side of an interval.\n\t\tThe date will come exactly from the datetime given.\n\n\t\t:param off: the number of seconds in the positive direction\n\t\t  for this interval (positive=right, negative=left)\n\t\t:return: a :class:`dateutil.relativedelta.relativedelta` object.\n\t\t\"\"\"\n\n\t\tnew = start = off + datetime.timedelta(self.days)\n\n\t\t# Work around a bug in pyinterval:\n\t\t# https://github."}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\ttry:\n\t\t\t# Note: The definition of this method is an approximation.\n\t\t\tdelta_y, delta_m, days_in_month = self.to_delta_tstr(self.datetime)\n\t\t\tdeltas = relativedelta.years(delta_y, delta_m)\n\t\t\tyears = (delta_y, delta_m, days_in_month, 0)\n\t\t\tres = relativedelta(years, days_in_month).years\n\t\t\treturn res\n\t\texcept ValueError:\n\t\t\t# It may happen that the result is not valid since our result is\n\t\t\t# invalid\n\t\t"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\n        return self\n\t\t\n\n    def __radd__(self, other):\n \n        if not isinstance(other, relativedelta):\n\t    raise TypeError(\"unsupported operand type: {0}\".format(type(self)))\n        \n        return relativedelta(_days=self._days + other._days,\n                              _seconds=self._seconds + other._seconds)\n \n\nclass date_range(object):\n    \"\"\"\n    A class for representing dates\n\n    A DateRange object is made a fixed frequency date range, both\n    constrained to billions only.\n\n    >>> from datetime import date\n    >>> d_date = date(2004, 12, 30)\n    >>>"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\n        raise NotImplementedError, \"You need to implement normalized().\"\n# -*- Mode: Python -*-\n# vi:si:et:sw=4:sts=4:ts=4\n#\n# gst-python - Python bindings for GStreamer\n# Copyright (C) 2005-2006  Johan Dahlin\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\n\n\treturn relativedelta\n# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\n    # Don't use self.tuple() or self.date() here, to avoid\n    # parsing both args and kwargs for set/accumulating the values\n    # for each time\n    def __getitem__(self, args):\n        return self.date(*args)\n\n    def __contains__(self, date):\n        \"\"\"\n        :type date: datetime.date\n        :rtype: bool\n        \"\"\"\n        return (date >= self).any()\n\n    def __format__(self, format_spec):\n        \"\"\"\n        :type format_spec: str\n        :rtype: str\n        \"\"\"\n        return self.date.__format__(format_spec"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\treturn relativedelta(self)\n\n\t@property\n\tdef name(self):\treturn self.normalized().name\n\ndef from_string(s: str) -> datetime:\n\t\"\"\"\n\tReturn a :class:`dateutil.relativedelta.relativedelta` object\n\twhich represents the ISO 8601 date and time.\n\n    :param s: a :class:`dateutil.relativedelta.relativedelta` object\n\t(see :class:`dateutil.relativedelta`)\n\t\"\"\"\n\n\ttry:\n\t\tdt = datetime.strptime(s, \"iso-8601\", dayfirst=True)\n\texcept ValueError"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\n\t\n\td = relativedelta(days=self.days,\n\t\t\t                    hours=self._get_offset())\n\tif d.months:\n\t\ttry:\n\t\t\tresult = relativedelta(d.year, d.month, d.day) + relativedelta(1,\n\t\t\t\t\t\t\t\t\t\t\t\t\t  (d.hour, d.minutes, d.seconds))\n\t\texcept ValueError:\n\t\t\tresult = relativedelta(self.years, self.months, d.days) + relativedelta(1,\n\t\t\t\t\t\t\t\t  d.hour, d.minutes, d.seconds)\n\n\t\tif result.months:\n\t\t\treturn '%s %"}, {"generated_text": "def normalized(self):\t\"\"\"\n\tReturn a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\t\"\"\"\n\t\ndef date(year, month, day=1, hour=0, minute=0, second=0, microsecond=0):\n\t\"\"\"\n\tReturn a :class:`datetime.date` object given a year and month, day and hour and minute\n\tand second, microsecond.\n\n\t>>> datetime(2000, 1, 1).date()\n\tdatetime(2000, 1, 2)\n\t>>> datetime(2000, 1, 3).date()\n\tdatetime(2000, 1, 2)\n\t>>> datetime(2000, 1, 3) + relativedelta(days=1)\n\tdatetime"}]}
{"task_id": "62b89640755ee91dce50a114", "prompt": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t", "suggestions": [{"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\treturn namefunc(ord(namefunc.decode('charmap')))\n\n\tif not PYTHON3:\n\t\tdef hex_to_str(nameval):\n\t\t\ttry:\n\t\t\t\treturn nameval.decode('hex')\n\t\t\texcept (ValueError, TypeError):\n\t\t\t\treturn nameval.encode('hex')\n\n\telse:\n\t\tdef hex_to_str(nameval):\n\t\t\ttry:\n\t\t\t\treturn nameval.encode('hex')\n\t\t\texcept (ValueError, TypeError):\n\t\t\t\treturn nameval.encode('hex')\n\n\tdef encode_idna_name(idna_nameval"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\tif namefunc.__doc__ is None:\n\t\t\t# 'builtins' codec can't decode byte strings using\n\t\t\t# codecs.CodecInfo.can_encode('UTF-8')(str); out=bytes(namefunc()).decode('UTF-8')\n\t\t\tnamefunc.__doc__ = namefunc.__doc__.encode('UTF-8')\n\t\treturn namefunc\n\treturn tzname_in_python3\n\n#!/usr/bin/python\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2015  Red Hat, Inc.\n# Written by Fabian Deutsch"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\treturn namefunc.encode('ascii', 'backslashreplace')\n\ndef tzname_out_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 3\n\ntzname() API changed in Python 3. It used to return Unicode text, with utf8 bytestrings\n\ntzname_out_python2() API changed in Python 3. It used to return UNC strings, if\nused. The unicode output still shows as a byte string.\"\"\"\n\t\treturn unistr(namefunc.encode('ascii', 'backslashreplace'))\n\ndef unistr(name):\n\tif type("}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\t# get python2/3 function\n\t\t# to unicode\n\t\tnamefunc = unicode\n\t\t# see if unicode(namefunc) works\n\t\tnamefunc.encode\n\t\treturn namefunc()\n\treturn codecs.lookup('ASCII').encode(namefunc)\n\n\n# py2.7 differences\nimport hashlib\nimport os\nfrom datetime import datetime\nimport time\nimport logging\nimport re\nimport sys\nimport tempfile\nimport traceback\n\nimport chardet\n\n\n# https://www.unicode.org/reports/tr34/#General_Category_Functions\n# https://mail.python.org/"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\tname = namefunc.__name__\n\t\tif name in (\"mbcs\",\"cp949\"): return name\n\t\telse: return name.encode(ENCODING)\n\n\n__all__ = ['tzname', 'countryNameList','countryCodeList']\n\ncountryNameList = countryNameRegex()\n\n# aliases\ncountryNameList = {\n\t\"Afghanistan\": \"Afghanistan\",\n\t\"Albanian\": \"Albanian\",\n\t\"American Samoa\": \"Asia\",\n\t\"Andorra\": \"Andorra\",\n\t\"An"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\tif namefunc.__name__ in ('unichr', 'unicode') or namefunc is None:\n\t\t\treturn namefunc.__name__\n\n\t\treturn namefunc.__name__.encode('ascii','backslashreplace')\n\n\tdef strftime(self, format_string):\n\t\targs = (self.name, format_string)\n\t\treturn strftime(args).decode('ascii', 'backslashreplace')\n\n\tdef strptime(self, format_string, arg):\n\t\treturn arg.decode('ascii', 'backslashreplace')\n\n\tdef format(self, *args, **kwargs):\n\t\timport locale,"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\tnamearr = unicode(namefunc(u\"Zoubo\"), \"utf-16-le\") # unicode, python 3, 2\n\tnameint=bytes(namefunc(b\"Zob\"), \"utf-16-le\").decode(\"utf-16-le\")\n\tintarr = nameint.split(b\"_\")\n\treturn (b\"%.\" + decode_string(namearr[0]) + \", \" + decode_string(namearr[1]) + \", \" + decode_string(namearr[2]) + \", \" + decode_string(namearr["}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\tif isinstance(namefunc, type(u'')):\n\t\t\tnamefunc = namefunc.encode('utf-8')\n\t\tnamefunc = namefunc.decode('latin1')\n\t\tencoding = sys.getfilesystemencoding()\n\t\treturn namefunc\n\n\t# Rename 'foo' to 'bar'\n\tnamefunc = decode_name(name)\n\n\t# Convert from hex to decimal.\n\tnamefunc = namefunc + '.hex'\n\n\tif sys.version_info[0]!= 3:\n\t\tnamefunc = namefunc.encode(sys.getfilesystemencoding())\n\n\treturn codecs"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\treturn namefunc\n\nif hasattr(time, 'tzname'):\t\n    def tzname_unicode(namefunc):\t\n\treturn text_type(namefunc).encode(sys.stdout.encoding, 'backslashreplace')\nelse:\t\n    def tzname_unicode(namefunc):\n\treturn text_type(namefunc)\n\t\t\n\t\t\t\n# ---\n\ndef _add_method(object):\t\n# ---\n\ndef _add_builtin(cls):\t\n# Python -> C++\n\tdef _(c):\n\t\treturn '%s' %(c.__name__)\n\tsetattr(cls"}, {"generated_text": "def tzname_in_python2(namefunc):\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\t\"\"\"\n\t\treturn namefunc(sys.stdout.encoding,'replace').decode(\"utf-8\")\n\n# vim:ts=4:sw=4:et\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n"}]}
{"task_id": "62b87d24d292efb640a55670", "prompt": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\ttry:\n\t\t\treturn open(os.path.join(cwd, '.git/info/alternativailable'))\n\t\texcept IOError:\n\t\t\tpass\n\n\t\tdef get_commits():\n\t\t\t# Was:  git:\t\tcreated\tmodified\trev1\trev2\n\t\t\trepo = get_repository()\n\t\t\treturn repo.branch_format % (''.join(repo.git.log('--pretty=%B')).splitlines(),\n\t\t\t\t\t\t\t\t\trepo.head.label)\n\n\t\tlog.init_log(git_dir=cwd)\n\n\t\tif not os.path.isdir(cwd):"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\tversion = \"v.\"+str(v)+\".0\"\n\t\tif os.path.isfile(os.path.join(cptdir,'cptdir.c')):\n\t\t\tcptd_version = open(os.path.join(cptdir,'cptdir.c'), \"r\").read().split(\" \")[0]\n\t\t\tversion = cptd_version\n\t\telse:\n\t\t\tversion = \"0\"\n\t\treturn version\n\t\ndef _build_cmd(cmd,version,target,opts):\n\t\"\"\"\n\tBuild a command line and execute"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\treturn get_versions\n\n##################################################################################\n#\n# Copyright (C) 2012-2014 Jaakko Luttinen\n#\n# This file is licensed under the MIT License.\n##################################################################################\n\n\"\"\"\nModule for handling the data and the handling of an experiment.\n\nThis module defines the interface that the plugin supports, as well as a\nsimple API for creating and interacting with the experiment, as well as a\nbasic utility functions so that the data can display\n(or accept a response from the API) and interact with the GUI.\n\"\"\"\n\nfrom __future__ import print"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\ttry:\n\t\t\treturn get_metadata()[\"version\"]\n\t\texcept:\n\t\t\tversion_string = read(os.path.join(dpkgdir, \"PKG-INFO\"))\n\t\t\tif version_string:\n\t\t\t\tversion = version_string.split(\" \")[0].strip()\n\t\t\t\tversions = {\n\t\t\t\t   \"version_major\": version[0:2],\n\t\t\t\t   \"version_minor\": version[2:4],\n\t\t\t\t   \"version_revision\": version[4:6],\n\t\t\t\t   \"version_release\": version[6:8],\n\t\t\t\t   \"version_codename\": version["}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\tif CROSSWALK_VERSION in os.environ:\n\t\t\treturn int(os.environ[CROSSWALK_VERSION])\n\t\telse:\n\t\t\t# get_timestamp() does not raise an exception even though\n\t\t\t# get_timestamp() does... (See http://stackoverflow.com/questions/6524549/how-do-i-get-the-right-way-to-get-timestamp-of-python-2-5-line-2-3)\n\t\t\t# https://stackoverflow.com/questions/2025884/get-current-timestamp-of-"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\tversion=None\n\t\t#TODO: Change version number if the current version exists\n\t\ttry:\n\t\t\twith open(VERSION_FILE) as version_file:\n\t\t\t\tversion=version_file.read().strip()\n\t\texcept: pass\n\t\tif not version: return version\n\n\t\t#TODO: Change git revision?\n\t\treturn version\n\n\tdef get_git_revision(self,version=None):\n\t\t\"\"\"\n\t\tGet the git revision information or return default if unable to do so\n\t\t\"\"\"\n\t\tgit_revision=None\n\t\t#TODO: Change git commit id if none\n\t"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\treturn {}\n\n\n__version__ = get_versions()['version']\ndel get_versions\n\n# Initialize the logger.\nlogger = logging.getLogger(__name__)\n\n# Create and configure the log messages.\nhandler = logging.StreamHandler(sys.stdout)\nhandler.setFormatter(defaultFormatter)\nlogger.addHandler(handler)\n\n# Set a formatter for the log\nformatter = logging.Formatter('%(levelname)s - %(message)s')\nhandler.addFilter(LogLevelFilter(logging.WARNING))\nhandler.setLevel(logging.DEBUG)\nlogger.addHandler(handler)\n"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\ttry:\n\t\t\tfrom git import Repo, Commit, Version\n\t\t\tif Version.VERSION < 'devel': raise ImportError()\n\t\texcept ImportError:\n\t\t\traise ImportError(\"Get versioninfo from git, revision control \" + \\\n\t\t\"information from pypi: \" + str(__version__))\n\t\treturn Version.read_svn_info(\"rdiff-backup\")\n\tdef get_version_from_git():\n\t\t\"\"\"\n\t\tGet current version\n\t\t\"\"\"\n\t\ttry:\n\t\t\tfrom subprocess import check_output\n\t\t\timport sys\n\t\t\tinfo = check_output(\"git log --pretty="}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\t# get the version_table from the repo and merge from commit/version (if it exists).\n\t\t# if there's no version table, there's nothing to do\n\t\tversion_table = get_head_from_repo(working_copy_dir, head_hash)\n\t\tif version_table is None:\n\t\t\t# use head_hash, fetch from the remote before retrieving sha 1 and git.\n\t\t\treturn (None, None, None, None, {})\n\n\t\t# do an initial fetch\n\t\tprint(version_table)\n\t\tret = version_table['head']\n\t\t"}, {"generated_text": "def get_versions():\t\"\"\"\n\tGet version information or return default if unable to do so.\t\"\"\"\n\t\ttry:\n\t\t\timport pkg_resources\n\t\texcept ImportError:\n\t\t\treturn default_versions\n\n\tversion, release_date, machine, processor = uname()\n\tif version is None:\n\t\tversion = default_version\n\n\tif machine is None:\n\t\tmachine = default_machine\n\n\tif processor is None:\n\t\tprocessor = default_processor\n\n\treturn version, release_date, machine, processor\n\n###############################################################\n\ndef display_cpu_info(ctx):\n\t\"\"\"\n\tDisplay information about computer's CPU architecture and processor\n\t\"\"\"\n\tif sys.platform == '"}]}
{"task_id": "62b87d24d292efb640a5566f", "prompt": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\treturn pieces[\"version\"] + (style, pieces[\"closest-tag\"])[style]\n\ndef render_pep440_pre(pieces):\n\t\"\"\"TAG[.post.devDISTANCE] -- No -dirty.\n\n\tThis is only used for nowrap-related scripts (ANTI) since\n    the output of this script is hard-coded into the source file\n    (and therefore.post.devDISTANCE.post.dev0+dev0).\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE.dev0+dev0: this tag only saw the last block,\n     and is"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\traise AssertionError, \\\n\t\t\t\"Style '%s' not supported yet\" % style\n\toutput = \"\"\n\tfor piece in pieces:\n\t\toutput = output + render_piece(piece, style)\n\n\treturn output\n\ndef render_piece(piece, style):\n\t\"\"\" Render the given piece into the requested style. \"\"\"\n\tfrom re import compile as compiled_re\n\tfrom cStringIO import StringIO\n\tfrom mercurial import match_revision\n\n\t# Special piece\n\tif type(piece) == str:\n\t\tpiece = compile_replacements(piece, style)\n\n\tif"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\treturn platypus.render(pieces, style)\n\n\n# The *major* and *minor* specifiers are associated to a particular *format*\n# block (as opposed to a group of variants).  All numbers are in\n# the specified *format* attribute.\n\n#    \"minor\" means \"the next major release number\", \"the next minor release\n# number\", \"and so on\" means \"the next patch release\", etc.\n\ndef _parse_format(format):\n\t\"\"\"Parses the given format specification into its constituents.\n\n\tWhen reading the source format"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\treturn getattr(pieces,'render_' + style)(pieces)\n\ndefault_processors = [\n\trender_to_string,\n\trender_to_html,\n]\nimport re\n\nfrom django import VERSION\n\nfrom.exceptions import BadArgumentError, ConfigurationError\n\n\ndef is_url(url):\n    # type: (Text) -> bool\n    \"\"\"\n    Attempt to determine if a URL has been found at the given URL\n    string.\n\n    :type url: ``unicode``\n    :param url: The URL to check for.  Should be unicode already.\n\n    :rtype: ``bool"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\traise NotImplementedError\n\n\treturn pieces[\"version\"] + pieces[\"dirty\"]\n\n\ndef find_version(*file_paths):\n\t\"\"\"\n\tFind the version string in a file path using part of our branch log.\n\t\"\"\"\n\t# the version string can be constructed by adding + or # followed by an arbitrary\n\t#  - followed by a colon followed by one or more digits.\n\t# These're so things like \"2.4.0\" and \"2.4.0.0\", as well as\n\t#  \"2.4b1\" and \"2.4c"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\trendered = \"\"\n\t\tfor version, rendered_pieces in pieces.items():\n\t\t\t# Ignore anything that isn't a tag or branch\n\t\t\tif version is None: continue\n\t\t\telif rendered_pieces.filter(lambda t: t[0] not in (\"|\", \"#\")):\n\t\t\t\t# We've reached a non tag or branch and the version list\n\t\t\t\t# contains only a single version, so we emit a level\n\t\t\t\t# annotated tag or branch with the entire revision.\n\t\t\t\trendered += \"<h%d id='version_branch_%d'>\" % (version, rendered_pieces.start("}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\n\t\n\tresult = \"\"\n\t\n\tfor i, piece in enumerate(pieces):\n\t\tpieces[i] = piece[0]\n\t\n\tfor attr in ('style', 'attrs'):\n\t\tmatchattr = attr + \"_match=\" + pieces[i][1]\n\t\tfor attrValue in pieces[i][0]:\n\t\t\tif not (attrValue in attrs.keys() or matchattr in attrs.keys()):\n\t\t\t\tresult += \"%s=\\\"%s\\\"\" % (attr, attrs[attrValue])\n\n\n\timport operator\n\tresult = result.replace(\"<\", \"&lt;\")\n\tresult = result.replace"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\trenderer = BakefileRenderer()\n\t\tfor style_ in pieces:\n\t\t\trenderer.render_style(style_)\n\t\tyield \"\".join(b.render_pieces(style) for b in pieces)\n\nclass RecipeRenderingBakeFiles(RecipeRendering):\n\t\"\"\"\n\tSaves the RecipeRenderer object into a given path based on a\n\tfilename (for example), in the recipe directory named by the\n\trecipe name and using filename.\t\n\t\"\"\"\n\tdef __init__(self, recipe):\n\t\t\"\"\"\n\tTakes a path name as a string containing"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\n\t\n\trendered = \"\"\n\tfor piece in pieces:\n\t\tpart = piece[0].strip()\n\t\tif part == '+':\n\t\t\tcontinue\n\t\tif part in rendered:\n\t\t\tcontinue\n\t\tif \"dirty\" in piece:\n\t\t\tcontinue\n\t\tif \"error\" in piece:\n\t\t\trendered = rendered + \"Error in \"+piece\n\n\treturn pieces\n\n\n\n# Helpers\n\ndef _get_unignored(text, text0=\"\", text1=\"\"):\n\t\"\"\"\n\tGet an array of words that should not be displayed.\n\n\tThe text0 array is assumed to be a list"}, {"generated_text": "def render(pieces, style):\t\"\"\"\n\tRender the given version pieces into the requested style.\t\"\"\"\n\t\n\t\n\tstyles = []\n\tfor part in pieces:\n\t\tstyle = copy.copy(style)\n\t\tif part.renderContents is True:\n\t\t\tstyle['pageBreakBefore'] = pieces[1][0][0]\n\t\t\tstyles.append(style)\n\t\telse:\n\t\t\tstyle['pageBreakAfter'] = pieces[0][0][1]\n\t\t\tstyles.append(style)\n\n\tstyle = style.copy()\n\ttStyle = {}\n\ttStyle.update(styles[-1])\n\tstyle['pageBreakBefore'] = None\n\tstyle['pageBreakAfter"}]}
{"task_id": "62b87d24d292efb640a5566d", "prompt": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tif len(pieces) < 2:\n\t\t\treturn ''\n\t\tif pieces[0]!= pieces[1]:\n\t\t\treturn pieces[0] + self.combine_with_two(pieces[1:])\n\t\telse:\n\t\t\treturn pieces[0] + self.combine_with_two(pieces[1:])\n\n\tdef combine_with_two(self, pieces):\n\t\"\"\"\n\tReturn the best result to combine a list of pieces.\n\tThat is, return a list for each piece.\n\t\"\"\"\n\tif len(pieces) <= 1:\n\t\treturn pieces\n\t"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tif pieces[0] == None:\n\t\t\treturn pieces[0]\n\t\tif pieces[0].isspace():\n\t\t\treturn None\n\t\tpieces[0] = \" \" + pieces[0].strip()\n\t\tfor i in range(1, len(pieces)):\n\t\t\tif pieces[i-1].isspace():\n\t\t\t\tpieces[i-1] = \" \"\n\t\t\tif pieces[i] == '\"':\n\t\t\t\tpieces[i-1] = \"\"\n\t\t\tif pieces[i+1].isspace():\n\t\t\t\tpieces[i+1] = \" \"\n\tif pieces[0]"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tif pieces.count(\".\") == 1:\n\t\t\treturn \".\" + pieces.pop()\n\t\telse:\n\t\t\treturn \".\" + pieces.pop()\n\n\tif len(pieces) == 1:\n\t\treturn \"..\"\n\telse:\n\t\treturn \"\\n\" + \" \".join(pieces + [\"\"])\n\ndef split_list(items, sep):\n\t# split a string at semi-colon(;)\n\t# splits it into parts (separated by \";\", or no space between\n\t# items, which is the case with no space between items is a\n\t# list of strings"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t    \n\tif len(pieces)==1:\n\t\treturn pieces[0]\n\telif len(pieces)==2:\n\t\treturn pieces\n\telse:\n\t\treturn type(pieces[0])((pieces[0], pieces[1]))\n\ndef find_key(words, search_key):\n\t\"\"\"Return the index of the first occurrence of 'word_list' in 'words'.\"\"\"\n\tword_index = bisect.bisect(words, search_key)\n\treturn word_index\n\ndef read_stopwords_file():\n\t\"\"\"Read the stopwords file and make"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tif not (\"+\" in pieces):\n\t\t\treturn \"\"\n\t\tif pieces.count('-') == 1:\n\t\t\treturn pieces.split('+')[0]\n\t\telse:\n\t\t\treturn pieces\n\n\tplus_or_dot.required_tags = ['img']\n\n\t@property\n\tdef source(self):\n\t\t\"\"\"Get the source image\n\n\t\tSource images can be used to upload more images.\n\t\tUse this when adding images on the source page.\"\"\"\n\t\treturn self._get_source(\"src\")\n\n\t@property\n\tdef thumbnail(self):\n\t\t\"\"\"Get the path to the source"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\n\n\tfor piece in pieces:\n\t\tif piece is None:\n\t\t\tcontinue\n\t\tif piece.type == 'piece':\t\n\t\t\t#print piece.name, piece.text\n\t\t\tif len(pieces) == 1:\n\t\t\t\tpieces[0] = piece.name\n\t\t\t\tpieces[0].text += piece.text\n\t\t\telse:\n\t\t\t\tpieces.extend(piece.text)\n\t\telse:\n\t\t\tif piece.type == 'piece':\n\t\t\t\tpieces.append(piece.name)\n\n\tif len(pieces) > 1:\n\t\treturn join(''+ pieces[1"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tif pieces:\n\t\t\tfor piece in pieces:\n\t\t\t\tif piece: return piece\n\t\telse:\n\t\t\treturn None\n\t\t\n\tdef get_current_time():\n\t\t\"\"\"\n\t\tGet the current time, as a string (not converted through time.time()) and\n\t\ttimezone.timezone() as appropriate for pytz timezones if required.\n\t\t\"\"\"\n\t\t\n\t\treturn datetime.datetime.now().strftime(\"-%H:%M:%S UTC\")\n\t\t\n\tdef get_elapsed_seconds(seconds):\n\t\t\"\"\"\n\t\tReturn the seconds elapsed since the EPOCH clock, in seconds.\n\t\t\"\"\"\n\t\t\n\t"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\nclass Context(BaseContext):\n\t\"\"\"\n\tA sequence containing strings that can be searched for in the\n\tparser's grammar.  Can include regular and global variables and\n\tthe included expression.  The lexer is passed a rule number (starting\n\twith 0) and a context object.  Preprocessing stops when there is\n\tno more input is known.  If the expression is invalidated, raises\n\tNoContext.  The parser updates context.lineno to stop pulling in\n\ta lexer rule.\n\t\"\"\"\n\tdef __init__(self, **keywords):\n\t\tBaseContext"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\tpieces2 = copy.deepcopy(pieces)\n\t\tif pieces2 == []:\n\t\t\tpieces2 = [pieces]\n\t\tif len(pieces2) > 1:\n\t\t\tif pieces2[0][0] =='':\n\t\t\t\t# only one item, remove the whole\n\t\t\t\tpieces2.pop(0)\n\t\t\tif not pieces2[1:]:\n\t\t\t\t# remove the first item\n\t\t\t\tpieces2[0] = pieces2[1]\n\t\treturn''.join(pieces2)\n\t\n\tdef __eq__(self, other):\n\t\t\"\"\"\n\t\tSee if this token is equal"}, {"generated_text": "def plus_or_dot(pieces):\t\"\"\"\n\tReturn a + if we don't already have one, else return a .\t\"\"\"\n\t\t# TODO: not hard-coded\n\t\ttry:\n\t\t\tsplit_index = int(pieces.pop(0))\n\t\t\treturn pieces.pop(0)\n\t\texcept ValueError:\n\t\t\tsplit_index = None\n\t\tif split_index is None:\n\t\t\tsplit_index = -1\n\t\tindex_i = index_p = None\n\t\twhile index_i < len(pieces):\n\t\t\tif pieces[index_i] == '+':\n\t\t\t\tpieces.insert(index_i+1, '+')\n\t\t\telif pieces[index_i] == '-':\n\t\t\t\tpieces.insert"}]}
{"task_id": "62b87d23d292efb640a5566b", "prompt": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tlogging.debug(\"Running command {s}\".format(command=commands))\n\t\tos.chdir(cwd)\n\n\t\tif env is None:\n\t\t\tenv = os.environ\n\t\tstdout = subprocess.PIPE if hide_stderr else None\n\t\tstderr = subprocess.PIPE if hide_stderr else None\n\t\tif verbose:\n\t\t\tprint(\"Running in {s}\".format(s=commands))\n\n\t\tsys.stdout.flush()\n\t\tsys.stderr.flush()\n\n\t\tout, err = subprocess.Popen(command, bufsize=0, stdout=stdout, stderr=stderr)\n\t\tout ="}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tif type(commands) == str:\n\t\t\tcommands = split_multiline(commands)\n\n\t\tfor command in commands:\n\t\t\tif command in ('check', 'diff', 'checkdiff'):\n\t\t\t\tif verbose:\n\t\t\t\t\t_print_command(command, cmd=cwd)\n\t\t\t\tret = call(command, cwd=cwd, env=env)\n\t\t\t\tif ret and not hide_stderr:\n\t\t\t\t\tstdout = _norm_out(stdout) if stdout is not None else None\n\t\t\t\t\tprint output_command(command, ret, stdout, cwd)\n\t\t\t\treturn ret, ret\n\t\t\telif command"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tfor c in commands :\n\t\t\tc = c.strip()\n\t\t\tif'' not in c:\n\t\t\t\tcontinue\n\t\t\ttry:\n\t\t\t\tchild = subprocess.Popen(c, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\t\t\texcept Exception, e:\n\t\t\t\tif verbose :\n\t\t\t\t\tprint >> sys.stderr, \"failed\\ncommand: %s\" % c\n\t\t\t\traise e\n\t\t\tchild.args = args\n\t\t\tself.children.append(child)\n\t\t\tchild.wait()\n\t\t\tif not hide_stderr:\n\t\t\t\tc = child."}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tif not commands:\n\t\t\treturn []\n\t\tout, err = run(commands, args, cwd, env=env)\n\t\tif verbose:\n\t\t\tprint 'Command output:'\n\t\t\tprintf('%s%s%s' % (out[0], out[1], out[2]))\n\t\t\tprint out\n\t\treturn out\n\n\tdef _print_success(mytxt):\n\t\tret = True\n\t\tfor msg in mytxt:\n\t\t\tif debug:\n\t\t\t\tcolor(msg)\n\t\t\tif isinstance(msg, string_types):\n\t\t\t\tret = False\n\t\t\t\tmsg = colorize("}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\traise NotImplementedError\n\n\tif isinstance(commands, basestring):\n\t\tcommands = split_multiline(commands)\n\t\n\tif not isinstance(commands, (list, tuple)):\n\t\tcommands = str(commands)\n\t\n\tfor c in commands:\n\t\tif verbose:\n\t\t\tprint \"Running\", c, \"-\", cwd,\n\t\tproc = subprocess.Popen(c, stdout = subprocess.PIPE, shell = True, cwd = cwd)\n\t\tif hide_stderr:\n\t\t\tout, err = proc.communicate()\n\t\telse:\n\t\t\tout = []\n\t\t\terr = b\"\"\n\t\tif proc."}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tif env is None:\n\t\t\tenv = os.environ\n\t\telse:\n\t\t\tenv = dict(os.environ, **env)\n\t\tprint(\"Running command\", \" \".join(commands), \" \".join(args))\n\t\tif verbose:\n\t\t\tif isinstance(commands, str):\n\t\t\t\tprint(\"Running command %s\" % commands, file=sys.stderr)\n\t\t\telse:\n\t\t\t\tprint(commands, file=sys.stderr)\n\t\tif hide_stderr:\n\t\t\tsubprocess.check_call(commandline)\n\t\ttry:\n\t\t\tif isinstance(commands, str):"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\t# this should make all shell output work, even if commands is empty\n\t\tif not commands:\n\t\t\treturn (False, None)\n\n\t\t# If no cwd -- it gets invoked directly\n\t\targs = shlex.split(args)\n\n\t\t# If extra_ignorecodes is set the command will be a list of\n\t\t# (command, args, returncode) tuples\n\t\tif extra_ignorecodes and len(args) > 1 and args[1] not in extra_ignorecodes:\n\t\t\targs[1] = extra_ignorecodes\n\n\t\tif verbose:\n\t\t\tprint(u\"[%s"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\t# set cwd, set process ID before commands\n\t\tif cwd is None:\n\t\t\tcwd = os.getcwd()\n\t\tos.chdir(cwd)\n\t\tsys.argv = args\n\n\t\tprint \"Running command:...\"\n\t\tlogging.debug(\" \".join(commands))\n\n\t\tp = Popen(commands, stdout=PIPE, stderr=PIPE)\n\n\t\tstdout = []\n\t\tstderr = []\n\t\tif quiet:\n\t\t\tstdout, stderr = p.communicate()\n\t\telse:\n\t\t\tp.wait()\n\n\t\t\tif not p.returncode:\n\t\t\t\tstdout, stderr = p.communicate()"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tif verbose:\n\t\t\tprint(commands)\n\n\t\tprocess = None\n\n\t\tif root is None:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# pragma: no cover\n\t\t\troot = None\n\n\t\tif cwd is not None:\n\t\t\tcwd = cwd.rstrip(os.path.sep)\n\t\t\tos.chdir(cwd)\n\n\t\tfor c in commands:\n\t\t\tc = c.strip()\n\t\t\tif c == \"\":\n\t\t\t\tcontinue\n\t\t\tif os.path.isfile(c):\n\t\t\t\tprocess = Popen(c, shell=True, stdout=PIPE, stderr=PIPE, cwd=cwd)\n\t\t\t\t(out"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tproc = Popen(commands, cwd=cwd, stdout=PIPE, stderr=PIPE, env=env)\n\t\tif not quiet:\n\t\t\tout = Proc(proc).communicate()[0]\n\t\telse:\n\t\t\tout = None\n\t\tif verbose:\n\t\t\tif not out: print \"\"\n\t\t\tif hide_stderr:\n\t\t\t\tfor line in out.splitlines(): print line\n\t\t\tout = out.strip()\n\t\tif not hide_stderr and proc.returncode!= 0:\n\t\t\tif verbose: print out\n\t\treturn out\n\nclass Commands(object):\n\t\"\"\"\n\tClass"}]}
{"task_id": "62b87d23d292efb640a55668", "prompt": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\tconfig = VersioneerConfig()\n\t\tconfig.VCS = \"git\"\n\t\tconfig.style = \"pep440\"\n\t\tconfig.tag_prefix = \"v\"\n\t\tconfig.parentdir_prefix = \"\"\n\t\tconfig.versionfile_source = \"versioneer.py\"\n\t\tconfig.verbose = False\n\t\treturn config\n\n\nclass NotThisMethod(Exception):\n\t\"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\n\tpass\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method): "}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\t# these strings are filled in when'setup.py versioneer' creates\n\t\t# _version.py\n        cfg = VersioneerConfig()\n        cfg.VCS = \"git\"\n        cfg.style = \"\"\n        cfg.tag_prefix = \"\"\n        cfg.parentdir_prefix = \"git:\"\n        cfg.versionfile_source = \"c6d/_version.py\"\n        cfg.verbose = False\n        return cfg\n\n    def get_cmdclass(self, cmdclass):\n        \"\"\"\n        Creates the class object that represents the generated command line\n        arguments.\n\n        Args:\n          cmdclass (str): the name of"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\tconfig = VersioneerConfig()\n\t\tconfig.VCS = \"git\"\n\t\tconfig.style = \"pep440-pre\"\n\t\tconfig.tag_prefix = \"v\"\n\t\tconfig.parentdir_prefix = \"None\"\n\t\tconfig.versionfile_source = \"none\"\n\t\tconfig.verbose = False\n\t\treturn config\n\n\nclass NotThisMethod(Exception):\n\t\"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\tconfig = VersioneerConfig()\n\t\tconfig.VCS = \"git\"\n\t\tconfig.style = \"pep440\"\n\t\t# add_config() adds configuration options\n\t\tconfig.add_section(\"versioneer\")\n\t\tconfig.add_section(\"find_links\")\n\t\tconfig.add_section(\"optimize\")\n\t\tconfig.add_section(\"tag_prefix\")\n\t\treturn config\n\n\nclass NotThisMethod(Exception):\n\t\"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\n# these dictionaries contain VCS-specific tools\nLONG_VERSION_PY = {}\n"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\t# these strings are filled in when'setup.py versioneer' creates\n\t\t# _version.py\n        cfg = VersioneerConfig()\n        cfg.VCS = \"git\"\n        cfg.style = \"pep440\"\n        cfg.tag_prefix = \"v\"\n        cfg.parentdir_prefix = u\"bokeh-\"\n        cfg.versionfile_source = \"bokeh/_version.py\"\n        cfg.verbose = False\n        return cfg\n\n    def get_cmdclass(self, cmdclass):\n        \"\"\"\n        Provide a cmdclass to create new instances of the VersioneerConfig\n        class.\n\n        :param"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\tglobal VERsioneerConfig\n\t\tCONFIG = VersioneerConfigParser(\n\t\t\tname=NAME,\n\t\t\tversionfile_source=VERSIONFILE_SOURCE,\n\t\t\tversionfile_build=VERSIONFILE_BUILD,\n\t\t\t# The following file representations are not available on PyPi.\n\t\t\tdo_vcs=VCS,\n\t\t\ttags=TAG,\n\t\t\tcategory_order=CATEGORY_ORDER,\n\t\t\tauthor=AUTHOR,\n\t\t\tauthor_email=AUTHOR_EMAIL,\n\t\t\tcopyright=COPYRIGHT,\n\t\t\tlicense=LICENSE,\n\n\t\t\t# The project's main homepage.\n\t\t\turl=URL,\n\t\t\t# Author"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\tconfig = VersioneerConfig()\n\t\tconfig.VCS = VCS\n\n\t\t# This should be updated whenever'setup.py versioneer' creates\n        # _version.py\n        cfg = VersioneerConfig()\n        cfg.VCS = VCS\n        cfg.style = get_style_guide(versionfile_source)\n        cfg.tag_prefix = tag_prefix\n        cfg.parentdir_prefix = parentdir_prefix\n        cfg.versionfile_source = readme_file\n\n    #AUTORELOAD_YAML = True,\n    # We don't want to use the EasyInstallAssertions extension instead.\n    # Some of the"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\timport distutils.version\n\t\t# must be done in sdist_sdist\n        try:\n            from multiprocessing.common import (check_output, check_call)\n        except ImportError:\n            from multiprocessing.version import VERSION as multiprocessing_version\n            # For DistributionMetadata, setup the VERSION and install that\n            # setuptools version\n            version = Version(multiprocessing_version)\n            config = get_config_from_options()\n            # must be done in sdist_sdist\n            check_call([\"python\", \"-c\",\n                       \"import setuptools; __version__=\\\"\\\" {}\\\"\".format(version),\n                       \"setup.py\", \""}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\tconfig = VersioneerConfig()\n\t\tconfig.VCS = \"git\"\n\t\t# The following line had a bug that causes'version' to not be\n\t\t# compliant with _version.py. In particular, assume the version was derived\n\t\t# from a number, but may rely on agit to evaluate inline versions\n\t\t# somehow, so we leave it alone for now.\n\t\tversion = get_config().version\n\t\tif version is None:\n\t\t\traise NotThisMethod(\"no version\")\n\t\t# skip config files that are not set up to hold user packages\n\t\tif \"versioneer"}, {"generated_text": "def get_config():\t\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\t\"\"\"\n\t\traise NotImplementedError\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n\t\"\"\"Get the project version from the parent directory name.\n\n\tSource tarballs conventionally unpack into a directory that includes both\n\tthe project name and a version string. We will also support searching up\n\ttwo directory levels for an appropriately named parent directory\n\t\"\"\")\n\trootdirs = []\n\n\trootdirs.append(root)\n\trootdirs.append(config.versionfile_source)\n\trootdirs.append(root)\n\n\t# Source tarballs conventionally unpack into a directory"}]}
{"task_id": "62b87d23d292efb640a55667", "prompt": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef inner(f):\n\t\t\tvcs.connect(vcs._create_terminal)\n\t\t\tmethod(f)\n\t\treturn inner\n\treturn register_vcs_handler\n\n\ndef _check_required_packages():\n\timport pkg_resources\n\timport setuptools.command.install_lib2to3_fixers\n\timport sys\n\tfor req_py in pkg_resources.iter_entry_points(\n\t\t\tgroup='setuptools',\n\t\t\tname='reqpy',\n\t\t\tmember='setuptools.command'):\n\t\tif sys.hexversion < 0x03000000 and req_py.attr"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(func):\n\t\t\tsetattr(vcs, method, func)\n\t\t\treturn func\n\t\treturn decorate\n\treturn decorate\n\ndef install_scm_handler(vcs, method):\n\t\"\"\"\n\tMark a method as the handler of a VCS that will perform its own\n\tinstallation process. Note that this makes it nicer to call this\n\tmethod in your own vcs class!\n\t\"\"\"\n\tdef decorate(func):\n\t\tsetattr(vcs, method, func)\n\t\treturn func\n\treturn decorate\n\ndef uninstall_scm_handler(vcs, method):\n\t\"\"\"\n"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(f):\n\t\t\tvcs.register_vcs_handler(method, f)\n\t\t\treturn f\n\t\treturn decorate\n\treturn decorate\nimport io\nimport sys\nimport os\nimport unittest\n\nfrom coalib.results.Result import Result\nfrom coalib.settings.Section import Section\n\n\nclass TestBear(unittest.TestCase):\n\n    def setUp(self):\n        self.out_stream = io.StringIO()\n        self.sections = {}\n        self._do_section = Section('name')\n        self._do_section.add_option('enable', None, 'Enable setting"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(f):\n\t\t\tvcs.register_vcs_handler(method, f)\n\t\t\treturn f\n\t\treturn decorate\n\n\tdef run_command(self, cmd, args, cwd = None, quiet = False,\n\t              debug = False, stdout = PIPE,\n\t              stderr = STDOUT):\n\t\t\"\"\"\n\t\tExecute a VCS command.\n\t\tIf cwd is not empty, then changes to cwd are processed first.\n\t\tOtherwise, run_command must be fed (self.run(**args), cwd=cwd).\n\t\t\"\"\"\n\t\t# Note that'self.run(**args"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(f):\n\t\t\tvcs.register_vcs_handler(method, f)\n\t\t\treturn f\n\t\treturn decorate\n\treturn decorate\n#encoding: utf8\nfrom __future__ import print_function, unicode_literals\nimport sys,json\nfrom collections import OrderedDict\nimport six\nreload(sys)\nsys.setdefaultencoding('utf-8')\nimport os\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../lib')))\n\nimport py_const as pyconst\nBASE_"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(f):\n\t\t\t\"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n\t\t\tif vcs not in HANDLERS:\n\t\t\t\tHANDLERS[vcs] = {}\n\t\t\tHANDLERS[vcs][method] = f\n\t\t\treturn f\n\t\treturn decorate\n\n\treturn register_vcs_handler\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tExecute a list of commands, and return the output. The output\n\ttype is returned as a list, with the first item in which contains"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(f):\n\t\t\tvcs.register(f, method)\n\t\t\treturn f\n\n\t\treturn decorate\n\n\tdef run_command(self, command, args=None):\n\t\t\"\"\"\n\t\tRun the VCS command line as a process. Set the VCS status, \n\t\tand emit an \"info\" message and return it.\t\t\t\n\t\t\n\t\t:command: command line to be executed\n\t\t\"\"\"\n\n\t\tlogger.debug(\"Running %s\" % command)\n\n\n\t\tfrom datetime import datetime\n\t\t\n\t\t# If this is a submodule, make sure it's handled as a VCS\n\t\tif self.config"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(f):\n\t\t\tdef wrapped_f(*args, **kwargs):\n\t\t\t\tvcs_handler.vcs_handlers[vcs][method] = f(*args, **kwargs)\n\t\t\t\treturn vcs_handler.vcs_handlers[vcs][method]\n\t\t\treturn wrapped_f\n\t\treturn decorate\n\n\tdef register_vcs_error_handler(vcs, vcs_error_code):\n\t\tdef decorate(f):\n\t\t\tvcs_error_code_handler.vcs_error_handlers[vcs] = f\n\t\t\treturn vcs_error_code_handler.vcs_error_handlers["}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(f):\n\t\t\tvcs.register_vcs_handler(method, f)\n\t\t\treturn f\n\t\treturn decorate\n\n\treturn register_vcs_handler\n\ndef get_fallback_handler(vcs):\n\tvcs_handler = None\n\n\tif vcs:\n\t\tvcs_handler = vcs.fallback_handler\n\n\treturn vcs_handler\n\ndef init_vcs (vcs, update=True):\n\t# NOTE: Must be called before importing any other VCS instances.\n\n\tlogger = create_logger(__name__)\n\n\tif update or vcs.check_requirements():\n\t\tlogger"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\t\"\"\"\n\t\tdef decorate(f):\n\t\t\tvcs.register(f, method)\n\t\t\treturn f\n\t\treturn decorate\n\n\treturn decorate\n\n\ndef get_supported_locales():\n\t\"\"\"Returns a list of supported locales.\"\"\"\n\tlocales = []\n\tfor directory in os.listdir(settings.LOCALE_DIR):\n\t\tpath = os.path.join(settings.LOCALE_DIR, directory)\n\t\tif os.path.isdir(path) and path!= '__init__.py':\n\t\t\tif os.path.exists(os.path.join(path, '__init__.py')):\n\t\t\t\t"}]}
{"task_id": "62b45e945108cfac7f210a4a", "prompt": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n        try:\n                    self.validate_data()\n        except InvalidFileDataError:\n            logger.debug('Valid storage root tree has invalid data: %s' % content_hash)\n\n\t    return 0\t\n\t\t\t\n        if content_hash!= self.content_hash:\n            logger.debug('Valid storage root tree has data invalid: %s' % content_hash) \n\n\n        # check the stored root object in case of errors\n        try:\n            self.hash_object()\t\n            num_objects = len(self.data_info['trees'])\n            logger.debug('')\n            logger.debug('VALID storage root"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n    if DEBUG:\n\t    print '-------------------------------'\n\t    print 'validate_hierarchy'\n\t    print 'num obj (for checking and existence)      :', num_objects\n\t    print 'good objects (for checking and existence)'\n\t    print 'check digests (for checking objects)'\n\t    print'show warnings (for checking objects)'\n\t    print '================================================'\n\tprint 'obj_name(in) : object  is of type %s' % self.name\n\n\tassert validate_objects, \"ERROR: objects found to be valid\"\n\tassert show_warnings, 'WARNING: objects found to be invalid'\n\t"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n#########################################\n#   Validators\n#########################################\nfrom validate_validators import *\n\"\"\"\nTests of the neo.core.neuron import Neuron, Synapse\n\"\"\"\nimport copy\nimport unittest\n\nfrom copy import deepcopy\n\nimport numpy as np\n\ntry:\n    import matplotlib.pyplot as plt\n\n    have_matplotlib = True\nexcept:\n    have_matplotlib = False\n\nhave_matplotlib.__version__ >= '0.7'\n\nfrom neo.core import (core,\n                      adict,\n                      neo.core.container,\n                      all_to_all,\n                     "}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n        num_objects = len(validation_errors_in_directory.keys())\n\t\t\n\t\t#if len(validation_errors_in_directory) > 0:\n\t\t#\traise ValueError('There were {} objects that are not present in storage'.format(len(validation_errors)))\n\t\t\n\t\tfor i in range(0, num_objects):\n\t\t\tobjects_errors = validation_errors_in_directory_for_this_directory.get(validation_errors_in_directory.keys()[i])\n\t\t\tobjects_ok = True\n\t\t\tfor file_name in utils.get_file_paths"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\t# Find all the objects\n\t\tobjs = self.filter(name__startswith=self.parent_path)\n\t\t# Now find all the matching trees\n\t\ttree_paths = [self._get_obj_path(c) for c in objs]\n\n\t\tassert len(tree_paths) == len(self.tree.leaves())\n\t\tif not show_warnings:\n\t\t\tif validate_objects:\n\t\t\t\tfor obj_path in tree_paths:\n\t\t\t\t\tobj = self.get(obj_path)\n\t\t\t\t\tif obj.is_valid():\n\t\t\t\t\t\tself._verify_object("}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\t# Validate the hierarchy structure\n\t\tif not isinstance(self.storage_roots, list):  # No collection given\n\t\t    raise ValueError(\"Invalid hierarchy structure in storage root\")\n\t\tnum_objects = len(self.storage_roots)\n\t\tgood_objects = len([m for m in self.storage_roots if m.valid()])\n\t\tif num_objects!= good_objects:\n\t\t\tvalidate_object_parents(self.storage_roots, self.good_objects, num_objects)\n\t\t\tif show_warnings:\n\t\t\t\tLOGGER.warning(\"Warning: %s incorrect hierarchy\", self"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n\n        # Check all objects for errors\n        if not self.storage.is_valid():\n            return []\n        else:\n            if self.verbose:\n                self.info(\"Validating storage hierarchy '%s'\" % self.storage.get_name())\n \n            # Check digests \n            if self.validate_md_digests(check_digests):\n                return self.good_objects\n\n            # Check warning message\n            if show_warnings:\n                self.warning(\"Validation of %d objects.\" % len(self.storage.get_objects()))\n                self.warning(\"... Regenerating error report.\")\n                self."}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\n        return num_objects(self.conn, self.db_dict, \"root\")\n\n    def _get_rootid_from_path(self, path):\n\n\t\tobj = self.lookup_path(path)\n\t\tif obj is None:\n\t\t\treturn None, None\n\n\t\treturn obj.getparent().lstrip(\"/\")\n# -*- coding: utf-8 -*-\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\tself.num_objects = len(self.objects_with_acls_in_cache)\n\t\tself.good_objects = 0\n\t\tgood_hashes = self.object_hashes\n\t\tgood_length = len(self.hashes)\n\t\tif not validate_objects:\n\t\t\tself.objects = [None, None]\n\t\telse:\n\t\t\tself.total_size = 0\n\t\tfor key in self.object_hashes:\n\t\t\tif key not in self.hashes:\n\t\t\t\tself.total_size += self.object_hashes[key].size()\n\t\tif self"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\t\"\"\"\n\t\t# check the values of self fields\n\t\tfor obj in self._tree.iter_children_objects(self._oid_base + \".\" + self._name, self._oid_base):\n\t\t\tchild = getattr(obj._children, \"_oid_object\")\n\t\t\t# check if this is one of the expected trees\n\t\t\tif validate_objects:\n\t\t\t\tvalidate_data(validate_objects, obj, child)\n\t\t\t# check if we should stop the iteration\n\t\t\tif child._oid_object is None:\n\t\t\t\tbreak\n\t\t\tif child._name!= \"_oid_object\":\n\t\t\t\tcontinue"}]}
{"task_id": "62b45e515108cfac7f210a3c", "prompt": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\n\n\tdef destroy(self):\n\t\t\"\"\"\n\t\tDestroy the storage folder created earlier in storage dir.\n\t\t\"\"\"\n\t\tself.m_o_c_o.destroy()\n\n\tdef commit(self):\n\t\t\"\"\"\n\t\tCommit the last transaction.\n\t\t\"\"\"\n\t\tself.m_o_c_o.commit()\n\n\tdef rollback(self):\n\t\t\"\"\"\n\t\tRollback the last transaction.\n\t\t\"\"\"\n\t\tself.m_o_c_o.rollback()\n\n\tdef create(self, value):\n\t\treturn self.m_o_c_"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tself.storage.initialize()\n\tdef close(self):\t\"\"\"\n\tClose the storage root.\n\tDoes not remove the file object for the storage object if any.\n\t\n\tNote we do not automatically close the storage object if it\n\texists.\n\t\"\"\"\n\tself.storage.close()\n\tself.storage.close()\n\tself.storage = None\ndef write(message):\n\t\"\"\"\n\tWrite a python object to the storage.\n\t\n\tMessage should be of the format \"<prefix> <data>\"\n\t\n\t\"\"\"\n\tmessage_str = message + \"\\0\""}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\t# make sure we have an existing root before continuing:\n\t\ttry:\n\t\t\tself._run_command(\"umount -n -- NoRecurse\", expected=1)\n\t\tfinally:\n\t\t\t# Remove the directory we're going to leave behind:\n\t\t\tself._run_command(\"rmdir\", path=self.storage_root)\n\n# vim: set ts=4 sw=4 noet:\n# coding=utf-8\nr\"\"\"\nThis code was generated by\n\\ / _    _  _|   _  _\n | (_)\\/(_)(_|\\/"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\treturn ocio_root.OcioConfRoot()\n\n\tdef destroy(self):\n\t\t\"\"\"\n\t\tDestroy an OCFL storage folder.\n\t\t\"\"\"\n\t\tocio_root.OcioConfRoot().destroy()\n\n\tdef save(self, path=None):\n\t\t\"\"\"\n\t\tSave new OCFL storage to the specified path name.\n\t\t\"\"\"\n\n\t\tif not path:\n\t\t\texport_dir = 'c:/temp/ociofsbackup/'\n\t\telse:\n\t\t\texport_dir = path\n\n\n\t\t# Determine directory path\n\t\tdirectory = ocio_root"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tself._logger = logging.getLogger('ocfl')\n\n\tdef cleanup(self):\n\t\t\"\"\"\n\t\tStop our OCFL instance.  Subclasses can override this method.\n\t\tDo any required network cleanup.\n\t\t\"\"\"\n\t\tpass\n\n\tdef is_available(self):\n\t\t\"\"\"\n\t\tCheck if we can do OCFL.\n\t\tDo some initialization validation.\n\t\tAny exceptions may happen here.\n\t\t\"\"\"\n\t\traise NotImplementedError(\"To be implemented in subclass.\")\n\t\n\tdef is_in_file(self, in_file):\n\t\t\"\"\"\n\t\tCheck if the provided"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tif self.__o_options.initialize: \n\t\t\treturn\n\t\t\n\t\t## Create an instance of OCFL storage root.\n\t\tself.__o_options.initialize = True\n\t\t\n\t\t# Create an instance of OSFileNode.\n\t\troot = OCFLStorageNode()\n\t\t\t\t\n\t\t# Create an instance of OSDirectoryNode.\n\t\tif OCFLStorageNode.h_is_root(self, None):\n\t\t \n\t\t\traise ObitbakeError(self, GENERIC_ERROR, \"Cannot create root directory.\")\n\t\t\n\t\t# Create an instance of OSFileContainer.\n\t\tif OC"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tlogger.debug(\"Initialize OVERWRFStorage root\")\n\n\tdef destroy(self):\n\t\t\"\"\"\n\t\tDisconnect from the storage.\n\t\t\"\"\"\n\t\tlogger.debug(\"Disconnecting from OVERWRFStorage root\")\n\t\tself.delete_entry(\"root\")\n\t\tself.session.unlock();\n\t\tself.session = None\n\n\tdef _parse_options(self):\n\t\t\"\"\"\n\t\tParses the command-line options.\t\n\t\t\"\"\"\n\n\t\tparser = optparse.OptionParser(prog = \"samba-tool-drs\", usage = \"\", add_help_option=True"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tif not os.path.exists(self._OCF_ROOT):\n\t\t\tos.makedirs(self._OCF_ROOT, 0755)\n\n\t\t# Initializes all the components\n\t\tfor name in [\n\t\t\t (\"device\", self._device),\n\t\t\t (\"vendor_id\", 0),\n\t\t\t (\"product_id\", 0),\n\t\t\t (\"firmware_version\", 0),\n\t\t\t (\"firmware_build\", 0),\n\t\t\t]:\n\t\t\tif not os.path.exists(os.path.join(self._OCF_ROOT, name)):\n\t\t\t\traise EnvironmentError(\"cannot create OCFL storage root\")\n\n\t"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tsuper(OCFS, self).initialize()\n\t\tself.update_root_hash()\n\t\t#self.add_root_hash_row('file hash', hash_id=self.id.digest('md5'))\n\t\tself.add_root_hash_row('size', size=0)\n\t\tself.add_root_hash_row('creation_time', int(time.time() * 1000000))\n\t\tself.add_root_hash_row('owner_id', owner_id=self.id.owner_id) #self.id.owner_"}, {"generated_text": "def initialize(self):\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\t\"\"\"\n\t\tself._data = []\n\t\tself._index = None\n\n\tdef close(self):\n\t\t\"\"\" Close the root resource. \"\"\"\n\t\tfor node in self._index['nodes']:\n\t\t\tdel node\n\t\tself._index = None\n\n\tdef add_node(self, node):\n\t\t\"\"\" Add a node to the root. \"\"\"\n\t\tif self._data:\n\t\t\tif self._data[-1]['node'] == node:\n\t\t\t\traise ValueError('node exists')\n\t\telse:\n\t\t\tself._data.append(node)\n\n\tdef get_node(self, node):\n\t"}]}
{"task_id": "62b45e2eb89c9fd354170232", "prompt": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\tif \"_\" in version and not re.match(r\"^[^a-zA-Z0-9\\-]\", version):\n\t\t\traise ValueError(\"Invalid git number '%s'\" % version)\n\t\telif \"-\" in version and not re.match(r\"^[^0-9A-Fa-f]+$\", version):\n\t\t\traise ValueError(\"Invalid git number '%s'\" % version)\n\t\telse:\n\t\t\treturn version\n\telse:\n\t\treturn \"%s.%s\" % (VERSION_PREFIX[2], version)\n\n# Return string with date+time like \"YYYY-MM-DD HH"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\tparts = version.split('.')\n\t\tif len(parts)!= 2:\n\t\t\traise KeyError(\"Illegal version, {}\".format(version))\n\t\treturn int(parts[0]) + 1\n\ndef get_version(version):\t\"\"\"\n\tReturn version from the input package.\n\nReturns 0 if version is not found or version could not be\ndetermined.\n\"\"\"\n\tv = versions_for(version, lambda v: v >= 0)\n\n\tdef prefix_version(v):\n\t\tif v.count('.') == 0:\n\t\t\tv_list = v.split('.')\n\t\t"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\tif version is None:\n\t\t\treturn ''\n\t\tm = 0\n\t\twhile True:\n\t\t\tif version[m] == '0':\n\t\t\t\tbreak\n\t\t\tm += 1\n\t\treturn version[m:]\n\n# Return maximum version that can be used as a template.\n# (This value is in the range of all known version).\ndef max_version(version):\n\t\"\"\"\n\tMaximal version that can be used as a template.\n\n\tThe tuple contain format like \"0.6.4\" (\"0\", \"1\")\n\tetc.\n\t\"\"\"\n\tmax"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t # TODO: remove after the first non-null `v`.\n\tif version.count('.') > 1:\n\t\tnew_version = version.split('.')\n\t\tversion = new_version[0] + ''.join(['0']*(len(new_version)-2)) + version\n\treturn version\n\nclass VersionGenerator():\n\tdef __init__(self, version_path, output_dir = '.', ignore_extension = True):\n\t\tself.version_path = version_path\n\t\tself.output_dir = output_dir\n\t\tself.ignore_extension = ignore_extension\n\n\t"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\tmatch = re.match(reversion, version)\n\t\tif match is None:\n\t\t\traise InvalidVersion(value=version)\n\t\treturn {match.group(1): int(match.group(2))}\n\nclass BuildOptions:\n\tdef __init__(self, options, settings, trees):\n\t\tself.__options = options\n\t\tself.__settings = settings\n\t\tself.__trees = trees\n\n\tdef __getitem__(self, k):\n\t\treturn BuildOption(self.__options, k, self.__settings[k])\n\n\tdef __setitem__(self, k, v"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\n\n\tif len(version) < 2:\n\t\traise ValueError('invalid version')\n\telif version[0] == 0:\n\t\treturn 0\n\n\tif len(version) == 2:\n\t\tif version[1] == 0:\n\t\t\tversion = version[0] + 1\n\t\telse:\n\t\t\tversion = version[1] + 1\n\t\t\n\t\tnext_index = version[0] + 1\n\t\t\n\t\twhile next_index < len(version):\n\t\t\tif version[next_index] == 0:\n\t\t\t\tbreak\n\t\t\t\n\t\t\tnext_index += 1\n\t\t\n\t\tindex = version[0"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\tif not version: return 0\n\t\t\n\t\tversion = map(int, version.split('.'))\n\t\t\t\n\t\tif len(version) == 1:\n\t\t\tversion = (version + [0])\n\t\t\t\n\t\treturn len(version)\n\t\t\n\ttry:\n\t\tif int(version[0]) == 1:\n\t\t\treturn version[0]\n\t\telif int(version[0]) > 999:\n\t\t\traise ValueError\n\texcept (ValueError, IndexError):\n\t\tpass\n\t\t\n\t# All versions have 'alpha', where the 'alpha' or 'beta' must be valid\n\tif int(version[1])"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\tif (version[0] == \"_\"):\n\t\t\tsuffix = \"_\"\n\t\telif (version[0] == \"$\"):\n\t\t\tsuffix = \"-\"\n\t\telse:\n\t\t\tsuffix = \".\"\n\t\t\n\t\treturn str(\"%s%s\" % (version, suffix))\n\t\n\t\n\tdef get_next_versions(self, version):\n\t\t\"\"\"\n\t\tReturn a 4-tuple:\n\t\t\t\n\t\t\t- (version, identifier)\n\t\t\t\t- version is a 4-tuple:\n\t\t\t\t\t- 0 = \"0\"\n\t\t\t\t\t- 1 = \"1\"\n\t\t\t\t\t- 2 = \"2\"\n\t\t\"\"\"\n\t\t\n\t\tif"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\ndef next_version_no_suffix(version):\t\n\t\"\"\"\n\tNext version identifier following missing pattern.\n\nMust deal with both non-zero-prefixed (non-zero) prefix versions.\t\"\"\"\n\t\ndef next_version_no_suffix_no_prefix_no_versions(version):\t\n\t\"\"\"\n\tNext version identifier following missing pattern without version prefix/version prefix.\n\nMust deal with non-zero non-zero prefixed version prefixes with no version prefix.\t\"\"\"\n\ndef next_version_no_suffix_no_prefix_suffix(version):\t\n"}, {"generated_text": "def next_version(version):\t\"\"\"\n\tNext version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\t\"\"\"\n\t\twhile version[0] == '0' and version[1]!= '.':\n\t\t\tversion = '.'.join(version[2:])\n\treturn version\n\ndef get_version():\n\t\"\"\"\n\tReturn the next version identifier (PEP 408) with an\n\tempty final string in parentheses. This supports incrementing a\n\tnumber when it reaches zero as well as incrementing zero when the\n\tfirst character on the list.\n\t\"\"\"\n\tv = next(sys.version_info)\n\tif not v[2]:\n\t\traise ValueError\n\tfirst, rest = v["}]}
{"task_id": "62b45e23e0d4551b0392c90a", "prompt": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tfor version in version_dirs:\n\t\t\tverrec = open(os.path.join(self.root_dir, \"version.{0}\".format(version.lower())))\n\t\t\tfor line in verrec.readlines():\n\t\t\t\tif line.strip().startswith(\"#\"):\n\t\t\t\t\tcontinue\n\t\t\t\telse:\n\t\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\traise ParseError(\"Unexpected file format version line.\")\n\n\t\t\tversion = \"{0}.{1}\".format(version[:version.find(\".\",3)], version[version.find(\".\",1)])\n\t\t\tversion_dirs.append(version)\n\n\t\tself[\"version\"] = version"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tvalid_versions = set()\n\t\tversion_dirs = set([version_dirs])\n\t\tfor version in self._walk(version_dirs):\n\t\t\tif self.get_version(version) not in valid_versions:\n\t\t\t\tvalid_versions.add(self.get_version(version))\n\t\tif not is_valid_version_directory(version_dirs):\t# No valid versions\n\t\t\treturn\n\n\tdef get_root_versions(self):\n\t\t\"\"\"\n\t\tGet all version directories, and their parents. Return a list\n\t\tof all root versions without traversing order"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tif not isinstance(version_dirs, list):\n\t\t\traise InvalidVersion(\"version_dirs must be a list\")\n\n\t\tif len(version_dirs)!= len(set(version_dirs)):\n\t\t\traise InvalidVersion(\"Version directories must be unique\")\n\n\t\t# Check that each entry type has the correct format\n\t\tversion_dirs = filter(lambda x: not x.startswith(\"version\"), version_dirs)\n\t\tfor entry in version_dirs:\n\t\t\tif not self._valid_entry_type(entry[0]):\n\t\t\t\traise InvalidVersion(\"Entry type {} must start with v{}"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tfor version_dir in version_dirs:\n\t\t\tfor root in portage.settings[\"PORTAGE_RDEPEND\"].strip().split():\n\t\t\t\ttry:\n\t\t\t\t\tvarexer_digest = canonical_version_tuple(root)\n\t\t\t\texcept UnicodeEncodeError:\n\t\t\t\t\tcontinue\n\t\t\t\tif not version_dir in portage.const.VERSION_DIRS.split(\",\") or \\\n\t\t\t\t\tversion_dir not in varexer_digest:\n\t\t\t\t\tcontinue\n\n\t\t\t\tebuild_path = portage.versions.cpv_getkey(varexer_digest,\n\t\t\t\t\t[\"repo_name\"], \"E"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\traise NotImplemented(\"Implement in subclass\")\n\n\tdef validate(self, scheduler, state, content):\n\t\t\"\"\"\n\tValidate the given content against the content in the root inventory.\n\n\t@param state: the state\n\t@type  state: Dictionary\n\t@param content: the contents of the inventory\n\t@type  content: string\n\t@raises: validation_error if the content is not valid.\n\t\"\"\"\n\t\t# No content to validate.\n\t\tif content is None:\n\t\t\traise MissingContent(\"No data received\")\n\n\t\t# Must be a sequence of sequentially increasing"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tfor i in range(len(version_dirs)):\n\t\t\tversion_dir = version_dirs[i]\n\t\t\tfor j in range(i+1):\n\t\t\t\tif version_dir.count('/') == 0:\n\t\t\t\t\t# the v-1.0 symlink here is a bad design for a valid\n\t\t\t\t\t# directory\n\t\t\t\t\tcontinue\n\t\t\t\tp = join(version_dir, v.rstrip(\"/\"))\n\t\t\t\twith open(p, 'rb') as f:\n\t\t\t\t\tversion = f.read(len(version_dir))\n\t\t\t\t\tif i == 0: # v-1.0 symlink"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tfor version in version_dirs:\n\t\t\tfor inventory_dir in self.__get_inventory_dir(version):\n\t\t\t\t# See if there is an inventory directory this shouldn't\n\t\t\t\t# be, something has gone wrong\n\t\t\t\tself.__update_inventory_dir(inventory_dir)\n\n\t\tfor source_dir in self.__get_code_dir(version[1], version[2]):\n\t\t\tself.__update_code_dir(source_dir)\n\n\tdef validate(self):\n\t\t\"\"\"\n\t\tValidate this inventory and return a hash of it (assuming\n\t\tthat the inventory needs"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tcurrent_version_dirs = []\n\t\tseen_version_dirs = []\n\t\tall_version_dirs = version_dirs\n\t\t# first, add all possible directories to each inventory\n\t\tfor x in version_dirs:\n\t\t\tfor i in range(0, len(all_version_dirs)):\n\t\t\t\tif find_inventory_dir(all_version_dirs[i]):\n\t\t\t\t\tmatch = self.inventory_dir_regex.match(x)\n\t\t\t\t\t# find all the version of this dir\n\t\t\t\t\tcurrent_version_dirs.append(match.group(2))\n\t\t\t\t\tall"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tversion_dir = os.path.join(*version_dirs)\n\t\tvalid_dirs = []\n\t\tfor dir in version_dirs:\n\t\t\tif os.path.isdir(dir):\n\t\t\t\tvalid_dirs.append(dir)\n\n\n\t\tseen = set()\n\t\tinvalids = []\n\t\tfor dir in valid_dirs:\n\t\t\tfor name in os.listdir(dir):\n\t\t\t\tp = os.path.join(dir, name)\n\t\t\t\tif not name.startswith('.'):\n\t\t\t\t\tcontinue\n\t\t\t\tif name in seen:\n\t\t\t\t\tcontinue\n\t\t\t\twith open(p) as"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\t\"\"\"\n\t\tfor pkg in version_dirs:\n\t\t\tif pkg not in self.version_cache:\n\t\t\t\tcontinue\n\t\t\ttry:\n\t\t\t\tver_dir = VersionDir(self.settings.versions_dir, pkg)\n\t\t\t\tself._check_hash_for_each_root()\n\t\t\t\tself._check_version_trees()\n\t\t\texcept Errors.WafError:\n\t\t\t\tcontinue\n\t\telse:\n\t\t\tver_dir = self.settings.versions_dir\n\n\t\ttry:\n\t\t\tfor ver in VersionList(ver_dir).version_keys():\n\t\t\t\tif not isinstance(ver, ("}]}
{"task_id": "62b45e21e0d4551b0392c8ed", "prompt": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\n\n\n# I hate Python? - there is a \"get_path_depth\" in this module\n# that returns a value. \"You aren't interested\" how to use\n# functions. It gives us an ordered list of \"Nones\".\n\n\nimport json\nfrom datetime import datetime\n\nfrom sqlalchemy import Column, SmallInteger, ForeignKey, Numeric, String, DateTime\n\nfrom meta import BaseMixin\nfrom constants import *\n\n# A small amount of characters to represent the size of the image, as we'll show a lot of space at the left edge\nIMAGE_SIZE = 1024\n\n#"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\t# XXX: find the path's in the filesystem path\n\t\t# XXX: this will also require getting the parent directory, if present in the path\n\t\t# XXX: we might also want to search in the filesystem.\n\t\ttry:\n                return os.path.normpath(path).startswith('..') and \\\n                       path[len(os.path.normpath(os.path.abspath(path)))+1:]\n        except IndexError:\n                return path.startswith(os.path.normpath(os.path.abspath(path)))\n\ndef get_directory_name(directory):\n\t\"\"\"\n\tLook"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t      \n\tobjType='object'\n\tobjMode='file'\n\tif objPath.find('*')!= -1:\n\t\tobjType='root'\n\t\tobjMode='dir'\n\telif objPath.find('.')!= -1:\n\t\tobjType='root'\n\t\tobjMode='objlist'\n\t\tobjPath='/'.join( objPath.split('.')[:-1] )\n\telif objPath.find(':/')!= -1:\n\t\tobjType='root'\n\t\tobjMode='dir'\n\telif objPath.find('\\\\')!= -1:\n\t\t"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\t# we only need this information at the top level\n\t\tfile_type = None\n\t\tif path.endswith(os.sep + \"*\"):\n\t\t\tfile_type = \"ROOT\"\n\t\telse:\n\t\t\tfile_type = \"OBJECT\"\n\t\tif file_type in path_types:\n\t\t\troot_type = path_types[file_type]\n\t\t\tret_type = path_types[os.sep.join(root_type)]\n\t\t\tif ret_type:\n\t\t\t\treturn ret_type\n\t\ttry:\n\t\t\tfor x in os.listdir(file):\n\t\t\t\t"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\n\t\n\tpath_type = path[path.index('/')+1:]\n\t\n\tif not os.path.isfile(path_type):\n\t\treturn 'unknown'\n\t\n\tif not os.access(path_type, os.X_OK):\n\t\t\n\t\t# check for hidden directories\n\t\tpath_type = path[path_type.index('/'):].lower() + '/'\n\t\tif not is_allowed_to_create() and not ocfl.OCFL_DIR_NAME_TO_SAVE.match(path_type):\n\t\t\traise ocfl.OCFLError(\""}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t    \n\tfrom Tools import Notifications\n\t#TODO: Implement a system for this: file type\n\tif getPreferredLanguage() == \"de_DE\":\n\t\treturn \"directory\"\n        else:\n\t\treturn \"file\"\n\t#TODO: Implement a filesystem-based filesystem\n\treturn Notifications().notifications.location()\n\ndef verifyFile(path, location):\n\t\"\"\"\n\tVerify that \"path\" is a file, either on file system, or on\n\tsubdirectory.\n\n\t@type path: String.\n\t@param path: a path (either on a PA"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\n    #TODO: We might need to figure out if the path is a directory\n\n    if not path[0] == '\\\\':\n        idx = path.find('file')\n    elif not path[1] == '\\\\':\n        idx = path.find('/')\n    else:\n        raise RuntimeError(\"unknown OCXFS object\")\n\n\tif idx == 0:\n\t    return \"directory\"\n\n\tif len(path) == 1:\n\t    return \"directory\"\n\n\tif path[1].find('#') == 0:\n\t    return \"file\"\n\n\tif len(path) > 1:\n            idx = path.find('/')\n\t    while idx"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\tret = 0\n        import glob\n\t\tfor path_spec in path.split('/'):\n\t\t\tm = glob.glob(path_spec)\n\t\t\tif m:\n\t\t\t\tif m:\n\t\t\t\t\tpath_type = m[0]\n\t\t\tif os.path.isdir(path_spec):\n\t\t\t\tret = 1\n\t\t\t\tbreak\n\t\tif ret == 0:\n\t\t\tret = 1\n\t\treturn ret\n\n\"\"\"\nCheck /proc/bus/pci/bus and if /proc/pci/vendor are 0, check /sys/devices/pci0000 on /proc/cpu/bus/0"}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\ndef find_directory_type(filename):\t\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given\n\tfilename.\n\t\n\tIf \"0=*\" is a path, we assume that it is a leaf directory\n\tand we can't tell what path type we are dealing with,\n\tsince that might be a file.\n\t\"\"\"\n\tmatch = re.search(r'(?i)(0=*(.+))', filename)\n\tif match:\n\tmatch_file = match.group(1)\n\treturn match_file\n\n\ndef find_filename_type("}, {"generated_text": "def find_path_type(path):\t\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\t\"\"\"\n\t\n        # OCFL File Type check\n        if not path.endswith('/'):\n                return 'root', path\n\n        # OCFL Object Check\n        if path.count('/') > 1:\n                return '', path\n\n        obj = ocfl.readfile(path)\n        file_type = obj[0]\n        filename_without_extension = obj[2]\n        dirname = str(obj[3])\n        obj = ocfl.readfile('%s%s' % (dirname, filename_without_extension))\n\n        if 'root' in file_type:\n                return '', path\n\n        obj = ocfl.read"}]}
{"task_id": "62b45b396decaeff903e1001", "prompt": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t", "suggestions": [{"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tparams['comment'] = 'Reverted To BZ#{0}.'.format(\n\t\t\tself.bugzilla_code)\n\t\tbzparams = ''\n\t\tif bug_ids:\n\t\t\tbzparams = \"[Bug {}]({})\".format(\n\t\t\t\tparams['id'],\n\t\t\t\tparams['resolution']\n\t\t\t)\n\t\telif params['bugs']:\n\t\t\tbzparams = \"[{}]\"\n\t\telse:\n\t\t\tbzparams = \"[]\"\n\t\treturn bzparams\n\n\tdef get_bzparam(self, params):\n\t\tbzparams = self.get_bzparams(params)\n\t\treturn bz"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\turl = self.base + '/api/0/bugzilla/rest/get/' + params\n\t\t# Note the below are some of the bugs:\n\t\t# https://bugzilla.selenic.com/show_bug.cgi?id=109280\n\t\tparams += (bug_ids,)\n\t\treturn self.client.post(url, params, \"application/json\")\n\n\n\tdef get_bug_status(self, bug_id):\n\t\t\"\"\"\n\t\tRetrieve the status of a particular bug that is set to `bug_id`\n\t\t\"\"\"\n\t\treturn self.fetch"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t # Append some other parameters in if the bugzilla needs to change stuff\n\t params['action'] = {'type': 'patch'}\n\t params['data'] = {'flags': [], 'include_fields': []}\n\n\t # Reposition the params dict, there is no point in making a backup\n\t new_params = {}\n\t for key, value in params.items():\n\t\t\t# Don't include fields if they is not defined\n\t\t\tif key.find('__reserved_')!= -1:\n\t\t\t\tnew_params[key + '__reserved_bug_id'] = str(value)\n\t\t\tif key not in bug_ids"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tself.log.info(\"Amending \" + params['URL'])\n\t\tdata = {'ids': bug_ids}\n\n\t\tself.data_out['params'] = xmlrpclib.dumps(data)\n\t\tself.data_out['token'] = self.token\n\n\tdef log_bug(self, bug_id, **values):\n\t\t\"\"\"\n\t\tLog a bug using a dictionary - for a bug ID\n\t\t@param bug id id of the bug to log\n\t\t@param values dictionary of the parameters to log\n\t\t\"\"\"\n\t\tbug_url = self.url_bug %"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\t(added, removed) = self._ammend_list(*params)\n\t\t# we need to find the bugzilla ids\n\t\tfor bug_id in bug_ids:\n\t\t\tparams = self._fix_bug_parameters(\"bug_id\", params, bug_id)\n\t\t\tparams['fields'] = self._listify(params)\n\t\t\t# TODO: replace with data from bug.add\n\t\t\tparams['_source']['bugs'] = str(bug_ids[bug_id][0]).split(', ')\n\tdef get_bz_params(self, params, bug_ids):\n\t"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tparams.url = self._get_url()\n\t\tparams.payload = self._get_payload()\n\t\tparams.signature_verification = self._get_signature_verification(bug_ids)\n\t\tparams.signature_verification_date = self._get_signature_verification_date(bug_ids)\n\t\tparams.signature_verification_product = self._get_signature_verification_product(bug_ids)\n\t\tparams.url = self.request_url\n\t\tparams.content = self.get_post_json_header()\n\t\treturn params\n\n\tdef fetch_report"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t    \n\t  # Check if we have access to the repository. It doesn't matter if\n    \t# the configuration file has some other mandatory information.\n     \tif not len(params[\"bzparams\"].keys()):\n\t\treturn\n\n       # Determine if the configuration file already exists\n    \tif os.path.exists(os.path.expanduser(\"~/.config\")):\n\t\n    \t \t# Read in the configuration file and check to see if \n\t     # the configuration entries appear valid. If so, read it and\n\t     # check to see that it contains valid data.\n    \t\tfh = open(os.path.expanduser(\"~/."}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tif 'id' not in params:\n\t\t\treturn (0, None)\n\t\tparams['id'] = str(bug_ids[0])\n\t\treturn (0, None)\n\n\tdef update_params(self, bug, params, comment):\n\t\t\"\"\"\n\t\tUpdate the existing params and return a single dict\n\t\t\"\"\"\n\t\treturn (None, None)#!/usr/bin/python\n\n# Copyright: (c) 2018, Marcin Szymanski <marcin@mazhus-curseul.pl>\n# GNU General Public License v3."}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t     \n\tdef set_bug_status(self, bug_ids, summary):\n\n\t     \n\tdef get_bug_status(self, params):\n\t     \n\tdef get_params(self, bug_ids):\n\t     \n\tdef get_param(self, params):\n\t     \n\tdef log_bug(self, bug_ids):\n\t      \n\tdef fetch_bug(self, bug_id):\n\t      \t\n\tdef fetch_bug_comments(self, bug_id, *args):\n\t      \n\tdef fetch_bugzilla_bug_comment_ids(self"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\t\"\"\"\n\tAmend the Bugzilla params\t\"\"\"\n\t\tbug = self.get_bz(params[\"bug_id\"])\n\t\tif bug is None:\n\t\t\treturn\n\t\tfor (bug_id, name) in bug_ids:\n\t\t\tb = self.get_bz(bug_id)\n\t\t\tif b is None:\n\t\t\t\tcontinue\n\t\t\tparams[\"bug-%s\" % name] = b\n\t\t\treturn params\n\t\treturn None\n\tdef close_bug_bug(self, bug_ids=None):\n\t\t\"\"\"\n\t\tClose a bug\n\t\t\"\"\"\n\t\tif not bug_ids:\n\t\t\tbug_ids = self"}]}
{"task_id": "62b4567ed7d32e5b55cc83d9", "prompt": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\truamel.yaml.merge_nodes(deep_merge_nodes(node_merge))\nelse:\n    print \"This should never happen...\"\n\n\n    ###\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\treturn deep_merge_mapping\n\n\ndef deep_merge(nodes):\t\"\"\"\n\tGiven a borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode,\n        ruamel.yaml.nodes.MappingNode\n    )\n\n... this version does not change when merging the contents of the borgmatic configuration file.\nWe currently take care of merging metadata in the config.yaml files\nin place on top of it to avoid making sure that we do not change any\nsettings that we use to override.\n\nThe idea"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\tresult = [\n\t\t\t(\n\t\t\t\tScalarNode(\n\t\t\t\t\tkey='tag:yaml.org,2002:str', value='retention'),\n\t\t\t\tMappingNode(key='tag:yaml.org,2002:map', value=[\n\t\t\t\t\t(\n\t\t\t\t\t\tScalarNode(\n\t\t\t\t\t\t\tsplit_contents=lambda content: 'tag:yaml.org,2002:str', value=content, sep=('|', '\\\\n')),\n\t\t\t\t\t\tScalarNode(\n\t\t\t\t\t\t\tsplit_contents=lambda content: 'tag:yaml.org,2002:str', value=content, sep=('|', '\\\\n')),\n\t\t\t\t\t\tScalarNode(\n\t\t\t\t\t\t"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\ndef deep_dict_merge(dct, udict, override_value=None):\n\t\"\"\"\n\tGiven a dictionary of dictionaries, merge all the keys and values found\n\tin key-value pairs of another. Overriding a parameter override_value\n\tproduces the behavior described in the documentation.\n\n\tArguments:\n\tdct (dict):\n\t\tThe dictionary (dict) to merge into the existing one.\n\tudict (dict):\n\t\tOverriding the dictionary before merging with a key that is already\n\t\tin the new dictionary.\n\toverride_value (mixed):\n\t\tOverride the"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\traise InvalidConfigException(\"deep_merge_nodes(): expected merged file was named:\\n{}\".format(self.yaml_file))\n\ndef deep_merge_node_list(node_list):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are collisions between nodes,"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\traise NotImplementedError\nimport sys\nimport yaml\nimport re\n\nclass CppSettings(object):\n    def __init__(self, config_file):\n        self._config = yaml.load(config_file)\n        self._config = self._config\n        self._settings = self._config['settings']\n\n        # Add settings defined in settings file\n        self._settings.update(\n            {\n                'command': {\n                    'command': self._configuration['command']\n                },\n                'help': {\n                    'help': self._configuration['help']\n                },\n\n               'subcommand': {\n                   'subcommand': self"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\truamel_node = None\n\t\truamel_node_key = None\n\t\truamel_node_value = None\n\t\truamel_node_comments = []\n\t\truamel_node_comments_by_key = {}\n\t\tdef node_add(node_value, node_comments, comments, parent_key, key_comments_by_key):\n\t\t\tnew_parent_key_comments = comments.get(parent_key, [])\n\t\t\tfor comment in node_comments:\n\t\t\t\tif comment['key'] == node_value:\n\t\t\t\t\tnew"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\treturn deep_replace(nodes)\n\t\t\t\n\nclass _BorgConfigException(Exception):\n    \"\"\"\nAn error raised when the BORG config file is not formatted correctly:\n\"\"\"\n\tdef __init__(self, message):\n\t\tsuper(_BorgConfigException, self).__init__(message)\n\nclass BorgConfig(object):\n\n\tdef __init__(self, path):\n\t\n\t\tself.path = path\n\t\n\tdef get(self, item):\n\t\n\t\tif os.path.exists(self.path):\n\t\t\treturn BorgConfig(self.path)\n\t\n\t\traise _B"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\n\n\t\timport yaml\n\t\timport types\n\t\timport os.path\n\n\t\tnode_remap = None\n\t\tmappings = None\n\t\troot = None\n\n\t\ttry:\n\t\t\tos.chdir(deep_merge_dir)\n\t\texcept OSError as e:\n\t\t\t# directory may not exist\n\t\t\tif e.errno == errno.ENOENT:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\traise\n\n\t\ttry:\n\t\t\tnode_remap = {}\n\t\t\tmappings = None\n\t\texcept ValueError as e:\n\t\t\t# some nodes had to be merged, if there wasn't some: ignore\n\t\t\tif len(nodes"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\ndef parse_from_borg_config(filename):\n\n\timport re\n\timport pyyaml\n\n\tremember_tag = filename[-1].split('tag:yaml.')[0]\n\n\tremember_value = ''\n\tremember_default = False\n\ttry:\n\t\tremember_value = pyyaml.load(file(filename))\n\t\tremember_default = pyyaml.load(file(filename + '.defaults'))\n\t\tremember_value = remember_value + remember_default\n\texcept Exception, e:\n\t\tprint \"Error parsing {}: {}\"."}]}
{"task_id": "62b4567ad7d32e5b55cc83af", "prompt": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n    description = \"A script which wraps the arguments for an experiment in order to provide a proper interface for the experiment. \" \\\n    \"It also has support for a custom class/method/variable that is returned to make them easier to access these arguments.\"\n\n    epilog = \"Please see README.txt for details.\"\n    parser = ArgumentParser(prog=program_name, description=description, epilog=epilog, add_help = False)\n    parser.add_argument('-v', '--verbose', action='store_true', help=\"print out extra information.\")\n    parser.add_argument('-l', '--logprefix', dest"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\n\t# setup variables to be used\n\targs = vars(args)          # init the parser\n\targs['verbose']           = False\n\targs['file_output']      = False\n\targs['outfile_name']      = args['outfile_name']\n\targs['notitle']           = args['notitle']\n\targs['version']            = VERSION\n\targs['title']            = args['title']\n\targs['debug']            = args['debug']\n\targs['input_path']       = args['input_path']\n\targs['config']             ="}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\t# parse the arguments as usual\n\t\tparser = argparse.ArgumentParser(description='')\n\t\tparser.add_argument('-t', '--target', help='directory containing the target', required=True)\n\t\tparser.add_argument('-f', '--format', dest=\"file_format\", help='format of the file', required=True)\n\t\tparser.add_argument('-d', '--directory', help='directory to process', required=True)\n\t\tparser.add_argument('-v', '--version', action='version', version='%(prog)s {}'.format(version()))\n\t\tparser.add_argument"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser( description = 'A Toolset-based CLI class that can be used to build and run the toolset.')\n\t\tparser.add_argument( '--version', action='version', version=__version__ )\n\t\tparser.add_argument( 'args', nargs='*', type=parse_arguments, help='The argument list to pass to the program when called. If omitted, all arguments are passed to the script.')\n\t\treturn parser\n\ndef main():\n\tparser = parse_arguments( )\n\targs = parser.parse_args()\n\treturn tool_run("}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=__version__)\n\t\tparser.add_argument(\"args\", nargs = \"+\", help=\"Any command-line arguments that this script was invoked from\", metavar = \"\")\n\n\t\tparser.add_argument(\"-v\", \"--version\", action = \"version\", version = \"%(prog)s 0.1 beta\" % {\"prog\": __version__})\n\t\tparser.add_argument(\"--author\", action = \"author\", help = \"Author of the app: Evan Menger\")\n\t\tparser.add_argument(\"-v,--verbose\", help = \"print output to stdout if"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description='Extract info about.ebooks,.epub, and.docx files in your calibre book database.',  epilog='For example, a Python module can be installed to run this script in a command prompt with the \\\"run\\\" command.')\n\tparser.add_argument('names', nargs='*', help='The name to extract')\n\tparser.add_argument('-i', '--input', nargs='+', help='The name of the input file (single file for stdin).')\n\tparser.add_argument('-o', '--output', nargs=1, help='the"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tresult = dict(map(argparse.ArgumentParser.parse_argument, arguments))\n\t\treturn result\n\ndef main():\n\t# TODO:\n\t# * Handle -c and --color for command-line\n\t# * Handle -l\n\t# * handle -m\n\t# * handle -q\n\n\tglobal parser_options\n\t\n\t# Parse input\n\targs = parse_arguments(' '.join(sys.argv[1:]))\n\n\tif args.color:\n\t\tcolor_parser = ColorParser()\n\telse:\n\t\tcolor_parser = ColorParser()\n\n\t# Parse command"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser()\n\t\tparser.add_argument(\"command\", nargs=1, type=str, help=\"Command\")\n\t\tparser.add_argument(\"-v\", \"--verbose\", action='count', dest='verbosity', help=\"Be more verbose\", default=1)\n\t\treturn parser\n\nif __name__ == '__main__':\n\n    args = parse_arguments(sys.argv[1:], sys.argv)\n    \n    main()\n\n#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2011-2018 EDF SA\n"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser()\n\t\tparser.add_argument(\"--name\", required=False, dest=\"name\")\n\t\tfor argument_name in arguments:\n\t\t\tparser.add_argument(argument_name, dest=\"arguments_file\", \n\t\t\t  nargs=\"?\", default=\"\")\n\t\treturn parser\n\n\tdef run(parameters):\n\t\t(argv_length, argv_name) = parameters.arguments_file\n\t\tfilename = argv_name\n\t\tparser = parse_arguments(argv[1:])\n\t\targs = parser.parse_args(argv[2:])\n\t\treturn (filename, parse("}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\t\n\targumentParserArguments = argparse.ArgumentParser(prog=\"parse-arguments -d\", add_help = False)\n\targumentParserArguments.add_argument('--help', action = \"help\", help = \"Prints help information about the module specified. This is useful eases debugging and testing.\", default = False)\n\n\targumentParserArguments.add_argument('-e', dest = \"arguments\", type = str, nargs = \"?\", default = None)\n\targumentParserArguments.add_argument('--install', action = \"install\", help = \"Installs the given script to the provided directory.\")\n\n\targumentParser"}]}
{"task_id": "62b45679d7d32e5b55cc83a9", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n    parser.add_argument(\"-l\", \"--list-plugins\", dest=\"plugin_lists\",\n                        action=\"store_true\",\n                        help=\"list available plugins in a given directory, if they're in a plugin list.\")\n    parser.add_argument(\"-P\", \"--plugins-by-path\", dest=\"plugin_paths\",\n                        action=\"store\",type=str,default=\"\",\n                        help=\"list plug-in paths. If a path is specified, only plugins \"\n                        \"that match the filter will be selected.\")\n    parser.add_argument(\"-T\", \"--timeout\", dest=\"timeout\",\n                        action=\"store\","}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\tparser.add_argument('--use', default='', \n\t\thelp=\"\"\"Use the given flag with a -i/-t flag argument. For example '-t1', '-l 1', \n\t\tare equivalent to '-c 1'. Note that \"t1\" takes precedence over \"-l 1\".\"\"\")\n\t\n\tparser.add_argument('--verbose', '-v', action='count', \n\t\thelp=\"\"\"Increase verbosity. Specify -v to increase verbosity.\"\"\")\n\t\n\treturn parser\n\ndef main():\n\tparser = get_parser()\n\topts, args = parser.parse_args()\n\tlogging.basicConfig"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\n\tflags = []\n\n\tflags.append( make_option('--log',\n\t\taction='store', dest='logfile',\n\t\tmetavar=\"<path>\", help='write log data to FILE'),)\n\tflags.append( ('-c', '--config',\n\t\taction='store', dest='config',\n\t\tnargs='?', const='CONFIG', default=None,\n\t\thelp='specify the config file to load'),)\n\tflags.append( ('-t', '--timestamps',\n\t\taction='store', dest='timestamps',\n\t\ttype=int, metavar='N',\n\t\thelp='filter events within N"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\n\tflags          = {}\n\targparser  = argparse.ArgumentParser( usage = USAGE, description = DESCRIPTION)\n\t\n\targparser.add_argument('-q', action = 'help', help = 'Suppress arguments that are in quotes. Default is \\\\\"', actiontext ='   0 : No arguments; 1 : All arguments', dest = 'line_limit', default = 0, type = int)\n\t\n\targparser.add_argument('-r', action = 'help', help = 'Reroute the program parsing and usage information to extract arguments. Default is \\',\\'', actiontext ="}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\topts = \"\"\n\t\tfor opt in parser._optionals._group_actions:\n\t\t\topts += \" --%s\" % opt\n\t\treturn opts\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--debug\", \"--debug\", action='store_true', default=False, help=\"Enable debug logging\")\nparser.add_argument(\"-v\", \"--verbose\", \"-v\", action=\"count\", default=0, help=\"Print debugging information\")\nparser.add_argument(\"--logging-level\", type=int, help=\"Logging level: 0 = lowest, 1 = highest, 2 = all"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\n\t# These flags are defined here for use in the case of \"multiple add\" and other\n\t# parsers, like --foo=bar=\"bar-baz\" would use the \"-\" argument to prevent\n\t# errors (see http://bugs.python.org/issue13849)\n\tparser.add_argument('--foo', '--foo-bar', action='store_true',\n\t                 help='foo help')\n\t\n\t# these flags are defined here for use in the case where a flag was provided that\n\t# was not explicitly defined previously\n\tparser.flag='flag'\n\n\t# These flags"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t        # TODO: remove support for a leading -- for example.\n\n\t                      ## parser.add_argument(\n\t                      ## 'foo', type=int, default=-1,\n\t                       ## help='bar', metavar='FOO')\n\t                      parser.add_argument(\n\t                      '--bar', dest='bar', type=int, default=0,\n\t                       ## help='bat', metavar='BAR')\n\t                      return parser.parse_args()\n\n\tdef __init__(self, usage=None, description=\"\"):\n\t\tself.parser = argparse.ArgumentParser(\n\t\t\tdescription=description,\n\t\t\tusage=usage)"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n        return '\\n{0}'.format(\",\".join(parser.option_strings))\n    \n    def getDescription(self):\n                return self.__doc__\n\t\n    def getCommandLineArgument(self, name):\n    \tif name in self.__args:\n            return self.__args[name]\n        else:\n            raise ArgumentException('Invalid command: %s' % name)\n\n_ARGPARSER = ArgumentParser(description=__doc__, \n                           prog='pyopl-gadgets',\n                             conflict_handler='resolve')\n\n# -- Options for this group ------------------------------------------\nGROUP_SETTINGS = parser_flags(parser_"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\toptions, argparser = parser.parse_args()\n\t# get the option flags: <filename>;<mode>,..<number>.<action>.\n\tglobal_args = vars(parsed_args)\n\tglobal_args.pop(\"-\" + global_args.pop(\"-\"))\n\n\t# build our argument options\n\toptargs = {}\n\tfor k, v in optargs.iteritems():\n\t\toptargs[k] = v\n\t\n\tif args:\n\t\tfor arg in args:\n\t\t\tif arg not in global_args:\n\t\t\t\tglobal_args[arg] ="}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\n\t@staticmethod\n\tdef flag_to_key(flags):\n\t\t\"\"\"\n\t\tUsed to convert an argparse argument flag into a list of\n\t\tflags\n\t\t\"\"\"\n\n\t\t# We assume that no flag is being used, even though when we're\n\t\t# passed this a flag, the first element is always the string\n\t\tflags = flags.split(',')\n\n\t\tcmd_flags = {}\n\t\tcommand_flags = []\n\n\t\tfor flag in flags[1:]:\n\t\t\tbit = flag.split('-')\n\t\t\tcmd_flags[bit[0]] = bit[1]\n\n\t\t\tif len("}]}
{"task_id": "62b45665d7d32e5b55cc8365", "prompt": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description=DESCRIPTION, epilog=EPILOG, formatter_class=argparse.ArgumentDefaultsHelpFormatter, \\\n\t\t\t\t\t\t\t\t\t\tdescription_formatter=EPILOG)\n\n\tparser.add_argument(\"-b\", \"--backup\", help=\"\", action=\"store_true\", default=False)\t\t\t\n\tparser.add_argument(\"-c\", \"--config\", help=\"\", action=\"store\", dest=\"config\")\n\t\n\tparser.add_argument(\"--logfile\", help=\"\", action=\"store\", dest=\"logfile\")\n\tparser.add_argument(\"--no-update\", help=\"\", action=\"store_false"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\targs = unparsed_arguments.pop()\n\t\tparser = argparse.ArgumentParser(description=subparsers[args.subcommand_name])\n\t\targs.subcommand_name_or_args = args # So pylint can be used as an attribute as well\n\t\tparser.add_argument('command', nargs='?', \n\t\t\t\thelp='The command line argument that you had run with this script.')\n\t\targs.run_local = getattr(args, 'run_local', None) or (args.command is not None)\n\t\tif len(unparsed_arguments) > 1:\n\t\t\tfor param"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(prog=self.path)\n\t\tparser.add_argument(\n\t\t\t'subparser', nargs='*', help=argparse.SUPPRESS,\n\t\t)\n\n\t\tfor name in _Command.COMMANDS + _Subcommands.COMMANDS:\n\t\t\tparser.add_argument(\n\t\t\t\t'-' + name, action='store_true', dest=name.lstrip('-'),\n\t\t\t\tdefault=False, help=argparse.SUPPRESS,\n\t\t)\n\n\t\tsubparser = parser.add_subparsers(\n\t\t\thelp='Commands', metavar=self.command_name_metavar,\n\t\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\t# get the parser and print info\n\t\tparser, arg_ns = PARSER, arg_ns_str\n\t\t\n\t\t# create namespace\n\t\tcmd_ns = make_parser_ns()\n\t\t# get subparsers args\n\t\tsubparsers = [\n\t\t\targ for arg in shlex.split(arg) if arg!= '-h'\n\t\t]\n\t\t# parse each subparsers\n\t\tfor subcommand in subparsers:\n\t\t\tcmd_ns.add_parser(subcommand[1], help=subcommand[2])\n\t\tpy_ns_ns = namespace.update_namespace(cmd_ns,"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\t\n\tdef extract_data_options():\n\t\t\"\"\"\n\t\tReturn the parsed arguments and options for `parse_arguments`, \n\t\tor ``None`` if not present.\n\t\t\"\"\"\n\t\t# Extract the source, and any parsed args, so that the parser class\n\t\t# can use it to process the parsed options.\n\t\t#\n\t\tsource = \"\"\n\t\tpargs = parse_arguments(args,\n\t\t\t[\"--config\",\n\t\t\t\"path:\"] if source is None else [\n\t\t\t\tsource,\n\t\t\t\targs_parse.NOPARSER, args_parse.SOURCEFILE\n\t\t\t])) if"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparsed_arguments = {}\t# this will contain everything in the args list (after our arguments)\n\t\t\t\t\t# so we can parse them later.\n\t\t# First we check if we are dealing with an argument or an unparsed argument (or an \"unparsed\" argument)\n\t\tfor arg in unparsed_arguments:\n\t\t\tif arg.replace(\" \", \"\") in arg_parser.arg_namespace:\n\t\t\t\tparsed_arguments[arg.replace(\" \", \"\")] = arg\n\n\t\treturn parsed_arguments\n\n# this will be used for the case in which we are executing in the \"command line\"."}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\t# Prepare the \"parse arguments\" object to parse for this script\n\t\tparse_args = _prepare_parse(unparsed_arguments)\n\t\treturn _parse_args(parse_args)\n\telse:\n\t\tparse_args = _prepare_call(parsed_arguments)\n\t\treturn _parse_args(parse_args)from django import template\nfrom django.shortcuts import render, redirect\nfrom django.http import Http404\nfrom songs.models import Song\nfrom election.models import Election\nfrom songs.forms import NewSongForm\nfrom webb"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\n\t# Check all the command-line arguments.\n\t#\n\tfor arg in parsed_arguments:\n\t\targparser = argutils.ArgParser(parser=arg)\n\t\tprint(argparser.parser.__dict__)\n\t\targs = argparser.parse_args(unparsed_arguments)\n\n\tif args.version:\n\t\tprint('-- version argument is deprecated (use --help)')\n\t\t\n\treturn args    \n\n\n\n# -*- coding: utf-8 -*-\n# Part of Odoo. See LICENSE file for full copyright and licensing details.\n\nimport pytz\nfrom datetime import datetime,date,"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=\"Test script for iotopatools.\");\n\t\tparser.add_argument(\"--version\", action=\"version\", version=\"0.0.1\")\n\t\tparser.add_argument(\"-a\", \"--app\", dest=\"app\", required=True)\n\t\tparser.add_argument(\"--no-app\", dest=\"app\", required=False, default=\"main\")\n\t\tparser.add_argument(\"-d\", \"--debug\", action=\"store_true\")\n\t\targs = parser.parse_args(unparsed_arguments);\n\n\t\treturn args;\n\n\tdef"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t \n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\n\t\t\"action\",\n\t\tchoices=global_subargs,\n\t\thelp=\"the action to be performed\"\n\t)\n\tparser.add_argument(\n\t\t\"args\",\n\t\tnargs=\"*\",\n\t\thelp=\"the arguments to parse\"\n\t)\n\tglobal_options = parser.parse_args(unparsed_arguments)\n\n\treturn {'global_options':global_options}\n\n# Copyright 2012 OpenStack Foundation\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2"}]}
{"task_id": "62b45665d7d32e5b55cc8364", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\tparser = subparsers.add_parser(\"repository\", description=\"Gitolite repository\")\n\t\tparser.add_argument(\"--name\", type=str)\n\t\tparser.add_argument(\"--repository\", type=str)\n\t\tparser.add_argument(\"--directory\", type=str)\n\t\tparser.add_argument(\"--prefix\", type=str)\n\t\tparser.add_argument(\"-Y\", \"--yes\", action=\"store_true\",\n\t\t\t\t\t\t\t\t\t\thelp=\"Don't prompt for confirmation\")\n\t\tparser.add_argument(\"-n\", \"--dry-run\", action=\"store_true\",\n\t\t\t\t\t\t\t\t\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\t\n\taction = unparsed_arguments.pop(0)\t# name of the subcommand we will execute for\n\t# print \"-\"*70  # debug\n\t# print action # debug\n\t# print str(unparsed_arguments) # debug\n\t# print \" \" * 10 + \"  \" + \" \".join(map(str, unparsed_arguments)) # debug\n\targs_list = []\n\twhile args_list:\n\t\tsubparser_name = args_list.pop(0)\n\t\taction = actions[subcommand_name]\n\t\targs_list.extend"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\targs = subparser.parse_args(unparsed_arguments)\n\n\tif len(args.positional_arg) < len(args.arguments):\n\t\t\traise argparse.ArgumentError(\"Missing arguments.\")\n\n\treturn (dict((a, []) for a in args.positional_arg), args.positional_arg)\n\n\ndef execute(args):\n\t\n\trepository_name = args.repository\n\trepository = get_repository(repository_name, args.repository_directory)\n\troot = repository.get_tree(args.arguments[0])\n\t\n\t# Get a parser for arguments\n\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\t# This decorator is necessary in order to use it in both python 2 and python 3.\n\tdef process_subparser_args(func, args, parent, prefix):\n\n\t\tif len(parent)!= len(prefix):\n\t\t\treturn None\n\n\t\taction_name = parent[prefix[0]] # action name\n\t\targs = args[len(prefix[1]):] # actual arguments\n\t\tif action_name in action_name_usage_examples:\n\t\t\tprint(f\"[{prefix}] {prefix}\\n\".format(prefix=prefix,\n\t\t\taction_name = action_"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\tactions = []\n\tremaining_arguments = []\n\tfor arg_num, arg in enumerate(unparsed_arguments):\n\t\t\n\t\tif type(arg) == dict:\n\t\t\tactions.append(\n\t\t\t\tsubparser_argument_factory(\n\t\t\t\t\tparser_name,\n\t\t\t\t\tsubparser,\n\t\t\t\t\targ,\n\t\t\t\t\t#args,\n\t\t\t\t\tactions\n\t\t\t\t)\n\t\t\t)\n\t\t\tremaining_arguments.append(\n\t\t\t\t(\n\t\t\t\t\t(\n\t\t\t\t\t\tsubparser_name,\n\t\t\t\t\t\targ_num,\n\t\t\t\t\t\targ,\n\t\t\t\t\t\t#args,\n\t\t\t\t\t\tactions\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\tglobal verbose, debug, parseVersion\n\t\tlogger = logging.getLogger(u\"parse-subparser\")\n\t\tparser = get_subparser(subparsers, description=\"sub Parsers\")\n\t\tdef a_func(args):\n\t\t\tglobal verbose, debug, parseVersion\n\t\t\tusage = u''.join(unparsed_arguments)\n\t\t\tif \"--list-plugins\" in subparser.__doc__:\n\t\t\t\tpluginDict = {}\n\t\t\t\tfor name, value in parseVersion.items(subparser.__doc__).iteritems():\n\t\t\t\t\tpluginDict[name] = value\n\t\t\t\tparser.exit()\n\t\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\tresult = []\n\t\tns = [x for x in subparsers]\n\t\tfor i in range(len(ns)-1):\n\t\t\tsplit_args = ns[i+1].split()\n\t\t\tif len(split_args) <= 1:\n\t\t\t\t# We're still past the parser\n\t\t\t\tcontinue\n\t\t\tnamespace = {\"name\": \"\".join(split_args[0:1]), \"namespace\": []}\n\t\t\tif ns[i][0] == 'help':\n\t\t\t\tnamespace[\"help\"] = True\n\t\t\telif ns[i][0] =='repository':\n\t\t\t\t# Don't use"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n    parsers_actions = { \n                         \"create\": (lambda argv: (argparse.ArgumentDefaultsHelpFormatter(\n                                prog=\"create\",\n                                parents=[argparse.RawDescriptionHelpFormatter]) if argv is not None\n                               else argparse.RawDescriptionHelpFormatter),\n                         (\"repository\", lambda argv: True),\n                         \"tag\": (lambda argv: True),\n                         \"push\": (lambda argv: False),\n                        }\n      \n    subparsers_actions = {\n                    \"list\": (list_parsers, argparse.ArgumentParser),\n                    \"search\": (search_parsers, argparse.ArgumentParser,\n                             argparse.ArgumentParser),\n                    }\n    args_actions = {\n                    \""}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\tresult = []\n\tnext_parser = None\n\targs_map = { }\n\tfor (subparser,subparsers) in map(None,subparser_args.items()):\n\t\tparser = subparsers.add_parser(subparser,conflict_handler=ConflictHandler())\n\t\tfor argument,parser_args in filter(lambda subparser: subparser.name == argument, subparsers.items()):\n\t\t\targument = argument.copy(default=argparse.SUPPRESS)\n\t\t\targument.override = parser.add_argument\n\t\t\targument.default = parser.parse_args.default\n\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\tdef set_subparsers(self, argparse_parser, subparsers):\n\t\tfor name, attr in vars(argparse_parser).items():\n\t\t\tif attr is None:\n\t\t\t\tcontinue\n\t\t\tsetattr(subparsers, name, attr)\n\t\targparse_parser.add_argument(*subparsers.names, **subparsers.kwargs)\n\n\tdef parse_subcommand(self, args, subparsers):\n\t\t\"\"\"\n\t\tParse the command line arguments and call the subcommand function corresponding to\n\t\tthe selected command's subcommand parser(s) in the order specified by the argument list.\n\t\t\"\"\"\n\t"}]}
{"task_id": "62b45665d7d32e5b55cc8363", "prompt": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('-i', '--input-dir', default = \"../samples\", help = \"Input directory that hold results.\")\n\tparser.add_argument('-o', '--output-dir', default = \"../output\", help = \"Output directory that hold results.\")\n\tparser.add_argument('-q', '--query-file', default = \"../queries_data.txt\", help = \"queries data file.\")\n\tparser.add_argument('-d', '--data-source', default = \"../../../data/sources/en/en-ru.txt\","}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\t\n\tdef create_parser(app_name):\n\t\t\"\"\"\n\t\tCreate a top-level parser.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tparser = argparse.ArgumentParser(\n\t\t\t\tprog='gen',\n\t\t\t\tdescription=app_name,\n\t\t\t\tepilog=APP_DESCRIPTION.strip()\n\t\t\t)\n\t\t\tparser.add_argument(\n\t\t\t\t'--no-print',\n\t\t\t\taction ='store_false',\n\t\t\t\tdefault = False,\n\t\t\t\tdest = 'print',\n\t\t\t\thelp = 'Do not print the results to stdout.',\n\t\t\t)\n\n\t\t\tparser.add_argument(\n\t\t\t\t"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = argparse.ArgumentParser()\n\t\tparser.add_argument('--debug', '-d', dest='debug', action='store_true',\n\t\t\t\t\t\t\thelp='set this flag to display debug information', default=False)\n\t\tparser.add_argument('--version', \"-V\", dest='version', action='store_true',\n\t\t\t\t\t\t\thelp='display version information and exit', default=False)\n\t\tparser.add_argument('--build', '-b', dest='build', action='store_true',\n\t\t\t\t\t\t\thelp='build or install the package', default=False)\n\t\tparser.add_argument('--"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(prog=PROG_NAME)\n\n#\tAdd custom options to parser (sub-parsers)\n\tparser.add_argument('--db_settings', dest=\"db_settings\", default=settings.default_db)\n\tparser.add_argument('--engine', dest='engine', default=engine)\n\tparser.add_argument('--args_namespace', dest='args_namespace', default=args_namespace)\n\tparser.add_argument('--args_list_namespaces', dest='args_list_namespaces', default=args_list_namespaces)\n"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\n\n\tparser = argparse.ArgumentParser(\n\t\tdescription =\t'Python 2.6 shell client',\n\t\tepilog\t    =\t'The shell client is a python 2.7 wrapper and implements the server side of python and command line programs.',\n\t)\n\n\tparser.add_argument('--version', action='version', version='''%%prog %(prog)s 2.6''', default='3.3')\n\n\tparser.add_argument('--help', action='help', help='''Print help''')\n\n\tparser.add_argument('--version_message', action='version-message',  type=argparse"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(description='Parse all files given on the command line.', epilog='''\n    A simple example.  First, to get a list of all files, with the appropriate extension,\n    we can call it ``extract_files``.  Then, the ``parser`` parameter, a parsed version\n    will be generated and parsed into a parser that contains the appropriate information for\n    the file it is being written.  Then, the ``add_args`` class method is called with a set of\n    arguments, one for each file to read from and a dict of arguments, that would be passed\n    to the ``"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(\n\t\tusage=USAGE,\n\t\tdescription=\"Convert the.docx and./lib/*.lib files to the docs folder.\",\n\t\tepilog=\"Once done, files are translatable and added to the documentation folder.\")\n\tparser.add_argument(\"-C\", \"--copyright\", dest=\"copyright\", action=\"store\",\n\t\thelp=\"The file contains the copyright that should be included in the docs. By default, this is the'master' document. This page assumes that this file is from an Invenio repository.\\nIt can be copied to a \" +\n\t\t \"subversion repository (which"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t  \n\tparser = argparse.ArgumentParser(description=\"Generates a HTML report of the top level library\")\n\tparser.add_argument('-i', '--input', metavar='INPUT_PATH', nargs='?', type=argparse.FileType('r'), help=\"Path to XML report file\")\n\tparser.add_argument('-o', '--output', metavar='OUTPUT_PATH', nargs='?', type=argparse.FileType('w'), help=\"Path to report file\")\n\tparser.add_argument('-t', '--template', metavar='TEMPLATE_FILE', nargs='?', type=argparse.FileType('r'), default=None, help"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(\n\t\tdescription = 'Command line interface for the Brain Vision GAN',\n\t\tusage\t\t= '''\n\t\t\t''',\n\t\tprog\t  = 'pybreadv4' )\n\t\n\tparser.add_argument(\n\t\t'-d',\n\t\taction    ='store',\n\t\trequired = True,\n\t\tdest     = 'database',\n\t\thelp     = 'Path to a SQLite3 database containing all records of interest',\n\t\tmetavar  = 'DATABASE' )\n\tparser.add_argument(\n\t\t'-b',\n\t\taction   "}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(\n\t\t\tdescription='A tool for converting to AFLD style AIFF files.')\n\n\t\t# Version info, options, files from\n\t\tparser.add_argument(\n\t\t\t'-v', '--version', action='version',\n\t\t\tversion='Version info (defaults to the AFLD version).',\n\t\t\thelp='Print version number',\n\t\t)\n\t\tparser.add_argument(\n\t\t\t'-d', '--dry-run', action='store_true',\n\t\t\thelp='Do not write results file but do produce plots'\n\t\t)\n\t\tparser.add_"}]}
{"task_id": "62b438ba66fea644fe22cca2", "prompt": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\n#\n# The merging method is used to handle the merging of a node. It is only called\n# on unambiguous input borgmatic configuration files as well as different nodes.\n#\n\ndef deep_merge_nodes_in_dictionary(node_list):\n\t\"\"\"\n\tGiven a list of node dictionaries, generate the merged node in place using the\n\tdictionary to create a node that uses the borgmatic configuration files to merge.\n\tThis step is necessary because in that order, the merged data must be\n\tloaded sequentially for the subsequent merging.\n\t\"\"\"\n\n\tres"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\tassert isinstance(nodes, list)\n\t\tassert len(nodes) > 0\n\t\troot = nodes[0]\n\n\t\tnew_nodes = []\n\t\tfor node in root:\n\t\t\tnew_nodes.append(\n\t\t\t\tdeep_merge_nodes(node),\n\t\t\t)\n\t\tnew_nodes.append(root)\n\n\t\treturn new_nodes\n\n\tdef deep_merge_nodes2(nodes):\n\t\tassert len(nodes) > 0\n\t\troot = nodes[0]\n\t\tchildren = nodes[1:]\n\t\tkeys = [flat_node.tag for flat_node in"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\n    for cfgtuple in nodes:\n        if isinstance(cfgtuple, tuple):\n            cfgtuple, res = deep_merge_nodes(cfgtuple)          \n        else:\n            res = ast.literal_eval(cfgtuple)\n        new = merge_into(deep_merge_nodes, node_dict=res)\n        ast.copy_location(new, cfgtuple)\n        return new.value\n\n    \ndef merge_into(nodes, new_node_dict=None):\n    \"\"\"\n    Given a list of tuple in the form of (tag:yaml.org,2002:str,*,mappingnode,metadata"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\n\n        for group in groups:\n                if group[0]!= 'tag:yaml.org,2002:str,':\n                    raise YeditException(\"Unable to parse as a group name, use a '\\n' separated group name.\", group)\n\n                rval = None\n                if group[1] in groups:\n                    rval = groups[group[1]]\n\n                group_name = group[0]\n\n                if rval is not None:\n                    # pylint: disable=no-member\n                    if group_name in rval:\n                        raise YeditException(\"While merging config files, the key name was duplicated. \" +\n                                                \"Check your configuration file"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\ttemp_list = []\n\t\tfor node in nodes:\n\t\t\tif (isinstance(node, MappingNode) and\n\t\t\t   isinstance(node.value, MappingNode) and\n\t\t\t   node is not node.value and\n\t\t\t   isinstance(node.value.value, MappingNode) and\n\t\t\t   node.value.value is not None and\n\t\t\t   node.comment!= '0'):\n\t\t\t\tfor k in node.value.value.keys():\n\t\t\t\t\tif k not in temp_list:\n\t\t\t\t\t\ttemp_list.append(k)\n\t\tfor node in temp_list:\n\t\t\tif not hasattr("}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\n        # TODO: Add more error checking to ensure we still have an empty topdown.\n\n        # If no topdown and not merge_nodes are supplied in either config_paths or root,\n        # a list of them is assumed to be [root_path,...]. \n        if topdown and not data_structures.is_loaded_with_root(topdown):\n            raise Exception(\"Cannot find topdown configuration file: %s\" % topdown)\n        elif not data_structures.is_loaded_with_root(root):\n            raise Exception(\"Could not find configured topdown configuration file: %s\""}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\treturn deep_merge_nodes(to_merge(nodes, deep=True, merge_lists=True))\n\ntry:\n    from django.conf.urls.defaults import *\nexcept ImportError:\n    from django.conf.urls import *\n\nfrom django.conf import settings\nfrom django.conf.urls.i18n import i18n_patterns\nfrom django.views.defaults import page_not_found\n\nfrom.views import home\n\nurlpatterns = i18n_patterns(\n    url(r'^$', home, {'template_name': 'zerver/helloworld/hello."}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\t# pylint: disable=too-many-locals\n\t\treturn yaml.merge(\n\t\t\t[\n\t\t\t\t(node, merge(node.children))\n\t\t\t\tfor node in deep_merge_nodes(node.children)\n\t\t\t]\n\t\t)\n\ndef flatten(nodes, name):\n\t\"\"\"\n\tFlatten a list of nested borgmatic tags into a single list of nested python dictionaries.\n\tExample of an input structure:\n\n    {\n        \"first.yaml\":\n            [{\"second\": \"2015\", \"fourth\": \"2015\"},\n             {\"second\": \"2015\", \"third\": \"2015\"}"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\ttry:\n\t\t\tnodes.deep_merge_nodes(\n\t\t\t\t[\n\t\t\t\t\t(\n\t\t\t\t\t\tscalars.ScalarNode(\n\t\t\t\t\t\t\ttag='tag:yaml.org,2002:str', value=dv['name'])\n\t\t\t\t\t),\n\t\t\t\t\t(\n\t\t\t\t\t\tscalars.ScalarNode(\n\t\t\t\t\t\t\ttag='tag:yaml.org,2002:str', value='retention',\n\t\t\t\t\t\t\toptions=dv['options'],\n\t\t\t\t\t),\n\t\t\t\t\t(\n\t\t\t\t\t\tscalars.ScalarNode(\n\t\t\t\t\t\t\ttag='tag:yaml.org,2002:str', value='keep_hourly',\n\t\t\t\t\t\t\toptions=dv['"}, {"generated_text": "def deep_merge_nodes(nodes):\t\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.\t\"\"\"\n\t\tfor node in nodes.itervalues():\n\t\t\tmerge_dicts(*deep_merge_nodes(node))\n\t\treturn dict((node[\"tag\"], node) for node in nodes.itervalues())\n\n\nif __name__ == \"__main__\":\n\tprint deep_merge_nodes([\n\t\t'root',\n\t\t'config',\n\t\t'mapping.yaml',\n\t\t'--all','-d'\n\t])\nfrom __future__ import unicode_literals\n\nimport re\nimport time\n\nfrom.common import InfoExtractor\nfrom..utils import (\n    float_or_none,\n    qualities,\n"}]}
{"task_id": "62b438b666fea644fe22cc78", "prompt": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n# Get an ArgumentParser instance.\nap = ArgumentParser(description='This tool implements the basic functionality of the Australian National Science (Economic) Project.')\n\n# Add the subcommands to the parser.\nargs = vars(ap.parse_args())\t\n\n# Parse the arguments and get the subcommands.\nap.parse_args(args)\n\n# Set the name (and arguments for a run).\n# Create a parser for the arguments.\nap.set_parser(ap.get_parser())\n\ntry:\n\t# Run the main program. The return values"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\t\n\t# Create a minimal OptionParser instance and set the argument parser to the argument parser.\n\tparser = argparse.ArgumentParser(description='')\n\tparser.add_argument('-i', '--input-file', '--input_file_list', metavar = 'FILE', help='')\n\tparser.add_argument('-o', '--output-file_list', metavar = 'FILE', help='')\n\tparser.add_argument('-n', '--number_of_threads', metavar = 'NUMBER', type=int, default = 1,\n\t\t                   help = 'Number of threads to open when running program.')\n\tparser"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = ArgumentParser()\n\t\t# The following variables are all optional.\n\t\tparser.add_argument(\"-a\", \"--args\",\n\t\t\taction = \"store_true\",\n\t\t\tdest=\"args\",\n\t\t\tdefault=False,\n\t\t\thelp = \"show this message and exit\")\n\n\texcept Exception as e :\n\t\tprint( \"Command-line error: \" + str(e) )\n\t\treturn None\n\treturn parser.parse_args(args = arguments)\n\n\ndef getUserArgs():\n\t\"\"\"\n\tRetrieve arguments using the command-line arguments.\n\tIf there are no"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\targ_parser = argparse.ArgumentParser()\n\t\targ_parser.add_argument('--file',required=True,nargs=1,required=True)\n\t\targ_parser.add_argument('args',nargs=argparse.REMAINDER,default=argparse.SUPPRESS)\n\t\targs, extras = arg_parser.parse_known_args(arguments)\n\t\treturn args, extras\n\n\t'''\n\tParse the arguments and save them.  The.args string contains the arguments that might\n\tbe overridden by each user. If you want to parse those, add them to the argument parser."}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t \n\n    parser = Parser(prog=\"SpartaiPep\")\n    \n    if len(arguments) == 0:\n        return parser.parse_args(sys.argv[1:])\n  \n    return parser.parse_args(sys.argv[1:])\nfrom django.conf import settings\nfrom django.contrib.gis.geos import GEOSGeometry, Point, Polygon\n\ndef set_coordinates(coords, srid=4326):\n    \"\"\"\n    Re-set the geometry coordinates from PostGIS. Coordinates are in degrees.\n    Spatial backends are queried with their back-end database connection.\n    \"\"\"\n    # Note: SRID of the"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(prog=prog, description=description, epilog=epilog, parents=[parser])\n\tparser._parents = [parser]\n\tparser.parse_args(arguments[1:])\n\treturn parser\n\ndef main(argv):\n\tparser = parse_arguments(argv[1:])\n\targs = parser.parse_args(argv[1:])\n\treturn parser.parse_args(args) \n\nif __name__ == \"__main__\":\n\tmain(sys.argv[1:])\nimport cv2\nimport math\n\nclass Point():\n\n    def"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=\"Pywikibot Bot - A Bot for Advanced \"\n\t\t\t\t\t\t\t\t\t\t\tlong-term communication with the Horrible framework \"\n\t\t\t\t\t\t\t\t\t\t\t\t\tPython Toolkit\", prog=\"LorrBot\",\n\t\t\t\t\t\t\t\t\t\t\t\t\tversion=\"1.0\")\n\t\tparser.add_argument(\"-l\",\"--language\",       action=\"store\", dest=\"lang\", choices=self._languages,\n\t\t\t\thelp=\"Select which language\", dest=\"lang\")\n\t\tparser.add_argument(\"-t\",\"--template\",            action=\"store\", dest=\"template\",\n\t\t\t             help=\"Select which template\", dest=\""}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\n#\targuments = parse_command( sys.argv[1:] )\n\t\n#\t# Set the global options\n#\toptions = parse_options()\n\n#\t# Set the logging file\n#\tlogger = configure_logger(logger_name, options)\n\n\n#\t# Run the experiment\n#\texperiment = run_experiment(argv)\n\n#\t# Exit cleanly\n#\tsys.exit(0)\n\n\n#def usage( msg ):\t\n#\t\"\"\"Display msg and a message about what was wrong with the program\"\"\"\n#\t\tprint \"Error:"}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\nparser = argparse.ArgumentParser(description='Make a CDFT file from a list of CDC values. Inherits from the argparse module')\n#parser.add_argument('cdsfile', type=str, help='The CDS file name', nargs='+', required=True)\nargs = parser.parse_args()\n#\n# Check the arguments\n#\nif len(args.cdsfile) == 0:\n    parser.print_help()\n    raise argparse.ArgumentTypeError('--cdsfile argument requires an input cds file')\n    \n# Parse the arguments\ncds = CDS(args."}, {"generated_text": "def parse_arguments(*arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\t\"\"\"\n\t\tparser = argparse.ArgumentParser()\n\t\tparser.add_argument(\"command\", \n\t\thelp=\"name of the command-line arguments\")\n\t\tparser.add_argument(\"-v\", \"--verbose\", \n\t\taction=\"store_true\", \n\t\thelp = \"show logging\")\n\t\targs = parser.parse_args(arguments)\n\t\treturn args\n\n\t# The command-line arguments to the script must all have the following attribute names\n\targs = parse_arguments(sys.argv[1:])\n\n\t# If the user didn't enter a command, print a generic error message and\n\t# exit"}]}
{"task_id": "62b438b666fea644fe22cc72", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\n\tparser.add_argument(\"-o\", \"--output\", type=str,\n\t\t                  default=\"default_output.txt\",\n\t\t                  help=\"path to the output file.\")\n\n\tparser.add_argument(\"-s\", \"--skip_headers\", type=int,\n\t\t                  default=\"0\",\n\t\t                  help=\"skip header lines.\")\n\n\tparser.add_argument(\n\t\t\"-v\", \"--verify_headers\",\n\t\taction='store_true',\n\t\tdefault=False,\n\t\thelp=\"verify that there are headers after all headers.\")\n\n\tparser.add_argument(\n\t\t\"file\","}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\tparser.add_argument('-v', '--verbose', \n\t\thelp=\"Turn on VERBOSE logging\",\n    \t\tnargs=-1,type=str,nargs=\"+\",\n\t\tdefault=\"\",action=\"store_true\")\n\t\n\treturn vars(parser.parse_args())\n\ndef print_debug_info():\n\t\"\"\"\n\tPrint global DEBUG information into the output to the console.\n\t\"\"\"\n\targs = vars(parser.parse_args())\n\t\n\targs['-v'] = '1'\n\t\n\ttry:\t\n\t\tprint args[\"-v\"].split()[0]\n\texcept"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t \n\tparser.add_argument(\n\t\t# --version\n\t\tdest ='version_of_package',\n\t\t# AKA 'alpha', an int defining the range of the versions from 0 to 6\n\t\tmetavar = 'NUM',\n\t\t# Specify an alternative version range to use (starting at 0)\n\t\tmetavar = 'IDY_PACKAGES',\n\t\t# Specify the name of a subdirectory\n\t\tmetavar = 'SUBDIR',\n\t\t# Specify the build tree directories for the package. Overrides $PWD environment variable\n\t\tmetavar = 'SRCDIR',\n\t\t# Specify the output directory for"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\t# Note that we just pass the argument through to the right constructor.\n\t\tparser.add_argument('flag_string', nargs='?', default=None, metavar='FLAG',\n\t\t\thelp='the flag string to parse')\n\n\t\t# Parse the argument. This should have arguments in a \"FlagList\" or the\n\t\t# \"FlagString\" subparser.\n\t\tflags = parser.parse_args(sys.argv[1:])\n\t\tprint 'flags:'\t\t\tflags\n\t\t\n\t\treturn flags\n\t\ndef main(argv):\n\t# Parse the arguments.\n\tflags = parser_flags("}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tg = parser.add_argument_group(\"Flags\")\n\t\tg.add_argument('--verbose', '-v', dest='verbose', action=\"store_const\", const=DebugFlags,\n\t                    default=Config.DEFAULT_VERBOSE)\n\t\tg.add_argument('--quiet','-q', dest='quiet', action=\"store_const\", const=DebugFlags,\n\t                    default=Config.DEFAULT_QUIET)\n\t\tg.add_argument('-q','--quiet-quick',dest='quiet', action=\"store_const\", const=DebugFlags,\n\t                    default=Config.DEFAULT_QU"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tdef _parser_flags(group):\n\t\t\tflags = ['--%s' % group.dest, '--%s' % group.action]\n\t\t\tflags.extend(parser.option_list)\n\t\t\treturn flags\n\t\treturn _parser_flags\n\n\tdef add_option(self, *args, **kwargs):\n\t\tflags = self.parser_flags(*args, **kwargs)\n\t\tparser.add_argument(*args, **kwargs)\n\t\tself._parser.defaults.update(flags)\n\ndef list_opts(config=None):\n\tdef add_parser_options"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n    flags = 'adafrfa'\n\n\tparser.add_argument(\"-m\", \"--max-line-length\",\n\t\t\t\t\t      dest=\"max_line_length\",\n\t\t\t\t\t      type=int,\n\t\t\t\t\t      metavar=\"N\",\n\t\t\t\t\t      help=\"\"\"\n\t\t\t\t\t\tMaximum allowed line length.\n\t\t\t\t\t\t\"\"\"\n\t\t\t\t\t\t      )\n\n    def parser_error(parser, message):\n\t\"\"\"\n\tIf the parser encountered a syntax error in the command-line, show the syntax\n\toutput instead.\t\n\n\t\n\t:param parser: the parser object\n\t:type parser: argparse.ArgumentParser \n\t\t\n\t:param message"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\treturn tuple(map(str, parser.option_list))\n\n\ndef main(argv):\n\tparser = GetOpts()\n\tif os.path.isfile(sys.argv[0]):\n\t\tsys.argv = [sys.argv[0]] + [arg.strip()\n\t\t\t\t\t\t\t\t\t\t\t\tfor arg in sys.argv[1:]\n\t\t\t\t\t\t\t\t\t\t\t\tif not arg.startswith(\"-\")]\n\t(opts, args) = parser.parse_args(argv)\n\n\t# Override some options of the calling module\n\tfor i in xrange(len(args)) :\n\t\tif \"--\" in"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\n\tflags = \"\"\n\tif parser.add_argument_group(\"Flags\"):\n\t\tflags = parser.add_argument_group(\"Flags\")\n\tif parser.add_argument_group(\"Actions\"):\n\t\tactions = parser.add_argument_group(\"Actions\").actions\n\tif parser.add_argument_group(\"Actions/Regex\"):\n\t\tactions = parser.add_argument_group(\"Actions/Regex\")\n\n\treturn flags, actions\n\t\n      \ndef arguments(argv, p_version):\n\t\"\"\"\n\tParse the arguments.\n\t\n\t:param argv: A list of commandline"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\tparser.add_argument(\n\t\t'--port',\n\t\tdefault=':8888',\n\t\trequired=False,\n\t\ttype=int,\n\t\thelp='Port number for the remote server')\n\t\n\tparser.add_argument(\n\t\t'--file',\n\t\trequired=False,\n\t\ttype=argparse.FileType('rb'),\n\t\tdest='filename',\n\t\thelp='File describing what we will run the remote.')\n\t\n\tparser.add_argument(\n\t\t'--cmd',\n\t\tdefault='./commandline.py',\n\t\tmetavar='command line arguments',\n\t\ttype=str,\n\t"}]}
{"task_id": "62b438b566fea644fe22cc70", "prompt": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\timport os\n\t\tfrom borgmatic.cmddata import cmddata\n\n\t\tif not os.path.exists(os.path.join(\"borgmatic\", \"cmddata\")):\n\t\t\twith file(os.path.join(\"borgmatic\", \"cmddata\"), \"w\") as f:\n\t\t\t\tf.write('\\n'.join(cmddata.commandparser._commands))\n\n\t\tif not os.path.exists(os.path.join(\"borgmatic\", \"cmdargs\")):\n\t\t\treturn None\n\n\t\treturn borgmatic.cmddata\n\telse:\n\t\timport subprocess\n\n\t"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\t# print \"Executing bash completion script...\", sys.argv\n\tcmd = 'cd %s; py.test.cmdline; exec Py.test.cmdline% %s' % (borgmatic_path, sys.argv[0])\n\tstatus = os.system(cmd)\n\tbash_completion = os.popen('/bin/bash -c \"< %s\"' % cmd).read()\n\treturn bash_completion\n## END def bash_completion class\n\n\n\n## This will be called if a borgmatic command does not exist on the system.\n## It expects that users who didn"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\tif len(sys.argv) == 2:\n\t\t\tret = sys.argv[1]\n\t\t\tif ret[0] == \"/\":\n\t\t\t\treturn ret\n\t\telse:\n\t\t\tret = raw_input(\"The command-line 'build bash completion' command expects a command, e.g: build bash completion && chatter\")\n\t\t\tif ret.lower() == \"build\":\n\t\t\t\treturn ret\n\t\t\telif ret.lower() == \"chatter\":\n\t\t\t\tprint(repr(ret))\n\n\tdef set_command(self, cmd, value):\n\t\t\"\"\"\n\tSet the command-"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\n\t\n\tBORGMIRROR_COMMANDS_DIR = join(\".\", \"borgmatic_commands\")\n\tBORGMIRROR_COMMAND_DIR = join(\".\", BORGMIRROR_COMMANDS_DIR, \"borgmatic_command\")\n\tBORGMIRROR_COMMANDS = [join(BORGMIRROR_COMMANDS_DIR, i) for i in os.listdir(BORGMIRROR_COMMANDS_DIR) if isfile(join(BORGMIRROR_COMMANDS_DIR, i))]\n\n\tprint(BORGMIRROR"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\t# This could require changes in the future.\n\t\t# This will work with 'bash completion --help' output.\n\t\t# See: http://stackoverflow.com/questions/260899/how-to-check-if-executable-exists-in-python\n\t\timport borgmatic.bashcompletion\n\t\treturn borgmatic.bashcompletion.BorgmaticCodeCompleter(bash_completer)\n\n\n###############################################################################\n#\n# Copyright 2012 Webdriver_name committers\n# Copyright 2012 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\tcmd = \"pyborgmatic -q -C\".split()\t\t\t\t\t\t\t# get command-line argument list\n\t\tcompletion = subprocess.check_output(cmd).decode(\"utf-8\")\t\t\t\t\t# get completion result\n\t\treturn completion\n\ndef test_borgmatic_complete(file1):\n\treturn bash_completion() == bash_complete(file1)\n\ndef test_list_projects():\t\"\"\"\n\tList projects\n\t\"\"\"\n\tprojects = bash_completion().split(\"\\n\")\n\treturn projects\t\n\t\ndef generate_project_file(project,"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\treturn \"bash completion\"\n# coding: utf-8\nfrom __future__ import unicode_literals\nimport os\nimport tempfile\n\nfrom django.conf import settings\nfrom django.core.urlresolvers import reverse_lazy\nfrom django.db.models import get_model\nfrom django.test.utils import override_settings\n\nimport pytest\nfrom mock import call, create_autospec\nfrom mock import ANY, patch, MagicMock\nfrom pyquery import PyQuery\nfrom waffle.testutils import override_switch\n\nfrom olympia import amo\nfrom olympia.activity"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\tbash_completion_script_arg_parser = argparse.ArgumentParser(\n\t\t\tprog=\"bash-completion\",\n\t\t\tdescription=\"bash-completion, for the borgmatic command.\",\n\t\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t\t\tepilog=get_documentation())\n\t\tbash_completion_data_arg_parser = argparse.ArgumentParser(\n\t\t\tprog=SYNTHETIC_USAGE_MESSAGE,\n\t\t\tdescription=SYNTHETIC_USAGE_MESSAGE,\n\t\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t\t\tepilog=SYNTHETIC_USAGE_MESSAGE,\n\t\t)"}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\n# vim: tabstop=4 expandtab shiftwidth=4 softtabstop=4\n#!/usr/bin/python\n\nimport re\n\ndef strip_comments(s):\n    return s.lstrip().replace(\"  \",\" \").replace(\" \\n\", \"\").replace(\" \",\" \").replace(\";\", \" \")\n\ndef extract_title(s):\n    if \"title\" in s:\n        return s.split(\"title=\", 1)[1].strip()\n    else:\n        return \"\"\n\ndef clean_title(s):\n    s = s.lower()\n    s = strip_comments(s)\n    s ="}, {"generated_text": "def bash_completion():\t\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.\t\"\"\"\n\t\toptions = dict()\n\t\toptions[\"command\"] = os.path.basename(sys.argv[0])\n\t\toptions[\"command_arguments\"] = sys.argv\n\t\toptions[\"action\"] = os.environ[\"BORGMON_COMMAND_COMMAND\"] \n\n\t\treturn options\n\n\t##\n\t## The actual command, to be written to a file like sys.stdin. Remember the\n\t## file descriptor of the resulting bash completion, and return the actual command\n\t## as a Python 3 string.\n\tdef completion(self):\n\t\t\"\"\"\n\tA completion script that reads from stdin and"}]}
{"task_id": "62b438a266fea644fe22cc2e", "prompt": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tglobal __parsedArgs\n\t\tparser = argparse.ArgumentParser(parents=[parser] + unparsed_arguments)\n\t\tparser._add_argument(\"-l\", \"--logfile\", help=\"Filename containing command-line arguments (-n <LOGFILE>)\",\n\t\t\t\t\t\t\t\t\t\ttype=argparse.FileType('w'), default=sys.stdout)\n\t\tparser._add_argument(\"-d\", \"--debug\", help=\"Display debugging output\",\n\t\t\t\t\t\t\t\t\t\taction=\"store_true\", default=False)\n\t\tparser._add_argument(\"-v\", \"--verbose\", help=\"More verbose output\",\n\t\t\t\t\t\t\t\t\t\taction=\"store_true\","}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\n\t# Parse command-line arguments.\n\tparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser.add_argument(\"--log-level\", dest=\"log_level\", default=\"INFO\")\n\tparser.add_argument(\"--debug\", dest=\"debug\", action=\"store_true\")\n\tparser.add_argument(\"--quiet\", dest=\"quiet\", action=\"store_false\", default=True)\n\tparser.add_argument(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\")\n\tparser.add_argument('-p', '--print"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\t# TODO implement a regex to give additional info to ArgumentParser?\n\t\t#   - Add better \"help\" message for argparse?\n\t\t#   - Perhaps add more documentation and syntax?\n\t\tparser = argparse.ArgumentParser(\n\t\t\tprog='gimmedb',\n\t\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t\t\tdescription='Python script agnostic way for a gimmedb tool for identifying Grimmings.',\n\t\t\tusage='%(prog)s [--version]')\n\t\t\t# - Add more arguments to make usage more clear.\n\t\tparser.add_argument('--version', action"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\ttry:\n\t\t\timport argparse\n\t\t\tparser = argparse.ArgumentParser()\n\t\t\tparser.register(\"global\", \"global\")\n\t\t\targs = parser.parse_args(*unparsed_arguments)\n\t\texcept ImportError as e:\n\t\t\tprint(\"error: unable to interpret the arguments as \" + str(e));\n\t\t\texit(1);\n\n\t\tglobal __name__, __version__, __package__\t\t\t\t= __import__(\"mylib\").MyLib()\n\n\t\treturn args\t\t# return a Namespace instance\n\ndef main():\n\ttry:\n\t\topts, args = parse_arguments();\n\t\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\t\n\tglobal arguments\n\targuments = {}\n\tunparsed_arguments = []\n\n\tfor argument in unparsed_arguments:\n\t\tif not argument == 'local':\n\t\t\tparsed = argument.replace(\"=\",\"=\")\n\t\t\tparsed = parsed.replace(\" \",\"=\")\n\t\t\tunparsed_arguments.append(parsed)\n\t\telif argument.startswith('{{') and argument.endswith('}}'):\n\t\t\tif \" \" in argument:\n\t\t\t\t args = argument.split(\" \")\n\t\t\t\t arguments[args[0]] = args[1]\n\t\t\telse:\n\t\t\t\targuments[argument] = False\n\t\t\t\t\n\treturn arguments"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\tif len(parsed_arguments) < 1:\n\t\t\tret_args = Namespace()\n\t\telse:\n\t\t\tret_args = parsed_arguments[1:]\n#\t\t\tprint ret_args\n#\t\t\tprint \"\"\n\t#\t\t\t\n#\t\tret_args.__dict__.update(args)\n#\t\treturn ret_args\n#\t\n# #def parse_config(*config_file):\n# # \t\t\n# #parse_arguments(*config_file)\n\t\n\targs = parse_arguments()\n\targs.run = True\n\t\n\tsys.exit(args"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(description=\"A command-line tool to build an autoconf file.\")\n\n\t# Add the parser arguments\n\tconfig_group = parser.add_argument_group(\"Configuration\", \"Global Arguments\")\n\tconfig_group.add_argument(\"-c\", \"--config\", help=\"Configuration File\")\n\tconfig_group.add_argument(\"-e\", \"--exec\", help=\"Execfile\")\n\tconfig_group.add_argument(\"-t\", \"--target\", default=None, nargs=1, help=\"Target\")\n\tconfig_group.add_argument(\"-r\","}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\n\tparser = argparse.ArgumentParser(description='Commandline interface for the C++ driver')\n\tsubparsers = parser.add_subparsers()\n\tparser_initialize = subparsers.add_parser(\"initialize\",\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdescription=\"Initialize the C++ driver\")\n\tparser_initialize.add_argument(\n\t\t\"name\",\n\t\tnargs=\"?\",\n\t\thelp=\"name of C++ driver you wish to initialize (e.g. C++)\", dest=\"name\", metavar=\"name\")\n\tparser_initialize.set_defaults(function=execute_driver)\n\n\tparser_update"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\n\t# parse cmd arguments\n\tdef parse_args(*unparsed_commands):\t\n\t\t\"\"\" Parse the command-line arguments with which this script was invoked, parse the arguments \"\"\"\n\t\timport argparse\n\t\tparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter, add_help=False)\n\t\tparser.add_argument('--version', action='version', version='You')\n\t\tparser.add_argument('--quiet', '-q', action='store_true')\n\t\tparser.add_argument('--version', action='version', version='The')\n\t\tglobal args\n\t\t"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\t\"\"\"\n\t\n\t\n\tparser = optparse.OptionParser(usage=\"python %prog -f json_config_file -i file.json\" + \n\t              \" [options]\")\n\tparser.add_option(\"-p\", \"--project\", dest='project', help='the project name', default=project_name, metavar='PROJECT', \n\t                     nargs=1)\n\t#parser.add_option(\"-s\", \"--source\", dest='source', help='name of source for the repository', default=source_name, metavar='NAME', \n                     nargs=1)\n\t\n\toptions, args = parser.parse_args("}]}
{"task_id": "62b438a266fea644fe22cc2d", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\t# The actual action that gets passed to the command, as well as any\n\t\t# subactions that have been processed.\n\t\tactions = []\n\t\taction_name_map = {}\n\t\t# The namespace of our subparser.\n\t\tnamespace = {}\n\n\t\tfor index, arg in enumerate(unparsed_arguments):\n\t\t\t# The action that got passed to the command, as well as any\n\t\t\t# subactions that have been processed.\n\t\t\taction_name = arg[0]\n\n\t\t\tif action_name == 'help':\n\n\t\t\t\toptions = arg[2:]\n\n\t\t\t\t# Extract the"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n# Parse arguments of the form:\n#   -s --repository\n#   -R <repository>\n#   -c --config-file=path\n#\n# The first, that's all, will always be parsed as a command line argument.\n# If none of these are present, then you get an empty result as a command line argument.\n# If a subcommand name does not exist in the namespace, then your subcommand\n# will be treated as a subcommand name and not parsed as a subcommand arguments.\n#\n# Return a tuple of all subcommand names. The arguments in this case are\n"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\tdef parse(self, args):\t\t\t\n\t\t# find the repository in arguments\n\t\trepository = repository_path(args)\n\t\t\n\t\t# find what action the subparser used (args.subparser_action) was.\n\t\tsubparser_action = None\n\t\ttry:\n\t\t\tfor arg in self.args:\n\t\t\t\tif arg in self.subparsers:\n\t\t\t\t\tsubparser_action = arg\n\t\t\t\t\tbreak\n\t\t\t# if the user did not specify an additional action just return the result\n\t\t\t# and the list that contains the subparser's registered arguments\n\t\t\t# (if that list is"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\t# Use the first parser class to parse subcommands first; there should only ever be\n\t\t# one in there!\n\t\taction = subparsers[0]\n\t\targs = unparsed_arguments[0]\n\t\tif args[0] is not None:\t\t\t\t\n\t\t\t# Take the first parser class argument as a namespace and a dict from\n\t\t\t# the namespace. This will make it easier to test all argument types with\n\t\t\t# different namespace values.\n\t\t\tnamespace, remaining = args[0], {}\n\t\t\tremaining_keys = remaining.keys()\n\t\t\tfor k in remaining_keys:\n\t\t\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\t# get the command name\n\n\tcommand_name = unparsed_arguments.pop(0)\n\n\t# we need to get the subparser name (if any), because argparse has subparser specific\n\t# arguments but isn't really a parser object\n\tif not hasattr(argparse, command_name):\n\t\tname = \"parse_%s\" % command_name.replace('_', '-')\n\t\tsubparser = subparsers.add_parser(name, description=module_description)\n\t\tsubparser.set_defaults(func=get_subcommand)\n\n\t\tfor arg in unparsed_arguments:"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\tif (any(x in subparsers for x in ('actions', 'action', 'parser'))):\n\t\tif (subparsers.has_key('repository')):\n\t\t\tparsed_namespace = subparsers['repository']\n\t\telse:\n\t\t\tparsed_namespace = parser.add_parser('namespace', help='Set the namespace to identify where these arguments are stored')\n\n\t\tsubparser_actions = []\n\t\t\n\t\tfor arg in args:\n\t\t\t# For each argument\n\t\t\targ = arg.strip()\n\n\t\t\t# For each keyword argument\n\t\t\tif (arg.find(\"=\")!=-1):\n\t\t\t\t"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\t# Argument parsers - register their own argument parsers when they're parsing\n\t# TODO we might want to change the behaviour of subparsers later\n\targparse._SubParsersAction.subparsers = [ ]\n\targparse._ParseAction.argparse_handlers.append(argparse_parser_factory)\n\n\t# Note: parser.add_argument adds subparser specific arguments.\n\targparse._ArgumentGroup.add_parser(\targparse.ArgumentParser,\n\t\"\"\"Subcommands of this package do not return command-line arguments. The actual\ncommand-line parser has all subparser specific arguments, and may have one"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\t_args = vars(unparsed_arguments)\n\t\tsubparser = subparsers.add_parser(u\"subparser\")\n\t\tsubparser.add_argument(u\"repository\", metavar=\"URI\", nargs=1, action=\"store\", dest=u\"repository\")\n\t\tsubparser.add_argument(u\"subcommand\", metavar=\"COMMAND\")\n\tresult = parser.parse_args(u\"repository [-repository...]\")\n\tglobal _args, global result, _args\n\ttry:\n\t\treturn parser.parse_known_args(args=_args, unparsed_arguments=_"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n    # Parse arguments that we will need to accept at the command-line.\n    parser = actions.ArgumentGroup(\"Action arguments\", formatter_class=argparse_subparser)\n    subparsers = parser.add_subparsers(help='Action arguments sub-commands', dest='subcommand')\n\n    parser_action = actions.argparse.ArgumentParser(add_help=False)\n    subparser = parser_action.add_subparsers(dest='subcommand')\n    \n    args, defaults = parser.parse_known_args()\n\n    if args.subcommand not in subparsers:\n        setattr(parser.parser, 'dest' +"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\t\"\"\"\n\t\n\n\tdef parse_and_apply_args(args, remaining_args):\n\t\tfrom functools import partial\n\t\tif args is None:\n\t\t\t# No arguments\n\t\t\treturn remaining_args, arguments, {}\n\t\telif isinstance(remaining_args, partial):\n\t\t\t# More args, but a callable\n\t\t\treturn remaining_args, arguments, args\n\t\telif isinstance(remaining_args, dict):\n\t\t\t# More args, a dict mapping from subparser name to the result\n\t\t\treturn args, arguments, args\n\t\telif callable(remaining_args):\n\t\t\t# A callable that uses `args"}]}
{"task_id": "62b438a266fea644fe22cc2c", "prompt": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description='PyEvo v{} {} {} {}.'.format(__version__, __author__, __license__, __project_url__))\n\t\tparser.add_argument('dataset_file_path', help=\"Path to the image data file.\")\n\t\tparser.add_argument('source_root', nargs='?', help=\"Path to the source project folder.\")\n\t\tparser.add_argument('-v', '--verbose', help=\"Output verbose info.\", action='store_true')\n\t\tparser.add_argument('-c', '--config', help=\"The Evo v{} config file path.\"."}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\ndef parse_args():\n\t\"\"\"\n\tCreate the argument parser and return it's arguments.\t\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Command line processor')\n\tparser.add_argument(\"-s\", \"--seed\",help=\"Random data seed which will always have\", type=int,default=0)\n\tparser.add_argument(\"-b\", \"--begin\",type=int,default=0)\n\tparser.add_argument(\"-e\", \"--end\",type=int,default=-1)\n\tparser.add_argument(\"-r\", \"--rounds\",type=int"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\n\tsubparsers = parser.add_subparsers(help=\"\", dest='commandhelp')\n\tsubparsers = subcommand.add_parser('clean', help=\"Clean a database\")\n\tsubparsers.set_defaults(func=clean)\n\n\treturn subparsers, subparsers.add_parser('version')\n\n\n# Copyright (c) 2014 The Bitcoin Core developers\n# Distributed under the MIT/X11 software license, see the accompanying\n# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n#\n\nimport re\nimport fnmatch\nimport sys\nimport subprocess"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\n\tif args.version:\n\t\t# print out the version\n\t\tlogger.warning(\"version {}\".format(f\"{args.version}\").format(\n\t\t\t\tf\"\"\"`{version}`\n\nPython module summary:\n\n\tVersion: {VERSION}\n\n\tBy {__date__}\n\tVersion: {VERSION_DATE}\n\n\tRelease: {RELEASE}\n\n\tDocumentation:\n\n\t{__doc__}\n\t\n\tThis program is free software: you can redistribute it and/or modify\n\tit under the terms of the GNU General Public License as published by\n\tthe Free Software Foundation, either version"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(add_help = False)\n\t\tsubparsers = parser.add_subparsers(help=\"\" )\n\n\t\t# --set\n\t\tset_cmd = subparsers.add_parser(\"set\", help=\"Sets the value of a given name.\")\n\t\tset_cmd.set_defaults(func = set)\n\t\tset_cmd.add_argument(\"name\", type = str)\n\t\tset_cmd.set_defaults(func = set)\n\n\t\t# --unset\n\t\tunset_cmd = subparsers.add_parser(\"unset\", help=\"Unsets the value of a"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\n    parser_top = subparsers.add_parser(\n        'new',\n        help=\"Create a new file-like object from disk. I.e. 'old' files can be added. \"\n             + \"Alternatively, the existing file is left untouched. \"\n             \"File-like objects (e.g. OpenOffice work, etc..) are not \"\n             \"supported. (Not yet implemented)\"))\n    parser_top.add_argument('source', help=\"Name of the file and the file name is used from this command.\")\n    \n    parser_top.add_argument('-o', '--option')"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\ndef my_parser():\n\t'''\n\tCreate a parser that parses the command line arguments and returns a parser.\n\t'''\n\tparser = argparse.ArgumentParser(description=\"Blender, an object with various functions for manipulating data.\");\n\n\tsubparsers = parser.add_subparsers(help=\"Sub-commands\")\n\t#TODO : add subparsers that support both of the --build/-build syntax and the --build-log flag\n\tsubparsers.required = True;\n\tsubparsers.required = False;\n\tsubparsers.dest = \"blender\";\n\tsubparsers"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\t# Note: this is still a list of top-level options\n\t\tparser = argparse.ArgumentParser(\n\t\t\tconflict_handler=\"resolve\", description=\"Resolve files\",\n\t\t\tepilog=\"\"\"Examples:\n\t\t\t{command} --file my_data.json --config-dir /tmp/my_config.yaml\n\t\t\t\"\"\"\n\t\t)\n\n\t\t# Add common options\n\t\tparser.add_argument(\n\t\t\t\"-q\", \"--quiet\",\n\t\t\taction=\"store_true\", default=False,\n\t\t\thelp=\"Omit error messages during --file. Exit with status 1.\")\n\t\tparser.add_"}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=description)\n\n\t\tadd_argument = parser.add_argument\t\t# Add this to the argument parser\n\t\tadd_argument('config', type = str, help = 'The path to the.conf file')\n\t\tadd_argument('--list-config', action ='store_true', default = False, required = False,\n\t\t\t\t\t\t\thelp = 'Dumps all configurations instead of the config.xml')\n\n\n\t\tappend_argument = parser.add_argument\t\t# Append this to the argument parser\n\t\tappend_argument('--configuration-path', type ="}, {"generated_text": "def make_parsers():\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\t\"\"\"\n\t\n\t\n\tparser = argparse.ArgumentParser(description=\"Convert a source file to a VCF-like format\")\n\tparser.add_argument(\"sourceFile\", help=\"the VCF Filename or handle\")\n\t\n\tparser.add_argument(\"-i\", \"--input\", help=\"VCF File for the source file.\", type=FilePath, required=True)\n\tparser.add_argument(\"-g\", \"--gapped\", help=\"Number of bases after the clip\", type=int, default=1)\n\tparser.add_argument(\"-r\", \"--clip-region\",  help=\"The genome region where the clip"}]}
{"task_id": "62ece4982e6aefcf4aabbd5f", "prompt": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\tpages = wapi_response_to_dict(response.getchildren())\t\n\t\tfor page in pages:\n\t\t\tyield page['data'], page['next'], page['count']\n\ndef get_objects(obj_type):\n\t\"\"\"\n\t\tReturns WAPI objects given type.\n\t\"\"\"\n\tif isinstance(obj_type, str):\n\t\tobj_type = obj_type.split(',')\n\tif 'objects' not in globals():\n\t\tglobals()['objects'] = wapi_objects_to_dict(obj_type)\n\treturn globals()['objects']\n\n"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n\n\tpaging_kwargs = {\n\t\t'total': max_results,\n\t\t'result_key': 'paging',\t\t\n\t\t'result_field': 'next_page_token'\n\t}\n\n\tdef list(self, response):\n\t\t\"\"\"\n\t\tGet list item.\n\n\t\tArgs:\n\t\tresponse (list): List item list to be returned.\n\t\t\"\"\"\n\t\titems = []\n\t\tfor paging_response in response:\n\t\t\ttry:\n\t\t\t\titems.extend(self.list_response(paging_response))\n\t\t\texcept Exception as e:\n\t\t\t\terrors."}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n    for i in range(1, max_results + 1): \n        wapi = response[i]\n        yield wapi\n\ndef get_next_page(request_path_url, page_size):\n    \"\"\"Return next page of results.\n\n    Parameters:\n    request_path_url (str): Location of WAPI endpoint for which to retrieve.\n    page_size (int): Size of current page.\n\n    Returns:\n    wapi.data.Page object containing next page of results.\n    \"\"\"\n    page_num = 0\n    while page_num < page_size:\n        response = request_path"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n    def next_marker_generator(response):\n        \"\"\"\n        Returns wapi object for the next page of markers returned.\n        \n        Arguments:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n\n        Yields:\n        wapi.page.objects.WeightedObjectList\n        \"\"\"\n        # TODO: Write more description\n        # TODO: Return more links to the last entry.\n        return response.next_link or wapi.Page(\n            link[1].get('next'),\n            next_marker=True,\n            max_results=str("}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\tfor object in response:\n\t\t\tyield _page_generator(object, max_results)\n# ----------------------------------------------------------------------\n# Numenta Platform for Intelligent Computing (NuPIC)\n# Copyright (C) 2013, Numenta, Inc.  Unless you have an agreement\n# with Numenta, Inc., for a separate license for this software code, the\n# following terms and conditions apply:\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero Public License version 3 as\n# published by the Free Software Foundation.\n#\n# This program is"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n        for result in response:\n                yield result\n        for page in range(1, max_results):\n                result = self.paginate(\n                        response=response,\n                        page_limit=page,\n                        max_results=max_results,\n            )\n                if result.paging:\n                        yield result\n                else:\n                        result = Page(paging_result)\n                        yield result\n\n\nclass Page:\n    \"\"\"\n    Object to represent a single page\n    with a number of pages.\n    \"\"\"\n    def __init__(self, data, endpoint):\n        self.url = endpoint\n        self.data = deepcopy(data)\n\n        self.next"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n    return Generator(__construct_paging_endpoint(\"paging\"), response,\n                   max_results)\n\t\t\ndef get_properties(response):\n\t\"\"\"\n\tReturns WAPI response.\n\t\"\"\"\n\tif response == True:\n\t\treturn None\n\telif response == False:\n\t\treturn response.get(\"properties\", None)\n\telse:\n\t\treturn response.get(\"properties\", None)\n\t\ndef send_request(url, method, body={}):\n\t\"\"\"\n\tSends the specified url to WAPI, with optional body specified.\n\tReturns HTTP code and json data\n\tReturns HTTP"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n\t\n\tresp_page = wapi.Response(response[\"pageIndex\"], response[\"pageSize\"], response[\"pageToken\"])\n\tparams = {\n\t\t\"q\": \"page\",\n\t\t\"limit\": max_results,\n\t}\n\t\n\t\n\tgenerator = wapi.GAPIGateway_v1(str(response[\"maxResults\"]), params, resp_page)\n\t\n\tfor obj in generator.iterate_results():\n\t\tyield obj\n\ndef get_current_jobs(request):\n\t\"\"\"\n\tReturn current jobs data \n\t\"\"\"\n\tjobs_data = {\n\t\t\""}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n@deprecated('deprecated', '0.7', 'Use `paging` instead')\ndef page(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page.\n\nArgs:\n    response (list): WAPI response.\nReturns:\n    str: Page data.\n\n@deprecated('deprecated', '0.7', 'Use `paging` instead')\ndef _build_url(resource_list, fields, filters, labels):\n\t\"\"\"\n\tBuilds URL for the given list.\n\tUses 'fields' and 'filters' to add necessary GET params.\n\t\n\t"}, {"generated_text": "def paging(response, max_results):\t\"\"\"\n\tReturns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\t\"\"\"\n\t\n\n\twhile response is not []:\n\t\tfor item in response:\n\t\t\tyield item\n\n\t\t\tfor res in paging(item, max_results-2):\n\t\t\t\tyield res\n\t\tyield \"\"\n\t\n\n#!/usr/bin/env python\n# encoding: utf-8\n# Thomas Nagy, 2005-2010 (ita)\n\n\"\"\"\nTests for HTML.\n\"\"\"\n\nimport os, sys\nfrom StringIO import StringIO\n\nfrom zope.interface import implementer\nfrom zope.schema import TextLine, Element\nimport pygen.css\nfrom pygen.exceptions import ElementPathException"}]}
{"task_id": "62ece4982e6aefcf4aabbd60", "prompt": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t", "suggestions": [{"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\tsize = size.upper()\n\t\tsize = size.replace(\"k \", \"\")\n\t\ttry:\n\t\t\tassert int(size) > 0\n\t\texcept ValueError:\n\t\t\traise InputTypeError(\"File size \\\"%s\\\" must be an integer value between 0 and 0xFFFFFFFF (inclusive).\" %size)\n\t\treturn int(size)\n\n# vim: set expandtab shiftwidth=4 softtabstop=4 textwidth=79:\n# -*- coding: utf-8 -*-\n\"\"\"\nSimple python script that converts a JSON string to an xlsx file\nfor use in an upload.  Note that due to the"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\t# use python's math module just in case some weirdness occured\n\n\tunits = ['bytes', 'KB', 'MB', 'GB', 'TB', 'PB']\n\tmultiplier = [1000, 1, 1000, 1000, 1000, 1000, 1000000000, 100, 100, 100, 100, 100, 100]\n\tsize = str(round(float(size), 2))\n\tif size[-1].lower() not in units:\n\t\tprint('Not found size',size)\n\t\treturn ''\n\t\n\ttry:\n\t\treturn size_to_bytes(size)\n\texcept ValueError"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\treturn int(size)\n\n\t# FIXME: What is this code from? It should probably be refactored\n\t# in future?\n\t# The above formula still needs to come back\n\t# return size_to_bytes(str(size))\n\t#return str(size)\n\treturn size\n\ndef get_nice_list_format(list_type=\"str_tuple\"):\n    \"\"\"Convert a possible nice list representation given input list type\n\n    Args:\n        list_type: An acceptable list.\n    \"\"\"\n    # TODO: Check list_type type.\n    if list_type =="}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t    \treturn _human_readable_bytes_size(size)\n\telif \"K\" in size:\n\t\treturn _human_readable_kibibyte_size(size)\n\telif \"M\" in size:\n\t\treturn _human_readable_mobibyte_size(size)\n\telif \"-\" in size:\n\t\tsuf = float(size[:-1])\n\t\tif suf < 1000.0:\n\t\t\treturn \"%s %s\" %('1', suf)\n\t\t\treturn \"1\"\n\t\telif suf >= 1.0:\n\t\t\treturn \"%"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\tif type(size) is str:\n\t\t\tsize = size.upper()\n\t\tfor decimal in ['KB', 'MB', 'GB', 'TB']:\n\t\t\tif decimal in size:\n\t\t\t\tsize = decimal\n                break\n\t\tsize = ''\n\treturn int(size)\n\ndef split_number(item_number: int) -> str:\n\t\"\"\"\n\tSplits the integer into a number.\n\nArgs:\n    item_number: An integer in format: 12xxx or 10xxx\n\nReturns:\n    An integer number\n\n    Examples::\n        >>> split_number(12)\n       "}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\tsi = bytes_to_time_units_string(size)\n\t\treturn int(float(si))\n\ndef percentage_to_integer(percentage: int) -> int:\n\t\"\"\"\n\tConvert percentage to an integer.\n\n\tExamples::\n\n        >>> percentage_to_integer(\"1K\")\n        1000000000\n\n\tInputs:\n\t\tnumber, unit\n\t\tA number between 0 and 100.\n\tReturns:\n\t\tA number\n\t\"\"\"\n\tsi = int(float(percentage))\n\treturn si\n\ndef str_to_time_units_string(s: str"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\ti = 0\n\t\tj = ord(\"0\")\n\t\twhile i <= len(size) and j < len(type):\n\t\t\tif (size[i] < 1 << j)!= (size[i] >> j):\n\t\t\t\treturn \"%d %d\" % (size[i], size[i + 1])\n\t\t\tj += 1\n\t\treturn \"%d %d\" % (size[0], size[1])\t\t\n\ndef get_size_from_path(path) -> int:\n    \"\"\"\n\tConvert absolute path to human readable number\n\nTakes:\n    path:"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\n        size = size.upper()\n        if size == \"MB\":\n            return str(round(size, 1)) + \" \" + SINT_FORMAT\n        if size == \"GB\":\n            return str(round(size, 2)) + \" \" + GIGAN2_FMT\n        # FIXME: Figuring out in python2 we currently can not check the \n        # output of os.lseek() when the file was truncated.\n        return get_bytes_human_readable(size, SINT_FORMAT)\n\n    @staticmethod\n    def get_size_human_readable(filesize: str, unit: str"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\timport sys\n\t\timport math\n\t\tassert size, \"SIZE has to be a positive integer!\"\n\t\treturn int(round(size,6))\t\n\texcept:\n\t\tprint(\"ERROR: \", sys.exc_info()[1])\n\t\tsys.exit(exit(-1))\n\ndef time_to_number(seconds: float) -> float:\t\"\"\"\n\tConvert number of seconds to a number of seconds\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n\tseconds: A number or time to divide to\n\nReturns:\n"}, {"generated_text": "def size_to_bytes(size: str) -> int:\t\"\"\"\n\tConvert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\t\"\"\"\n\t\n        symbols_re = \"\\d\\.\\d*([A-Za-z]+)\"\n\t\n        match0 = re.match(symbols_re, size)\n                   \n        if (match0):\n                exponent = match0.group(2)\n                exp_value = float(exponent)\n        else:\n                return 0\n        return round(float(size), 2)\n                \n\ndef _get_size_from_system_native(sysc: str, bytes_size: str) -> Optional[str]:\n    \"\"\"\n    Get human readable file size from system native.\n\n    Args:\n        sysc: A string"}]}
{"task_id": "62ece4982e6aefcf4aabbd61", "prompt": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\ti = iter(dicts)\n\t\tsum = 0\n\t\tfor i in i: sum += i[0]\n\t\treturn sum\n\n\nclass Frozen(object):\n\t\"\"\"A frozenset of the free elements of a set. This is useful\n\tfor holding frozensets of elements that can be\n\tactually used in a set that must be kept in a stable dict.\n\n\t\"\"\"\n\tkeys = __slots__ = ['_map', '_frozen']\n\n\tdef __init__(self, frozen = []):\n\t\tself._map = {}\n\t\tfrozen = frozen or self._frozen\n\t\tfrozen"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\tsum = {}\n\t\tfor item in dicts:\n\t\t\tsum.update(item)\n\t\treturn sum\n\n\t@staticmethod\n\tdef flatToIterable(flat):\n\t\tflat = flat.copy()\n\t\tfor element in flat:\n\t\t\tif hasattr(element, '__iter__'):\n\t\t\t\tfor item in flat[element]:\n\t\t\t\t\tfor key in element.keys():\n\t\t\t\t\t\tindex = flat[element][key]\n\t\t\t\t\t\tflat[element][key] = item[index]\n\t\treturn flat\n\n\t@staticmethod\n\tdef listToFlat(list):\n\t\tassert len(list) > 0"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\treturn sum(dicts)\n\ndef get_size(size, precision=1, truncate=4):\n\t\"\"\"\n\tGet approximate size of a type.\n\n\t>>> get_size(12, 2)\n\t10 # Maximum\n\t\"\"\"\n\tif truncate == -1:\n\t\tdef trunc(num, format_string):\n\t\t\treturn format(num, format_string)\n\t\treturn trunc\n\telif precision == 1:\n\t\tdef trunc(num, format_string):\n\t\t\treturn format(num, format_string)\n\t\treturn trunc\n\telif"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\n\n\t__n_dict__ = 0\n\n\tdef __init__(self):\n\t\tself.__dict__ = dict()\n\t\tself.__dict__['c_list'] = list(self.__class__.c_list)\n\t\tself.__dict__['c_dic'] = dict()\n\t\tself.__dict__['c_total'] = 0.0\n\t\tself.__dict__['sum'] = []\n\n\tdef __call__(self, *k, **kw):\n\n\t\tkey_d = kw.keys()\n\t\tkey_d.extend([str(k)] * len(k))\n\t\tif"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\tsum_vals = []\n\t\tfor d in dicts:\n\t\t\tif not isinstance(d, dict):\n\t\t\t\treturn None\n\t\t\tkeys = d.keys()\n\t\t\tsum_vals.append(len(d) if isinstance(d, dict) else len(d[keys[0]]))\n\t\treturn sum_vals\nfrom django import forms\nfrom django.contrib.admin import widgets\nfrom django.contrib.admin.widgets import AdminTextInputWidget\nfrom taggit.models import Tag\n\nfrom django.contrib.auth.forms import UserCreationForm\nfrom django.contrib.auth import password"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\treturn sum(dicts, {})\n\t\t\n\ndef _iterdict(dict_, key=None):\n\t\"\"\"Convert any iterable dict into a dictionary.\n\n>>> _iterdict([1, 2, {'a': 5}, {'a': 6}])\n{1: {'a': 5, 'b': 0}}# -*- coding: utf-8 -*-\n'''\nLocal settings\n\n- Run in Debug mode\n- Use console backend for emails\n- Add Django Debug Toolbar\n- Add django-extensions as app\n'''\n\nfrom.common import *  # noqa\n\n# DEBUG\n# ----------------------------------------------------------------"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\treturn dict(((k, d[-1]) for k, d in iteritems(d)), items)\n\treturn d.items()\ndef flatten(_list):\n\t\"\"\"\n\tFlatten items() syntax from an iterator or any other iterable to nested tuples.\n\n>>> flatten([{'a': 1, 'b': 2, 'c': 3}, {'a': 5, 'b': 0}])\n[{'a': 6, 'b': 0, 'c': 3}, {'a': 5, 'b': 0, 'c': 3}]\n>>> list(_flatten([[0, 1, 0],"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\tdicts = [d.values() for d in dicts]\n\n\t\treturn collections.Counter(dicts)\n\n\tdef _merge_dicts(self, dicts):\n\t\t\"\"\"Update the values of the given dictionaries.\"\"\"\n\t\tnew = self.copy()\n\t\tfor d1, d2 in dicts:\n\t\t\tif d1 not in new or type(d1)!= d2:\n\t\t\t\tnew[d1] = {}\n\t\t\tif d2 not in new or type(d2)!= d1:\n\t\t\t\tnew[d2] = {}\n\t\treturn new\n\n\t@property\n\t"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\treturn dict(reduce(lambda x, y: x + y, dicts, []))\n\n# TODO make it like this one at a time\ndef _dictsum(dlist):\treturn dict(((k, 0) for k in dlist))\n\nclass Fitness:\n\t\"\"\"\n\tFitness-like datastructure: a datastructure that allows easy to manipulate\n\tdata structures as well as list-specific datastructures.\n\n\tEach item in the datastructure is represented by a \"key\", which\n\tis the name of a field in the list of values and each record in the\n\t"}, {"generated_text": "def _dictsum(dicts):\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\t\"\"\"\n\t\tsum_keys = sorted(dicts.keys())\n\t\tsum_values = ['%s: %s' % (dictkey, value) for dictkey, value in dicts.items()]\n\t\treturn''.join(sum_values),''.join(sorted(sum_keys))\n\n\tsums = []\n\tfor value in dicts:\n\t\tsum_keys = sorted(dicts[value].keys())\n\t\tsums.append('%s: %s' % (dictkey, sums[value][dictkey]))\n\treturn''.join(sums)\n# -*- coding: utf-8"}]}
{"task_id": "62ece4982e6aefcf4aabbd62", "prompt": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tfor k in url_args:\n\t\t\tif (isinstance(k, unicode)):\n\t\t\t\tk = k.encode('utf-8')\n\t\t\telif (isinstance(k, str)):\n\t\t\t\tk = ''.join(map(lambda c: c.encode('utf-8'), k))\n\t\t\tstr_args[k] = url_args.pop(k)\n\t\turl_args.update(url_args)\n\n\treturn unicode(url_args)\n\ndef _parse_query_string(query_string):\n\t\"\"\"\n\tReturns the query string from the url\n\t\n\t"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tif not isinstance(url_args, tuple):\n\t\t\tfor x in url_args:\n\t\t\t\turl_args[x] = url_args[x].replace('\\\\','/').replace(\"//\", \"/\").replace(\":443\")\n\n\treturn url % url_args\n\n\ndef _get_url(base_url, extra_param=\"\"):\n\tif not base_url:\n\t\treturn None\n\n\ttry:\n\t\treq = urllib2.Request(base_url + extra_param)\n\t\tres = urllib2.urlopen(req)\n\t\treturn res.read().strip()\n\texcept"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\t# get rid of any stray slashes at the end of the URL and/or the url_arg list,\n\t\t# for later insertion by cmd line option processing\n\t\turl_items = [url_arg[1] if len(url_arg) > 1 else url]\n\t\tfor i in range(0, len(url_items)):\n\t\t\turl_items.insert(0, url_items[i] % quote)\n\t\treturn ''.join(url_items)\n\n\tbase_url = get_base_url()\n\n\t# If relative path is given, we use the"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\targs_for_replace_url_args = \\\n\t\t\tutils.normalize_url_args_for_url_args(url_args)\n\t\tfor key, value in url_args.items():\n\t\t\targs_for_replace_url_args[key] = \\\n\t\t\t\tUtils._clean_url_args_for_url_args(value)\n\t\treturn url + url_args + args_for_replace_url_args\n\n\tdef _replace_form_urlencoded(self, content):\n\t\tif \"form-urlencoded\" in content:\n\t\t\treturn content\n\n\t"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tif url_args:\n\n\t\t\targ_list = [x if x!= \"no_url\" else \"\" \\\n\t\t\t\t\t\tfor x in url_args.split(\"&\")]\n\t\t\turl_args = \",\".join(arg_list)\n\t\telse:\n\t\t\turl_args = url\n\n\t\treturn url_args#!/usr/bin/python\n# -*- coding: utf-8 -*-\n#\n# === This file is part of Calamares - <http://github.com/calamares> ===\n#\n#   Copyright 2014, Teo Mrnjavac <http://www."}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\t# pylint: disable = unused-argument\n\t\tdef helper(match):\n\t\t\tmatch = regex.escape(match.group(1))\n\t\t\tfor key, value in url_args.items():\n\t\t\t\tif not match.group(0).startswith(key):\n\t\t\t\t\tmatch = regex.escape(match.group(0))\n\t\t\t\toutput_arg = match.group(1)\n\t\t\t\targs[key] = output_arg\n\t\t\t\treturn output_arg\n\t\treturn helper\n\n\n\nclass UrlListParser:\n\t\"\"\"\n\tParse a list of url strings\n\t\"\"\"\n\tdef __"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tif isinstance(url_args, (types.ListType, tuple)):\n\t\t\tfor i in range(len(url_args)):\n\t\t\t\t\tif not isinstance(url_args[i], (types.ListType, tuple)):\n\t\t\t\t\t\turl_args[i] = (url_args[i],)\n\t\t\t\t\t_replace_url_args(*url_args[:i] + url_args[i + 1:])\n\t\telse:\n\t\t\t# Convert everything to string.\n\t\t\tfor i in range(len(url)):\n\t\t\t\t\tif isinstance(url[i], (types.ListType, tuple)):\n\t\t\t\t\t"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tif not url_args:\n\t\t\treturn url\n\t\tif url.startswith(\"http\"):\n\t\t\tparts = list(urlparse.urlparse(url))\n\t\t\tparts[0] = url\n\n\t\t\tif parts[1] == '':\n\t\t\t\turl_args.extend([\"force\", \"forceToDisk\"])\n\n\t\t# If we're coming from a remote server, make sure that any files\n\t\t# found in the command line are actually URL escaped; otherwise,\n\t\t# re-evaluating the url in shell variables will not work.\n\t\telif url.startswith(\"https://\"):\n\t\t\turl_args"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\tfor k,v in url_args.items():\n\t\t\tif isinstance(v, list) and v and all(e in url_args[k] for e in v):\n\t\t\t\tfor vv in v:\n\t\t\t\t\turl_args[k] = vv\n\t\t\telif isinstance(v, str):\n\t\t\t\turl_args[k] = v.encode('utf-8')\n\n\t\treturn url_args\n\t\t\n\tdef set_cookies(self):\n\t\t\"\"\"\n\t\tSets a cookie containing Caches and Host from the configuration data store.\n\n\t\tIf self.cookies is set,"}, {"generated_text": "def _replace_url_args(url, url_args):\t\"\"\"\n\tReplace any custom string URL items with values in args\t\"\"\"\n\t\treplace = url.replace(url_args[4], url_args[3])\n\t\treturn replace\n\n\t# Build (key name, value) pair for the HTTP body\n\theaders = {\n\t\t'User-Agent': USERAGENT,\n\t\t'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n\t\t'Accept-Language': 'en-us,en;q=0.5',\n\t\t'Content-Type': 'application/x-www-form-"}]}
{"task_id": "62ece4982e6aefcf4aabbd63", "prompt": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn isinstance(val, str)\n\treturn False\n\n# A set of rules that apply to all string operations\n_string_rules = {\n\tis_none_string: is_not_none_string,\n\tis_empty_string: is_not_empty_string,\n\tis_none_array: None,\n\tis_not_empty_array: is_not_empty_array,\n\tis_empty_map: is_not_empty_map,\n\tis_empty_array: None,\n\tis_empty_object: is_"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\tif isinstance(val, str):\n\t\t\treturn val.lower() == \"none\"\n\t\treturn True\n\n\tdef is_regex_string(val: Any) -> bool:\n\t\t\"\"\"\n\t\tCheck if a string represents a regular expression.\t\"\"\"\n\t\tif isinstance(val, str):\n\t\t\treturn regexStr.search(str(val)) is not None\n\t\treturn True\n\n\tdef is_regex_list_tuple(val: Any) -> bool:\n\t\t\"\"\"\n\t\tCheck if a string represents a regular expression in tuple format.\t\"\"\"\n\t\tregexList = []\n\t\t"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn all(val is value for val in val)\n\n\tif not isinstance(val, string_types):\n\t\tval = str(val)\n\n\treturn isinstance(val, string_types)\n\n@register.filter(is_none)\ndef is_none(val: Any) -> BooleanExpr:\n\t\"\"\"\n\tDetermine if a string represents a Not None value.\n\t\"\"\"\n\treturn val is None or val == 'None'\n\n@register.filter(is_not_none)\ndef is_not_none(val: str) -> bool:\t\"\"\""}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn is_none_string_like(val) \\\n\t\t and not val\n\tif is_none_string(val) or val is None:\n\t\treturn is_none_string(val)\n\telif val.strip() == \"\":\n\t\treturn True\n\telse:\n\t\traise InvalidConfigException(\"%s: expected a string but had an empty value\" % (key))\n     \ndef parse_key_values(key_values: List[Any],\n\t\t\t\t\tsep: str,\n\t\t\t\t\tcomment_char: Optional[str] = None) -> Any:      \n\t\"\"\"\n\t"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn is_string(val)\n\ndef is_string(val: Any) -> bool:\n\t\"\"\"\n\tCheck if a string value is either a string (or a list of strings).\n\t\"\"\"\n\tval = is_string(val)\n\tif isinstance(val, string_type):\n\t\treturn val.lower() == 'false'\n\tif not val:\n\t\treturn False\n\treturn bool(val)\n\ndef comma_or(items: List[Any], separator: str = DEFAULT_COMMA) -> str:\n\t\"\"\"\n\tCheck that items"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn False\n\n\tdef is_whitespace_or_comment(\n\t\t\t\tself,\n\t\t\t\tvalue: str,\n\t\t\t\tlast=False,\n\t\t\t\tany_all=False):\n\t\ttypecian = None\n\t\tif not last:\n\t\t\ttypecian = self._get_type(value)\n\t\tfor line in value.splitlines(True):\n\t\t\tif line.lstrip().startswith('#'):\n\t\t\t\tcontinue\n\t\t\telif self.is_comment is not False and \\\n\t\t\t\t\tany_all or last and any(self.is_comment(line)) \\\n\t\t\t\t\tand last.rstrip()"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\tif not val:\n\t\t\treturn False\n\t\ttry:\n\t\t\tbool(v)\t\t# Test for booleans\n\t\t\treturn True\n\t\texcept TypeError:  # Not a boolean\n\t\t\treturn False\n\treturn False\n\ndef int_to_bool(val: any) -> bool:\n\t\"\"\"\n\tFind the value as a boolean-like string.\n\tAnything that can't be parsed as a bool or int must be coerced to such.\n\tReturns val unchanged.\n\n\t:raises: argparse.ArgumentTypeError\n\t:raises: argparse.ArgumentValueError\n\t\"\"\""}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn val is None or val == 'None'\n\n\t@staticmethod\n\tdef is_str(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a value of unknown type.\t\"\"\"\n\t\treturn val is None or issubclass(val, string_types)\n\n\t@staticmethod\n\tdef is_boolean(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a value of unknown type.\t\"\"\"\n\t\treturn val is None or issubclass(val, bool)\n\n\tdef is_float(val: float) -> bool:\t\"\"\"\n\tCheck"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn True\n\tif type(val) is not str:\n\t\treturn True\n\n\t# if we pass a string instead of a list\n\tif type(val) is not list:\n\t\treturn False\n\n\tfor regex in regexes:\n\t\tif re.match(regex, val):\n\t\t\treturn True\n\n\treturn False\n\n\nclass Template:\n\t\"\"\"\n\tTemplate class to do template tags.\n\t\"\"\"\n\n\tdef __init__(self, name, code):\n\t\tself.name = name\n\t\tself.code = code\n\t\tself.vars = {}\n\n\tdef"}, {"generated_text": "def is_none_string(val: any) -> bool:\t\"\"\"\n\tCheck if a string represents a None value.\t\"\"\"\n\t\treturn isinstance(val, str) and len(val) > 0\n\treturn True\n\ndef is_none(val: any) -> bool:\t\"\"\"\n\tCheck if a string is ''.\t\n\t\"\"\"\n\treturn not val\n\ndef is_boolean(val: any) -> bool:\t\"\"\"\n\tCheck if a string is boolean.\n\t\"\"\"\n\ttry:\n\t\tval.lower()\n\t\treturn True\n\texcept ValueError:\n\t\treturn False\n\n# ----------------------------------------------------------------------\n# Exceptions\n\nclass ParseException(Exception):\n\t\"\"\" An error occurred when parsing an expression"}]}
{"task_id": "62ece4982e6aefcf4aabbd65", "prompt": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\n\tlongopts = ['options', 'help', 'format=', 'help-all', 'help-short']\n\tfor o in longopts:\t\t\n\t\to = o.replace('-', '_')\n\t\to = o.replace('?', '_')\n\t\to = o.replace('#', '_')\n\t\to = o.replace(':', '_')\n\t\to = o.replace('-', '_')\n\t\to = o.replace('\"', '_')\n\t\to = o.replace(' ', '_')\n\t\to = o.replace('+', '_')\n\t\to = o.replace('!', '_')\n\t\to = o.replace('(', '_')"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\n\tparser.add_argument(\"--debug\", \"-D\", dest=\"debug\", help = \"\", metavar = \"TEXT\")\n\tparser.add_argument(\"--version\", \"-V\", dest=\"version\", help = \"\", metavar = \"TEXT\")\n\tparser.add_argument(\"--debug-imports\", \"-D\", dest=\"debugImports\", help = \"\", metavar = \"TEXT\")\n\n\treturn parser\n\n\ndef _parse_commandline(argv):\n\n\tif not argv or len(argv) == 1:\n\t\treturn None\n\t\n\tparser = _get_parser()\n\n\targs = parser.parse_args"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\ts ='s'\t# set the default\n\ts += 'p'\t# set the type of input: 'int' or 'float', default = 'float'\n\n\t# parse the cmdline arguments\n\tp = parser(s)\n\tp.add_argument(\"--x\", help='x file path')\n\tp.add_argument(\"--y\", help='y file path')\n\tp.add_argument(\"--s\", help='string to use to specify x file path', default = '')\n\n\t# parse the argument description\n\tg = p.parse_args()\n\t\n"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\treturn parser.parse_args()\n\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\treturn parser.parse_args([])\n\ndef main():\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"path\", type=str, help=\"Location to input.tcd file\")\n\tparser.add_argument(\"--format\", \"-ftb\", type=str, help=\"Target format\")\n\tparser.add_argument(\"--verbose\", \"-v\", action=\"count\", help=\"Increase verbosity\")\n\tparser.add_argument(\"--overwrite\", \"-b\", action=\"store_true\",\n\t\thelp=\"Overwrite existing output file\")\n\t#parser.add_argument"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\tif len(parser.option_store.option_list) == 0:\n\t\tparser.add_option(\"--%s\" % (parser.name,),\n\t\t\t\taction='store', type='string', dest='flag_names',\n\t\t\t\thelp=\"Sets the flag names to use for this flag.\");\n\t\tparser.add_option('--%s' % (parser.name,),\n\t\t\t\thelp=\"Specifies the name of the flag.\");\n\t\n\t\n\treturn parser\n\t\t\ndef get_args():\n\t\targ_parser = argparse.ArgumentParser(\n\t\t\t\t\tusage=\"Usage: %(prog)s"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\t\n\t##\n\t# If someone passes the argument directly to the parser_flags constructor, it could be the one we have already created on the command line, or it could be a bit awkward.\n\t# We always return True, because if the argument is a string rather than a list/tuple you need the flag names directly on the same line as\n\t# parser_flags.\n\targv = []\n\t\n\t# If they specified - the argument was parsed and now has arguments, return them\n\tfor arg in parser.rargs[1:]:\n\t\tif (arg[1:] in argparse."}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tparser.add_argument('name', metavar='command', nargs=1, type=(str, bool, int, float, list),\n\t\t\t\t\t\t\thelp='The subcommand name')\n\t\t\n\t@staticmethod\n\tdef add_arg(parser, arg, help):\n\t\t\"\"\"\n\t\tAdd a arg to the parser for this application.\n\n\t\tReturns the new argument. (A single value.)\n\t\t\"\"\"\n\t\tparser.add_argument(arg, type=arg.get('type'), nargs=arg.get('nargs', 0), help=help)\n\n\t@staticmethod\n\tdef get_parser(name"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\n\n\t# This is a flag for debugging purposes\n\t# We need this in order to support Python 2: one of:\n\t#  - no flags\n\t#  - flags that are only available in Python 3 \n\t#  - flags that are currently unused\n\t#  - flags that are specified so far\n\tflags = [\"debug\"]\n\t\n\t# Flags specified by default\n\topts = [[\"-b\", \"-b\", \"--builtins\"], [\"-r\", \"--builtins\"], [\"-O\",\"--builtins\"],\n\t\t\t\t[\n\t\t\t\t\"--std:out=\",\n\t\t\t\t[\"-d\", \"--debug\", \"--"}, {"generated_text": "def parser_flags(parser):\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\t\"\"\"\n\t\tglobal GLOBAL_HELPERS_FLAG\n\t\tFLAGS_FLAG_LIST = None\n\t\tfor flag in FLAGS:\n\t\t\t# Some flags are listed to match a flag in their own help string.\n\t\t\tif flag.startswith('-'):\n\t\t\t\tGLOBAL_HELPERS_FLAG += flag[1:]\n\t\t\telif not flag or flag[0] in GLOBAL_HELPERS_FLAG:\n\t\t\t\tFLAGS_FLAG_LIST = None\n\t\t\t\tLOGGER.warning('--%s requires a flag: %s', flag[0], flag)\n\t\t\telse:\n\t\t\t\tFLAGS_FLAG_LIST = flag\n\t\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd66", "prompt": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\t#if os.path.split(path_name)[1] in processed:\n\t\tif verbose: print \"Skipping %s\" % path_name\n\t\treturn False\n\telse:\n\t\t#if os.path.exists(path_name):\n\t\tif verbose: print \"Skipping %s\" % path_name\n\t\treturn True\n\n\n#def get_paths_for_paths(path_to_file, ignore_case=False):\n#\n#\t\"\"\"\n#\n#\tReturns a list of paths to the folders that need to be processed.\n#\n#\t"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n        # Get any parent directories\n        (parent, dirnames, filenames) = os.walk(path_name)\n\n        if not dirnames:\n                os.chdir(path_name)\n\n        # If parent directories are empty add to result list\n        if not filenames:\n                results = []\n        elif len(dirnamenames) == 0:\n                print \"Path %s has no entries and nothing in it: skipping.\" % path_name\n                return False\n\n        # Get any files in current directory\n        for f in filenames:\n                (results, path_name_included) = os.path.splitext(f)\n\n                if result:\n                       "}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\t#print(\"processed:\", path_name)\n\t\tif processed is None:\n\t\t\treturn False\n\t\telif verbose: print(\"processing\")\n\t\tif os.path.isdir(path_name):\n\t\t\treturn not os.path.splitext(os.path.basename(path_name))[0].lower() in processed\t\n\t\telse:\n\t\t\t#print(\"no dir\")\n\t\t\treturn True\t\n\t# Add the processed path_name to the set; otherwise continue\n\t# to walk up for the given file and directory\n\treturn not os.path.normpath(os.path."}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t                \n    result = False\n    if verbose:\n\t\tprint \"[DEBUG] Checking for a path: {}\".format(path_name)\n\tif path_name in processed:\n\t\tresult = True\n\t\n\treturn result\n\t\ndef file_passes(path_name):\n    \"\"\"Decide whether a file or directory has already been tested.\"\"\"\n    # Check if a file or directory has been processed\n    global was_processed\n    \n    if path_name in processed:\n            return False\n    \n    if not os.path.isdir(\"file/dir\"):\n        return False\n    else:\n        return True\n \ndef is_dirty(path"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n    #Check if not processed \n    #print(processed)\n    if (not processed.issubset(set(pathname.split(os.sep)[len(path_name.split(os.sep))-1]))):\n        return False\n\t\n\t#Check, if the path is outside the set, check for the parent files\n\tfound_path = False\n\t\n\tresult = False\n\tfor dir_name in pathname.split(os.sep)[len(path_name.split(os.sep))-1]:\n\t\tdir_path = os.path.join(path_name."}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n    subpath = path_name\n\tfor dir in processed:\n\t\tfor (dirpath, dirnames, filenames) in os.walk(dir):\n\t\t\tfor fname in filenames:\n\t\t\t\tif fname.endswith('.py'):\n\t\t\t\t\tpy_fpath = os.path.join(dirpath, fname)\n\t\t\t\t\tif ((not os.path.isfile(py_fpath)) or\n\t\t\t\t\t\t\t(os.path.isdir(py_fpath) and fname!= '__init__.py')):\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tif ((verbose and not os.path.isfile(py_fpath)) or\n\t\t\t\t\t\t\t(os.path.islink"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n    #if verbose:\n\t#\tprint \"processing %s\" % path_name\n    if path_name in processed:\n        #print path_name, \"already emitted\"\n\t\treturn False\n\n    if verbose:\n        print \"adding\",path_name\n    entries = os.listdir(path_name)\n    if verbose:\n        print \"entry is \",entries\n    \n    if 'file' in processed:\n        check_file_status(paths=set(entries))\n\t\n    if 'directory' in processed:\n        check_recursive(paths=set(entries),verbose=verbose)\n\n\ndef check_file"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n# Check if the folder exists\ndef is_folder(p):\n    \"\"\"Return whether the path belongs to a folder.\"\"\"\n    return os.path.isdir(p)\n\n# Check if a file exists\ndef is_file(p):\n    \"\"\"Return whether the path refers to a file.\"\"\"\n    return os.path.isfile(p)\n\n# Check if a directory exists\ndef is_directory(p):\n    \"\"\"Return whether the path falls within a directory.\"\"\"\n    return os.path.isdir(p) or is_file(p)\n\n# Check if a directory has a file\n"}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t\n    if (verbose):\t\t\t\t# DEBUG\n\t\t\n    result = True                                                    # TODO\n\n# TODO: Remove all the functions here\n# TODO: Create tests for these\ndef path_exists(path):\n    \"\"\"\n\t\"path in path_name\"\n\tReturn True if path exist in path_name or is not a directory\n\t\n\tArgs:\n\t\tpath: The path (which must have been in the path_name)\n\t\n\tReturns:\n\t\tTrue if path exists, else False.\n\t\"\"\"\n\tif path in path_name:\t\t\n\t\treturn True if path_name."}, {"generated_text": "def was_processed(processed, path_name, verbose):\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.\t\"\"\"\n\t \n    base = os.path.basename(path_name)\n    if verbose:\n        print \"Checking whether %s contains in base folder. If it is not found, generate it\"%base\n        print \"and retry creating it.\"\n    if not os.path.isdir(base):\n        print \"Creating the directory %s\"%base\n        os.mkdir(base)\n    \ndef walk_files(*walk_args):\n    \"\"\"\n    Recursively iterate through files in dir and return a list of paths.\n    Arguments are same as os.walk() function.\n    \"\"\"\n    return map(lambda w: w.rstrip('\\r"}]}
{"task_id": "62ece4982e6aefcf4aabbd67", "prompt": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t", "suggestions": [{"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tfaces=''\n\t\tfor vertex in vertices:\n\t\t\tfor x in range(3):\t\t# the 3-line polyline\n\t\t\t\tif vertex[x]==vertex[x+1]:\t# vertex to the edge\n\t\t\t\t\t#print(\"VCC_point \",vertex[x+1],\" VCC_line \",vertex[x+2],\" v \",vertex[x+3],\" VCC_line \",vertex[x+4],\"v \",vertex[x+5],\" v \",vertex[x+6])\t# this are for a single triangle, not a \"dual b\"\n\t\t\t\telse:"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tvertices = np.array(vertices)\n\n\t\treturn vertices.reshape((-1,3))\n\n\ndef point2vertices(pt):\n\t\"\"\" Given a point, returns vertex indices that are in the polyline as it's in `xy_list[xy]`, such that\n\tpoint p is the vertex, given a point and a vector.\n\npt can be of any shape or a list that contains 3 or 4 points\n\nreturns:\n\t1D array of vertex coordinates\n\t\"\"\"\n\tpoints = []\n\tfor p in pt:\n\t\tp = p*np.array("}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\n\n\tif len(vertices)==0:\t\t\t# if no vertices, return empty string\n\t\treturn \"0\"\n\telif len(vertices)==1:\t\t\t# 1 point\n\t\ta=[]\n\t\tb=[]\n\t\tc=[]\n\t\td=[]\n\t\tfor i in xrange(len(vertices[0])):\n\t\t\ta.append([])\n\t\t\tb.append([])\n\t\t\tc.append([])\n\t\t\td.append([])\n\t\tvertex1 = vertices[0]\n\t\tvertex2 = vertices[1]\n\t\tnpt=len(vertices)\n\t\ta=[]\n\t\tb=[]\n\t"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\n\n\ndef polygon3tuple(vertices):\t\t\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 4 points on both sides of the vertex::\n\npolygon with vertices ABCDBA\nWill return\nAAB, ABCBA, BCDBA, CDABA -> returns 3tuples\n#D    D\n\"\"\"\n\"\"\"\n    REST API Documentation for the NRS TFRS Credit Trading Application\n\n    The Transportation Fuels Reporting System is being designed to streamline compliance reporting for transportation fuel suppliers in accordance with the Renewable"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\n\n\ndef edge_iterator(edge_vertex_1, edge_vertex_2):\n\tif edge_ vertex_2 == edge_vertex_1 or edge_vertex_2 == edge_vertex_1:\n\t        return 'end'\n\n\tif edge_vertex_2 == edge_vertex_2 or edge_vertex_1 == edge_vertex_2 or edge_vertex_1 == edge_vertex_1 + 1:\n\t        return'start'\n\n\tif edge_vertex_1 == edge_vertex_2:\n\t        return 'end'\n\n\treturn\n\n\ndef parse_"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tvertices = list(vertices)\n\t\tvertices.reverse()\n\t\tvertices.append(vertices[0])\n\t\tvertices.append(vertices[1])\n\t\treturn (vertices[0] * vertices[1] - vertices[2] * vertices[2])\n\n\nclass Spline3tuple(object):\n\t\"\"\"\n\tclass Spline3tuple is the class which extends the Spline class from pygmin.\n\n\tClass attributes:\n\t#vertices, a list of the vertexes (list of edges)\n\t#indices, a list of the indices of the vertices (list of indices"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tp = [vertices[0]]\n\t\tn = [vertices[1]]\n\t\t#DAB\n\tif len(p) >= 2: p[0].append(b2A)\n\tif len(n) == 2: n[0].append(b2A)\n\tif len(p) >= 3:\n\t\tp[0].append(b2A)\n\tfor p in p:\n\t\tpts = []\n\t\tfor v in p:\n\t\t\tp.append(v)\n\tfor v in n:\n\t\tpts.append(v[0])"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\n       \n    # This code is borrowed from Peter Norvig's algorithm\n    result = [(1, 0, 0), (0, 1, 0), (0, 0, 1), (1, 0, 1), (1, 1, 1), (1, 1, 2), (1, 2, 0), (1, 2, 1), (2, 0, 0), (2, 2, 1), (2, 1, 1)]\n\n    # Convert (a, b) so we can modify the vertices list and list elements\n    return [(v[0] * v[4], v"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tP = {}\n\t\tDAB, ABCD, BCD = {}, {}, {}\n\t\tfor vertex in vertices:\n\t\t\tP[vertex] = 0\n\t\tfor v in P:\n\t\t\tDAB[v] = {v: k for k in range(1, v + 1)}\n\t\tfor v in DAB:\n\t\t\tfor k in range(3, v + 1):\n\t\t\t\t#print k, v\n\t\t\t\tif p.get(B[v], 1) == 2:\n\t\t\t\t\tbreak\n\t\t\t\telif p.get(B[v], 6) == 3:\n\t\t\t\t\t"}, {"generated_text": "def vertex3tuple(vertices):\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices\t\"\"\"\n\t\tvertices_count = 0\n\t\tvertices_on_side = 4\n\t\tfor vertex in vertices:\n\t\t\tside_vec = vertex3point(vertical_vector(vertex[0],vertex[1]))\n\t\t\tp1 = side_vec.co.copy()\n\t\t\tp2 = side_vec.co.copy()\n\t\t\tp1.x = p2.x -side_vec.co.x\n\t\t\tp1.y = p2.y -side_vec.co.y\n\t\t\tp1.z = p2.z -side_vec.co.z\n\t\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd68", "prompt": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tif len(alphabet) <= number:\t# no padding necessary\n\t\t\treturn alphabet[0]\n\telse:\n\t\tfor x in alphabet:\n\t\t\tif x!= number:\n\t\t\t\treturn x\n\ndef int_to_float(number: int, num_digits: int) -> Union[str, float]:\n\tnum_of_digits, remainder = divmod(number, len(str(int(number))), 1)\n\treturn str(num_digits) + '.' + str(remainder)\n\ndef float_to_string(number: float, width: str) -> Union"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tif is_prime_number(number) == False:\n\t\t\tnumber = int_to_prime(number)\n\t\t\tnumber = number % b'\\0'\n\n\t\tnum_digits = len(alphabet)\n\t\tnum_chars = len(alphabet)\n\n\t\tif number < 0:\n\t\t\traise ValueError(\"Cannot convert number\")\n\n\t\tmax = len(alphabet) - 1\n\n\t\talphabet = alphabet[:-1]\n\t\talphabet.reverse()\n\t\talphabet_length = len(alphabet)\n\t\tnum_digits = 16\n\n\t\tif padding:\n\t\t\treturn chr(padding).join([chr(_"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tresult = ''\n\n\t\tfor i in range(0, len(alphabet)):\n\t\t\tresult += alphabet[i % len(alphabet)]\n\n\t\tif len(result) > (number + 1):\n\t\t\tresult = result[:(max(number, total_digits))]\n\n\t\treturn result\n#!/usr/bin/python\n# vim:fileencoding=UTF-8:ts=4:sw=4:sta:et:sts=4:ai\nfrom __future__ import (unicode_literals, division, absolute_import,\n                        print_function)\n\n__license__   = 'GPL"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\n\t\n\t# Make sure number is not a multiple of 2.\n\tif not isinstance(number, (int, long)):\n\t\traise TypeError('number must be an integer')\n\tif not 0 < len(number):\n\t\traise ValueError('number must be greater than 0')\n\t\n\tnum_digits = len(alphabet)\n\t\n\tif number < 2 or number >= num_digits:\n\t\traise ValueError('number must be no more than 2 digits')\n\t\t\t\n\tif number == 0:\n\t\treturn alphabet\n\t\n\t# Make sure number is larger than 2.\n\tif number > max_number:"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tif number == 0:\n\t\t\tresult = alphabet[0]\n\t\telse:\n\t\t\tlast_n_digits = int(\"9\".join([str(i) for i in number if i <= 9]))\n\t\t\tresult = alphabet[0] + (alphabet[last_n_digits] if padding is None else padding) + result\n\tif isinstance(number, float):\n\t\treturn str(number)\n\treturn str(number) + alphabet\n\ndef string_to_list(string: str) -> list:\n\t\"\"\"Given some string as a list, return a list\n\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tn: int = 0\n\t\tprev: int = 1\n\t\twhile n == 0:\n\t\t\tn = int(n) + 1\n\t\t# For each value in the list, the number of '0's is to\n\t\t# be the length of 'prev'.\n\t\tresult = ''\n\t\tchar_index = 0\n\t\twhile char_index < len(alphabet) else alphabet_len:\n\t\t\t# The next character may be smaller than the current number in the\n\t\t\t# last position of 'n'\n\t\t\tc = alphabet[char_index]\n\t\t\tprev, n = divmod("}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tremainder_by = digits_reversed(number)\n\t\tremainder = int(''.join(remainder_by))\n\t\talphabet = alphabet if alphabet else [remainder]\n\n\tif not alphabet:  # pragma: no cover\n\t\treturn ''\n\tremainder = str(remainder_by).rjust(padding, '0')\n\treturn '0123456789abcdef' + ''.join(alphabet) + remainder\n\"\"\"\nCopyright 2017 Deepgram\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tl = []\n\t\tfor i in range(0, len(alphabet))[::-1]:\n\t\t\tl.append(alphabet[int(number/len(alphabet))])\n\t\treturn ''.join(l)\n\"\"\"\nThis module serves as an example from the `pylons.config` script.\n\n:copyright: 2010, James Pic <jpaic@gmail.com>\n:author: marcel <marcel@hol.gr>\n:organization: Mayur Parker <mg.pasteur.cegard@gmail.com>\n:license: GPLv"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tif alphabet == [0, 1]:\n\t\t\talphabet = [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n\t\telif len(alphabet) == 2:\n\t\t\talphabet = alphabet * 2\n\n\t\t# convert the string to a number with the given padding\n\t\tif number >= 0 and number < 6:\n\t\t\tnumber = number - 6\n\t\telse:\n\t\t\tnumber = int(number)\n\n\t\tif alphabet == [0, 1]:\n\t\t\talphabet = \"0123456789abcdefABCDEF\"\n\t\telif len(alphabet) == 4:\n\t\t"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\t\"\"\"\n\t\tres = ''\n\t\tif number == 0:\n\t\t\tres += alphabet[0]\n\t\t\tif len(alphabet) > 1:\n\t\t\t\tres += alphabet[1]\n\t\t\treturn ''.join(alphabet) + res\n\t\tbits = len(number) - 1\n\t\tbit_length = 0\n\t\twhile number[bit_length]:\n\t\t\tbit_length -= 1\n\t\t\tif number[bit_length] & (1 << bit_length - bit_length):\n\t\t\t\tbit_length += 1\n\t\t\tnumber[bit_length] >>= 1\n\t\t\n\t\tif res:\n\t\t\tres +="}]}
{"task_id": "62ece4982e6aefcf4aabbd69", "prompt": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tassert register_value is not None, \\\n\t\t\t\t\"Register value is not specified for register number\"\n\t\tassert 0 <= register_number <= 2, \\\n\t\t\t\t\"Register number {0} out of range (expected 2 or 3)\" \\\n\t\t\t\t.format(register_number)\n\t\tglobal _registered_register_tbl_list\n\n\t\tassert _registered_register_tbl_list is not None,\\\n\t\t\t\t\"Register table list of length == number of registered table entries\"\n\t\ttmp_list_len = len(_registered_register_tbl_list)\n\n\t\tnew_register_number ="}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tif len(flow_params)!= register_number+1 or \\\n\t\t\t   not (register_value in flow_params):\t\t\t\t\t\t# If key and value already exists\n\t\t\traise KeyError()\t\t\t\t\t# Then key and value are not allowed\n\n\tif register_value and register_number+1==register_value:\n\t\tremove_index = list(set(flow_params[register_number+1:register_number+2+1]))\t# Add the register value to a set\n\t\tif len(remove_index)!= 0:\n\t\t\tflow_params.pop(register_"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\t# Remove duplicate register number\n\t\tif register_number in flow_params:\n\t\t\tflow_params.pop(register_number)\n\t\t# Append new register number and new value into the flow\n\t\ttry:\n\t\t\tnew_params = flow_params.copy()\n\t\t\tnew_params[register_number] = int(register_value)\n\t\t\tflow_params[register_number] = new_params\n\t\texcept ValueError:\n\t\t\t# In case a register number does not exist in dictionary, raise an error\n\t\t\traise KeyError(\"The register number was not found in parameters \"\n\t\t\t\t\t+"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\treturn register_value.format(int(register_number), register_number, register_params)\n\n\tdef _define_register_entry(self, value, register_number):\t\t\n\t\t\"\"\"\n\tReturn key which should be used to store register at register_number\n\n'registered_entries' key in dictionary will be used to store actual\nregisters\n\n:return: A tuple (register name, register number) where register number\nwhether the register has been added to the flow\n\n:rtype: tuple (str, str)\n\"\"\"\n\t\t# get register number\n\t\tpattern"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\traise NotImplemented(\"Unimplemented method\")\n\n\tflow_params.update({'register_number':register_number,'register_value':register_value})\n\n\tdef _replace_rate(flow_params, register_number, rate_value):\n\t\t\"\"\"\n\t\tReplace value from flows to the given register number\n\t\tand add new rate to given register number\n\t\t\"\"\"\n\t\traise NotImplemented(\"Unimplemented method\")\n\n\tflow_params.update({'rate_register_number':register_number, 'rate_register_value':rate_value})\n\ndef _replace_value(flow_"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\t# This function is just like for data.key = value, but that data is saved in the flow\n        if register_number in flow_params:\n\t\t\t# Only looping for the values stored on the given register number\n\n\t\t\tresult = flow_params[register_number]\n\n\t\t\tif result[register_number] is None:\n\t\t\t\t# The value is not present in the flow yet. That means that the register number was not found\n\t\t\t\treturn None\n\n\t\t\t# Reading data from the store.\n\t\t\tif result[register_number] == register_value:\n\t\t\t\t# The value is present"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tif register_number!= '0' and register_value!= '0':\n\t\t\tflow_params['register_number'] = register_number\n\t\tif register_number < register_value:\n\t\t\traise AssertionError('Wrong register number given, expecting'+\n\t\t\t\t    register_number +'to be an integer')\n\t\tflow_params['register_value'] = register_value\n\n\tdef _clear_flows(self):\n\t\t\"\"\"\n\t\tClear any existing flows created by this module that are not in the\n\t\tdatabase yet. Returns list of modified values\n\t\t\"\"\"\n\t\t# make sure that"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\n\n        self.remove_key(register_number)\n        self.add_entry(flow_params['flow_id'],\n                   flow_params.get('value'),\n                   flow_params.get('register_number'),\n                   flow_params.get('register_value'))\n\n\n    def _delete_entry(self, flow_id, flow_params):\n        \"\"\"Delete flow entry\n\n        :param flow_id: Flow id\n        :param flow_params: Dictionary containing defined flows\n\n:return: None\"\"\"\n        if flow_id in self.registered_flows.keys():\n            # Remove the register number key from"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\tregister_params = flow_params[register_number]\n\t\tregister_number_new = register_params[register_number] \n\t\t# TODO: fix this method properly\n\t\t# This is probably broken in case of multiple registers\n\t\t# in case of multiple registerion.\n\t\t# It could be made quicker (e.g. more than one), in the future\n\t\t# it may mean that the value will not be stored twice,\n\t\t# but it is not always the case in the loop\n\t\tif register_number > 1:\n\t\t\t# TODO: better exception handling"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\t\"\"\"\n\tReplace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\t\"\"\"\n\t\t# We check that register_number is within the flow_params and flow_params.\n\t\t# If so, we change the value and register_number accordingly.\n\t\tfor i in range(len(flow_params)):\n\t\t\tif (flow_params[i] > register_number) and (flow_params[i] < register_number):\t# Remove register number if the value is valid\n\t\t\t\tflow_params[i] = register_number\n\t\t\telif (flow_params[i] < register_number):\t# Add register number if the value is valid\n\t\t\t\tflow_params"}]}
{"task_id": "62ece4982e6aefcf4aabbd6a", "prompt": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t", "suggestions": [{"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t if type(value) is float:\n\t\t\tstring = '%f' % value\n\t\telif type(value) is int:\n\t\t\tstring = '%d' % value\n\t\telse:\n\t\t\tstring = '%s' % escape_quote(str(value))\n\t\targ = regex(\"\\$\\s*%s%s\" % (string, arg))\n\t\treturn arg\n\tdef get_default_values(self):\n\t\treturn \"\"\n\tdef set_default_values(self, value):\n\t\treturn \"\"\n\tdef get_use_regex(self):\n\t\treturn self.get"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t try:\n\t\t\treturn str.replace('.', '_', str(value));\n\t\texcept:\n\t\t\treturn str(value);\n\telse:\n\t\t# If it isn't a string, we must look up \n\t\t# for all of the separators.\n\t\tsep=string.sep\n\t\twhile arg in sep:\n\t\t\tif string.find(arg,\"(\")>=0:\n\t\t\t\targ=string.replace(arg,\"(\", arg);\n\t\t\telif arg.find(\",\")>=0:\n\t\t\t\targ=string.replace(arg, \")\", arg.replace(\",\", r\"\\[\");\n\t\t\telif arg"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\tif (arg[0:len(arg) - len(arg.lstrip()))] not in arg:\n\t\t\tvalue = replace_dots(value, arg)\n\tif(arg[0:len(arg) - len(arg.rstrip()))] not in arg:\n\t\tvalue = replace_dots(value, arg)\n\treturn value\n\n\n###\n###  Globals\n###\n\ndef setup_file_handling(args, current_directory):\n\t\"\"\"\n\tCreates argument list based on the current_directory if it is not empty\n\t\"\"\"\n\t# Create argument list"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\tfor i in range(len(value)):\n\t\t\tif arg.count('.') == 0:\n\t\t\t\tif not value[i].isupper():\n\t\t\t\t\tvalue[i] += '.'\n\t\tvalue = value[:-1]\n\treturn value\n\n_config_data = None\n\ndef configFromFile(option, default=None, parentDir=None):\n\tglobal _config_data\n\tif default is None:\n\t\tdefault = {}\n\tif parentDir is None:\n\t\tparentDir = option.get('parentDir', os.path.expanduser('~'))\n\t_config_data"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t if len(value) > 1:\n\t\t\treturn value + ',' + arg\n\t\treturn arg \n\ndef print_dot(value, arg):\n\t\"\"\"\n\tPretty prints a dot with appropriate formatting\n\tArgs:\n\t\tvalue: \tString, input to parse as list format\n\t\"\"\"\n\tif len(arg) > 0:\n\t\treturn str(arg)\n\treturn value\n\ndef pretty_print(key, value):\n\t\"\"\"\n\tPrints a dot with appropriate formatting\n\tArgs:\n\t\tkey: \t\tString, input to parse as list (must be a"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t # http://stackoverflow.com/questions/13184568/how-do-i-replace-dots-in-python\n\tfor i in arg:\n\t\tif '://' not in i and '/' not in i:\n\t\t\traise argparse.ArgumentTypeError('Expected a ~ in the form of \\'~\\' but got :/')\n\t\n\treturn value\n\n\ndef make_dict(l):\n\t\"\"\"\n\tGiven a dict in the form {'a':1, 'b':'2'}, find the keys occurring in lexicographical order\n\tand return those values\n\n\t\"\"\" \t\n\td"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\treturn re.sub(r'(?:[a-z])', r'\\1_\\2', value).replace('.', arg)\n\n\t# remove empty items from a list that contain only dot\n\treturn re.sub(r'^\\s+(\\.\\w+.)*$', '', value)\n\n# get integer value from a list or dict\ndef get_int_value(arg):\n\t\"\"\" Convert single integer value to int, if possible\t\"\"\"\n\n\t# check if arg is a list or a dict\n\tif isinstance(arg, list) or isinstance(arg, dict):\n\t\tfor val in arg:\n\t\t"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\tfor i,c in enumerate(arg):\n\t\t\tif c in string.ascii_letters.encode('ascii'):\n\t\t\t\tvalue = value.replace(c, '_')\n\t\n\treturn value\n\t\ndef remove_nonprinting_characters(value):\n\t\"\"\"\n\treplaces all nonprintable characters from the given value \t\n\t\"\"\"\n\tfor c in value:\n\t\tc = unicodedata.normalize(\"NFD\", unicodedata.normalize(\"NFC\", c))\n\t\tc = ''.join([i for i in c if i not in unicode(c)])\n\t\tif c.lower() not in un"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\tif not value: return \"\"\n\t\telse: return value.replace('.', '_').replace('+', '_')\n\n\t#Replace all special characters in a value using''\n\tfor char, value in list(string.ascii_lowercase).items():\n\t\tvalue = replace_dots(value, char)\n\n\treturn value\n\ndef sanitize_string(str_in):\n\t\"\"\"\n\tCleans string in the form of'something_in_string' that follows '&gt;some string\n\t\"\"\"\n\tstr_in = str_in.replace('gt;', 'a')\n\tstr"}, {"generated_text": "def replace_dots(value, arg):\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\t\"\"\"\n\t\tif len(arg)!= 1:\n\t\t\treturn 'Error - bad string number.'\n\t\tvalue = str(value)\n\t\treturn re.sub(r'^[^a-z0-9]', '_', value)\n\tdef parse_args(self):\n\t\t\"\"\"\n\t\tParses all usernames and user emails from command line args\n\n\t\t\n\t\tThe command line arguments will be the command\n\t\t'arg_list' is always a list of arguments \n\t\t\"\"\"\n\t\ttry:\n\t\t\toptions, args = getopt.getopt(self.args,'hv:s:v:',['help','no-"}]}
{"task_id": "62ece4982e6aefcf4aabbd6b", "prompt": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t", "suggestions": [{"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tif isinstance(cls, type):\n\t\t\tfor b in cls.subclasses():\n\t\t\t\tyield b\n\t\telse:\n\t\t\tyield cls\n\n\n\n\nclass TacticException(Exception):\n\tpass\n\nclass TacticFunctionException(Exception):\n\tpass\n\nclass TacticServerException(TacticException):\n\t\"\"\"Thrown when a tactic server cannot find a server class\"\"\"\n\n\tdef __init__(self, exc_info):\n\t\tException.__init__(self)\n\t\tself.set_message(str(exc_info))\n\n\tdef set_message(self, msg):\n\t"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tclsname = type(cls).__name__ if hasattr(cls, '__bases__') else cls.__name__\n\t\treturn [cls for mro_entry in inspect.getmro(cls) for mro_entry in mro_entry.__bases__\n\t\t\t\tif inspect.isclass(getattr(mro_entry, '_subclass_cache', []))]\n\n\tsubclasses_cache = {}\n\tsubclasses_cache.update(subclasses)\n\tsubclasses_cache.update(subclasses_cache.values())\n\treturn subclasses\n\n\nfrom cifsdk.common.input_output import InputOutput\n\n\nclass TestInputOutput("}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tsubclasses = []\n\t\tfor name in cls.__subclasses__():\n\t\t\tsubclasses.append(name)\n\t\t\tsubclasses = subclasses + superclasses(cls)\n\t\tsubclasses = subclasses + (object, )\n\n\t\treturn subclasses\n\n\tdef has_attribute(self, obj, attr):\n\t\t\"\"\"Checks if an attribute exists in object or list of objects\"\"\"\n\t\tfor k in dir(obj):\n\t\t\tif not method_allowed( k ) or not k.startswith('_') \\\n\t\t\tand callable( getattr( obj, k ) ):\n\t\t\t\treturn False\n\t\treturn True\n\nclass OC_Base"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tsubclasses = inspect.getmro(cls)\n\t\tif subclasses == []:\n\t\t\treturn\n\t\tbase = subclasses.pop(0) or object\n\t\tfor scls in subclasses:\n\t\t\tif hasattr(scrp, scls):\n\t\t\t\tbase = scrp.get_class(scls)\n\t\t\t\tyield base\n\t\tbases = inspect.getmro(cls)\n\t\tif bases == []:\n\t\t\t#print('Can not use subclasses for', cls.__name__)\n\t\t\tsubclasses = base.__subclasses__()\n\t\t\twhile subclasses!= []:\n\t\t\t\tsubclasses.extend(subclasses)\n\t\t\tif len(subclasses"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tsubclasses = []\n\t\t\n\t\tfor _name, obj in inspect.getmembers(type(cls), inspect.isroutine):\t\t# python 2.5 does not\n\t\t\tsubclasses.append(obj)\t\n\treturn subclasses\n\nclass Task(object):\n\t\"\"\"\n\tRepresents a single task that will be executed\n\t\"\"\"\n\t\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tCreate a task\n\n\t\t:param task: task name or string \n\t\t:type task: string or list\n\t\t:return: \n\t\t\"\"\"\n\t\tassert type(task) == str or _is_str("}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tsubclasses = []\n\n\t\t# walk children hierarchy\n\t\tfor chld in cls.__subclasses__():\n\t\t\tsubclasses.extend(chld)\n\n\t\tfor cls in subclasses:\n\t\t\tyield cls\n\n\t\tfor cls in subclasses:\n\t\t\tfor sub in cls.__subclasses__():\n\t\t\t\tsubclasses.extend(sub)\n\n\t\tfor chld in cls.__subclasses__():\n\t\t\tsubclasses.extend(chld)\n\t\treturn subclasses\n\n# coding=utf-8\n# --------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tfor subcls in inspect.getmro(cls):\n\t\t\tyield subcls\n\n\tsubclasses = subclasses(cls)\n\t\n\n\tallSubclasses=dict()\n\n\tfor i, v in enumerate(subclasses):\n\t\tallSubclasses[i]=[]\n\t\tfor cls in inspect.getmro(v):\n\t\t\tif cls==object:\n\t\t\t\tcontinue\n\t\t\tif cls.__name__ not in allSubclasses:\n\t\t\t\tallSubclasses[cls.__name__].append(cls)\n\t\t\tallSubclasses[cls.__name__]+=subclasses[i]\n\n\tallSubclasses.values()"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tsubclasses = [cls]\n\t\tfor subclass_ in bases:\t# Recurse into class\n\t\t\tsubclasses += subclass_._subclasses(cls)\n\t\treturn subclasses\n\nclass Simple:\n\t\"\"\"\n\tSimple value class\n\t\"\"\"\t\n\tdef __init__(self, value):\n\t\tself.value = value\n\n\tdef __str__(self):\n\t\tres = '{}({})'.format(self.value, self.name)\n\t\treturn res\n\nclass Complex(Simple):\n\t\"\"\"\n\tComplex value class\n\t\"\"\"\t\n\tdef __init__(self, value):\n\t"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tret = set()\n\t\tfor name in dir(cls):\n\t\t\t#print '%s: %s' % (name, str(type(getattr(cls, name))))\n\t\t\tif isabstract(getattr(cls, name, None)):\n\t\t\t\tret.add(cls.__name__)\n\t\treturn ret\n\t#------------------------------------------------------------\n\t# Access\n\t#------------------------------------------------------------\n\tdef __repr__(self):\n\t\tif hasattr(self, \"typename\"):\n\t\t\t#self.typename = str(self.typename)\n\t\t\t#return \"<%s '%s' at 0x%x>\" % (self.__"}, {"generated_text": "def subclasses(cls):\t\"\"\"\n\tReturn all subclasses of a class, recursively\t\"\"\"\n\t\tsubclasses = filter(lambda obj: (issubclass(obj, Object), obj.__subclasses__(),), cls.__subclasses__())\n\t\tsubclasses.sort(key=operator.itemgetter(1))\n\t\treturn subclasses\n\nclass ClassFactory(object):\n\t\"\"\"\n\tA callable object which creates a class instance.\n\t\"\"\"\n\tdef __init__(self, class_or_name, *args, **kwargs):\n\t\tif issubclass(class_or_name, basestring):\n\t\t\tclassname = class_or_name\n\t\telse:\n\t\t\tclassname = unicode(class_or_name)\n\n\t\tif"}]}
{"task_id": "62ece4982e6aefcf4aabbd6d", "prompt": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\treturn int(\"\".join(\"{:0x}\".format(c) for c in string))\n\ndef base_repr(number: float = 0.0) -> str:\t\t\n\treturn \"\".join(\"{:0.1f}\".format(c * number) for c in string)\t\n\ndef decode_number(number: int) -> int:\n\t\"\"\"\n\tConvert a given number to an integer, encoding that number to a string\n\tand return either a decimal or the base 10.\n\nThe given number can either be 0 (zero), a positive\nnumber, a decimal or the"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\talphabet = alphabet.upper()\n\t\tnumstr = ''.join([c for c in string if c.isdigit()])\n\t\tassert len(numstr) == numstr[0].isdigit()\n\t\treturn int(numstr)\n\t# END PRECISION\n\ndef clean_str(a_str: str) -> str:\n\treturn [c.strip() for c in a_str.strip().split()]\n# END PRECISION\n\n\ndef clean_string(a_str: str, alphabet: List[str], max_digit: int) -> str:\n\tprint"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\n\n\tdef num_to_string(i: int) -> str:\n\t\t\"\"\"\n\t\tconvert a number to a string, using the character alphabet.\n\t\t\n\t\tArguments:\n\t\t\ti: the integer to be converted.\n\t\t\"\"\"\n\n\t\tstr_num = ''\n\t\tfor x in alphabet:\n\t\t\tif x == '0':\n\t\t\t\tstr_num += '0'\n\t\t\telif x == '':\n\t\t\t\tstr_num += x\n\t\t\telse:\n\t\t\t\tstr_num += x\n\n\t\treturn str_num\n\t\t\n\t# \tconvert to string (for comparison) the strings\n\tn"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\tif alphabet[0] == '0':\n\t\t\treturn 0\n\t\telif alphabet[0] == '1':\n\t\t\treturn 1\n\t\t# Check if the string is even or odd\n        if alphabet[0] == '2':\n\t\t\treturn 2\n\t\t# Check if the string is odd or even\n\n\t\tfor digit in reversed(alphabet):\n\t\t\tif string.endswith(digit):\n\t\t\t\treturn string.index(digit)\n\n\telse:\n\t\t# Check if the string is even or odd\n\n\t\tfor digit in list(reversed(alphabet)):\n\t\t\tif string.startswith(digit):\n\t\t\t"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\ta, b, c = bytearray(hex_to_int(string), 2), bytearray(hex_to_int(string))\n\t\tassert len(alphabet) == 1, \\\n\t\t         \"Incorrect length of alphabet. Expected one of '\" + \\\n\t\t         str(len(alphabet)) + \"'\"\n\t\tassert len(b) == 1, \\\n\t\t         \"Incorrect length of byte. Expected one of '\" + str(len(b)) + \"'\"\n\t\tassert len(c) == 1, \\\n\t\t         \"Incorrect length of byte. Expected one of '\" + str(len(c)) + \"'\""}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\treturn sum(alphabet_dict[c] * int(a) for c in string)\n\nalphabet_dict = {\n\t\"ACCURIOUS\": 2, \"ACCENT\": 2, \"ADD\": 2,\n\t\"ADVANTAGE\": 2, \"APPROBE\": 2, \"AWAIT\": 2,\n\t\"ALL\": 3, \"ALLELE\": 3, \"AMBER\": 3, \"AMBI\": 3,\n\t\"AMBIGU\": 3, \"AWAY\": 3, \"AZURE\": 3, \"BECAY\": 3,\n\t\"B"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\ta = 0\n\t\tcount = 1\n\t\twhile a < 26:\n\t\t\ta += alphabet.find(string[a])\n\t\t\tcount += 1\n\t\treturn count\n\n\tdef get_int_type_for_length(sequence: str, num: int) -> str:\n\t\t# type : tuple ['A', 'B',]\n\t\treturn [(int(num / base) * base % base) for base in range(0, num / base)]\n\ndef create_str_list(lst: List[str], length: int) -> List[str]:\n\treturn ["}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\tnum_string: string = str(int(alphabet.pop()[:-1], 0))\n\t\tif alphabet:\n\t\t\tnum_string += alphabet.pop()\n\t\treturn int(num_string)\n\ndef int_to_string(string: str) -> str:\t\"\"\"\n\tConvert a integer to a string, using the given alphabet.\n\nReturns the string in plain English, with a'' character appended.\n\t\tReturn a string in Unicode, with a'' character appended.\n\t\tReturn '-'.\n\t\"\"\"\n\tif alphabet:\n\t\tnum_string = str("}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\treturn \"\".join([i if alphabet[i] == \"1\" else \"\" for i in string])\n\ndef string_to_number(string: str, alphabet: List[str], width: int = 12) -> str:\n        \"\"\"Convert a string to a number, using the given alphabet.\"\"\"\n        s = \"\"\n        for letter in alphabet:\n            s += letter\n        while len(s) < width:\n                s += \"0\"\n\n    return s\"\"\"\nTest Cases for the Mixed Modulestore, with DDT for the various stores (Split, Draft, XML)\n\"\"\"\n# pylint:"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.\t\"\"\"\n\t\n\t\n\tassert alphabet!= [0]\n\tout = 0\t\t\n\tfor i in range(len(string) - 1, 0, -1):\n\t\tletter = string[i]\n\t\tout += int(alphabet.index(letter) * 10**i)\n\t\n\treturn out\n\t\n\t\t\t\n\n\ndef decode_binary(code: str) -> list:\n\t\"\"\"Get the binary coded as a list of integers from the given encoding.\n\nThe input is assumed to have the least significant digit first.\nThe output is a list of integers.\n\nThe first element of code is for"}]}
{"task_id": "62ece4982e6aefcf4aabbd6f", "prompt": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef main():\n\ttry:\n\t\twith open( 'description.txt', 'r' ) as descfile:\n\t\t\tdescriptions=descfile.readlines()\n\t\tdownload_description=[]\n\t\tfor line in descriptions:\n\t\t\tlineSplit=line.split()\n\t\t\tprint lineSplit\n\t\t\tfilename=lineSplit[0]\n\t\t\tdownload_description.append(filename)\n\t\tprint download_description\n\t\tdownload_md5={}\n\t\tfor download in download_description:\n\t\t\tf=open( download, 'r' )\n\t\t\tprint f.read( 2 )\n\t\t\ttar_"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef main():\n\tprint(Fore.GREEN + \"********************************************************\")\n\tprint(Fore.WHITE + \"- Downloading \", url, \"\\n\")\n\n\twith open(destination_path, 'wb') as ftp:\n\t\tfor file_ in ftp.listdir(): #Iterate over files\n\t\t\tfile_path = os.path.join(filePath, file_)     #Filename of file path\n\t\t\tif(os.path.isfile(file_path) == False):\n\t\t\t\tftp.urlretrieve(url + file_, \n\t\t\t\t\tos.path.join(filePath + '.tar.gz"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef install_archive(archive_path, destination_path):\n\t\"\"\"\n\tGiven a local file path and a destination path, execute the download function\n\tto install that local file.\n\n\tArgs:\n\t\tarchive_path: the local path of the archive.\n\t\tdestination_path: the path on disk where to extract archive\n\tReturn:\n\t\tdownload function to install that local file.\n\t\"\"\"\n\n\tdownload(archive_path, destination_path)\n\t\n\"\"\"\nDownload archives.\n\"\"\"\ndef download(archive_path, destination_path):\n\t\"\"\"\n\tGiven"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\n\n\tpath= os.path.dirname( destination_path )\n\n\tif not os.path.exists(path):\n\t\tif os.path.exists(destination_path + '.tar.gz'):\n\t\t\tpass\n\t\telse:\n\t\t\tos.makedirs( path )\n\t\n\n\t\tpackage = os.path.join( path, 'core' )\n\t\tif os.path.exists(package):\n\t\t\t# remove the 'core' dir, if present\n\t\t\tshutil.rmtree( package )\n\n\t\t\tcmd = \"wget -c \" + package + \" \" + destination_path\n\t\t\t"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef installAll():\n\t\"\"\"\n\tThis function should be called by user, because the installer needs\n\tto be called with arguments before this function is executed.\n\t\"\"\"\n\tprint(\"Extracting installers\")\n\tos = {'fedora':'linux', 'linux': 'linux'}\n\tos_release = os[osName]\n\tos_major, os_minor = os_release.split('.')\n\tos_release = os_release + '12'\n\trelease_url = release_url % {'os_major': os_major, 'os_minor': os_"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef get_package_to_extract():\n\t\"\"\"\n\tReturns a path to the extracted package in the archive.\n\t\"\"\"\n\t# get the URL\n\turl = \"https://github.com/dmlc/modded/\" + VERSION + \"/\" + VERSION + \".tar.gz\"\n\t\n\t# the URL for the compressed archive of the repo\n\tcompressed_url = requests.get(url, stream=True, auth=REQUEST_SIGN)\n\t\n\t# open the.tar.gz archive\n\tcompressed_file = open(compressed_url, 'rb')\n\tarchive ="}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\twith open(url, 'w') as tar_file:\n\t\t\tfor root,dirs,files in os.walk(destination_path):\n\t\t\t\tfor dir in dirs:\n\t\t\t\t\ttar_file.write(os.path.join(root,f))\n\t\t\ttar_file.close()\ndef extract_archive(url, destination_path):\n\t\"\"\"\n\tgiven an url and a destination path: download archive\n\tand extract.tar.gz archive which contains 'desc' file for each package.\n    each.tar.gz archive corresponds to an Arch Linux repo ('core', 'extra"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef download_pkgbuild_log(filename):\n\t#download url (this function downloads the url from disk) from\n\turl=\"\"\n\turl_directory=\"\"\n\tftp=\"\"#host_url=\"\"\n\tftp_directory=\"\"\n\thost_dir=\"\"\n\t#target of downloading files from FTP through FTP\n\tftp_ftp=\"ftp.ncbi.nlm.nih.gov/pub/ncbi/archive/master\"\n\tftp_ftp_folder=\".tar.gz\"#folder to be downloaded \n\t#define ftp path\n\tdownload_folder_dir"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\nbase_url = \"https://pypi.python.org/packages/source/d/d/django/%s\"\t#TODO\n\"\"\"\nThe base URL for all source packages. For the latest version of d,\nthe base URL is https://d.d-forge.org/\n\"\"\"\n\n\ndef clean_url(url):\n\t\"\"\" Clean a URL and any trailing slashes from it \"\"\"\n\turl = re.sub(r' ', r'~', url)\n\treturn \"%s/\" % url.strip('/')\n\n\t\ndef url_to_dir_str(url_string):"}, {"generated_text": "import requests\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.\t\"\"\"\n\t\ndef get_build_path(archive_path):\n\n\treturn os.path.join(archive_path, 'build')\n\t\ndef get_package_version(url):\n\n\turl = url + '/archive'\n\n\treq = requests.get(url)\n\n\tif req.status_code == 200:\n\n\t\tarchive = req.content\n\n\telse:\n\t\tprint('Something went wrong while parsing'+ archive_path)\n\t\texit()\n\n\n\twith open(get_build_path(archive_path), 'rb') as f:\n\t\tdata = f.read()"}]}
{"task_id": "62ece4982e6aefcf4aabbd70", "prompt": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\ndef isMacOS():\n\t\"\"\"\n\tChecks if the is-MacOS OS. This function works like Linux, which we need to find a folder called'resources'.\n\tFor example, Linux: \n\tOn Unix-x86: \"/Users/Sarah Hanna/Documents/MacOS\"\n\tFor Mac OS X: \"/Users/Sarah Hanna/Documents/MacOS/Profiles\"\n\t\tAt the moment only Windows is used. \n:param str\tpath/dir name\n\t:return: True if macOS is macOS\n:rtype: bool"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\ndef CheckForDos(os):\n\t\"\"\"\n\tFor a given OS, if it is a macOS, try to see if it is part of a given\n\tmacOS file, and return it.\n\n\t:param os: The operating system name to check for.\n\n\t:type os: str   \n\t:return: A boolean indicating if the os is part of a given macOS file, else False.\n\t:rtype: bool\n\t\"\"\"\n\tif os and os.isfile(os):\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef IsMac"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\ndef get_cpu_info():\n\t\"\"\"\n\tGets the cpu information using lscpu\n\n:return: A dict with key the following format:\n\t<system type>:<processor number>\n\twhere <processor number> is an integer\n\t\"\"\"\n\tfp = open('/proc/cpuinfo', 'r')\n\tlines = fp.readlines()\n\tfp.close()\n\tcpu_info = {}\n\t\n\tfor line in lines:\n\t\ttry:\n\t\t\ttmp = line.strip().split(' ', 1)\n\t\t\tif tmp[0]!= 'processor':\n\t\t\t\t"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\tismac=True\n\t\tret = self.isMacOSX()\n\t\tif ret:\n\t\t\tismac=False\n\n\t\treturn ismac\n\t\t\t\n\tdef isMacOSX(self):\n\t\t\"\"\"\t\n\t\tChecks if a given OS is a macOS\n\t\t\"\"\"\n\t\tif os.uname()[-1]:\n\t\t\t# get macOS from os\n\t\t\tstatusoutput=self.runcommand('arch-scan -p')\n\n\t\t\t# try to get the MAC OS name\n\t\t\tif len(statusoutput.split(' '))==1:\n\t\t\t\tmacos=statusoutput.split(' ')[0"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\n\t\n\tos_is_mac = get_os()\n\tif (os_is_mac[0] == 0):\n\t\treturn False\n\t\n\treturn True\n\t\n\n###############################################################################\n# Tests for XlsxWriter.\n#\n# Copyright (c), 2013-2015, John McNamara, jmcnamara@cpan.org\n#\n\nfrom..excel_comparsion_test import ExcelComparisonTest\nfrom...workbook import Workbook\n\n\nclass TestCompareXLSXFiles(ExcelComparisonTest):\n    \"\"\"\n    Test file created by XlsxWriter against a file"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\treturn os.system(\"stty -an\")\n\n\"\"\"\nCreates config files that can be used in Linux systems\n\n\"\"\"\ndef make_config(os_version, device, macOS, cpu):\n\tprint(\"Create config files for OS: %s, MacOS: %s, CPU: %s\" % (os_version,\n\t\tmacOS, cpu))\n\tshutil.rmtree(\"build\")\n\tshutil.rmtree(\"runtime-build\")\n\tshutil.rmtree(\"system\")\n\n\tshutil.copytree(\"..\", \"android/\")\n\n\t# Create config files for OS:"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\ndef getKernel32Version(kernel):\n\t\"\"\"\n\tGets the kernel 32 version using sysctl.\n\n\t:param kernel: Kernel version.\n\n\t:rtype: int\n\t:return: Kernel 32.0 or greater\n\t:rtype: int\t\"\"\"\n\n\t\n\ttry:\n\t\tif kernel[5:].startswith('32'):\n\t\t\tif int(kernel[5:]) > 31 and int(kernel[:9]) < 255:\n\t\t\t\treturn True\n\t\telse:\n\t\t\tif int(kernel[:9]) >= 1 and int(kernel[:9]) > 255:\n\t\t\t\treturn True\n\t"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\ndef get_version():\n\t\"\"\"\n\tReturns the os.version (for checking that the required version is 2.4 or 3.2):\n\tAssumes the OS is greater than 2.4\n\t\"\"\"\n\treturn re.findall('\\d+\\.\\d+\\.\\d+', os.uname()[2])\n\ndef get_date():\n\t\"\"\"\n\tReturn the date the script was executed\n\n:return: string date-time\n:rtype: string\n\"\"\"\n\t#get date\n\tdate = file('update_version.html')\t\n\tlines = date.read()."}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\tosstatus\t\t= subprocess.check_output(\"type'macOS'\", shell=True).decode(encoding='UTF-8').strip().split(\"\\n\")\n\t\tif osstatus:\n\t\t\tosstatusList\t\t= osstatus.split(\"\\n\")[:-1]\n\t\tmacUserList\t= []\n\t\tfor mac in osstatusList:\n\t\t\tif mac:\n\t\t\t\tif mac[0]==\"\\xa4\\xe4\\x04\": # Windows 3.0 : Mac OS X\n\t\t\t\t\tmacUserList.append(mac)\n\t\t\n\t\t\t# check if mac was in OS X"}, {"generated_text": "import os\t\"\"\"\n\tChecks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\t\"\"\"\n\t\ndef mac_os_detect_node():\n\ttry:\n\t\tstatus,output = shellutil.run(\"if! (ping -c 1 -w 10 -c 1 \"\n\t\t\"openssh-server -T -t 2 -l 5 | \"\n\t\t\"tail -1 )\"\n\t\t)\n\t\toutput = status + output\n\t\tif (output == 1):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n\ndef mac_os_detect_version(path):\n\tif mac_os_detect_node():\n\t\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd71", "prompt": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\ndef main(argv, modifiers):\n\t# set up arguments \n\targc = len(argv)\n\targlen = len(modifiers)\n\n\t# read the whole input file\n\tdata = parse_file(argv[0])\n\n\t# remove comments and blank lines\n\tdata = filter_comments(data)\n\n\t# read the actual rules dict\n\tdata = parse_and_save_rules(data, arglen)\n\n\t# write results\n\twrite_result(data, argc, arglen, modifiers)\n\nif __name__ == \"__main__\":\n\t"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\n# regex_keys = {'\\\\w': 'WALL_VERILOG'}\n# regex_keys = {'\\\\w': 'RENAMED_VERILOG'}\n\n# TODO make more regexed regexes\n\ndef get_rule2(rules):\n    \"\"\"\n    Generate regex_keys from rules\n    \"\"\"\n\n    rules = {'all': dict((k, re.compile(k).match(v))\n                    for k, v in rules.items())}\n    regex_keys = {}\n    for group, regex in regex_dict().items():\n        regex_keys.update(regex(group"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\ndef regex_key_transform(item):\n\tr = re.compile(item['b'], re.IGNORECASE)\n\treturn {'a': r.sub(item['b'], item['a'])}\n\n#def do_search(items, regex_key, regex_key_transform, results):\n#\tfor r in search_regex(\n\t\t[regex_key_transform(i) for i in items],\n\t\tfilter_regex = items!= None\n\t\t):\n\t\tappend_item = dict()\n\t\t#print r\n\t\tresult = regex_key(r,"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\treturn re.compile(item['a'] + item['b'] + r'.*\\.H\\(\"(' + item['h'] + r'[^)]\\)', re.IGNORECASE)\n\t\ndef search_cpp(name):\n\tlist = get_files(name)\n\tpatterns = []\n\tfor rule in list:\n\t\trules = rules.copy()\n\t\tif not rule['a'][0] in regex_keys:\n\t\t\traise Exception('#' + name +'does not have the key'+ rule['a'][0] +'in rules')\n\t\t#rules = rules.copy"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\ndef is_glob_pattern(filename, pattern):\n\t\"\"\"\n\tReturn whether the given path contains the given pattern.\n\n\tFirst argument is the pattern, the second the path to be\n\tcompiled and the regexp used to extract the glob matches with\n\tthose patterns.\n\n\tRaises:\n\t\tValueError\n\t\"\"\"\n\treturn re.compile(pattern, re.I|re.L)\n\ndef compile_regex(filename, items):\n\t\"\"\"\n\tCompile regex to regex, with matching items from items\n\n\tArgs:\n\tpattern\t-- regex pattern\n\titems\t"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\n\t\n\t# Regex to convert regex into regexes\n\tpattern_re = re.compile(\"({0}[^{0}]+?)\".format(\"|\".join(regex_keys.keys())),re.IGNORECASE)\n\n\tdef _convert_regex(r):\n\t\t# Re-write the regex\n\t\tregex = re.compile(pattern_re.format(item['a']))\n\t\t\n\t\t# Re-split the regex by commas, if any, and convert to regexes\t\n\t\tregex_split = '({1})'.format(\"|\".join(regex_keys.keys())).split(',')\n\t\treturn"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\n# Get the strings from the regexes to compile them into regexs\nregex_strs = regex_dict(regex_dict(item))\n# We want all the regex strings to be lowercase to avoid\n# including the '_' separators\nregexes = { \n    'android': '^arm\\b', 'armeabi-v7a': '^armv7a',\n    'asrt': '^asrt$', 'asrt': '^mips', 'bsd': '^bcm\\b', 'bull': '^bull\\b',\n    'bull': '^barr', 'cmov': '^c"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\t#TODO: Check this!\n\treturn {'a': re.compile(key)}\n    \ndef process_header_guard_rule(rules, filepath):\n    \"\"\"\n    Process header guard rules to match the \"header guard rule\" rules.\n    (The rule format is the same as the rule format used to be\n    when detecting and correcting the header guard)\n\n    rules:\n        Dictionary mapping the rule format string (keys are string names)\n        to a dictionary like object (values depend on a particular rule format)\n        and return a match dict where rules are stored as object instances.\n        The values of the dictionary"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t\ndef parse_rules(item):\n\trules = []\n\tfor rule in item['rules']:\n\t\trules.append(rule)\n\n\treturn rules\n\ndef filter_keys(regex_pairs):\n\titems = []\n\n\tfor rule in regex_pairs:\n\t\tfor i in range(0,len(rule['rules'])):\n\t\t\tif re.compile(rule['rules'][i]['rule']) is True:\n\t\t\t\titems.append(rule)\n\n\treturn items\n\n\ndef get_matching_keys(files_list, regex_dict):\n\t\n\tkeys = filter_keys"}, {"generated_text": "import re\t\"\"\"\n\tConvert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\t\"\"\"\n\t # TODO: What if different lengths?\n\tkeys_count = len(re.findall(r'^\\*', item))\n\n\tnew_list = []\n\tfor key in keys_list:\n\t\tif len(new_list) > idx:\n\t\t\tnew_list.pop()\n\t\telse:\n\t\t\tregex_keys[key] = re.compile(re.escape(key), re.IGNORECASE)\n\treturn new_list\n\n\"\"\"\nReturn new pattern as python regex string\n(e.g. '#this is a comment')\n\npython regex rules for: \n- for"}]}
{"task_id": "62ece4982e6aefcf4aabbd72", "prompt": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\ndef getSuff1String(word,count_quote):\n\tif count_quote>0: count_suf = count_quote\n\telse: count_suf = \"?\"\n\t#print \"count_suf of: \", [count_suf]\n\t\n\t#count_quote +=1\n\treturn count_suf\ndef getSuff(target):\n\tlist = []\n\tfor letter_num in xrange(len(target)):\n\t\tfor d in xrange(len(target)):\n\t\t\tif target[d]==letter_num:\n\t\t\t\tcontinue\n\t\t\telse"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\tname = re.sub(\"\\\"\",\" \",name)\n\treplace = \"\\\"\"+\" | \"\n\treturn  name.replace(replace,\"\")\n\ndef get_all_field(doctype):\n\t\"\"\"\n\t\t\tGet all fields of type document\n\t\t\"\"\"\n\timport webnotes\n\tdoctype = webnotes.local.form_dict.get(doctype)\n\treturn get_doctypes_with_parent(doctype)\n\ndef get_all_link_fields(doctype, meta):\n\t\"\"\"\n\t\tGet link fields of doctype\n\t\tand fields of meta doctype\n\t\"\"\"\n\t"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\ndef replace(source, replace, new_lines):\n\t\"\"\"\n\tReplace a given line in an open file.\n\t\"\"\"\n\tf = open(source, 'r')\n\tcontent = f.read()\n\tf.close()\n\tnewstr = re.sub(replace, '', content)\n\tf = open(new_lines, 'w')\n\tf.write(newstr)\n\tf.close()\n\ndef process(source, dest):\n\t\n\tsource = source.replace('\\'','\"')\n\tdest = dest.replace('\\'','\"')\n\t\n"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\nclass Cut:\n\t\"\"\"\n\tRepresents a censor\n\t\"\"\"\n\tdef __init__(self,name):\n\t\tself.name = name\n\t\tself.name_lower = \"\"\n\t\tself.name_lower_to_title = \"\"\n\t\tself.name_lower_to_first = \"\"\n\t\tself.name_lower_to_last = \"\"\n\t\tself.name_lower_to_title = \"\"\n\t\tself.name_upper = \"\"\n\t\tself.name_upper_to_title = \"\"\n\t\tself.name_upper_to_first"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\tname = name.replace('\\'', '\\'\\'')\n\t\tname = name.replace('\"','')\n\t\treturn name\n\n\"\"\"\n\tGet string to parse the given name.\n\t\"\"\"\t\tinput = raw_input(\"Please enter a string: \")\n\t# strip punctuation\n\tstripped_name = clean_string(input)\n\tname = cleaned_name.translate(string.maketrans(\"\",\"\"),'\u2014/')\n\t\n\treturn cleaned_name\n\n\"\"\"\n\tParse the given path to an API path and return JSON format to be\n\treturned as a dictionary\n\t\"\"\"\t\n"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\treturn re.sub( '\\'', r'', name ) \t\n\t#\n\t# Get the filename with a trailing character (\\0)\n\t_name_with_trailing_slashes = re.sub( '/$', r'', name )\n \n\t# Check for the last argument of the name\n\tif len( name ) > ( len( _name_without_trailing_slashes ) - 1 ):\n\t\tprint( 'The name \"' + name + '\" is longer than the maximum length of'+\n\t\t\t'the argument to the function.  The name \"' + name + '\"'\n\t\t\t+ '. It"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\n#\tdef set_parent_quote(self):\n#\t\tself.parent_quote = None\n\n\n#def set_parent_quote_char(self):\n#\t\t\"\"\"\n#\t\tChange the parent quote character. \n#\t\t\"\"\"\n#\n#\tdef set_char(self, char):\n#\t\told_parent_quote = self.parent_quote  # remember parent quote\n#\t\tif char == self.parent_quote:\n#\t\t\treturn self\n#\t\telse:\n#\t\t\tself.parent_quote = char"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\ndef main():\n\tparser=argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,epilog=\"\"\"example usage:\n\tpy-parse.py -c <name-of-file> \n\t[arg0:list[regex]]\n\toutput the list of files in a given directory using 'glob' in name\n\tor 'glob-fullnames'; for a description of the syntax used in the\n\targument, see 'name.py -h'.\n\t\"\"\")\n\tparser.add_argument('--version',action='version',version='0.0.1')\n\tparser"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\n\t\n\tprint \"Removing quote from '%s'...\" % quoted_name\n\tquoted_name = re.sub('\"', '\\\\\"', quoted_name)\n\tprint \"The quote removed from '%s': \" % quoted_name\n\t\n\tprint \"Removing quote from '%s'...\" % quoted_name\n\tquoted_name = re.sub('\"', '\\\\\"', quoted_name)\n\tprint \"The quote removed from '%s': \" % quoted_name\n\t\n\tprint \"removing quote from '%s'...\" % quoted_name\t\n\t\t\n\tprint \"The quoted-name of '%s' removed.\" % quoted_"}, {"generated_text": "import re\t\"\"\"\n\tRemove quote from the given name.\t\"\"\"\n\t\n#class RemindMe:\n\t\n#def StripNulls (text, pattern):   # Removes all whitespaces from the given text, where the given pattern matches.\n#\tpattern = re.compile(pattern)\n#\ttext = text.rstrip()\n#\ttext = text.rstrip(\"'\").rstrip('_')\n#\ttext = text.rstrip(\"'\")\n#\tprint(text)\n\n\ndef Suffix (string):     # Returns the suffix (e.g. 'foo-bar-1' becomes 'foo').\n#\tstring = string.replace('foo-', 'foo"}]}
{"task_id": "62ece4982e6aefcf4aabbd73", "prompt": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\t)\n\t\treturn r.split(None, 1)\n\tif not is_string(cmd):\n\t\tcmd =''.join(cmd)\n\treturn _call_shlex2(\n\t\tcmd, shell, env = env,\n\t\tuniversal_newlines = universal_newlines,\n\t\tshell = shell\n\t)\n\ndef _call_shlex2(cmd, shell = True, env = None,\n\tuniversal_newlines=True, shell = True):\n\t\"\"\"Run a command with input text, expanding ``shell`` (if required) and\n\tthen logging the output using"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\n        if sys.platform == 'win32':\n\t\t\n        def get(self, section, option):\n                if (program_exists('grep') and has_option('grep', section, option)):\n                        lines = self.get_as_list(section, option)\n                        for l in lines:\n                                m = re.search('([^:]*):(.*)', l)\n                                if m:\n                                        return l\n                return\n\n#\t\tif has_option('grep', section, option):\n#\t\t\t\tlines = self.get_as_list(section, option)\n#\t\t\tfor l in lines"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\treturn r\n\telse:\t\t\n\t\treturn shlex.split(_to_bytes(w, errors='surrogate_or_strict'))\n\n\ndef _split_args(argstring):\n\t\"\"\"\n\tTokenizer for cmd line arguments.\n\n\tArgs:\n\t\targstring: The command line arguments as a String, MultiLine, or a callable\n\t\t        supporting one or more arguments from the command line.\n\tReturns:\n\t\tA list containing the args received.\n\t\"\"\"\n\tsplits = argstring.strip().split(os.pathsep)\n\targs = []\n\tfor arg in splits"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\nclass Sha256 (object):\n\t\"\"\"The shexr and stout file reader.\"\"\"\n\n\tdef __init__(self, arch, mode = 'rb', bufsize = 376):\n\t\tself.arch = arch\n\t\tself.mode = mode\n\t\tself.bufsize = bufsize\n\t\tself.buf = bytearray()\n\t\tself.pos = 0\n\n\tdef read(self, num = 1):\n\t\ti = 0\n\t\tself.buf = bytearray()\n\t\twhile i < num: self.buf.append(self.arch[i] & 0xFF)\n\t\t# reset"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\tpass\t\n\t\t#try:\n\t\t\t#import subprocess\n\t\t\t#exit()\n\t\t#except ImportError:\n\t\t\t#print('Using non-Windows platform.')\n\t\t#import sys\n\t\t#print('sys.byteorder:')\n\t\t#print(sys.byteorder)\n\t\t#from os import sys\n\t\t#if sys.byteorder!= 'little':\n\t\t\t #raise ValueError(\"System byteorder!= 'little'!!!\")\n\t\t\t\t#ret = subprocess.call(['uname','-m'])\n\t\t#else:\n\t\t\t #raise ValueError(\"Platform is not supported!\")\n\n\ndef make_path_absolute(path):"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\n\n\t# If we're given a string, return the arg as a single line\n\t# If not, let the subprocess decide\n    if isinstance(cmd, str):\n\t\tif os.name == 'nt':\n\t\t\t# On Windows, subprocess.call() requires a trailing space\n\t\t\tcmd = cmd + \" \"\n\t\t\tif os.environ.get('TERM') in (\"Windows\",\"\"):\n\t\t\t\tcmd = cmd + \" \"\n\treturn [x.replace(\"\\n\",\"\").rstrip()+\"\\n\" for x in shlex.split(cmd)]\n\n\n\n\ndef Popen(cmd, shell=False"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\tsplitline = re.split(r'[\\s\\t]', cmd)    \n\t\ttry:\n\t\t\t(opt, arg) = splitline[0], splitline[1]\n\t\texcept IndexError:\n\t\t\traise ValueError(\"invalid shlex syntax '%s'\" % (cmd,))\n\n\t\tif opt not in cmd_prefixes:\n\t\t\traise ValueError(\"invalid shlex syntax '%s'\" % (cmd,))\n\t    cmd_prefixes[opt] = cmd_prefixes[opt].replace(arg, '\"') #!!\n\n\t\tif not is_List(arg):\n\t\t\traise ValueError(\"invalid argument to '%s'\""}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\n    class re_pattern(object):\n        def __init__(self, pattern, flags=0):\n            self.pattern = re.compile(pattern, flags)\n        def match(self, s):\n            result = self.pattern.match(s)\n            return result!= None\n        def search(self, s):\n            return self.pattern.search(s)\n            return self.pattern.search(s).span()\n\nif __name__ == '__main__':\n    import re\n    r = re.compile(re.escape(_binstr), re.IGNORECASE)\n    m = r.match('blah blah"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\ndef execute(*cmd, **args):\n    \"\"\" \n    Execute a shell command, return stdout and stderr.\n    \"\"\"\n    with Popen(cmd, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT, **args) as proc:\n        if'stdout' in proc.stdout:\n            (out_, er_) = (proc.stdout, proc.stderr)\n        else:\n            (out_, er_) = proc.communicate()\n    return (out_, er_)\n\ndef find_modules_and_package_files(*modules):\n    \"\"\" find_modules_and"}, {"generated_text": "import re\t\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)\t\"\"\"\n\t\tshell\t  = \"/bin/sh\"\n\t\tlinesep\t = '\\r\\n'\n\tif  platform == 'win32':\n                sep = '\\\\'\n                pathsep = ';'\n\tshell\t  = win32_quote(shell) + \"cmd\"\n\tlinesep = os.linesep\n\tcmd \t    = win32_quote( cmd )\n\tsep = os.sep\n\tpathsep = ';'\t\t# Fix up Windows path separator\n\targs\t    = args.strip()\n\t\n\t\n\tdebug(\"Running : %s \", cmd)\n\treturn os.system( args +"}]}
{"task_id": "62ece4982e6aefcf4aabbd74", "prompt": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\ndef main():\t\t\n\tprint(\"Start\")\n\tsubprocess.call(\"gzip\", \"-9c\", \"./gen-buildpackage.py\")\n\tsubprocess.call([\"wget\",\"./gen-buildpackage.py\"])\n\tsubprocess.call([\"cp\", \"../gen-buildpackage.py\", \"./\")\n\tsubprocess.call([\"gzip\",\"-9c\", \"./*.gz\"])\n\n\tprocess = subprocess.Popen([\"sh\", \"./gen-buildpackage.sh\"],stdout=subprocess.PIPE,stderr=subprocess.PIPE,shell=True)\n\tout, err = process.communicate()\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\t# If archive_path is none then that is the path to the file repos.\n\t\tif archive_path:\n\t\t\trepo_url = archive_url(archive_path)\n\t\telse:\n\t\t\trepo_url = repo_url.rstrip('/')\n\n\treturn repo_url\n\ndef fetch_archive_path(url, local_path, archive_path = None):\n\t\"\"\"Fetches the file at url using the local path. \n\t\tAssumes download is in $PATH\"\"\"\n\t\n\tif local_path.endswith('/'):\n\t\tlocal_path = path_strip(local_"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\ndef fetch_repo(source_dir_path):\n\t\"\"\"Recursive function to download repo from user's machine.\n\tThe repository path is converted to directory+user@host/path\n\tReturns a file repo url.\"\"\"\n\tprint\t(\"Fetching {}\".format(source_dir_path))\n\tos.chdir(source_dir_path)\n\tsubprocess.call(['git', 'clone',source_dir_path])\n\tos.chdir(source_dir_path)\n\treturn repo_url\t\n\n\ndef main():\n\t# fetch a copy of the repo from"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\ndef find_repository (archive_path):\t\n\t\"\"\"\n\tGiven a valid path to a source archive, read the contents. Returns the path.\n\t\n\t@type  archive_path: unicode\n\t@param archive_path: A full path to an archive.\n\t           Must contain at least one sub-directory.\n\t\n\t@rtype: unicode\t\n\t@returns: A path to the source archive's contents.\n\t\"\"\"\n\t\n\t# Make a new archive repository, with empty parent directories, etc\n\tos.mkdir (archive_path)\n\tos.mkdir (os.path."}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\ndef download_url(archive_path, file_name):\n\t\"\"\"\n\tGiven a full URL with archive, download it to `archive_path`, and check it exist.\t\n\t\"\"\"\n\turl = 'http://' + archive_path\n\ttry:\n\t\toutput = subprocess.check_output(['curl', url], cwd=archive_path, stderr=subprocess.STDOUT).strip()\n\t\treturn output\n\texcept:\n\t\treturn 'Failed to fetch '+url\n\ndef unzip_archive_tar(archive_path):\n\t\"\"\"\n\tUnzip the archive in the given"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\ndef get_repo_url_with_auth(archive_path):\n\t\"\"\"\n\tGiven a specific archive_path and the repository path, return its url with\n\ttwo levels of keys \"username\", \"password\", \"location\"\n\t\"\"\"\n\treturn 'http://localhost:8000' + \\\n\t\t'/repository/v3.1/repository/archive-v3.1.zip' + \\\n\t\t'/repository/v3.1/repository/archive-v3.1.zip' + \\\n\t\t'/archive-v3.1.tar.gz' + \\"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\ndef createZippedBaseUrl(archivePath, zipPath):\n\t\"\"\"\n\tMake the zipped base url\n\tgiven the archive\n\treturns the path of directory where the new archive/directory should be created as\n\t\n\tzipPath - path to.zip file of the compressed directory to put in\n\t\"\"\"\n\tunzipCommand=\"unzp < \\\"%s\\\" 2> %s/Contents/Resources/Recon/Archive/unzip\" %(archivePath,zipPath)\n\tcall(unzipCommand,shell=True) \n\ndef readArchive(archivePath):\n\t\"\"\"\n\tread"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\ndef get(repo_url):\n\t\"\"\"\n\tGiven a URL, try to decrypt it's mirror location.\t\n\tRetain the last component of the filename.\n\t\"\"\"\n\tmirror_location = subprocess.Popen([\"./chromedocs/\", \"uncompressor-decrypt\", \"--output=\" + repo_url], stdout=subprocess.PIPE).stdout.read().strip().decode(\"utf8\").encode(\"utf8\")\n\tmirror_location = mirror_location.strip()\n\tmirror_location = mirror_location.split('/mnt/hdp/')[-1]\n\treturn"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\nclass GPG(object):\n\t\"\"\"Base class for handling GPG files.\"\"\"\n\tdef __init__(self, path):\n\t\t# Get archive path directory from command line argument\n\t\tpath, self.archive_path = path, path\n\t\tif not os.path.exists(self.archive_path):\n\t\t\tos.makedirs(self.archive_path)\n\t\t\n\tdef list_items(self, folder, recurse=True):\n\t\t\"\"\"Search for files and items under given folder and optional recursively if True is given.\"\"\"\n\t\t\"\"\"Return list of items in subfolder or items of items"}, {"generated_text": "import subprocess\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.\t\"\"\"\n\t\ndef main():\n\tif os.listdir:\n\t\tprint \"Removing the temp dir\" \n\t\tshutil.rmtree(os.getcwd() + \"/temp\", ignore_errors=True)\n\t\t\nif __name__ == \"__main__\":\n\tmain()\n'''\nCreated on Jan 7, 2016\n\n@author: bogdanz\n'''\n\nfrom slovar.network.shared import Network\nimport numpy as np\nfrom slovar.distributions import *\n\nclass NetworkML(Network):\n\n    def __init__(self, shape, weights=None):\n        super(NetworkML,"}]}
{"task_id": "62ece4982e6aefcf4aabbd75", "prompt": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\treturn [name for name in os.listdir(path_to_repo)\t+ ignore]\n\t\"\"\"\n\tReturn a list containing the files in the current directory that are not ignored.\n\t\"\"\"\n\tdef isIgnoreFile(name):\n\t\tif name not in ignore:\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\t\"\"\"\n\tDo the git command and capture the log so we know to make sure the current commit is on the git repo.\n\t\"\"\"\n\ttry:\n\t\tcheckCall([\"git\", \"log\", \"--pretty=%h\", \"--reverse\"])\t#"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\ttry:\n\t\t\tfiles = subprocess.check_output(\n\t\t\t\t[sys.executable, '-c', 'import git; git.cmd.list(files)']\n\t\t\t)\n\t\texcept subprocess.CalledProcessError as e:\n\t\t\toutput = e.output.decode()\n\t\t\tprint(output)\n\t\t\traise # Should have found some error?\n\t\tif len(files.strip()) > 0:\n\t\t\treturn files.strip()\n\t\telse:\n\t\t\traise OSError('No valid files found,'+ str(files))\n\n\tdef extract_archive_file(self, archive_file_"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\ndef main(argv):\n        with open(argv[1], \"r\") as fp:\n                with subprocess.Popen(to_cmd_lines(fp.read()),\n                                     shell=True) as proc:\n                        lines = proc.stdout.readlines()\n                        for i, line in enumerate(lines):\n                                if i:\n                                        subprocess.Popen(['git log --pretty=format:\"%an %ae%n\"', line],\n                                                     stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True).wait()\n                                \n                names = []\n                files = []\n                lines = []\n                for i, line in enumerate("}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n\n\nif __name__ == \"__main__\":\n\tmain()import glob\nimport os\nfrom sys import version_info\nfrom PyQt4 import QtGui, QtCore\n\nimport Orange\nimport Orange.data\nfrom Orange.widgets import widget, gui, settings\nfrom Orange.widgets.utils.scaling import (\n    LinearScaleEngine, OWGUI, FactorRange,\n    OWGUI_SLIDER,\n    OWGUI_SORTING_HORIZ,\n    OWGUI_SORTING_LEAST)\nfrom Orange.widgets.utils.itemmodels import PyTreeModel"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n\n\tif args.command!= \"list\":\n\t\tcommand_lst = [args.command]\n\t\t\n\t\tif args.command == \"git\":\n\t\t\tsubprocess.call([\"git\", \"-C\", args.path, \"diff\", \"--quiet\"], stdout=subprocess.DEVNULL, stderr = devnull)\n\t\telif args.command == \"diff\":\n\t\t\tsubprocess.call([args.path, \"diff\", \"--no-color\"], stdout=subprocess.DEVNULL, stderr = devnull)\n\t\telif args.command == \"unstaged\":\n\t\t\tsubprocess.call([\"git\", \"-C\", args"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n\n\toutput = subprocess.run(shlex.split(command), stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, check=True)\n\t\n\toutput_temp = output.stdout.decode(sys.stdout.encoding).splitlines()\n\tfile_list = []\n\n\tfor filename in output_temp:\n\t\tfile_list.append(filename.strip())\n\n\tfile_list = sorted(set(file_list))\n\treturn file_list\n\ndef process_file_list (directory,file_list):\n\n\tfiles_to_process=[]\n\n\tfor"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n\t\n\ttry:\n\t\tfrom cStringIO import StringIO\n\texcept ImportError:\n\t\tfrom cStringIO import StringIO\n\timport re\n\t\n\t\n\tversioninfo = 1\n\tfilenametempl = \"templ\"\n\tfilename = \".\" #default templ name\n\tfilenametempt = \"/t\"\n\tfilenamet = None\n\tfilenamel = None\n\tprefix = \"\"\n\t\n\t#get filename: \n\ttry:\t\tf = open(\"templ/\")\n\texcept IOError:\n\t\ttry:\t\tf = open(\"templ/\")\n\t\texcept IOError:\n\t\t\tsys.stderr.write(\""}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n\n\tdef call_git_p4(self, cmd): # pylint: disable = R0201\n\t\t'''\n\t\t\tCall p4\n\t\t\n\t\t\t@param\tcmd\t\tString command\n\t\t\t@returns\t\tString result\n\t\t'''\n\t\targs = shlex.split(cmd)\n\t\tp4_ret=''\n\t\tretcode = subprocess.call(args)\n\t\tp4_output=''\n\t\tstdout = subprocess.Popen(args, stdout=subprocess.PIPE, shell=False).stdout\n\t\tfor line in stdout.readlines():\n\t\t\tif stdout.fileno()!= -1:\n\t\t\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\nclass Usage(Exception):\n\tpass\n\nclass InvalidRepository(Exception):\n\tpass\n\ndef init_arguments():\n\tparser = OptionParser(help=\"git init <path> <ref>, where <ref> is either a local or a remote repository (e.g., the \"  +\t\t\"full git URL containing local, remote, or upstream repository)\")\n\tparser.add_option(\"-d\", \"--debug\",\n\t\t\t\t  help=\"DEBUG level\",\n\t\t\t\t  action=\"store_true\")\n\tparser.add_option(\"-v\", \"--verbose\",\n\t\t\t\t  help=\"Verbose level\",\n\t\t\t\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\t\"\"\"\n\t\n\n\tfile_list=[]\t\n\n\tfor line in open(file_path,'r').readlines():\n\t\tif line.startswith('#'):\n\t\t\tcontinue   \n\t\tfile_list.append(line) \n\n\treturn file_list\t\n\t\t\n\t\t\ndef execute_file(file_path, shell=False):\n\t\"\"\"\n\tExecute the given file\n\t\n\t:param file_path: The path of the file to be executed.\n\t:param shell: If True, execute the command in a shell.\n\t:return exit code, the output of the command.\n\t\"\"\"\n\t\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd76", "prompt": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\n####################################################################\ndef is_module_exists(module):\n\t\"\"\"check if the given module name already exists\"\"\"\n\treturn os.path.exists(module.__file__)\t# Is there any filename in that folder? (if so return false)\n\n####################################################################\ndef is_module_file(module):\n\t\"\"\"check if the given module is actually a module file\"\"\"\n\tmodulename=module.__name__\n\ttry:\n\t\t(f,ext)=(modulename.split('.'))\n\texcept (AttributeError,ValueError):\n        \treturn False\n\textname=ext\n\t"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\n\n\n\n\ndef CheckPcd(filename):\n\t\"\"\"\n\tChecks to see if the filename is a PCD archive.\n\t\n\t@param filename: A file object\n\t@returns: A file object if it is a PCD archive, False otherwise\n\t\"\"\"\n\n\ttry:\n\n\t\tf = open(filename, \"rb\")\n\n\n\t\tcontents = f.read()\n\n\t\t# The first time through this is the type, then it will be a PCD file\n\t\t# The next will be filename\n\t\tp = re.search('PCD format',contents)\n\t\tif p:\n\t\t"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef matchFile(filename):\n\t\"\"\"checks whether the match passed and returns true if it is an archive file, False if not\n\t\"\"\"\n\treturn os.path.splitext(filename)[1] in ['.tar', '.tar.gz']\n\t\t\t\ndef matchExt(filename):\n\t\"\"\"checks whether the match passed and returns true if it is a.ext.tar file, False if not\n\t\"\"\"\n\treturn os.path.splitext(filename)[1] in ['.ext', '.ext.tar', '.tar.bz2']\n\t\t\t\t\t\t\t\ndef matchDsc(filename):\n\t\"\"\""}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\n\n\texts = { \n\t\t\t'gif':'gif',\n\t\t\t'jpg':'jpeg',\n\t\t\t'tif':'jpeg',\n\t\t\t'png':'png',\n\t\t\t'webm':'webm'\n\t\t\t}\n\n\tfor ext in exts:\n\t\tif filename.endswith(ext):\n\t\t\treturn False\n\n\treturn True\n\n###########################################################################################\ndef matchFile(filename):\n    \"\"\"Check if the specified file object is a valid file\n\n\tArgs:\n    filename: Filename to match\n\tReturns:\n    True if the file matches\n\t\"\"\"\n\n\n\tif not os.path.exists(filename):\n\t\t"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef extractImage(imagePath):\n\t\"\"\"\n\tGiven a path, return PIL Image class\n\tReturn None if image is not supported\n\nArgs:\n    imageName: The absolute path to the file\n\tReturns:\n    True, False if not supported\n\n\"\"\"\n\t#get the directory of the file\n\timagePath = Path(imagePath,'..')\n\timagePath.mkdir()\n\t#check if the file exists\n\tif os.path.exists(imagePath):\n\t\tpass\n\t#retrieve each image file.\n\tfiles = ImageFiles(imagePath)\n\n"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef verifyType(filename):\n\t\"\"\"\n\tCheck if the filename is a type that this file is, and check that the file does exists\n\n    args, path, name = os.path.split(filename)\n\t# Ignore the directory names\n\tif len(name) > 0 and name[0] == \"/\":\n\t\t# Ensure the path exists\n\t\tif os.path.isdir(path + \"\\\\\" + name):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\treturnFalse\n\"\"\"\n\t\ndef extractFiles(dir, prefix, name):\n\t\"\"\"\n\tExtract"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef filter_extensions(path, extensions):\n\t\"\"\"This is the function in the base, returns all the extensions that are\n\tused according to the extensions list.\n\nArgs:\n    #path: input path\n\t#extensions: list of extension to recombine to\n\t#Returns:\n\tfiltered file, list of all extensions, empty list on error\"\"\"\n\textensions = [extension for extension in extensions if extension in path.split(os.extsep)[-1].split('.')]\n\treturn extensions\n\ndef clean_name(name):\n\treturn re.sub('[?!\" -"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef usage():\n  print(\"To test:  \", sys.argv[0],\" -h\")\n  sys.exit(0)\n\t\n\t\n\t\n\"\"\"\nThis config file runs the simplest dev environment using sqlite, and db-based\nsessions. Assumes structure:\n\n/envroot/\n        /db   # This is where it'll write the database file\n        /edx-platform  # The location of this repo\n        /log  # Where we're going to write log files\n\"\"\"\n\n# We intentionally define lots of variables that aren't used, and\n# want to import all variables from base settings"}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef CheckFileType(filename):\n\t\"\"\"Returns the filetype if filename is a valid type, else False\"\"\"\n\treturn os.path.splitext(filename)[1][1:].lower() in FileType.types\n\n\n# Global Variables - global variables\n\nInstrumented = False\nExited = None \nWarning = None\nDebug = None\n\ndef log_print(txt, err_file = None):\n\t\"\"\"Prints a line containing line with a pattern to be added to stderr\"\"\"\n\tglobal Instrumented\n\n\tif debug.DEBUG == Debug:\n\t\tsys.stdout."}, {"generated_text": "import os\t\"\"\"\n\tCheck if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported\t\"\"\"\n\t\ndef __check_filename(file_name, extlen):\n\tif not os.path.isfile(file_name) or not file_name.find('.')==-1: return False\n\ti=file_name.find('.')\n\te_ext=len(extlen)\n\t# Check for file ending - if it's present in the given\n\t# extension, add it to the filename at least once to make sure\n\t# the same file name will never end up in.py\n\tif file_name[(i+e_ext):-e_ext]!=ext"}]}
{"task_id": "62ece4982e6aefcf4aabbd77", "prompt": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\n\t\n\ndef check_timestamp(string):\n    \"\"\"\n    Given a string, return a timestamp.  If it's a time stamp, return a\n    datetime.datetime() object representing the number milliseconds past the\n    epoch, or None if milliseconds past the epoch.\n    \"\"\"\n    if len(string) < 3:\n        return None\n    \n    if string[0]!= \"0\" and string[0]!= \"-\":\n        return datetime.datetime.fromtimestamp((int(string[0:3]))*1000)\n    return None\n    \n    \n    \nclass DataRange: # {{{\n\n    def __init__(self, start, end):\n       "}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef parseDurationWord(s):\n\t\"\"\"\n\tReturns a 'timedelta' instance from a string prefixed with a word,\n\tor return None if the string is 'None'.\n\t\n\t>>> parseDurationWord('2 weeks')\n\t'3 weeks'\n\t\n\t>>> parseDurationWord('  3 weeks') and parseDurationWord('2 weeks')\n\t'2 weeks'\n\t\n\t>>> parseDurationWord('  3 weeks ')\n\t'  3 weeks'\n\t\n\t>>> parseDurationWord('7weeks')\n\t'7 weeks'\n\t\n\t>>> parseDurationWord('7 weeks ')\n\t'7"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef parse_number(frequency_string):\n\ttry:\n\t\tnum = int(frequency_string)\n\t\treturn long(num)\n\texcept ValueError:\n\t\treturn None\n\n# def parse_unit(frequency_string):\n# \ttry:\n# \t\tunit = frequency_string[:-3]\n# \texcept IndexError:\n# \t\treturn None\n# \tunit_list = frequency_string.strip(\"'\").split('.')\n# \tfrequency = float(frequency_list[0])\n# \treturn unit\n\ndef validate_frequency("}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef int2time(day, hour, minute, second):\n\ttry:\n\t\tintday = int(day)\n\t\tinthour = int(hour)\n\t\tintminute = int(minute)\n\t\tintsecond = int(second)\n\texcept ValueError,e:\n\t\tif DEBUG:\n\t\t\tprint 'Incorrectly formatted time format, error: ', e\n\t\traise ValueError,e\n\t\t\n\thour,minute,second,milliseconds = intday / 1440, inthour / 1440, intminute / 1440, intsecond / 1440\n\t\n\tminute = (second - int"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef parse_year_int(year):\n\t\"\"\"Return a year month by the given year: zero-padded if invalid.\"\"\"\n\tif year.startswith('j') and len(year) <= 2:\n\t\treturn datetime.timedelta(days=1)\n\telif year.startswith('j'):\n\t\treturn datetime.timedelta(days=int(year[1:]))\n\telif year.startswith('iv'):\n\t\treturn datetime.timedelta(days=int(year[1:3]),\n\t\t\t\tseconds=31 * int(year[3:]))\n\telif year.lower().startswith('iv"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef time_tuple_to_timedelta_with_unit(time_tuple):\n\t\"\"\"Convert a string representing the time at midnight, hours, minutes, and seconds, return\n\ta datetime.timedelta instance, or None if the frequency is not in the correct\n\ttime format.\n\n\tThis function is only relevant for non-leap years.\t\"\"\"\n\tdef dt_tuple_to_time_with_unit(datetime_tuple):\n\t\t\"\"\"Convert a date and a hour, minutes, seconds, to a time at midnight at midnight.\n\t\tThe datetime_tuple is assumed"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef time_is_less_than(a, b):\n\t\"\"\"Does this time_is_less_than_a_time_b(a, b)?\"\"\"\n\tif a > b:\n\t\treturn True\n\telif a <= b:\n\t\treturn False\n\telse:\n\t\treturn ValueError\n\ndef time2seconds(a_seconds, b_seconds):\n\t\"\"\"\n\tGet a Python datetime.timedelta or numeric value from seconds b_seconds.\n\t\"\"\"\t\t\n\tif type(b_seconds) in [int, float]:\n\t\t# Convert back to"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\n\n\n\"\"\"\nThe following code can be considered as a parser for each time element.\nNote: \n- the format is a standard python syntax for describing time intervals\n  (in python strings, like \"weeks:minutes:seconds\", \"1d\")\n- or the first argument if this flag is set to True, return datetime.timedelta(minutes=1)\n- or all following arguments if this flag is set to True, return\n  datetime.timedelta(seconds=1)\n\"\"\"\n\n\ndef parse_time(inp):\n\tif type(inp) is str:\n\t\treturn datetime.timedelta("}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\ndef int_or_None_if(string):\n\tif string:\n\t\ttry:\n\t\t\treturn int(string)\n\treturn None\n\ndef parse_frequency_string(string, freq_format=None):\n\tif string is None:\n\t\treturn None\n\tif freq_format:\n\t\ttry:\n\t\t\tfrequency_format = freq_format.lower()\n\t\t\tfrequency_format = frequency_format[0] == '+' and freq_format[1:] or frequency_format\n\n\t\texcept AttributeError:\n\t\t\treturn None\n\t\tstring = frequency_format_lower(string"}, {"generated_text": "import datetime\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.\t\"\"\"\n\t\nimport sys\n\nif sys.version_info < (3, 0):\n    from sets import Set as set\nelse:\n    from sets import Set as set    \n\nfrom datetime import datetime, timedelta\n\nclass Time(tzinfo):\n    \"\"\"Encapsulates a time object using a tzinfo instance.\"\"\"\n    def __init__(self, time=0, offset=None, name=None):\n        self.__offset = timedelta(seconds=-time)\n        self.__time = time\n        self.__name = name\n\n    def utcoffset(self, dt):\n        return self.__offset\n\n    def tzname(self, dt):\n       "}]}
{"task_id": "62ece4992e6aefcf4aabbd78", "prompt": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t", "suggestions": [{"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\nclass SSHConnectErrorException(Exception):\n\t\"\"\"\n\tThrown when there's a problem connecting to the SSH server\n\t(which happens when you receive a server answer from the user).\n\t\"\"\"\n\tpass\n\t\nclass SSHConnectException(Exception):\n\t\"\"\"\n\tThrown when there's a problem connecting to the SSH server\n\t\"\"\"\n\tpass\n\n\nclass SSHException(Exception):\n\t\"\"\"\n\tThrown when there's an error communicating with the server\n\tthat we are handling.\n\t\"\"\"\n\tpass\n\n\n#------------------------\ndef get_local_address"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\n\t\n\thost = re.sub(\":[0-9]{1,3}$\", \"\", host)\n\tif \":\" in host:\n\t\thost = host.split(\":\")[0]\n\treturn host.startswith(\"localhost\")\n\t#!BPY\n\"\"\"\n    Monster.py -- a base command line interface to Monster program objects.\n    Copyright (C) 2013  Yahoo! Inc.\n    Email:  yahoo<at>yhoo@zihao.com\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\tif \":\" in host:\n\t\t\thostname = socket.getfqdn(socket.gethostname())\n\t\t\tsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\t\tsock.connect((hostname, int(port)))\n\t\t\tdata = sock.recv(1024)\n\t\t\tif data==\"127.0.0.1\":\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\telse:\n\t\t\treturn False\n\ndef checkIP(host, port):\n\n\tprint(\"IP\",host, port)\n\t\n\tif \":\" in host:\n\t\thostname = socket.get"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\ndef is_localhost(host):\n\t\"\"\"\n\tChecks if the host is the localhost\n\tThe hostname is the localhost if\n\tHost is a fqdn or is IP address. This value may change if DNS is used\n\tor has a port and hostname in a non-FQDN format\n\t\"\"\"\n\t\n\ttry:\n\t\tsocket.gethostbyname(host)\n\texcept socket.gaierror:\n    \t\treturn False\n\texcept socket.herror:\n\t    return False\n\nif __name__ == \"__main__\":\n\n\t\"\"\"\n\tChecks if the output file name is exists\n\t\"\"\""}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\n\t\n\tif host.startswith(\"127.\") or host in (\"localhost\", \"127.0.0.1\"):\n\t\treturn True\n\telse:\n\t\treturn False\n\t\ndef read_lines():\n\t\"\"\"\n\tReads the contents of the file and returns a list of tuples\n\t\"\"\"\n\tlines = []\t\n\t\n\twith open(\"/etc/dnsmasq.conf\", \"r\") as cf:\n\t\tcontent = cf.readlines()\n\t\n\tfor line in content:\n\t\t(host,ip,port) = line.split(\":\")\n\t\t\n\t\tif ip == \"\":\n\t\t\tcontinue\n\t\t\n\t\t"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\n\t\n\ttry:\n\n\t\t#check for the port - will need to do this before using a socketio:\n\t\tsocketio_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) \n\t\tsocketio_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) \n\n\t\tsocketio_socket.connect((host, 7888))\t\n\t\t\n\t\tremote = socketio_socket.recv(8192)\n\t\tprint \"Connected to\", repr(remote)\n\t\ttime.sleep(0.002)\n\n\t\t"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\ndef ipv4():\n\t\"\"\"\n\tReads the name of all defined IPv4 addresses to the localhost\n\n:return: list of IPv4 addresses\n\t\"\"\"\n\treturn [line.strip() for line in file('hosts/hostnameonly/ipv4.txt')]\n\ndef ipv6():\n\t\"\"\"\n\tReads the name of all defined IPv6 addresses to the localhost, including\n\tthe localhost as a proxy domain name instead of the host name.\n\t\n\t\"\"\"\n\treturn ([line.strip() for line in file('hosts/hostnameonly/ipv6.txt')], \n\t\t\t[line"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\n\n\ndef start(host, timeout=1, verbose=False):\n\t\"\"\" \n\tSets up the socket and calls up the TCP server.  On failure returns a string with a\n\t'1','2','3','4',etc.\n\t\"\"\"\n\tsocket_file = '/tmp/pygubu.sock'\n\tif verbose: \n\t\tprint \"socket_file= \",socket_file\n\t\n\ts = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\ts.connect((host, 4444))\n\n\ttry:\n\t\ts.set"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t\nif __name__ == \"__main__\":\n\thostList = []\t\n\twhile True :\n\t\thostList = socket.gethostbyname_ex(socket.gethostname())[2] + \".\" + hostList\t\n\t\tprint hostList\n\t\tif (len(hostList) > 2):\n\t\t\tprint hostList.upper()\n\t\t\tprint hostList[1:]\n\t\t\tbreak\t\n\t\telif len(hostList) < 20: \n\t\t\thostList = self.getHost()\n\t\t\t\n\t\telse: \n\t\t\thostList=\"\"\n\t\t\t\n\t\tsleepNumber = 5\n\t\tfor i in range (0"}, {"generated_text": "import socket\t\"\"\"\n\tChecks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost\t\"\"\"\n\t \n\ttry:\n\t\tclient = socket.getservbyport(int(host), int(port))\n\t\tmessage = \"Connection to server failed\"\n\t\tlogger.info(message)\n\t\tclient.send(\"HTTP/1.1 200 OK\\r\\n\\r\\nPress enter to continue...\\r\\n\")\n\t\tclient.recv(1024)\n\texcept:\n\t\treturn False\n\treturn client\n\n\"\"\"\nThis function opens a TCP UDP socket\n\nIn all other cases (e.g. UDP connections are not allowed)\n\"\"\"\n\n\ndef soc(host"}]}
{"task_id": "62ece4992e6aefcf4aabbd79", "prompt": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\tsubpatterns = []\n\t\tdef recurse_patterns(path):\n\t\t\tfor pattern in path:\n\t\t\t\tif re.match(pattern, self.re):\n\t\t\t\t\tfor subpattern in subpatterns:\n\t\t\t\t\t\tsubpatterns.append(subpattern)\n\t\t\t\t\t\tbreak\n\t\t\t\telif re.match(os.path.join(pattern, '**/*.txt'), self.re):\n\t\t\t\t\tfor subpattern in recurse_patterns(path + '***'):\n\t\t\t\t\t\tsubpatterns.append(subpattern)\n\t\t\t\t\t\tbreak\n\t\t\t\telif re.match(os.path.join(pattern, '**/*/*.txt'), self"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\n\n\tdef _find(patterns, pattern_iter):\n\t\t# this works by searching path in various ways:\n\t\t# * a single glob pattern to pattern_iter, with a match for each glob pattern\n\t\t# * a list of all non-directories (e.g. [\"**\"]):\n\t\t# list of all file names (e.g. [\"foo.txt\"]). Will return the files\n\t\t#    from the given directory, or \"*\" if \"*\" is given, and \"\" if \"*\" is not passed\n\t\tfiles = [p for p in pattern_iter if re.search(p, p"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\n    \ndef strip_root_from_file(root, file):\n    regex_files = glob.glob(root + '/*.patterns') # regexs of files on which globbing path changes\n\tfile_lines = str(\"    \".join(filter(None, regex_files))) # remove empty lines at start\n\tif len(file_lines) < 14:\n#\t\traise ValueError, \"too small\"\n#\t\treturn\n\tdir_match = re.match(r'^\\w+$', file_lines)\n\tif dir_match:\n\t    file = file_lines["}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\tdef _find(root, subdir):\n\t\t\tif subdir in ['/', '\\\\']:\n\t\t\t\tglob = os.path.join(root, subdir) + '*'\n\t\t\t\tnonglob = re.escape(glob)\n\t\t\t\treturn re.compile(nonglob, re.I)\n\t\t\telse:\n\t\t\t\treturn os.path.normpath(root)\n\t\t\treturn glob\n\n\t\tpaths = _find('.')\n\n\t\tif len(paths) < 1:\n\t\t\traise SafeException(_('No matches found in %s') % basedir)\n\n\t\tpath = paths[0]\n\t\tpaths = map(lambda p:"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\nclass PathTransform:\n\tdef __init__(self):\n\t\tself.found = {}\t\t# The list of all path fragments that were matched\n\t\tself.path   = []\t\t# List of paths whose contents were found in patterns\n\tdef find(self, path):\n\t\tif path.endswith('/'):\n\t\t\tpath += 'index'\n\t\tlpath = path.split('/')\n\t\tfor p in lpath[:-1]:\n\t\t\tif p in self.path:\n\t\t\t\tself.found[p] = True\n\t\t\t\tself.path.remove(p)\n\t\tif not self"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\tif len(paths):\n\t\t\tpaths = list(paths) + [paths[0]]\n\treturn \"\\n\".join(map(lambda p: \"{}/{}: {}\".format(re.escape(p), pattern), paths))\n\ndef find_all_paths(pattern):\n\t\"\"\"Return map from any input path matches.\n\tThis assumes that the path starts with `/`.\n\t\"\"\"\n\tpatterns = re.findall(pattern, glob.glob(pattern + \"/.*?\"))\n\treturn {x[0]:x[2] for x in patterns if len(x[0])"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\ndef print_path(path_str, out=None, prefix='-'):\n\t\"\"\"\n\tFor debugging, this function takes `prefix` as a string, and\n\tprints the path as a string to stderr instead of stdout.\n\n\tIf the output stream is not a terminal, the contents of the file is\n\tread.\n\n\t`prefix` is only used for filtering out paths, and is applied to\n\tsubdomains (excluding the path itself).\n\t\"\"\"\n\tassert not out, \"Not running into a terminal (e.g. for testing).\"\n\tsys.stdout."}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\n\n\ndef find_dirs(pat):\n\t\"\"\"Generator for all the directory patterns in `pat`\"\"\"\n\tdirs = []\n\tdir_re = re.compile(pat)\n\tfor i, root_dir, dir_names, file_names in os.walk(pat):\n\t\tif i == 0:\n\t\t\tdel dir_names[:]\n\t\telse:\n\t\t\tdir_names[:] = list(dir_names)\n\t\tdirs.extend(file_names)\n\n\tdir_regex = \"^{0},.*\".format(pat.replace('*','.*')+\"*.*\");"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\tif pattern_type!= \"glob\":\n\t\t\tpath_re = []\n\t\t\tfor path_pattern in pattern_paths:\n\t\t\t\tpath_re.append(\"/%s\" % path_pattern)\n\t\t\tpattern_re = \"|\".join(path_re)\n\t\t\tpaths = glob.glob(pattern_re)\n\t\t\tpaths.sort()\n\t\t\tnames = {path.split(\"/\")[-1].rstrip(\"/\"):path for path in paths}\n\t\t\tnames.sort(key = len, reverse=True)\n\t\t\tnew_names = [name for name in names if name.startswith(\"**"}, {"generated_text": "import re\t\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\t\"\"\"\n\t\ttry:\n\t\t\tpattern = re.compile(pattern)\n\t\t\tpaths = [''+p for p in paths]\n\t\t\treturn [i for i in paths if not pattern.match(i)]\n\t\texcept TypeError:\n\t\t\treturn paths\n\ttry:\n\t\tpattern = re.compile(pattern)\n\t\tpaths = [''+p for p in paths]\n\t\treturn [i for i in paths if pattern.match(i)]\n\texcept TypeError:\n\t\treturn paths\t\n\ndef get_subdirs(config, subpath):\n\t\"\"\"\n\tGiven a subdirectory of a config object,"}]}
{"task_id": "62ece4992e6aefcf4aabbd7a", "prompt": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\n#--------------------------------------#\n# Main program\n#--------------------------------------#    \nif __name__ == '__main__':\n\n    ##############################################\n    # check if git has already been run\n    # --------------------------------------------#\n    \n    if not os.path.isfile((path+'/bin/git') and os.path.isfile((path+'/bin/git.exe')):\n        print('Error: git not found!')\n        exit(1)\n    \n    ##############################################\n    # check is git already up-to-date\n    # --------------------------------------------#\n    \n    if (os.path.isfile((path+'/bin/git.exe')) and \n        # check for git\n       "}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\n\n\n# ------------ Main ------------\nif __name__ == \"__main__\":\n\tif len(sys.argv) > 1 and sys.argv[1] == \"-git\":\n\t\tgit()\n\t\tsys.exit()\n\telse:\n\t\tsys.exit()\n\n# ------------ Main ------------\n\n\n## Entry point\nif __name__==\"__main__\":\n\tmain()\n\"\"\"\nThis module is an extension to the B-factor service in M-Lab\n\n\"\"\"\nimport numpy as np\nimport astropy.constants as const\n\nimport mlab\n\n\ndef b"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\treturn True and \\\n\t\t(os.name == 'nt' or (os.name == 'cygwin'))\n\n# --------------------------- \tFunctions (and classes) for handling user input ----\n\n# ---------------------------------------------------\n# General functions\n# ---------------------------------------------------\ndef set_to_home_directory(new_directory):\n\t\"\"\"\n\tset to be called with the directory to be used by program\n\t\"\"\"\n\t\n\tif new_directory=='.':\n\t\tnew_directory=getcwd()\n\t\n\treturn new_directory\n\t\ndef get_file_name(file):\n\t\"\"\"\n\treturns"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\n@contextmanager\ndef GitBash(path_to_git):\n\t\"\"\"\n\tgit bring up the requested git\n\t\"\"\"\n\tassert os.path.isfile(path_to_git)\n\n\timport subprocess\n\tpid = subprocess.Popen([path_to_git, \"cmd\", \"check-ref HEAD\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False)\n\tpid.wait()\n\tout, err = pid.communicate()\n\tpid = None\n\n\tif out:\n\t\tprint(\"git bring up: %s\" % ("}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\n\t\n\tdef getGitName(self, commit_id):\n\t\t\"\"\" getGitName\n\t\tcreates: path in git\n\t\tauthor: github\n\t\tparameters: commit id\n\t\treturns: path to the git\n\t\t\"\"\"\n\t\treturn subprocess.check_output([\n\t\t\t\"git\", \"rev-parse\", \"--short\", \"HEAD\",  commit_id])\n\t\n\t'''\n\tdeletes the last commit for this branch. \n\tdeletes the revision of this branch with the highest commit number \n    from the head.\n\t'''\t\n\tdef removeGitName(self, commit_id):\n\t\t"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\ndef checkOS():\n\t\"\"\"\n\tChecks the OS\n\n:return: True if OS is a supported platform\n\n:rtype: bool\n\"\"\"\n\tif platformcheck() == 'windows':\n\t\treturn True\n\telif platformcheck() == 'cygwin':\n\t\treturn True\n\t\t\"\"\"\n\telse:\n\t\treturn False\n\ndef systemVersion():\n\t\"\"\"\n\tChecks that Linux is on\"\"\"\n\tif platformcheck() == 'windows' or platformcheck() == 'cygwin':\n\t\treturn True\n\telif platformcheck() == 'linux2':\n\t\treturn True\n\t\t"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\ndef check_repo_type(repo_type):\n\t\"\"\"\n\tChecks whether this is a git repo, ie: \n\tgit, gitdir, or mvn\n\t\"\"\"\n\tif \"git\" in repo_type:\n\t\treturn True\n\telif \"gitdir\" in repo_type:\n\t\treturn True\n\t\ndef get_git_path(repo_dir, is_git_symlink = False):\n\t\"\"\"\n\tThis function assumes that an 'git symlink', is in the working directory\n\tis_git_symlink, and that there are both\n\tfiles:"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\ndef get_directory_from2(path):\n\t\"\"\"finds (and corrects path from 3.2) the first entry from 3.0 to end\n\tbut returns the path with most files and folders with the same name\n\treturn path without directories\"\"\"\n\n\tpath_list= []\n\n\t# first test the parent 2 folder \n\tfind_parents=os.walk(path)\n\tif len(find_parents)>1:\n\t\tfor dir in find_parents[1]:\n\t\t\tif dir[-5:-3]!='_copy':\n\t\t\t\tpath_list.append("}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\ndef get_path():\n\tif ('Windows.git' in os.path.basename(os.path.expanduser('~'))):\n\t\treturn os.path.dirname(os.path.expanduser('~'))\n\telse:\n\t\treturn \"\"\n\ndef remove_path_quotes(path):\n\t\"\"\"\n\tRemove quotes, double slashes, etc.\n\tinput: path - String with which to remove quotes\n\toutput: Nothing\n\t\"\"\"\n\n\tif(len(path)>2):\n\t\tpath=path[0:2]\t\n\n\tif(path[0]==\"'\")"}, {"generated_text": "import os\t\"\"\"\n\treturns True if you run in a Windows gitbash\n\n:return: True if gitbash\t\"\"\"\n\t\ndef createTarball(tarDir, dir):\n\t\"\"\"\n\t@note: This function has no side effects (like on Windows, you can't rely on shutil.copytree, but I removed it)\n\t\"\"\"\n\tcmd = \"tar -czf \"%s\" \"%s\" %s\"%(tarDir,zipDir,dir)\n\tos.system(cmd)\n\treturn True\n\t\t\ndef moveIntoTarball(folder):\n\t\"\"\"\n\t@note: This function has no side effects (like on Windows, you can't rely on shutil.move)\n\t\"\"\"\n\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd7b", "prompt": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\ndef create_config_file_from_yaml(config=None, filename=None, overwrite=False):\n\tconf = None\n\tif filename:\n\t\tif os.path.exists(filename):\n\t\t\tconf = yaml.safe_load(open(filename, 'r'))\n\t\telse:\n\t\t\tstderr.write('Error: file'+ filename +'not found\\n')\n\t\t\texit(1)\n\telse:\n\t\t# Try the config file specified and use the global one\n\t\tconfig_filename = os.path.expanduser(args.config)\n\t\tif os"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\tif os.path.isdir(sourceconfig):\n\t\t\tself.printError(\"ERROR: File is a directory\")\n\t\t\treturn False\n\n\t\tfile = codecs.open(sourceconfig, \"r\", \"utf-8\")\n\t\tsourcedata = file.read()\n\t\tfile.close()\n\t\tfile = open(sourcefile,\"w+\", encoding=\"UTF-8\")\n\t\tsourcedata = string.replace(sourcefile, \"\\r\",\"\\n\")\n\t\tsourcedata = sourcedata.expandtabs(8)\n\t\tfile.write(sourcedata)\n\t\tfile.close()\n\t\tself.print"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\n\t\n\tprint(color_text())\n\tprint(\"Generating '{}'...\".format(directory), end=\"\\t\")\t\n\tif os.path.exists(directory):\n\t\t# Check if directory and write it if it does not\n\t\tif not ask(\"Overwrite Existing Directory?\", \"Yes\", \"Yes\") and overwrite:\n\t\t\tif not ask(\"Overwrite Existing Folder?\", \"Yes\", \"Yes\"):\n\t\t\t\tos.mkdir(directory)\n\t\t\t\tos.chmod(directory, 0755)\n\t\t\telse:\n\t\t\t\tprint(\"Overwrite Existing Directory already exists and overwrite is false. Re"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t \n\t# Parse the target config file.\n\tconfig_template = load_config(filename)\n\ttry:\n\t\tconfig = json.loads(config_template)\n\texcept AssertionError as exc:\n\t\tmsg = \"Error reading: {}\"\n\t\tmlog.fatal(msg.format(filename))\n\t\traise exceptions.ConfigurationError(msg.format(filename))\n\n\t# Validate the data.\n\ttry:\n\t\tif 'data' not in config.keys():\n\t\t\traise exceptions.KeyError(\"No valid data provided for config.\")\n\t\t\n\t\treturn config\t\n\texcept (KeyboardInterrupt"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\ttmpfilename=self.get_full_filepath(filename);\n\t\tif not os.path.isdir(tmpfilename):\n\t\t\tself.make_dir(os.path.dirname(tmpfilename))\t# make base directory\n\t\t\tself.make_dir(os.path.dirname(filename))\t# make the filename, make directory path relative to out\n\t\t\tself.write_to_file(tmpfilename, self)\n\n\tdef write_to_file(self, filename, node=None):\n\t\t\"\"\"\n\t\tWrite the config file to the given file path\n\t\tTo see what"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\ndef copy_if_different(newfile, dest):\n\tif not os.path.exists(dest):\n\t\treturn\n\ttarget_file = os.path.join(dest, os.path.basename(newfile))\n\tif os.path.exists(target_file) and filecmp.cmp(target_file, newfile):\n\t\treturn\n\tshutil.copyfile(src_file, dest)\n\t\ndef write(f):\n\twith open(f, 'w') as f:\n\t\tf.write(str(os.getpid()))\t\t\n\t\tif"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\n\n    \tos.mkdir(configPath)\n       \n    \tif not os.path.exists(configPath):\n                os.mkdir(configPath)\n       \n\nclass DefaultConfigHandler (): #TODO -- should do something useful?\n\tdef __init__ (self, sourceDirs, templatePath, override, filename, defaultConfigPaths={}):\n\t\tself.sourceDirs = sourceDirs\n\t\tself.templatePath = templatePath\n\t\tself.fileName = filename\n\t\tself.defaultConfigPaths = defaultConfigPaths\n\t\tif len(filename) > 20:\n\t\t\tself.targetDirsAndSubDirnames = zip"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t \n\tif not os.path.exists(target_config_path):\n\t\tos.makedirs(target_config_path)\n\ttemp_list = []\n\tindex = 0\n\ttarget_name = os.path.basename(target_config_path)\n\twhile True:\n\t\ttarget_file = target_config_path + '-' + str(index)+ \".conf\"\n\t\tif os.path.exists(target_file):\n\t\t\tif not overwrite:\n\t\t\t\t#print \"\"\n\t\t\t\ttemp_list.append(target_file)\n\t\t\telse:\n\t\t\t\tprint \""}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\tif not os.path.isfile(filename): # file doesn't exist yet, create it\n\t\t\tprint \"create \" + filename + \"...\",\n\n\t\t\t# Create directory\n\t\t\tos.makedirs( os.path.split(filename)[0] )\n\n\t\t\tfilename_temp = filename + \"_\" + strftime(\"%Y%m%d%H%M%S\", localtime()) + \".yaml\"\n\t\t\tfilename_yaml = filename + \"_\" + strftime(\"%Y%m%d%H%M%S\", localtime()) + \".yml\"\n\t\t\t\n\t\t\t# Dump to file\n\t\t\twith open(filename"}, {"generated_text": "import os\t\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.\t\"\"\"\n\t\ndef getTargets():\n\t\"\"\"\n\tFind and load the targets for this version of the module. \n\t\"\"\"\n\tglobal targets, env, source_data, config_data, template, out\n\tif os.path.isfile(target):\n\t\ttarget_data = target_data_from(target)\n\t\tif not target_data:\n\t\t\treturn\n\t\ttargets.append(target_data['target'])\n\telif target_data['template']:\n\t\ttemplate = target_data['template'];\n\t\tin_file = os.path.normpath(os.path.join"}]}
{"task_id": "62ece4992e6aefcf4aabbd7c", "prompt": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t", "suggestions": [{"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\treturn script.split('\\n')\n\t# return string\n\ndef gen_cmdline(script, input):\n\t\"\"\"\n\tGenerate the script\n\t\"\"\"\n\t# add script header\n\tscript_header = \"\n\tscript_header += 'cd \"$HOME\" && pwd'\n\tscript_header +='' + script\n\n\t# add input to script\n\tfor line in textwrap.wrap(input, 40):\n\t\tscript_header = script_header + line + \"\\n\"\n\n\t# replace script line in script\n\tfor line in script.split('\\n'):\n\t"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\ndef run_command(script):\n\t\"\"\"\n\tRuns a command and executes.\n\n:code:\n\t\"\"\"\n\treturn subprocess.check_call(\n\t\t[script],\n\t\tshell=False,\n\t\tstderr=subprocess.STDOUT,\n\t\tuniversal_newlines=True\n\t)\n\n## Run a simple command, without shell, but with additional handling.\n#TODO: Is this a safe way?\ndef run_command_with_shell(script):\n\t\"\"\"\n\tRuns a single command with shell and a single line script.\n\n\t:param script:\n\t"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\nclass ThermothlessSlack(object):\n\t\n\tdef __init__(self, script):\n\t\t\"\"\"\t\n\t\tThermothlessSlack constructor. \n\t\t\n\t\t:param script:\n\t\t\"\"\"\n\t\t# Initialize trough the client if it isn't set\n\t\tif not script:\n\t\t\treturn\n\t\tself.state = self.say\n\t\tself.scrib = True\n\t\ttry:       \n\t\t\tself.slack_client = SlackClient(script)\n\t\texcept Exception:\n\t\t\tself.slack_client = None\n\t\t\n\n\tdef go(self, s"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\ndef print_bash ():\n\t\"\"\"\n\tprints the script to be executed at the bash prompt.\nThis is useful when you want to do anything with the bash command in a shell.\n\n:return:\n\n\t\"\"\"\n\tprint textwrap.fill ( textwrap.fill ( command ))\n\t\n\tbash_command = \"\"\n\tfor line in open(\"bash\"):\n\t\tbash_command = bash_command + line\n\tprint subprocess.check_output ( bash_command, shell=True  )\n\n\ndef run_command_line_script ():\n\t\"\"\"\n\truns a single ssh command"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\twith open(script, 'r') as f:\n\t\t\t#with open(name,mode='r') as fh:\n\t\t\tcommand = ''\n\t\t\twhile 1:\n\t\t\t\tline = f.readline()\n\t\t\t\tif line == '':\n\t\t\t\t\tbreak\n\t\t\t\tcommand+= line.replace('\\n', '')\t\t#remove trailing line break\n\t\t\t\tif line[0] in ['&', '{']:\n\t\t\t\t\tcommand += line[0]\n\t\t\t#print \"command\", command\n\t\t\tprint command\n\t\t\toutput = subprocess.Popen(command, shell=True)\n\t\t\toutput.wait()\n\t\treturn output"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\n#  print 'writing command:'+ script\ndef write_cmds(script):\n\t\n\tf = open(script[0],'r')\n\t#line = script[0].split('\\n')\n\tnext = f.readline()\n\tline = f.readline()\n\n\twhile next!= '':\n\t\tnext = f.readline()  #skip line \n\t\tif next!= '':\n\t\t\tnext = 'if'+ next\n\t\tnext = next.replace(\"\\t\",\"|\")\n\t\tprint 'writing'+ next\n\t\tf.write('\\nscript'+ next + '\\n')"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\treturn subprocess.check_output(\n\t\t\t\t\tscript, shell=True).decode(\"utf8\")\n\ndef remove_comments(file_lines, output_file):\n\t\"\"\"\n\tremoves line comments from a file while keeping comments simple.\n\t:param file_lines: String or iterable of strings representing the input file's lines (default: \"#\").\n\t:return:\t\"\"\"\n\toutput_file.seek(0)\n\toutput_lines = output_file.readlines()\n\tfor i in range(len(file_lines)):\n\t\tline_comments = file_lines"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\n\t\n\tfor line in script.splitlines():\n\t\tif line.startswith(\"echo \") and line.count(\";\") > 1:\n\t\t\tprint(\"{0} >\".format(line))\n\t\t\tbreak\n\t\telif line.startswith(\"--\") and line.count(\"--\") < 2:\n\t\t\tprint(\"{0} <\".format(line))\n\t\t\tcontinue\n\t\telif line.isdigit() and line.count(\",\") > 1:\n\t\t\tprint(\"{0} {: <10} {: <10} {: <10} {: <10} {: <10}\\n\".format(line,int(line[1]),int"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\tdef _func(file_):\n\t\t\timport StringIO\n\t\t\tprint_s = StringIO.StringIO()\n\t\t\tfile_.write(file_.read())\n\t\t\tsys.stdout.write(print_s.read())\n\n\t\t\tif len(file_.name) > 0:\n\t\t\t\tsys.stdout.write('script')\n\t\t\t\tfile_.write('\\n')\n\t\tfunc()\n\n\ndef shell(script):\n\t\"\"\"\n\tShell command\n\t\"\"\"\n\n\tos.system(bash(script))\n\n\ndef shell_print(script):\n\t\"\"\"\n\tThis function prints a shell command for the"}, {"generated_text": "import textwrap\t\"\"\"\n\tconverts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:\t\"\"\"\n\t\tsys.stdout.write(str(script))\n\tsys.exit()\n\ndef isVIP(command):\t\n\tfor line in command.split(\",\"):\n\t\tif line.startswith(\"-\"):\n\t\t\tline = line.replace(\"-\",\"\")\n\t\telse:\n\t\t\tline = line.replace(\" \",\"\") \n\t\tif line.count('.')>2 and line.find('(')>0:\n\t\t\tline=line[:line.find('(')]\n\t\tif line.count(',')>0 and len(line)==0:\n\t\t\tline=line.replace(',','')\n\t\twhile line"}]}
{"task_id": "62ece4992e6aefcf4aabbd7d", "prompt": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\n\t\n\tcmdline =''.join(sys.argv)\n\t\n\tp = subprocess.Popen([func] + tuple(args), **extra_env)\n\twhile p.poll()!= None:\n\t\tline = p.stderr.readline()\n\n\t\tif len(line) == 0:\n\t\t\tp.stderr.readline()\n\t\t\tcontinue\n\n\t\tif line[0:9] == ':WARNING:':\n\t\t\tcontinue\n\n\t\tline = line.rstrip()\n\t\tline_split = line.split(' ')\n\t\tname = line_split[0]\n\t\toutput = line_split[2]"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\n      \ndef main(func=subprocess.run, extra_env={}):\n\t\n    \"\"\"\n    Execute a Python script from a shell as if it were a script.\n    \"\"\"\n\n    start = time.time()\n\n    # Get the arguments supplied at the terminal\n    args = [func] + list(extra_env.keys()) + [str(start)]\n\n    process = subprocess.Popen(args=args,\n                                stdin=subprocess.PIPE,\n                                stdout=subprocess.PIPE,\n                                stderr=subprocess.PIPE)\n\n    out_str, err_str = process.communicate()\n\n    end = time.time()\n\n    print(\"Execution"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\ndef start_subproc(func, args=None, extra_env=None):\n\t\"\"\"\n\tStart a subprocess sub-process, while keeping it running.\n\t\n\tParameters\n\t----------\n\tfunc : function\n    The function to be run.  It must be in a module that is importable.\n\targs : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.Popen``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\ndef check_call(*args,**kwargs):\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\ndef run(func, args, env=None, shell=True, check_status=True):\n    extra_env = extra_env or {}\n    if env is None:\n        env = os.environ.copy()\n    if 'TERM' not in env:\n        env['TERM'] = 'dumb'\n    return subprocess.run(\n        [os.path.join(dirname(__file__), 'bower.py'), func] + args,\n        stdout=subprocess.PIPE,\n        stderr=STDOUT,\n        check=check_status,\n        env=env,\n        **extra_env)\n\n\ndef run"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\ndef callproc(*args, **kwargs):\n\t\"\"\"Run the function in a subprocess.\n\t\n\t*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\n\t\n\tReturns\n\tsubprocess.Popen\n\t \"\"\"\n\treturn subprocess.run(*args, **kwargs)\n\ndef _run(f, *args, **kwargs):\n\t\"\"\"Run a function in a subprocess.\"\"\"\n\tsubprocess.run(f, *args, **kwargs)\n\ndef _run_call(cmd):\n\t\"\"\"Run a command with the supplied ``cmd``"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\ndef check_for_output(cmd, stdout=subprocess.PIPE):\t\n\tfor line in process_output(cmd, stdout=stdout, shell=True.5):\n\t\tprint line.rstrip()\n\tproc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\tout, err = proc.communicate()\n\tif proc.wait()!= 0:\n\t\traise IOError('Command %s returned non-zero status %d in directory `%s`.'%\\\n\t\t\t(cmd, proc.returncode, os.getcwd()))\n\n"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\nclass sub_process(subprocess.Popen):\n\t\"\"\"\n\tSubstitute or run the process.\n\nParameters\n----------\ncmd : str or unicode\n    The command to be run.  To run a shell command string or a callable\n    object, must be in the form ``<cmd> = <args>``.\ncwd : str\n    The current working directory. It must be passed in using the Python\n    ``os.chdir`` syntax ``'.'}, ie:\n\n    >>> import tempfile\n    >>> from os import getcwd\n    >>> child = getcwd()\n    >>> with open(os.path.join(tempdir, '"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\n\t\n\trun_command = \"\"\n\tif 'env' in extra_env:\n\t\trun_command =''.join([\n\t\t\textra_env['env'],\n\t\t\t' '.join(['-e', env_var])\n\t\t])\n\t\n\tif subprocess.run(run_command, **args) == 0:\n\t\tproc = subprocess.Popen(['run_test_with_signal.sh'], shell=True, cwd=BASE_DIRECTORY)\n\t\tproc.wait()\n\t\tprint(subprocess.getstatusoutput(proc))\n\n'''def test_command_not_found(func):"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\n\n\tprint \"Call %s... \" % os.getpid()\n\tproc = subprocess.Popen(args=args, **extra_env)\n\ttry:\n\t\tproc_out, proc_err = proc.communicate()\n\t\tmsg = (\"Running '%s' took: %s\" % (args, proc_out))\n\t\ttry:\n\t\t\tprint msg\n\t\texcept UnicodeEncodeError:\n\t\t\tprint ('UnicodeEncodeError on Windows.')\n\t\tif proc_err:\n\t\t\tprint (\"Exception raised with error code %d\" % proc_err)\n\t\t\traise SafeException(msg)\n\texcept"}, {"generated_text": "import subprocess\t\"\"\"\n\tRun a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.\t\"\"\"\n\t\nfrom argparse import ArgumentParser\nfrom argparse import RawTextHelpFormatter\n\nclass CLIError(Exception): pass\n\ndef run_process(func, *args, **kwargs):\n\t\"\"\"Run the function in a subprocess.  Returns an integer value.\"\"\"\n\ttry:\n\t\treturn func(*args, **kwargs)\n\texcept Exception as e:\n\t\traise CLIError(e, \"run_process\")\n\t\nRUN_OPTIONS = [\n  '-i', 'port',\n  '--config',      None,\n  '-q', '-v',    '-c', \n  '-d', '--debug',    '-d"}]}
{"task_id": "62ece4992e6aefcf4aabbd7e", "prompt": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\n    \tenvName    = os.environ.get('YOP_ENV_VAR_NAME')\n    \tif envName == None:\n    \t\traise Error, \"Environment variable not defined\"\n        \n        match       = re.search(regex_match, envName)\n        if match:\n    \t\tdefaultVal    = match.group(1)\n    \t\tif defaultVal == \"\":\n    \t\tdefaultVal = \"\"\n    \t\n\t\tprint 'Setting values for'+ envName +'='+ str(defaultVal)\n\t\t\n\t\tif match:\n    \t\t\treturn match.group(1)\n\t\telse:\n    \t\t\t"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\ndef GetEnvironment():\n\t\"\"\"\n\tGet environment from user\n\t\"\"\"\n\ti_var = 0\n\tif not os.path.isfile(config_file):\t#Check file existence\n\t\tprint ('Error: No config file. Check if you specified the environment filename.' )\n\t\tprint ('Please set the file or create one.')\n\t\treturn False\n\tfor line in open(config_file):\t\t\t\n\t\ti_var = i_var + 1\n\t\tif i_var == len(line):\n\t\t\tprint ('ERROR! the file \"'+config_file+'\" does"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\ttry:\n\t\t\tenv = os.environ[var]\n\t\texcept KeyError:\n\t\t\traise Error(\"Environment variable '%s' was not set.\" % (var))\n\t\texcept:\n\t\t\traise Error(\"Could not read configuration variable '%s'.\")\t\n\t\tself.env_var = env\t\n\t\ttry:\n\t\t\tif var in self.env_default_values:\n\t\t\t\tself.env_default_values[var] = self.env_var\t\n\t\t\telse:\t\n\t\t\t\t# TODO: add warning to logger, see #3697.\n\t\texcept:\t\n\t\t\tpass"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\tglobal _default_value\t\n\t\tdefault_value = _default_value\n\t\ttry:\n\t\t\tdefault_value = _get_env_value() \n\t\texcept AttributeError:\n\t\t\tif default_value == False:\n\t\t\t\traise Exception(\"Environment variable %s does not exist. Check your \\\"Variables\\\" variable.\" % (_variable))\n\t\t\telse:\n\t\t\t\traise Exception(\"Environment variable %s does not exist. Check your \\\"Variables\\\" variable.\" %(_variable))\n\n\t\treturn default_value\n\telif environment == \"\":\n\t\tif _parameter_value == \"\":\n\t\t\traise Exception(\"Environment variable"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\tenv_var_re = re.compile(r'''(?:(?P<name>(?:[^/]|#)[^/])).*(?P<default>.*)''')\n\t\tenv_regex = re.compile(r'^\\s*(?P<default>.*)$')\n\t\tif not env_var_re.match(env_var):\n\t\t\traise Exception('Variable %s does not contain a variable named %s' % (env_var, env_var_re.pattern))\n\t\t# search for default\n\t\tvar_def = os.getenv(env_var_re.pattern % env_var"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\n\n\tenv\t\t= Environment()\n\tdefault = \"$$$\"\n\tif key in env:\n\t\ttry:\n\t\t \tvalue = env.get(key)\n\t\t\tif value: \n\t\t\t\treturn value\n\t\t\telse: \n\t\t\t\treturn default\n\t\texcept:\n\t\t\tif default: \n\t\t\t\treturn default\n\t\t\telse:\n\t\t\t\traise e\n\n\t\n\treturn default\n\t\t\ndef readConfFile(section, name):\n    \"\"\"Read a configuration file to read and interpolate the environment variables.\"\"\"\n\n    # Check the presence of this name in the environment variable we're in\n    if not name in os.environ"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\n_module_logger = logging.getLogger(\"_module_logger\")\n_module_console_handler = sys.stderr \n_module_console_handler.write = os.write\n_module_console_handler.flush = sys.stdout.flush\n_logging_enabled = (logging.DEBUG >= logging.DEBUG_COLLECTABLE)\t\ndef get_module_logger():\n\t\"\"\"\n\tReturn a module logger instance\n\t\"\"\"\n\treturn _module_logger\n\ndef get_module_console_handler():\n\t\"\"\"\n\tReturn a module ivaro instance\n\t\"\"\"\n"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\tenv={}\n\t\tenvname='TEXINPUTS'\n\t\tp=os.environ\n\t\tif p.has_key(envname):\n                        env[envname]=p[envname]\n                        del(p[envname])\n                else:\n                        raise Error('Environment variable '+environmentname+'must be defined in config file or in ~/.tshrc')\n\telse:\n\t\treturn config[sys.argv[2]]\n\n# ----------------------------------------------------- The main program starts here!\n# ----------------------------------------------------- Do not change anything below this line\n# ----------------------------------------------------- Do not change anything below this line\n\nsys"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\n\t\t\n\t\t_filename=''.join(\"_\".join(v.split('_')[:-1]) for v in os.environ.keys()) #Filename\n\t\t_config_dict \t\t= {}\n\t\t_key_config_list\t\t= []\n\t\t_value_config_list\t= []\n\t\t\n\t\t_filename_regex\t= re.compile(r\"(\\A|nmea|rj|e|o|m)?_(?P<v1>(\\d+(\\.\\d*)?|_?\\d+)[_a-z].*)?\")        #Value format regex\n\t\t_value_"}, {"generated_text": "import os\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.\t\"\"\"\n\t\tcommand=\"echo -e '[[[$]]' | egrep -v '[[$]]' | egrep -v '[[$]]' | egrep -v '[[$]]' | egrep -v '[[$]]' > /etc/os-release || echo $?'\"\t\n\t\tres=os.popen(command, \"r\")\n\t\tfor line in res.readlines():\n\t\t\tif line.find(\"=\") == -1:\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tfor val in defaults:\t\t\t\n\t\t\t\t\tif val==\"1\":\n\t\t\t\t\t\tret=True\n\t\t\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd7f", "prompt": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t", "suggestions": [{"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\n\n\tm = re.search(\"(http|https):\\/\\/(?:static|img|js))\", image_href)\t\n\tif not m:\n\t\t# Fallback\tAssume it exists\n\t\t#print \"Image href not found!\"\n\t\turlparts = urlparse(image_href) #urlparse(image_href) doesn't support 'filename'\n\n\telse:\n\t\t# Test the URI pattern\n\t\tp1 = urlparse(m.group('uri'))                  #uri = absolute( url, options )\n\t\tp2 = urlparse(p1.group('uri'))                  #uri"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\n\n\n### Main code, not used\n\ndef main():\n    \"\"\"Main function\"\"\"\n    opt_parser = optparse.OptionParser(usage='usage: %prog [options] path')\n\topt_parser.add_option('', '--help',\n\t\t\taction='store_true', dest='help',\n\t\t\thelp='Print this help message and exit')\n\topt_parser.add_option('-o', '--out',\n\t\t\taction='store', dest='out',\n\t\t\tdefault=os.path.join(os.path.expanduser(\"~\"), \".local\", \"img\", \"default\"),\n\t\t\thelp"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\ndef _parseImageURI(uri, defaultScheme=None):\n\t\"\"\"Parses an image href into a tuple of strings (image_id, netloc,\n\tuse_ssl)\n\t\"\"\"\n\tscheme, netloc, path, params, query, frag = urlparse.urlsplit(uri)\n\t# convert urlparse to urlparse.ParseResult, otherwise\n\t# urlparse.urlparse is too brain-deadly\n\tscheme = scheme.lower()\t\t# Normalize scheme\n\tnetloc = netloc.lower()\t\t\t# Normalize host\n\tpath = path.lower()\n\t\n\n\tif defaultScheme is"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\n\t\n\tparsed = urlparse.urlparse(image_href)\n\timage_id = urllib.unquote(parsed.path.split('/')[-1])\n\tnetloc = urllib.unquote(parsed.netloc)\n\tuse_ssl = (parsed.scheme == \"https\")\n\n\tparts = urlparse.urlparse(image_href)\n\tif parts.scheme!= \"http\":\n\t\traise ValueError(\"URL for image_href scheme must be 'http'.\")\n\treturn (image_id, netloc, use_ssl)\n\ndef replace_urls(text):\n\t\"\"\"\n\tReplaces the URLs of images"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\ndef _image_from(href):\n\t\"\"\"\n\tGiven an image href, return a tuple of an \"archive:\" url\n\n\tThe image will be downloaded from the provided URL.\n\n\t@param href: image href\n\t@type url:\tstring\n\t@rtype:\t\ttuple\n\t@return:\t\ta tuple of the form (url, path, fragment)\n\t\"\"\"\n\tassert not (hasattr(href,'read') and href.read,\n\t\t\t'file content not set')\n\tparsed = _urlparse.urlparse(href)\n\t\n\tif parsed.scheme"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\n# construct the path, query string and fragment part of the url\ndef split_image_id_auth(image_href):\t\n    _url = re.match('^(?:data:)?data:image/([\\w\\d]+)[^\\n]*', image_href)\n    if _url: \n        return tuple(_url.groups())\n        \n    # No url found. Return empty tuple\n    return '', '', ''\n\n\ndef split_volume_ref(image_href):\n\t\"\"\"\n\tUse \"href\", but ignore the extremities, return the parts of the path \n\tto the volume reference of"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\n# TODO: handle this better\ndef extract_images(html_string):\n\t\"\"\"\n\tExtract images from the HTML string by removing headers and images.\n\n\t:param html_string: html string\n\t:returns: the decoded html\n\t\"\"\"\n\treturn urllib.unquote(re.sub(r'<(?:img|embed).*?>', '', html_string))\n\ndef _parse_idtag(img_id, href=True):\n\t\"\"\"\n\tGiven the ID tag, extract the image id and the domain.\n\tIf the domain matches, it returns a tuple of the"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\n\n\tcomponents = urllib.parse.urlparse(image_href)\n\tnetloc = components.netloc\n\tuse_ssl = components.scheme == \"https\"\n\timage_id = urllib.request.url2pathname(image_href)\n\treturn (image_id, netloc, use_ssl)\n#!/usr/bin/env python\n\n# Copyright (c) 2018 Orange.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License."}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\ndef parse_image_links(image_hrefs):\n\t\"\"\"Parse a list of image hrefs to the url-encoded parts.\n\n:param image_hrefs: hrefs as a list of strings\n:returns: a tuple of (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t\ndef parse_images(data_string):\n\t\"\"\"Parse the data string and return an iterator \n\tthat returns individual string parts.\n\n\t>>> parse_images(\"http://foo\")\n\t('http://foo', ['foo'])\n\t>>> parse_images"}, {"generated_text": "import urllib\t\"\"\"\n\tParse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:\t\"\"\"\n\t \n\tm = re.match(r\"(https?://[^/]+\\..+):(\\d+)\", image_href);\n\tif m:\n\t\tnetloc = m.group(1);\n\telse:\n\t\traise ValueError, \"Not a parsed image \" + str(image_href)\n\tnetloc = netloc.partition(\"://\")[-1]\n\tuse_ssl = False\n\tif netloc.count(\":\") > 1:\n\t\tuse_ssl = True\n\tif use_ssl:\n\t\turlp = urllib.parse.urlparse(netloc);\n\t\turls = urlparse(urlp"}]}
{"task_id": "62ece4992e6aefcf4aabbd80", "prompt": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\n    if len(input_list)!= len(input_list[0]):\n\t\traise TypeError(\"Error: The list must contain the same number of entries as the list\")\n    for i in range(len(input_list[0])):\n\t\tif os.path.samefile(input_list[1][0], input_list[1][i]):\n\t\t\tpass\n\t\telif os.path.isdir(input_list[1][i]):\n\t\t\tpass\n\t\telse:\n\t\t\traise TypeError(\"Error: The list item must be either a single directory or a single file\")\n\t\t\tbreak"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\ndef cleanup_sep(file_list):\n\t\"\"\"\n\tRecursively remove all the trailing os seperator characters.\n\n\tArgs:\n\t\tfile_list: list of strings\n\t\"\"\"\n\tfor sep in [ os.path.sep + os.path.sep, \"\\t\", os.path.join(os.path.sep, \"*\") ]:\n\t\tfor index, file in enumerate(file_list):\n\t\t\tif sep == '\\\\': # '\\\\' for '\\'\n\t\t\t\tfile = file.replace(sep, \"\\\\\")\n\t\t\tfile= file.strip()\n\t\t\tfile_list["}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\n\t\n\t# Ensure an error occurred in reading more\n    file_input = open(input_path)\t    \t\t     # Create file_input object\n\twhile 1:\n\t\tline = file_input.readline()\n\t\tif not line: break\n\t\t\n\t\tline = line.strip()\n\t\tif len(line) < 1: continue\n\t\n\t\t# Split output\n\t\tsplit_list = line.split(\"\\\\\")\n\t\t# print split_list\n\t\tfor k in split_list:\n\t\t\tif k == \"\": continue\n\t\t\ttry:\n\t\t\t\tr_index = int(split_list[k"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\n    for i in range(0, len(input_list)):\n        if input_list[i][-1:]==os.sep and os.path.sep in input_list[i]:\n            return input_list[i]\n        else:\treturn input_list[i:]\n\ndef get_list_with_path(dir_path):\n    \"\"\"Extracts list of absolute filepaths.\n\n    arg:\n        dir_path: a string, path in config folder\n\n    returns:\n        list of strings, [relative path from dir_path]\n    \"\"\"\n    # Get directory from config folder \n    config_"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\n# This function is only here to get around Py3.\ntry:\n\tinput_list = [ord(c) for c in input_list]\nexcept TypeError:\n\tpass\n\texit()\n\nfilepath_in = input_list[0] # this is a pathname to string (no directory or file names)\nfilepath_str = input_list[1]\n\n# Check that the pathname that was given matches the list of strings.\n# We do this with file names. Do not check whether the pathname doesn't\n# include a drive, drive+drive, and file names"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\n\n\ndef is_str_list_or_str_no_error(input_list):\n\t\"\"\"\n\tCheck whether a string is a valid list or a string no errors are thrown\n\twith a meaningful error message.\n\t\"\"\"\n\t# Check that len(input_list) is a valid list of strings\n\tfor i in input_list:\n\t\tif not is_string_list(i):\n\t\t\traise TypeError('Invalid input: ', i)\t\t\n\treturn input_list\t\t\n\n\ndef check_is_str_list_or_str_no_error("}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\n\t\t\ndef clean(input_list):\n\tif len(input_list) == 0:\n\t\traise TypeError(\"Input must not be empty\")\n\t\t\n\tinput_list = map(str.strip, input_list)\n\n\t\n\t# remove trailing os-seperator characters\n\toutput_list = []\t\n\tfor eachChar in input_list:\t\n\t\tif not eachChar in [' ', None]\t# do not add ''\n\t\toutput_list = output_list + eachChar.split()\n\n\t# remove trailing '\\'\n\toutput_list = filter(bool, output"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\ndef get_string(input_list):\n    \"\"\"\n    Returns a string consisting of only strings\n\t\"\"\"\n    new_input_list = []\n    for input_str in input_list:\n        if os.path.isdir(input_str):\n            for name in os.listdir(input_str):\n                if os.path.splitext(name)[-1]:\n                    new_input_list.append(os.path.join(os.pardir, input_str, name))\n        else:\n            new_input_list.append(input_str)\n    \n    if len(new_input_list):"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\n\n\ndef list_to_string(input_list):\n    \"\"\"Returns a converted list of integers, i.e. strings from the strings in the\n    given list.\n\n    Args:\n        input_list: list of integers\n\t\"\"\"\n\n    return [str(int(i)) for i in input_list]\n\ndef split_path(input_path):\n    \"\"\"If the input path ends with a path separator, splits it up into parts seperated\n    seperated by empty elements by ':'.\n\n    Args:\n        input_path: path to split apart\n\t\"\"\"\n\n    input_list = input_"}, {"generated_text": "import os\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\t\"\"\"\n\t\ndef RemoveString(input_list, output_list, remove):\n\t\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\n\tFirst, remove from output list after finding delimeter character or directory.\n\tIf so, the pathname seperator character is removed. If not, the\n\tpathname seperator character is kept and removed.\n\tIf so, the pathname seperator character is added to output_list.\n\tFor each file in the list, remove from output_list if it is in\n\tthe set remove list.\n\tIf it"}]}
{"task_id": "62ece4992e6aefcf4aabbd82", "prompt": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\nclass regex_pattern_to_ascii_list(re.compile([\"^\\d]+$\")) :\n\t\"\"\"\n    \tthis method replaces all letters in each sublist of an array with the\n    \tthe numbers in the regex pattern as the integer and string length as the list of string lengths and the rest of the string\n    \tthe strings are stored as a list of numbers\n\t\"\"\"\n\tdef __init__(self) :\n\t\tself.patterns = {\n\t\t\t\"a\" : re.compile(\"[\\d]+$\"),\n\t\t\t\"b\" : re.compile(\"[\\$\\+\\+\\&]+"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\ndef generate_regex():\n\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\ttry:\n\t\ttemp_list = []\n\t\twith open(\"regex.tmp\", \"r\") as file_in:\n\t\t\tfile_out = file_in.readlines()\n\t\t\tfor line in file_out:\n\t\t\t\ttemp_list.append(line.replace(\"\\n\", \"\"))\n\t\t\n\t\tregex = \"\"\n\t\tfor n in temp_list:\n\t\t\tregex = re.sub(n, \"\", regex)\n\n\t\tregex = regex.strip().lower()\n\t\tregex = \""}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\n\n\tdef translate(self, text, mapping):\n\t\t\"\"\"\n\t\t\tThis method translate an string to a python regex pattern format\n\t\t\"\"\"\n\t\ttext = re.sub(r'\\{([^\\}]+)\\}', self.regex, text)\n\n\t\treturn self.replace(text, mapping)\n\n\tdef reverseTranslate(self, text, mapping):\n\t\t\"\"\"\n\t\t\tThis method reverses the given string\n\t\t\"\"\"\n\t\ttext = re.sub(r'{([\\(\\)}]+)}', self.regex, text)\n\n\t\treturn self.replace(text, mapping)\n\n\tdef makeKey(self"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\n\n\tdef cprint_regex_pattern(self,str1,str2):\n\t\t\"\"\"\n\t\t\tprint regex to regex pattern\n\t\t\"\"\"\n\t\tregex_patterns = {\n\t\t\t'%': self.re_append\n\t\t}\n\t\tre_expr = re.compile(str2)\n\n\t\tresult = re_expr.sub('', str1)\n\n\t\tfor p in regex_patterns.keys():\n\t\t\tresult = result.replace(p, str1)\n\t\treturn result\n\tdef cprint_regex_pattern_split(self, str1, str2):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\treturn_val = self.convert_regex(pat)\n\n\t\tif return_val!= None:\n\t\t\treturn return_val\n\n\t\tif self.debug == True:\n\t\t\tprint(\"pattern is empty\")\n\n\t\treturn None\n\n\tdef convert_regex(self, pat):\n\t\t\"\"\"\n\t\t\tReturns the regex converted according to regex-match rules\n\t\t\"\"\"\n\t\tmatches = re.findall('%s\\.%s\\.%s' % (PAT_STRING, PAT_REG, pat), self.buf2.get_value())\n\t\tdebug_output(DEBUG, \"PAT_STRING."}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\n\t\t\n\t\tout = ''\n\t\tfor i in range(len(regex)):\n\t\t\tout += str(re.escape(regex[i]).encode('utf-8')) + '\\n'\n\n\t\t\tif re.search(regex[i], x)!= None:\n\t\t\t\tout += '\\t' + x\n\t\t\telse:\n\t\t\t\tout += '\\n'\n\telse:\n\t\tout += \"This argument is not allowed!\"\n\t\t\n\tout+='</script>\\n'\n\t\n\treturn out\n\t\n\t\n\nif __name__ == '__main__':\n\tinput_input = '1 2 3'\n"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\nclass RegExp:\n\tdef __init__(self, patterns):\n\t\tself.patterns = []\n\t\tfor pattern in patterns:\n\t\t\tself.patterns.append(re.compile(pattern))\n\t\tself.regexes = self.patterns\n# Copyright (c) 2013 OpenStack Foundation\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache."}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\n#\tinputString = inputString.strip()\n\tlist1 = re.split(\"\\+\",inputString)\t\t\t###\t\tlist1 = list1\t\t\t### create list that hold all the elements\n\tregex_str = r''\n\t\n\tfor word in list1:\n\t\tword = word.strip()\n\t\tif word ==\"\" or word == \".\":\n\t\t\t#print(\"word is empty!\")\n\t\t\tregex_str += \"[]\"\n\t\t\tif len(regex_str) > 0:\n\t\t\t\tpattern = re.compile(regex_str)\n\t\t\t\tstr_with_word ="}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\tresult = ''\n\n\t\t''' Check if string has digits in string\t'''\n\t\tif len(string) > 10:\n\t\t\tresult = re.match(r'(?i)\\s*(.{0})?'.format(string), string, flags=0)\n\t\t\tif result is None:\n\t\t\t\traise RuntimeError('Could not match format \"{0}\" into \"{1}\"'.format(string, string.decode('utf-8')))\n\t\t\treturn result.group(0)\n\n\t\treturn ''\t\t\n\n\tdef convert_to_string(self, s):\n\t\t\"\"\"\n\t\tMethod Converts the given string"}, {"generated_text": "import re\t\"\"\"\n\tThis method converts the given string to regex pattern\t\"\"\"\n\t\t# Replace dots\t\n\t\tres=re.sub(r\"[^A-Za-z0-9+.\\_]*\", \"_\", str(pattern))\n\n\t\t\n\t\t# Convert to string\n\t\treturn re.sub(\"<[^A-Za-z0-9+.\\ ]\", \"^\"+res+\"$\", pattern)\n\t\t\n\ndef parse(word):\n\t\"\"\" Function converts a word to its ascii-art symbol\t \"\"\"\n\treturn ngram.synset(word)\n\t\n# Speaking for debugging:\n#import nltk.corpus\n#print(nltk.corpus.senna('"}]}
{"task_id": "62ece4992e6aefcf4aabbd83", "prompt": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\n\n\n\tdef read(self, bytes):\n\t\t\"\"\"\n\t\tRead bytes from serial.\n\t\t\n\t\tInput: Bytes to read\n\t\tOutput: Raw bytes\n\t\t\"\"\"\n\t\t\n\t\tdata = \"\"\n\t\tfor byte in range(bytes):\n\t\t\tdata += self.serial.read()\n\n\t\traw_str_len = len(data)\n\n\t\tlength = 0\n\t\twhile raw_str_len < length:\n\t\t\tdata += self.serial.read()\n\n\t\t\tlength += 1\n\n\t\tself.serial.flushInput()\t\t\n\t\treturn data\n\n\tdef close(self):\n\n\t\t\"\"\"\n\t\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tif command == 'ls':\n\t\t\treturn subprocess.Popen(args)\n\t\telse:\n\t\t\treturn subprocess.Popen(args, shell=True)\n\t\t\n'''\n\tMethod creates a temp file using subprocess\n\tIt creates a temp file and writes its contents in.tmp and.stderr\n\t'''\ndef run_temp_file(command, args, shell=False):\n\tcommand = 'tmp.stdout'\n\tif shell:\n\t\tcommand = 'tmp.stderr'\n\t\n\tif type(args) == str:\n\t\targs = args.splitlines()\n\t\n\t(tempFile,"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tprint \"RUNNING COMMAND:\",command\n\n\t\tp = subprocess.Popen(command, stdout=subprocess.PIPE, stdout_stderr=subprocess.PIPE);\n\t\tstdout, stderr = p.communicate()\n\t\tstatus = p.returncode;\n\t\t\n\t\tif status == 0:\n\t\t\tprint \"Command succeeded\"\n\t\telse:\n\t\t\tprint \"Command failed\"\n\n\t\tprint \"STDOUT:\",stdout\n\t\tprint \"STDERR:\",stderr\n\n\t\treturn status\n\t\t\t\n\tdef update_listings(lists):\n\t\t\"\"\"Update the current listings.\n\t\t\n\t\tlistings [list] - name [current listings]\n\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\nclass Curl(Command): passfrom Screens.Screen import Screen\nfrom Screens.InputBox import InputBox\nfrom Screens.MessageBox import MessageBox\nfrom Components.ActionMap import ActionMap\nfrom Components.Label import Label\nfrom Components.ScrollLabel import ScrollLabel\nfrom Tools.StbHardware import getFPVersion\n\nclass STBExtensionsList(Screen):\n\tdef __init__(self, session, type, url):\n\t\tScreen.__init__(self, session)\n\t\tself.type = type\n\t\tself.url = url\n\t\tself.list = []\n\t\tself.update"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\ndef callCommand(cmd, cwd=None):\n\t'''Call the given command and return output, return exit code,\n\t\tor None if error.'''\n\t\n\tif isinstance(cmd, list):\n\t\tret = cmd_wrapper.call(cmd, cwd)\n\t\tif ret!= 0:\n\t\t\tprint \"\"\"Error running command %s\\n%s\"\"\" % (cmd, ret)\n\t\t\treturn -1\n\t\treturn ret\n\telse:\n\t\tcommand = \"\"\n\t\tif cwd:\n\t\t\tcommand = \"cd %s;%s\" % (cwd, \" \".join(cmd))"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tprint (\"Running: %s\" % command)\n\t\treturncode = subprocess.call(command, shell=True)\n\t\tif returncode > 0:\n\t\t\t#command failed\n\t\t\traise subprocess.CalledProcessError(returncode, command)\n\t\telif returncode < 0:\n\t\t\t#the call failed\n\t\t\tprint (\"Failed: %s\" % command)\n\t\t\toutput = subprocess.call (command, shell=True)\n\t\t\tprint (output)\n\t\t\tif output:\n\t\t\t\tprint (\"Stdout:\")\n\t\t\t\tprint (output)\n\t\telse:\n\t\t\t#the call had an error (e.g. the"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tp = subprocess.Popen(args=args, shell=False, cwd=cwd,\n\t\tstdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n\t\tclose_fds=True)\n\t\ts_output, s_error = p.communicate()\n\t\treturncode = p.returncode\n\terr = s_error.rstrip()\n\tif returncode!= 0:\n\t\traise IOError(\"Shell call to %s failed \" % \" \".join(args))\n\treturn s_output\n\ndef removeFiles(destFiles):\n\tfor fname in destFiles:\n\t\tprint \"Removing\", fname"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\tfor i in cmd:\n\t\t\tproc = subprocess.Popen(i, shell=True, stdout=subprocess.PIPE)\n\t\t\tret = proc.wait()\n\t\t\tif ret == 0:\n\t\t\t\treturn int(proc.communicate()[0].strip())\n\t\tif os.environ.get(\"CHRINT_PYTHON_VERSION\")==\"2\":\n\t\t\tproc = subprocess.Popen([\"python\", \"run.py\"])\n\t\t\tret = proc.wait()\n\t\t\tif ret == 0:\n\t\t\t\tprint(\"running\", os.environ[\"PYTHONEXEC\"])\n\t\t\t\treturn 0\n\t\t\n\n\tdef main():\n\t"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t\n\t\t\n\t\tproc = subprocess.Popen(command, shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n\t\tout, err = proc.communicate()\n\t\n\t\treturn out,err # Return error code\n\n\n# --------------------- Main function ----------------------------------- #\ndef main():\n\n\tprint __name__ \n\tprint \"Python2 version:\", sys.version.split(\"\\n\")[0]\n\n\ttry:\n\t\t# Set timeout.\n\t\t# timeout = 0,0\n\texcept Exception as e:\n\t\tprint \"Exception thrown when running python2's timeout.\"\n\t\tprint \"Error:"}, {"generated_text": "import subprocess\t\"\"\"\n\tCall the given command(s).\t\"\"\"\n\t \n\tproc = subprocess.Popen(command_string=command)\n\t    proc.wait()\n\toutput = proc.communicate()[0]\n\tprint \"\\noutput= \", output\n\tprint \"\\n\", output\n\n\n\t\n    def make_pymol_files():\n      \t\"\"\"\n\t  \tMakes a list of pymol files.\n      \t\"\"\"\n       \n      \t# This is the list of python files in the current folder\n       \n      \tpythonfiles = [\"Python_1.py\",\"Python_10.py\",\"Python_11.py\",\n\t\t\"Python_12.py\",\"Python_13.py\"]\n      "}]}
{"task_id": "62ece4992e6aefcf4aabbd84", "prompt": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t", "suggestions": [{"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\ttry as_ipv4\n\t\treturn(True)\n\texcept ImportError:\n\t\treturn(False)\n\t\t\n\t\t\ndef list_allIP():\n\t\"\"\"\n\t\tList all IP addresses\n    \t\"\"\"\n\ttry as_listIP\n\texcept ImportError:\n\t\treturn True\n\t\t\t\ndef ip_range():\n\t\"\"\"\n\t\tReturn all IPv4 addresses\n\t\"\"\"\n\treturn(list_allIP().replace(\":\",\"\")[:-1])\n\t\n\n\n#\tReturn IPv6 Address List\ndef ip_to():\n\t\"\"\"\n\tReturn IPv6 Address List\n\t\"\"\"\n\treturn"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\nclass test_netmask(Test):\n\t\"\"\"\n    Testing if IPv4 netmask (ex: 255.255.255.0)\n    \t\"\"\"\n\tdef test_netmask_network(self):\n\n\t\t\"\"\"\n\t\tMake sure /etc/sysconfig/network/sysconfig exists as expected\n\t\t\"\"\"\n\n\t\tip = self.net[\"ip\"]\n\t\tnetmask = self.net[\"netmask\"]\n\t\tfor x in range(1,16):\n\t\t\tif x < 40 or (x % 2) == 0:\n\t\t\t\tnetmask += \"128\"\n\t\tself.delname(self.net[\"name"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\n\n\tif __name__ == \"__main__\":\n\t\texit(main())# (C) Copyright 2014 Voyager Search\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. "}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\ndef get_ipv4():\n\t\"\"\"\n\t\tReturn:\n\t\t(string) the IPv4 address\n\t\t- (string) ip\n\t\t- (string) version\n\t\"\"\"\n\tresult = ''\n\t\n\tip_v4=ipaddr.IPv4Address(ip_address)\n\t\t\n\tip_v6 = ipaddr.IPv6Address(ip_address)\n\tip_v6.set_uncompressed(True)\n\t\n\tif version == 4:\n\t\t#IPv6 address\n\n\t\tip_v6.explode_sub(7, 8, 1, '2001:db"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\n\t\n\tdef testIPv6Address(self):\n\t\t\"\"\"\n\t\tIPv6 address check\n\t\t\"\"\"\n\t\tipaddress.ip_interface(ipaddress.IPv6Address('FE80::1234'))\n\t\t\n\t\tself.assertTrue(ipaddress.ip_address('FE80::1234') in [ipaddress.ip_network('FE80::12'), ipaddress.ip_network('1:2:3::')])\n\t\tself.assertTrue(ipaddress.ip_address('FE80::12') in [ipaddress.ip_network('FE80::123'), ipaddress.ip_network('fe80::12'), ipaddress.ip"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\n\t\n\t\"\"\"\n\t>>> ipaddress.IPv4Address(34, 45)\n\tipaddress.IPv4Address(34, 45)\n\t>>> ipaddress.IPv4Address('192.168.0.1')\n\tipaddress.IPv4Address('192.168.0.1')\n\t\"\"\"\n\tdef testIP2LongBasics(self):\n\t\tself.assertEqual(type(ipaddress.IPv4Address(0x102)), long)\n\t\tself.assertEqual(type(ipaddress.IPv4Address(-1)), long)\n\n\n\tdef testIP2Long(self):\n\t\tself"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\nelif sys.version_info.major == 3:\n    import ipaddress\t\"\"\"\n    Test if IPv4 address or not\n    \t\"\"\"\n\t\nelse:\n\timport argparse\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\nsys.stdout.writelines( \"       * -           \\n\\n\"           )\n\ntry:\n\t\n\timport ipaddress\nexcept ImportError: pass\n\t\n\timport socket\n\tfrom   ipaddress import *\t\n\tfrom   ipaddress import __version__\n\t\n\timport sys\n\timport re\n\tglobal ipv4_addr_pattern\n\t#global ipv6_"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\n\n\tif len(sys.argv[1]) == 0:\n\t\t\n\t\tprint(\"\\tUsage : python3.6 [check]\")\n\n\t\tprint(\"   Example : python3  IPv4 Check\")\n\n\t\tprint(\"\\t          python3 test_ipv4.py 127.0.0.1\")\n\t\tprint(\"\\n\")\n\n\t\t\n\tif sys.argv[2] == \"list\":\n\t\tlist_test()\n\n\tif sys.argv[2] == \"ip\":\n\t\tipv4_address = sys.argv[3]\n\telse:\n\t\tipv4_address = \""}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t       \n\t       \n\tdef testIPv4Query(self):\t\n    \t\"\"\"\n\t\tCheck if IPv4 query result, if not return error\n\t\t\"\"\"\n\t       \n\t       \n\tdef testIPv6Query(self):\t\n    \t\"\"\"\n\t\tCheck if IPv6 query result, if not return error\n\t\t\"\"\"\n\t       \n\tdef testIPv6Unicast(self):\t\n    \t\"\"\"\n\t\tCheck if IPv4 multicast on port 21 ingress is unicast\n\t\t\"\"\"\n               \n\t\tresult = self.vapi.cli(\"show ip interface incl 5\")\n                 \t\n\t\tprint"}, {"generated_text": "import ipaddress\t\"\"\"\n\tTest if IPv4 address or not\n    \t\"\"\"\n\t\n\t\t\n\t\ttry:\n\t\t\tipaddr=ipaddress.IPv4Address(ipaddress.IPv4Address._ip)\n\t\t\tif ipaddr.is_multicast:\n\t\t\t\t\tfor family,ip,netmask in host_ip_netmask:\n\t\t\t\t\t\tif ipaddr.overlaps(family, ip):\n\t\t\t\t\t\t\treturn True\n\t\t\t\t\t\treturn False\n\t\t\telif ipaddr.is_loopback:\n\t\t\t\t\tfor family,ip,netmask in host_ip_netmask:\n\t\t\t\t\t\tif ipaddr.overlaps(family, ip):\n\t\t\t\t\t\t\treturn True\n\t\t\t\t\t\treturn False\n\t\texcept:\n\t\t\timport sys\n\t\t\tsys.stderr.write('Exception reading host IP"}]}
{"task_id": "62ece4992e6aefcf4aabbd85", "prompt": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t", "suggestions": [{"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\n\n\tif graph.qname!= None:\n\t\troot = rdflib.URIRef(graph.qname)\n\telse:\n\t\troot = rdflib.BNode()\n\n\tdef append_children(node, childs):\n\t\tfor child in childs:\n\t\t\tappend_children(child, node[child])\n\n\tnodes = filter(lambda n: not n.is_child() and not n.is_parent(), \n\t\t\tmap(create_graph, nodes))\n\t\n\toutput = StringIO.StringIO()\n\t\t\n\t\t\n\t\t\n\t\t\n    for node in nodes:\n        if node.is_parent"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\n\n\troot = graph.subjects(parent)\n\t\n\tprop, relation = rdflib.URIRef(graph.value(parent, XSI.subClassOf))\n\t\n\treturn set(root.premise(prop, relation, None))[0]\n\t\t\t\n\ndef find_roots_recursive(graph, rdflib.RDFS.subClassOf, include_self=False):\n  \"\"\"Finds all root nodes with an ancestor of rdfclasses in the graph which\nare under the parent. Use full traversal by default\"\"\"\n\n\troot = rdflib.RDFS.subClassOf(graph, rdflib"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\n\t\t\n\n#from rdflib\timport rdflib\n#from rdflib\tfrom rdflib\tfrom rdflib\tfrom rdflib\t# used for searching rdflib by property\n\n\ndef find_roots(graph, rdflib.RDFS.subClassOf):\n    \"\"\"Finds all the roots of the subgraph that have a given root predicate \n    expression using rdflib.\n    \"\"\"\n    \n    if rdflib.__version__!= \"1.1.0\" :\n        raise ImportError(\"rdflib.__version__.rdflib() required for find_roots() in sub class\")\n    return set(graph.predicate_objects(rdflib"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\ndef root_node(graph, n, attr, directed=True, prefixes=None):\n\t\"\"\"Find the root of root node n with respect to the subtree node n:\n\troot_node(graph, n, child, prop, directed=True, prefixes=[], attr=attr)\n\t\"\"\"\n\tfrom rdflib\t# rdflib.term.URIRef if ns is None\n\tfrom skos.core.skosattr.types.core import BFObjRef\n\t#print \"root_node(%s, %s, %s, %s): \\n%s\"%("}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\trdflib.graph.DiGraph.__init__(self)\n\t\tgraph.add((self, RDFS.subClassOf, SKOS.broader))\n\t\tfor edge in graph.predicate_objects(RDFS.subClassOf, SKOS.broader):\n\t\t\tgraph.add((edge, SKOS.broader, RDFS.subClassOf))\n\t\tgraph.add((self, SKOS.broader, RDF.type, SKOS.broader))\n\t\tgraph.add((self, SKOS.broader, SKOS.property, SKOS.bro"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t \n\tchild = getFirstParent(graph, rdflib.RDFS.subClassOf)\n\tparent = getFirstParent(graph, rdflib.RDFS.subClassOf)\n\n\tif not child:\n\t\treturn set([])\n\n\tchild = rdflib.URIRef(child[0])        # e.g. '/p:A->P/Q'\n\tprop = rdflib.URIRef(resource=resource)  # e.g. spb:subClassOf\n\n\t# for some reason, only the parent has a property so we have to reverse\n\t# its"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\n\n\tdef __init__(self, graph, direction = rdflib.RDFS.subClassOf):\n\n\t\tif not graph:\n\t\t\traise TypeError, \"graph argument required\"\n\n\t\tif not graph.identifier == \"root\":\n\t\t\traise TypeError, \"graph identifier required\"\n\n\t\tsuper(SubClassOf, self).__init__()\n\n\t\tif self.direction!= rdflib.RDFS.subClassOf:\n\t\t\traise TypeError, \"direction incorrect\"\n\n\t\tself.graph = graph\n\t\n\t\tself.subClassOf = self.graph.value(predicate = self.direction)\n\t\t\n\t\"\"\"\n"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\ndef find_roots(graph, rdflib.RDFS.subClassOf, depth = -0, triples = False):\n\t\"\"\"Find all roots in rdflib.RDFS and return the results.\n\nGiven a rdflib.Graph instance and a depth of depth, this function iterates\nthrough subgraphs to find the roots of the given depth.  In the case of\nvery complex graphs, the graph is represented as a list.\"\"\"\n\t# Recursively start at the deepest level.\n\tif not triples:\n\t\troot = rdflib.RDFS.subClassOf(root"}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\ndef find_roots(graph, sp, root, depth = 0):\n\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\n\t:type sp: rdflib.Literal\n\t:type root: String\n\t:type depth: Integer\"\"\"\n\t\n\tif root == None or sp==None or root == \"\" or root == \"..\":\n\t\treturn set()\n\t\t\n\tr = []\n\td = []\t\t\n\tn = []\n\t\n\tfor c in graph:\n\t\tif c not in d:\n\t\t\tcontinue\n\t\tfor e in c.children:\n\t\t\tn."}, {"generated_text": "import rdflib\t\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader\t\"\"\"\n\t\n\tdef __recurse(self, current_triples, parent_triples, graph, cur_graph, parent_graph, parent_prop, parent_label, label):\n\t\t\"\"\"\n\t\t\"\"\"\n\t\t\"\"\"\n\t\tif current_triples[0][0] in (\"subClassOf\", \"b\"):\n\t\t\t# In the case of a single target node, and each is a root\n\t\t\t# of this triple to the root, see if it is a root (or no)\n\t\t\tfor target_triple in current_triples:\n\t\t\t\tself.__recurse(current_triples["}]}
{"task_id": "62ece4992e6aefcf4aabbd86", "prompt": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t", "suggestions": [{"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\ttry: \n\t\t\timport unicodecsv\n\texcept ImportError: #import unicode as _py2_unicode and use the right encoding\n\t\t\timport codecs\n\t\t\t_unicode = unicodecsv.writer(codecs.getwriter('utf8'), encoding='utf8')\n\t\t\t_unicode.writerow(dict(zip(csv.reader([line[:-1] for line in lines], 'utf8'))))  \n\t\t\t_unicode.writerows([[colnames.get(str(line[-1]),None)]\n\t\t\t\tfor line in lines])\n\nclass SDReader(object):\n\t\"\"\"\n\t\tA class for"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\ttry :\n\t\t\tdump_str = \"\".join(yaml.dump(self, Dumper=yaml.RoundTripDumper))\n\t\t\treturn dump_str\n\t\texcept:\n\t\t\t self.handleError(None, \"cannot dump\", \"\\nwhile dumping:\\n%s\" % dump_str)\n\t\"\"\"\n\t__repr__ = getrepr2func(self)\n\t__str__ = getstr2func(self)\n\n\tdef handleError(self, error, message, level):\n\t\tif self.args.verbose:\n\t\t\ttraceback.print_stack()\n\t\timport sys"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\treturn yaml.dump(self)\n\n# end class Dumper\n\n\nclass YamlToPythonDumper(YamlToPyDumper):\n\t\"\"\" Dump the contents of an YAML-encoded block into a python file \"\"\"\n\tdef __init__(self, stream):\n\t\tYamlToPyDumper.__init__(self, stream, skipunknownelems=True)\n\n\tdef add_item(self, name, value, itemstart, endidx):\n\t\t\"\"\" Add a simple item \"\"\"\n\t\traise NotImplementedError('cannot create custom item names')\n\n# end class YamlToPythonDumper\n\n\n"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\ttry:\n\t\t\treturn yaml.safe_load(str(value), Loader=yaml.SafeLoader)\n\t\texcept yaml.YAMLError as ex:\n\t\t\traise ValidationError(_(\"This yaml file is not well-formed: %s\") % ex)\n\nclass SimpleNamespace(object):\n\t\"\"\"\n\tThis is not used anymore. Used only for debugging.\n\t\"\"\"\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(SimpleNamespace, self).__init__(*args, **kwargs)\n\t\tself.set_name()\n\tdef setName(self):\n\t\t"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\n#import re\tIMPORT\n\n#from jinja2\timport Template\t\n#from jinja2.exceptions\t\timport TemplateSyntaxError\n\n# The next line we can replace is used to set the template parameters\nTEMPLATE_PARAMETERS = '\\n'.join(['       '+ line + '\\n'\n\tfor line in template.split(\"\\n\")])\n\n# For now the following is for testing & for validating & encoding the HTML\n# parser, or a library or module. For parsing and test cases, pass the\n# environment variable.\ndef setup_module():\n\timport sys, os, imp,"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\n##############\n# CLASSES #\n##############\n\nclass DumpManager():\n    \"\"\"\n    Dumps a YAML serialization to standard output.\n    \n      Dump.yaml - dump YAML to standard output.\n    \"\"\"\n\n    def __init__( self ):\n        \"\"\"\n        Sets up the registry.\n        \"\"\"\n\n        # Dictionary of all defined attributes\n        self.loaded = {}\n\n    def load( self, filename ):\n        \"\"\"\n        Loads a YAML serialization (i.e. a dictionary). \n        \n        filename - path to YAML file to load. \n        \"\"\"\n        \n        # The original filename\n        original_filename = os.path.splitext(os.path"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\n\t\n\tyaml.add_representer(yaml.representer.SafeRepresenter, representer.SafeRepresenter.represent_str)\n\tyaml.add_representer(yaml.representer.SafeRepresenter, representer.SafeRepresenter.represent_unicode)\n\tyaml.add_representer(yaml.representer.SafeRepresenter, representer.SafeRepresenter.represent_bytes)\n\tyaml.add_representer(yaml.representer.SafeRepresenter, representer.SafeRepresenter.represent_long)\n\tyaml.add_representer(yaml.re"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\tif hasattr(yaml, '__line_buffer__'): #PY2\n\t\t\tfor line in text.splitlines():\n\t\t\t\tif line.startswith('#') or line == '':\n\t\t\t\t\tcontinue\n\t\t\t\tif line.endswith('\\\\'):\n\t\t\t\t\tline = line[:-1]\n\t\t\t\tif line.endswith('\"') and '\"' not in line:\n\t\t\t\t\tline = line.replace('\"', '\\\\\"')\n\t\t\t\tif line.endswith(\"'\") and quote_re.match(line):\n\t\t\t\t\tline = line.replace('\"', '\\\\\"')\n\t\t\t\ttry:\n\t\t\t\t\tline = line.encode('utf-8')\n\t\t\t\texcept:\n\t\t\t\t\timport traceback"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\n\n\tif verbose:\n\t\tprint(yaml.dump(yaml.load(open(\"cracker.yaml\").read())))\n\n\n\tif is_ipython:\n\t\tipython.embed()\n\n\tif is_sqlite:\n\t\tcracker = Cracker()\n\t\tcracker.start()\n\n\treturn\n\n\n\nclass App(object):\n\n\t\"\"\"This is where we build our app's template\"\"\"\n\n\tapp = None\n\t_is_frozen = True\n\n\tdef _set_app(self, app):\n\t\t\"\"\"Set the app\"\"\"\n\t\tsys.modules['crack"}, {"generated_text": "import yaml\t\"\"\"\n\tDump to a py2-unicode or py3-string\t\"\"\"\n\t\tresult = yaml.dump(self.obj)\n\t\treturn result\n\t\t\n\tdef json_dump(self):\n\t\t\"\"\"\n\t\tDump to a json file to make sure it can be dumped\n\t\t\n\t\t>>> a = yaml.safe_dump\n\t\t>>>\t<safe.unicode object at 0x2427cd0>\n\t\tFalse\n\t\t>>>\t<safe.str object at 0x2427cd0>\n\t\tTrue\n\t\t>>>\t<safe.int object at 0x2427cd0>\n\t\tFalse\n\t\t\"\"\"\n\t\ttext = yaml.dump(self.obj)\n\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd87", "prompt": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\t\n\ttry:\n\t\t\n\t\t# create logger\n\t\tlogger = logging.getLogger(\"M3Crypto\")\n\n\t\t# configure log level\n\t\tif params.verbose >= 1:\n\t\t\tloglevel = 10\n\t\telif params.verbose == 1:\n\t\t\tloglevel = 0\n\t\telse:\n\t\t\tloglevel = 50\n\n\t\t# create file handler\n\t\tlogfilename = os.path.join(os.path.curdir,'M3Crypto'+str(hex(params.mMsgId))) + \"_log.txt\"\n\t\tlogging.basicConfig(filename=logfilename, level=loglevel, format='%(asctime"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n    \tself.log = logging.getLogger('MainLogger')\n    \t\n    \tself.log.info('Logger started with filename  : %s', filename)\n    \t# Set the logger object\n\tself.log.setLevel(logging.DEBUG)\n       \n       \n#        self.log.info('# DEBUG: Log start ###')\n#        self.log.info('Logger start done with filename : %s', filename)\n    \tself.logformat = logging.Formatter('%(asctime)s -%(levelname)s - %(message)s')\n#        self.dateformat = logging.Formatter('%(asctime)"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\n\tdef initlogger( self, l, msg = None ):\n\t\t# define the format\n\t\tself.time = strftime(\"%Y-%m-%d\", gmtime())\n\t\tdt_log_format = '%x-%m-%d %H:%M:%S'\n\t\tif type(l) is str:\n\t\t\tl = strftime(dt_log_format, gmtime())\n\t\telse:\n\t\t\tl = \"\"\n\t\tif msg is None:\n\t\t\tmsg = l\n\t\tif type(msg) is str:\n\t\t\tmsg = msg.decode('gbk')\n\n\t\tself"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\n\tdef setup(self, logfilename=\"logs/log.log\"):\n\t\tlogging.basicConfig(filename=logfilename, level=logging.DEBUG, format='%(asctime)s %(message)s')\n\t\tself.logger = logging.getLogger(__name__)\n\t\tself.logger.setLevel(logging.DEBUG)\n\t\tself.logger.addHandler(logging.StreamHandler())\n\t\t# self.logger.addHandler(logging.FileHandler(logfilename))\n\n\n\t\tif ('Raspberry PI' in self.__class__.__name__):\n\t\t\tself.RaspberryPI=True\n\t\telse:"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\nLOGFILEPATH   = \"/var/log/meteor/log.txt\"\nlogging.basicConfig(filename=LOGFILEPATH,level=logging.DEBUG)  \n\nlog = logging.getLogger(\"MeteorLog\")\n#set a log to a file handler which logs even debug messages. \n#I recommend adding a loghandler so the library knows the logging level with DEBUG messages\n#http://www.saltycrane.com/blog/?p=933\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\tpass\t# pragma: no cover\n\n\tdef init(self):\n\t\t\"\"\"\n\t\tDoes nothing for this object besides open().\n\t\tMust be called before daemonization.\n\t\t\"\"\"\n\t\tpass\t# pragma: no cover\t\n\n\t@staticmethod\n\tdef get_file_logger(fpath):\n\t\t\"\"\"\n\t\tReturns a file logger.\n\n\t\tIt is currently a logger that has been created.\n\t\t@param fpath:\n\t\t@type fpath:\tC{string}\n\t\t@return:\t\t\n\t\t@rtype:\tC{logging.Logger}\n\t\t"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\nlog = logging.getLogger(\"Nikola\")\n\ndef create_app():\n\tapp = Nikola()\n\tapp.config.update(read_config_file())\n\tapp.config.from_envvar('NEWSBLUR_ENVVAR','${newsblur_appname}')\n\t\n\t# we create the tables after models\n\tmetadata.bind = metadata.tables['metadata'].bind \n\tmetadata.reflect = metadata.tables['metadata'].reflect\n\tmetadata.drop_all()\n\t\n\t# create the model manager\n\tmetadata.create_all()\n\tapp"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\t\t\n\ttry:\n\t\tlogging.basicConfig(format='%(asctime)s - %(filename)s - %(levelname)s\\nLog file', \n\t\t\t\tdatefmt='%m-%d %H:%M',filename=log_name, level=logging.INFO)\n\texcept Exception as err:\n\t\tlogging.info(\"Error in main_logger: \" + str(err))\n\t\t\n\tlogger=logging.getLogger('log')\t\n\tlogger.setLevel(logging.INFO)\n\tlogger.addHandler(log_handler)\n\t\n\tlog_handler.setLevel(logging.INFO)\n\t\n\treturn log_"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\n\t# Default logging verbosity level\n\tclass Logger:\n\t\tdef __init__(self):\n\t\t\tself.handler = logging.StreamHandler()\n\t\t\tself.formatter = logging.Formatter('[%(asctime)s] %(levelname)s %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n\t\t\tself.handler.setFormatter(self.formatter)\n\t\t\tself.handler.setLevel(logging.INFO)\n\t\t\tself.logger = logging.getLogger('kombulog')\n\t\t\treturn\n\n\tlogger = Logger()# -*- coding: utf-8 -*-\n\n"}, {"generated_text": "import os,logging\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\t\"\"\"\n\t\n\t\n\tdef log(self,message):\n\t\ttry:\n\t\t\tfile = open(self.log_filename,\"a\")\n\t\texcept IOError:\n\t\t\tfile = open(self.log_filename,\"w\")\n\t\tfile.write(time.strftime(\"%Y-%m-%d %H:%M:%S\")+\". \"+message)\n\t\tfile.close()\n\t\n\tdef warning(self,message):\n\t\ttry:\n\t\t\tfile = open(self.log_filename,\"a\")\n\t\texcept IOError:\n\t\t\tfile = open(self.log_filename,\"w\")\n\t\tfile."}]}
{"task_id": "62ece4992e6aefcf4aabbd88", "prompt": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\ndef make_tensor(data, dtype=None):\n\t'''Convert an array to a tensor.\n\nParameters\n----------\ndata : numpy array\n    data-type of the array to convert\ndtype : `numpy.dtype` (optional)\n    \n    Returns\n-------\n tensor : array\n    tensor data, which is then converted\n\n'''\t\n\tif np.isscalar(data):\n\t\treturn np.asarray(data)\t\n\telse:\n\t\ttry:\n\t\t\treturn np.lib.arraysetops.tensor(data, dtype=dtype)\t\n\t\texcept AttributeError:\n\t\t\treturn"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t \n\tif (isinstance(shape, list)):\n\t\tassert (isinstance(dtype, list))\n \telif (type(shape)==tuple):\n\t\tassert (len(shape)>=0)\n \t\tassert((dtype==dtype.base_dtype))\n \t\tassert((dtype==dtype.base_dtype).all())\n \t\tprint(str(shape))\n \t\tprint(dtype)\n \t\tprint(dtype)\n \t\tprint(shape[0])\n \t\tprint(shape[1])\n \t\tprint(shape[2])\n\telse:\n\t\traise TypeError(\""}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\ndef writearray(filename, array):\n\t\"\"\"\n\tFunction to write the array \n\t\"\"\"\t\n\toutfile = open(filename,'w') \n#\t\n\tfor i in range(0,len(array.dtype)):\t# Write every element of the array\n\twritearray_elem(outfile, array[i], array.dtype[i])\t\n\toutfile.close()\t  \n\n\ndef writearray_elem(outfile, array, typecode_str):\n\t\"\"\"\n\tFunction to create an array with shape and dtype and save the array \n\tin its proper"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n    dtype = np.dtype(dtype)\n\tif dtype is not dtypes.empty:\n\t\tif value is None: value=0\n\t\tif isinstance(value, float): value=0.0\n\t\telif isinstance(value, complex): value=0.0\t\t\n\treturn np.zeros(shape=shape, dtype=dtype), np.array(value, dtype=dtype)\n\t\t\n\t\t#!/usr/bin/env python\nimport logging\nimport numpy as np\n\nimport ray\nfrom ray import tune\nfrom ray.utils import format_index\n\nlogger = logging.getLogger(__"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n        size = np.size(arr)\n\tret = np.zeros(size, dtype=dtype)\n        for i, s in enumerate(arr):\n\tret[i] = np.random.random(shape)\n\t\n\treturn ret\n\t\nif __name__ == \"__main__\":\n\tprint \"Running...\"\n\tprint \"Reading... \"\n\treader = csv.reader(open(\"train_sample3\", 'rb'))\n\n\tshape = (4, 3, 2)\n\tdtype = np.float32\n\t\n\tdata = np.zeros(shape, dtype=dtype)\n\t"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n    if shape is not None :\n\t\tshape = list(shape)\n\n\telif dtype is not None :\n\t\traise ValueError(\"Shape and dtype cannot be set together.\")\n\t\n    return np.array(shape, dtype)\t\n\nclass MplException(Exception):\n\tpass\n\ndef zero_var(dtype, zero_values):\n    \"\"\"\n    Returns the zero tensor for a given numeric type.\n    It throws an MplException if the dtype cannot be cast as a scalar.\n\n    Parameters\n    ----------\n    dtype : str\n        dtype of the value\n    zero_values : float or int\n        The zero value"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\ndef array_to_uint_array(arr):\n\t\"\"\"\n\tConverts an array into the UINT8 array.\n\nParameters\n----------\narr : `array.array`\n    an array whose range is assumed to already be sorted\n\"\"\"\n\t\n\tn = len(arr)\n\tsort_array = np.sort(arr)\n\tarray_as_uint8 = array_to_uint8(arr)\n\t\n\tsort_array -= n\n\tsort_array *= -1\n\t\t\n\tsize_array_into_uint8 = len(array_as_uint8)\t#number"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n\t\n\tdef gen_indices(shape):\n\t\t\"Generate an equivalent array of values.\"\n\t\treturn asarray(np.random.random(shape))\n\n\tdef gen_indices_np(shape):\n\t\t\"Generate an equivalent array of values\"\n\t\tnp.random.seed(1)\n\t\treturn asarray(np.random.random(shape))\n\n\tdef test_shape():\n\t\t\"Test size argument.\"\n\t\tif len(shape) == 0:\n\t\t\treturn\n\t\t\n\t\t# Test that all array shapes have the same length.\n\t\tfor shape in shape:\n\t\t\tassert(len"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\n################################################\n# Data-Type Description\n################################################\n\nclass DataType:\n\tdef __init__(self,value=None,ndim=None,dtype=None):\n\t\tself.value=value\n\t\tself.ndim=ndim\n\t\t\n\t\tif dtype:\n\t\t\tself.dtype=dtype\n\t\telse:\n\t\t\tself.dtype=\"numeric\"\n\t\t\t\n################################################\n\tdef __str__(self):\n\t\tprint self.value\n\t\n\tdef __len__(self):\n\t\treturn len(self.value)\n\n################################################\n# Global data-type Description\n################################################\n\nclass Type(DataType):\n"}, {"generated_text": "import numpy as np\t\"\"\"\n\tFunction to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\n\t\ndef zeros(shape, dtype=None, order='F', subok=False):\n \"\"\"\n\tReturn an array of zeros\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create\t\"\"\"\t\t\n\tif len(shape) == 0:\n\t\treturn\n\ts = np.zeros(shape, dtype=dtype, order=order, subok=subok)\n\tif offset == 0:\n\t\treturn s\n\tif not subok and dtype == np.float16:\n\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd89", "prompt": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n\t\t\n\tm = 1./30.\n\tsigma = 0.001\n\tn = 3      \n\tnwalkers = 0\n\tl = 2.  \t\t# l is half of the initial distance from center of mass.\n\tk = 0.\n\tphi = 4.\n\ttheta = 0.75\n\td = 1.\n\tdt = 0.005\n\tnint = 10;\t # Number of timesteps to perform random walk.\n\t\n\tp = []\n\ti = 0.\n\twhile i <= n + int(np.random.rand(nwalk"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n#     try:\n#         from scipy import optimize\n#         import matplotlib.pyplot as plt\n#     except:\n#         print \"Unable to import scipy.optimize\"\n#         raise\n#\n#     #\n#     #     parameters\n#     mu,sig,i = optimize.curve_fit(func,xdata,ydata,args=(xdata,sigma) )\n#\n#     mu, sig,i = optimize.curve_fit(func,xdata,ydata,args=(xdata,sigma,ydata) )\n\n\n#     plt.gca().xaxis."}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n\n\tfrom functools import wraps\n\tfrom numpy.random import uniform,normal,lognormal,gaussian\n\n\tdef gauss(r,c):\t\t## returns a normalized copy of a normal distribution\n\t\t## r is the mean radius\n\t\tr *= 1.\n\t\tc *= 1.\n\n\t\t# the following is a translation from Math.min(abs(r)), since\n\t\t## math.min is a function of R, and numpy.min will produce the same \n\t\t# result as math.f(r) / math.f(c)\n\t\t# r = 1 + (1"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n\n\t# Calculate the center of the gaussian\n\tSigmaGauss = 1./(Sigma[0]*Sigma[0])\n\tCenterYNorm = np.max(SigmaGauss + 1./(SigmaGauss+np.log(2.*SigmaGauss**2)))\n\tCenterYNorm = CenterYNorm/CenterYNorm.max()\n\t#print \"CenterYNorm = \", CenterYNorm\n\tSigma = np.sqrt(np.diag(Sigma))\n\n\n\t# Normalization the data\n\tVelVals = VelVals*(1.0/(Sigma[0"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n\n\tdef __init__(self, x):\n\n\t\t\"\"\" Initialize the Gaussian model with the Mean\n\t\t\t\"\"\"\n\t\tself.mu = np.zeros(len(x)) / len(x)\n\t\tfor i, v in enumerate(x):\n\t\t\tself.mu[i] = v\n\t\tself.params = self.mu \n\t\n\t\tself.fit()\n\n\n\tdef fit(self):\n\n\t\tself.params = 1 - (self.mu + np.mean(self.y_cent_x_r) * np.sum(self.y_cent_x_r"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\tif (np.sqrt(np.power(x-0.,2))<=1.e-12):\n\t\t\tg = 0.2 \n\t\telif (np.sqrt(np.power(x+0.,2))<=2.e-12):\n\t\t\tg = 1.\n\t\telse:\n\t\t\tg = 0.2\n\t\tsigY = np.arccos(x-0.)\n\t\tsigZ = np.arcsin(x-0.)\n\n\t\t#Gauss centered around 0.2 with a size of 1.\n\t\tg0 = np.sqrt(sigY"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\ndef gaussian(x, y, fwhm):\n\t\"\"\"\n\tGaussian of x, y. \n\t\n\t@param x, y - center x and y\n\t@param fwhm - FWHM of Gaussian\n\t@return the gaussian\n\t\"\"\"\n\t\ndef gaussian_sigma(x, y, fwhm):\n\t\"\"\"\n\tGaussian of x, y, sigma, where center x - x = x - sigma\n\n\t@param x, y - center x and y\n\t@param fwhm - FWHM of Gaussian\n\t@return the gaussian\n\t"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n\t\n\tif not isinstance(x,np.ndarray):\n\t\tx = np.array(x)\n\t\n\ta    = -np.sqrt (\tx[2,1::2] )\n\t\n\tb    = - np.sqrt (\tx[0,1::2] )\n\t\n\treturn 2*(np.pi / (b**2)*a**2)**8 - 1*x[2,1::2] )\n\n\n\n    \n \n \n\n\n\n\ndef fit_gaussian_sequ(x,y,xtest,ytest, sigma):\n\t\"\"\"\n\tGiven the x and"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\nif __name__ == '__main__':\n\t\n\tN = 10\n\tp, y = np.ogrid[0:N, 0:N]\n\tv = 1e-3\n\t\n\tv = p - v\n\t\n\tdef f(x):\n\t\treturn 1/(1+np.exp(-v*((x-p).T)))\t\t\n\tdef g(x):\n\t\treturn (x-p)/p\n\t\t\n\t#f = lambda x: (np.exp(-3*x.T)*g(x))**2\n\tp = 0.7\n\ty1"}, {"generated_text": "import numpy as np\t\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\t\"\"\"\n\t\n\n\t# def get_dist_matrix_r(self, k):\n\t# \tv_r = []\n\t\n\t# \tfor i in range(self.num_samples):\n\t# \t\tv_r.append(np.linalg.norm(self.covariance[i, :] - self.covariance[i, :]))\n\t# \t\n\t# \treturn np.array(v_r)\n\n\t\n\t# def get_distance_matrix_r(self, i, k):\n\t# \tdistance_mat = self.get_distance_matrix"}]}
{"task_id": "62ece4992e6aefcf4aabbd8a", "prompt": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t", "suggestions": [{"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\n\t\t\n\tfilename_regex = re.compile('.*\\.py$')\n\tconfigLines = [open(filename).read()[:-1] for filename in filename_regex.findall(log_file)]\n\tlogging.info('Parsing configuration %s with %d entries' % (len(configLines), len(configLines) - 1))\n\tnumValidEntries = sum(len(d) for d in configLines)\n\t\n\tlogging.info('Checking that all configuration files were specified correctly')\n\t\n\tconfigs_parsed = []\n\tlog_conf_parser = None\n\tfor configName in configLines"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\ndef validate_log_configuration(config, record_list):\n\tresult = {}\n\tfailed = []\n\t\n\t# Go through the set of logging.LogRecord instances that were found\n\t\n\tfor record in record_list:\n\t\t# Parse the configuration file for this record, storing the parsed configuration.\n\t\t# Remove any invalid characters, and then run through the validate routine.\n\t\tfor bad_char in config:\n\t\t\tbad_record = record.replace(bad_char, \"\")\n\t\t\tif bad_record.startswith(\"log_\") \\\n\t\t\t\t or bad_record.startswith(\"parse_\") \\"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\n\n\tlogging.basicConfig( level = logging.DEBUG );\n\tlogname = logname.split('.log')[0]\n \t\n\ttry: \n\t\tcfgfile = open( LOGFILENAME);\n\t\tparsed_config = dict();\n\texcept Exception, e:\n\t\tlogging.getLogger().exception( e )\n\t\traise\n\n\tfor cfgline in cfgfile:\n\t\t# Skip comments\n\t\tif ( len(cfgline.strip()) == 0 ):\n\t\t\tcontinue;\n\n\t\tnewcfg = Config( cfgline );\n\t\tnewcfg.parse()\n\t\t#logging.getLogger().debug("}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\tlog = logging.getLogger(\"ConfigurationParser\")\n\t\tlog.addHandler(logging.NullHandler())\n\t\t# Create configuration file with no errors\n\t\tconfiguration = ConfigurationParserConfiguration({}, self._fileConfig)\n\t\tlog.setLevel(logging.ERROR)\n\t\tconf = configuration._validate(log)\n\t\tif conf is None:\n\t\t\t# Parse of configuration failed - this is okay; just bail\n\t\t\t_logger.error(\"Failed to parse configuration file '%s': %s\" % (self._fileConfig, self._errors.get(conf.message, \"Unknown error '%s'\" % conf."}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\n\n\tlogger = logging.getLogger( __name__ )\t\t# to ensure that only one instance of Logger is ever created.\n\tlogger.info( \"Configuration file parsed as [%s]\" % _LOG_FILE_NAME )\n\n\tconfiguration_filenames = _get_configuration_filenames()\n\ttry :\n\t\tparsed_files = [ f for f in configuration_filenames if re.match( _LOG_FILE_NAME, f ) ]\n\texcept ( AttributeError, TypeError, InvalidTemplate ) as excp:\n\t\tlogger.debug( \"Configuration [%s] were not parsed as expected by configuration"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\n\n\t# define file locations\n\tbasepaths  = path_join(PATH_TO_DATAFILE, 'files')\n\tsamplefiles={}\n\tlogginglogger = logging.getLogger('logging')\n\tparsedfilename=os.path.join(basepaths, 'parsed_file.csv')\n\tparselogfile = os.path.join(basepaths, 'parse_errors.log')\n\tfor fname in os.listdir(genepattern(os.path.join(basepaths, 'parsers', '*.gz'), '*.f90')):\n\t\tlogginglog = logging.getLogger('logs"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\n\n\t__metaclass__ = type\n\t__slots__=['parse_errors','logger']\n\tdef __init__(self,configuration_file_names,logname):\n\t\tself.configuration_file_names=configuration_file_names\n\t\tself.logger=logging.getLogger(__name__)\n\t\tself.logger.info('loading %s configuration file(s): %s' % (self.__class__.__name__,configuration_file_names))\n\t\tself.config_files=[i.strip() for i in configuration_file_names.split(\",\")]\n\t\tself.warnings={}\n\t\t"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\t# Set root logger to the root logger, so that any subsequent calls to setup_logging may see the same logger\n\t\tlogger = logging.getLogger('yandextank')\n\t\tlogger.setLevel(config.loglevel)\n\t\tfor root, dirs, files in os.walk(config.rootdir):\n\t\t\t# Process the configuration\n\t\t\tconfig = configparser.SafeConfigParser()\n\n\t\t\tfor filename in files:\n\t\t\t\t# Skip these files and skip any other files that are not supported by this parser\n\t\t\t\tif filename == 'configuration.ini':\n\t\t\t\t\tcontinue\n\t\t\t\t# Add any errors found in this"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\treturn get_config_dict_from_files(configuration_filenames, logging.ERROR, None)\n\ndef get_config_dict_from_files(filenames, logging_errors, parse_errors = True):\n\t\"\"\"\tLoads the configuration from all configured files (a filename) and\n\treturns a ParseResults namedtuple instance. The ParseResults namedtuple contains the parsed and non-configuration \n\tconfigurations.\n\n\tArguments\n\t----------\n\t\tfilenames: str\n\t\t\ta sequence of configuration filenames\n\t\tlogging_errors: list of strings\n\t\t\ta list of strings, for which a parse error is desired"}, {"generated_text": "import logging\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\t\"\"\"\n\t\tparsed_confs = load_configuration(configpaths, None)\n\t\tparsed_logger = create_logging_instance()\n\t\treturn _combine_parse_errors(parsed_confs, parsed_logger, parsed_parse_errors, parse_parser, parse_logging)\n\texcept Exception as e:\n\t\tparsed_logger = create_logging_instance()\n\t\tparsed_logger.exception(\"Error parsing configuration file %r.\", configpaths)\n\t\tprint(e)\n\t\treturn ([], [], [])\n\nparser = None\n\ndef _parse_args():\n\treturn __parse_"}]}
{"task_id": "62ece4992e6aefcf4aabbd8b", "prompt": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\n\t\n\tdef get_utf8(obj):\n\t\t\"\"\" Gets UTF-8 version of string.\"\"\"\n\t\tl = ''\n\t\tfor c in obj:\n\t\t\tif ord(c) < 128:\n\t\t\t\tutf8 = chr(c);\n\t\t\t\tl += utf8\t\n\t\treturn l\n\t\n\tif not isinstance(obj, bytes):\n\t\tobj = obj.encode(get_utf8)\n\t\n\treturn obj\n\t\n\t\n\n\n# ----------------------------------------------------------\n#\t MAIN\n# ----------------------------------------------------------\n\tlog_to_file=True\n\tlog_to_screen=False\n\tcolor_map"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\n\t\n\tout = StringIO.StringIO()\n\t\t\n\tif hasattr(obj, 'tostring'):\n\t\tout.write( obj.tostring() )\n\t\t\n\treturn out.getvalue()\n\t\n\t\ndef unicode_to_string(u):\n\t\"\"\"Converts unicode ``u`` into a utf-8 ``str`` object.\"\"\"\n\treturn u.decode('UTF-8')\n\t\t\ndef str_to_unicode(s):\n\t\"\"\"Converts unicode ``s`` into utf-8 ``str`` object.\"\"\"\n\treturn unicode(s, 'utf-8')\n\ndef ustring_to_unicode(ustring"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\ndef load_svmlight_file(folder_path_file):\n    try:\n        from scipy.io import loadmat\n    except:\n        print(\"This function requires the scipy python package!\")\n        raise ImportError\n    \n    \n    \n    f = open(folder_path_file, \"rb\")\n    if sys.version >= '3.x':\n        encoding = 'latin-1'\n    else:\n        encoding = None\n    f_info = loadmat(f, encoding=encoding)\n    return f_info\n    \n                      \ndef load_svmlight_file_binary(mat_file_path):\n    try:\n        #f"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\ndef i(bytes):\n\tif isinstance(bytes, (str, bytes)):\n\t\t# if this is a string string, then it is\n\t\treturn bytes\n\t\t# if this is not a string list, then it is an integer\n\t\t# so we try to convert it assuming that it is in the list\n\t\ttry:\n\t\t\treturn int(bytes)\n\t\texcept ValueError:\n\t\t\tif isinstance(bytes[0], (int, float)):\n\t\t\t\treturn bytes\n\t\t\traise TypeError, \"Invalid input: '%s'\" % bytes\n\telse:\n\t\t# just make sure a list or"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\ndef pystr_to_dict(s):\n\tif isinstance(s,str):\n\t\treturn ast.literal_eval(s)\n\telif not s:\n\t\treturn None\n\telse:\n\t\treturn ast.literal_eval(s[1:-1])\n\ndef parse_dict(in_str):\n\t\"\"\"\n\tParses a dict to a list of tuples\n\tThis function is used to parse each entry of a dict\n\tfrom an ini file. \n\tThis is a convenience function for the parsing. \n\t\"\"\"\n\tin_list = []\n\tfor"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\n\n\tif type(obj) in StringTypes:\n\t\treturn obj\n\treturn obj.encode('utf-8')\n\t\ndef read_array(filename):\n\t\"\"\"\n\tThis function reads a file. \n\t\n\t:param filename: The file to read.\n\t:type filename: string (py2 or py3)\n\t:returns: A dictionary containing the contents of the file.\n\t  The dictionary includes ``None`` element (not used).\n\t:rtype: data structure (python dicts)\n\t\"\"\"\n\timport json, os\n\t\n\ttry:\n\t\twith open(filename"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\n\n\treturn obj.encode(encoding)\n\n\t\ndef array2string(a, dtype=None):\n    # from http://stackoverflow.com/questions/22753089/convert-a-1d-array-into-a-csv-file-as-an-array.\n    cols = array([map(lambda x: struct.pack('<2d', int(x)), a.flatten().tolist())\t])\n    return sep.join(map(lambda x: sep.join(map(str, x)), cols))\n\ndef list2string(l, fmt='%ds'):\n    \"\"\""}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\ndef array_len (obj):\n\t\"\"\"\n\t\tGiven a byte array or array of word strings, return the length,\n\t\tinstead of just returning the number of strings.\n\t\"\"\"\n\treturn obj.count(u'\\u00C0') + 1\n\t\ndef bytes_len (obj):\n\t\"\"\"\n\t\tGiven a byte array or array of word strings, return the length,\n\t\tinstead of just returning the number of strings.\n\t\"\"\"\n\treturn obj.count(u'\\u00' + obj) + 1\n\t\ndef u(byte):\n\t\"\"\"\n\t\t"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t return open(StringIO(obj),'rb').read().decode(\"utf-8\")\n\ndef npfloat(obj):\n\t\"\"\"\n\tThis function returns the numpy float value corresponding\n\tto ``obj`` in case it is a string using UTF-8. \t\"\"\"\n\t return pynp.nan_to_num(obj)\n\ndef npint(obj):\n\t\"\"\"\n\tThis function returns the numpy integer value corresponding\n\tto ``obj`` in case it is a string using UTF-8. \t\"\"\"\n\t return pynp.int_to_num(obj)\n\n"}, {"generated_text": "import numpy\t\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \t\"\"\"\n\t\ndef ascii(s, encoding=None):\n    \"\"\"\n    Return the string in ASCII encoding or return the original ``s``\n    bytes object (``s.encode(\"string_escape\")``).  On Python 2:\n\n      >>> from cStringIO import StringIO\n      >>> s = StringIO(\"string with embedded NUL\")\n      Using Unicode Strings:\n      >>> import numpy\n      >>> print ascii(s, \"UTF-8\")\n      \\\\\\\\\"string with embedded NUL\\\\\"\"\n      >>> s.encoding\n      'utf-8'\n      >>> s\n      <string with embedded NUL and \\\\\\\\\"string with embedded NUL\\\\\">\n\n    Similarly,"}]}
{"task_id": "62e60723d76274f8a4026b76", "prompt": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\tformatter = cls.ticks_formatter\n\t\tif not ticks < 1:\n\t\t\traise ValueError(\"ticks should be >= 1\")\n\t\t\n\t\t# todo - can it be less flexible here with dates\n\thours = divmod(ticks, 24 * 60*60)\n\tminutes = divmod(ticks, 60*60)\n\tseconds = secs = 0\n\n\tparts = []\n\tparts.append(int(hours))\n\tparts.append(int(minutes))\n\tparts.append(int(seconds))\n\n\tdays_in_month = 60 * (ticks % 1)\n\tdays_in"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n\n\tn = len(ticks)\n\tassert n>=0, \"ticks are out of bounds\"\n\t# we use the \"right\" ticks of the \"right\" subtraction as\n\t# described in Wikipedia.\n\t#\n\t# So the left ticks of \"right\" subtraction are the number of ticks\n\t# that appear in the \"left\" subtraction.\n\t#\n\t# Note that 'right'' is in the same direction as the \"left\" subtraction.\n\t#\n\t# We could have had to add a check for this because that doesn't\n\t"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n    # TODO: handle leap seconds!\n#\treturn Time(decimals * (1e9 / float(1000))) - TimeDelta(ticks)\n\ndef to_decimal(t, tz=None):\n    \"\"\"\n   Convert datetime.datetime to decimal.Decimal.\n   \"\"\"\n    return to_datetime(t, tz=tz)\n\n\n# TimeDelta\n\ndef utcnow():\n  \"\"\" Return current time in UTC time zone.\"\"\"\n  return datetime.datetime.utcnow()   # utc in seconds\n\ndef utcnowf():\n  \"\"\"Return current time in local timezone without timezone.\"\"\"\n  return datetime.timedelta"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\treturn None\n\n\tif not isinstance(ticks, int) or (ticks < 0):\n\t\traise ValueError(\"ticks must be an integer >= 0\")\n\n\tticks = abs(ticks)\n\n\ttry:\n\t\tminutes = ticks / 60.\n\t\tseconds = ticks % 60\n \t\tresult = datetime.time(minutes, seconds, tz)\n\texcept OverflowError:\n\t\texception = ValueError(\"ticks must be bigger than 0\")\n\texcept OverflowError:\n\t\texception = ValueError(\"ticks must be bigger than %d (1,999)\\n\" % (ticks))\n\texcept ValueError as e:"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\t# no limits, or ticks below the midnight (which is just a timestamp)\n\t\tticks = time.mktime((ticks % 1e6) - seconds.TimeDelta(0, tz=tz))\n\tif not 0 <= ticks <= 100000000:\n\t\traise ValueError('ticks outside range(0 <= ticks < 86400000000000)')\n\n\treturn cls(ticks)\n\n\ndef from_floats(cls, floats, tz=None):\n\t\"\"\"\n\tCreate a time from a float list.\n\n:param floats: list of floats to create time from\n:type floats: list"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\traise NotImplementedError\n\n\tsecs = float(ticks % 10)\n\tsecs += (ticks // 10000) * 10\t# add 10 and use 10*16 bit precision\n\treturn cls(\n\t\tsecs=secs,\n\t\tmicroseconds=(tick * 10.**6).quantize(Decimal('.1') * Decimal('.1'), rounding=ROUND_HALF_UP),\n\t\tticks=ticks\n\t\t)\n\ndef to_ticks(ticks, tz=None):\n\t\"\"\"\n\tConvert the time interval to a tick count.\n\n:param ticks: the tick count, in milliseconds.\n:type ticks"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n\n\tif isinstance(ticks, (int, float)):\n\t\t\t\tif ticks < 0 and ticks > 2000000000:\n\t\t\t\t\traise ValueError(\"ticks: %f sec < 2000000000\" % ticks)\n\telse:\n\t\t\t\tticks = None\n\n\t\thours = 0\n\t\tminutes = 0\n\t\tseconds = 0\n\t\tfor sec in ticks:\n\t\t\t\tif sec is None: continue\n\t\t\t\tif abs(sec - (days * 24 * 3600)) < 0.00000001:\n\t\t\t\t\t# If it's 2-hour earlier, don't allow minutes to be\n\t\t\t\t\t# ignored, but do not include minutes here ("}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\tif isinstance(cls.dtype, np.datetime64):\n\t\t\tif tz is None: tz = offsets.Nano()\n\t\t\treturn cls.from_datetime64(ticks, tz)\n\telif isinstance(cls.dtype, np.timedelta64):\n\t\tif tz is None: tz = offsets.Nano()\n\t\treturn cls.from_timedelta64(self.dtype, tz)\n\traise ValueError(\n\t\t\"{cls.__name__} is not a supported Time\".format(cls=cls.__name__)\n)\n\n\n\n# Copyright: (c) 2013, Jordan B"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\n\n\t# TODO: is this really allowed?\n\ttry:\n\t\tticks = int(ticks)\n\texcept ValueError:\n\t\traise ValueError(\"invalid ticks\")\n\n\t# convert tick as an int, then to a float.\n\tticks = ticks * 10.0**6.2831237\t\t\t\n\n\t# if a UTC offset is passed in, use that as UTC date time\n\tif isinstance(ticks, int):\n\t\treturn datetime.utcfromtimestamp(ticks)\n\telif isinstance(ticks, float):\n\t\treturn ticks\t\n\n\t# otherwise, assume the clock is UTC by default\n"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\t\"\"\"\n\t\ndef get_seconds_for_tick(ticks):\n\t\"\"\"\n\tReturn a unix time from tick time.\n\n\t:param ticks: unix epoch seconds since midnight\n\t:type ticks: int\"\"\"\n\n\tbase_time = timestamps.get_unix_time_tz()\n\tdelta = base_time - timestamps.EPOCH\n\n\tif delta.days > 1:\n\t\tdelta = delta + datetime.timedelta(days=delta.days)\n\n\tvalue = (ticks - 1) * 86400000000000 / delta.days\n\n\tif seconds is None:\n\t\tseconds ="}]}
{"task_id": "62e60873d76274f8a4026bd8", "prompt": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\tversion = protocol_version\n\t\tif not isinstance(version, tuple):\n\t\t\traise TypeError(\"protocol version must be tuple e.g. (3, 5)\")\n\t\thandlers = dict((protocol_version, cls) for protocol_version in version)\n\t\thandlers.setdefault(protocol_version, lambda: _handlers[protocol_version])\n\telse:\n\t\thandlers = dict(_handlers)\n\treturn handlers\n# Licensed under a 3-clause BSD style license - see LICENSE.rst\n# -*- coding: utf-8 -*-\n\"\"\"\n\n\"\"\"\nimport warnings\n\nimport numpy as"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t    \n\n\tif isinstance(protocol_version, (tuple, NoneType)):\n\t\tprotocol_version = protocol_version[0]\n\tassert protocol_version is None or \\\n\t\tisinstance(protocol_version, int)\n\n\tif protocol_version is None:\n\t\traise TypeError(\"protocol_version not supplied\")\n\t\n\t# convert to dict\n\tif isinstance(protocol_version, (list, tuple)):\n\t\tprotocol_version = dict([(v,k) for (k,v) in protocol_version])\n\t\n\treturn protocol_version\n\n\nclass Message(object):\n\t\"\"\"\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n\t\n\tversions = {}\n\tversion = version or '3'\n\ttry:\n\t\tversion = tuple(version.split('.'))\n\texcept AttributeError:\n\t\tversion = (3, 5, None)\n\tif version >= version:\n\t\traise TypeError('unsupported protocol version %s' % version)\n\t\n\thandlers = {}\n\t# get version spec in the format:\n\t#  major, minor, micro: minor, micro\n\tmatch = re.match(r'major\\d+\\.(b|rc)(?:rc)?', version)\n\tif match:\n\t\tmajor ="}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\tversions = protocol_map.keys()\n\n\t\tif version not in versions:\n\t\t\traise NameError(\"Protocol version %s not available\" % (str(version)))\n\t\tif version <= 0:\n\t\t\traise ValueError(\"Protocol version %d not available\" % (str(version)))\n\n\t\tif version > len(versions)-1:\n\t\t\traise ValueError(\"Protocol version %d not available\" % (str(version)))\n\n\t\thandler_class = protocol_handlers.get((versions[version-1], cls))\n\t\t\n\t\tif handler_class:\n\t\t\treturn handler_class(protocol_version)\n\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t    \n\tret = {}\n\tfor cls_dict in cls.all_protocols(protocol_version):\n\t\tret.update(cls_dict)\n\treturn ret\n\ndef protocol_version(protocol_version_str):\n\t\"\"\"\n\tGiven a protocol version string, return either\n\tan integer for version \"3\" or the special string \"5\".\n\t\"\"\"\n\tmatch = __regex_from_version__(protocol_version_str)\n\tif not match:\n\t\treturn match\n\tversion_tuple = match.groupdict()\n\tversion_tuple = normalize_version"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n    global _protocol_handlers\n    if protocol_version is None:\n        protocol_version = tuple(\n            _protocol_handlers.iterkeys())[-1][0] # most recent version supported\n    protocol_version += 1\n    \n    d = dict()\n    for idx, version in enumerate(protocol_version):\n        d[str(idx+'-v')] = version\n\n    return d\n\"\"\"\nThis module contains the general information for Branches classes.\n\"\"\"\nimport abc\nimport collections.abc\n\n\nclass AbstractBranchable(Exception):\n    \"\"\"\n    An abstract branchable object that can be included in a project."}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n    if protocol_version is None:\n\t\tprotocol_version = (3, 5)\t# no protocol version here\n\t\n\tif protocol_version >= (3, 6):\t\t# Protocol version 5\n\t\t\n\t\treturn {\n\t\t\t'_protocol_{}'.format(protocol_version[0]): protocol_handler,\n\t}[protocol_version[0]];\n\n\treturn None;\n\n\ndef register_protocol_handler(handler):\n\t\"\"\"Add a function to handle protocols that require support.\"\"\"\n\tfor version in protocol_handlers(handler.protocol_version):\n\t\tif handler.protocol_version in"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n    if protocol_version is None:\n        protocol_versions = {\n            (3, 5, True): protocol_handler('3.5', True, None),\n            (3, 5, False): protocol_handler('3.5', False, None),\n            (3, 5, None): protocol_handler('3.5', None, None),\n            (3, 5, 'default'): protocol_handler('3.0', None, None),\n        }\n    else:\n        if protocol_version not in protocol_versions:\n            raise TypeError(\n                \"invalid version value {} for bolt configuration\".format("}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\treturn dict(_iteritems())\n# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2011 OpenStack Foundation\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\t\"\"\"\n\t\n    # check protocol version\n    if not protocol_version:\n\t    protocol_version = (3, 5) if protocol_version is None else (protocol_version,)\n\n    handler_classes = {'ping':'ping',\n                       'get_server':'get_server_version',\n                       'get_version':'get_server_version',\n                       'get_protocol_version':'get_protocol_version'}\n\n    if protocol_version in handler_classes.keys():\n        return handler_classes[protocol_version]\n    elif protocol_version is None:\n\t    raise Exception(\"missing protocol version: {0}\".format(protocol_version"}]}
{"task_id": "62e60e3bd76274f8a4026d1a", "prompt": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tfor bookmarks in values:\n\t\t\ttry:\n\t\t\t\treturn cls(\n\t\t\t\tfrom_raw_bookmark_string(bookmarks['bookmark']), \n\t\t\t\tbookmarks['type'], \n\t\t\t\tbookmarks['data'],\n\t\t\t\tbookmarks.get('metadata', None))\n\t\t\texcept KeyError as err:\n\t\t\t\traise KeyError(err)\n\t@staticmethod\n\tdef from_raw_bookmark_string(raw_bookmark_string):\n\t\treturn Bookmarks.from_raw(raw_bookmark_string, Bookmarks.METADATA_VERSION)\n\n\nclass Bookmark(object):\n\t\"\"\"\n\tAbstract base class for bookmark-"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t          # TODO\n#\timport time\n#\n#\timport math\n#\n#\t# create a \"raw\" bookmark file\n#\ttry:\n#\t\tf1 = open(values[0]).read()\n#\t\tf1 = bytes(f1, 'latin-1')\n#\t\tf1 = f1.replace('\\\\x', '\\\\n')\n#\t\tf1 = f1[2:]\n#\t\ti1 = f1.lower().find('\\\\x')\n#\t\tf1 = f1[:i1] + '\\\\' + f1"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\ttry:\n\t\t\tv = cls(list(values))\n\t\texcept TypeError as e:\n\t\t\traise TypeError(\"Value of 'Values' parameter must be str/unicode: %s\" % str(str(e)))\n\t\tv._values = tuple(list(v.values()))\n\t\treturn v\n\n\tdef save(self):\n\t\t\"\"\"Write the bookmarks (and bookmarks file) to disk.\n\t\tPass the bookmark filenames in set first.\"\"\"\n\t\td = join(self.dir, \"%04i.bookmarks\" % self.index)\n\t\tassert os.path.exists(d),"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\treturn cls._from_obj(values)\n\"\"\"\nProvides a client to the Flickr API.\n\"\"\"\n\n# Author: Prabhu Ramachandran <prabhu_r@users.sf.net>\n# Copyright (c) 2005, 2006, 2007, 2008, 2009, 2010 HUDORA <h.taka@gmail.com>\n#\n# This file is part of h.Tigers.\n#\n# h.Tigers is free software; you can redistribute it and/or modify\n# it under the terms of the GNU Lesser"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tfor bookmark_string in values:\n# TODO: implement this to validate values...\n\t\t\t\tbookmark_text, bookmark_size = bookmark_string.split(',',1)\n\n\t\t\t\tif bookmark_size == '':\n\t\t\t\t\traise EmptyBookmarksException('Empty bookmark: ', bookmark_string)\n\t\t\t\telse:\n\t\t\t\t\tbookmark = Bookmark(int(bookmark_size), bookmark_text)\n\t\t\t\t\tcls.raw_bookmarks.append(bookmark)\n\n\t@classmethod\n\tdef from_xml(cls, xml_string):\n\t\t\"\"\"\n\t\tTake a Bookmarks object from XML (which is a list of"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tif values is None:\n\t\t\traise ValueError(\"bookmarks must be a list of strings\")\n\n\t\tif isinstance(values, str):\n\t\t\tvalues = [values]\n\n\t\titems = []\t\n\t\tfor bookmark_str in values:\n\t\t\tif bookmark_str == '[Bookmark]':\n\t\t\t\titems.append([])\n\n\t\t\titems.append(Bookmark())\n\n\t\tif len(values)!= len(items):\n\t\t\traise ValueError(\"bookmark string of {} values must have \" \\\n\t\t\t                           \"length equal to len ({}!= {})\".format( \\\n\t\t\t                                values, len(values), len("}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tobj = cls()\n\t\tfor char in values:\n\t\t\tif char in cls.ASCII_VALUES:\n\t\t\t\tobj.add_raw_bookmark(char)\n\t\t\telse:\n\t\t\t\tobj.add_bookmark(char)\n\t\tobj.dirty.changed.connect(partial(setattr, obj, 'dirty', False))\n\t\treturn obj\n\n\t@classmethod\n\tdef from_bookmark_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from an array of bookmark strings values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\treturn cls(values)\n\n\tdef parse_line(self, line):\n\t\tline = line.strip()\n\t\tif line.startswith('#'):\n\t\t\treturn\n\t\tline = line.expandtabs()\n\t\tif len(line) == 0:\n\t\t\treturn\n\n\tdef merge(self, other):\t\t\t# (self, other) must be an iterable\n\t\tfor bookmark in other:\t\t\t\t# self & other must be an iterable\n\t\t\tself.bookmarks.extend(bookmark.bookmarks)\n\t\t\tself.bookmarks.extend(other.bookmarks)\n\n\t@property\n\tdef bookmarks("}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tif isinstance(values, str):\n\t\t\tvalues = [values]\n\t\telif not isinstance(values, Iterable) and values == [] and len(values) == 0:\n\t\t\t# This should never happen.\n\t\t\tlogger.warning(\"Invalid bookmark values: \\\"%s\\\"\"%values)\n\t\t\traise BookmarksError(\"Invalid bookmark input\")\n\t\tfor bookmarkstr in values:\n\t\t\tbookmarkstr = bookmarkstr.split(\",\")\n\t\t\tbookmarkstr = b','.join(map(b64encode,bookmarkstr))\n\t\t\tlogger.debug(\"Bookmark string parsed:\\n%s\" %bookmarkstr)\n\t\t"}, {"generated_text": "def from_raw_values(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]\t\"\"\"\n\t\tvalues = values or ()\n\t\traw_values = []\n\t\tfor val in values: raw_values.append(Bookmark(val))\n\t\traw_values = tuple(raw_values)\n\t\treturn raw_values\n\n\t@classmethod\n\tdef from_pairs(cls, values):\t\"\"\"\n\tCreate a Bookmarks object from a list of (bookmark, index)\n\tstrings. If index does not exist, create one.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII strings values, in a format that can"}]}
{"task_id": "62b87b199a0c4fa8b80b354c", "prompt": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\n#\tif isinstance(seq, tuple) or isinstance(seq, list):\n#\t\tif len(seq) == 1:\n#\t\t\treturn seq[0], type\n#\t\telif len(seq) == 2:\n#\t\t\tif seq[0] in _seq_types:\n#\t\t\t\treturn seq[1], type\n#\t\treturn seq\n\telif isinstance(seq, types.GeneratorType):\n\t\ttry:\n\t\t\treturn seq.__iter__(), (type(seq.next()), type(seq.next()))\n\t\texcept StopIteration:\n\t\t\treturn seq.__"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\n#    kind, data, pos = _get_pos(data)\n##    kind, stream, pos = _get_pos(data)\n##    return (kind, stream, pos) if kind else (kind,)\n\ndef _get_pos(data):\n\t\"\"\"\n\tMove *data* on to the end of the stream of len 'data'.\n\tReturns the pos of the longest common prefix of *data*.\n\t\"\"\"\n##    if not data:\n##        return None, 0, 0\n##    else:\n##        pos = bisect_right(data"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\tif bufsize is None:\n\t\t\tfor item in reversed(seq):\n\t\t\t\tif type(item) is not type(0):\n\t\t\t\t\tbufsize = -1\n\t\t\t\telse:\n\t\t\t\t\tbufsize = 1\n\t\telse:\n\t\t\tbufsize = int(bufsize)\n\t\tif bufsize < 1:\n\t\t\tfor item in seq:\n\t\t\t\tif type(item) is not type(0):\n\t\t\t\t\treturn item\n\t\t\t\telse:\n\t\t\t\t\treturn (item, type(0))\n\t\telse:\n\t\t\treturn (seq, type(0))\n\n\tif isinstance(object, type):\n\t\t# XXX We"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\t# First we tokenize.  See docstring for SequenceType.__new__\n\t\ttry:\n\t\t\tif buf == '<':\t# <-- The calling sequence is already running.\n\t\t\t\tresult = ('>', '')\n\t\t\t\tmsg = \"expected %x chars; got %x\" % (result, seq)\n\t\t\t\traise _GiveupOnException(msg)\n\t\t\twhile 1:\n\t\t\t\tmid = seq.find('> ', 0, (2*bufsize) - 1)\n\t\t\t\tif mid:\n\t\t\t\t\t# Found the next chunk.\n\t\t\t\t\tbreak\n\t\t\t\tresult += mid\n\t\t\t\tif seq[mid-"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\tif bufsize is None:\t# if bufsize is None, reap the buffered\n\t\t\tchunk = seq.popleft()\n\t\t\twhile len(chunk) < bufsize:\n\t\t\t\tchunk += get(chunk)\n\t\t\treturn (chunk, type(seq)())\n\n\t\t# else we have a string\n        else:\n\t\t\tlen_str = len(seq)\n\t\t\tif not (-1 << len_str) <= len_bytes: \n\t\t\t\traise ValueError(\"length is too small\")\n\t\t\t\n\t\t\tif len_str < bufsize: \n\t\t\t\traise ValueError(\"length is too big\")\n\t\t\t\t#return (seq, type"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\n\n\tif bufsize is not None:   # output may be shared with any amount\n\t\tl = len(seq)\n\t\tif bufsize < 0:               # new in Python 2.2\n\t\t\tpipe = seq\n\t\telse:\n\t\t\tpipe = seq[0:bufsize]\n\t\tif l == 1:               # compressed output: copy it to STDOUT\n\t\t\treturn types.frozenarray(\n\t\t\t       [types.frozenarray(\n\t\t\t\t\t\t\t(\n\t\t\t\t\t\t\t   (\n\t\t\t\t\t\t\t\t\ttypes.EINTR,\n\t\t\t\t\t\t\t\ttypes.EAGER,\n\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\t(pipe, types.EINTR)\n\t\t\t\t\t\t\t"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\n\n\tif bufsize is None:\n\t\tx = ()\n\telse:\n\t\twarnings.warn(\n\t\t\t\"bufsize=None is currently ignored. \"\n\t\t\t\"Use.seek() instead.\",\n\t\t\tDeprecationWarning\n\t\t\t)\n\t\tx = bufsize\n\n\treturn ((seq, type(None)) + x)\n############################################################\n# Example to generate a simple CRS/JSON.\n############################################################\n\nimport argparse\nimport os\nimport copy\nimport re\nimport shutil\nimport subprocess\nimport sys\nimport tempfile\nimport textwrap\n\n\ndef parse_args():\n    \"\"\"Parse the command line arguments."}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\n\t\n\tif bufsize is None:\n\t\tfor item in seq:\n\t\t\tif type(item) is StringType:\n\t\t\t\tif len(item) > 2:\n\t\t\t\t\traise ValueError, \\\n\t\t\t\t\t\t\"sequence element too long\"\n\t\t\t\telse:\n\t\t\t\t\treturn (item, type(item))\n\t\telse:\n\t\t\treturn (seq, type)\n\t\n\titem = seq.next()\n\tif item == '':\n\t\tif type(seq) is TupleType:\t# sequence with one component\n\t\t\treturn seq[0]  # items are always tuples\n\t\traise ValueError, \\\n\t\t\t\t\"there"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\n\t\n\tif bufsize is None:\t\t# default of getbufsize()\n\t\treturn seq, 1\n\t\n\tif len(seq) == 1:\t\t# only one item\n\t\treturn seq[0], 0\n\t\n\tif bufsize is None:\t\t# choose some sensible default\n\t\treturn (seq[0], 1)\n\t\t\n\tfor one in seq[:bufsize]:\t# pick arbitrary type\n\t\tif one:\n\t\t\treturn (one, 0)\n\t\n\traise RuntimeError (\"sequence or length too long\")\n\n##  Sequence protocol\n##\n\nclass Sequence:\n\t\"\"\"Data structure which holds a"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\t\"\"\"\n\t\tif bufsize is None:\n\t\t\tbufsize = len(seq) // 2\n\t\tif seq is None or len(seq) < bufsize:\n\t\t\treturn ()\n\t\tresult = zip(seq, seq)\n\t\tif not is_string(result):\n\t\t\tif hasattr(result, 'next'):\n\t\t\t\tresult = result.next()\n\t\t\telse:\n\t\t\t\treturn result[0]\n\t\treturn result\n\t#\n\tdef get_seq_type(self):\n\t\treturn self._seq_type\n\t#\n\t# Return the raw data sequence type of the sequence.\n\t#"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3581", "prompt": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\timport sys\n\t\tif other is not None:\n\t\t\traise TypeError('rescale method was overridden')\n\t\trescale = self.rescale_max()\n\t\tsys.stderr.write(\"Recomputed histogram has no scale so rescaleing...\")\n\t\tsys.stderr.write('{} rescale is {} ({} samples)\\n'.format(\n\t\t\trescale, len(self),\n\t\t\t'scale' if rescale > 1 else 'truncate'\n\t\t))\n\t\tfor bin in self.bins():\n\t\t\tother_hist = self.rescale(other=bin, recompute=recompute"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\n#****************************************************************************\n# Copyright 2002, 2003, 2004.\n#\n# This file is part of (OOIA) software.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t     \n\t     \n\t     \n\t     \n\t     \n\t     \n\t# check this works correctly before calling the\n\t# superclass's __init__\n\tif other == None:\n\t\tthisscale = self.scale();\n\t\tif len(self) == 1:\n\t\t\tif other.scale() == 0:\n\t\t\t\treturn\n\t\t\telse:\n\t\t\t\traise ValueError(\"Cannot scale: zero-lenth-by-1 series.\")\n\t\telse:\n\t\t\treturn thisscale\n\n\telif isinstance(other, (int, float, long)):\n\t\t# Check for zero\n\t\tif other == 0.0 and"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tif other is None:\n\t\t\traise ValueError(\"cannot rescale empty histogram\")\n\t\treturn self\n\t\n\treturn float(self * other) / other\n\n\ndef cummax(values, weights = True):\n\t\t\"\"\"\n\tCompute and return the cummax of the given values.\n\n:param *values: A sequence containing the values to be used.\n:param int weights: If `True` averages `values` exactly only by\n\t\t       multiplying the average of values, if `False` sums\n\t\t       corresponding to each other.\n\t\n\t\"\"\"\n\t\tif weights:\n\t\t\t# `values"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tif other is None:\n\t\t\traise ValueError('Cannot scale an empty histogram.')\n\t\tself._data *= other.scale(1. + other.count())\n\t\n\t@staticmethod\n\tdef _hist_name_to_value(hist_name):\n\t\t\"\"\"\n\t\tConvert a histogram name to a float.\n\t\t\"\"\"\n\t\tif hist_name in ['', None]:\n\t\t\treturn None\n\t\ttry:\n\t\t\treturn float(hist_name)\n\t\texcept ValueError:\n\t\t\ttry:\n\t\t\t\treturn float(hist_name[0])\n\t\t\texcept ValueError:\n\t\t\t\treturn float(hist"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\n        if other is None:\n                        return self\n                  \n        if (self.get_scale() <= 0. and self.get_scale() > 1.+other.get_scale()) or \\\n                        (abs(self.get_scale()) < \\\n                         abs(other.get_scale())):\n                    raise lena.LenaValueError(\n                        \"Can't scale: %s is not within %s\" % (\n                            self, other))\n                        \n        else:\n# TODO: Check if it still exists here.\n            self.set_scale(other)\n\n        return self\n\n    def get_scale(self):\n        \"\"\""}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tif other is None:\n\t\t\tother=self.__class__\n\t\tif numpy.dtype(other)!=numpy.dtype(self).dtype:\n\t\t\traise ValueError(\"Can't recompute Histogram: %s!= %s\" % (numpy.dtype(self).dtype, other.dtype))\n\t\tif not hasattr(other,'_hist'):\n\t\t\traise ValueError(\"Can only rescale Histogram object to scalar\")\n\t\telse:\n\t\t\t_hist=self.__array_wrapper__.hist(other._hist,self.shape)\n\t\t\t_hist.scale(None,recompute=True"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tif not hasattr(other, '__array__'):\n\t\t\traise ExponentialScaleException(\"No other object for \"\n\t\t\t\t\t\t\t\t \"_scale: {!r}\".format(type(other)))\n\t\tfor obj_index, obj in enumerate(self.__array__):\n\t\t\tif hasattr(obj, '_scaled_history'):\n\t\t\t\tobj._log_scale(other, obj_index, obj.scale_factor + other.scale_factor)\n\n\tdef _recompute_other(self, other, recompute=False):\n\t\tif not hasattr(other, '__array__'):\n\t\t\traise exponentialScaleException(\""}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tif other is None:\n\t\t\treturn self\n\t\treturn self.__class__(\n\t\t\tself._data/other.data,\n\t\t\tself.scale(False)/(other.length+other.length))\t\n\tdef histogram(self, n=256):\n\t\t\"\"\"Compute a histogram of the values in this histogram.\"\"\"\n\t\t# n: integer; number of bins\n\t\t# rescale: if True, rescale (decrease the density)\n\t\tr = range_histogram(self, n)\n\n\t\t# rescale: return the histarization (as opposed to lowster)\n\t\t"}, {"generated_text": "def scale(self, other=None, recompute=False):\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\t\"\"\"\n\t\tnew_counts = self * 1.0\n\t\tself._nbins = (self._min, old_max + 1)\n\t\tfor j in range(len(self)):\n\t\t\tassert self[j] < new_counts[j]\n\t\t\tnew_counts[j] = self[j]\n\n\t\t\tif j > 0:\n\t\t\t\tassert new_counts[j] < old_counts[j]\n\n\t\tif recompute:\n\t\t\tself._normalize()\n\n\t\tif other is not None:\n\t\t\tassert self._nbins\n\t\t# Invalidate both the histogram counts is the same"}]}
{"task_id": "62b87b519a0c4fa8b80b3583", "prompt": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif other is not None:\n\t\t\tself.ensure_2d()\n\t\t\tself.n = self.m.a.shape[1]\n\t\t\tself._check_len()\n\t\t\trescale = (self.f * other).sum()\n\t\t\tif rescale == 0.0:\n\t\t\t\traise ValueError(\"Incompatible dimensions: %f\" % self.m.a.shape[1])\n\t\t\telif rescale == 1.0:\n\t\t\t\tself.n /= rescale\n\t\t\telif rescale == 2.0:\n\t\t\t\tself.n = min(1, self.n"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif other is None:\n\t\t\traise ValueError('Cannot scale a non-None object.')\n\t\tassert_(isinstance(other, self.__class__)\n\t\t\tand other._graph is self._graph\n\t\t\tand other._scale is self._scale)\n\t\treturn self._graph.scale(map(self.__getattribute__, ['x', 'y', 'z']),\n\t\t\t     other._scale + map(self.__setattr__, ['x', 'y', 'z']))\n\n\t\t\n### Private attributes, with overriden __str__()\n#\n#@set_creation_time\n#\n#@{\n#@}\n#@"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\treturn Graph.new_scale(self, other)\n\n\t#\n\t# get number formats, and unpack\n\t#\n\n\t@property\n\tdef size(self):\n\t\t\"\"\"The ``Size`` and ``PointSize`` objects. This represents the\n        total number of elements in the :class:`~graph_tool.layout.Graph`.\n\n\tReturns:\n\t    A tuple defining the number of elements in the graph as it is\n\t    multiplied by all ``PointSize`` objects.\n\t\"\"\"\n\t\ttotal = 1\n\t\tfor (pt, size) in self._edges:\n\t\t\ttotal *= pt."}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tself.__check_undirected()\n\t\tif other is not None:\n\t\t\tif (isinstance(other, (int, long))\n\t\t  or isinstance(other, basestring)):\n\t\t\t\tother = nx.get_node_attributes(other)\n\t\t\telse:\n\t\t\t\tother = nx.get_node_attributes(other)\n\t\tnx.set_edge_attributes(nx.get_edge_attributes(other))\n\t\tif self.__is_directed():\n\t\t\tnx.set_node_attributes(nx.get_node_attributes(self))\n\n\t@property\n\tdef is_directed"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif other is not None:\n                return Graph(self.copy().scale(other))\n        else:\n                raise ValueError, \"scale(-x, -x).scale(y) for z\"\n\n###################################################\n# Graphs\n###################################################\n\n## Graph functions:\n\ndef nodes_of(self, node_name):\n        \"\"\"\n\tRetrieve the nodes associated with *node_name*.\n<p>\n\tReturn all the nodes that belong to the *node_name*. If a node\n\talready has node *name*, then return ``None``.\n\t\"\"\"\n\tnodes = []"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t self.scale_xy(other)\n\t\treturn self\n\tdef scale_x(self, other=None, other2=None):\t#TODO check the type of other?\n\tdef scale_y(self, other=None, other2=None):\t#TODO check the type of other?\n\t\tif other == None:\n\t\t\tother = self\n\t\tassert other == None or isinstance(other, self.__class__)\n\t\tassert other2 == None or isinstance(other2, self.__class__)\n\t\treturn other.scale_x(other2)\n\tdef scale_z"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tret_val = GraphView(len(self), name ='scale')\n\t\tif other is not None:\n\t\t\tret_val.__graph__['view'] = other.view\n\t\telse:\n\t\t\tret_val.__graph__['view'] = self.view\n\t\tfor col in ('x', 'y', 'z'):\n\t\t\tself[col] = (self[col] - other[col]) / (other[col] - 1)\n\t\treturn ret_val\n\n######################################################################\n#   Graph Views\n######################################################################\n\nclass View(GraphView):\n\t\"\"\"\n\tThe"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\t# This property cannot be removed, as otherwise a warning is emitted\n\t\t# when an attempt is made to set something to a property value.\n\t\tpass\n\t\n\tdef _copy(self):\n\t\treturn Graph(_util.refcopy(self))\n\ndef in_edges(in_self, nodes):\t\treturn tuple(True if in_self.has_edge(*i).has_node(*i) else False\n\t\t\tfor i in range(1, len(nodes)+1))\n\ndef out_edges(out_self, nodes):\t\treturn tuple(True if out_"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif other is None:\n\t\t\traise ValueError(\"You must specify a value for `other`\")\n\t\tif type(other) is not Graph:\n\t\t\traise TypeError(\"This is not a `Graph` object.\")\n\t\treturn self._scale(other)\n\n\tdef offset(self, x, y):\t# FIXME is there's no point in raising errors\n\t\t\"\"\"\n\t\tShift the graph by ``x - y``\n\n\t\t>>> a = Graph()\n\t\t>>> a.insert(30)\n\t\t>>> b = Graph()\n\t\t>>> b.insert(20)\n\t\t>>> c = Graph"}, {"generated_text": "def scale(self, other=None):\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\t\"\"\"\n\t\tif other is None:\n\t\t\tother = self\n\t\t\trescale = False\n\t\telse:\n\t\t\trescale = True\n\t\treturn Graph.Scale(self, other, rescale)\n\n\tdef isScaled(self):\t\treturn self._isScaled\n\tdef isSize(self):\treturn self._isSize\n\tdef isHorizontallyDecomposed(self, other):\treturn self._isHorizontallyDecomposed | isSize(self)\n\n    def invert(self, other):\treturn Graph.Invert(self, other)\n\n\t@property\n\t"}]}
{"task_id": "62b87b869a0c4fa8b80b35e1", "prompt": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\thist = hist.with_context(make_value=make_value)\n\t\t\n\t\tbin_width = hist.bin_width\n\t\tbin_contents = make_value(hist.bin_contents)\n\t\t\n\t\taxis_label_align = make_value(\"{:s}px\".format(get_coordinate), \"center\")\n\t\taxis_label_padding = make_value(\"{:5}px\".format(axis_label_align), \"\")\n\n\t\thist_line = make_value(\"x\", bin_contents[0], bin_width)\n\t\thist_line2 = make_value"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\tif not isinstance(hist, Hist):\n\t\t\traise TypeError(\"hist must be a Hist object, got a %s instead\" % repr(hist))\n\tif not make_value:\n\t\tmake_value = hist.get_scale()\n\n\t# convert the values to a histogram object using object's method\n\thist_bin_values = get_hist_bin_values(hist, make_value)\n\n\t# compute the bin content (if desired)\n\tif hist_bin_values is not None:\n\t\thist_content = make_value(hist_bin_values"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\t# NOTE: This is an optimization, it could just be replaced with an\n\t\t# internal function that would be like to check that the value is\n\t\t# valid\n\n        hist_min = max(hist - min(hist))\n        hist_max = min(hist + max(hist))\n\n        if make_value:\t\t\t\n\n            hist_edges = hist.value_counts()\n            hist_values = hist.value_counts()\n\n            # If there was only one value, then just use those\n            if len(hist_values) == 1:\n                axis = 0\n                values = hist_values[0]\n\n               "}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\tprint(make_value.__doc__)\n\n\t\t# Check for single argument\n\t\tif make_value in (True, False):\n\t\t\taxis = make_value\n\t\telif make_value is None:\n\t\t\traise Exception(\"One of the inputs is None.\")\n\t\telse:\n\t\t\taxis = make_value[0]\n\n\t\tif axis is None:\n\t\t\t# No defaulting, get_coordinate function is default\n\t\t\taxis = Axis()\n\n\t\taxis.x = make_value[1]\n\t\taxis.y = make_value[2]\n\n\t\thist = hist(axis."}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\tresult = nx.histogram(hist, range=from_=0, **kwds)\n\telse:\n\t\tbins = np.linspace(from_value,to_value,num=len(hist))\n\t\tif isinstance(bins,int):\n\t\t\tbins = numpy.linspace(from_value, to_value,num=bins+0.5,endpoint=True)\n\t\tresult = nx.grid_histogram(hist, bins=bins,**kwds)\n\n\t# We can now use our data in the result to create bin_edges\n\tif make_value:\n\t\treturn result/"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\n\t\n\tdef __init__(self):\n\t\tself.points = []\n\t\t\n\tdef set_data_points(self, data_points):\n\t\tself.points = data_points\n\t\n\tdef set_error_bars(self, error_bars):\n\t\tself.error_bars = error_bars\n\t\n\tdef add_point(self, point):\n\t\tself.points.append(point)\n\t\t\n\tdef plot(self, ax, color, *args, **kwargs):\n\t\t\"\"\"\n\t\tPlot the graph and return the plot image.\n\t\tTo display the plot in a new window,"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\n\t\n\thist = hist.copy()\n\t\n\tif make_value is None:\n\t\tmake_value = hist.find_most_hist()\n\t\n\tfor i, c in enumerate(histogram_set):\n\t\tif make_value is not None:\n\t\t\thist.find_histogram(hist.get_value(i, make_value))\n\t\thist = hist.add_history(i, c)\n\n\treturn hist\n# Copyright (c) 2013, Web Notes Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\ttry:\n\t\t\tbins = asterix.histogram(hist.value)\n\t\texcept:\n\t\t\tbins = (0, len(hist.value))\n\n\t\t# create a graph for each bin, that bin content is the\n        # bin value for each field of this graph\n\t\tfor bin_ in bins:\n\t\t\tdata = make_value(bin_)\n\t\t\tline = bin_.start + data[2]\n\t\t\t# if the bin is not 0, then make it blank (no data)\n\t\t\tif not bin_.empty():\n\t\t\t\tline += \" x\"\n\t\t\toutput[line]"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\n\n\tbins = hist_split(hist)\n\tx_min = min(bins[0])\n\tx_max = max(bins[0])\n\n\treturn Histogram(\n\t\tx_min,  ## bin centers\n\t\tx_max, # upper bin edge\n\t\tbins[1], # lower bin edge\n\t\tbins[0],\n\t\t# bottom bin edge\n\t\tbins[2], # left bin edge\n\t\tbins[3], # right bin edge\n\t\tbins[4], # top bin edge\n\t\tbins[5], # bottom bin edge\n\t\t# left bin edges"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\t\"\"\"\n\t\tif isinstance(hist, Array):\n\t\t\thist_data = hist.get_data()\n\t\telif isinstance(hist, list):\n\t\t\thist_data = list(hist)\n\t\telif make_value is not None:\n\t\t\thist_data = make_value(hist)\n\t\telse:\n\t\t\traise TypeError(\"Expected an Array or a list/array.\")\n\n\t\tn, bins = hist_data.shape\n\t\tif make_value:\n\t\t\tn = bins\n\t\t\t# convert to bin content type\n\t\t\thist_data = hist_data.astype(make_value)\n\t\t\t"}]}
{"task_id": "62b8b4baeb7e40a82d2d1136", "prompt": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n\n\tif candidate is None:\n\t\tif DEBUG:\n\t\t\tiface.warning('no candidate available - running test on %r' % (self,))\n\t\t\tiface.notice(zope.interfaces.ExceptionWarning,\n\t\t\t\t'The selected interface %r is undecided' % (self,))\n\t\treturn True\n\tif candidate._providedBy is not None:\n\t\t# Only use the interface provided by *thing*\n\t\treturn False\n\t# Otherwise, use the *providedBy* interface\n\treturn True\t\t## I was supposed to have had the required_by attribute\n\n\n#### ST"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\treturn False\n\t\t\n\n\n@zope.interface.implementer(zope.interface.Interface)\ndef _implements(*interfaces):\n\t\"\"\"\n\tCreate a simple object whose methods will check a single\n\telement according to the interface of *interfaces*.\n\t\n\tThis will pass \"providedBy\" and \"providedByValue\" to the constructor\n\tof :class:`zope.interface.Interface`. Additional provided methods can\n\tbe specified either directly or as lists of interfaces, where the last\n\telement in the returned objects list will be given to *providedBy*.\n\t\n\t\"\"\"\n\tattrs ="}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    # FIXME: If we were not told it that means that it contains itself,\n    # remove this in place.\n    try:\n        raise zope.interface.NoClassInfo\n\texcept KeyError:\n            # Use getattr to decodify the interface.\n            raise TypeError\n\ndef _set_pyobjects_and_class_methods(iface, iface_bases):\n\t\"\"\"\n\tAssign C{iface} class methods to our implementation objects.\n\t\"\"\"\n\tfrom z3c.python.collections.object import zobjects \n\n\tif _vfc_pyobjects is None:\n\t    _import"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    if candidate is None:\n        return True\n        \n    methods = candidate.providedBy()\n    for method in methods:\n        if method.isMethod or method.isAttr or method.isUnnecessary:\n            continue\n        elif not method.isProvidedBy(iface):\n            continue\n        elif method.isProvidedBy(candidate):\n            raise Invalid(\n                '%s implements %s, which does not provide %s.%s' % \n                (repr(candidate), method.identifier.name[:-1],\n                 iface.name, method.identifier.name[:-1]))\n        elif method.isPrivate:\n            return True\n        else"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    def get_all(iface):\n        \"\"\"\n        Get the interface's direct-object proxies in order of preference.  The\n        only attribute that is returned is the one that returns a list of the methods in\n        the interface in question.\n\n        :param iface: The interface\n        :return: A list of direct-object proxies in order of preference\n        :raise zope.component.Invalid: If any of the previous conditions\n        does not hold.\n        :param type: The type of interface, one of the types in the\n        interface interface_implements.types.interfaces_and_interfaces.\n        \"\"\"\n    \tinterfaces = iface"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    assert isinstance(candidate, zope.interface.providedBy)\n    iface_spec = candidate.providedBy()\n    base_spec = iface_spec.base_spec\n    # Don't use issuer(), since __implements__ only accepts single-dotted\n    # classes.\n    if ((base_spec is iface_spec) or\n        (issubclass(base_spec, iface_spec.IInterfaceSpec) and\n         base_spec.implementedBy(iface_spec))):\n        raise Invalid('found multiple match in %r: %r contains %r' %\n                        (iface_spec, iface_spec.filename, base"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n     return iface.providedBy(candidate)\n    \n    \n#class IZStringIO(StringIO):\n\n#   \"\"\"Wrapper around StringIO that doesn't raise an Exception (as per _unpack).\"\"\"\n#   def __init__(self, output=sys.stdout):\n#       StringIO.__init__(self, output)\n\n#   def __getattr__(self, name):\n#       \"\"\"\n#       Return a Zope-style value.\n\n#       Note that this method is called during the initialisation phase. It\n#       is intended to be used in place of a file-like object that\n#"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n### A utility function used for unit testing.\ndef _formatError(e):\n\t\"Format an argument.\"\n\te.args = (str(e),) + e.args\n\treturn formatError(e)\n\n### Functions used to wrap the pyunlocbox objects for the tests\ndef wrapTestImplementation(name, cls, iface=None):\n\t\"\"\"\n\tAdd a class to the pyunlocbox interface and return the results.\n\t\n\t@param name: the name of the class to wrap\n\t@param cls: Class object to wrap\n\t@param iface: optional"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\t# \"If no exception occurs, \"\n\t\n\tiface_candidate = None\n\t# First, if only one failed, return the iface (and the candidate)\n\t# otherwise, check for any error.\n\t#\n\tif len(previous):\n\t\te = None\n\t\tf_type, v_type = previous[0]\n\t\tif previous[1]:\n\t\t\te = e or v_type\n\t\t\tif not _verify(iface_candidate, e, tentative=tentative):\n\t\t\t\tif vtype:\n\t\t\t\t\treturn False\n\t\t\t\telse:\n\t\t\t\t\traise zope.Interface"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    if member_of(candidate)!= iface:\n        raise Exception('%r must be: %r' % (candidate, iface))\n\ndef providedBy(object):\n    \"\"\"\n    Determine the interface that must satisfy the requirements of an\n    object.\n\n    :param object: The object to check that `providedBy` may be a class\n      containing an interface.\n    :return: The interface that requires the object, or None if not.\n    \"\"\"\n    if not IDeclaration.providedBy(object):\n        raise Exception('%r must be an interface' % object)\n\ndef providedByAttribute(object, attr):"}]}
{"task_id": "62b8b4baeb7e40a82d2d1137", "prompt": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    for c in candidate.__mro__:\n\t    if (\n               c in (\"interfaces\", \"__dict__\", \"__weakref__\",\n                  \"iface_implemented\", \"__new__\", \"__init__\")):\n                  raise Invalid(\n                        \"%s: Circular reference detected\" %\n                        c.upper())\n\t\n    extra = []\n    for c in candidate.__mro__:\n            c = c.__dict__\n            if (\n                 inspect.ismethod(c) and \\\n                  c.__self__ is not getattr(c, \"__self__\", None)):\n                extra.append((c, getattr(c, \"__self__\", None)))\n    if not extra:\n\t    if ("}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    if candidate.providedBy(iface=iface):\n        return True\n    return (candidate.name == iface.name)\n\ndef verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the appropriate\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements the <zope.interface.implementer"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t    \n    if not _check_method_compatibility(_candidate, candidate):\n        raise exc.InterfaceError(\n\t\t\t'Method %s is not compatible with %s (yet).'\n\t\t\t% (_candidate.name, _candidate.interface))\n\t\n    # Make sure the interface is implemented by the object, not the\n    # interface itself. It will raise an error if all needed methods are\n    # specified. This can happen easily if one interface requires the\n    # object, or if it is implemented *after* all functions are\n    # defined. This is particularly useful for instance when a plugin\n    # implements that interface. To"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    if len(candidate.interfaces) > 1:\n\t    # This is the case when we use one or more interfaces\n\t   # with a particular interface to make sense.\n\t    #\n\t    error = sys.exc_info()\n\t    try: \n\t\tiface1, iface2 = candidate.interfaces\n\t    except AttributeError:\n\t\tiface1 = candidate\n\t\tiface2 = None\n\t\tfound1 = False \n\t\tfound2 = False\n  #}}}\n  #}}}\n   #{{{ Check if candidate provides certain interfaces.. {\n   requiredMethods = [ x for x in candidate.requires if hasattr(x, 'implements') ]"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    candidateAttr = iface._getAttribute((iface.IAttributeSource(candidate),))\n    isValid = True\n    try:\n        error = candidateAttr['exception']\n    except KeyError:\n        error = candidateAttr\n        isValid = False\n    \n    if error == 'zope.component.invalid: member for'\\\n        'zope.interface\" returns None.',\n    else:\n        if not isinstance(error, str) or len(error)!= 4:\n            return isValid\n        elif error[1] in str(memberVal.type):\n            return isValid\n        else:\n            msg = \"Expected 4-element value for %"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\ndef verify(name, value=None):\n\t\"\"\"\n\tVerify that *name* is a valid candidate for compliance with *value*.\n\n\tIf there is a problem verifying that the specified candidate for *name\n\t*didn't* meet the requirements defined by *didn't* defined by *d.\n\n\tIf *value* is given, it is added at the end of the returned\n\tvalue. This is usually a problem, to get a more helpful error message\n\t(e.g. the function isn't returning ``False``). By using this utility\n\tit is helpful to use"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t \n    def _getRequirements(ob):\n\tob.__module__ = '<unknown module>'\n        try:\n\t    ob.__provides = getattr(iface, ob.__name__+interface.__name__+'/provides')\n\t    ob.__requires = getattr(iface, ob.__name__+interface.__name__+'/requires')\n\t    ob.__obcapabilities = ob.__provides.copy()\n\t    ob.__requires['require'] = ob.__obcapabilities.copy()\n\t    ob.__obcapabilities['prohibited'] = ob.__requires.copy()\n\t    ob.__requires['requiredBy'] = ob.__obcapabilities['requiredBy']\n\t    ob"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    candidates = getAllDescriptors(iface)\n\n# The following condition may *always* mean a function is defined:\n# def verifyObject(iface, candidate):\n#\t\"\"\"\n#\tVerify that a candidate's iface provided by *iface* is acceptable.\n#\t\n#\tThe parameters :\n#\t- iface The interface object to which the candidate is supposed\n#\t- candidate A pyzope.interface.Interface\n\ndef verifyObjects(iface, candidates):\n\t\"\"\"\n\tVerify that **candidates** might be a member of the candidate's\n\tmethods; if only one"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n        if hasattr(candidate, '__implements__') \\\n\t\tand hasattr(candidate.__constraints__, '__implements__'):\n\t\t\t# TODO: add a better exception message. If you ever decide\n\t\t\t# to implement this properly in your own code, you\n\t\t\t# should do that anyway as well.\n\t\t\t\n\t\t\tcompat.super(ZopeInterfaceClass, candidate).validateInvariants()\n\t\t\t\n\t\telse:\n\t\t\treturn True\n\t\t\n\t# Since we should have more information than issuing as to when\n\t# checking if an information-only class has been defined\n\t# (that is, they haven't been defined yet),"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\t\"\"\"\n\t\n    if len(candidate.names) == 1 and str(candidate) == str(iface):\n\t\t# One error of each name found in the interface, raise\n\t\t# Unrecognized/unsupported interface or any kind of problem\n\t\traise Invalid(iface, candidate,\n\t\t\tmessage=(\"The interface %r has one of the methods %r and \"\n\t\t\t\"attributes %r\" % (iface, iface.methods()),\n\t\t\texpected=iface.methods()))\n\t\t\t\n    if not (isinstance(\\\n\t\t\ttype(candidate),\n\t\t\ttype) and issubclass(candidate, iface))\\\n\t\tand type(candidate)!="}]}
{"task_id": "62b8b4c1eb7e40a82d2d1139", "prompt": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tif not (iface.isInterface() and iface.isCallback()): \n\t\t\treturn False\n\t\tiface2 = iface.copy()\n\t\tif not iface2.isInterface() or not iface2.getExtendedAttribute(\"AIChullpoint\"):\n\t\t\treturn False\n\t\tiface1 = iface2.copy()\n\t\tiface1.originalFirst = not isinstance(iface1, iface)\n\t\tplist = iface1.plist()\n\t\tif iface1.uri.value == PLIST_PATH:\n\t\t\tfor i in plist:\n\t\t\t\tif iface2 == i.name:\n\t\t\t\t\tif p"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tglobal iface_cache  # pylint: disable=W0603\n\t\tif iface in iface_cache:\n\t\t\treturn iface_cache[iface]\n\n\t\tiface_key = (iface, candidate)\n\n\t\t# we need a candidate for a given iface and see if the\n\t\t# interface is consistent with one of the candidate's\n\t\t# config.\n\t\tif iface_key in config.iface_cache:\n\t\t\treturn iface_cache[iface_key], candidate\n\n\t\tif (candidate.iface is not None\n\t\t\tand (candidate.iface == iface or candidate.iface == iface_key)):"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tif not tentative and iface.isAndFromAgainstVirus(candidate):\n\t\t\treturn False\n\t\t# If we're a candidate, and it isn't neglected, then the *interface*\n\t\t# should be present and the *candidate* should be absent.\n\t\tif candidate in map(lambda x: not x.isNeutral(), candidate.getChildren()):\n\t\t\treturn False\t# There won't be an ifconfig for this case.\n\t\treturn True\t# The candidate exists, and we're not yet supposed to be.\n\t# The interface matches, and we"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tassert iface!= None\n\t\tif tentative and iface[0] == \"class\":\n\t\t\tif pyatspi.find_descriptor(candidate,\n\t\t\t\t\"java.util.concurrent\", tentative)!= None:\n\t\t\t\treturn [pyatspi.find_descriptor(candidate,\n\t\t\t\t\"java.lang.reflect.reflect_class\")]\n\t\telse:\n\t\t\tret=[]\n\t\t\tfor a in iface:\n\t\t\t\tif a.getName()!=None:ret.append(a.getName())\n\t\t\treturn ret\n\t\n\tdef findClassByName(name):\n\t\tfor a in iface"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\t# we expect at most one iface\n\t\tif len(candidate) < 1:\n\t\t\treturn False\n\t\tiface = candidate[0]\n\t\tif iface not in ifacetree.byname(iface):\n\t\t\tif tentative:\n\t\t\t\tif tentative in ifacetree.by_type(iface):\n\t\t\t\t\t# we found it,\n\t\t\t\t\tif tentative in ifacetree.by_type(iface, None):\n\t\t\t\t\t\t# good\n\t\t\t\t\t\treturn True\n\t\t\t\treturn False\n\t# get the list of matching interfaces\n\tmatches = []\n\tfor iface in ifacetree.by_type"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tif not tentative and iface == 'dummy':\n\t\t\treturn 1\n\t\tiface = iface.replace('.', '')\n\t\tprint('%s:' % iface)\n\t\tif sys.version_info >= (2,7):\n\t\t\tprint(\"Skipping %s.\" % iface)\n\t\t\treturn 0\n\t\t#\n\t\tif iface[0] == '/':\n\t\t\tfor element in candidates:\n\t\t\t\t#\n\t\t\t\tif element[1].startswith(iface):\n\t\t\t\t\tif element[0]!= '/':\n\t\t\t\t\t\tif component_has_key(element[0], element[2]):\n\t\t\t\t\t\t\treturn 1\n\t\t\t\t\t\t"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t                                                      #(None, iface)\n\t\ttry:                                                            # for a particular descriptor\n\t\t\ti = candidate.index(\":\")\n\t\t\tif i >= 0:\n\t\t\t\t# strip off the ;\n\t\t\t\t(candidate, iface) = candidate[:i], candidate[i+1:]\n\t\t\t\tif rtconfig._get_property_(iface, [\"noauto\"], False) == \"yes\":\n\t\t\t\t\treturn True\n\t\t\tif rtconfig._get_property_(iface, [\"noauto\"], False) == \"no\": # no preference\n\t\t\t\treturn False\n\t\texcept:\n\t\t\treturn False\n\t\treturn True\n\n\t#"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tfor iface in iface_cache.get(candidate, []):\n\t\t\tif iface not in iface_cache_map:\n\t\t\t\tiface_cache_map.add(iface)\t# if iface was still there, delete it\n\t\tif tentative:\n\t\t\tfor portage in preferred_networks:\n\t\t\t\tif iface_cache_map.get(portage[0], {}).get(\"iface_id\", None) == iface:\n\t\t\t\t\tdel iface_cache_map[portage[0]]\n\n\ndef writeFile(candidates, file, append=False):\n\t\"\"\"\n\tWrite the file"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tif not candidate:\n\t\t\treturn iface, []\n\t\tif isTrustedAt(candidate):\n\t\t\treturn iface, []\n\n\t\ts = _classFor(candidate)\n\t\tret = _classFor(s)\n\t\tif targeted:\n\t\t\t# if we get the same class back again, we're not sure it is\n\t\t\t# the same object to be of the same type as the first element\n\t\t\t# (so we are certain that this is the case with pychecker)\n\t\t\tif ret.targeted is not None:\n\t\t\t\treturn ret.targeted, []\n\n\t\t\ttargeted"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\t\"\"\"\n\t\tif tentative:\t\t\t\t# If we didn't get a targeted class yet, assume we know that we are\n\t\t# ultimately a known class. Therefore, assume that we were given a known\n\t\t# tentative mapping.\n\t\tiface_candidate = iface_cache.lookup(iface)\n\n\t\tif iface_candidate is None:\n\t\t\treturn False\n\n\t\treturn (iface_candidate.providedBy(candidate.providedBy(iface))\n\t\t\tand not iface_candidate.isUnmodifiable())\n\n\tif iface_candidate is None:\n\t\treturn False\n\n\tif"}]}
{"task_id": "62b8b559eb7e40a82d2d11f6", "prompt": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t", "suggestions": [{"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tbases = findClassAndCustomization(bases)\n\t\tif explicit_mc is None:\n\t\t\timplicit_mc = bases\n\t\telse:\n\t\t\timplicit_mc = explicit_mc\n\n\t\treturn _getmro(implicit_mc)\n\telse:\n\t\traise TypeError(\"No class for bases %s\" % bases)\n\n\ndef _getmro(bases, explicit_mc=None):\n\t\"\"\"\n\tGather classes' bases of the given bases and the given explicit_mc. It is called recursively\n\tto find all instances of the bases that implement inheritance and also pass in the inheritance"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tif explicit_mc is None:\n\t\t\texplicit_mc = findClass(bases)\n\t\tmetaclass_for = tuple(base.__name__ for base in bases \n\t\t\tif type(getattr(base, '__metaclass__', None)) == type or \n\t\t    isinstance(getattr(base, '__metaclass__', None), type))\n\t\tassert all([ c==explicit_mc for c in classes])\n\t\t# if explicit bases, explicitly declared base may override metaclass\n\t\tif explicit_mc is not None and issubclass(bases[0], bases[0]):\n\t\t\tmetaclass_for = base.__metaclass__\n\t\tif"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tm = None\n\n\t\t# MI2\n\t\t# XXX: If the metaclass is missing then it should be considered as\n\t\t# abstract, not ABCs. This means that you have to specify only an\n\t\t# explicit metaclass in the module definition.\n\n\t\tif bases!= (object,):\n\t\t\tif m is None:\n\t\t\t\traise AssertionError(\n\t\t\t\t\t'If you cannot extract a metaclass from an imported module and'\n\t\t\t\t\t' then you must pass a metaclass in this method')\n\t\t\telif m not in BASE_MRO:\n\t\t\t\traise AssertionError\n\n\t\t\traise AssertionError\n\n\t\tif not implicit_mc"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tif explicit_mc is None:\n\t\t\texplicit_mc = mro(types)\n\n\t\tfor base in bases:\n\t\t\tif inspect.isabstract(base) or (inspect.isabstract(base) and clsname(base) == \"type\"):\n\t\t\t\texplicit_mc = base\n\n\t\tif explicit_mc is None: \n\t\t\traise TypeError(\"metaclass constructor requires explicit __metaclass__ variable\")\n\n\t\t# FIXME: get rid of metaclass!  This is really ugly right now, in any\n\t\t# case, and not quite clear.  Look in __metaclass__ above: \n\t\t# http://www"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tbases = tuple(bases)\n\t\tdeclared = { 'classmethod' : declaredClass }\n\t\tif explicit_mc is None:\n\t\t\texplicit_mc = implicitMc\n\t\tif explicit_mc:\n\t\t\tdeclared['metaclass'] = explicit_mc.classes\n\t\textends = bases[0]\n\t\tif extends[0]!= object or extends[1]!= object:\n\t\t\traise TypeError, \\\n\t\t\t       \"cannot determine metaclass from %r\" % bases\n\t\t\tdel bases[0]\n\t\t\traise RuntimeError, \"cannot determine metaclass from %r\" % extends\n\t\tfor base in bases["}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\t#print \".. determineMetaclass -- bases=%s\"%(bases)\n\t\tclasses = {}\n\t\tif explicit_mc or bases:\n\t\t\tfor c in bases:\n\t\t\t\tclasses[c.__name__] = c\t# allow mixing in the same ctor, that's why this may affect future prohibited class registration\n\t\tmcs = list(classes.values()) + list(classList)\n\t\t#debug(\"classes:\", mcs)\n\t\tfor c in mcs:\n\t\t\tif issubclass(c, class_type):\n\t\t\t\t#print \"-\"*30\n\t\t\t\t#print c.__name__,"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tnewbases = tuple(bases) + (explicit_mc,)\n\t\t\t\n\t\tif explicit_mc is None:\n\t\t\texplicit_mc = newbases\n\t\t\texplicit_mc.append((bases, newbases))\n\t\t\n\t\telif explicit_mc!= newbases:\n\t\t\traise Exception(\"Error: metaclass conflict: bases=%r, additional_bases=%r\" % (\n\t\t\t(bases, newbases), explicit_mc))\n\t\t\n\t\treturn metaclass\n\ndef deriveClassFactory(bases, *named_args, **named_kwargs):\n\t\"\"\"\n\tGiven a list of base classes that define ``__metaclass__"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\t#print \"mro: \", bases, explicit_mc\n\t\t#print \"explicit_mc: \", explicit_mc==2\n\n\t\tmclasses = []\n\t\twhile len(bases):\n\t\t\tmclasses.append(bases.pop(0))\n\t\t\tif explicit_mc and mclasses[-1]==0:\t# no explicit metaclass!\n\t\t\t\traise RuntimeError,\"no explicit metaclass on bases\"\n\t\t\tmclasses.append(explicit_mc)\n\n\t\tm = createEmptyMClass(mclasses)\n\t\tassert len(m.mro())==len(mclasses)\n\t\treturn m\n\t"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\tif explicit_mc is None:\n\t\t\texplicit_mc = bases is None\n\t\telse:\n\t\t\texplicit_mc = explicit_mc+bases\n\n\t\tdef get_metaclass(self):\n\t\t\t\"\"\"\n\t\t\tIf explicitly specified, return a __metaclass__ that \n\t\t\tis the metaclass of the class being created for the given bases.\n\t\t\t@return metaclass or none\n\t\t\t\"\"\"\n\t\t\tif explicit_mc:\n\t\t\t\treturn explicit_mc\n\t\t\telse:\n\t\t\t\treturn bases_mapping.get(self.__class__, self.__class__)\n\t\treturn property(get_metaclass)\n# -*-"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\t\"\"\"\n\t\t# XXX: This needs to be moved to the metaclass. The problem with that\n\t\t# is that a metaclass conflict is never made when defining a metaclass:  classes\n\t\t# that are defined in other base classes.  Don't worry about anything about\n\t\t# the actual metaclass.\n\t\t#\n\t\t# For example:\n\t\t#\n\t\t# @classmethod\n\t\t# class E(object):\n\t\t# \tdef __metaclass__(cls, name, bases, dict): pass\n\n\t\tif explicit_mc is not None:\n\t\t\tif '__metaclass__' in explicit_mc.__dict__:\n\t\t\t\t"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47d", "prompt": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\tif len(key) == 0:\n\t\t\treturn default\n\t\telif len(key) == 1:\n\t\t\traise KeyError(key)\n\t\td = self.__getitem__(key.lower())\n\t\tif isinstance(d, dict) and key in d:\n\t\t\treturn d.pop(key)\n\t\treturn default\n\n\tdef popitem(self):\n\t\t\"\"\"\n\t\tD.popitem() -> (k, v), remove and return some (key, value) pair.\tIf key is not found, d is returned.\n\t\t\"\"\"\n\t\tif len(self) == 0:\n\t\t"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\ttry:\n\t\t\treturn self.__getitem__(key)[0]\n\t\texcept (IndexError, KeyError):\n\t\t\tif default is self.__marker:\n\t\t\t\traise\n\t\t\treturn default\n\n\tdef setdefault(self, key, default=None):\n\t\tif key not in self: self.__setitem__(key, default)\n\t\treturn self[key]\n\n\tdef __getitem__(self, key):\n\t\ttry:\n\t\t\treturn self.__getitem__(key)\n\t\texcept KeyError:\n\t\t\treturn default\n\n\tdef get(self, key, *default):\n\t\ttry:\n\t\t\treturn self.__getitem"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\tif None == key:\n\t\t\treturn default\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\traise KeyError(key)\n\n\tdef setdefault(self, key, default=None):\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\tself[key] = default\n\t\treturn default\n\n\tdef clear(self):\n\t\t\"\"\"\n\t\tD.clear() -> None.  Remove all items from D\n\t\t\"\"\"\n\t\tself._pdict = {}\n\n\tdef copy(self):\n\t\t\"\"\"\n\t\tD.copy() -> No"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\tv = default(self, key)\n\t\t\tself.discard(key)\n\t\t\treturn v\n\tpop.__doc__ = pop.__doc__\n\n\tdef popitem(self):\n\t\t\"\"\"\n        'D.popitem() -> (k, v), remove and return some (key, value) pair as a 2-tuple;\n\t'thie'.\n\t\tUse :func:`pair` if you want to remove and return some (key, value) pair as\n\t\t- *key* and *value*.\n\t\t"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\ttry:\n\t\t\tv = self[key]\n\t\texcept KeyError:\n\t\t\treturn default\n\t\tdel self[key]\n\t\treturn v\n\tdef popitem(self):\n\t\t\"\"\"\n\tD.popitem() -> (k, v), remove and return some (key, value) pair as a 2-tuple;\n\traise KeyError if D is empty.\t\"\"\"\n\t\ttry:\n\t\t\tk, v = next(self.items())\n\t\texcept StopIteration:\n\t\t\traise KeyError\n\t\tdel self[k]\n\t\traise key(k)\n\tdef setdefault(self, key"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\tif key in self:\n\t\t\treturn self.pop(key)\n\t\telse:\n\t\t\tif default is self._marker: return default\n\t\t\telse: return self.default(key)\t\n\t\n\tdef popitem(self):\n\t\t\"\"\"\n\t\tD.popitem() -> (k, v), remove and return some (key, value) pair as a\n\t\t2-tuple; but raise KeyError if D is empty.\n\t\t\"\"\"\n\t\tif len(self) == 0: raise KeyError\n\t\tv, k = self.popitem()\n\t\traise KeyError,(k,v)\n\n\tdef"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\tif key in self:\n\t\t\tv = self[key]\n\t\t\tdel self[key]\n\t\t\treturn v\n\t\telse:\n\t\t\treturn default\n\n\tdef setdefault(self, key, default=None):\n\t\ttry:\n\t\t\treturn self[key]\n\t\texcept KeyError:\n\t\t\tself[key] = default\n\t\treturn default\n\nif sys.hexversion >= 0x3000000:\n\tfrom collections import MutableMapping as DictMixin\nelse:\n\tfrom collections.MutableMapping import MutableMapping as DictMixin\n\nclass DictMixinDict (DictMixin, MutableMapping):\n\t\"\"\"Support dictionary"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\tif key in self:\treturn self[key]\n\t\telse:\treturn default\n\n\tdef __repr__(self):\n\t\t\"\"\"String representation of D.\n\t\n\t\tThis is so we can see where possible to do this in python - \n\t\t\"\"\"\n\t\tif len(self) == 0: return ''\n\t\t\n\t\ts = '[]'+'\\n'.join([repr(v) for v in self.values()])+'\\n'\n\t\ttry:  \n\t\t\treturn s\n\t\texcept RuntimeError:\n\t\t\twarn(\"dict.pop(x): problem in D\", self.keys())\n\t\t\treturn s\n\n"}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\ttry:\n\t\t\tv = self[key]\n\t\texcept KeyError:\n\t\t\tif default == self.__marker:\n\t\t\t\traise KeyError(\"Key wasn't found\")\n\t\t\treturn default\n\t\telse:\n\t\t\tdel self[key]\n\t\t\treturn v\n\n\tdef remove(self, key):\t\"\"\"\n\tD.remove(k[,d]) -> None, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\ttry:\n\t\t\tdel self[key]\n\t\texcept KeyError:\n\t\t\tif default =="}, {"generated_text": "def pop(self, key, default=__marker):\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\t\"\"\"\n\t\ttry:\n\t\t\tv = self[key]\n\t\texcept KeyError:\n\t\t\tif default is self.__marker:\n\t\t\t\traise\n\t\t\telif default is self.__sep:\n\t\t\t\traise KeyError(key)\n\t\t\telse:\n\t\t\t\traise ValueError(\"Default argument {0!r} not found\".format(key))\n\t\telse:\n\t\t\tdel self[key]\n\t\t\treturn v\n\tdef popitem(self):\n\t\t\"\"\"\n\t\tRemove and return an arbitrary (key, value) pair from the\n\t\tstack that has only one element.\n\t\tWe always take the last element, but raise"}]}
{"task_id": "62b8d23748ba5a41d1c3f497", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\titem = self.head\n\t\tself.head = self.tail = self.head\n\t\treturn item\n\n\tdef __repr__(self):\n\t\tif self.head is None: return \"(empty list)\"\n\t\telse: return \"(tail)\"\n\n\tdef __str__(self):\n\t\tstr = \"(\" + str(self.head) + \", \" + str(self.tail) + \")\"\n\t\treturn str\n\nclass ListPair:\n\t\"\"\"\n\tSpecialized list base class. Individual implementations are the\n\tmost basic class primitives that work with the list object.\n\t\"\"\""}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\twhile 1:\n\t\t\tkey = self.myhash()\n\n\t\t\tvalue = self.pop(key)\n\n\t\t\tif value is not None:\n\t\t\t\treturn value\n\n\t\t\telse:\n\t\t\t\tself.mydict.pop(key)\n\t\t\t\tbreak\n\n\t@staticmethod\n\tdef myhash():\n\t\t\"\"\"\n\t\tReturn the hash value of the dictionary. \n\n\t\tReturns:\n\t\t\tint -- The hash value of the dictionary.\n\t\t\"\"\"\n\t\treturn -1 if len(list(M)) == 0 else sum([M[_]) for M in list(M.items()])\n\n\tdef pop"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\treturn self.funclist[-1]\n\t\n    \n\n\tdef clear(self):\t\n\t\t\"\"\"\n\t\tRemove all elements from set.\n\t\t\"\"\"\n\t\tdel self.funclist[:]\n\t\t\n\t\t\n\tdef copy(self):\n\t\t\"\"\"\n\t\tReturn a new (deep) copy of the tree.\n\t\t\"\"\"\n\n\t\tret = self.__class__()\n\t\tret.funclist = self.funclist[:]\n\t\t\n\t\treturn ret\n\t\n\n\ndef randfunc(args, dur):\n\t\"\"\"\n\tReturn a function that can be used in random cases.\n\t\n\t>>> randfunc(['x'],"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\t(key, value) = self.items[-1]\n\t\treturn (key, value)\n\n\tdef pop(self, idx=-1):\n\t\t\"\"\"\n\t\tRemove and return the `(key, value)` pair least frequently used.\n\t\t\"\"\"\n\t\tif idx >= 0:\n\t\t\titem = self.items[idx]\n\t\t\tdel self.items[idx]\n\t\t\treturn (key, item)\n\t\tif idx < 0:\n\t\t\traise IndexError()\n\t\treturn (None, None)\n\n\tdef popitem_idx(self, idx=-1):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\twhile True:\n\t\t\tkv = self.pop()\n\t\t\tif kv:\n\t\t\t\tyield kv\n\t\t\telse:\n\t\t\t\tbreak\n\n\tdef peeknext(self):\n\t\t\"\"\"\n\t\tReturn the next key\n\t\t\"\"\"\n\t\twhile not self.empty():\n\t\t\ttry:\n\t\t\t\t(key, nextstate) = self.popitem()\n\t\t\texcept IndexError:\n\t\t\t\traise StopIteration\n\t\t\t\t\n\t\t\tif key == nextstate: break\n\t\t\telse: continue\n\n\t\traise StopIteration\n\n\tdef __iter__(self): return self\n\nclass ConfigDict(UserDict):\n\tdef __init__(self,"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\tif not self._items:\n\t\t\tself, self.clear()\n\t\treturn self._items.pop()\n\nclass OrderedSet(set):\n\n\tdef __init__(self, iterable = None):\n\t\tself.end = end = []\n\t\tend += [None, end, end]         # sentinel node for doubly linked list\n\t\tself.map = {}                   # key --> [key, prev, next]\n\t\tif iterable is not None:\n\t\t\tself |= iterable\n\n\tdef __len__(self):\n\t\treturn len(self.map)\n\n\tdef __contains__(self,"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\treturn self.pop(self.key())\n\nclass ConfigLoader(object):\n\t\"\"\"\n\tThis object is suitable to load a config file into its directory and \n\tparse it. The file is specified in the same directory as the program \n\tentry_point.\n\t\"\"\"\n\tdef __init__(self, filepath):\n\t\tself._filepath = filepath\n\t\n\tdef _load_from(self):\n\t\twith open(self._filepath, 'r') as file:\n\t\t\ttry:\n\t\t\t\tdata = file.read().splitlines()\n\t\t\texcept OSError as err:\n\t\t\t\traise ParseError"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\treturn self.data.popitem()\n\tdef pop(self, last=True):\t\"\"\"\n\tRemove an item and return it.  If no items exists with a given (key, value),\n\treturn None. If *last* is true, remove the last occurrence.\n\t\"\"\"\t\n\ttry:\n\t\tval = self[self.key(last)]\n\t    if val: return val\n\t    else: self[self.key(last)] = None\n\t    return None\n\texcept KeyError: pass\n\texcept IndexError:\n\t\traise\n\texcept ValueError:\n\t\traise\n\treturn self"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\tif not hasattr(self, '_queue'):\t\t# Dinu' Python 3.2+\n\t\t\treturn\tNone\n\t\tif not hasattr(self, '_queue'):\t\t\t# Python 3.2+\n\t\t\traise AttributeError\n\t\telem, key = self._queue.popleft()\n\t\tif isinstance(elem, basestring):\n\t\t\tsubsequence = (key, elem)\n\t\telif elem not in self._queue:\t\t# not in list\n\t\t\treturn\n\t\telse:\n\t\t\tk = self.find(elem)\n\t\t\tif k is None:\t\t# elem not in list"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\t\"\"\"\n\t\tif len(self)<=1: raise StopIteration()\n\t\tif self[0] is None: raise KeyError(repr(self))\n\t\treturn self[0][1:]\n\n\tdef pop(self,index=-1):\n\t\t\"\"\"\n\t\tRemove and return the index of the most frequently used item.\n\t\t\n\t\t>>> l = range(1000000)\n\t\t>>> l.pop(0)\n\t\t>>> 799 in l\n\t\tTrue\n\t\t\"\"\"\n\t\tif index<0: return self.popitem()[1]\n\t\telse: return self.pop()[index]-1\n\n\tdef clear"}]}
{"task_id": "62b8d23a48ba5a41d1c3f499", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\tif len(self[0]) == 1:\n\t\t\tif self[0][0] == \"value\":\n\t\t\t\tdel self[0]\n\t\t\telse:\n\t\t\t\treturn (\" \".join(self[:]))\n\t\telse:\n\t\t\traise KeyError(\"No key called key\")\n\t#\n\t# Delete this element\n\t#\n\t# @param key the identifier for which to delete the next element\n\t# @param value the contents of the element to delete\n\t#\n\t# @return the next element (after the delete) or None\n\t#\n\tdef delitem(self,"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\twith self._lock:\n\t\t\tif self._qsize > self._maxsize:\n\t\t\t\treturn None\n\t\t\tvalue, item = self._q[0]\n\t\t\tself._remove(item)\n\t\t\treturn item\n\t\n\tdef _qsize(self):\n\t\twith self._lock:\n\t\t\tqsize = len(self._q)\n\t\t\tif qsize: return qsize\n\t\t\treturn 0\n\t\t\t\n\tdef _remove(self, item):\n\t\t\"\"\"\n\t\tRemove the item from the queue.\t\n\t\t@param item: The item to remove from the queue.\n\t\t@type"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\tbest = self.pop()\n\t\tif best[1] in self:\n\t\t\tself.__delitem__(best[0])\n\t\t\tself.__setitem__(best[1], best[2])\n\t\t\tdel best[0], best[1]\n\n\tdef __len__(self): return len(self.__data)\n\n\t#\n\t# Private methods\n\t#\n\n\tdef __contains__(self, key):\n\t\treturn self.has_key(key)\n\n\tdef __repr__(self):\n\t\treturn ('{0.__class__.__name__}({0.__data!r})'.format("}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\titem = self.value\n\t\tif item is None:\n\t\t\traise KeyError, \"popitem: dictionary is empty\"\n\t\tkey = item[2]\n\t\tdel self.map[key]\n\t\tdel self.map[item]\n\t\treturn key, item\n\tpopitem = popitem\t# for Python 2.3 compatibility\n\n\nif __name__ == \"__main__\":\n\t# print unique_keys()\n\ta = UniqueDict()\n\ta[1] = \"one\"\n\ta[2] = \"two\"\n\t# print list(a) \n\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\tassert self.redis.sismember(ANY_KEY, ANY_VALUE)\n\t\tself.push(ANY_KEY, 'pop', NO_VALUE)\n\t\tif key in self.redis:\t\t# value already present: delete it\n\t\t\tdel self.redis[ANY_KEY]\n\t\telse:\n\t\t\treturn None\n\n\tdef popping(self):\t# pop item and return the lowest (key, value) pair.\n\t\tif self.redis:\t\t# item is present: delete it\n\t\t\tdel self.redis[ANY_KEY]\n\t\tif key in self."}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\twhile self.next(): yield self.next()\n\t# [START popitem]\n\ttry: self.popitem\n\texcept: pass\n\t# [END popitem]\n\t# [END popitem]\n\t# [END popitem]\n\n\tdef prev(self):\n\t\t\"\"\"\n\tReturns the first value that is not yet evaluated.\t\"\"\"\n\t\tif self.sentinel: return self[0]\n\t\telse: return self[1]\n\t# [START prev]\n\ttry: return self.current()\n\texcept: pass\n\ttry: self.prev"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\treturn self.data.popitem()\n\n\tdef get(self, key, default=None):\n\t\t\"\"\"\n\t\tGet the value from the cache; cast `key` to a callable.\n\t\tIf `key` is an int, returns that integer.\n\t\tIf `key` is a callable, returns `default`.\n\t\t\"\"\"\n\t\tv = self.data.get(key)\n\t\tif v is not None:\n\t\t\ttry:\n\t\t\t\treturn v()\n\t\t\texcept Exception:\n\t\t\t\tif default is not None:\n\t\t\t\t\tself.flush(default)\n\n\t\tv = self."}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\tif len(self.dict) > 1: return self.dict.popitem(last=False)\n\t\telse: return self.dict.popitem()\"\"\"Unit tests for io library.\"\"\"\n\n# Tests of io are scattered over the test suite:\n# * test_bufio - tests file buffering\n# * test_memoryio - tests BytesIO and StringIO\n# * test_fileio - tests FileIO\n# * test_file - tests the file interface\n# * test_io - tests everything else in the IO library\n# * test_univnewlines - tests universal newline support\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\tk = self._sequence.pop()\n\t\treturn k, self._value\n\n\tdef __getslice__(self, i, j):\n\t\t\"\"\"\n\tReturn a subsequence of items from `self`. If `i` \n\t\t`+`j` is a slice object, return a new :class:`Sequence` object:\n\n\t\t>>> from whoosh.util.iterables import unfold\n\t\t>>> list(unsplit(range(1,4), range(10,20)))\n\t\t[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\t\"\"\"\n\t\treturn self.pop()\n\n\tdef push(self, item):\n\t\t\"\"\"\n\t\tPush ``item`` onto stack. Return the new set of values.\n\t\t\"\"\"\n\t\tself.stack.append(item)\n\t\treturn self\n\n\nclass LRUDict:\n\n    \"\"\"\n    This data structure holds cached data and items, that are kept in memory.  It's\n    not particularly efficient for small amounts of data in memory.  It is recommended\n    to use one of the LRU implementations available in the future.\n\n    \"\"\"\n\n    \"\"\"\n    Object constructor\n    \"\"\"\n\n    def __init__(self):\n        self"}]}
{"task_id": "62b8d23c48ba5a41d1c3f49b", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\titem = self.cache[self.current_index]\n\t\tself.current_index += 1\n\t\tif self.current_index > self.maxlen:\n\t\t\tself.current_key = None\n\t\telse:\n\t\t\tself.current_key = item[0]\n\t\tself.current_value = item[1]\n\t\n#------------------------\n\nclass PkgIndex(object):\n\t\"\"\"\n\tThe base class for a search index.\n\t\"\"\"\n\tdef __init__(self, filename, packages):\n\t\t\"\"\"\n\t\t@param filename: the package index filename\n\t\t"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\twhile self.has_key():\n\t\t\tnew_key, value = self.pop(0)\n\t\t\tyield (new_key, value)\n\n\tdef remove(self):\n\t\t\"\"\"\n\t\tDelete all `value` values. If there are no items left,\n\t\tthis will raise `KeyError`.  Only its effects are the\n\t\t:meth:`~pickle.UnpicklingSet` primitive.\n\n\t\tThe :meth:`~pickle.Unpickler` instance can be reconstructed\n\t\twithout problems, although you can continue to use this\n\t\tmethod.\n\t\t\"\"\"\n\t\ttry: self.remove_"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\titems = self.getall(key)\n\t\tval = items.pop(self.section.key)\n\t\treturn (items, val)\n\t\n\tdef getvalue(self, key, default = None):\n\t\t\"\"\"\n\t\tReturn the value for the given section. If no section is specified\n\t\tit is created as an attribute of the parent of the current section\n\t\t(assuming a specific section does not exist yet); otherwise the\n\t\texception is raised at initialization.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self.section.getlist(key)\n\t\texcept:\n\t\t\tif default"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\tkey, value = self.pop()\n\t\tself.discard(key, self.pop(key))\n\t\treturn key, value\n\n# -*- test-case-name: twisted.test.test_sslverify -*-\n# Copyright (c) 2001-2004 Twisted Matrix Laboratories.\n# See LICENSE for details.\n\n\n\"\"\"\nTests for L{twisted.internet.ssl}.\n\nThese are tests for the L{twisted.test.sslverify} module.\n\"\"\"\n\nimport sys, os, StringIO\n\nclass StubSocket(object):\n    def __init__(self):\n        pass\n   "}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\tx = []\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\tx.append(self.entries.pop())\n\t\t\texcept IndexError:\n\t\t\t\tbreak\n\t\treturn x\t\n\t\n#--------------------------\n# List & set commands\n#--------------------------\n\t\n\tdef cmd_echo(self, to, message):\n\t\t\"\"\"\tEcho a string, optionally sending it to the IRC channel.\n\t\t\tReturns the reply.\n\t\t\t\"\"\"\n\t\tif not g_bot_supply('bot'): return\n\t\ttry:\n\t\t\tif not isinstance(message, (unicode, str)):\n\t\t\t\tmessage = unicode(message"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\tif not self.has_item():\n\t\t\traise KeyError('dictionary is empty')\n\t\tself.pop(key, None)\n\t\treturn self.items()\n\tpopitem = popitem\n\t# convenience method for popitem\n\tdef update(self, other={}):\n        if isinstance(other, UserDict):\n            for key in other.keys():\n                self[key] = other[key]\n        else:\n            for key, value in other:\n                self[key] = value\n        self._update(other)\n\ndef filecheck(filename):\n    \"\"\"\n    Check a given pathname for potential errors"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\treturn (self.data.pop(self.key, (None, \"\"))[1])\n\n\tdef pop(self, key): return self.data.pop(key, False)\n\n\tdef __setitem__(self, key, value):\n\t\tif not isinstance(key, str):\n\t\t\traise TypeError(\"Key is not a string\")\n\t\tif not isinstance(value, tuple):\n\t\t\traise TypeError(\"Value must be a tuple\")\n\t\tif not len(key) == 2:\n\t\t\tkey_s = tuple(key)\n\t\t\tvalue_s = tuple(value)\n\t\telse"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\treturn (-1, self.data.popitem()[0])\n\n\t# -- Methods implementing the iterator protocol\n\n\tdef __iter__(self): return self\n\tdef next(self, key=None):\n\t\t\"\"\"\n\t\tRemove and return the `(key, value)` pair most recently used.\tIf there are duplicate items, this\n\t\tfunction just returns itself.\n\t\t:param key: the item key to be looked up, if not found (and 'default')\n\t\t:return: the item value, or None\n\t\t:raises: KeyError if no matching item is found\n\t\t\"\"\"\n\t\tif"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\treturn self._map.popitem()[1]\n# -*- coding: utf-8  -*-\n\"\"\"\n@author: Maria Lafu\n\"\"\"\n\nfrom nflpool.blockdim import BlockDimension\nimport numpy as np\nfrom nflpool.blockdim import BlockVectorDimension\nfrom nfipool.blockdim import VectorDimension\nfrom nfipool.blockdim import BlockSparseDimension\nfrom nfipool.blockdim import VectorIndexedDimension\n\n\nclass block_mesh(BlockDimension):\n    def __init__(self, dimx, dimy):\n        \"\"\"\n        Initialize the block_"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\t\"\"\"\n\t\tassert self.marked_items\n\t\titem = self.marked_items.pop()\n\t\tself.marked_items.insert(0, item)\n\t\treturn item\n\n\tdef pop(self):\treturn self[len(self.marked_items):]\n\n\tdef current_item_values(self):\n\t\t\"\"\"\n\t\tReturn the current-value pairs forming the list: ``(key, value)``\n\t\t\"\"\"\n\t\treturn [item[1] for item in self.items if item[0]]\n\n\t# ----------------------------------------------------------------------\n\t# Return the item specified by the given tag.\n\t"}]}
{"task_id": "62b8d23e48ba5a41d1c3f49e", "prompt": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\tlst = self.l\n\t\tr = random.random()\n\t\tlst[:] = lst[r]\n\t\tself.l = self.l[res]\n\t\tself.r = r\n\n\tdef __len__(self):\t\"\"\"\n\tlength\n\t\"\"\"\n\tcnt = 0\n\twhile True:\n\t\tyield cnt\n\t\tcnt += 1\n\n\tdef tolist(self):\t# for generator\n\t\treturn list(self[0])\n\n\tdef lpush(self, new, idx=None):\t# from generator\n\t\tif idx:\n\t\t\t# add element at the"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\treturn self.randomkey\n\ndef rand():\n\t\"\"\"\n\tReturn the sample random number in the range of [-2, 2)\n\t\"\"\"\n\tkey = random.randint(0, 2)\n\treturn key, (key, key + 2)\n\ndef randchoice(*, size):\n\t\"\"\"\n\tReturn a random item from range of the given `size`.\n\t\"\"\"\n\titems = choice(list(get()))\n\treturn choice(items, size=size)\n\ndef randint(*, low, high):\n\t\"\"\"\n\tReturn a random integer, where"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\treturn random.choice(self._choices)\n\n\tdef clear(self):\t\"\"\"\tRemove all options from list.\"\"\"\n\t\tset = set()\n\t\tfor option, values in self._options:\n\t\t\tdel self[option]\n\t\t\tself._values[option] = tuple(values)\n\t\tself._options = set\n\n\tdef copy(self):\n\t\treturn self.__class__(**self._options)\n\n\tdef _normalize(self):\n\t\tvalue, *children = self._values, []\n\t\tfor child in children:\n\t\t\tif isinstance(child, str):\n\t\t\t\tchild ="}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\treturn self.random.pop(1)\n\n\tdef shift_list(self, key, reverse=False):\n\t\t\"\"\"\n\t\tRemove and return a random `(key, value)` pair.\tIf `reverse` is ``False``, it will be removed from the \n\t\tlist as well.\t\"\"\"\n\t\tif reverse:\n\t\t\tself.shift_list(key, reverse)\n\t\telse:\n\t\t\tself.shift_list_items(key, True)\n\t\tself.shift_len()\n\t\treturn self.random.pop(1)\n\n\tdef shift_list_items("}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\tl = self.redis\n\t\twhile not l.llen():\n\t\t\tl.popitem()\n\t\tr = []\n\t\twhile l:\n\t\t\ta = l.pop()\n\t\t\tr.append((a[0], a[1]))\n\t\treturn r\n\n\n\tdef random(self, n=1):\n                \"\"\"\n\tReturn a new instance of the RandomSet with `n` elements.\n\t\"\"\"\n                return self(set(), n)\n\n\nclass Randoms(collections.Sequence):\n\t'''Immutable base class of random number generators.\n\t'''\n\n\tdef __new__(cls"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\td = self.data.popitem()\n\t\tkey = d[0]\n\t\tval = d[1]\n\t\tself.data[key] = (val[0], val[1]+1, val[2])\n\t\treturn key\n\n\tdef pop(self, key, *args):\n\t\t\"\"\"\n\t\tRemove and return `(key1, value1)` pair.\tSee :func:`random.choice`.\n\t\t\n\t\t>>> d = popitem()\n\t\t>>> 'A' in d\n\t\t1\n\t\t>>> 'a' in d\n\t\t0\n\t\t>>> d.pop("}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\tif len(self.stack) > 0:\n\t\t\tif len(self.stack) == 1:\n\t\t\t\tnode = self.stack.pop()\n\t\t\telse:\n\t\t\t\tnode = self.stack[-1]\n\t\t\tdel self.stack[-1]\n\t\t\tdel self.stack[-1])\n\t\t\treturn (node.key, node.value)\n\t\telse:\n\t\t\treturn None\n\t\t\t\n\tdef popitemiter(self):\t\n\t\t\"\"\"\n\t\tRemove and return a random `(key, value)` pair until there are no further untagged items.\n\t\t\"\"\"\n\t\tif isinstance"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\tself.value = self.randomkey()\n\t\treturn random.choice(self.item)\n\n\n\tdef randomkey(self):\n\t\t\"\"\"\n\t\tGenerate a random key. Returns a pair of items (or (key, value)) pairs.\n\t\t\"\"\"\n\t\tif self.value is None:\n\t\t\treturn random.choice(self.items[random.randint(0, len(self.items))])\n\t\telse:\n\t\t\treturn random.choice(self.items)\n\n\n\nclass Maker(object):\n\t\"\"\"\n\tDecorator for adding monkey patches. Useful to write docstrings that\n"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\twhile(1):\n\t\t\tkey = choice(self.keys())\n\t\t\tself[key] = self.pop(key, 0)\n\t\t\treturn (key, self.pop(key))\n\n\tdef insert(self, i, pos, item):\t# TODO: fix missing types\n\t\twhile(1):\n\t\t\ttry:\n\t\t\t\tprev_i = pos + i + 1\n\t\t\t\tif(i - prev_i >= len(self)):\n\t\t\t\t\tbreak\n\t\t\texcept:\n\t\t\t\treturn (key, pos + i + 1)\n\n\tdef remove(self, key):\t\t\t#"}, {"generated_text": "def popitem(self):\t\"\"\"\n\tRemove and return a random `(key, value)` pair.\t\"\"\"\n\t\tif random.random() <.9:\n\t\t\tkey = [random.choice(self.items) for i in range(50)]\n\t\telif self.items:\n\t\t\tkey = self.items.pop()\n\t\telse:\n\t\t\tkey = None\n\n\t\treturn key\n# Copyright (C) 2006 Alvaro Navarro Clemente\n#\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n"}]}
{"task_id": "62b43425903eeb48555d3ea1", "prompt": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn self.__re.pattern\n\n\t@property\n\tdef description(self) -> str:\n\t\t\"\"\"\n\t\tA (singular) description of this parameter.\n\t\t\"\"\"\n\t\treturn self.__description\n\n\t@property\n\tdef params(self) -> Sequence[Parameter]:\n\t\t\"\"\"\n\t\tA (non-singular) parameter's parameters.\n\t\t\"\"\"\n\t\treturn self.__parameters\n\n\t@property\n\tdef name(self) -> str:\n\t\t\"\"\"\n\t\tA (singular) parameter's name.\n\t\t\"\"\"\n\t\treturn self.__name\n\t\n\t@name.setter\n\t"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn self.__in_pragmas.__class__(self.__in_regex.__class__, pattern=self.__in_pragmas)\n\n\tdef __call__(self, template_string, **params):\n\t\t\"\"\"\n\t\tReturn self.regex.search(output_string)\n\n\t\t:param template_string: The template string.\n\t:param params: Extra params to pass on to :meth:`~pexdoc.parser.PyDocParser.parse`\n\n\t:return: A compiled regular expression (re pattern object).\n\t:raises: :Exception`\n\t\"\"\"\n\t\treturn self"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn self._regex\n\n\t@property\n\tdef value(self) -> Any:\n\t\tre.compile = self._regex = re.compile\n\t\treturn self\n\n\t@property\n\tdef group(self) -> str:\n\t\treturn self._group\n\n\t@property\n\tdef re (self) -> str:\n\t\tif self._regex is not None:\n\t\t\treturn self._regex\n\t\treturn self.__class__.__name__\n\n# encoding: utf-8\n#\n#\n# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v."}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\nclass _Regex(AbstractRePattern):\n\t\"\"\"\n\tRegex for matching parameter names (see :class:`re.compile`)\n\t\"\"\"\n\tdef _regex(self, string: str) -> Pattern:\t\t\"\"\"\n\tRegular expression matching parameter names (see :class:`re.compile`)\n\t\n\tExamples:\n\t\n\t>>> _regex('[a-zA-Z][a-zA-Z0-9]*') # doctest: +NORMALIZE_WHITESPACE\n\tr'[a-zA-Z][a-zA-Z0-9]*'\n\t\n\tNote: the following examples show how"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\tin_pattern = self.RE_in_parameter.pattern\n\n\t\t# The pattern pattern.\n\t\ttry:\n\t\t\tregexp = re.compile(in_pattern, re.VERBOSE)\n\t\texcept Exception as e:\n\t\t\traise InvalidRegex(\"In-style regex {} failed to compile: {}\".format(in_pattern, str(e)))\n\n\t\treturn regexp\n\n\t@staticmethod\n\tdef _is_valid_parameter(param: dict) -> bool:\n\t\t\"\"\"\n\tValidate a value.\n\n\t:param param: dict or string: The dictionary.\n\t:return: True or"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\tin_re = self.re\n\t\n\t\tif self.optional:\n\t\t\tif not hasattr(self, 'optional_parameters'):\n\t\t\t\tself.optional_parameters = {}\n\t\t\tself.optional_parameters.update(dict(self.optional_parameters))\n\t\telse:\n\t\t\tself.optional_parameters = None\n\n\t\treturn re.compile(self.param_regexp, self.param_flags)\n\t\t\t\n\tdef _format_param_string_as_regex(self) -> Pattern:\n\t\tparam_string = self.param_string\n\t\n\t\tif not self.optional:\n\t\t\t"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\tif self._in:\n\t\t\treturn re.compile(self._in)\n\t\telse:\n\t\t\treturn super()._create_in_regex()\n\nclass FanLineRemover(object):\n\n\t\"\"\"\n\tCreate a list of lines removerable with a format function.\n\t\"\"\"\n\tdef __init__(self, formats: [Format], *patterns: [Pattern] = ()) -> None:\n\t\tself._formats: Formats = tuple(formats)\n\t\tself._patterns: Pattern = tuple(patterns)\n\n\tdef __contains__(self, line: str, re"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn self.re.compile(self.in_style)\n\n\t@staticmethod\n\tdef _is_in_style(regex: Pattern) -> bool:\n\t\t\"\"\"\n\t\tReturn `True` if `regex` is in-style, `False` otherwise.\n\t\t\"\"\"\n\t\treturn regex.match(pattern_to_string(regex)) is not None\n\n\t@staticmethod\n\tdef _return_in_pattern(pattern: Pattern) -> Pattern:\n\t\t\"\"\"\n\t\tReturn the given pattern, but which can be either a compiled re with or match\n\t\tif `pattern` is"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\tself.check_type('in-style')\n\t\treturn self._regex\n\n\t@property\n\tdef re_flags(self) -> int:\n\t\t\"\"\"\n\t\tReturns the number of flags in the regexp.\n\t\t\"\"\"\n\t\treturn 0\n\n\t@property\n\tdef re_flags_name(self) -> Union[str, int]:\n\t\t\"\"\"\n\t\tThe name of the re flags.\n\t\t\"\"\"\n\t\treturn 'flags.regex.%d' % (self.re_flags)\n\n\t@property\n\tdef re_flags_name_no_name(self)"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\t\"\"\"\n\t\treturn Pattern()\n\n\tdef _add_match_to_text_with_inset_(self, text_obj):\n\t\t\"\"\"\n\t\tAdd the given match to :attr:`text_obj` to the :class:`match_text`.\n\n\t\tArguments:\n\t\t    text_obj:\n\t\t\t\tMatch object.\n\t\t\"\"\"\t\t\n\t\tself.match_text.add_match(text_obj)\n\t\t\t\n\tdef set_match_text_in_text(self, text):\n\t\t\"\"\"\n\t\tSet the text of the match to this :class:`match_text` object.\n\n\t"}]}
{"task_id": "62b43426903eeb48555d3ea2", "prompt": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\tassert self.params_to_converter is not None, \"params_to_converter is not yet assigned!\"\n\t\treturn _converting._Converter()\n\n\tdef _set_converter(self, converter: _converting._Converter) -> None:\n\t\tassert isinstance(converter, _converting._Converter)\n\t\tself.params_to_converter = converter\n\nclass _Converting_Base(object):\n\t\"\"\"\n\tBase class for parameters conversion. (Internal)\n\t\"\"\"\n\tdef __init__(self, name, params: _Converting._ConvertingParam) -> None"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t    \n\treturn _converting._Converter(\n\t\t\tself._value_type,\n\t\t\tself._value_to_python,\n\t\t\tself._value_to_python_without_nulls,\n\t\t\tself._value_to_python_with_nulls\n\t\t)\n'''tzinfo timezone information for Europe/Sovietti'''\nfrom pytz.tzinfo import DstTzInfo\nfrom pytz.tzinfo import memorized_datetime as d\nfrom pytz.tzinfo import memorized_ttinfo as i\n\nclass GMTInfo(DstTzInfo):\n    '''America/Los_"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\treturn _converting._Converter\nimport os\nimport unittest\n\nfrom django import http\nfrom django.conf import settings\nfrom django.contrib.admin import AdminSite, ModelAdmin\nfrom django.contrib.admin.helpers import ACTION_CHECKBOX_NAME\nfrom django.contrib.admin.models import ADDITION, DELETION, LogEntry\nfrom django.contrib.admin.sites import LOGIN_FORM_KEY\nfrom django.contrib.admin.templatetags.admin_static import static\nfrom django.contrib.admin.templatetags.admin_urls import add_preserved_filters\nfrom django"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\t# Check compatibility and sanity\n\t\tself._convert = _converting\n\t\tfrom._core._parameter import Parameter\n\t\treturn _converting._Converter(self)\n\n\tdef getParameter(self, parameter: ParameterInterface) -> ParameterInterface:\n\t\t\"\"\"Retrieve parameter from the engine's Parameter interface.  This\n\t\tonly works for strings and tuples\"\"\"\n\t\treturn self.getEngineParameter(parameter.EngineParameter)\n\n\tdef getParameterFromString(self, parameter_name: str) -> ParameterInterface:\n\t\t\"\"\"Retrieve engine parameter value from the engine's Parameter\n\t\tinterface.  This only works"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef _create_function(self, parent, *args, **kwargs):\t\"\"\"\n\tCreate a function converter for the given parent.\n\nSee :func:`defaut_input_converter_from_cti`, :func:`defaut_input_converter_from_xml`,\n:func:`defaut_output_converter` from :func:`.to_cti`.\n\nSee :func:`cti_input_converter`.\n\"\"\"\n\t\traise NotImplementedError\n\n\tdef _create_extra_options_wrapper(self, parent) -> _converting._Converter:"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\treturn self._converted_converter\n\n\tdef convert(self, **kwargs) -> Any:\n\t\t\"\"\"\n\t\tReturns a :class:`.Converting` instance converted according to the specified\n\t\targuments.  This usually returns a :class:`.Converter` instance which can be\n\t\tused to convert a value between one scalar value (`float`).\n\n\t\t:param \\*\\*kwargs: Optional arguments\n\t\t\"\"\"\n\t\twith self.converted_converter as converter:\n\t\t\tkwargs[\"type\"] = converter\n\n\t\treturn converter(**kwargs)\n\"\"\"\nThe main QuerySet implementation. This provides the public API for the ORM."}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\nclass ModelConverter(_Converting):\n\tdef __init__(self) -> None:\n\t\t\"\"\"\n\t\tCreate the parameter style converter.\n\n\t\tIf argument is ``None`` the object will convert to standard.\n\t\t\"\"\"\n\t\tsuper().__init__()\n\t\tself._converter = _Converting.__instance = self\n\t\n\tdef _transform(self, doc: 'doc.Document') -> 'core.Model':\n\t\t\"\"\"\n\t\tTake a dict with all keys from the `doc` key-value mapping. \n\t\t\n\t\t:raises: :exc:`ValueError` if the parameter style document has an unknown"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\tconverter = (\n\t\t\tself.converter if (self._converter is not None) else _converting)\n\n\t\tif issubclass(converter, _Converting):\n\t\t\treturn converter\n\n\t\tif self._paramstyle == 'params':\n\t\t\tconverter = _ParamstyleConverter(\n\t\t\t\tconverter, self.data_array, self.label_array, self.style)\n\t\t\tself._paramstyle = converter\n\t\t\treturn converter\n\n\t\tconverter_name = '.'.join([self._converter.__name__, 'Converter'])\n\t\tconverter_class = type(_convert_to(converter))\n\t\tself._converter ="}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\treturn self.converters[self._param_style_name]\n\n\tdef _create_param_style(self, parent_name: str, converter: _converting._Converter,\n\t\t\t\t\t\t\t **kwargs) -> _converting._Converter:\n\t\t\"\"\"\n\t\tCreate the parameter style ``param-style`` for the top-level parameter\n\t\tstyle in ``_pillow`` (``pillow``) and the parameter style ``param-style``. Returns\n\t\tthe parameter style converter (:class:`._converting._Converter`).\n\t\t\"\"\"\n\t\t# We create converters as a regular converter.\n\t\tparam_"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\t\"\"\"\n\t\traise NotImplementedError\n\n\t@property\n\tdef parameter_styles(self) -> List[str]:\n\t\t\"\"\"\n\t\tOptionally generate a list with style to add to the list of optionally\n\t\tavailable options. The style is always added to the list.\n\n\t\t:return: list of str\n\t\t:rtype: list[str]\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef __str__(self) -> str:\n\t\traise NotImplementedError\n\n\nclass _ParameterDict(AbstractParameterDict):\n\t\"\"\"\n\tRepresents an ordered mapping to an already made parameter string.\n\n\tThe order of keys has"}]}
{"task_id": "62b8966c755ee91dce50a154", "prompt": "def isoparse(self, dt_str):\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 22.16 GiB total capacity; 20.36 GiB already allocated; 11.88 MiB free; 21.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b896de755ee91dce50a183", "prompt": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\n        try:\n            d = self.parse(timestr, **kwargs)\n        except ValueError:\n            raise ParserError(\"the parsed date exceeds the largest valid \"\n                            \"C integer on your system.\")\n\t   \n        dt = microsecond = 0\n\n        if d.tzinfo is None:\n            if fuzzy_with_tokens:\n                fuzzy_tokens = list(self.tokens)\n            else:\n                fuzzy_tokens = []\n                for x in self.tokens:\n                    fuzzy_tokens.extend(x.split())\n\n        # Convert to datetime\n        dt = dt.replace(tzinfo = None)\n        try:\n            return make_aware("}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\n        # You can only use this option if C previous year is 2038.\n\n\n\tif isinstance(timestr, datetime.datetime):\n            timestr = timestr.time()\n\tif not timestr:\n\t    return default\n\n\tm = TIMEZONE_PAT.match(timestr)\n\tif not m:\n\t    raise ParserError(_('unknown string format'))\n\n\tinfo = dict(name = m.group('name').lower(), abbr = m.group('abbr'), tzoffset = m.group('tzoffset'));\n        return datetime.datetime(**info)\n\n\n\t# Time specified in parsed,"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t # NOTE: This does not respect the timezone or UTC representation.\n\t\t#       See http://bugs.python.org/issue21606 for more info.\n\n\t# Use the metadata private variable as base.\n\t# This makes str(self) return str(self.decode()).\n\n\tif 'fuzzy_with_tokens' in kwargs:\n\t\twarnings.warn(\n\t\t\t'\"fuzzy_with_tokens\" is replaced by \"parse()\", but is ignored in favor '\n\t\t\t\"of parse()'\")\n\n\targ = str(self)\n\n\tparts = arg.split(',')\n\n\tif len("}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\n    if isinstance(timestr, _basestring):\n\ttimestr = timestr.strip()\n\n    if timestr == \"???\":\n\twarnings.warn(\"Using default time zone when parsing fails\")\n\n    if default is None:\n\tdefault = datetime.datetime.now()\n\tdefault = default.replace(tzinfo=tzutc())\n\n    ret = _parse(timestr, **kwargs)\n    if not ret:\n\twarnings.warn(\"default string replacement failed\")\n\n    if ignoretz:\n\twarnings.warn(\"Don't use --ignore-tzinfo instead of --tzinfo\",\n\t\tDeprecationWarning, stacklevel"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\t# Note: We use unicode_type, not isinstance(s, str), because\n        # of potential circular dependency issues (mro_error.DecodeError which\n        # subclasses bytes_type)\n        # from Python 2.7\n        if not isinstance(timestr, str):\n\t\t\traise TypeError(\"given string is not a valid date/time \"\n\t\t\t\t\t\t \"string\")\n\t\tif _has_str_like(timestr):\n\t\t\tif default is not None:\n\t\t\t\tdefault = _parse_common_strptime(default,\n\t\t\t\t\t\tself.default,\n\t\t\t\t\t\ttimestr,\n\t\t\t\t\t\ttzinfos,\n\t\t\t\t\t\tmax"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t    \n\tif isinstance(timestr, basestring):\n\t    for regexp, format in _PARSE_DATE_FORMAT:\n\t\tmatch = regexp.match(timestr)\n\t\tif match:\n\t\t    kw = {}\n\t\t    for name, offset in match.groupdict(default):\n\t\t\tkw[name] = int(offset or 0)\n                        kw[name] = custom_relativedelta(kw[name])\n\n\t\t    kwargs.update(dict(*kw, **timestr.groupdict(default)))\n\n            return _time.from_datetime_type(_time.tzinfo(**kwargs), **time.tzname)\n        else:\n            raise"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\n    # This function exists to satisfy Python 3.2\n    # Compatibility function taken from Python 3.3 source\n    year, month, day, hour, minute, second, microsecond, tz = \\\n        _parse(timestr, fuzzy_with_tokens=False)\n    if year!= year or (month!= month and day!= day and hour!= hour and minute!= minute and second!= second and microsecond!= microsecond):\n        raise ParserError(\"%s is not a valid date\" % (timestr))\n    dt = datetime(*time.strptime(str(year), '%Y-%m-%d %H:%M:%"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\n        if default is None:\n\t\t\tdefault = datetime.datetime.now()\n\n        if isinstance(default, datetime.timedelta):\n\t\t\tif default.days < 1:\n#\t\t\t\traise OverflowError, \"date too large\"\n\t\t\tdefault = default.replace(microsecond=0)\n\n        if default is None:\n            raise TypeError, \"a non-date time cannot be parsed into a datetime object\"\n\n        try:\n            # Try use getattr instead if it works...\n            result = _PARSECACHE[timestr]\n        except KeyError:\n            parsed = parse(timestr, **kwargs)\n\n            # Cache it if it exists"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\n    # If a tzinfo in the input started with no arguments is passed in\n    # then raise TypeError.\n    if len(kwargs) == 3 and kwargs.get(\"tzinfo\") and kwargs.get(\"tzinfo\")!= ZERO:\n        raise TypeError(errmsg(\"tzinfo not acceptable with most recent pycs\"))\n\n    if kwargs.get(\"fuzzy_with_tokens\", False):\n        fuzzy_with_tokens = kwargs.pop(\"fuzzy_with_tokens\")\n        del kwargs[\"fuzzy_with_tokens\"]\n\n    try:\n        # If called with a single string instead of a sequence,\n        # assume that the resulting parse action will not"}, {"generated_text": "def parse(self, timestr, default=None,\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\t\"\"\"\n\t\traise ParserError(timestr)\n\n\t# Parse the date\n\tm = self._parse_date_iso8601(timestr)\n\n\t# Parse any timezone aliases mapping\n\tnames = {\"BRST\":0, \"CST\":0}\n\tfor tzname in tzinfos.values():\n\t\tname = name.replace(tzname, \"\")\n\t\ttry:\n\t\t\taliases = self._timezones[name]\n\t\texcept KeyError:\n\t\t\traise TypeError(\"Unknown timezone '%s'\" % name)\n\n\t\tfor alias in aliases:\n\t\t\ttry:\n\t\t\t\tobj = parse(name"}]}
{"task_id": "62b8a4a4755ee91dce50a3d3", "prompt": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\n\tif  not isinstance(dt, datetime.datetime):\n\t\tdt = datetime.datetime.combine(dt, timezone.utc)\n\telse:\n\t\tdt = datetime.datetime(dt)\n\n\treturn dt\n\t\ndef fromstd(self, stdtime):\n\t\"\"\"\n\tGiven the UTC time \"standardized\" to a \"standard\" datetime in a timezone,\nreturns a timezone-aware datetime corresponding to the standard\ndatetime in a standardized timezone or of the input timezone.\nThis may be one of the special cases of the standard datetimes.\n\n:param stdtime:\n   "}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\treturn self.difference(self.subtracttz, dt) == 0\n\n\t# the 'fold' state will not be changed until we've calculated it\n\treturn self[-1]\n\n\t# the case of a \"fold\" (start, end-of-last-day, or not in the middle of\n\t# the \"fold\" state). We can get away (and get away) *that*\n\t# of the ambiguous datetime that they're matched.\n\t#\n\t# This is the most-used and is not the first time to decide which\n\t# state should be used for"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n    tz = getattr(dt, 'tz', None)\n    if tz is None:\n\treturn None\n    try:\n\treturn dt.replace(tzinfo=tz)\n    except ValueError:\n\t# the next we must give the same result\n\tpass\n   \nclass tztime(datetime):\n    \"\"\"\n\tA ``tztime`` object represents a time in a known timezone with\n\ta timezone offset. In particular, instances of this class may\n\thave their own ``tzinfo`` attribute (this class does not even have\n\tan ``localize`` method).\n\t\"\"\"\n    tzinfo = property(lambda _:"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n\n    def convert_naive_datetime(self, dt):\t\n\treturn dt.replace(tzinfo=utc)\n\nclass Duration(DurationType):\n    \"\"\"\n    Simple DurationType subclass that respects time boundaries.\n\n    This class does not subclass DurationType itself or does not\n    implement __repr__ as a custom type.  Instead, use the :func:`fromutc`\n    classmethod directly.\n\n    The most common type of the :class:`DurationValue` subclass is\n    :class:`DurationInterval`, which is an attempt at rounding\n    from/to the nearest millisecond value that is represented in the\n    specified duration."}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n# =============================================================================\n# Logging\n\n\nclass Logging(object):\n\t\"\"\"\n\tLogging object.\n\t\n\tUsage example::\n\t\t\n\t\tfrom lofar.logging.logging import Logging\n\t\tLOG = Logging('Lofar.PyLofarTests.mytest')\n\t\n\tSee:\n\t\n\thttp://pypi.python.org/pypi/lofar\n\t\"\"\"\n\t@staticmethod\n\tdef make_timestamp(now=None):\n\t\tnow = now or datetime.datetime.now()\n\t\treturn str(int(now.timestamp()))\n\t\n\tdef __init__(self, name='Log', path=None"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\tpass\n\n\t# Convert to a timezone-naive datetime instance.\n\tdtz = dt.astimezone()\n\n\t# Convert back into a naive datetime instance.\n\treturn dt.replace(tzinfo=dtz)\n\ndef settz(dt, tz, tzfile):\n\t\"\"\"\n\tConvert a timezone-aware datetime in a given timezone, given a filename.\n\t\"\"\"\n\t# Convert back into a naive datetime instance.\n\treturn dt.replace(tzinfo=tz).astimezone(tzfile)\n\ndef from_iso(self, isostr):\n\t\"\"\"\n\tConvert a"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n    if isinstance(dt, (datetime.tzinfo, datetime.timezone)):\n        return dt.replace(tzinfo=self)\n    return dt\n\n\n\ndef fromtimestamp(ts=None, tz=None):\n    \"\"\"\n    Returns a ``datetime_object`` corresponding to the given time-stamp and\n    given timezone.\n    \n    :param ts:\n        A time-stamp (in UTC) in one timezone. If unspecified,\n        current time is used.\n    :param tz:\n        A timezone-aware :class:`datetime.timezone` object indicating\n        the timestamp in seconds when the timestamp should be truncated.\n        If unspecified, current timezone"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n    if dt.tzinfo is None:\n        raise InternalException(_(\"%s timezone is invalid\") % dt)\n    seconds = int(calendar.timegm(dt.utctimetuple()))\n    return  self.astimezone(seconds, is_ambiguous=True)\n\ndef fromordinal(ordinal, tz=None):\n    \"\"\"Return a :class:`datetime.datetime` object for the given ordinal.\n\n:param ordinal: the ordinal.\n\n:deprecated:  Use timelib.to_time instead.\"\"\"\n\ndef fromordinalmonth(ordinal, tz=None):\n    \"\"\"The `fromordinal` routine from ordinal"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t\n# Timezone info classes, these should all be inherited by each\n# other, or they will have the same `fold` variable, but this isn't\n# easy.\n\n# Standard timezones.\n\nT = tztable_value('T')\nU = tztable_value('U')\nST = tztable_value('ST')\nMDT = tztable_value('MDT')\nWEDT = tztable_value('WEDT')\nDT = tztable_value('DT')\nTZ = tztable_value('TZ')\n\"\"\"This module contains code"}, {"generated_text": "def fromutc(self, dt):\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\t\"\"\"\n\t self.tzinfo = pytz.timezone(dt.tzinfo if dt.tzinfo else UTC)\t\t\n\n\n\treturn self._tz2dt(dt)\n\n#============================================================================\nclass tzfile(file):\n\t\"\"\"A file-like object with read only access to the tzfile.\n\t\n\t:param filename: File-like object\n\t@param fileobj: File-like object\"\"\"\n\n\tdef __init__(self, filename, fileobj, options=StringIO.StringIO()):\n\t\t\"\"\"\n\t\t:param filename: File-like object\n\t\t@param fileobj: File-like object\n\t\t\"\"\"\n\t\tsuper().__init"}]}
{"task_id": "62b8a7b2755ee91dce50a4a7", "prompt": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\tif dt.tzinfo is None:\n\t\t\treturn dt\n\t\tif dt.tzinfo is not None:\n\t\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n\n# datetime functions\n\ndef make_datetime(dt, tzinfo=None, localtime=False):\n\t\"\"\"\n\tMakes a :py:class:`datetime.datetime` object from a datetime object.\n\t\n\t:param dt:\n\t\tA :py:class:`datetime.datetime` object\n\t:param tzinfo: A tzinfo object\n\t:param localtime: Flag to use local time instead ("}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\ndef default_tzinfo_factory(dt, tzinfo):\n    \"\"\"\n\tCreates and returns the default time zone, either naive or\n\tslightly adjusted if necessary.\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt``.  If this is ``None``, then the local ``tzinfo`` is\n    used.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\ndef guess_timezone(dt_or_tzinfo):"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\ndef as_utc(dt, tz=None):\n\t\"\"\"\n\tConverts a :py:class:`datetime.datetime` to UTC.\n\tFor example:\n\n\t>>> from dateutil.tz import tz_utc\n\t>>> d = timezone.make_aware(datetime.datetime(2016, 1, 2, 3, 4, 3, 999999))\n\t>>> print(as_utc(d))\n\t2009-02-03 02:04:05+00:00\n\t\"\"\"\n\t# dt may be naive or aware\n\tif not isinstance(dt, datetime.datetime):"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n    if tzinfo is not None:\n        meta = DaysSinceUTC(tzinfo.utcoffset(dt or datetime.now()),\n                             tzinfo)\n        return make_aware(meta, tz=tzinfo)\n    else:\n        return dt.replace(tzinfo=tzinfo)\nimport os.path\nimport datetime\nimport logging\nimport json\nimport hashlib\n\nimport dcos_application\nimport jwt\n\nfrom django import dispatch\nfrom django.conf import settings\nfrom django.contrib.auth.models import User, AnonymousUser\nfrom django.core.exceptions import PermissionDenied\nfrom django.core.urlresolvers import reverse\nfrom"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n        return utcnow() + dt\n\nclass DateTime(tzinfo):\n\t\"\"\"\n\tA :py:class:`datetime.tzinfo` subclass (date and time)\n\twith useful defaults for dealing with dates without a timezone.\n\t\"\"\"\n\tdef utcoffset(self, dt):\n\t\t\"\"\"\n\t\tReturns the timezone-aware start datetime. -6 if the timezone is\n\t\tgiven in a timezone in the range -6:00.\n\n\t\tIf the timezone offset in the range is -5, it is assumed to\n\t\tbe in the localtime.\n\t\t\"\"\"\n\t\tif dt.tzinfo"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\t#TODO - error handling!!!\n\tdef _dt2dt(dt_, tzinfo):\n\t\tif dt_ is None or tzinfo is not None:\n\t\t\treturn dt_\n\t\treturn tzinfo.localize(dt_)\n\treturn _dt2dt\n\ndef dtz(dt):\n\tif dt.tzinfo is None: return dtz_factory()\n\treturn dt.tzinfo\n\n\n# -*- coding: utf-8 -*-\n#\n#\n#    Author: Nicolas Bessi\n#    Copyright 2013-2014 Camptocamp SA\n#\n#    This program is free software:"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n    if not isinstance(dt, datetime.datetime):\n        dt = datetime.datetime.fromordinal(max(dt.toordinal() - 2000, 0))\n    tzinfo = tzinfo or default_tzinfo()\n    if tzinfo is None:\n        raise ValueError('Naive tzinfo in default_tzinfo')\n    else:\n        return dt.replace(tzinfo=tzinfo)\n\n\ndef parse_date(string, tzinfo_factory=None, default=None):\n    \"\"\"\n    Parses the given date ``string``, or :py:class:`datetime.date`, or\n    returns :py:class:`datetime.date`, or :py:"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n    return dt.replace(tzinfo=tzinfo)     class Solution:\n    def longestPalindrome(self, sboard):\n        \"\"\"\n        :type sboard: str\n        :rtype: str\n        \"\"\"\n        if not sboard or not len(sboard) == 0:\n            return \"\"\n        maxlength = max(6, len(sboard) - 1)\n        lengths = {len(sboard):0}\n        for c in sboard:\n            i = 0\n            j = 0\n            ij = 0\n            while i < len(c):\n                if c[i] == '<':\n                    j +="}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\n#\n# Functions for parsing and manipulating time zones\n#\n# These are in no_tz.c, so they can be combined with the classes in\n# tzlocal._tzinfo.py.\n#\n\n_timeRE = re.compile(r\"^([^\\+\\-])(\\d{0,2})?(Z|(([-+])([0-9]+))([eE][-+]))?\")\n\nclass _timeRE_cache(dict):\n\t\"\"\"\n\tCallable object that can be used as :py:class:`datetime.datetime`\n\tinstances to specify cached UTC offset."}, {"generated_text": "def default_tzinfo(dt, tzinfo):\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\t\"\"\"\n\t\nimport datetime\n\nclass UTC(datetime.tzinfo):\n    \"\"\"Datetime class based on the aware *tzinfo*.\n\n:param dt:\n        Same as today, except this class is naive.\n\n\"\"\"\n    ZERO = datetime.timedelta(0)\n    for j, t in enumerate((ZERO, ZERO), ZERO):\n        dt = t + j * one_month\n\t_zero = timedelta(0)\n    _dst_rules = {0: _zero, 1: _dst_rules[1]}\n\tDSTOFFSET = STDOFFSET2TZ\n\n\nclass FixedOffset(tzinfo):\n    \"\"\"Fixed"}]}
{"task_id": "6305f9991d275c6667163c50", "prompt": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\tbytes_before: int = before[-2:]\n\t\tbytes_after: int = after[-2:]\n\tbefore_data: bytes = bytes_before.split(maxsplit=1)\n\tafter_data: bytes = bytes_after.split(maxsplit=1)\n\tif len(after_data)!= len(before_data): raise ValueError(\"Length of the after and before data do not match!\")\n\tself.before = before_data + bytes\n\n\t# Create the output file.\n\toutput_file = tempfile.NamedTemporaryFile(mode='w', dir=self."}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n    if not self.is_modified():\n\t# Ensure that the file path ends with a trailing slash\n\tif not before[-1] == '/':\n\t    before += '/'\n\tself.prepend_bytes(before,'')\n\tself.prepend_bytes(after, '')\n\n    def update(self, new_content: bytes):\n        \"\"\"\n        Updates the line number and position inside a buffer after the file has been modified\n        and a new line in 'contents'.\n\n        Args:\n         new_content: The content to be updated after the changes in the file.\n        \"\"\"\n        buf = StringIO()\n        # Set"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\tself.cut_chars.clear()\n\t\tself.cut_chars.extend(before)    \t\t# list of file paths in cut chars\n\t\tself.cut_chars.extend(after)\t\t# extended char list\n\ndef check_trim_lines(lines: List[str]) -> None:\n    \"\"\"\n    Check that the given lines contain text with a tab character.\n    \"\"\"\n    for line in lines:\n        if line[:1] in [' ', '\\t']:\n            return\n\n    raise IOError(\n        'the specified line has not been a tab line'\n        '\\nfound tab character,"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n    self.read_lines(before + 1)\n    \ndef main():\n\n\t# Parse command-line arguments.\n\n\tparser = argparse.ArgumentParser(\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\tdescription=u'Split files by splitting them on separate lines.')\n\tparser.add_argument(\n\t\t'-x', '--extend', action=u'store_false', dest=u'textend')\n\n\tparser.add_argument(\n\t\t'-e', '--ext', action=u'store', dest=u'ext', help=\n\t\tu'extension to extend ("}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\tfor i in range(2, 5):\n\t\t\tself.content.write(before + after * i)\n\n\nclass Pair:\n\t\"\"\"\n\tA simple file-like object with an unseekable property that\n\tallows one to read the data, and a number of attempts to change\n\ta line's content.\n\tAttributes:\n\tnext: The next character in the stream, which does \n\tnot depend on the particular read method.\n\tline: The line we're processing.\n\tnext_line_cb: The callback for processing the next line after this one\n\tis"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n    try :\n        delim_idx = int(self.get_token_index_at_line(before))\n        self.text_before = before[delim_idx]+self.text_before\n        self.set_token_index_before(delim_idx)\n        self.set_peeked_text_before(after[delim_idx])\n    except ValueError:\n        pass\n\n    try:\n        delim_idx = int(self.get_token_index_at_line(before))\n        if delim_idx > 1:\n            self.text_after = self.text_after[:delim_"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n    self._before = bytearray(before + after)\n    self._after = bytearray(before + after)\n\n  def read_bytes(self, n: int, chars: Iterable[bytes]) -> bytes:\n    \"\"\"\n    Reads n bytes from a file or file-like object.\n    \n    Read from the file or file-like object passed as 'chars',\n    read chunks of the specified size (or from the end when\n    n bytes were read). Returns the resulting bytes.\n    \n    Parameters:\n    n : Number of bytes read from the file or file-like object,\n        at most ``n``.\n    chars: Number of bytes"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t                # TODO FIXME\n\tself._cut_chars_list = after\n\tself._cut_lines: List[bytes]\t\t# Delimit lines in file.\n\n\t# Copy line characters to begin of file.\n\tfor line in BeforeStart:\n\t\tself._read_line(line)\n\tfor line in AfterStart:\n\t\tself._read_line(line)\n\n\t# Write current file.\n\tself._write_to_file(self._filename)\n\n\n# --- Main\n\ndef main(argv: List[str]) -> bool:\n\t\"\"\"\n\t"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\n    self.log(\"set_cut_chars\", \"set_before\", \"before\", before, \"after\", after)\n\t\n\tself.set_file_contents(before = before, after = after, cut_chars = before)\n\t\n    #########################\n    #####################################################################################################\n    ##          EOC3                 ##\n    ##                                                                      ##\n    ##  LINES                          ##\n    ##                                                                      ##\n    ##  1. Main                ##\n    ##                            ##\n    ##  2. File Operations            ##\n    ##                            ##\n    ##  3. Tif Files                   ##\n    ##                            ##\n    ##  4. E"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\t\"\"\"\n\t\t# if there's nothing, nothing is done\n\t\tif self.has_data():\n\t\t\treturn\n\n\t\t# Splitting the file before and after is handled\n\t\tlines = [x for x in self.data if \" \" not in x and x >= before and len(x) > before]\n\t\tself.files.remove(\".split\")\n\t\tlines = [x.strip(\"\\n\") for x in lines]\n\t\tself.files.append(\" \".join(lines))\n\t\tif self.has_data():\n\t\t\tself.data = []\n\n\tdef make(self, path,"}]}
{"task_id": "6306292052e177c0ba469f09", "prompt": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\t# TODO: add a check for public payloads.\n\t\treturn True\n\n\tvalid_public_messages = list(filter(identify_request, [request]))\n\treturn not request.is_private_message\n\n\ndef is_private_message(message: Message) -> bool:\n\t\"\"\"Throw away private message?\"\"\"\n\treturn (message.type == ContentType.TEXT and\n\t\t\tlen(message.to) > 0 and\n\t\t\tmessage.to.lower().strip() == BIO_SIGNATURE[\"PUBLIC\"])\n\n#!/usr/bin/env python\n# -*- coding: utf-"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tif not request[\"messageContent\"][\"requestType\"].startswith(\"/dev/dusk\") or request[\"messageContent\"][\"payload\"][\"origin\"] == \"/dev/dusk\":\n\t\t\treturn request\n\treturn request[\"messageContent\"][\"requestType\"]\n\n\ndef identify_response(response: ResponseType):\n\t\"\"\"\n\tTry to identify whether this is a Diaspora response.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tif not response[\"messageContent\"][\"responseType\"].startswith(\"/dev/dusk\"):\n\t\t\treturn response"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\trequest_id = request.id  # type: int\n\t\tis_legacy = not is_legacy_reply(request)\n\t\tis_not_legacy = (is_legacy and is_not_legacy_reply(request))\n\n\t\tif is_legacy_reply(request_id):\t\t\t# reply. id <legacy message\n\t\t\tis_legacy = True\n\t\telif is_not_legacy:\t\t\t\t\t\t\t# reply not legacy message.\n\t\t\tis_legacy = False\n\n\t\treturn is_legacy\n\n\"\"\"SCons.Tool.bcc32\n\nXXX\n\n\"\"\"\n\n"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tif request.payload.is_legacy:\n\t\t\treturn True\n\n\tdef create(sender_email: str, receiver_username: str, receiver_email: str,\n\t\tsubject: str, message: str):\n\t\tsender: UserProfile = get_user_profile_by_email(sender_email)\n\t\tif not sender:\n\t\t\treturn None\n\n\t\tfullname = get_user_fullname(sender)\n\t\tif fullname is None:\n\t\t\treturn False\n\n\t\tfullname = fullname.lower()\n\n\t\t# If the IRC user passed us a potential sender, use those fields.\n\t\t"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tfor token in request.get_tokens(no_token=True):\n\t\t\tif token.get('token_type') =='request':\n\t\t\t\treturn token\n\n\t\treturn request\n\n\tmsg = None\n\tfor token in request.get_tokens():\n\t\tif token.get(\"token\") is not None:\n\t\t\tsgn = token[\"access_token\"]\n\t\t\tif sgn[\"type\"] == \"message_id\":\n\t\t\t\tmsg = sgn[\"message\"]\n\tif msg is not None:\n\t\treturn {'request': msg, 'private_message': True}\n\treturn"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tif request.data!= b'payload' and request.command == b'GETID' and request.args[0] == b'id':\n\t\t\treturn False\n\n\t\treturn True\n\n\tdef check_for_new_api_key(self, request: RequestType) -> request:\n\t\t\"\"\"\n\t\tUse the ID of the current user if the user has not previously seen the API key.\n\t\tYou should not use this or the user needs to request it.\n\t\t\"\"\"\n\t\tif request.data!= b'id' and request.command == b'GETID' and request."}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\t#if self._request_identifier.match(request.identifier, strict=False) and not _is_legacy_payload(self):\n\t\tif request.type_ == RequestType.type_session_message:\n\t\t\treturn request\n\t\t\nclass TestIdentifyClient:\n\t# TODO: Test it on different types of data (e.g. payload and a PDP)\n\t# TODO: Check if it's the same as request?\n\tdef __init__(self, protocol):\n\t\tself._protocol = protocol\n\t\tself._identifier = None\n\t\tself._identifier_types ="}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tfor message in request.contacts.filter(Q(reply_to=message.content)):\n\t\t\tif message.sender and message.sender!= request.client:\n\t\t\t\tif message.recipient.nick == request.client.nick:\n\t\t\t\t\treturn 'Diaspora request'\n\t\t\t\telif message.recipient.nick == request.nick:\n\t\t\t\t\treturn 'Private message'\n\t\treturn 'Diaspora request'\n\n\n@socketio.on('message')\ndef reply(message: Message, sender_nick: str):\n\tmessage.reply(sender_nick)\n#"}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tfor msg in request.get_messages():\n\t\t\tmsg = strip_tags(msg)\n\t\t\tif msg.get('content'):\n\t\t\t\treturn msg\n\treturn None\n\t\ndef identify_reply(reply: Reply):\n\t\"\"\"\n\tTry to identify a reply from a contact. This check is performed recursively. The reply may be an email. This message will be sent to the contact.\n\t\"\"\"\n\tret_list = []\n\tret = None\n\t\n\temail_match = None\n\t\n\tif reply.get_content():\n\t\temail_match = reply.get_content()."}, {"generated_text": "def identify_request(request: RequestType):\t\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.\t\"\"\"\n\t\tif request.reqtype == RequestType.PUBLIC:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept ObjectDoesNotExist:\n\t\treturn False\n\nclass RequestDetails():\n\t\"\"\"\nClass that represents the object that makes a request from. \n\tAllows the user to determine if this is a Diaspora request.\n\t\"\"\"\n\n\trequest = models.ForeignKey(RequestData)\t\t\n\t\"\"\"Describes the request that was created \"\"\"\n\n\tkey = models.CharField(unique=True, max_length=32)\n\tname = models.CharField(max_length=32"}]}
{"task_id": "6306292152e177c0ba469f0d", "prompt": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\treturn request.headers.get(\"X-X\" if request.is_xhr else \"X-Matrix\") in config[\"xhr-profiles\"]\n\n\n# pylint: disable=abstract-method\nclass WsRequestProfile(RequestProfile):\n\t\"\"\"\n\tThe profile of a WS request\n\t\"\"\"\n\n\tdef __init__(self, request: RequestType, user):\n\n\t\tRequestProfile.__init__(self, user)\n\t\tself.request = request\n\t\tself.set_content_type(request.headers.get(\"Content-Type\", \"\"))\n\t\tself.response_"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\tif request.opcode < 5: # MatrixRequest\n\t\t\treturn True\n\t\tif request.opcode == 6: # GetMatrixRequest\n\t\t\treturn False # If there is another Matrix request\n\t\treturn True\n\n\tdef describe_request(self, request: RequestType) -> str:\n\t\t\"\"\"\n\t\tIf the request has a Matrix request then this is a Matrix request\n\t\t\"\"\"\n\t\tif request.opcode == 7: # MatrixRequest\n\t\t\treturn self.describe_matrix_request(request)\n\t\telif request.opcode == 8: # GetMatrixRequest\n\t\t\treturn self.get_matrix_"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\tif (request.requester_email is None or request.requester_email == ''):\n\t\t\treturn False\n\t\tif (request.requester_country is None or request.requester_country == ''):\n\t\t\treturn False\n\t\tif (request.requester_city_code is None or request.requester_city_code == ''):\n\t\t\treturn False\n\t\tif (request.requester_zipcode is None or request.requester_zipcode == ''):\n\t\t\treturn False\n\t\tif (request.requester_phone is None or request.requester_phone == ''):\n\t\t\treturn False\n\n\t\treturn True\n\n\tdef check"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\tif request.headers:\n\t\t\treturn True\n\n\t\theaders = None\n\t\tif settings.PRICING_HEADERS:\n\t\t\ttry:\n\t\t\t\tif not settings.PRICING_ALLOWED:\n\t\t\t\t\treturn False\n\t\t\t\t# Add to the list of headers\n\t\t\t\theaders = re.findall(r'\"[^\"]*\"', request.headers)\n\t\t\texcept AttributeError:\n\t\t\t\t# Probably a broken response, raise in case it did not contain\n\t\t\t\t# the body\n\t\t\t\treturn False\n\n\t\tsearch_url = get_search_for_array(request.data)\n\t\tsearch_text = get_"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\treturn is_matrix_request(request) or is_request_type(request)\n\n\ndef is_matrix_request(request: RequestType) -> bool:\n\t\"\"\"\n\tReturn true if this is a Matrix request as shown in the form \t\t\n\tof a valid matrix request.\n\t\n\tWe look only for matrix requests that are supported by the current\n\tuser request type.\n\t\"\"\"\n\tif request.method == 'POST':\n\t\tif 'name' in request.POST:\n\t\t\treturn True\n\telif hasattr(request, 'POST'):\n\t\tfor attr in ['"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\tif request.is_matrix_request is False:\n\t\t\t# Request does not request\n\t\t\treturn False\n\t\t# Return if user logged the matrix request\n\t\treturn True\n\n\tdef identify_matrix_request(request: MatrixRequestType):\n\t\treturn request.matrix_request\n# -*- coding: utf-8 -*-\n\n# Form implementation generated from reading ui file\nfrom PyQt4 import QtCore, QtGui\n\ntry:\n    _fromUtf8 = QtCore.QString.fromUtf8\nexcept AttributeError:\n    def _fromUtf8(s):\n        return s\n\ntry:\n    _encoding = QtGui"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\treturn ((request.operation!= RequestType.ADD) and (request.op_code == 9000 and not request.form == None)) or any((request.operation == RequestType.REMOVE) and (request.op_code == 9000 and not request.form == None))\t\t# Ideally we could validate request fields we got\n\t\n\t\n\tdef is_request(self, request: RequestType) -> bool:\t\t\t\"\"\"\n\tReturns whether this request is a Matrix request or not\n\n\t\tThis only validates the following fields if a Matrix request is valid\n\t\tand if it is a Point"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\t# The matrix request comes in pairs of elements:\n\t\t# - the value, is the response\n\t\t# - a string which is not a string, and a response\n\t\t# - the first string it belongs to\n\t\t# - the second string the type of the matrix request\n\n\t\tvector_id = request.vector.value\n\t\tif vector_id in self.MATRIX_REQUESTS:\n\t\t\tif matrix_id in self.REQUESTS_RESPONSE_MAP:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\telif vector_id in self.REQUESTS:\n\t\t\treturn request"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\tif type(request.contentRequest) == MatrixRequest:\n\t\t\treturn True\n\treturn False\n\ndef test_has_valid_request_method(request: RequestType) -> bool:\n\t\"\"\"Returns True, if the provided request is a valid Matrix request\"\"\"\n\tvalid_request_method = is_valid_request(request)\n\tif valid_request_method is True:\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef is_valid_request(request: RequestType) -> bool:\t\"\"\"Determines whether this is a valid Matrix request\"\"\"\n\t"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\t\"\"\"\n\tTry to identify whether this is a Matrix request\t\"\"\"\n\t\tif not request.is_read_only():\n\t\t\treturn True\n\t\tif not get_request_dict_exists_locally(request.key):\n\t\t\treturn False\n\t\tname = request.data['name']\n\t\tif not can_send_to_any_matrix:\n\t\t\t# this request has only unauthenticated and can't get the matrix id/matrix id\n\t\t\t# but we don't send it, it has no id\n\t\t\treturn True\n\t\tif name is None or name == '':\n\t\t\treturn True\n\t\treturn True\n\n\tvalid: DictType\n\t\"\"\"\n\ttype"}]}
{"task_id": "6306292252e177c0ba469f11", "prompt": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\treturn time.strftime(\"%d %b %Y %H:%M %Z\", dt)\n\tfrom datetime import datetime, timedelta\n\treturn datetime.strftime(\"%Y-%m-%d\", dt)\n\ndef d2t(dt):\t# Get the seconds past the epoch.\n\tt_format = format_dt(dt)\t# Format a datetime in the way that D* nodes expect.\n\treturn t_format.replace(minute=0).replace(seconds=1)\n\ndef t2t(t):\t# Get the minutes past the epoch.\n\tt_"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tif dt < datetime.timedelta(minutes=10):\n\t\t\treturn dt.strftime('%b %d, %Y')\n\t\telif dt < datetime.timedelta(hours=10):\n\t\t\treturn dt.strftime('%a %d %b %Y')\n\t\telif dt < datetime.timedelta(days=1):\n\t\t\treturn dt.strftime('%a %d %b, %Y')\n\t\telif dt < datetime.timedelta(days=365):\n\t\t\treturn dt.strftime('%a %b %d, %Y')\n\t\telif dt < datetime.timedelta(days=400):\n\t\t\treturn"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\treturn dt.strftime('%d-%m-%y %H:%M')\n\n\ndef format_node(*args):\n\t\"\"\"\n\tFormat a single node.\n\n\t\"D01\" node\n\t\"\"\"\n\tret = None\n\n\tif len(args) == 2:\n\t\tret = args\n\n\tif len(args) == 1:\n\t\tret = args[0]\n\n\tif len(args) == 0:\n\t\treturn 'D01'\n\n\tif len(args) == 1 or len(args) % 2!= 0:\n\t\treturn 'D01'\n\n\t"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tif dt is None:\n\t\t\treturn \"\"\n\t\treturn dt.strftime(\"%b %d %H:%M:%S\")\n\timport webnotes.utils\n\treturn webnotes.utils.format_dt(dt, '%Y-%m-%d %H:%M:%S')\n\ndef now_datetime():\n\t\"\"\"\n\treturn current datetime as yyyy-mm-dd hh:mm:ss\n\t\"\"\"\n\treturn now_datetime().strftime('%Y-%m-%d %H:%M:%S') \n\ndef today():\n\treturn now_datetime().strftime('%Y-%m-%"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tm = dt_re.match(dt)\n\t\tif not m:\n\t\t\traise ValueError(\"Bad datetime format\")\n\n\t\thour = int(m.group(1))\n\t\tif not hour:\n\t\t\traise ValueError(\"Bad datetime day format '%H:%M:%S' (e.g. 2000).\")\n\t\tminute = int(m.group(2))\n\t\tif not minute:\n\t\t\traise ValueError(\"Bad datetime hour format '%I:%M:%S' (e.g. 10:20)\")\n\n\t\treturn \"%s %s %02d:%02d\" % (dt."}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tif dt == 'N/A':\n\t\t\treturn \"%s days ago\" % dt\n\t\telse:\n\t\t\treturn date_format(dt)\n\n\treturn \"\"\n\t\ndef show_dates():\t\t\n\tglobal date, show_days\n\tdate = 'n/a'\n\tshow_days = 'n/a'\n\n\twhile date > show_days:\n\n\t\tprint\n\t\tprint 'date: %s' % date,\n\t\tprint\n\n\t\tdate = raw_input()\n\n\tdate = date[12:]\n\tshow_days = show_days[12:]\n"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tif isinstance(dt, dt_struct_time):\n\t\t\treturn dt.time().isoformat()\n\t\telse:\n\t\t\tif dt is None:\n\t\t\t\treturn ''\n\t\t\ttry:\n\t\t\t\treturn dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n\t\t\texcept:\n\t\t\t\treturn ''\n\n\treturn str(dt)\n\nclass Node:\n\tdef __init__(self, node_id, depth = 0):\n\t\tself.node_id         = node_id\n\t\tself.name               = None\n\t\tself.depth            = depth\n\n\t\tif self."}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\t# dt can be a datetime or a timedelta. For instance, there\n\t\t# is a type that matches dt is a date.\n\t\t# dt.replace(tzinfo=utc) will produce a datetime with a datetime object for the time being\n\t\t# formatted by the Dumino IRC. It is not a particular case when you have more than two\n\t\t# datetimes and the Dumino IRC could be a generator. That would be a function of\n\t\t# datetime.strptime and IRC expects you to create a datetime using the format given\n\t\t# and IRC expects it to return a datetime"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\ttime_str = dt.strftime('%d.%m.%Y, %H:%M')\n\t\tif time_str in time_dates:\n\t\t\treturn time_dates[time_str]\n\t\telse:\n\t\t\treturn ''\n\tdef format_timestamp(ts, timestamp_str='%d.%m.%Y %H:%M:%S %z'):\n\t\treturn datetime.datetime.fromtimestamp(ts).strftime('%d.%m.%Y %H:%M:%S %z')\n\n\n\tprint 'DAV:', dav\n\tif dav:\n\t\tprint 'timestamp:', timestamp_str"}, {"generated_text": "def format_dt(dt):\t\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\t\"\"\"\n\t\tif dt >= 3600:\n\t\t\tparts = dt.split(\":\")\n\t\t\tparts[1] = parts[1].split(\":\")\n\t\t\treturn parts\n\t\telif dt >= 60:\n\t\t\tparts = dt.split(\":\")\n\t\t\tparts[2] = parts[2].split(\":\")\n\t\t\treturn parts[2]\n\t\telse:\n\t\t\treturn str(dt)\n\ndef validate(fn):\n\tf = get_parser()\n\twith open(fn) as f:\n\t\tcontent = f.read()\n\n\tmessages = []\n\tmessages.append('Validating'+ fn + '\\"}]}
{"task_id": "6306292352e177c0ba469f1d", "prompt": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\ttag_re = re.compile(TagExpression).search\n\t\ttags = set()\n\t\twhile tag_re:\n\t\t\ttag_match = tag_re(text, tag_repl=(tag_repl & TagExpression()).regex)\n\t\t\tif not tag_match:\n\t\t\t\ttags |= set(re.findall(tag_repl, text, re.IGNORECASE))\n\t\t\ttag_re = re.match(tag, tag_match.group())\n\t\treturn tags, text\n\n\treturn tags, text\n\n\n@dataclass\nclass Template:\n\t\"\"\"\n\tDefines template for code blocks"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\ttagger = replacer if replacer is not None else lambda x: x\n\tif replacer == \"\":\n\t\ttagger = tagger_function\n\ttext1 = text\n\ttags = set()\n\twhile True:\n\t\tmatches = set(tagger(text1))\n\t\tif matches == REMOVE_BLOCK:\n\t\t\tbreak\n\t\tif matches!= REMOVE_CODE:\n\t\t\tmatches = re.sub(DELETE_MARKER_REGEX, \"\", matches)\n\t\t\tif not matches in tags:\n\t\t\t\tnew_text = re.sub(delete_marker_regex"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\tret = set()\n\t\ttext = str(text)\n\t\tfor match in TALKES.tag_re.finditer(text):\n\t\t\tfor tag in re.findall(tag_func, match.group(0)):\n\t\t\t\tif replacer:\n\t\t\t\t\tmatch = RE_TAGS_REPLACE.sub(repl=token, text=tag, match=match)\n\t\t\t\telse:\n\t\t\t\t\tmatch = RE_TAGS.sub(repl=token, text=tag, replace=match)\n\t\t\t\tret |= find_tags(match, replacer)\n\t\treturn ret\n\n\tdef"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\torig, replaced = [], []\n\t\ttagged = []\n\t\tfor token in re.findall(r\"(?u)\\S.*?:\", text):\n\t\t\t# Tag words have their own look-up table.\n\t\t\ttagged_token = tag_pattern.sub(token, token).lower()\n\t\t\t# Replace the last word in the replacer.\n\t\t\tword = re.sub(r\"(?u)\\S{2,}\", tagged_token, text)\n\t\t\treplaced.append(word)\n\t\t\tword = re.sub(r\"(?u)\\S{2,}\", replaced[-1], word)"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\tdata = __text_to_tags_tag(text, replacer)\n\t\treturn [tag for tag in set(re.findall('@[\\w-]+[.:]', data)) if tag!= \" \"]\n\treturn set(), text\n\ndef strip_tags(text: str) -> Set:\n\t\"\"\"Strip tags from text, removing all non-alphabetical tags.\n\nTries to strip tags inside tag text, or tags inside tag_string string\nwith the regular expression defined in _tag_pattern.\n\nReturns a list of new text (as a set of strings from RE"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\ttags: Set[str]  # noqa: E123  # untested\n\t\ttext: str = s.text\n\n\t\tdef replacer_wrapper(match: bool) -> str:\n\t\t\treturn replacer(text, match.end(0))\n\n\t\tdef search_pattern(tags: Set[str]) -> str:\n\t\t\treturn text.replace(text[0], tag)\n\n\t\tif replacer is None:\n\t\t\treturn tags.keys()\n\t\telse:\n\t\t\treturn search_pattern\n\n\treturn re.findall(r\"#.*\", text)\n\n\n"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\tp = re.compile(r\"(\\b\\w+)\")\n\t\tmatches = p.findall(text)\n\t\tif matches:\n\t\t\treturn (matches[0], matches[1] + \"\\n\" + text)\n\t\telse:\n\t\t\treturn self.replacer(text)\n\n\tdef replace_tags(self, matches: Set, replacer: Callable = None) -> None:\t\"\"\"\n\tMatch every tag in text.\n\nTries to skip tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\tres = set()\n\t\treplacer_options = [word.strip() for word, replacer in replacer.options.items() if replacer]\n\t\tfor replacer in replacer_options:\n\t\t\tres.update(replacer(line_break=False))\n\t\treturn res\n\n\t# Try to find all tags\n\ttags = find_tags.func_closure\n\tif not tags:\n\t\t# if no replacer function was specified, use the defaults\n\t\ttags = set()\n\t\tdefault_tags = replacer.options"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\tif text: return (None, set())\n\t\ttext = re.escape(text)\n\n\t\ttags = set() # type: Set[str]\n\t\tregex = re.compile('^'+text+'$')\n\t\tregexl = regex.findall(text)\n\t\twhile regexl:\n\t\t\ttag, text = regexl.pop()\n\t\t\ttags |= set([tag])\n\t\t\tregex = re.compile('^'+text+'$'.format(tag) )\n\n\t\ttagged_tags = list(tags)\n\t\ttagged_regex = re.compile('|'.join(regex))\n\t\ttext ="}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\t\"\"\"\n\t\t# We assume all text is unicode. Replace them with None to do the replacement in the\n\t\t# future.\n\t\t# Note: python will parse 'unicode' as string!\n\t\ttext = strip_tags(text)\n\n\t\t# Find tags\n\t\ttags = textlib.get_data(text, replacer)\n\t\t# Strip tags\n\t\ttags = tags.split()\n\t\treturn set(tags)\n\n\telse:\n\t\treturn set()\n\n\ndef find_lines_from_file(lines_file: str):\n\t\"\"\"\n\tReads line by line in file and"}]}
{"task_id": "6306292352e177c0ba469f1e", "prompt": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\ttext =''.join(text.split()).splitlines()\n\t\ttext = _keep_urls(text)  \n\t\treturn text\n\n\ts = 'http://www.filmschafat.fr/filmschafat/serien/index/liste/film-en/echever/'\n\treturn s.split()[0]\n\n\ndef extract_text(doc, url):\n\ttry:\n\t\tsoup = BeautifulSoup(doc, 'html.parser')\n\texcept:\n\t\ttext = soup.text.replace('(','')\n\t\ttext = text."}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\tfor match in LINK_PATTERN.finditer(text):\t# Find all LINKS\n\t\t\tnew_text = text\n\t\t\turl = match.group('text')\n\t\t\tif not url.startswith('http'):\n\t\t\t\t# Skip links that are only http links\n\t\t\t\tcontinue\n\t\t\tif not url:\t\t\t\t\n\t\t\t\t# Normalize URLs so URLs are converted to URLs when appropriate\n\t\t\t\turl = url.replace('\\u0026', '\\ufeff')\t\t\t# Unescape ampersands ('&')\n\t\t\t\tnew_text = new_text.replace('&','&amp;')\t\t# Must be done 1st"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\t# split link, type should be text and page to linkify\n\t\tif 'link' in text.attrib:\n\t\t\tlink = text.attrib['link']\n\t\telse:\n\t\t\tlink = text.text\n\n\t\trel = url_re.match(link)\n\t\tif rel:  # it's a real link!\n\n\t\t\tif is_archive_link(text):\n\t\t\t\ttext.drop_tree()\n\t\t\t\treturn link\n\n\t\t\telif is_html_link(text):\n\t\t\t\ttext.drop_tree()\n\t\t\t\treturn text\n\n\n\t\t# process links to other links\n\t\t"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\tfor match in LINK_RE.finditer(text):\n\t\t\t# Add the URL and title of the link into the dictionary\n\t\t\t# Only include links with the source text, not URL\n\t\t\tif not LINK_PATTERN.match(match.group(2)):\n\t\t\t\tlink = {'type': match.group(1), 'value':  match.group(3)}\n\t\t\t\tlinks.append(link)\n\n\treturn links\n\ndef remove_anchor_tags(text):\t\t\"\"\"\n\tRemove <a> tags from the text using regular expressions.\n\n\t:param text: text to be processed"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\tmatch = re.search(\"href\\=\\\"([^\\\"]+)\\\".*?\\\"([^\\\"]+)\",text)\n\t\tif match:\n\t\t\treturn \"%s%s\" % (match.group(1), match.group(2))\n\t\telif text.startswith('-'):\n\t\t\treturn \":%s%s\" % (self.linkify(text[1:]),text[1:])\n\t\telse:\n\t\t\treturn \":%s %s%s\" % (self.linkify(text[0]),text[1:],text[2:])\n\nclass IContentFeed(object):\n\t\"\"\""}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\tpass\n\n\nclass UnquoteHTMLParser(HTMLParser):\n\t\"\"\"Replaces <p> tags with a blank value.\n\n\tThis removes leading and trailing whitespace.\n\t\"\"\"\n\tdef _unescape(self, s):\n\t\treturn s.replace('<p>', '').replace('</p>', '')\n\n\tdef _find_next_link(self, m):\n\t\twhile True:\n\t\t\tm = m.next_link\n\t\t\tif m is not None: break\n\t\treturn m\n\n\tdef handle_starttag(self, tag, attrs):\n\t\t# Tag should be first\n\t\tattrs = dict"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\t# First preprocess tags\n\t\ttag_patterns = [\n\t\t\t(r\"<.*?>\", True),\n\t\t\t(r\"<.*?>).*?\", True),\n\t\t\t(r\"<.*?>)\", True),\n\t\t\t(r\"<.*?>\\s*<\", False),\n\t\t\t(r\"\\\\\\n\", True),\n\t\t]\n\t\t\n\t\tfor pattern in tag_patterns:\n\t\t\tfor match in pattern.finditer(text):\n\t\t\t\tif match.group(1) is self.tag_separator:\n\t\t\t\t\t# Ignore leading spaces and leading newlines\n\t\t\t\t\tcontinue\n\t\t\t\telif match.group(1) is None:"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\t# Convert inline links and URL links into Unicode characters\n\t\tfor match in re.finditer(\"\"\"<\"(.*)\">\"\"\", text, re.DOTALL):\n\t\t\ttext = text.replace(match.group(), unichr(int(match.group(1), 16)))\n\t\t# Convert special links and URLs links into Unicode characters\n\t\tfor match in re.finditer(\"\"\"<\"(.*?)\"\"\"):\n\t\t\ttarget = \"\"\n\t\t\tif match.group(2):\n\t\t\t\ttarget = \"http:\"+match.group(2)\n\t\t\t\tif match.group(1).find('http') < 2:\n\t\t\t\t\ttry"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t \n\t# Replace all occurrences of <br/> to XHTML with strong text <span>...</span>\n\ttext = re.sub('\\[\\s*<(br|span)[^>]?(?P<text>.*?)<\\1>', r'<strong>\\1</strong>', text)\n\t\n\t# Strip everything between tags\n\ttext = re.sub('<[^>]+>', '', text)\n\t\n\t# Process links to text\n\ttext = process_text_links(text)\n\t\t\t \t\n\treturn text\n\t\nimport xml.sax\nimport urllib.request\n\nclass Parser():\n\t\"\"\"\n\tParse"}, {"generated_text": "def process_text_links(text):\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\t\"\"\"\n\t\t# <h1 class=\"title\">Title</h1>\n\t\t#  A title is a <a...> tag\n\t\t#  A link is a <a...> link\n\t\t#  A footnote is <a...> <a...>\n\t\t#  A list of footnote labels of a paragraph of text\n\t\t#  A footnote definition is <a...>\n\t\t# NOTE: Links are either sections or the root, although not all sections are allowed inside them\n\t\t#  However the <a...> or <a...> sections do not necessarily have backlinks"}]}
{"task_id": "6306292652e177c0ba469f34", "prompt": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\ttry:\n\t\t\t# for non-HTTP responses, let's avoid showing an HTTP 404 error\n\t\t\trequests.get(url, timeout=5)\n\t\t\treturn ''\n\t\texcept requests.exceptions.RequestException as e:\n\t\t\tif e.response.status_code >= 500:\n\t\t\t\traise e\n\traise ValueError('Fetching %s failed!' % url)\n\ndef parse_json(response: Any, content_type: str) -> Optional[Dict[str, Any]]:\n\tresult: Mapping[str, Any] = {}\n\tif not content_type:\n\t\tcontent_"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tif url is None:\n\t\t\treturn None\n\t\telif not path.exists(url):\n\t\t\treturn None\n\t\twith url.open('rb') as f:\n\t\t\treturn f.read().decode('UTF-8')\n\treturn 'binary/'\n\nclass RemoteContent(ModelReprMixin, models.Model):\n\t\"\"\"\n\tThis table is read-only snapshots of content received over a\n\tremote server from the API. The content is used as an index to\n\tcheck the version number of the file.\n\n\tMetadata about the remote content type:\n\t\t- remotecontent\n\t\t\t"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\turl = get_referring_url(url)\n\t\tif '<meta name=mirror-message>', url:\n\t\t\treturn'mirror-message'\n\t\tif '<' not in url:\n\t\t\treturn 'plain'\n\t\thead, data = url.split('<')\n\t\tif content_type_match(head):\n\t\t\treturn 'application/octet-stream'\n\t\tif '<title>404 - Not Found</title>' in data:\n\t\t\treturn None\n\t\treturn 'text/html'\n\n\ndef set_message_deletions_flag(url: str, deactivated: bool) ->"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\ttry:\n\t\t\tr = requests.head(url)\n\n\t\t\t# check if content type is detected in response\n\t\t\tif r.status_code == 200:\n\t\t\t\treturn \"text/html\"\n\t\t\telse:\n\t\t\t\treturn r.status_code\n\t\texcept RequestsConnectionError:\n\t\t\treturn None\n\treturn None\n# -*- coding: utf-8 -*-\nfrom south.utils import datetime_utils as datetime\nfrom south.db import db\nfrom south.v2 import DataMigration\nfrom django.db import models\n\nclass Migration(DataMigration):\n\n    def forwards(self, orm"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\thttp_response = requests.get(url)\n\t\thttp_response.raise_for_status()\n\n\t\t# If we were able to determine the type of the file we were getting, assume\n\t\t# that it exists in case it's a remote URL.  If not, fall back to\n\t\t# a filename based off of the filename after doing the normal fetch and\n\t\t# assuming that it's a file URL.\n\t\tcontent_type_match = FILE_TYPE_PATTERN.match(http_response.headers[\"content-type\"])\n\t\tif content_type_match is None:\n\t\t"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tif os.path.isfile(fetch_cache(url, stream_url=None, validate_certificate=False)):\n\t\t\twith open(fetch_cache(url, stream_url=None, validate_certificate=False)) as f:\n\t\t\t\tcontent_type = f.read().strip()\n\t\t\treturn content_type\n\n\treturn fetch_content_type(url)\n\ngit_log = logging.getLogger(__name__)\n\ndef fetch_github_file(url: str) -> str:\n\t\"\"\"\n\tFetch the contents of a GitHub URL to determine the object name\n\t"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\trequest = urllib.request.Request(url)\n\n\t\tresponse = urllib.request.urlopen(request)\n\t\tif response:\n\t\t\tcontent_type = response.info().get(\"Content-Type\")\n\n\t\t\tif content_type is None or content_type == \"text/plain\":\n\t\t\t\t# Strip the status line\n\t\t\t\tline = response.read()\n\t\t\t\tline = line.strip()\n\t\t\t\tif line:\n\t\t\t\t\tif type(line) is bytes:\n\t\t\t\t\t\tcontent_type = line.split(\";\")[0]\n\t\t\t\t\telse:\n\t\t\t\t\t\tcontent_type = line\n\t\t\telse"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\tresult = http_proxy_support.sync_request(url, method=method)\n\t\ttry:\n\t\t\t# Read the response to determine the server's overall type.\n\t\t\tcontent_type = result[2]\n\t\texcept (ValueError, IndexError):\n\t\t\tprint(f\"Could not read remote server response: {result}\")\n\t\t\t# In case of any unexpected response, set content_type to '', so\n\t\t\t# we consider the content type as opaque.\n\t\t\tcontent_type = 'application/octet-stream'\n\t\t\tprint(f\"Unable to determine remote server response content-type of"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\thttp = urllib3.PoolManager(cert_reqs='CERT_REQUIRED')\n\t\tis_success, content_type = http.request(url)\n\n\t\ttry:\t\n\t\t\tif is_success:\t\t\n\t\t\t\tmatch = CONTENT_TYPE_REGEX.match(content_type)\n\t\t\t\tif match:\t\treturn match.group(1)\n\n\t\texcept AttributeError:\n\n\t\t\treturn None\n\n\traise ValueError('no content type given')\n\ndef fetch_body(url: str) -> bool:\n\t\"\"\"\n\tFetch the body of the remote url to determine the content type\n\t"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\t\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\t\"\"\"\n\t\ttry:\n\t\t\t# int() doesn't work on Python 3, use it in the\n\t\t\t# future\n\t\t\turl_str = _native_str(url.encode('ascii'))\n\t\texcept (ValueError, TypeError):\n\t\t\treturn \"application/octet-stream\"\n\t\tdecoded = url_str.decode('ascii')\n\t\t# Check over all common magic values and surprise\n\t\tif _all_magic(decoded)!= \"Content-Type\":\n\t\t\ttry:\n\t\t\t\treturn decoded.decode()\n\t\t\texcept UnicodeDecodeError:\n\t\t\t\tpass\n\n\traise IOError(\"The given URL is invalid\")"}]}
{"task_id": "6306292a52e177c0ba469f41", "prompt": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\tif tag in EXTERNAL_TAGS:\n\t\t\treturn True\n\t\telif tag in NON_TAGPHRASE_LANGUAGES:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\nINTERNAL_TAGS: Dict[str, str] = {}\n\ndef remove_exlc_tags(text: str) -> str:\n\ttags = [str(tag) for tag in text.split(\",\") if tag in EXTERNAL_TAGS]\n\tfor tag in tags:\n\t\tkey = EXTERNAL_TAGS[tag]\n\t\ttags.remove(key)\n\treturn \" \".join(tags"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\treturn tag in word_tags\n\n\ndef tag_by_name(tag: str) -> bool:\n\treturn tag_matches_exact(word_patterns, tag, name=True)\n\n\ndef tag_by_number(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag with a number.\t\"\"\"\n\treturn tag_matches_exact(word_patterns, tag, number=True)\n\n\ndef tag_by_number_word(tag: int, name: bool) -> bool:\t\"\"\"\n\tReturns True if and"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\ttag = str(tag)\n\t\tif tag.lower() in valid_tags:\n\t\t\treturn True\n\t\telif tag.isupper() and tag.islower():\t# This will be the case which the regex matches\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\treturn False\n\n\n\"\"\"\n\tValidate and format user name and email address\n\t\"\"\"\ndef check_valid_username(name: str, email: str, address: str, password: str) -> bool:\n\tif len(name) < 6:\n\t\treturn False\n\tif len(name)"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\tres = True\n\t\tif tag in get_tags('i'):\n\t\t\tif isinstance(res, bool):\n\t\t\t\tres = res and result_flag(tag, int(round(res)))\n\t\t\telse:\n\t\t\t\tres = result_flag(tag, 0)\n\t\treturn res\n\n\tif tag in get_tags('i'):\n\t\tif isinstance(res, bool):\n\t\t\tres = res and result_flag(tag, int(round(res)))\n\t\telse:\n\t\t\tres = result_flag(tag, 0)\n\treturn res\n\n\ndef execute(cmd: str"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\treturn False\n\n\tdef check_required_tag(tag: str) -> bool:\n\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\treturn False\n\n\tdef check_tag(token_tag: str) -> bool:\t\t# pylint: disable=W0622,W0221\n\t\t\"\"\"\n\t\tTest if token is valid.\n\n\t\t:param token_tag: The token that should be tested.\n\n\t\t:return: True or False.\n\t\t\"\"\"\n\t\tif token_tag and not token_tag.lower().strip():\n\t\t\t"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\treturn len(re.findall(\"[A-Za-z\\s]+\", tag))>=2\"\"\"\nTests for dit.maths.matrices.matrices module.\n\"\"\"\n\nimport numpy.linalg\n\nimport dit\nfrom dit.maths.matrices import matrices\n\n\n# pylint: disable=wrong-import-order, wrong-import-position\n\n\ndef almost_equal(a, b, atol=1e-10, rtol=1e-8):\n  \"\"\"\n  Test the matrix almost-equality between two numpy arrays.\n\n  Args:\n    a: numpy array object,"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\ttry:\n\t\t\tchar = chr(tag)\n\t\texcept ValueError:\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\treturn False\nfrom django.db import models\nfrom django.utils.translation import ugettext_lazy as _\nfrom oscar.apps.catalogue.abstract_models import Bookmark\nfrom oscar.apps.offer.managers import ActiveOfferManager\nfrom oscar.apps.dashboard.orders.forms import OrderForm, SavedLineForm\nfrom oscar.core.loading import get_model\nfrom oscar.templatetags.currency_filters import currency\n\n\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\tif tag in ('y', 'ye', 'yu', 'yep', 'yup'):\n            return True\n\t\telif tag in ('d', 'dle', 'dcy', 'dys', 'dysy', 'dc', 'dd', 'ddgy', 'dt', 'dty'):\n            return True\n\t\telse:\n\t\t\treturn False\n\tif (text_type(tag) is not bool):\n\t\treturn tag in ('y', 'ye', 'yu')\n\treturn False\n\ndef main():\n\timport sys\n\tif (len(sys.argv) =="}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\treturn tag in TAGWORD_ACCEPTATIONS\n# -*- coding: UTF-8 -*-\n# Copyright (C) 2012 HEN Zheng Zhang <hezihanz6@gmail.com>\n# Copyright (C) 2012 Roger Lew [see LICENSE.txt]\n\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option) any later version.\n\n# This program is distributed in the"}, {"generated_text": "def test_tag(tag: str) -> bool:\t\"\"\"\n\tTest a word whether it could be accepted as a tag.\t\"\"\"\n\t\tassert tag(\"foo\")\n\t\tassert token(\"foo\")\n\t\tassert not match(\"asdfgh\").match(token(\"foo\")), repr(list(token(\"foo\")))\n\t\n\tdef test_lemma(self):\n\t\t\"\"\"\n\t\tThe lemma tag returns the token, pos. and part of the parsed sentence.\n\t\tYou can also access this data via get_token().\n\t\tYou cannot pass in any POS tags:\n\t\t>>> from nltk.tag.util import untag\n\t\t>>> untag([\"foo\", \"bar\"]) == [(\"foo\", \"bar\")]\n\t\tTrue\n\t\t"}]}
{"task_id": "6306298b52e177c0ba469fdc", "prompt": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tfor n in node.children: \n\t\t\tn_dict = {}\n\t\t\ttag_name = n.tag\n\t\t\tif tag_name in n_dict:\n\t\t\t\tn_dict[tag_name].append(n)\n\t\t\telse:\n\t\t\t\tn_dict[tag_name] = [n]\t\t\t\t\n\t\t\t# print( \"Found \" + str(n.tag) + \" from node: \" + str(n.get(\"tag\")) )\n\treturn(n_dict)\n\n\ndef xml_to_dict(doc):\n\t\"\"\"\n\tTurn a text string to"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tchildren = []\n\t\tfor child_node in node.childNodes:\n\t\t\tchildren.append(dict_node_to_dict(child_node))\n\t\treturn(children)\n\t\t\n\txml_children = iterwrap(xml_children_as_dict(node))\n\n\tif node.nodeType!= xml.dom.Node.TEXT_NODE:\n\t\traise ValueError('Unexpected return type %d' % node.nodeType)\n\tassert len(node.childNodes) > 0\n\t\n\txml_children[-1].update(node.firstChild)\n\treturn(xml_children)\n\t\n\t\nxml_"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\ttag_name = node.tag\n\t\t\n\t\tif tag_name == \"children\":\n\t\t\tfor child in node.getchildren():\n\t\t\t\t#print(child)\n\t\t\t\t#print(tree2dict(child))\n\t\t\t\t#print(etree.tostring(child))\n\t\t\t\t#pprint(child.text)\n\t\t\t\t#print(child.tail)\n\t\t\t\tyield child\n\t\telif tag_name == \"items\":\n\t\t\tfor child in node.getiterator(tag_name, is_dict=True):\n\t\t\t\t#print(child)\n\t\t\t\t#print(\"name:\",child.tag"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tresult = {}\n\t\ttagName = node.tagName\n\t\ttagType = None\n\t\tif node.text is not None:\n\t\t\ttext = node.text\n\t\telse:\n\t\t\ttext = \"\"\n\n\t\tif node.tail is not None:\n\t\t\ttail = node.tail\n\t\telse:\n\t\t\ttail = \"\"\n\t\tif node.attrib is not None:\n\t\t\tfor k in node.attrib:\n\t\t\t\tresult[k] = text\n\t\telse:\n\t\t\tfor k in [\"text\", \"tail\"]:\n\t\t\t\tif k in text:\n\t\t\t\t\tresult[k] = self"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\treturn dict((x.tag.lower(),x) for x in [n.attrib for n in node.items])\n\n\tdef xml_dump(node_to_dump, out=sys.stdout):\n\t\t\"\"\" \n\t\tDumps the result of xml_dump() and writes result to out.\n\t\tnode_to_dump is a dict, keyed by tag name, with XML comments preserved.\n\t\t\"\"\"\n\t\tif type(node_to_dump) in dict:\n\t\t\tout.write('node:'+ node_to_dump.tag)\n\t\t\tfor key in list("}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\toutput = {}\n\t\toutput[node.tag] = node\n\t\tfor child in node:\n\t\t\tif child.tag == 'end':\n\t\t\t\t# print \"end child is : \", child\n\t\t\t\toutput[child.tag] = 1\n\t\t\telif child.tag == 'text':\n\t\t\t\toutput[child.tag] = tree.Text(child).asText()\n\t\treturn output\t\t\n\n\n# main()\n#\n# Parse arguments\n#   args         : [string]\n#\t\t\t\t  : Name of the input xml node that includes XML tags\n#   output     : Optional dictionary"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tfor child in node:\n\t\t\tif not child.is_child_node():\n\t\t\t\tyield {\n\t\t\t\t\t'path': child.get_path(),\n\t\t\t\t\t'tag': child.tag,\n\t\t\t\t\t'elem': child\n\t\t\t\t}\n\t\t\telse:\n\t\t\t\t#if child.is_parent_node():\n\t\t\t\t#\tyield child.as_dict()\n\t\t\t\tyield child\n\n\tdef as_dict(self):\n\t\t\"\"\"\tReturn a python dictionary representing the node in a xml text node.\n\n\t\tThis is to be compatible with previous versions, including:\n\n\t\t\t- ``comment``, ``"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tdict_nodes = dict()\n\n\t\ttag = node.tag\n\t\tif tag == 'child':\n\t\t\tdict_nodes[tag] = self.parse_child(node)\n\t\telse:\n\t\t\tdict_nodes[tag] = self.parse_node(node)\n\n\t\treturn dict_nodes\n\n\tdef parse_child(self, node):\n\t\t\"\"\"\n\t\tFor the given xml.etree XML node, parse the xml.etree.ElementTree.Element for a class, and set this tag as a child tag.\n\t\tThe class is then considered a child of the same parent node."}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tres = {}\n\t\tfor child in node:\n\t\t\tif hasattr(child, 'childNodes'): res[child.nodeName] = {}\n\t\t\tres[child.nodeName]['name'] = child.nodeName\n\t\t\tfor childelement in child.childNodes:\n\t\t\t\tif childelement.nodeType == 3:  res[childelement.nodeName][childelement.nodeName] = self.fill(childelement)\n\t\treturn res\n\n\tdef fill(self, value):\n\t\t\"\"\"Convert a value in various formats to their proper types.\"\"\"\n\t\tif type(value) == types.StringType: return value\n\t\tif"}, {"generated_text": "def xml_children_as_dict(node):\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\t\"\"\"\n\t\tretval = {}\t\n\tif node.getchildren() is []:\n\t\treturn retval\n\tchildren = node.getchildren()\n\tfor curElem in children:\n\t\tif curElem.tag == \"type\":\n\t\t\tretval[curElem.tag] = {'attrs':{}, 'type':curElem}\n\t\telif curElem.tag == \"string\":\n\t\t\tretval[curElem.tag] = curElem.get('type',\"\")\n\t\telif curElem.tag == \"class\":\n\t\t\t# we have a class, just recurse\n\t\t\tif curElem.text:\n\t\t\t\tretval"}]}
{"task_id": "6306299052e177c0ba469fe8", "prompt": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\ndef get_message_type(mtype):\n\t\"\"\"\n\tGiven a message type in this format `type/*`, return the associated type - the same type that was sent through the message.\n\tReturns None if the message type is not supported or invalid. Otherwise, it returns the type of the respective\n\tsender and entity (used for testing purposes only)\n\t\"\"\"\t\n\tif mtype not in MessageTypes:\n\t\treturn None\n\t\n\tif mtype == MessageTypes[mtype][0]:\n\t\treturn mtype\n\telse:\n\t\treturn mtype, None\n\t\t\t\t\t\t\t\ndef"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\tsender_entity = sender_handle.entity if sender_handle else entity_handle.entity\n\t\tactor_entity  = entity_handle.entity\n\t\tsender_handle_entity = sender_entity.owning_entity\n\t\tsender_handle_entity.entity  = sender_entity_handle\n\t\tsender_handle_entity.handle = sender_entity\n\n\t\t#We then verify that both the two users are on the same entity.\n\t\t#We assume the sender handle and entity matches\n\t\tsender_entity.handle = sender_handle\n\t\trecipient_entity.handle = entity_handle\n\t"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\t# if the sender is present, make it a match\n\t\tif not recipient_match(sender_handle, entity_handle):\n\t\t\treturn False\n\n\t\t# if the entity is present, check that the match is for all the related author\n\t\tquery = { 'object': entity_handle }\n\t\tquery = { 'author_id':  author_id,'sender_id': sender_id }\n\t\tresults = DBQ.match_data(sender_handle,  query, conn)\n\t\tresult_count = results['count']\n\n\n\t\t# if any result match found, return"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t global sender_handle, entity_handle\n\n\t\tsender_handle = sender_handle.strip()\n\t\tentity_handle = entity_handle.strip()\n\t\t\n\t\tif sender_handle!= sender_handle.lower() or entity_handle!= entity_handle.lower():\n\t\t\treturn False\n\n\t\tif sender_handle in admin_users.values():\n\n\t\t\tadmin_user_id = int(admin_users.get(sender_handle, -1))\n\n\t\t\tif (admin_user_id == owner.id):    \n\t\t\t\treturn False\n\n\t\t\n\t\t\towner_db = db.DBSession()\n\n\t\t"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\n\n\tsender = sender_handle.split()\n\tentity = entity_handle.split()\n\tsender_entity = sender[0]\n\tentity_entity = entity[0]\n\tif not sender_entity in sender_entity:\n\t\t# FIXME: Check why we do not check if this is really a user, so the sender may have been the recipient to\n\t\t# whom he/she is doing their action.\n\t\treturn\n\n\tsender_data = {}\n\n\trecipient_data = {}\n\thas_user_id = ''\n\tobject_type = None\n\tif sender"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\tif sender_handle!= entity_handle:\n\t\t\t# Make sure the sender's sender matches the object's sender in the same way\n\t\t\tif sender_handle.get_sender() == entity_handle.get_sender():\n\t\t\t\t\t# sender message is the same\n\t\t\t\t\tif sender_handle.get_state_name() == entity_handle.get_state_name():\n\t\t\t\t\t\t# sender message is the same\n\t\t\t\t\t\tif sender_handle.get_data() == entity_handle.get_data():\n\t\t\t\t\t\t\tfor c in sender_handle.get_contacts():\n\t\t\t\t\t\t\t\tif c.get_"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\tif sender_handle.name == entity_handle_name:\n\t\t\tsender_name = entity_handle_name\n\t\telse:\n\t\t\tsender_name = get_sender_name_by_object_name(sender_handle)\n\n\t\t# first find any sender who this object is in - if there are no author in the sender, this is a problem?\n\t\t# if so, then we haven't verified the sender yet (and we should have one active connection in the future)\n\t\tif sender_name is None:\n\t\t\tsender_name = None\n\n\t\t# Now test the author and"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\t# TODO: Validate sender / recipient\n\t\tif sender_handle!= sender_recipient:\n\t\t\traise Exception(f'Unexpected payload sender address(s) (should be {sender_handle} and {sender_recipient}, {sender_handle.recipients()})')\n\tif entity_handle!= entity_recipient:\n\t\traise Exception(f'Unexpected payload entity address(s) (should be {entity_handle} and {entity_recipient}, {entity_handle.recipients()})')\n\t# TODO: Validate recipient\n\tsender_address = sender_handle.address\n\trecipient_address = recipient_"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\tsender_match = models.Sender.objects.filter(username=sender_handle)\n\t\tactor_match = models.Actor.objects.filter(username=entity_handle)\n\t\tsender_match_key = sender_match[0].key.urlsafe()\n\t\trecipient_match_key = entity_handle.key.urlsafe()\n\t\tpayload_match_key = sender_match[0].key.urlsafe()\n\t\t# The payload itself hasn't sent it yet...\n\t\tif not key in payload_match_key:\n\t\t\tsender_key = entity_handle.key.parent"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\t\"\"\"\n\tEnsure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\t\"\"\"\n\t\ndef confirm_sender_message(sender_message, current_receiver_message):\n    # We've received the sender's message\n    current_sender_message['sender_user_id'] = sender_message['sender_uid']\n    current_sender_message['sender_id'] = sender_message['sender_uid']\n    current_sender_message['sender'] = sender_message['sender']\n    current_sender_message['recipient'] = sender_message['recipient']\n    current_sender_message['message'] = sender_message['message']\n    current_sender_message['sender_recipient']"}]}
{"task_id": "630629b952e177c0ba46a043", "prompt": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\n\n\tif document_path is None:\n\t\tdocument_path = url\n\n\tif not url.startswith('http'):\n\t\turl = 'http://' + url\n\n\treturn dict(\n\t\ttitle       = os.path.basename(document_path).split('.')[0],\n\t\tversion   = '2.0',\n\t\tabout       = 'The Comput Relay Station of Computers',\n\t\tlicense      = 'MIT',\t   \n\t\tauthor       = 'Andres Breuer',\n\t\tauthor_email = 'andrelis.breuer@gmail.com',\n\t\turl"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tif document_path is None:\n\t\t\tdocument_path = url\n\t\t\n\tdebug('Trying to fetch well-known nodeinfo document at %s', document_path)\n\tdocument = fetch_json_without_http_info(url, document_path)\n\tdebug('Got nodeinfo document')\n\treturn doc_to_well_known(document)\n\t\ndef _add_new_nodes(doc, parent_nodenames=[], parent_elementtype='node',\n\treturn_nodes=False):\n\t# Helper function for adding nodes in the document\n\tdef find_next_"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tif document_path: document_path = os.path.abspath(document_path)\n\t\treturn get_nodeinfo_well_known_document(url, document_path)\n\t# get_nodeinfo_well_known_document()\n\ndef get_nodeinfo_dbpedia_version():\n\treturn get_nodeinfo_dbpedia_path(server_url, \"dbpedia.xml\")\n\ndef find_available_port(my_default=None):\t# deprecated for 0.6.14\n\t\"\"\"DEPRECATED: use find_available_port"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\treturn None\n\nclass NodeInfo(object):\n\t\"\"\"A node information object for a single KonText node\"\"\"\n\t\n\tdef __init__(self, nodedir, datadir, nodeid, name, parent):\n\t\tself._nodeid = nodeid\n\t\tself._name = name\n\t\tself._parent = parent\n\t\tself._id = nodeid.lstrip(os.path.sep)\n\t\tself._name_lower = name.lower()\n\t\tif self._name_lower.endswith(\".well-known\"):\t# It can happen that we'll just be sure it's a"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\n        query_parts = _get_node_info_query_parts(url)\n\t\n        response = self._post(query_parts, data, query_index, return_empty_response=True)\n\t\t\n\t\tif response:\n\t\t\tdata = self._parse_result_part_tree(response, result=NodeInfo_WellKnownDocument.SCHEMA_KEY_RESULT_KEY, return_empty_results=False)\n\n\t\treturn data\n\t\t\t\n    def get_nodeinfo_well_known_format(self, url, format, document_path=None):\t\t\"\"\"\n\tFind the"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\n        \"\"\"Set up the HTTP parameters used by the request\"\"\"\n        params = {'url': url, 'document_path': document_path}\n\n        doc = None\n\n        try:\n                doc = get_soap_document(url)\n            except:\n                pass\t# if not, return dummy server response\n\n        except Exception, exception:\n                if exception.__class__.__name__ in [Exception21,\n                                                          Exception32.TypeException]:\n                        print \"Error:\", exception\n                    else:\n                        print 'Failed to build the SOAP request', exception\n\n\n                if not(hasattr(sys, \"_getframe\")):\n                        sys._getframe = inspect.currentframe"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\n        # Make sure a url was given either absolute or relative.\n        if url.startswith('file://'):\n            url = url[7:]\n\n        # Construct a dict of information for this server, based on the\n        # document's path and document URL as given.\n        document_info = {}\n\n        if document_path:\n            document_info['path'] = os.path.join(document_path, doc_path)\n\n        elif url.startswith('http://nodeinfo'):\n                # Opens the document at a URL.\n                try:\n                        # Opens the URL.\n                        document = urllib2.urlopen(url"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tassert url.find(\"https://my.diaspora.org/diaspora/+index.html\")!= -1\n\t\tassert url.find(\"opendata/opendata.xml\")!= -1\n\n\t\tif document_path is None:\n\t\t\tdocument_path = \"opendata.xml\"\n\t\turl = \"%s%s\" % (url, document_path)\n\n\n\t\tm = re.search(\"^node_id=(.+)\", url)\n\t\tif not m:\n\t\t\treturn\n\n\t\tnode_id = m.group(2)\n\n\t\t"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tnodeinfo_url = _make_bare_url(url)\n\t\tnodeinfo_document = _make_bare_url(_make_bare_url(document_path))\n\t\tnodeinfo_body = _make_bare_url(\n\t\t\tf\"http://nodeinfo.diaspora.software/nodes/{nodeinfo_url}/\")\n\t\turl_document = _make_bare_url(nodeinfo_document)\n\t\thttp = httplib2.Http()\n\t\thttp = credentials(\"node.softwareauth.com\")\n\t\thttp.add_credentials(_get_user"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\t\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\t\"\"\"\n\t\tmetadata_elem = get_document_with_metadata(url, document_path)\n\t\tif os.path.basename(document_path)!= \"NodeInfos.well-known\":\n\t\t\traise Exception(\"Wrong path for Document at \" + url)\n\t\treturn metadata_elem\n\t\n\tdef format_document_for_write(doc_elem):\n\t\t\"\"\"\n\t\tFormat a Document for a document node.\n\t\t\"\"\"\n\t\tdocument = doc_elem.document\n\t\tif document is None or document.get_uri() is None:\n\t\t\treturn \"\"\n\t\t# We want"}]}
{"task_id": "630629d052e177c0ba46a0a1", "prompt": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t global VERIFY_PAYLOAD_PAYLOAD_PROOF_MESSAGE, VERIFY_PAYLOAD_PROOF_MESSAGE_PROOF, VERIFY_PAYLOAD_PAYLOAD_PROOF_MESSAGE_END_TAG, VERIFY_PAYLOAD_PROOF_END_TAG\n\n\t\t# verify presence\n\t\tverify_signed_element(public_key)\n\t\tverify_message(doc)\n\n\t\t# verify signatures\n\t\tverify_signed_element(doc.documentElement)\n\n\t\t# verify signature matches\n\t\tverify_verify_xml_element(signature, signature)\n\n\ndef verify_signing_key(key):\n\t"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\tif not public_key.verify_key(XKEY_PEM, signature, \"sha256\"):\n\t\t\traise exceptions.UnexpectedDERError('public key signature could not be verified, but verify failed.')\n\t\t# signature version should be the one in the draft spec.\n\t\tif signature!= X509_SIGNATURE:\n\t\t\traise exceptions.UnexpectedDERError('public key signature is wrong')\n\n\tif hasattr(doc, \"verify_signature\"):\n\t\tverify_signature(doc, signature)\n\t\tdoc.verify_signature()\n\ndef verify_crl(doc, crl_name, signature"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\tfrom hashlib import md5\n\t\tfrom Crypto.Util import asn1 as ec\n\n\t\t# Verify document type\n\t\tassert isinstance(doc, str)\n\t\tassert isinstance(signature, str)\n\t\tassert isinstance(public_key, str)\n\t\tassert isinstance(signature, str)\n\n\t\t# Verify type\n\t\tassert isinstance(signature, str)\n\t\tassert isinstance(doc, str)\n\t\tassert isinstance(public_key, str)\n\t\tassert isinstance(signature, str)\n\n\t\t# Build document\n\t\tdoc = doc.replace('\\n\\n\\n', '\\n').replace('\\"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\trlist = doc.find('*//' + public_key + '*/')\n\t\tif rlist is None: return\n\t\t\n\t\tif len(rlist.findall('*//' + signature)) < 1:\n\t\t\tlogging.error(\"Malformed XML signature: %s\" % signature)\n\t\t\traise Exception(\"Malformed XML signature\")\n\n\t\tif len(rlist.findall('*//' + signature)) > 1:\n\t\t\tlogging.error(\"Malformed XML signature: '%s' should not be the one to be valid\" % signature)\n\t\t\traise Exception(\"Malformed XML signature\")\n\n\tdef check_signature("}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\t# Parse xml element tree and check signed_xml_doc against it\n\t\t#\n\t\ttry:\n\t\t\ttree = etree.fromstring(signature)\n\t\texcept etree.XMLSyntaxError, e:\n\t\t\traise InvalidXMLElement\n\t\tif tree.getroot().tag!= \"{http://openrosa.org/http/schemas/xmlentities/lax}Signed-Info\": return 0\n\t\t# Check the claimed signature against a signed header and signature\n\t\ttry:\t\t\t\t\t\t# Check the namespace\n\t\t\ttree = etree.fromstring(doc)\n\t\t\tsigned_header = root_sign.signature"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\tsigned_msg_nodes = doc.findall(signature)[0].findall(public_key)\n\t\tfor signed_msg in signed_msg_nodes:\n\t\t\tif signed_msg.get('message'): # and not signed_msg.get('assertionID'):\n\t\t\t\tfor cred_node in signed_msg.findall(signature):\n\t\t\t\t\tif cred_node.tag == '#text':\n\t\t\t\t\t\tself.logger.info(\"\\t\\t\\\"{0}\\\"\".\n\t\t\t\t\t\t\t\t.format(cred_node.get('message')))\n\t\t\t\t\t\tself.logger.info(\"\\t\\t\\\"{0}\\"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\tif len(signature)!= 1:\n\t\t\treturn False\n\t\t\t\n\t\txml_text = xml.dom.minidom.parseString(signature)\n\t\t\n\t\tsignature = signxml.Element(xml_text).firstChild.nodeName\n\t\tsignxml_elements = doc.getElementsByTagName(\"signAssertion\")\n\t\tfor i in range(0, len(signxml_elements)):\n\t\t\tsign_element = signxml_elements[i]\n\t\t\tif signature_element.getAttribute(\"type\") == \"http://www.w3.org/2000/09/xmldsig#rsa-sha1\":\n\t\t\t\tcontinue\n\t\t"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\tsignatureLength = len(signature)\n\n\t\tif public_key.verify_digest(doc, signature, signatureLength):\n\t\t\treturn True\n\t\t\n\t\treturn False\n\n\tdef send_response_message(self, xml):\n\t\t\"\"\"\n\t\tReceives XML messages from client and builds a signed signature with\n\t\tthe signed message.\t\"\"\"\n\t\tsigner = Signer()\n\t\tsignature = signer.sign(xml)\n\t\tsignatureXML = minidom.parseString(signature)\n\t\treturn self.send_signed_response(signatureXML)\n# -*- coding:utf-8 -*- \n\ndef"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\t#print(\"signature.signature\", signature, \"public_key\", public_key)   # DEBUG\n\t\t\ndef verify_msg_signature(doc):\t\t\t\t\n\t\"\"\"\n\tVerify the XML of the signed document as verified for validity.\n\t\"\"\"\n\t\n\tif not xml_verify(doc):\n\t\traise SignatureVerificationFailed(\"Verify message signature\")\n\n\t#print(\"\\tverify_msg_signature\", doc) # DEBUG\n\tdoc.verify = False\n\tdoc.verify_digest()\t# DEBUG\n\t\n\tdoc.verify_doc_signature()\t# DEBUG\n\tdoc.verify_"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\t\"\"\"\n\t\n##################################################\n# Validation Functions\n##################################################\n\ndef validate_relayable_signature(signed_xml):\n\t\"\"\"\n\tThis method validates an XML document signed with a C{signed_xml}.\n\t\n\t@param xml_signed_xml: The XML signed XML document.\n\t@type signed_xml: str\n\t@param signature: The signature which must be verified. This must be\n\teither a C{signed_xml} or a C{verified_key}.\n\t@type signature: str\n\t@raise InvalidSignature: if C{signed_xml} does not fit the"}]}
{"task_id": "630629e052e177c0ba46a0c4", "prompt": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tdiaspora_webfinger = {'diaspora_token' : document}\n\t\tif document[0]!= '{' and document[0:4]!= '}':\n\t\t\tdoc = document[6:]\n\t\telse:\n\t\t\tdoc = document\n\t\t\tdiaspora_webfinger['diaspora_token'] = document\n\t\treturn diaspora_webfinger\n\tif document[1]!= '{' and document[1:4]!= '}':\n\t\tdoc = document[4:]\n\t\tdiaspora_webfinger['dias"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\treturn json.loads(document)['data'][0]\n\tif document.endswith(\".xhtml\"):\n\t\treturn parse_xhtml_webfinger(document)\n\telse:\n\t\treturn parse_xhtml2webfinger(document)\n\n\ndef parse_xhtml_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tParse WML webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/xhtml_webfinger.html\t\"\"\"\n\t"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tdocument = content = document.strip()\n\t\t# find \"diaspora_\"\n\t\tfor token in document.split(r'[_ ]'):\n\t\t\tif token[:4] == r\"_\":\n\t\t\t\tdiaspora_id = token[3:]\n\t\t\t\tif diaspora_id == \"d2\":\n\t\t\t\t\tname = token[5:]\n\t\t\t\t\tcontent = document[9:]\n\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\tcontent = document[0:3]\n\t\t\t\t\tcontinue\n\t\t# print >>sys.stderr, document\n\t\treturn Content(content, document, di"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tdocument = document.lower()\n\n\tif (document == \"d\"):\n\t\t# TODO: Parse XRD from XML, convert to an XD-string.\n\t\tdoc = convert_xsd(extract_xsd(document))\n\t\t# Convert XML to XD object.\n\t\tdoc = Xd.parser.fromstring(doc.decode('utf-8'))\n\telif (document == \"f\"):\n\t\tdoc = extract_xsd(document)\n\t\tdoc = Xd.parser.fromstring(doc.decode('utf-8'))\n\telif (document == \"o\"):\n\t\tdoc"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\treturn {'json': document}\n\n\tdef parse_diaspora_federation(document: str) -> List[str]:\n\t\t\"\"\"\n\t\tParse Diaspora federation.\n\t\t\"\"\"\n\t\tdocs = []  # type: List[Dict[str, Any]]\n\t\tdocs_without_diaspora_prefix = defaultdict(list)  # type: Dict[str, List[str]]\n\t\tfor suffix, doc_without_diaspora in document.items():\n\t\t\tdocs_without_diaspora_prefix[suffix] = sorted(doc"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\ttry:\n\t\t\tdoc = json.loads(document)\n\t\texcept JSONDecodeError:\n\t\t\treturn None\n\t\treturn {\n\t\t\t\"type\": doc[\"type\"],\n\t\t\t\"id\": int(doc[\"id\"]),\n\t\t\t\"name\": doc.get(\"name\", doc[\"name\"]),\n\t\t\t\"color\": doc[\"color\"],\n\t\t\t\"text\": doc[\"text\"],\n\t\t}\n\n\t@staticmethod\n\tdef parse_emergency_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Emergency webfinger according to OLD docs:\n\t\"\"\"\n"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\treturn {\n\t\t\t'address': document[4:].split(\":\")[5],\n\t\t\t'sub': document[4:].split(\" :\", 1)[1],\n\t\t\t'data': document[1:],\n\t\t\t'operator': sub_operator if sub_operator is not None else None,\n\t\t\t'response': response\n\t\t}\n\n\tdef _parse_dinero_webfinger(doc: xml.etree.ElementTree.Element, response: dict) -> \\\n\t\t\tdict:\n\t\t\"\"\"\n\t\tParse Dinero webfinger which is a JSON response that has an array of items"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tdata = document.rsplit(u\"-\", 2)[-1]\n\t\tdata = data.split()\n\t\t# The Webfinger format has the format [number, type, category, points, value, text]\n\t\twebfinger = {\n\t\t\t\"name\": data[0].split(\"-\", 1)[-1].strip('\"'),\n\t\t\t\"points\": float(data[1]),\n\t\t\t\"value\": float(data[3]),\n\t\t\t\"text\": data[4],\n\t\t\t\"type\": parse_ddiaspora_webfinger_type(document),\n\t\t\t\"categories"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\n\n\tdocuments = document_to_dict(document)\n\tresults: Dict[str, Union[_DictType, List[Dict]]] = {}\n\n\tfor document in documents:\n\t\tfrom_date = datetime_to_date(document)\n\t\tdiaspora_data = parse_diaspora_data(document)\n\t\tif from_date not in results:\n\t\t\tresults[from_date] = {\n\t\t\t\tdocument: {\n\t\t\t\t\t\"name\": document,\n\t\t\t\t\t\"start_date\": from_date,\n\t\t\t\t\t# \"end_date\": \"\"\n\t\t\t\t\t\"url"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\t\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\t\"\"\"\n\t\tif document is None or not document.startswith('#'):\n\t\t\traise ValueError(\n\t\t\t\tf'Discovery document format must start with \\'#\\' as the name of an object, or it is empty.'\n\t\t\t)\n\n\t\twebfinger_data = json.loads(document)\n\n\t\tif webfinger_data[\"enabled\"] is True:  # TODO: Check what's inside schema?\n\t\t\twebfinger_schema = WafingerSchema()\n\t\t\twebfinger_schema.enable(document)\n\n\t\t\treturn webfinger_schema.tojson()\n\n\t\telse:\n\t\t\traise"}]}
{"task_id": "630629e152e177c0ba46a0d1", "prompt": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\tif len(handle) and not handle.startswith(b'\\x07') and not handle.startswith(b'\\x00'):\n\t\t\t\treturn handle.decode(DEFAULT_ENCODING, errors='replace')\n\texcept AttributeError:\n\t\tpass\n\treturn None\n\ndef is_url(webpage_url: str) -> bool:\n\tif '://' not in webpage_url:\n\t\traise Exception('Not a correct url: {}'.format(webpage_url))\n\treturn urllib.parse.urlparse(webpage_url).scheme == 'http'\n\ndef extract_from_"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\treturn wolframalpha.rest_client.fetch_webfinger(handle)['result']['key']\n\t\texcept:\n\t\t\treturn None\n\n\t@staticmethod\n\tdef try_retrieve_webfinger_document_with_regex(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document whose name matches the provided regex.\n\tDoes not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\treturn wolframalpha.rest_client.fetch_webfinger(handle)['result']['"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\t# Check if the document exists in the database\n\t\t\tcurrent_record = HocrineSearch._where_string_matches_regex(handle, \"([0-9]*)?-([0-9]*)\")[0]\n\t\t\treturn current_record\n\t\texcept NoRecordFoundError:\n\t\t\t# No record was found in the database\n\t\t\treturn \"No Hocrine Search document for \" + str(handle)\n\n\t# ----------------------------------------------------------------------------------------------------------------\n\n\t@staticmethod\n\tdef _check_doubledocrine_document(handle: str) -> Optional[str]:"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\t# Reading the document is not currently available via the WebFING service\n\t\tif not handle:\n\t\t\treturn None\n\n\t\twebfinger_url: str = urllib.parse.urlparse(handle).netloc\n\t\tif not webfinger_url or webfinger_url == \"http://websvc/webfinger-api\":\n\t\t\treturn None\n\t\tif webfinger_url + \"/doc\" not in hass.data[DOMAIN]:\n\t\t\treturn None\n\n\t\tfrom webfingershark.exceptions import HTTPBadRequest\n\n\t\ttry:\n\t\t\tdata = webfingershark"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\treturn get_webfinger_document(handle).decode(\"utf-8\").replace(u\"\\n\", \"\")\n\t\texcept UnicodeError:\n\t\t\tpass\n\t\treturn \"\"\n\n\ndef set_webfinger_document(handle: str, text: str) -> None:\n\t\"\"\"\n\tSet the webfinger document.\n\tIf no application is specified will detect a suitable one.\n\t\"\"\"\n\ttry:\n\t\tset_webfinger_document(handle, text)\n\t\treturn True\n\texcept KeyboardInterrupt:\n\t\treturn False\n\n\ndef is_web"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\twebfinger_conn = get_webfinger_connection()\n\t\t\ttry_retrieve_server_response = webfinger_conn.get_documents(handle).next()\n\t\t\texcept HTTPError as http_error:\n\t\t\t\tlogging.log(logging.WARN, \"Webfinger connection error: \" + str(http_error), severity=ConsoleWarning)\n\t\t\t\tif http_error.code == 406:\n\t\t\t\t\tcontent = b\"\"\n\t\t\t\t\trequest_data = http_error.read().decode(\"utf-8\")\n\t\t\t\t\tif not request_data:\n\t\t\t\t\t\t"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\ttry:\n\t\t\tresult = fsm.get_webfinger_document()\n\n\t\t\treturn result['WebfingerDocument'][0]['WebfingerType']\n\t\texcept PyObjCError as ex:\n\t\t\tmsg = \"FSI Error for {handle} at {handle_id} was encountered during retrieval. Details: {handle_info}.\".format(\n\t\t\t\thandle=handle, handle_id=ex.__str__()\n\t\t\t)\n\t\t\traise ex\n\n\t# TODO more general checking/validation\n\traise NoDocumentsMatchingError(\"FSI was unable to find any webfinger documents related"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\trequest_payload = {\"handle\": handle, \"output\": [output]}\n\t\ttry:\n\t\t\tresponse = get_text_from_service(service, request_payload)\n\t\texcept Exception:\n\t\t\tlogging.getLogger(__name__).warning(\n\t\t\t\t\"Can't retrieve a Web Finger document for {}, error = {}.\".format(handle, sys.exc_info()[1])\n\t\t\t)\n\t\t\tresponse = None\n\t\tif response:\n\t\t\treturn response\n\t\telse:\n\t\t\traise DrmHandled\n\n\traise WebFingerWebserviceError()\n\n\ndef try_add_metadata"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\tif not _is_webfinger_post(handle):\n\t\t\thandle = re.sub(\"<pre [^>]*\\/>\", \"\", handle)\n\n\t\t# First, try to retrieve the RFC7016-1 document.\n\t\ttry:\n\t\t\tresponse_data, document_name = _retrieve_httpx_document(handle)\n\t\texcept HTTP400ClientError as e:\n\t\t\tlog.error(\"HTTP400ClientError on {}\".format(document_name))\n\t\t\thandle = str(e)\n\t\t\tresponse_data = None\n\t\texcept HTTP404ClientError as e:\n\t\t\tlog.error(\""}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\t\"\"\"\n\t\t# The webfinger document is used in the file name to identify the\n\t\t# document, because webfinger file does not provide a complete\n\t\t# description of the file, so use its document.\n\t\tdoc_id = uuid.uuid4().hex\n\t\turl = \"http://www.rfc-editor.org/file/\" + doc_id\n\t\turl += \".txt\"\n\t\tprint(f\"Fetching the {doc_id} {url}\")\n\t\ttry:\n\t\t\tresp = requests.get(url)\n\t\t\tresp.raise_for_status()\n\t\texcept"}]}
{"task_id": "630629e152e177c0ba46a0d2", "prompt": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\ttry:\n\t\t\td_item_list = handle.Retrieve_handle(dnd.handle)\n\t\texcept TypeError:\n\t\t\treturn {'status': False} # Diaspora doesn't have valid handle. Handle was not found.\n\t\telse:\n\t\t\td_item_list = handle.Retrieve_handles(d_item_list)\n\t\n\t\treturn d_item_list\n\n\t@staticmethod\n\tdef get_item_count(handle, d_item_count, return_fields):\t\"\"\"\n\tGet the total, number of Pediatric Item objects in the Pedi"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\txml = handle.GetXmlDoc()\n\treturn _parse_xml_dineroa_webfinger(xml)\n\ndef retrieve_arbitrage(handle):\n    \"\"\"\n\tRetrieve an AR bitrage document from a remote Diaspora webfinger\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\txml = handle.GetXmlDoc()\n\treturn _parse_xml_arbitrage(xml)\n\ndef retrieve_arbitride(handle):\n    \"\"\"\n\tRetrieve an AR bus object from a remote Diaspora web"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\thandle.fetch(HTTP_IF_MATCH)\n\t\thandle.parse()\n\t\turl = handle.document.uri\n\t\treturn dict(url=url)\n\n\t@staticmethod\n\tdef retrieve_diaspora_document(handle):\t\"\"\"\n\tHandle the GetDiasporaDocument operation, which fetches the document to parse\n\tthe document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\thandle.fetch(HTTP_IF_MATCH)\n\t\treturn handle.document\n# Copyright (C) 2012-2015 by the Free Software Foundation"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\tdoc = xml.dom.minidom.parse(handle)\n\t\tret = {}\n\t\tfor child in doc.childNodes:\n\t\t\tdata = {}\n\t\t\tdata_el = child.nodeName.lower()\n\t\t\tif data_el == 'document':\n\t\t\t\tret[data_el] = get_node_document(child)\n\t\t\telif data_el == 'dreambox':\n\t\t\t\tret[data_el] = get_dreambox_element(child)\n\t\t\telif data_el == 'dream-player':\n\t\t\t\tret[data_el] = get_dream"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t # This is where we actually retrieve the document\n\t doc = {'result': None }\n\t handle.perform_request('https://webfinger.org/webfinger.do', doc)\n\n\t return doc\n\n\n\"\"\"\n\tRetrieve a single entry of a Diaspora database using the XML API.\n\n:arg _id: PHED-BY-MDDL-KEY or PHED-BY-SHA1-OF-HDN (not yet implemented)\n:returns: tuple\t: doc, handle - BibRec object and handle of the current record\n\"\"\"\t\ndef retrieve_record"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\thandle.sendCommand(\"help\")\n\t\tget_diaspora_webfinger_result(handle)\n\t\tresult = {}\n\t\twhile True:\n\t\t\tdata = handle.receiveResponse()\n\t\t\ttry:\n\t\t\t\tassert len(data) == 1\n\t\t\t\tdiaspora_webfinger_doc = {\n\t\t\t\t\t'id':data[0],\n\t\t\t\t\t'display_url':data[1],\n\t\t\t\t\t'retamber':data[2],\n\t\t\t\t\t'duration':data[3],\n\t\t\t\t\t'duration_units':data[4]\n\t\t\t\t}\n\t\t\t\t"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\n\t\n\ttry:\n\t\twebfinger_doc = etree.XML(handle)\n\t\twebfinger_doc.xpath('/document/d[@local_doc]')\n\t\tdua_webfinger_doc = document()\n\t\twebfinger_doc.text = xml_utils.basic_xml_to_text(dua_webfinger_doc)\n\t\t\n\t\treturn dua_webfinger_doc\n\texcept Exception, e:\n\t\treturn e\n\t\t\ndef get_dua_webfinger_name():\n\t\"\"\"\n\tRetrieve a the DuaWf"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\ndef display_main_window():\n\tglobal display_main_window\n\tdisplay_main_window = gtk.gdk.display_get_default()\n\tglobal window_main\n\tglobal main_window\n\tglobal window_main\n\tglobal top\n\tglobal dialog\n\tglobal diaspora_connection_manager\n\tglobal my_diaspora_connection_manager\n\tglobal my_document\n\tglobal main_window\n\tmain_window = window_main = Toplevel()\n\tmain_window.set_position(gtk.WIN_POS_CENTER)\n"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\n    document = fetch_url(handle, None)\n    document = simplejson.loads(document)\n\n    # We should get one of the top level nodes\n    nodes = document['nodes'] if 'nodes' in document else []\n    if len(nodes) == 0:\n\t\traise Exception(\"Node not found in specified handle.\")\n\n    # Each node is a dictionary containing all information in it. Keys are\n    # the values of the node,  values are the content of node, and each\n    # field is a list of the field's children. There\n    # are more items in the list, each item is a list of"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\t\"\"\"\n\t\thandle.retrieve_and_parse_content()\n\t\treturn handle.retrieve_and_parse_diaspora_document()\n\texcept:\n\t\treturn { 'exception': handle.exception }\n\ndef retrieve_document(handle):\t\"\"\"\n\tGet a document that we'll use to look for\n\n:arg handle: Remote handle to retrieve\n:returns: dict; Document if found; otherwise None\n:raises: RemoteException if not found\n:uses:\n\t- document\n\t\"\"\"\n\tif handle.type==\"document\":\n\t\treturn { 'document': handle.retrieve"}]}
{"task_id": "630629e252e177c0ba46a0d6", "prompt": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\turl = \"https://{0}\".format(host)\n\n\t\trequest = urllib2.Request(url, None, {'Host': host})\n\t\trequest.get_method = lambda: \"GET\"\n\n\t\tresponse = urllib2.urlopen(request)\n\n\t\t# TODO: Return 404 if not found. \n\t\t#return None\n\n\t\ttry:\n\t\t\tcontent = response.read().decode(\"UTF-8\")\n\t\t\t# xml.etree.ElementTree.fromstring(content)\n\t\t\treturn XRDVDIAHostMeta(content)\n\t\texcept:\n\t\t\t# TODO: Check if this is even"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t if host not in (\"www.diaspora.com\", \"localhost\", \"diaspora.com\"):\n\t\t\traise DisabledException(\"Unable to retrieve data for host \" + str(host))\t\n\t\tmeta_url = \"/ws/4ms5/meta?format=xml&host={0}\".format(host)\n\t\ttry:\n\t\t\trequest = urllib2.Request(meta_url)\n\t\t\tdoc = urllib2.urlopen(request)\n\t\texcept Exception, err:\n\t\t\traise DisabledException(\"Error reading meta data for host \" + str(host) + \":"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\ttry:\n\t\t\tresponse = requests.get(url=host + '/.diaspora', headers={'Host': ''})\n\t\t\t_logger.debug(\"%s response %s\" % (response.status_code, response.headers))\n\texcept Exception as e:\n\t\tpass\n\n\tif response.status_code == 200:\n\t\treturn response.text\n\telif response.status_code == 201:\n\t\twith open('host-meta.json', 'w'):\n\t\t\tnew_string = json.dumps(response.json(), indent=4, sort_keys=True)"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\t# Get the Diaspora host document\n\t\tmetadata = get_metadata(host)\n\t\t# Check if we already have the Diaspora host-meta document\n\t\ttry:\n\t\t\t# Parse the XML\n\t\t\t# Note that we'll only use the first Diaspora host document as a reply as we had parsed the xml\n\t\t\tXML = metadata.get_xml_string(encoding='utf-8')\n\t\t\tDiasporaHost = xmltodict.parse(XML)\n\t\texcept:\n\t\t\traise RDSException(\"Failed to parse the Diaspora host document"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\treturn DiasporaHost(connection=host.connection)\n\n################################################################################\n\ndef retrieve_diaspora_host(url):\t\"\"\"\n\tRetrieve a remote Diaspora host document from the given URL\n\n\t:arg url: URL from which to start the Diaspora host document\n:returns: ``XRD`` instance\"\"\"\n\t\treturn DiasporaHostReader(url=url)\n\n################################################################################\n\ndef retrieve_diaspora_host_meta(url):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document from the given URL"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\turi = 'https://{0:s}{1}'.format(host, settings.XRD_URI)\n\t\td = DiasporaHost(uri=uri, username=settings.XRD_USERNAME, password=settings.XRD_PASSWORD)\n\t\treturn d\n\t\t\n\ndef retrieve_drugbank_host_meta(host):\t\"\"\"\n\tRetrieve a remote Drugbank host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XLR`` instance\t\"\"\"\n\t\turi = 'https://{0:s}{1}'.format(host,"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\tif not _is_rdy_db(host):\n\t\t\tlogger.info('No host-meta file found for %s.' % hostname)\n\t\t\treturn None\n\t\tif not hostname in GDB.host_meta:\n\t\t\tlogger.info('Host %s not found.' % hostname)\n\t\t\tlogger.info('GDB.host_meta: %s' % (str(GDB.host_meta), ))\n\t\t\treturn None\n\t\tdiaspora_data = GDB.host_meta[hostname]\n\t\tif not diaspora_data:\n\t\t\tlogger"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\n\t\n\tmeta = host.get('meta')\n\t\n\t# Convert to dict.\n\treturn_value = {}\n\t\n\tif meta == {}:\n\t\treturn_value = {'data': None}\n\t\n\tfor key in meta:\n\t\tif key not in ('data', 'files'):\n\t\t\tpass\n\t\telse:\n\t\t\treturn_value[key] = meta[key]\n\t\t\n\treturn return_value\n\t\ndef retrieve_diaspora_host_filemeta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-filemeta document.\n\n:arg host: Host to"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\treturn requests.get('https://' + host + '/api/2.2/hosts/' + host + '/meta/diaspawn/').json()\n\n# TODO: this is a hack.\nclass Diaspora(object):\n\tdef __init__(self, host, doc_id):\n\t\t\"\"\"Creates a Diaspora instance for a document\"\"\"\n\t\tself.host = host\n\t\tself.doc_id = doc_id\n\t\tself.headers = {\n\t\t\t'version': '2',\n\t\t\t'content-type': 'application/x-www"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\treturn retrieve_xnat(host, {'verb': \"Retrieve Diaspora host meta\",'metadataPrefix': \"xrd\"})[0]\n\ndef retrieve_diaspora_host_hosts(host):\t\"\"\"\n\tRetrieve a remote Diaspora hosts document that contain the hosts.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\t\"\"\"\n\t\treturn retrieve_xnat(host, {'verb': \"Retrieve Diaspora hosts\"})\n\ndef add_diaspora_host(host, data):\n\t\"\"\"\n"}]}
{"task_id": "630629e752e177c0ba46a0fb", "prompt": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\n##\n# Helpers\n##\n\ndef _get_full_url(url, host, path):\n\t\"\"\" Returns full url for a given url \"\"\"\n\tscheme, host, path, query, fragment = urlparse.urlsplit(url)\n\treturn \"{0}://{1}{2}\".format(scheme, host, path)\n\ndef http_request(url, data, timeout=10, method=\"get\", *args, **kwargs):\n\t\"\"\"\n\tHelper function to send requests to a server via HTTP.\n\nA simple helper function to send requests through the API. If a"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\tif not HAS_SSLCONTEXT: return 1, \"SSLContext module not available\"\n\n\t\tconn = HTTPConnection(\n\t\t\tself.host, self.port, timeout=timeout,\n\t\t\tsource_address=url, source_address_extra=self._source_address(url))\n\t\tconn.request(\n\t\t\tmethod.upper(),\n\t\t\turlencode(data),\n\t\t\tNone,\n\t\t\t{\n\t\t\t\t'Content-type': 'application/x-www-form-urlencoded',\n\t\t\t\t'Content-Length': len(data),\n\t\t\t\t'Host': self.host,\n\t\t\t\t'Range"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\tif not data:\n\t\t\treturn None, None\n\n\t\t# build args\n\t\targs = {}\n\t\tif isinstance(data, dict):\n\t\t\targs = data.copy()\n\t\t\targs.update(dict(\n\t\t\t\t_url = url,\n\t\t\t\tdata = data,\n\t\t\t\ttimeout = timeout,\n\t\t\t\tmethod = method,\n\t\t\t))\n\t\telif isinstance(data, list):\n\t\t\targs = data[:]\n\t\t\targs.pop('timeout', None)\n\n\t\t# build kwargs\n\t\tkwargs = {}\n\t\tif isinstance(data, dict):\n\t\t\tkwargs.update(dict(\n\t\t\t\t"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\t# Default options\n\t\toptions = kwargs.copy()\n\t\toptions.update(options_defaults)\n\n\t\tmethod = method.lower()\n\n\t\ttry:\n\t\t\tif method == 'post':\n\t\t\t\tresult = requests.post(**data)\t\t# Submit and return request\n\t\t\t\tif result.status_code not in (OK, 201):\n\t\t\t\t\treturn result, None, \"request failed with status %d: %s\" % (result.status_code, result.text)\n\n\t\t\t\telif method == 'get' and result.headers.get(\"content-type\") == \"multipart/form"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\t# Set defaults\n\t\turl = url.format(*args, clientver=\"2.0\",  protocolversion=PRAINTEXT)\n\t\tkwargs = {'timeout': timeout, 'allow_redirects': False, 'allow_methods': False}\n\n\t\tif \"username\" in kwargs: kwargs[\"username\"] = kwargs[\"username\"].encode('utf8')\n\t\tif \"password\" in kwargs: kwargs[\"password\"] = kwargs[\"password\"].encode('utf8')\n\t\tif \"proxy\" in kwargs: kwargs[\"proxy\"] = kwargs[\"proxy\"].encode('utf8')\n\t\tif \"use_"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\turl_list = urlparse(url)\n\t\tmethod = url_list[7]\n\n\t\ttries = 0\n\t\tend = 0\n\t\twhile tries <= 10:\n\t\t\ttime.sleep(args.timeout)\n\t\t\ttry:\n\t\t\t\tresponse = requests.post(\n\t\t\t\t\turl_list[0] + url_list[1],\n\t\t\t\t\tdata=data.copy(),\n\t\t\t\t\ttimeout=args.timeout,\n\t\t\t\t\theaders={\"Content-type\": \"application/x-www-form-urlencoded\"},**kwargs)\n\t\t\texcept requests.exceptions.Timeout:\n\t\t\t\ttries += 1\n\t\t\t\tif"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\tif method in ('POST', 'PUT'):\n\t\t\theaders = {'Content-Type': 'application/x-www-form-urlencoded'}\n\t\telif method in ('GET', 'DELETE'):\n\t\t\tdata = urlencode(data)\n\t\t\theaders = {'Content-Type': 'application/x-www-form-urlencoded'}\n\n\t\tif hasattr(data, 'items') and len(data.items()) > 0:\n\t\t\tif 'data' in kwargs:\n\t\t\t\tdata = kwargs['data'] # Should be a file-like object, need some cleanup?\n\t\telif kwargs.get('timeout')"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\timport requests\n\t\tsession = requests.Session()\n\t\turl = url + url\n\t\tlogger.debug(\"send_document: sending url %s to session %s\", url, session)\n\t\tif timeout is not None:\n\t\t\tif isinstance(timeout, int):\n\t\t\t\ttimeout_seconds = (timeout, timeout * 1000)\n\t\t\telse:\n\t\t\t\ttimeout_seconds = timeout\n\t\t\tfor attempt in xrange(0, timeout_seconds):\n\t\t\t\ttry:\n\t\t\t\t\tsession.post(url, data=data, timeout=(attempt + 1))\n\t\t\t\texcept requests.exceptions.ConnectionError:\n\t\t\t\t\t"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\ttry:\n\t\t\tdata = urlencode(data)\n\t\texcept:\n\t\t\tdata = None\n\t\ttry:\n\t\t\t# Set timeout second to disable timeout\n\t\t\tkwargs.setdefault(\"timeout\", 180)\n\t\t\tresp = urlopen(url, data=data, timeout=timeout, **kwargs)\n\t\t\tresp.raise_for_status()\n\t\t\tcontent = resp.read()\n\t\texcept: \n\t\t\tcontent = None\n\t\tif not content:\n\t\t\tmsg = \"Request to %s failed.\" % url\n\t\t\traise Exception(msg)\n\t\treturn (content, resp)\n\n\ndef generate_"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\t\"\"\"\n\tHelper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\t\"\"\"\n\t\trequest = \"/echo%s\" % (method)\n\t\trequest = request.encode('utf-8')\n\t\trequest = request.strip()\n\t\tif request == \"\":\n\t\t\traise errors.ClientError(\"Client must set post command\")\n\t\theaders = {\"Content-type\": \"application/x-www-form-urlencoded\"}\n\t\tconn = httplib.HTTPConnection(self.host, self.port)\n\t\tconn.request(\"POST\", request)\n\t\tresponse = conn.getresponse()\n\t\tresp = response.read()\n\t\tresp_json = json.loads(resp)\n\t\tresp"}]}
{"task_id": "6306091073426c38ae68acac", "prompt": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tfor key, value in dic.items():\n\t\t\tif key in keys:\n\t\t\t\tif isinstance(value, dict) and isinstance(val, list):\n\t\t\t\t\tdict_insert(val, eval_expand(value), key, *keys)\n\t\t\t\telse:\n\t\t\t\t\tdict_insert(val, str(key), key, *keys)\n\ndef main():\n\t# load environment variables\n\tglobal __global_dict_binding\n\tglobal __global_key_dict\n\tglobal __global_key_list\n\n\t# parse arguments to set a parsed version of the map\n\tparser ="}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tif keys is None or len(keys) == 0:\n\t\t\tif dic is None:\n\t\t\t\treturn dic\n\t\t\tif key in dic:\n\t\t\t\tdic[key] += val\n\t\t\telse:\n\t\t\t\tdic[key] = [val]\n\t\telse:\n\t\t\tif len(keys) == 1:\n\t\t\t\tkeys = [keys]\n\t\t\tfor key in keys:\n\t\t\t\tdict_insert(dic[key], val, key)\n\n\nclass ConfigLoader(ConfigParser):\n\t\"\"\"\n\tConfigParser subclass that supports nested sections\n\t\"\"\"\n\tdef build_subsections(self, config"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tdic[key] = lst = dic[key]\n\treturn lst, ()\n\n\nclass _Node(dict):\n\t__slots__ = ('prev', 'next')\n\n\tdef __init__(self, prev, next, prev_next):\n\t\tdict.__init__(self)\n\t\tself.prev = prev\n\t\tself.next = next\n\t\tself.prev_next = prev_next\n\n\tdef __str__(self):\n\t\t\"\"\"display the current node\"\"\"\n\t\ts = \"%s [%d: %d]\" % (self.prev, self.next, id(self"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tfor k in keys:\n\t\t\tdic = dic.setdefault(k, {})\n\t\tset_nested_dict(dic, key, val)\n\n\tif isinstance(val, dict):\n\t\t# this has special handling for scalar values or lists of dicts e.g. for $some_name it may be given to the nested dict\n\t\treturn [dict_insert(dic, item, key, val) for item in val]\n\telif isinstance(val, (list, tuple)):\n\t\t# this has special handling for lists of different lengths e.g. a, b => [a, b"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tfor k in keys:\n\t\t\tdict_insert(dic, k, val, key + '.' + k)\n\nclass LRUCache(dict):\n\t\"\"\"lru-cache:\n\tCache using LRU and relying on the size of LRU objects\n\t\"\"\"\n\n  def __init__(self, size=100):\n   \tself.size = size\n\n  def __setitem__(self, key, value): \n\t# add a subkey to the tail of the sorted list\n\tbisect.insort(self.items, (key, value))\n\tself.popitem(False)\n\n  def"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tdic[key] = keys\n\t\tfor i, key_ in enumerate(keys):\n\t\t\tnew_key = 'val{0}'.format(i) if i > 0 else key_\n\t\t\tfor k in keys:\n\t\t\t\tdic[new_key] = val\n\treturn dic\n\n\n# The structure below is used in the model as a base for other dictionary types.\n# All the dictionaries defined here are passed on a top.\nclass BaseFile(object):\n\t\"\"\"base class for file objects, mainly a dictionary that holds the\n\tcurrent file information\"\"\"\n\n\t_file_ext"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tpass\n\n\nclass Simple_Field(object):\n\t\"\"\"\n\tThe object that holds a dictionary.\n\t\"\"\"\n\tdef __init__(self, dic=None, *args, **kwargs):\n\t\t\"\"\"\n\t\t:param dic: a dictionary object or None (default)\n\t\t:param args: positional arguments for the insert operation\n\t\t:param kwargs: various keyword args for the inserted operation\n\t\t\"\"\"\n\t\tself._dic = dic\n\t\tself._keys = list(keys) if keys else []\n\t\tself.set(*args, **kwargs)\n\n\tdef set(self,"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\n        dic[key] = val\n\t\ndef dict_remove(dic, val, key):\t\t#TODO\n\t\"\"\"\n\tremove a value from a nested key_value\n\nfrom docx.table import Table\n\ndef dict_lookup(dic, key):\t\t#TODO\n\t\"\"\"\n\tfind the value for a nested key by recursively searching its associated value\n\nfrom docx.table import Table\n\ndef dict_remove_by_key(dic, key, val):\t\t#TODO\n\t\"\"\"\n\tremove a value from a nested key_value\n\nif no"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t \n\tpath = dic\n\tdel dic[\"path\"]\n\tif not keys:\n\t\treturn\n\tfor k in keys[:-1]:\n\t\tpath = \"%s*%s\" % (dic[\"path\"], k)\n\tpath += \".%s\" % keys[-1]\n\t\n\tdic[\"_path\"] = path\n\n\t# make sure to copy\n\tfor key in dic:\n\t\tif not dic[key]: continue\n\t\tdic[key].extend(dic[key])\n\ndef dict_remove(dic, k):\n\t\"\"\"\n\tremove the given key from a nested dictionary\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\t\"\"\"\n\t\tfor k in keys:\n\t\t\tif isinstance(k, dict) or isinstance(k, list):\n\t\t\t\tnewd = dict_insert(dic.get(k, {}), k, val)\n\t\t\telif isinstance(k, tuple):\n\t\t\t\tnewd = dict_insert(dic.get(k[0], {}), k[1], val)\n\t\t\telse:\n\t\t\t\tnewd = dict_insert(dic.get(k.lower(), {}), val, key)\n\t\t\tdic[k] = newd\n\treturn dic\n\n#\n# Copyright (C) 2014 Shang Yuan"}]}
{"task_id": "6306091a73426c38ae68acc8", "prompt": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t", "suggestions": [{"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\tnew_files = [i for i in settings_dirs if (i.endswith('.i') or i.endswith('.c'))]\n\t\tfor f in new_files:\n\t\t\tif not spec_option in f:\n\t\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\telse:\n\t\treturn False\n\n#=======================================================\ndef generate_file_names(settings_dirs, spec_option):\t\n\t\"\"\"\n\tGenerate a dictionary that defines a list of files (i.e., options.npy)\n\t\"\"\"\n\tfiles = {}\n\tfor f in set("}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\treturn re.sub('[^\\w\\-]', '_', settings_dirs.replace('\\\\',''))+'_(\\\\d+_?(1|3))[.]'\n\n\nclass IniFile(object):\n\t# the filename of the inif file\n\tfilename = None\n\n\tdef __init__(self, filename):\n\t\tself.filename = filename\n\n\tdef set_filename(self, filename):\n\t\tself.filename = os.path.abspath(filename)\n\n\tdef get_filename(self):\n\t\treturn self.filename\n\nclass Include(object):\n\t# the filename of the include"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\tfrom jinja2 import TemplateNotFound\n\t\tfrom pygments import highlight\n\t\timport re\n\t\timport os\n\t\tfrom jinja2.exceptions import TemplateSyntaxError\n\n\t\tfile_name_list = settings_dirs.values()\n\n\t\t# Add files in the list\n\t\tsource_list = []\n\n\t\t# Get files\n\t\tfor filepath, dirname, filename in settings_dirs.items():\n\t\t\tsource_list += re.findall(\".+/?(.+)\", os.path.join(filepath, \"*.html\"))\n\t\tfor filepath in source_list:\n\t\t\tsource_name, extension = os.path.splitext"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\treturn [settings_dirs[0] + spec_option, settings_dirs[1] + spec_option]\n\n\t@staticmethod\n\tdef is_mandatory(settings_dirs, spec_option):\t\"\"\"\n\tPurpose:\t\n\t\tReturns:\tIf not a mandatory setting\n\t\t@return:\t\t\tBoolean\n\t\t@call: \t\t\tIni Type\n\t\t\"\"\"\n\t\treturn True\n\n\t@staticmethod\n\tdef is_list(settings_dirs, spec_option):\t\t\t\"\"\"\t\n\t\tPurpose:\t\n\t\tReturns:\tIf not a list and nothing"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\tfile_name = (setting_dirs.userConfig['input']['data_to_extract']\n\t                 + '.' + spec_option + '_to_' + file_name)\n\t\t# Write file\n\t\tprint('Creating new file', file_name)\n\t\t# Open the file\n\t\tfile_object = open(file_name, 'w')\n\n\t\t# Format header\n\t\tprint('\\n'.join([\n\t\t\t'file_header:', file_object.name,\n\t\t\t'file_type:', file_object.parent.fileType.value,\n\t\t\t'field_name: '"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\treturn list_of_types(settings_dirs,spec_option)\n\n# ------------------------------------------------------------------- #\n\ndef parse_setting(setting):\n\t\"\"\"\n\tParse the setting as defined in the file names\n\t\"\"\"\n\tglobal verbose\n\timport sys\n\tassert(isinstance(setting, dict))\t\n\tassert(isinstance(setting,list))\n\n\tfor opt_key in setting.keys():\n\t\ttry:\n\t\t\topt_val = setting[opt_key]\n\t\texcept:\t# key not found\n\t\t\topt_val = None\n\n\t\tif opt_val == None"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\treturn []\n\n\text_modules = [str(settings_dirs['settings'][spec_option])]\n\treturn generate_tasks(settings_dirs, ext_modules)\n\n\ndef task_factory():\n\t\"\"\"\n\tReturn a new Task object \n\twhich will serve as a task generator\n\t\"\"\"\n\treturn Task_gen()\n\n\ndef task_gen():\n\t\"\"\"\n\tReturn a new Task object which will serve as the tasks\n\tdefined in spec.json\n\t\"\"\"\n\treturn Task_taskgen()\n#!/usr/bin/env python\n# -*- coding:"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\n\n\n\t\n\n\n\tparser = argparse.ArgumentParser(description=\"Generate I18N for a project\")\n\tparser.add_argument('-project',\n\t\taction = \"store\",\n\t\thelp = \"Name of the project. For project specific files: \" + \\\n\t\t\"I18N.py \" + \\\n\t\t\"(must have.sip.src and.qmr. src extensions, \"\n\t\t\"not.qrc extensions), \"\n\t\tdefault = os.path.join(settings_dirs['base_directory'], \"project_locale\") ,\n\t\tmetavar = (\"I18N."}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\tfiles = []\n\t\tfor settings_dir in settings_dirs:\n\t\t\tname = settings_dir + \"/\" + spec_option + \".ini\"\n\t\t\tsettings = Settings(name, settings_dir)\n\t\t\tfiles.append(SettingsFile(os.path.join(settings_dir, name), settings, False))\n\t\treturn files\n\tdef get_files(settings_dirs, spec_option):\treturn list_of_file_names(settings_dirs, spec_option)\nfrom __future__ import absolute_import, unicode_literals\n\nfrom django import template\n\n"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\t\"\"\"\n\tCreate a new IniType complex type\t\"\"\"\n\t\t# list of files\n\t\tinfilename = settings_dirs[\"input filename (*.ins);\"]\n\t\tself.logger.info(\"creating a new %s complex type\" % infilename)\n\t\twith open(infilename, 'w') as outfile:\t\t\t\n\t\t\twriter = csv.writer(outfile)\n\t\t\t\n\t\t\t# list of tuples with first element set to the real path\n            # of the CSV file\t\n\t\t\tdata = namedtuple(\"data\", \"filetype, header, data\")\n\t\t\t# dict to store this complex type as a header and a data field in the outstream\n\t\t\theader = data("}]}
{"task_id": "6306091b73426c38ae68acd7", "prompt": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn cls._ansible_conf_manager\n\n\t@classmethod\n\tdef _create_ansible_conf_for_runtime(cls):\n\t\t\"\"\"\n\t\tCreates the ansible configuration to use when creating a ansible container.\n\t\tAs a side effect, this will create an ansible config file that will be used by the container\n\t\tto run Ansible.\n\n\t\t:return: The newly created ansible config, or None if none could be created.\n\t\t\"\"\"\n\t\tcfg = config.Config(__name__)\n\t\tansible_conf = cls._create_ansible_config_path(cfg.get(\"ansible\", \""}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn cls.manager\n\n\t@classmethod\n\tdef get(cls):\n\t\t\"\"\"\n\t\tReturns the ansible config manager.\n\t\t\"\"\"\n\t\treturn cls.manager\n\n\tdef resolve(self):\n\t\t\"\"\"\n\t\tFind the local ansible server and add it to the managers of any hosts \n\t\t\"\"\"\n\t\tself.logger.info(\"Resolving host to master and adding to list.\")\n\t\ttry:\n\t\t\tlocal = socket.gethostbyname_ex(socket.gethostname())\n\t\texcept socket.gaierror, e:\n\t\t\traise errors.HostError(\"Couldn't resolve %s to"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn ConfigurationManager(cls).get_ansible_config()\n\t#end ___get_ansible_config\n\nclass ConfigurationManager(object):\n\t\"\"\"\n\tA simple wrapper for all the configuration manager types.\t\n\t\n\tThis allows for simple configuration objects without having to write a config manager (which handles adding, updating and deleting components to it.\n\n\tAny settings in the configuration manager will be configured by using `self.get_settings()` and `self.write()`, respectively.\n\tThis class may also be used by the user to add new configurations and restore their state.\n\n\tConfigManager's"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\tlogger.debug(\"Loading Ansible config data\")\n\t\tlogger.info(\"Creating a new ansible config manager\")\n\n\tmanager = AnsibleConfigManager(config_path=cls.config_path, config_dict=cls.config)\n\tif manager.status == utils.ERROR:  # pylint: disable=E1103\n\t\tif manager.get_cli_parser(): # pylint: disable=E1103\n\t\t\tlogger.debug(\"Getting user config\")\n\t\t\tlogger.debug(manager.cli_parser.plugins)\n\t\t\tlogger.debug(manager.cli_parser.plugin_opts)"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn cls.ansible_config_managers[cls]\n\n#: the type of a configured provider.\nCFG_TYPE = TheConfigManager\n\n#: the type of a configured provider to be used. This must\n#: be a subclass of :class:`thefuck.config.base.BaseProvider.BaseProvider`\n#: class.\nCFG_PROVIDER = TheConfigManager\n\n#: the identifier of the provider.\nCFG_PROVIDER_ID = 1\n#: the identifier of a specific config file to configure (relative, absolute).\nCFG_CONFIG_FN = 2\n\n#!/usr/bin/"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn cls._ansible_config_manager\n\n\t@classmethod\n\tdef has_section(cls, instance):\n\t\t\"\"\"\n\t\tReturns True if the ConfigParser instance is in the configuration manager.\n\t\t\"\"\"\n\t\treturn cls._has_section(instance.instance_id)\n\n\t@classmethod\n\tdef has_name_server(cls, instance):\n\t\t\"\"\"\n\t\tReturns True if the ConfigParser has a name server in the configuration manager.\n\t\t\"\"\"\n\t\treturn cls._has_name_server(instance.instance_id)\n\n\t@classmethod\n\tdef has_type(cls,"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn cls.ConfigManager\n\n\t@classmethod\n\tdef get_config_manager(cls):\n\t\t\"\"\"\n\t\tRetrieves a ConfigManager object. Inherits from the ConfigManager class\n\t\tin order to allow for custom configuration using the configuration object.\n\n\t\tFor further details refer to the implementation of ConfigManager\n\t\tin the same module.\n\t\t\"\"\"\n\t\treturn cls.ConfigManager(env=os.environ)\nfrom __future__ import print_function\n\n# Author: Nic Wolfe <nic@wolfeden.ca>\r\n# URL: http://code."}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\tif not cls.config_manager:\n\t\t\tcls.config_manager = ConfigManager(source=constants.DEFAULT_AGENT)\n\t\treturn cls.config_manager\n\nclass ContextBase:\n\t\"\"\"\n\tThe ContextBase is a singleton class that provides a context manager, with limited\n\tconfig. Configuration may be subclassed in the future to allow context managers\n\tto be subclassed.\n\n\tNote, that it's not strictly necessary to access the config values, which are simply\n\tshared across multiple modules, hence can create them within the configuration methods.\n\t\"\"\"\n\n\tdef __init"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\treturn cls.api.config_manager\n\n\t@property\n\tdef core_managers(self):\n\t\t\"\"\"\n\t\tThe list of the core manager instances.\n\n\t\t:returns: A list of the core manager instances\n\n\t\t:rtype: list\n\t\t\"\"\"\n\t\treturn self.api.core_managers\n\n\t@property\n\tdef core_modules(self):\n\t\t\"\"\"\n\t\tThe list of the core module instances.\n\n\t\t:returns: A list of the core module instances\n\n\t\t:rtype: list\n\t\t\"\"\"\n\t\treturn self.api.core_modules\n\n\t@property\n"}, {"generated_text": "def ansible_config_manager(cls):\t\"\"\"\n\tGets the ansible config manager.\t\"\"\"\n\t\traise NotImplementedError()\n\n\t_config_manager = None\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tInitialize the config manager.\n\t\t\"\"\"\n\t\tself._config_manager = self.get_configmanager()\n\nclass ConfigManager(object):\n\n\t\"\"\"\n\tClass to get the config manager.\n\tAll methods in this class are thread safe.\n\tIt will also automatically handle missing data.\n\t\"\"\"\n\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tInitialize config manager.\t\t\n\t\t\"\"\"\n\t\tself._config_manager = None\t\t# Configuration manager\n\t"}]}
{"task_id": "6306091b73426c38ae68acd9", "prompt": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\tif cls is None:\n\t\t\traise ValueError(u'Invalid call to `workspace_manager()`')\n\t\treturn cls._workspace_manager\n\t\n\t@classmethod\n\tdef is_workspace_manager(cls):\n\t\t\"\"\"\n\t\tDetermines whether or not the workspace manager is active.\t\n\n\t\tReturn value - ``True`` or ``False``. See `is_system` for details.\n\t\t\"\"\"\n\t\treturn not cls.is_system()\n\t\n\tdef is_system(self):\n\t\t\"\"\"\n\t\tChecks whether or not the :class:`WorkspaceManager` instance\n\t\tis currently open."}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\tif cls._ws_manager is None:\n\t\t\twsman_cls = cls.import_cls(cls.__name__)\n\t\t\twsman_cls._ws_manager = workspace_manager(wsman_cls)\n\t\treturn cls._ws_manager\n\n\t@classmethod\n\tdef get_config(cls):\n\t\t\"\"\"\n\t\tReturns the configuration object, returning a configuration object if\n\t\tit is not yet initialized. If the workspace manager has already been\n\t\tconfigured, the configuration object is returned first, otherwise an\n\t\tinvalid configuration object is returned.\n\t\t\"\"\"\n\t\tif cls._config"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn cls._workspace_manager\n\n\tdef workspace_object(self):\n\t\t\"\"\"\n\tReturns the workspace object.\t\"\"\"\n\t\treturn self._workspace_manager.workspace_object\n\n\tdef _workspace_manager_is_active(self):\t\t\"\"\"\n\tReturns False or false depending on whether the active workspace manager has been passed. \"\"\"\n\t\treturn any((self._workspace_manager._data_is_for_workspace_creation, self._workspace_manager._data_is_for_workspace_deletion).values())\n\n\tdef __current_time_and_duration(self):"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\tws = None\n\t\ttry:\n\t\t\tws = cls.__registry__.get(cls._ws_type)\n\t\texcept Exception, e:\n\t\t\tprint \"Can not access workspace manager '%s' - %s\"%(cls._ws_type, str(e))\n\t\t\treturn None\n\t\tif ws is None:\n\t\t\tprint \"Can not access workspace manager '%s': %s\"%(cls._ws_type, str(e))\n\t\t\treturn None\n\t\treturn ws\n\n\t@classmethod\n\tdef register(cls):\n\t\t\"\"\"\n\t\tReturns the global workspace manager\n\t\t\"\"\"\n\t\tglobal"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn cls._workspace_manager\n\n\t@abc.abstractmethod\n\tdef _workspace_exists(self, workspace):\n\t\t\"\"\"\n\t\tChecks if the specified workspace exists.\tReturns true if the workspace exists, otherwise returns false.\n\t\t\"\"\"\n\t\treturn False\n\n\t@classmethod\n\tdef _workspace_factory(cls, workspace, workspace_manager=False):\n\t\t\"\"\"\n\t\tCreates a new workspace type to work with the given workspace manager. \n\t\t\n\t\t:returns obj(type): The created workspace type if successful, None if not.\n\t\t:returns _Workspace_factory: The factory that"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\twscc = cls.workspace_manager_class()\n\t\treturn wscc.workspace_manager\n\tworkspace_manager = property(workspace_manager)\n\tworkspace_manager_class = classmethod(workspace_manager_class)\n\n\tobjects_container = property(lambda self: self.__api.objects_container.__class__) \n\n\tdef __init__(self, api, json_result):\n\n\t\tself.__api = api\n\t\tself.__json = json_result\n\n\tdef add_workspace(self, workspace):\n\t\t\"\"\"\n\t\tAdds the specified workspace to this model's list"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\tif not hasattr(cls, '_wsman'):\n\t\t\tcls._wsman = create_wsman(cls.userpath)\n\t\t\tcls._wsman.initialize()\n\t\t\tcls._workspace = cls._wsman.get_workspace(cls.username)\n\t\treturn cls._wsman\n\n\t@classmethod\n\tdef __has_workspace(cls, workspace_name):\n\t\treturn cls._wsman.has_workspace(workspace_name)\n\n\t@classmethod\n\tdef __read_workspace(cls, workspace_name, workspace_path):\n\t\tprint 'Reading workspace', workspace_"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\tif cls.get_workspace_manager() is None:\n\t\t\traise RuntimeError(\"Workspace Manager is not set.\")\n\t\treturn cls.get_workspace_manager()\n\n\t@classmethod\n\tdef get_workspace_manager(cls):\n\t\tworkspace_name = settings().get([\"workspaceManager\", \"workspace\"])\n\t\tif workspace_name is None or workspace_manager.get_active_workspace() is None:\n\t\t\tfrom octoprint.settings import settings\n\t\t\tworkspace_manager_directory = settings().get([\"workspaceManager\", \"workspace\"])\n\t\t\tworkspace_manager = WorkspaceManager(name="}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn ToolBackendManager(cls).get()\n\n\t# -------------------------------------------------------------------------\n\t# Create a new backend manager for the given path.  The\n\t# location (either absolute or relative to the plugin's directory) must contain the plugin's\n\t# identifier, separated by a colon.  The name may only contain letters, numbers,\n\t# and underscore.\n\t#\n\t# For example, a tool manager might be ``sqlite`` where tool storage is located at\n\t# ``/home/_plugins/my-plugin.db``.  That location is located relative to the parent of the\n\t# filesystem"}, {"generated_text": "def workspace_manager(cls):\t\"\"\"\n\tGets the workspace manager.\t\"\"\"\n\t\treturn cls._wmr\n\t\n\t@classmethod\n\tdef workspace_version(cls):\n\t\t\"\"\"\n\t\tGet the workspace version.\t\"\"\"\n\t\tif hasattr(cls, \"_version\"):\n\t\t\treturn cls._version\n\t\treturn 0\n\t\n\t@classmethod\n\tdef _check_options(cls, opts):\n\t\tif opts.workspace is not None:\n\t\t\treturn False\n\t\tif opts.project is None:\n\t\t\treturn False\t\n\t\tif opts.workspace_path is None:\n\t\t\treturn False\n\t\t# Check project paths\n\t\tif opts.workspace_user is None and opts"}]}
{"task_id": "6306091b73426c38ae68acda", "prompt": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\tplugin_manager_name = cls.__name__.replace('_', '')\n\t\tfor attr_name in get_plugins_setting('PLUGINS'):\n\t\t\tplugin_manager = get_attr_manager(attr_name)\n\t\t\tif plugin_manager.has_plugin(plugin_name):\n\t\t\t\treturn plugin_manager\n\n\treturn None\n\n\ndef setup_plugin_settings(plugin_names=[],\n\t\t\ttitle=PLUGIN_TITLE,\n\t\t\tdescription=PLUGIN_DESC,\n\t\t\tabout=PLUGIN_ABOUT,\n\t\t\tversion=PLUGIN_VERSION,\n\t\t\tabout_label=PLUGIN_ABOUT_"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn get_registered_plugins()\nimport unittest\nimport os\nimport shutil\nimport tempfile\nimport zipfile\n\nfrom pyidf import ValidationLevel\nimport pyidf\nfrom pyidf.idf import IDF\nfrom pyidf.plantcontroloequipmentcoefficients import PrmControlOequipmentCooling\n\nlog = logging.getLogger(__name__)\n\nclass TestPrmControlOequipmentCooling(unittest.TestCase):\n\n    def setUp(self):\n        self.fd, self.path = tempfile.mkstemp()\n\n    def tearDown(self):\n        os.remove(self.path)"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\tif cls._plugins is None:\n\t\t\tfor name in cls.plugin_names:\n\t\t\t\tcls.register_plugin(name)\n\t\treturn cls._plugins\n\n\t@classmethod\n\tdef get_plugin(cls, name):\t\n\t\tif name in cls._plugins:\n\t\t\treturn cls._plugins[name]\n\t\traise PluginLookupError(\"Plugin '{}' does not exist\".format(name))\n\n\tdef dispatch_functions(self, app):\n\t\t\n\t\tfor name in dir(self):\n\t\t\tmember = getattr(self, name)\n\t\t\tif not issubclass(member, AbstractFunction): \n\t\t\t\t"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn cls._plugins_manager\n\n\t@classmethod\n\t@requires_model\n\tdef plugin_settings(cls, model_name, nolock=False):\n\t\t\"\"\"\n\t\tReturns a :py:class:`.PluginSettings` instance for the specified model, or ``None`` if a model has been\n\t\tuninstalled.\tThe created plugin settings may be uninstalled itself by calling :py:func:`uninstalled_modules`,\n\t\trespectively.\n\n\t\tIf ``nolock`` is ``True`` and no lock is made, a new manager is opened to set ``PluginSettings`` values for"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn __plugins__['plugins']['plugin_manager']\n\n\t@property\n\tdef plugins(self): return self.core.core_plugins\n\n\nclass PluginManagerProxy:\n\t\"\"\" Manages plugins and allows plugins to interact with them as well as the :py:class:`WebPluginManager`. Note that there is a limited number of plugin managers that it spawns in the framework to register plugins from within a single core. The :py:class:`WebPluginManager` maintains references to the :py:class:`WebPluginManager`.\n\t\"\"\"\n\tdef __init__(self, core, core_"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn PluginManager()\n\n\t@classmethod\n\tdef enabled(cls):\treturn PluginManager().enabled()\n\"\"\"\nTest the support for HTTPS.\n\"\"\"\nfrom datetime import datetime\n\nimport pytz\nfrom django.conf import settings\nfrom django.contrib.auth.models import User\nfrom django.core.urlresolvers import reverse\nfrom django.test import TestCase\nfrom django.test.utils import override_settings\nfrom mock import patch\nfrom pytz import UTC\nfrom uuid import uuid4\n\nfrom badges.tests.factories import BadgeFactory\n\nfrom badges.queue.models import"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\tglobal _plugin_manager\n\t\tassert cls.plugin_manager is not None\n\t\treturn cls.plugin_manager\n\n\tdef plugin(self, plugin_name, key, **kw):\n\t\t\"\"\"\n\t\tGets a plugin from a plugin manager. You won't have to provide a separate\n\t\t``PluginInfo`` class around. If you have some code between the \n\t\timplementations you'll have to contact the plugin manager.\n\t\t\n\t\t.. note:: You don't need to access ``self.config`` here,\n\t\t      instead the returned instance might be overriden by\n\t\t      ``self.config['plugins"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn cls.PluginManager\n\n\tdef plugin_settings_window(self, plugin_id):\t\n\t\n\t\treturn self.PluginSettingsWindow\n\n\tdef plugins_settings_window(self, window):\n\n\t\t\"\"\"\n\t\tGets the plugins settings window.\n\n\n\t\tParameters:\n\t\t\twindow (PluginWindow.PluginDialog):\n\t\t\"\"\"\n\n\t\tself.plugin_id = window.plugin_id\n\t\tself.plugins = [plugin.plugin_object for plugin in self.plugins_managers() if plugin.plugin_id == self.plugin_id]\n\t\t\n\tdef plugins_current_row("}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\treturn cls.__plugin_manager\n\n\"\"\"\nModule for the creation of a course in Studio\n\"\"\"\n\nimport logging\nimport urllib\nimport json\nimport requests\n\nfrom. import get_courses\nfrom.helpers import get_studio_url\n\nlog = logging.getLogger(__name__)\n\n\nclass CourseException(Exception):\n    \"\"\"\n    An exception class for exceptions in this module.\n    \"\"\"\n    pass\n\n\nclass DuplicateCourseError(CourseException):\n    \"\"\"\n    This error indicates that course.xml already exists.\n    \"\"\"\n    pass\n\n\nclass XMLModuleStore(object):\n    \"\"\"\n    This class manages"}, {"generated_text": "def plugins_manager(cls):\t\"\"\"\n\tGets the plugin manager.\t\"\"\"\n\t\tfrom octoprint import plugins\n\n\t\tpath = cls.get_repository_root()\n\t\tpath = path.replace(os.sep, \"/\") + os.path.sep\n\t\tpath = os.path.join(path, \"OctoPrintRepo\")\n\t\treturn octoprint_plugin.get_plugin_manager(path)\n\n\n\t@property\n\tdef repositories(self):\n\t\t\"\"\"\n\t\tReturns a list of all OctoPrint's installed plugins as a dictionary containing \"name\": plugin\"\"\"\n\t\tfrom allura.lib.markdown.convert import MarkdownConverter\n\t\treturn Markdown"}]}
{"task_id": "6306091c73426c38ae68acdc", "prompt": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\treturn None\n\t\t\n\t\t\n\t\n\t@classmethod\n\tdef open(cls, spec_content=None, validate_spec_files=True):\n\t\tcontent = read_yaml_content(spec_content)\n\t\t\n\t\tif not content:\n\t\t\traise NoSpecContent(\"Missing configuration file. Should be an artifact yaml file\")\n\t\t\n\t\tinstance = cls()\n\t\t\n\t\ttry:\n\t\t\tinstance.load_from_content(content, validate_spec_files)\n\t\t\tinstance.name = get_key(instance, \"module\")\n\t\t\tinstance.spec_files = SpecFile.from_dict(SPEC_"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tif spec_content is None:\n\t\t\traise IRValidatorException(\"SPEC MUST declare spec file content\")\n\n\t\tspec = OrderedDict()\n\t\tspec_map = OrderedDict()\n\n\t\tfor field_name, field_value in spec_content.items():\n\t\t\ttry:\n\t\t\t\ttype_spec, type_type, type_name = FieldSpec.TYPE_SPEC_MAP[field_name]\n\t\t\t\tspec[field_name] = field_casts[type_type](field_value)\n\t\t\texcept KeyError as err:\n\t\t\t\ttype_spec = type_name\n\t\t\t\ttype_type"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tvalidated = None\n\t\tif spec_content: # try to load specification's content\n\t\t\tdata = yaml.safe_load(spec_content)\n\t\t\tdata['data'] = cls.parse_data(data['data'])\n\t\t\tvalidated = data\n\t\telse:\n\t\t\tvalidated = cls.parse_data(schema_data)\n\t\tif not validated:\n\t\t\traise IRValidatorException(\"Failed to validate\")\n\t\treturn validated\n\t\t# try to convert to yaml\n\t\tdata = cls.convert_input(cls.clean_data(data))\n\t\tvalidated = {}\n\t\tfor"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\t# check file headers\n\t\tf = open(spec_content, 'r')\n\t\tlines = f.readlines()\n\t\tf.close()\n\n\t\t# the following code generates valid Spec_File content in the same\n\t\t# name as spec_content\n\t\tspecification_file_content = u''\n\t\tfor l in lines:\n\t\t\tl = l.strip()\n\n\t\t\t# check if spec file contains line before the spec file (should\n\t\t\t# check the line in first field)\n\t\t\tif l.startswith(\"%s\" % LAST_SPEC_FILENAME):\n\t\t\t\tline_index ="}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tvalid_specs = {\n\t\t\t\"file\": cls.validate_from_file_object,\n\t\t\t\"yaml_dict\": cls.validate_from_yaml_object,\n\t\t\t\"spec\": cls.validate_from_spec_object,\n\t\t}\n\t\tif spec_content is not None:\n\t\t\traise IRValidatorException(\"Spec file content is not valid: %s\" % spec_content)\n\t\treturn valid_specs\n\n\n\t@classmethod\n\tdef validate_from_file_object(cls, path):\n\t\t\"\"\"\n\t\tValidates that file content is valid\n\t\t"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tspec_str = None\n\t\tspec = yaml.load(spec_content)\n\n\t\tdata_file_path =  \"%s/%s\" % (cls.SPEC_JSON_BASE_NAME, spec[\"path\"])\n\t\tif not os.path.isfile(data_file_path):\n\t\t\tlogger.error(\"ERROR: data file '%s' is missing\" % data_file_path)\n\t\t\treturn {}\n\n\t\tlogger.info(\"validate.spec_file '%s', with fields %s\" % (data_file_path,\n\t\t\t\t\t\t\t\t\t\t\tspec['fields'].keys()))\n\n\t"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tif not (spec_content or isinstance(spec_content, dict)) or \\\n\t\t\tnot isinstance(spec_content, string_types):\n\t\t\traise IRValidatorException(\"No %s content found in spec\" % spec_content)\n\t\tfor key, val in spec_content.items():\n\t\t\tif not (key in cls.REQUIRED_FIELDS) or \\\n\t\t\t\tnot isinstance(key, string_types):\n\t\t\t\traise IRValidatorException(\"key '%s' not provided in spec\" % key)\n\t\t\tval.validate(cls, spec_content[key])\n\t\tfor"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tif not spec_content:\n\t\t\traise IRValidatorException(\"no content to validate\")\n\t\ttry:\n\t\t\treturn cls.validate_yaml(spec_content)\n\t\texcept YAMLError as rce:\n\t\t\traise IRValidatorException(repr(rce))\n\nclass XMLValidator(Validator):\t\n\t\"\"\" XMLValidator is an OpenNSchemaValidator which checks a IR file against each schema\n\t\tMust pass instance of IRValidator.\n\tFor usage examples see README and README.md:\n\t\thttp://stackoverflow.com/questions/197969/validate-against"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tfrom copy import deepcopy\n\t\tspec = cls.Spec()\n\t\tspec_content['spec'] = deepcopy(spec)\n\t\treturn spec\n\n\t\n\tdef validate_from_spec_file(cls, spec_file_content=None):\n\t\t\"\"\"\n\tvalidates that spec_file content has all required fields\n\n:param spec_file_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tfrom copy import deepcopy\n\t\tspec = cls."}, {"generated_text": "def validate_from_content(cls, spec_content=None):\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\t\"\"\"\n\t\tif not spec_content:\n\t\t\traise IRValidatorException(\"spec content is missing\")\n\t\tif not isinstance(spec_content, dict):\n\t\t\traise IRValidatorException(\"spec content is not a dict\")\n\t\ttry:\n\t\t\tparsed_spec_content = yaml.safe_load(spec_content)\n\t\texcept yaml.YAMLError, exc:\n\t\t\traise IRValidatorException(\"Failed to load spec file: %s\" % exc)\n\t\treturn spec_content\n\n\tdef __init__(self, spec_content=None):\n\t\tself.spec_file = spec"}]}
{"task_id": "6306091c73426c38ae68acdd", "prompt": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tfrom.validator.validation import Validator\n\t\ttry:\n\t\t\tdata = cls.yaml_load(yaml_file)\n\t\t\tValidator.validate_schema(data)\n\t\texcept Exception as e:\n\t\t\traise InvalidValidatorException(e)\n\t\treturn data\n\n\t@classmethod\n\tdef yaml_load(cls, yaml_file=None): \n\t\t\"\"\"\n\t\tLoad/deserialize a YAML file\n\t\t:param yaml_file: path to a YAML file to read\n\t\t:return: The parsed data.\n\t\t\"\"\"\n\t\tfrom.validator.validator import Validator\n\t\ttry:"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tif not yaml_file:\n\t\t\ttry:\n\t\t\t\tyaml_file = resource_filename(__name__, 'data/sos.yaml').decode('utf-8')\n\t\t\texcept AttributeError:\n\t\t\t\t# The yaml file does not contain a YAML document\n\t\t\t\traise IRValidatorException(\"YAML file not provided!\")\n\n\t\tr = cls(yaml_file, self.schema)\n\t\tif not r.validate():\n\t\t\traise IRValidatorException(r.message)\n\t\telse:\n\t\t\treturn r\n\n\tdef get_property_values(self):\t\treturn self.json_data"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tres = cls._validate_from_file(yaml_file)\n\t\treturn res\n\n\t@staticmethod\n\tdef _validate_from_file(file):\n\t\timport yaml\n\t\tfrom. import config\n\t\tfrom.yaml.converters import SafeLoader\n\n\t\tdata = yaml.load(file)\n\n\t\t# validate the data:\n\t\t# check if 'data_to_validate' key is valid for validity of the data_to_validate key\n\t\tdata = cls._validate_data(data, yaml.full_load)\n\n\t\treturn data\n\n\t@staticmethod\n\tdef _validate"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\timport yaml\n\t\ttry:\n\t\t\tyaml_file_data = yaml.safe_load(yaml_file)\n\t\t\tif yaml_file_data:\n\t\t\t\t# We have some new data loaded, so we need to update\n\t\t\t\treturn validate(yaml_file_data, cls.get_data)\n\t\t\telse:\n\t\t\t\traise IRValidatorException(\"The data is invalid or missing in file: %s\" % yaml_file_data)\n\t\texcept yaml.scanner.ScannerError as e:\n\t\t\traise IRValidatorException(\"The file %s has broken YAML: %s\""}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\td = yaml.load(open(yaml_file))\n\t\tif not d:\n\t\t\traise IRValidatorException(\"Cannot read YAML file\")\n\t\n\t\t# TODO: Implement validation with the file\n\t# TODO: Write default validation code\n\t# TODO: Implement validation with the file\n\t# TODO: Write user data validation\n\traise IRValidatorException(\"Failed to parse YAML file correctly. Missing required fields\")\n\"\"\"Test class for Remote Execution\"\"\"\n# pylint: disable=too-many-lines\nfrom __future__ import absolute_import\nimport logging\nimport os\nimport sys\n"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tdata = _load_with_defaults(yaml_file)\n\t\treturn cls.from_dict(data)\n\ndef load_yaml_file(yaml_file):\n\t\"\"\"\n\tLoads and validates a YAML file, in any format it can be processed\n\n:param yaml_file: Path to YAML file\n:return: (data_dict) Dictionary with data loaded from a YAML file\t\t\t\n\"\"\"\n\treturn _load_with_defaults(yaml_file)\n# Copyright (c) 2015 Intel, Inc.\n# All Rights Reserved.\n#\n#    Licensed under"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tif yaml_file:\n\t\t\ttry:\n\t\t\t\tparser = YamlParser(yaml_file) \n\t\t\t\tif any(\n\t\t\t\tnot isinstance(data, dict)\n\t\t\t\tand not isinstance(data, list)\n\t\t\t\tand not parser.supports_multiple_values()\n\t\t\t\tfor data in parser.get_data()\n\t\t\texcept Exception as ex:\n\t\t\t\traise IRValidatorException(\"YAML\", ex)\n\n\t\t\tif \"title\" not in data:\n\t\t\t\traise IRValidatorException(\"YAML\", \"Title is required\")\n\t\t\tif \"description\" not in data:\n\t\t\t\traise IR"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tfrom configparser import ConfigParser\n\t\timport json\n\n\t\twith open(yaml_file) as yml_file:\n\t\t\tconfig = ConfigParser()\n\t\t\tconfig.read_string(yml_file)\n\t\t\treturn cls.validate_from_dict(config)\n\n\t@classmethod\n\tdef validate_from_dict(cls, yaml_dict):\n\t\t\"\"\"\n\t\tLoads & validates that a DSL file has all required fields\n\t\t:param yaml_dict: Dictionary whose keys are attribute/value pairs\n\t\t:raise IRValidatorException: when mandatory data is missing in file\n\t\t:return:"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\tpath_to_validator = None\n\t\tif not yaml_file or not isinstance(yaml_file, file):\n\t\t\traise IRValidatorException('YAML file must be a valid YAML file')\n\t\tif yaml_file is None:\n\t\t\tpath_to_validator = path_join(settings.BASE_DIR,'validator',__name__)+'.txt'\n\t\twith path_join(settings.BASE_DIR,'validator',__name__,'data.yml') as data_file:\n\t\t\tdata_file_yml = file(data_file,'r')\n\t\t\tdata_file_"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\t\"\"\"\n\t\t# validate if yaml file is present\n\t\tif yaml_file is not None:\t\n\t\t\ttry:\n\t\t\t\twith open(yaml_file) as yf:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tobj = yaml.load(yf)\n\t\t\t\t\texcept Exception as e:\n\t\t\t\t\t\traise IRValidatorException(\"YAML format error in file \"+yaml_file+\", line \"+str(e))\n\t\t\t\t# check if required fields defined by the YAML file have to be defined\n\t\t\t\tfor required in obj:\n\t\t\t\t\tvalidate_required_fields(obj[required])\n\t\t\texcept Exception as e:\n\t\t\t\texcp"}]}
{"task_id": "6306091d73426c38ae68ace5", "prompt": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tfor include, include_node in parser_dict.items():\n\t\t\t# It is important to not include subnodes (ex self)\n\t\t\t# because it is not really necessary in the specfile. It will\n\t\t\t# only not be correct in the actual group.\n\t\t\t# The reason is the removal of the include_node in the spec file is\n\t\t\t# that what we are doing here. Also we cannot exclude the group\n\t\t\t# from the include dict that we get, since group_no_excl.cpp\n\t\t\t# is not an essential part of the module; because some of the"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\t# First try to include all groups from this file's group list.\n\t\tfound_include_groups = False\n\t\tfor group, groupdict in parser_dict.items():\n\t\t\tif not found_include_groups: break\n\t\t\tincludedict, tokens = self._parse_include_group(groupdict, None)\n\t\t\tfor token in tokens:\n\t\t\t\tm = includedict.match(token)\n\t\t\t\tif not m: continue\n\t\t\t\tkey = m.group('key')\n\t\t\t\tif m.group('value') is not None:\n\t\t\t\t\ttokens.pop(key)\n\t\t\t\t\tinclude"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tfor include in parser_dict.keys():\n\t\t\tpattern_type = \"include_groups\"\n\t\t\tif isinstance(include, Pattern):\tself.file_patterns.append(include)\n\t\t\t\tself.included[include.name] = []\n\t\t\t\tfor filename in include.filenames:\n\t\t\t\t\tself._include_file(filename)\n\t\treturn\n\n\n\t#\n\t# Pre-process the given include.\n\t#\n\t# This method will process the include. It will then try to read\n\t# the dependencies, then if present, add them to the include dict,\n\t# depending"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tif self.__include_groups is not None:\n\t\t\tself.__include_groups.extend(parser_dict.get(self.filename, []))\n\t\treturn parser_dict\n\n\ndef _get_global_defines(parser_dict):\n\tdefines = {}\n\n\tfor define_group in parser_dict.get(\"DEFINE\", \"\").split():\n\t\tdefines[define_group] = parser_dict.get(\"DEFINE\", \"\").split(True)[1].split()[0].strip()\n\n\treturn defines\n\n\nclass IncludeGroup:\n\t\"\"\"\n\tBase class for the include group and parser"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\t# Find the include file in the current package.\n\t\tinclude_path = parser_dict.get(Section.INCLUDE_PATH).value\n\t\tif not include_path:\n\t\t\tself._include_groups_no_path(parser_dict)\n\n\t\t\treturn\n\n\t\t# Parse include flags\n\t\tincluded_list = parser_dict[Section.INCLUDE_LIST]\n\t\tfor item in parser_dict[Section.INCLUDE_LIST].split(','):\n\t\t\tincluded_list += item.split(',')\n\t\tself._add_include_specs([x.strip() for x in included_list])\n\n"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tconfig = parser_dict['config']\n\t\tinclude_paths = config.parser.include_paths\n\n\t\t# First we need to see if it's a custom section\n\t\tsection = 'DEFAULT'\n\t\tif include_paths and config.parser.has_section(section):\n\t\t\tinclude_path = include_paths.pop(0)\n\t\t\tinclude_paths.extend(config.parser.include_paths)\n\t\t\tif include_path not in include_dict:\n\t\t\t\tinclude_dict[include_path] = dict()\n\t\t\t\tself._include_groups(include_dict[include"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tret_dict = {}\n\t\tdef_dict = {}\n\t\tin_section = False\n\t\twith io.open(self.filename, 'r+', errors='include') as inf:\n\t\t\tfor line in inf:\n\t\t\t\tline = line.strip()\n\t\t\t\tif line == '' or line.startswith('#'):\n\t\t\t\t\tif in_section:\n\t\t\t\t\t\tsection_name = line\n\t\t\t\t\telse:\n\t\t\t\t\t\traise ParseError('Bad include section in {0}:\\n{1}'\n\t\t\t\t\t\t.format(self.filename, line))\n\t\t\t\t\tin_section = True\n\t\t\t\t\tcontinue\n\n\t\t\t\tline ="}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tfor group_name in parser_dict.get(\"include\", []):\n\t\t\tself._include_group(parser_dict, group_name)\n\n\tdef _parse_args(self, command_map):\n\t\tparser = argparse.ArgumentParser(parents=[parser_dict, self.arguments.parser])\n\t\tcommands = command_map.get(\"command\", \"commands\")\n\t\tparser.parse_args(commands)\n\t\treturn parser.parse_args()\n\n\tdef _include_group(self, parser_dict, group_name):\n\t\tgroup_dict = {}\n\t\ttry:"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\tif not parser_dict['include_groups']:\n\t\t\tlog.log(\"<include-groups> with no group directives, not processing\")\n\t\t\treturn\n\n\t\tinclude = {}\n\t\tfor opt in parser_dict['include_groups']:\n\t\t\tif isinstance(opt, basestring):\n\t\t\t\tname, args = opt, ''\n\t\t\telse:\n\t\t\t\tname = '*'\n\t\t\t\targs = None\n\n\t\t\tif args:\n\t\t\t\taddopt = opt.add_option\n\t\t\t\tif addopt is None:\n\t\t\t\t\tif not opt.dest:\n\t\t\t\t\t\topt.dest = opt.default or os.path.basename"}, {"generated_text": "def _include_groups(self, parser_dict):\t\"\"\"\n\tResolves the include dict directive in the spec files.\t\"\"\"\n\t\t# Use group declarations from config file\n\t\tself._add_include_groups(parser_dict)\n\t\treturn\n\n\tdef _override_include_groups(self, parser_dict):\n\t\t\"\"\"\n\t\tOverride the group declarations from config file or not.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tself._add_include_groups(parser_dict)\n\t\t\treturn\n\t\texcept Exception as e:\n\t\t\t# Error handling here is kind of hacky but we need this in order \n\t\t\t# to get the full path\n\t\t\tlogger.error(\"Unable to override include groups for spec file: %"}]}
{"task_id": "6306092373426c38ae68acfa", "prompt": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tdef __str_hook(spec):\n\t\t\tif not spec:\n\t\t\t\treturn '?'\n\t\t\tstr = spec.location()\n\t\t\tif not str: return ''\n\t\t\tif type(str) == type(''): return '=' + str\n\t\t\treturn str\n\t\tresult = []\n\t\twhile spec:\n\t\t\tspec = spec.parent\t\t\t\t# grab parent from stack\n\t\t\tif not spec:\n\t\t\t\tif spec.has_location_: break        # we've consumed all this loc until we find it\n\t\t\tresult.append(spec)\n\t\t\tspec = spec.parent\n\t\treturn self.resolve("}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tfor d in self.features.values():\n\t\t\td.resolve(self)\n\n\tdef get_features(self, *names):\n\t\t\"\"\"\n\t\tResolve input feature values.\n\t\tIf names are given, resolve names; otherwise return a default value.\n\n\t\t:param names: input names to resolve\n\t\t\"\"\"\n\t\tfeatures = [self.features.get(name) for name in names]\n\t\tif not features:\n\t\t\tfor name, d in self.features.items():\n\t\t\t\td.resolve(self, feature_class=d)\n\t\t\treturn self.features.get"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tsrc = self.src\n\t\tsrc = Utils.to_list(src)\n\t\tdst = self.dst\n\n\t\tsrc_len = len(src)\n\t\tsrc = self.adjust_paths(src, src_len)\n\n\t\tdst_len = len(dst)\n\t\tdst = self.adjust_paths(dst, dst_len)\n\n\t\tret = self.src\n\t\tret_args = []\n\t\tfor i in range(dst_len):\n\t\t\tdst_arg = self.relative_path_to_args(dst[0], i)\n\t\t\tsrc_"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tif os.environ.has_key('SPEC_INC_PATH') or os.environ.has_key('SPEC_LIB_PATH'):\n\t\t\tself.spec_inc_paths = os.environ['SPEC_INC_PATH']\n\t\t\tself.spec_lib_paths = os.environ['SPEC_LIB_PATH']\n\t\telse:\n\t\t\tself.parse_arguments()\n\n\t\tif os.environ.has_key('SPEC_LIB_PATH'):\n\t\t\tself.spec_lib_paths = os.environ['SPEC_LIB_PATH']\n\t\telse:\n\t\t\tself."}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tself.spec_defaults_dict = {}\n\t\tfor key in self.source_defaults:\n\t\t\tself.spec_defaults_dict[key] = None\n\t\tfor key in self.compiler_defaults:\n\t\t\tself.spec_defaults_dict[key] = None\n\n@conf\ndef get_compiler_types(self):\n\t\"\"\"\n\tResolve compiler arguments for specific target compilers.\n\t\"\"\"\n\tret = []\n\n\tif not hasattr(self, 'compiler') or self.compiler is None:\n\t\treturn ret\n\n\tfor compiler_name in self.compiler:"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\treturn self.get_default(self.inputs.default)\n\t\n\t@default_traits(apply=default_traits_apply)\n\tdef apply(self, context):\n\t\tinputs = self.inputs.copy()\n\t\tinputs.emit(self.inputs.copy())\n\n\nclass Select2Select(Select):\n\t\"\"\"Select filter: only select values which of type bool/int/string type, and\n\toptionally restrict the selection. All values set in selection must satisfy\n\teach other value.\n\t\"\"\"\n\tdef __init__(self, *args, **config):\n\t"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tif self.spec_defaults:\n\t\t\treturn self.spec_defaults\n\t\tspec_defaults = dict(self._default_source)\n\t\tif'spec_defaults' in self.__dict__:\n\t\t\tspec_defaults.update(self.__dict__['spec_defaults'])\n\t\treturn SpecSpec(self.name, self.default_value, spec_defaults)\n\n\tdef _parse_dependencies(self, node_or_string, node_or_dict, verbose=False):\n\t\t\"\"\"\n\t\tParses dependencies from given node or a node list and returns a \n\t\tSpec that has no"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\treturn Spec(\n\t\t\tname = 'default',\n\t\t\tfile ='source',\n\t\t\tpath = self.path,\n\t\t\tformat = 'properties',\n\t\t\tmandatory = ['default'],\n\t\t)\n\n\tdef validate(self):\n\n\t\t# Set default value\n\t\tif not self.isValid():\n\t\t\tself.setBadValue('source', self.name,'source', '<unknown source>')\n\n\t\t# Set source path\n\t\tif self.sourcePath is None:\n\t\t\tpath = self.getSourcePath(self.name)\n\t\t\tif not path:\n\t\t\t\tself.setBadValue"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\tassert(isinstance(self.spec_or_spec, Spec))\n\t\tif self.spec_or_spec.spec_options:\n\t\t\tspecdef_name = self.spec_or_spec.spec_options['id'].value\n\t\t\tfor c in self.spec_or_spec.spec_options.items():\n\t\t\t\tyield { 'name': specdef_name, 'value': c }\n\tdef add_spec_defaults(self, specdef_name, specdef_value, defaults):\n\t\tassert(isinstance(specdef_name, basestring))\n\t\tspecdef"}, {"generated_text": "def get_spec_defaults(self):\t\"\"\"\n\tResolve arguments' values from spec and other sources.\t\"\"\"\n\t\trequired = []\n\t\toptional = []\n\t\t\n\t\tif self.get(\"optional\"):\n\t\t\trequired_string = \":[ REQUIRED | OPTIONAL ]\"\n\t\t\tif self.has(\"required\"):\n\t\t\t\trequired_string += \"[ REQUIRED | OPTIONAL ]\"\n\t\t\tif self.has(\"optional\"):\n\t\t\t\toptional_string = \":[ OPTIONAL | OPTIONAL ]\"\n\t\t\t\tif self.has(\"optional\"):\n\t\t\t\t\toptional_string += \"[ OPTIONAL | OPTIONAL ]\"\n\t\t\t\n\t\t\ttry:\t\t\t\t\n\t\t\t\tif self.has(\"spec\"):\n\t\t\t\t\tself.get(\"options\");\n\t\t\t\t\toptional = \" \" + self.get(\""}]}
{"task_id": "6306092973426c38ae68ad01", "prompt": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\treturn {\"ignore_read_only\": frappe.get_hooks(\"ignore_read_only\")}\n\n\tdef run_patch(self, args=None, **kwargs):\n\t\targs = args or frappe._dict()\n\t\targs.update(kwargs)\n\t\tself.run(**args)\n\t\treturn frappe.get_attr(args.doctype, kwargs.get('name'))\n\n\tdef run_patch_method(self, args=None, **kwargs):\n\t\targs = args or frappe._dict()\n\t\targs.update(kwargs)\n\t\tself.run_custom_method"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tdeprecated_args = {}\n\t\tfor k in self.deprecated_for_removal:\n\t\t\tif k in frappe.local.conf:\n\t\t\t\tif k in frappe.local.conf.deprecated.get_reserved_columns():\n\t\t\t\t\tif d.meta.get_field(k):\n\t\t\t\t\t\ttmp_key = cstr(d.get(k)).strip()\n\t\t\t\t\t\tif tmp_key in deprecated_args:\n\t\t\t\t\t\t\tmsg = _('Deprecated {0}').format(tmp_key)\n\t\t\t\t\t\t\tdeprecated_args[tmp_key] = msg\n\n\t\t\t\t\telse:\n\t\t\t\t\t\ttmp_key ="}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tret = {}\n\t\tfor module_name, module in self.modules.items():\n\t\t\ttry:\n\t\t\t\tret[module_name] = sys.modules[module_name].__deprecated__\n\t\t\texcept:\n\t\t\t\tif module.startswith(\"zim\") and module not in self.deprecated_modules:\n\t\t\t\t\tret[module_name] = sys.modules[module].__name__\n\t\treturn ret\nimport unittest\nfrom unittest.mock import MagicMock, call, patch\n\nimport torch\n\nfrom allennlp.common import Params, ParamsDict, ParamsDictVar, DictVar\n"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tret = dict([(key.lower(), value) for key, value in self.args.items()])\n\t\tret.update(self.deprecated_args)\n\t\treturn ret\n\t@deprecated('Please use get_args instead of get_deprecated_args')\n\tdef get_help(self):\n\t\treturn self.help\n\ndef task_include_roles(task):\n\t\"\"\"\n\tDecorates ``frappe.states.get_global_default()`` from the :py:func:`waflib.Task.Task.name` task's\n\t``conf.env.role_"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\treturn dict([(word, True) for word in re.findall(\n\t\t\tr'^(\\s+)?(.*?)$',\n\t\t\tself.__deprecated_actions) if not word.endswith('@#')\n\t\t\tand word.startswith('@')\n\t\t\tfor word in self.__deprecated_options and not word == word\n\t\t)])\n\n\tdef get_action_options(self):\n\t\t\"\"\"\n\t\tReturning dict with the options which should be passed for the action.\n\t\t\"\"\"\n\t\taction_actions = {}\n\t\tfor action, arguments in self.__action_names.items():\n\t\t\tif action not in action_"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tDEPRECATED_ARGS = {\n\t\t\t'do_prefer_basicauth': ['--prefer-basicauth'],\n\t\t\t'o_prefer_basicauth_failed': ['--prefer-basicauth-failed'],\n\t\t\t'o_verbose': ['-v'],\n\t\t\t'o_use_userpass': ['--use-random'],\n\t\t\t'o_version': ['--version'],\n\t\t\t'o_help': ['-h'],\n\t\t\t'return_arg': ['--argument'],\n\t\t\t'return_action': ['--action'],\n\t\t\t'return_policy': ['--policy'],\n\t\t\t'return_interval': ['--retries'],\n\t\t"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tdeprecated = {}\n\n\t\tdeprecated['shortdesc'] = \\\n\t\t\t\"\"\"\n\tBoolean or boolean indicating whether the deprecated option is displayed or not.\n\t\"\"\"\n\t\tdeprecated['booleandesc'] = \\\n\t\t\t\"\"\"\n\tBoolean or boolean indicating whether the deprecated option is displayed or not.\n\t\"\"\"\n\t\tdeprecated['valuedesc'] = \\\n\t\t\t\"\"\"\n\tBoolean or boolean indicating whether the deprecated option is displayed or not.\n\t\"\"\"\n\t\tdeprecated['multiple'] = \\\n\t\t\t\"\"\"\n\tBoolean specifying whether the deprecated option is display as multiple choice or not.\n\t\"\"\"\n\t\tdeprecated['"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\targs_deprecated = {}\n\t\tfor short_name in ('deprecated_by_user', 'deprecated_by_user_id', 'deprecated_by_group',\n\t\t\t'site_config','system_manager_enabled', 'root_controller', 'system_manager'):\n\t\t\tattr = getattr(self, short_name)\n\t\t\tif callable(attr):\n\t\t\t\t# calling hooks get the property and condition from the base document\n\t\t\t\ttry:\n\t\t\t\t\tproperty, condition = attr.fget(self)\n\t\t\t\t\t# check if the user wants to use the property\n\t\t\t\t\toverride = attribute in"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\tresult = dict(deprecated_for_removal=self.deprecated_for_removal, deprecated_and_removed=self.deprecated_and_removed)\n\t\tresult.update(self.as_dict())\n\t\treturn result\n\n\tdef set_deprecated(self,new_key,old_value):\n\t\t\"\"\"\n\t\tThis method updates the description of a deprecated parameter.\n\t\tParameters are the same as in :py:func:`set_deprecated`\n\n\t\t:param new_key: Deprecated key\n\t\t:type new_key: str\n\t\t:param old_value: Parameter default\n\t"}, {"generated_text": "def get_deprecated_args(self):\t\"\"\"\n\tReturning dict with options which deprecate others.\t\"\"\"\n\t\treturn {'args':{}}\n\t\t\n\tdef write(self):\n\t\t\"\"\"Print info about what is happening.\"\"\"\n\t\tmsg = ''\n\t\tprint \"<ElfFile %s: %s, %s (pre-1.8:%s)>'\" % (\n\t\t \telf.get_name(self.elf['entry']),\n\t\t    self.get_label(),\n\t\t    self.get_date(),\n\t\t \t', '.join('%s=%s' % (k,self.get_value(k)) \\\n\t\t\tfor k in self.deprecated)))\n\t\t\t\t\n\tdef output(self):\n\t\t"}]}
{"task_id": "6306092c73426c38ae68ad02", "prompt": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\t# TODO do not hardcode values, only valid for required\n\t\tif self.__mandatory__:\n\t\t\treturn False\n\t\tif self.__missing:\n\t\t\treturn False\n\t\tif list_or_argparse.count(getattr(cli_args, self.__arg_name__, None)):\n\t\t\treturn False\n\t\tfor req_name in self.__req_names__:\n\t\t\treq_arg = self.__req_names__[req_name]\n\t\t\tif not check_path_exists(optpath(os.path.join(question_dir, req_arg['name_without_ext'] + \"."}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\t# Validate cli arguments\n\t\tfor cli_kwarg, cli_val in cli_args.items():\n\t\t\tif cli_kwarg not in answer_file_args:\n\t\t\t\tmsg = (\"Did not find valid cli argument for argument '%s'.\" \n\t\t\t\t\t\t\"This is probably a typo. Please contact the \"\n\t\t\t\t\t\t\"Madok API.\") % (cli_kwarg,)\n\t\t\t\traise PeachException(msg, cli_kwarg)\n\n\tdef validate_arg_missing_from_cli(self, cli_args, answer_file_args):\n\t\t\"\"\"\n\t\tValidates and prints the"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tfor arg in cli_args:\n\t\t\tif not arg in answer_file_args.keys():\n\t\t\t\tprint \"argument error: {} doesn't seem to exist\".format(arg)\n\n\t\t\tif (arg!= \"run_as\" and arg!= \"task_id\"):\n\t\t\t\tprint \"Argument error: {} doesn't seem to exist task_id\".format(arg)\n\n\tdef _check_required_arguments(self, args):\n\t\tfor name, action in self.actions.iteritems():\n\t\t\tif action.required and action.default!= args.get(name):\n\t\t\t\tprint \""}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tif cli_args['verbose']:\n\t\t\tsys.excepthook = cli_file_args['excepthook'] = None\n\t\telif cli_args['nohup']:\n\t\t\tif cli_args['debug']:\n\t\t\t\tif cli_args['verbose']:\n\t\t\t\t\tsys.excepthook = cli_file_args['debug'] = True\n\t\t\t\telse:\n\t\t\t\t\tsys.excepthook = cli_file_args['quiet'] = exc_type_default\n\t\telse:\n\t\t\tif cli_args['verbose']:\n\t\t\t\tsys.stderr.write(\"The following"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\twarnings.warn(self.deprecation_error(\n\t\t\t\"The command line argument has been deprecated and will be removed in release 0.12\"),\n\t\t\tDeprecationWarning, stacklevel=2)\n\n\tdef validate_options(self, cli_args):\n\t\t\"\"\"\n\t\tValidates and prints the CLI options.\n\n\t\t:param cli_args:  the parsed CLI options\n\t\t@return: True if all is well, False if not\n\t\t\"\"\"\n\t\twarnings.warn(self.deprecation_error(\"\", \"The method validate_options is obsolete.\"),\n\t\t\tDeprecationWarning, stacklevel=3)\n\t\treturn True"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\t# If a file has been specified, use that\n\t\tif answer_file_args.get(\"action\", \"\") == \"append\":\n\t\t\tself._set_flag(\"append\")\n\t\telse:\n\t\t\tself._set_flag(\"rename\")\n\n\tdef get_action(self):\n\t\t\"\"\"\n\t\tReturns the action to perform. This might be either \"-sync\", \"-async\"\n\t\tor \"-async-confirm\". In both cases, either \"sync\" or \"async-confirm\".\n\t\t\"\"\"\n\t\treturn self._action\n\n\tdef get_parser(self):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tif cli_args.warnings:\n\t\t\tfor warning in cli_args.warnings:\n\t\t\t\tself.print_message(warning)\n\t\t\t\texit(1)\n\t\tif cli_args.quiet is not True:\n\t\t\traise ValueError(\"A boolean value, '%s' is not allowed as %r as a boolean.\" % (cli_args.quiet, args.quiet_argument))\n\t\tif cli_args.verbosity < 1:\n\t\t\traise ValueError(\"Value of 'quiet' is set to more than one percent. (default is 1).\")\n\t\tif cli_args.verbosity > 1:"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tfor i, arg in enumerate(cli_args):\n\t\t\tif self._is_deprecated(arg):\n\t\t\t\tclick.echo(\"  - {}.{}\".format(i+1, arg), err=True)\n\t\t\telse:\n\t\t\t\tlogging.warning(\"  - {}.\".format(i+1))\n\t\n\tdef run(self):\n\t\targuments = self._get_arguments()\n\t\tcli_args = self._get_cli_arguments(arguments)\n\t\tfor cli_arg in cli_args:\n\t\t\tself.validate_arg_deprecation(cli_arg, cli_args)\n\t"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\tfor key in cli_args:\n\t\t\tif key not in answer_file_args:\n\t\t\t\traise API_MissingArgException(key)\n#!/usr/bin/python\n# Copyright (c) 2014 Adafruit Industries\n# Author: Tony DiCola\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense,"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\t\"\"\"\n\t\twarnings = []\n\n\t\tfor file_name in answer_file_args:\n\t\t\t# Get path relative to the script\n\t\t\tscript_path = os.path.join(os.path.dirname(self.path), file_name)\n\n\t\t\t# Get the command arguments\n\t\t\tcommand_args = cli_args[file_name]\n\n\t\t\t# Check the script is loaded\n\t\t\tif not os.path.isfile(script_path):\n\t\t\t\tsys.stderr.write('Error - missing path in answer file (' + file_name + ')\\n')\n\t\t\t\tsys.stderr.write('"}]}
{"task_id": "6306092d73426c38ae68ad04", "prompt": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\treturn 'command --help'\n\n\tdef get_options_summary(self):\t\n\t\t\"\"\"\n\t\tPrints the command specific options in a human-readable way\n\t\n\t\t@return: (str) the formatted options list\n\t\t\"\"\"\n\n\t\tcmd_option_specs = []\n\t\tfor name, opts in self.option_specs.items():\n\t\t\tname = self.cmdname(name)\n\t\t\tcmd_option_specs.append(name +'')\n\t\t\tif not opts.get('help'):\n\t\t\t\tflags = ', '.join(map(lambda x: x[1],opts['"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\tresult = []\n\t\tfor opt in self.options:\n\t\t\tif opt.dest == command_name:\n\t\t\t\tif getattr(self.options, 'debug_flag', False):\n\t\t\t\t\tpass\n\t\t\t\telse:\n\t\t\t\t\tresult.append(opt)\n\t\treturn result\n\n\tdef add_option(self, name, value='', force=False):\n\t\t\"\"\"Adds option specified by command line argument.\n\n        :param name: the option's name\n        :param value: the option's value\n        :type value: str\n        :param force: if True, force removal of option;\n        :type force"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\targs = self.command_object.get_attr(\"args\")\n\t\topt_names = self.command_object.get_attr(\"optnames\")\n\t\topt_dict = {}\n\t\tfor o in opt_names:\n\t\t\topts = self.command_object.get_attr(\"opts\")\n\t\t\tfor opt in opts:\n\t\t\t\tif \"optnames\" in opt:\n\t\t\t\t\topt_dict[opt] = o\n\t\t\t\telse:\n\t\t\t\t\topt_dict[opt] = None\n\t\t\tif options_help.has_key(o):\n\t\t\t\tcontinue\n\t\t\tif opt in"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\tself.check_arguments(command_name)\n\n\t\tcmd = None\n\t\tfor opt in self.options.values():\n\t\t\t# See if the command is valid\n\t\t\tcmd = opt.get_cmd(command_name)\n\n\t\t\tif not cmd:\n\t\t\t\tbreak\n\n\t\t\t# See if the user asked for help\n\t\t\tcmdline_help = cmd.get_help(command_name)\n\t\t\tif cmdline_help:\n\t\t\t\tself.process_cmdline_args(cmdline_help)\n\t\t\t\tbreak\n\n\t\t# Return the collected options\n\t\treturn sorted(self.options.keys())\n\n"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\treturn sorted(self.cli.command_options(self.cli_cmd_name))\n\n\tdef get_option(self, option):\n\t\t\"\"\"\n\tRetrieves the value of specified option and returns value of the specified option\n\t\"\"\"\n\ttry: \n\t\treturn getattr(self.options,option)\n\texcept AttributeError:\n\t\treturn None\n\n\n\"\"\"\nThe following functions run_qemu_console and run_qemu_session on a testbed node\n\"\"\"\ndef run_qemu_console(node):\n\t'''\n\tRuns qemu console on a running node\n\n\t:return"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\tcmd_options = []\n\t\tfor option in dir(self):\n\t\t\tif option.startswith('cmd_') or option == 'usage':\n\t\t\t\tcmd_options.append((option, getattr(self, 'help_' + command_name + '_' + option, None)))\n\t\treturn list(set(cmd_options))\n\tget_parser_options = get_parser_option_specs\n\t# get all options starting with'--'\n\tdef get_arg_value(self, arg_name, arg_obj=None):\n\t\tresult = None\n\t\ttry:\n\t\t\targ"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\treturn self._command_options[command_name][\"options\"][:]\n\n\tdef get_command_options(self, command_name):\t\t\"\"\"\n\tFor a command, if it is defined in the command, return its name and value\n\t\"\"\" \n\tfor command in self._command_options[command_name][\"options\"]:\n\t\tif not command[0] in self._command_prefix[command[1]].keys():\n\t\t\treturn \"command\" + command_name + command[1] + \" = \" + command[2:]\n\n\treturn \"command\" + command_name +"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\treturn self.cparser.get_all(command_name)\n\n\tdef get_option_spec(self, opt):\n\t\t\"\"\"\n\tGets the named option from an optparse.Values object\n\n:param opt: an optparse.Values object\n\t:return: the value of the option at the command-line\n\t     \"\"\"\n\t\treturn self.cparser.get_opt_tag(opt)\n\n\tdef get_option(self, opt, default=''):\n\t\ttry:\n\t\t\treturn self.cparser.get_any(opt, default)\n\t\texcept:\n\t\t\treturn"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\treturn self._get_all_options_from_command(command_name,\n\t\t\tself._options_context, self._all_subcommands)\n\n\tdef get_parser_args(self, command_name):\t\t\t# Get command line args\n\t\treturn self._get_all_options_from_command(command_name,\n\t\t\t\tself._options_context, self._subcommands)\n\nfrom __future__ import division\n\nfrom flask import current_app\nfrom selenium.webdriver.remote.webelement import WebElement\nfrom selenium.webdriver.remote.webelement"}, {"generated_text": "def get_parser_option_specs(self, command_name):\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\t\"\"\"\n\t\tret = self.send_command(command_name)\n\t\tretval = {}\n\t\tret, out = self.__parse_help(out)\n\t\tret = cmd_methods.get_options_list(ret, out)\n\t\treturn retVal\n\tpass # end of get_parser_option_specs\n\n\tdef get_host_cpu_count(self, host_ref):\n\t\t\"\"\"\n\t\tReturn the count of host CPUs on a host by setting the variable \\\n\t\tCPU_COUNT in config file \n\t\t\"\"\"\n\t\tret = self.send_command(\"cat /proc/"}]}
{"task_id": "6306092d73426c38ae68ad05", "prompt": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\t# get the argument names for this command, and their types\n\t\targuments = getattr(self, \"_get_option_spec_arg_defs\", None)\n\t\toption_string = ''.join('%s=%s' % (name,'%s' % (argument_name, a, ))\n\t\t                       for a, arguments in arguments.iteritems())\n\t\treturn '%s %s' % (command_name, option_string)\n\n\tdef _get_option_spec_arg_defs(self, command_name, option_name):\n\t\t\"\"\"\n\t\tGet the flags or values for"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\treturn self._options_manager.get_option_spec(self.name, self.subcommands, command_name, argument_name)\n\n\tdef add_option(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tAdds a command-specific option to this :class:`Subcommand` instance,\n\t\tincluding all of its options and subcommands from the argument list.\n\n\t\tIt is possible to give more flexibility in the context of this\n\t\t:class:`SubcommandCollection` instance to call into a method of\n\t\tthis class. The arguments to this method will be used in order"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\tif command_name in self.__options[argument_name][\"actions\"]:\n\n\t\t\treturn self.__options[argument_name][\"arguments\"][command_name]\n\n\t\treturn None\n\n\tdef get_command(self, command_name):\n\t\t\"\"\"\n\tRetrieves an action command from its name.\t\"\"\"\n\t\treturn self.__options[command_name][\"action\"]\n\n\tdef get_options(self):\n\t\treturn self.__options\n\n\t\n\t#=======================================================\n\t# Action\n\t#=======================================================\n\tdef do_action(self):\n\t\t\"\"\"\n\t\tHandles the actions for a specific"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\treturn getattr(self._registry[command_name], argument_name)\n\n# ***** BEGIN LICENSE BLOCK *****\n# Version: MPL 1.1/GPL 2.0/LGPL 2.1\n#\n# The contents of this file are subject to the Mozilla Public License Version\n# 1.1 (the \"License\"); you may not use this file except in compliance with\n# the License. You may obtain a copy of the License at\n# http://www.mozilla.org/MPL/\n#\n# Software distributed under the License is distributed on an \"AS IS\" basis,\n"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\ttry:\n\t\t\tgetopts = utils.cmd_dict[command_name]['getopts']\n\t\texcept KeyError:\n\t\t\traise exceptions.CommandError(command_name, command_name+' command option not defined.')\n\t\t\n\t\tif argument_name is not None:\n\t\t\tif len(argument_name) > 1:\n\t\t\t\traise exceptions.CommandError(command_name,command_name+' command option cannot be specified once.')\n\t\telse:\n\t\t\tif len(argument_name) == 0:\n\t\t\t\traise exceptions.CommandError(command_name,command_name+' option name cannot be empty.')\n\t"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\tassert self.__find_option_spec is not None, 'Get option specification.'\n\t\tif not self.__find_option_spec:\n\t\t\treturn None\n\t\treturn self.__find_option_spec.get(command_name, None)\n\n\tdef get_options(self):\n\t\t\"\"\"\n\t\tReturns options as a dictionary containing name-value options.\n\t\tMust return a dictionary.\n\t\t@return A dictionary of options.\n\t\t\"\"\"\n\t\toptions = {}\n\t\tfor option_name, option_info in self.__find_option_spec.items():\n\t\t\toptions[option"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t      \n\t      \n\t      \n\t      \n\t      \n\t      \n\t      \n\t      \n\t        self.commands['get_option'] = command_name.split('_')[1]                                \n\t      \n\t      \n\t      \n\t      \n\t      \n\t       self.parameters[argument_name] = eval(argv[ArgumentName])\n\n       \t\t\texcept:\n\t      \n\t      \n\t      \n\t      \n\t      \n\t       self.parameters[argument_name].set(argv[ArgumentName], argv[ArgumentValue])\n\n\t      \n\t      \n\t      \n\t      \n\t       self.parameters['[file]'].set(argv[File])\n\n\t      \n\t"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\tspec = sys.argv\n\t\tfor i in range(min(len(spec), len(command_name))):\n\t\t\tif command_name[:len(command_name) - len(prefix)] == spec[:len(name)]:\n\t\t\t\tspec[i] = spec[i][:len(name)]\n\t\t\t\tbreak\n\t\treturn spec\n\n\tdef get_options(self, options_spec):\t# @ReservedAssignment\n\t\treturn options_spec\n\nclass TestCommand(Command):\n\tdef __init__(self,*args):\n\t\tcommand_name = args[0]\n\t\tself."}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\traise NotImplementedError()\n\n\t# -------------------------------------------------------------------------\n\t# Configuration of the given option\n\t# -------------------------------------------------------------------------\n\n\t@abc.abstractmethod\n\tdef get(self, file_name, option_name=None):\tif option_name is None:\n\t\t\traise NotImplementedError()\n\n\t\tconfig = self.get_config(option_name)\n\n\t\tif file_name not in config:\n\t\t\treturn \"The option '%s' has no configuration defined for %s\" % (\n\t\t\t\t\toption_name, file_name)\n\n\t\tvalue = config[file_name]\n\t\tif value is not None:\n\t\t"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\t\"\"\"\n\tGets the specification for the specified option name.\t\"\"\"\n\t\treturn self.__get_value_for_parameter(command_name, argument_name)\n\t\n\tdef __get_value_for_parameter(self, command_name, parameter_name):\t\n\t\tcommand_line = utils.to_unicode(self.command_line_for_parameter(command_name, parameter_name))\n\t\tsplit_command_line = command_line.split(None, 1)\n\t\tvariable = split_command_line[0]\n\t\tvariable_value = split_command_line[1]\n\t\tif variable_value:\n\t\t\treturn variable"}]}
{"task_id": "6306092d73426c38ae68ad06", "prompt": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\tif self._silentargs:\n\t\t\tself._silentargs.sort()\n\t\t\tlst = [[str(arg), arg.__doc__ or 'No description provided'] for arg in args]\n\t\t\treturn lst\n\t\telse:\n\t\t\treturn None\n\n\tdef getargdescs(self, arg):\n\t\t\"\"\"\n\tGet one line description for an argument::\n\t\t\n\t\tdef f(a):\n\t\t\tpass\n\n:param arg: The received argument.\n:return: string, slienced argument description\n:since: 1.6\n\"\"\"\n\t\tif self._verbose:\n\t\t\tprint('"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\targs_str = []\n\t\tfor arg in args:\n\t\t\tif (not arg) or arg=='\\0': # skip strings\n\t\t\t\tcontinue\n\t\t\t# skip string sequences\n\t\t\telif (not arg) or not arg.strip():\n\t\t\t\tcontinue\n\n\t\t\targs_str.append(self.decode_string(arg))\n\n\t\treturn args_str\n\n\tdef get_list(self, str):\n\t\t\"\"\"\n\tread a string value\n\n:param str: a string value\n\"\"\"\n\t\tfor item in self.re_list.finditer(str):\n\t\t\tstr_groupidx"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\tresult = []\n\t\tfor arg in args:\n\t\t\tif arg.startswith('-'):\n\t\t\t\tresult.extend(arg.split(' - ', 2))\n\t\t\telse:\n\t\t\t\tresult.append(arg)\n\n\t\treturn result\n\n\tdef get_output_type(self):\n\t\t\"\"\"\n\treturn the output type\n\t\"\"\"\n\t\treturn \"table\"\n\n\n\n\"\"\"\n#\n# MAIN CLASS\n#\nclass MAIN(BaseManager):\n\t\n\tclass Manage(BaseManager.Manager):\n\t\t\"\"\"\n\t\tManage class \n\t\t\"\"\"\n\n\tclass List(BaseManager.Manager):"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\treturn\n\nclass SilentArg:\n\t'''\n\tThe sleeper argument which will exit the application at. By\n\tpassing it argument an invalid value will exit the application. For\n\texample:\n\n\texample:\n\t\tsl.print_red(\"This operation cannot be executed. \\\n\t\t\tPlease try again later.\")\n\t'''\n\tdef __init__(self, name, value):\n\t\tself.name = name\n\t\tself.value = value\n\nclass SilentBooleanArg(_SilentArg):\n\tpass\n# -*- Mode:Python; indent-tabs"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\treturn []\n\t##!!!!!!!!! ##\n\tdef reset_silent(self):\n\t\t##!!!!!!!!! ##\n\t#!!!!!! ##\n\t#!!!!!! ##\n\t#!!!!!! ##\n\tdef to_list(self):\n\t\treturn list(self.__slots)\n\tdef to_list_with_index(self, index):\n\t\t\"\"\"convert id list to list with indices of length specified and\n\t\treplace elements with index\"\"\"\n\t\tl = self.__slots.copy()\n\t\tfor idx in self.__slots"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\treturn [x[1] for x in silenced_list\n\t\t\tif x[0] not in args and x[2]]\n\n\n\tdef process_argv(self):\n\t\t\"\"\"\n\trun the parser, parse our command list, and output usage\n\t\n\t:param cmd:\n\t:param args:\n\t:return:\n\t\"\"\"\n\n\t\t# parse command line arguments\n\t\top = argparse.ArgumentParser(description=\"Silent Python IDE\",\n\t\t  formatter_class=argparse.RawDescriptionHelpFormatter,\n\t\t  epilog=\"Available commands:\\n\" \n\n\t\t)\n\n\t\t#"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\tsilent_args = []\n\t\tdebug(\"args: {}\".format(args))\n\t\twhile True:\n\t\t\tsilent_arg = \"\"\n\t\t\tfor char in args:\n\t\t\t\tif char == \"silent\":\n\t\t\t\t\tsilent_arg += \"!\"\n\t\t\t\telif char == \"debug\":\n\t\t\t\t\tverbose(debug)\n\t\t\t\telse:\n\t\t\t\t\tsilent_arg += \"-\"\n\t\t\tdebug(\"#silent {} {}\".format(silent_arg, silent_arg))\n\t\t\tsilent_args.append(silent_arg)\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\ntry:"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\treturn [arg.name for\n\t\t\t\t\targ in self._arguments.get(self.name)]\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\treturn list(args)\n\n\tdef call(self):\n\t\t\"\"\"\n\t\tParse the protocol instruction\n\n\t\t1st parse command and store result\n\t\t2nd scan command (single argument)\n\t\t 3rd loop for loops\n\t\t\n\t\t2nd call command if not in command\n\t\t\"\"\"\n\t\tcmd = self.parse_cmd()\n\t\tif isinstance(cmd, basestring):\n\t\t\tcmd = shlex.split(cmd)\n\t\tself.handle_cmd(cmd)\n\t\t\t\n\n\tdef handle_cmd(self, cmd):\n\t\t\"\"\"\n\t\tHandle a command\n\t\tReturns the command-string for"}, {"generated_text": "def get_silent_args(self, args):\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\t\"\"\"\n\t\tsanitized_args = []\n\t\tfor arg in args:\n\t\t\tsanitized_args.append(arg.replace(\"_\", \" \"))\n\t\treturn sanitized_args\n\n\n\t# Get args and the savings\n\tdef do_arguments(self, args, silent):\n\t\t\"\"\"\n\t\tget arguments passed to a function\n\n\t\tArgs : \n\t\t\t* args: the received arguments to be tested.\n\t\t\t* silent: optional. Set to False to suppress message.\n\t\t\"\"\"\n\t\targs = self.check_arguments(args) ## args need to be parsed beforehand\n\t\ts"}]}
{"task_id": "6306092d73426c38ae68ad07", "prompt": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tif self.args is None:\n\t\t\tself.log.error('argument required for %s is not specified' % (self.file))\n\n\tdef validate_option(self, option, val, param, skip_missing=True, flag=False):\n\n\t\ttry:\n\t\t\tret = self.validate(option, val)\n\t\t\tif not ret:\n\t\t\t\tself.log.error('failed with arg: %s' % arg)\n\t\t\t\tif not skip_missing:\n\t\t\t\t\tself.log.error(\n\t\t\t\t\t\t'Missing required argument: %s' % (option))\n\t\t\t"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tdef _check(requires):\n\t\tfor require in requires:\n\t\t\tif not self.requires.has_key(require[0]) or \\\n\t\t\t\t\tlen(self.requires[require[0]])!= require[1]:\n\t\t\t\traise ValueError, \\\n\t\t\t\t\t_('missing one of: %s' % str(requires))\n\n\tdef parse(self, force=0):\n\t\t\"\"\"\n\t\tParse the arguments. A list of arguments\n\t\tcan be passed in from a line:\n\t\t\t- the name of the object(ies) class\n\t\t\t- the string list of dependencies\n\t\tIf strict"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tif not self.has(\"cmd_args\") or not self.get(\"cmd_args\"):\n\t\t\tprint self.getUsage()\n\t\t\tsys.exit(2)\n\n\tdef getUsage(self):\n\t\t\"\"\"\n\t\tParse the args and make their description.\t\n\t\t\"\"\"\n\t\ts = \"Usage:   mpdinfo [COMMAND_OPTION]\"\n\t\ts += \"\\nCommands:\\t\"\n\t\tfor a in self.get(\"commands\").split(\"\\n\"):\n\t\t\ts += \"    %s [%s]\" % (a[0], a[1])\n\t\treturn s"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif not arg.isOptional:\n\t\t\t\tif self.needs.get(arg, False):\n\t\t\t\t\treturn False\n\t\treturn True\n\n\t@property\n\tdef takes_args(self):\n\t\tif self.needs.get(\"args\", False):\n\t\t\treturn True\n\n\tdef get_arg(self, arg):\n\t\tif type(arg) is str:\n\t\t\treturn self.needs.get(arg, False)\n\t\treturn arg\n\n\tdef getNrOfArgs(self):\n\t\treturn len(self.getFullArgList()) +"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tmissing_args = []\n\t\tfor key in args:\n\t\t\t# Check for required argument\n\t\t\tif not self.params.get(key) and self.params.get(key) == None:\n\t\t\t\tmissing_args.append(key)\n\n\t\tif len(missing_args) > 0:\n\t\t\tprint \"Parameters missing or not filled.\"\n\t\t\tfor item in missing_args:\n\t\t\t\tprint item\n\t\telse:\n\t\t\tprint \"Parameters are missing.\"\n\n\tdef initialize_parameters(self, args):\n\t\tvalid_fields = self.params.keys()\n\n\t\tfor k"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif not arg in self.command_line_options:\n\t\t\t\treturn False\n\n\t\tfor req in self.required_args:\n\t\t\tif not req in self.command_line_options:\n\t\t\t\treturn False\n\n\t\tfor arg in args:\n\t\t\tif not arg in self.command_line_options:\n\t\t\t\treturn False\n\n\t\treturn True\n\n\n\tdef is_valid_password(self, password):\n\t\t\"\"\"\n\t\tValidate password argument is valid.\n\t\t\"\"\"\n\n\t\treturn (password == \"\" or password is None or password == self.password)\n\n\n"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tif self.args == self.get_defaults_dict() or self.blocker:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n\tdef get_defaults_dict(self):\n\t\t\"\"\"\n\t\tCheck if all the arguments have been provided. This method simply\n\t\treturns all the (args, kwargs) that can be passed as keyword arguments.\n\t\tIf all the args have not been given, this method will raise a KeyError.\n\t\t\"\"\"\n\t\tif self.args == self.get_defaults_dict():\n\t\t\treturn {}\n\t\telse:\n\t\t\treturn {}\n\n\n"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tif args:\n\t\t\tself.verify_install_type_argument(args)\n\n\t\telse:\n\t\t\tlogger.error(\"Either -h or -i must be specified.\")\n\t\t\traise SystemExit(2)\n\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n#"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif arg not in frappe.local.form_dict:\n\t\t\t\twebnotes.msgprint(\"%s is required!\" % arg)\n\t\t\t\traise Exception\n\n\tdef send_sms(self, mobile):\n\t\t\"\"\"\n\t\tSend incoming SMS through WS Message.\n\n\t\tWill throw exception if not implemented\n\t\t\"\"\"\n\t\tif len(self.get_messages()) > 0:\n\t\t\ttry:\n\t\t\t\tself.sms.send(mobile)\n\t\t\t\twebnotes.msgprint(\"%s has been sent\" % self.msg_dict.get(\"from\") or self.msg"}, {"generated_text": "def validate_requires_args(self, args):\t\"\"\"\n\tCheck if all the required arguments have been provided.\t\"\"\"\n\t\tfor req in require:\n\t\t\tif not req in self.required_args and req not in args:\n\t\t\t\traise ValueError(\"Missing required argument\")\n\n\t\t\tif not args[req]:\n\t\t\t\traise ValueError(\"Required argument \" + req + \"required twice\")\n\n\n\tdef process_request(self, socketio, socketio_accept):\n\n\t\ttry:\n\t\t\tdata, addr = socketio.recvfrom(4096)\n\t\texcept socket.error:\n\t\t\tlog.warn(\"Socket error.\")\n\t\t\tsocketio_accept.close()\n\t\t\treturn False\n\n\t\t# Process the"}]}
{"task_id": "6306092d73426c38ae68ad08", "prompt": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\tif options_spec:\n\t\t\targs += options_spec.split(' ')\n\t\tif command_name not in self._commands_with_argument_names:\n\t\t\traise AttributeError('No such command for %s' % command_name)\n\t\tif len(args) == 1 or len(args) > 2:\n\t\t\traise ValueError('Command %r expects exactly one argument '\n                             'and received %s with %s. A command expects no '\n                             'arguments.' % (command_name, len(args),\n                                self._commands_with_argument_names))\n\t\treturn args[0"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\n\t\tcommand_name = \"{0}_{1}\".format(command_name, command_name)\n\t\trequired_args = _get_arguments_with_required_when(options_spec)\n\t\t\n\t\t# find first arg for which we can satisfy the condition and\n\t\t# the argument value\n\t\tfor first_arg in itertools.ifilter(lambda x: self.condition(args, x, first_arg), commands):\n\t\t\tif first_arg in required_args:\n\t\t\t\treturn first_arg\n\n\t\traise ConfigException(u\"\\ncommand '{0}' not matched by its {1}\"."}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\tmatch_obj = re.match('required[_a-z]+(_a-z+)',\n\t\t\t\t\t\t\t\tcommand_name)\n\t\toption_set = {}\n\t\tif match_obj.group(3) == _a:\n\t\t\tif len(args) - 1!= len(option_set):\n\t\t\t\traise InvalidArguments('Need arguments after last '\n\t\t\t\t\t\t'keyword/option name.')\n\t\telif match_obj.group(3) == _b:\n\t\t\tif len(args) - 1 < len(option_set):\n\t\t\t\traise InvalidArguments('Need either arguments after last '\n\t\t\t\t\t\t"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\tassert len(self.argv) <= 1\n\n\t\tcommands = self._get_commands().command.values()\n\n\t\tif len(commands) == 0:\n\t\t\t# No command - return what it is\n\t\t\treturn ['command_not_found', 'unknown command']\n\n\t\tmatched_args  = [ command_name ]\n\t\tfor command in commands:\n\t\t\tif command in options_spec.command:\n\t\t\t\t# If parameter is required, check next arg\n\t\t\t\tmatched_args.append('-{0}'.format(command))\n\t\t\telif command in options_spec.boolean_arguments:\n\t\t\t"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\n        get_parser = get_subparsers.add_parser(command_name,\n                                              description=commands_metadata['Command Descriptions']['command_description'],\n                                              formatter_class=argparse.RawDescriptionHelpFormatter,\n                                              help=commands_metadata['Command Descriptions']['msg'])\n        get_parser.set_defaults(func=self.__command_help_callback, required_args=self._required_args)\n\n        get_parser.add_argument('-h', '--help', action='help',\n                    help='display available command help')\n\n        get_parser.add_argument('-o', '--optional', action='"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\tif options_spec:\n\t\t\tcommand_name = '{}!{}'.format(command_name, options_spec._short_name)\n\n\t\tif command_name in self.required_when:\n\t\t\treturn self.required_when[command_name]\n\t\telse:\n\t\t\treturn []\n\n\tdef _get_command_args(self, command_name, required_when):\n\t\t\"\"\"\n\t\tGet the specified command args.\n\t\tIf required_when, use list of commands that include required\n\t\twhen conditional.\n\n\t\t:param command_name: the command name.\n\t\t:"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\treturn []\n\n\tif options_spec.required_when:\n\t\tneeded_conditions = _get_conditionally_required_args(command_name, options_spec, \n\t\t\t\t\t\"required_when\")\n\telse:\n\t\tneeded_conditions = []\n\n\tfor option in needed_conditions:\n\t\tif options_spec.default is None:\n\t\t\tdefault = options_spec.required_when\n\t\telse:\n\t\t\tdefault = options_spec.default\n\n\t\tif option not in options_spec.required_when and not getattr(command_module, option):\n\t\t\traise ConfigurationError"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\n        return self._arguments.get(command_name, []) + \\\n                options_spec['required_when']\n\n    def _parse_command_line_args(self, argv):\n        \"\"\"\n        Parse command line arguments from ``argv`` (a list)\n\n        :param sys.argv: the parsed command line arguments\n        :return: a dictionary containing the arguments\n        \"\"\"\n\n        parser = _argument_parser()\n        self._arguments = parser.parse_args(argv)\n\n    def _get_options_spec(self):\n        \"\"\"\n        Parse command line options and options_spec from our command line\n\n        :return:"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\targs_parsed = []\n\t\tfor param in options_spec:\n\t\t\tif not param in args:\n\t\t\t\targs_parsed.append(param)\n\t\t\t\t\n\t\tif len(args_parsed) == 0:\n\t\t\treturn []\n\t\t\n\t\trequired_when_options = {}\n\t\tfor param in args_parsed:\n\t\t\tif param in options_spec:\n\t\t\t\trequired_when_options[param] = True\n\t\t\t\tbreak\n\t\t\n\t\tflags = self._get_flags(command_name, command_arg)\n\t\t\n\t\tfor required_mode in required_when_options:\n\t\t\tdel required_"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\t\"\"\"\n\t\tif not self.command:\n\t\t\traise NoCommandSpecified(command_name=command_name, message=_(\"Missing command definition for '%s'\") % command_name)\n\t\tif len(self.command) <= len(options_spec):\n\t\t\traise InvalidCommandSpecified(command_name=command_name, message=_(\"Too few arguments for '%s'\") % command_name)\n\t\tremaining_args = self.command[len(options_spec):][:len(options_spec)] + args + options_spec + ['optional_not_passed']\n\t\treturn remaining_args\n\n\t"}]}
{"task_id": "6306092e73426c38ae68ad09", "prompt": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tlength_args = len(args)\n\t\tif length_args > self.min_length:\n\t\t\tif self.min_length > length_args:\n\t\t\t\traise ValueError, \"%r is too long, max %d\" % (args, self.min_length)\n\t\t\tself.min_length = length_args\n\nclass Message:\n\t@staticmethod\n\tdef create(data, target):\n\t\tif isinstance(data, str):\n\t\t\treturn Message(SourceFactory.for_string(data), target)\n\t\telif isinstance(data, list):\n\t\t\treturn Message."}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) > self.min_length:\n\t\t\tself.print_error('length', args)\n\t\t\treturn False\n\t\tif not isinstance( args[self.min_length], type(1) ) or args[self.min_length] < -1:\n\t\t\tself.print_error('length', args)\n\t\t\treturn False\n\t\tif args[self.min_length] in args[1:]:\n\t\t\tself.print_error('length', args)\n\t\t\treturn False\n\t\treturn True\n# -*- coding: utf-8 -*-\n# Copyright (c"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tparam_idx = -1\n\n\t\tfor param, arg in zip(args, args.as_tuple()):\n\t\t\tif len(param) > param_idx and param[param_idx]!= len(arg):\n\t\t\t\traise SenseException(\"Not enough arguments\")\n\t\t\tparam_idx += 1\n\t\t\tmsg = 'Argument must be of length {}.\\n'.\\\n\t\t\t\tformat(param_idx)\n\t\t\traise ValueError(msg)\n\n\tdef check_parameter_length_args(self):\n\t\t\"\"\"\n\t\tCheck if value of args after args are longer than length specified.\n\t\t"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) > self._max_length:\n\t\t\tfrappe.throw(_(\"Length of arguments exceeds the allowed limit. {0} > {1}\")\n\t\t\t\t.format(self._max_length, len(args)))\n\n\tdef validate_max_length(self, name, length):\n\t\t\"\"\"\n\t\tValidate the length of a string\n\t\t:param name: The name of the string to validate.\n\t\t:param length: The length of the string to validate.\n\t\t\"\"\"\n\t\tif length in (None, \"\", False):\n\t\t\treturn\n\n\t\tif len(name) >"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\t# if the given length is less than 10, assume it's a string\n\t\tif len(args) < 10:\n\t\t\targs = args.encode(\"utf-8\")\n\t\tlength = len(args)\n\t\tif length <= 0:\n\t\t\tfrappe.throw(frappe._(\"Length is required\"))\n\n\tdef raise_validation_exception(self):\n\t\t\"\"\"Exception if validation does not succeed.\"\"\"\n\t\tif self.get(\"__islocal\"):\n\t\t\tthrow(_(\"This document is currently accessed.\"))\n\n\tdef delete_key(self):\n\t\t\"\"\"Delete the `public` (user or `"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) < 1:\n\t\t\tfrappe.throw(_(\"Argument length must not be greater than 0\"))\n\nclass GanttView(WebsiteGenerator):\n\twebsite = frappe._dict(\n\t\ttemplate = \"templates/includes/gantt_widget.html\",\n\t\tcondition_field = \"published\",\n\t\tpages = {\n\t\t\t'title': \"Gantt\",\n\t\t\t'route': '/gantt/',\n\t\t\t'pages': {\n\t\t\t\t'title': \"Gantt\",\n\t\t\t\t'route': '/gantt/?page=',\n\t\t\t\t'page': 0"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) < 2:\n\t\t\treturn False\n\t\tl, r = self._len_valid_args_for_len(args)\n\t\tif l!= len(args):\n\t\t\treturn False\n\t\treturn True\n\t\n\tdef _len_valid_args_for_len(self, args):\n\t\t'''\n\t\tValidate all the lengths between 2 integers and 0. \n\t\t'''\t\t\n\t\tlengths = set()\n\t\tfor arg in args:\n\t\t\tlengths |= set(arg)\n\t\tfor arg in args:\n\t\t\tlengths |= set([arg.length])\n\t\treturn"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tfor arg in self._signature:\n\t\t\tlen_error = 0\n\t\t\ti = 0\n\t\t\tfor a in args:\n\t\t\t\tif a is None:\n\t\t\t\t\tpass\n\t\t\t\telif not a.is_identifier():\n\t\t\t\t\tlen_error += 1\n\t\t\t\telif a >= 0:\n\t\t\t\t\tlen_error += (-1)\n\t\t\t\telse:\n\t\t\t\t\ti += 1\n\t\t\tif i >= len_error:\n\t\t\t\traise TypeError(\"Function has too many arguments for %s\" % self._get_name())\n\n\tdef get_length(self, name):\n\t\t\"\"\"\n\tReturn the argument"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) < 2:\n\t\t\traise SuperdeskApiError.badRequestError(\"The number of arguments has to be longer than one\")\n\n\tdef build_response(self, args):\n\t\t\"\"\"\n\t\tCheck that provided arguments are valid.\n\n\t\t:param args: The received arguments.\n\n\t\t:raises APIException: If arguments are invalid.\n\t\t\"\"\"\n\t\t# TODO: If arguments is invalid, then error message with empty and non-empty value\n\t\t# is returned from the server.\n\t\tif not len(args):\n\t\t\traise SuperdeskApiError.badRequestError(\"The"}, {"generated_text": "def validate_length_args(self, args):\t\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\n:param args: The received arguments.\t\"\"\"\n\t\tlength = len(args) + 1\n\t\tif length > self._length:\n\t\t\traise ValueError(\"length should not be smaller than {}\".format(self._length))\n\t\treturn args\n\n\tdef validate_type_args(self, args):\n\t\t\"\"\"\n\tCheck if value of arguments is not of the expected type.\n\n:param args: The received arguments.\t\"\"\"\n\t\ttype_ = args[0]\n\t\tif type_!= self._default:\n\t\t\traise ValueError(\"expected argument to be of type {}, but received {}\".format(type_, self._default))\n\t\treturn args"}]}
{"task_id": "6306092e73426c38ae68ad0a", "prompt": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif not isinstance(args, (list, tuple)):\n\t\t\targs = args.split()\n\t\tfor arg in args:\n\t\t\targ = arg.strip()\n\t\t\tif arg in self.choices:\n\t\t\t\tself.choices.remove(arg)\n\n\n#-*- coding: utf-8 -*-\nimport json\nimport urllib\nimport time\nimport urllib2\nimport traceback\nimport logging\nimport random\nfrom collections import deque\nfrom threading import Thread\nfrom queue import Queue\n\nlogging.basicConfig(level=logging.DEBUG)\n\nbase_url  = \"http://www.espn"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tfor k in args:\n\t\t\tif not k in self.choices:\n\t\t\t\traise ValidationError(_(\"{0} not a valid choice\".format(k)))\n\n\tdef validate_invalid_link_doctypes(self):\n\t\t\"\"\"\n\t\tvalidate restrictions based on user permissions and link type\n\t\t\"\"\"\n\t\tstandard_fields = (\"Data\", \"Eval\", \"Print Format\", \"Large Text\",\n\t\t\t\"Text Editor\", \"Data and View\", (\"Data\", \"Report\", \"Custom Printing\"))\n\n\t\tdef get_link_dt_or_form():\n\t\t\treturn \"/form/Data/\" \\"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif not args:\n\t\t\treturn\n\n\t\tfor entry in self.choices:\n\t\t\tif entry[0] not in args:\n\t\t\t\treturn\n\n\tdef validate_valid_choices(self):\n\t\t\"\"\"\n\t\tPerform some automatic action. The method first checks that all\n\t\tentries in the order that they were asked for, then checks that\n\t\tthe options and values are valid (does not require duplicates) that\n\t\tshould have been used before returning 'value'. If the valid\n\t\tvalue is false, it must raise an exception to be raised,\n\t\twith the message that the entry was required but"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif args[0][0] == \"all\":\n\t\t\targs.append([\"All\", \"Yes\"])\n\t\telse:\n\t\t\targs.append([\"All\", \"No\"])\n\n\tdef validate_options(self):\n\t\t\"\"\"\n\t\tValidate if provided options exist.\n\t\tValidates each option individually.\n\t\t\"\"\"\n\t\tfor x in self.options:\n\t\t\tif x[0] in (\".\", \"\") and x[1] in ('', \"false\", \"0\", \"no\"):\n\t\t\t\tif x[2] == \"enabled\":\n\t\t\t\t\tif len(x) > 3:"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\treturn self.get_values(\"input_parameters\", args, \"choices\",\n\t\t\t\"[0, 1, 2, 3, 4, 5, '\"] + args.get(\"choices\", []))\n##############################################################################\n# Copyright (c) 2013-2018, Lawrence Livermore National Security, LLC.\n# Produced at the Lawrence Livermore National Laboratory.\n#\n# This file is part of Spack.\n# Created by Todd Gamblin, tgamblin@llnl.gov, All rights reserved.\n# LLNL-CODE-647188\n#\n"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif type(args) not in (tuple, list):\n\t\t\tself.print_usage()\n\t\t\tsys.exit(1)\n\n\t\tfor i in xrange(1, len(args)):\n\t\t\tif not isinstance(args[i], (tuple, list)):\n\t\t\t\tself.print_usage()\n\t\t\t\tsys.exit(1)\n\n\t\t\tname = args[i][0]\n\n\t\t\tchoices = args[i][1:]\n\n\t\t\tif name not in self.choices:\n\t\t\t\tself.print_usage()\n\t\t\t\tsys.exit(1)\n\n\t\t\tself.choices[name].value"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif self.name in args:\n\t\t\treturn None\n\t\tfor option in args:\n\t\t\tif option in self.choices.choices:\n\t\t\t\treturn option\n\t\treturn None\n\n\tdef _get(self, value):\n\t\tif value in self.options:\n\t\t\treturn self.options[value]\n\t\telse:\n\t\t\treturn self._get(value)\n\n\tdef _get_impl(self, name):\n\t\tif name in self.impl:\n\t\t\treturn self.impl[name]\n\t\telse:\n\t\t\tif name in self.choices:\n\t\t\t\treturn name\n\t\t\t"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif isinstance(args, tuple):\n\t\t\tmsg = args[0]\n\t\t\targs = args[1:]\n\t\t\tmsg = _make(msg)\n\t\t\tchoices_str = \",\".join([str(c) for c in args])\n\t\t\tmsgprint(_(\"Please do not use spaces around the following \"\n\t\t\t\t\"letters: {}\").format(choices_str))\n\n\t\telif isinstance(args, list):\n\t\t\tchoices = args\n\t\t\tif not isinstance(choices, (tuple, list)) or \\\n\t\t\t\tnot args:\n\t\t\t\tmsg = args[0]\n\t\t\t\tmsgprint(_(\"Please enter at"}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif self._choices is not None:\n\t\t\treturn args[1] >= self._choices\n\t\treturn False\n\n\tdef validate_no_overlap(self, args):\n\t\t\"\"\"\n\t\tFind missing arguments where there is overlap.\n\t\t\"\"\"\n\t\tif self._missing and len(args) < len(self._missing):\n\t\t\tmessage = (\n\t\t\t\t\"Argument {0} requires at least one of: \"\n\t\t\t\t\"[{1}].\".format(\n\t\t\t\t\tself.name,\n\t\t\t\t\t\",\".join(self._missing)\n\t\t\t\t)\n\t\t\t)\n\t\t\tself.fail("}, {"generated_text": "def validate_choices_args(self, args):\t\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif self.__fields and arg not in self.__fields:\n\t\t\t\traise ValueError(\"Invalid choice argument: '%s'\" % arg)\n\t\t\tif arg not in self.choices.keys():\n\t\t\t\traise ValueError(\"Invalid choice argument: '%s'\" % arg)\n\n\tdef validate_valid_choice_args(self, args):\n\t\t\"\"\"\n\t\tCheck if choice arguments are valid, and if so validate they are of the\n\t\tvalid choices.\n\n\t\t:param args: The args in to validate.\n\n\t\t:return: True, if valid, False\n\t\t:"}]}
{"task_id": "6306092e73426c38ae68ad0b", "prompt": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif arg > args[arg][0] or arg <= args[arg][1]:\n\t\t\t\traise ValueError(\"Argument '%s' was not between minimum and maximum values.\" % arg)\n\n\tdef convert_args_for_regex(self, *args):\n\t\t\"\"\"\n\t\tConvert argument for regex.\n\n\t\t@param args: The received arguments.\n\t\t@type args: [object,...]\n\t\t@return: The converted arguments.\n\t\t:rtype: [object]\n\t\t\"\"\"\n\n\t\tregex = None\n\n\t\tif args[0] is not None:"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\ttry:\n\t\t\tmin_value = float(args[0])\n\t\t\tmax_value = float(args[1])\n\t\texcept:\n\t\t\traise ConfigError('Minimum value was wrong.')\n\t\t# Check that argument is between the minimum and maximum value.\n\t\tif min_value > max_value:\n\t\t\t# Replace the default values with \"None\" if any parameter\n\t\t\t# has no value.\n\t\t\tif min_value is not None:\n\t\t\t\targs = args.replace(\"None\", 'None')\n\t\t\telse:\n\t\t\t\targs = args.replace(\"0\", '0')"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tmin = args[0]\n\t\tmax = args[1]\n\t\tif min is not None and max is not None:\n\t\t\tif min <= max:\n\t\t\t\traise ValueError('Value must be between minimum and maximum values.')\n\t\telif (min is not None) and (max is not None):\n\t\t\tif min >= max:\n\t\t\t\traise ValueError('Value must be between minimum and maximum values.')\n\t\telse:\n\t\t\traise ValueError('Value is outside range of valid numbers.')\n\n\tdef _get_parser_options(self):\n\t\topts = super(_OptParser, self)._get_"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif not len(args):\n\t\t\traise ValueError(\"at least one value expected\")\n\t\telif (len(args) == 2):\n\t\t\tif (len(args[0]) < 2):\n\t\t\t\traise ValueError(\"invaild value of argument max value\")\n\n\tdef validate_max_num_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of arguments is between max and minimum values.\n\t\t\"\"\"\n\t\tif not len(args):\n\t\t\traise ValueError(\"at least one value expected\")\n\t\telif (len(args) == 2):\n\t\t\tif (len(args"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args)!= 2:\n\t\t\traise BadArgumentException(\"Expected to receive from {0} but got {1}\".format(args, len(args)))\n\n\t\ttry:\n\t\t\tint(args[0])  # Make sure it's a minimum, so we can convert to integer\n\t\texcept ValueError:\n\t\t\traise BadArgumentException(\n\t\t\t\t\t\"Expected minimum to be integer, however not {0}\".format({ 'Value': type(args[0]) }\n\t\t\t\t))\n\n\t\ttry:\n\t\t\tint(args[1])  # Make sure it's a maximum, so we can"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif isinstance(args, (list, tuple)) and len(args) > 0:\n\t\t\tfor a in args:\n\t\t\t\tif _compare_types(a[0], \"str\"): self.min_length = max(str(_get_local_path(a[0])), _get_local_path(a[0]))\n\t\t\t\t\tself.min_value = a[1]\n\t\t\t\t\tbreak\n\n\tdef validate_range(self, args):\n\t\t\"\"\"\n\t\tCheck if value of range is between minimum and maximum value.\n\t\tIf not there, range should be between minimum and maximum"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) < 2: return False\n\t\tfor x in ['min','max']:\n\t\t\tif not self.args.get(x): return False\n\t\tfor x in ['min_valid','max_valid','min_max']:\n\t\t\tif self.args.get(x) < self.min_valid: return False\n\t\tfor x in ['min_max','max_max']:\n\t\t\tif self.args.get(x) > self.max_valid: return False\n\t\tif not self.min_max <= args['min']: return False\n\t"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif not (len(args) >= 1 and type(args[1]) in (str, unicode)):\n\t\t\traise TypeError(\"invalid arguments passed\")\n\t\tcheck = True\n\t\tfor arg in args:\n\t\t\tif not (len(arg) >= 1 and type(arg[1]) in (str, unicode)):\n\t\t\t\traise TypeError(\"invalid arguments passed\")\n\t\treturn check\n\t\n\tdef validate_min_max_value(self, value, min_value, max_value):\n\t\t\"\"\"\n\t\tChecks if value is between min and max value.\n\t\t:type value: str\n\t\t"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) > len(self._args):\n\t\t\t_raise_invalid_args(self._name, self._args, args)\n\ndef validate_range_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between the maximum and minimum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tif len(args) < len(self._args):\n\t\t\t_raise_invalid_args(self._name, args, self._args_min, self._args_max)\n\ndef validate_domain_max(self,"}, {"generated_text": "def validate_min_max_args(self, args):\t\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.\t\"\"\"\n\t\tfor arg in list(args):\n\t\t\tmatch = re.match(r'(\\d)+(?:\\.\\d+)?(e|E[\\d\\.e]\\.?\\d+)(?:-(\\d\\.)?\\d+)?', arg)\n\t\t\tif match is None or match.group(1) == '':\n\t\t\t\tcontinue\n\t\t\trange_min = int(match.group(2))\n\t\t\trange_max = int(match.group(3))\n\t\t\tif range_max < range_min:\n\t\t\t\tif args[0] =='min':\n\t\t\t\t\tif (args[1] > range_max"}]}
{"task_id": "6306092e73426c38ae68ad0d", "prompt": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t", "suggestions": [{"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\treturn create_complex_type(self.get_type_name(type_name), subcommand, option_name, spec_option)\n\n\tdef parse_argumet_with_options(self, subcommand, options=None,  name=None, additional_options=None, description=None):\n\t\t\"\"\"\n\t\tParse the argument flags and options\n\t\t:param subcommand: the command name\n\t\t:param options: the argument options\n\t\t:param name: the argument name\n\t\t:param additional_options: additional options to add to the options list (overwritten in main parser function)"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\tself.complex_args.append([argv_translator.normalize_arg_type(type_name, self.complex_args)])\n\t\tif subcommand == 'argc' or subcommand == 'arg':\n\t\t\treturn self\n\t\tif subcommand!= 'argc':\n\t\t\tself.complex_args.insert(0, [argv_translator.normalize_arg_type(type_name, self.complex_args[0])])\n\t\treturn selffrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom argparse import RawText"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\tcomplex_type_instance = TypeInstance(self._core)\n\t\tcomplex_type_instance.set_command_and_args(subcommand, type_name, option_name, \"*\", spec_option)\n\t\tcomplex_type_instance.set_description(option_name)\n\t\tcomplex_type_instance.set_options(spec_option)\n\t\treturn complex_type_instance\n\n\tdef execute_command(self, args, **kwargs):\n\t\t\"\"\"\n\t\tExecute a command\n\n\t\t:param args: the arguments\n\t:param kwargs: the keyword arguments\n\t"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\tcomplex_argument = self.options.compose_function(type_name, self.options.type_name_mapping[option_name])+ self.arguments.compose_function(type_name, subcommand)\n\t\targumet_spec = ComplexArgumentType(complex_argument.name,\n\t\t\t\t  self,\n\t\t\t\t  self.project )\n\t\tself.components.append(argumet_spec)\n\n\tdef create_primitive_type(self, option_name, option_type_name, option_description, options_list, type_map):\n\t\tprimitive_type = Option."}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\t# TODO: Implement option names here, and then call them\n\t\ttry:\n\t\t\treturn self.command_subparsers.add_parser(type_name, metavar=type_name+' command')\n\t\texcept AttributeError:\n\t\t\treturn self.command_subparsers.add_parser(type_name,\n\t\t\t\t\t\tdest=type_name+' command', nargs=\"?\",\n\t\t\t\t\t\thelp=SUPPRESS_HELP)\n\n\t@add_parser\n\tdef create_options_and_args(self, subcommand, option_name,\n\t\t\t\t\t\t\t  command_line_args=None, subcommand_instance=None,"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\ttype_instance_name = self.build_arg_name(type_name)\n\t\tparser = argument_parser.TypeParser(type_instance_name, option_name, help=\"A spec to specify the argument type for this argument\")\n\t\targument_parser.add_argument('--argument-type', '-s', dest='type_name', metavar='TYPE', action=FullType())\n\t\targument_parser.add_argument('--argument-format', '-f', dest='format', metavar='FORMAT',\t\t\t\t\t\t\thelp='format of the argument')\n\t\targument_parser.add_"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\tif self.argumet_type is not None:\n\t\t\treturn self.argumet_type( subcommand, type_name, option_name,\t**spec_option )\n\n\nclass ExtensionOption(SectionedObject):\n\t\"\"\"\n\tRepresents a command line option with the extension option.\n\t\n\tThis section is built by making it a class instance, the option itself,\n\twhich is initialized with an argument structure as an argument.\n\t\n\tThe following are common as the command line option:\n\n\t\textension -- The ext option argument line option\n\t\toption\t-- The complete path to the option"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\ttype_name += '_' + type_name.replace('-', '_')\n\t\treturn type_name\n\n\tdef create_simple_command(self):\n\t\t\"\"\"Create a simple command\"\"\"\n\t\tcomplex_cmd = \"complex_cmd\"\n\t\tcomplex_cmd = '%s -nosplash %s %s' % (\n\t\t\t\tself.__tool_path, self.__test_tool_name, complex_cmd)\n\t\treturn self.__parser.get_command(complex_cmd)\n\n\n###############################################################################\n# MAIN\n###############################################################################\n\nif __name__ == '__main__':\n\ttest_type"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\tsignature = ''\n\t\tif getattr(type_name, '__spec__'):\n\t\t\tsignature = getattr(type_name, \"__pysig__\")\n\n\t\tif hasattr(type_name, \"__getinitargs__\"):\n\t\t\tsignature = sys.stdin.read() + signature\n\n\t\t\tsig = \"{0}({1})\".format(type_name.title(), signature)\n\t\telse:\n\t\t\tsig = \"__{0}__\".format(type_name.title())\n\n\t\toption_type = spec_option +'{0} {1}'.format(repr(getattr(type_name, \"__doc"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\t\"\"\"\n\tBuild the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance\t\"\"\"\n\t\tself.commands.append(self.command_entry_list[subcommand][type_name + '_' + option_name])\n\t\tself.commands[-1][2]['spec_option'] = spec_option\n\t\tself.commands[-1][2]['description'] = 'Command - %s' % subcommand\n\treturn 0\n\ndef create_complex_opt(self, opt, name, description, option_name,  arg_type ='string'):\n\t\"\"\"\n\tBuild an option\n\t\"\"\"\n\tself.commands[-1][1]['opt'] = opt\n\topt"}]}
{"task_id": "6306092e73426c38ae68ad0f", "prompt": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\targs = args.split()\n\t\targs = [self.args_to_dict(arg) for arg in args]\n\n\t\t# Define a list of lists, each sublist contains a list that's either\n\t\t# the value one or a string or one value, if so treat it the same\n\t\t# and return a list\n\t\tif isinstance(args[0], list) or isinstance(args[0], tuple):\n\t\t\targs_list = []\n\t\t\tfor i, arg in enumerate(args):\n\t\t\t\targs_list.append([])\n\t\t\t\targ = args[i]\n\t\t\t\targs_"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\tadvanced_args = {}\n\t\tadvanced_args = merge_dict_opts(args, dict())\n\t\tcontrol_args = advanced_args\n\t\t(advanced_args, nested_args) = merge_dynamic_args(args, AdvancedExtends.CONTROL_ARGS, dict())\n\t\tif nested_args:\n\t\t\tadvanced_args = json.dumps(nested_args)\n\t\treturn (advanced_args, control_args)\n\n\tdef add_dynamic_args(self, args, advanced_args, advanced_vars):\n\t\t\"\"\"\n\t\tAdd arguments to the args, using a"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\n\n\t\targs = eval(args)\n\n\t\tif args:\n\t\t\t (options, nested_args) = args\n\t\telse:\n\t\t\toptions = []\n\t\t\tnested_args = []\n\n\t\t# Split in option/param sections\n\t\tfor opt in options:\n\t\t\tif opt in ['--help']:\n\t\t\t\tprint 'Options:'\n\t\t\t\tprint_help()\n\t\t\t\tcontinue\n\n\t\t\tif opt in ['--debug']:\n\t\t\t\tself.debug.append(opt)\n\t\t\t\tcontinue\n\n\t\t\tif opt in ['--dry-run', '--simulate']:\n\t\t\t\tself.simulate.append(opt)\n\t\t\t\tcontinue\n\n\t\t\t#"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\tcontrol_args = \"\"\n\t\tnested_args = \"\"\n\n\t\t# Special case for nested control variables, e.g, {{foo}}, and\n\t\t# {{foo}}\n\t\tif args and args[-1] == \"inline\":\n\t\t\tcontrol_args += \", %s\" % args[-2]\n\t\t\targuments = args[:-2]\n\t\telif args and args[-1] == \"control\" and len(args) > 2 and args[-2] == \"control\":\n\t\t\tcontrol_args += \", %s\" % args[-2]\n\t\t\t# TODO: this would be probably useless\n\t\t"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\targs = args[1:]\n\t\tret = dict()\n\t\tstart_time = time.time()\n\n\t\tfor arg in args:\n\t\t\targs_copy = copy.copy(arg)\n\n\t\t\tif self._check_nested_custom_and(\n\t\t\t\targs_copy, \n\t\t\t\tself.opts['recursive'], \n\t\t\t\tself.opts['recursive_arg_index']):\n\t\t\t\tret.update(self._flatten_args(\n\t\t\t\t\tself.parse_args(arg, self.opts['recursive_arg_index'])))\n\t\t\telif self._check_nested_custom_and_control("}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\tif len(args) == 5:\n\t\t\tcontrol_args = args[0]\n\t\t\tnested_args = args[1]\n\t\telse:\n\t\t\tcontrol_args = args[0]\n\t\t\tnested_args = args[1]\n\n\t\tcustom_args = {}\n\t\tfor key in self.custom_vars.keys():\n\t\t\tcustom_args[key] = self.custom_vars[key]\n\n\t\tadvanced_args = {}\n\t\tadvanced_args['args'] = basic_args\n\t\tadvanced_args.update(custom_args)\n\n\t\tnested_args = {}"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\tcontrol_args = {'args': args}\n\t\tnested_args = {}\n\n\t\tfor item in self.custom_commands:\n\t\t\tif (not item in options):\n\t\t\t\tcontinue\n\n\t\t\tfor i in item:\n\t\t\t\tcontrol_args['args'][i] = args[i]\n\n\t\tfor i in options:\n\t\t\tnested_args[i] = options[i]\n\t\t\t# control_args['args'][i] = args['args'][i]\n\n\t\treturn control_args, nested_args\n\n\tdef get_control_args(self, control, args):\n\t\t\"\"\""}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\targs_with_controls = {}\n\t\tfor arg_name, arg_params in args.iteritems():\n\t\t\tadd_control_args = False\n\t\t\tif \"control_\" in arg_name and arg_name in self.control_names \\\n\t\t\t\tand arg_params['control'] in self.ctrl_commands:\n\t\t\t\tadd_control_args = True\n\t\t\tif not add_control_args:\n\t\t\t\targs_with_controls.update({arg_name: arg_params})\n\n\t\toptions_dict = {}\n\t\toptions_with_params = {}\n\t\tfor opt_name"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\n    global nested_command_args\n    def parse_args(self, args):\t# pylint: disable=too-many-locals\n        \"\"\"\n\t\tParse arguments. It is a list of strings, the actual args.\n\n\t\tInput:\n\t\t self.args = [a_string_array,...etc] (without whitespace)\n\n\t\t\tOutput:\n\t\t [options]\n\t\t\t[parsed options]\n\t\t\"\"\"\n\t\tif self.args is None:\n\t\t\tself.args = sys.argv\n\n\t\targs_raw = {}\n\n\t\tfor arg in self.args:\n\t\t\t# args.remove(a"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\t\"\"\"\n\t\t_ = args.split()\n\t\tif _[0] == \"control\":\n\t\t\tcontrol = _[1]\n\t\telif _[0] == \"nested\" and len(_) >= 2:\n\t\t\tcontrol = _[1]\n\t\telse:\n\t\t\tprint \"error reading \" + _[0]\n\n\t\targs = _\n#\t\t\t_ = args.replace(\" \",\"\")\n\t\targs = args.split()\n\t#args = _\n#\t\targs = [\" \".join(args) for x in args if x] #remove duplicates\n#\t\tfor x"}]}
{"task_id": "6306092e73426c38ae68ad11", "prompt": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t", "suggestions": [{"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tif extra_vars:\n\t\t\ttry:\n\t\t\t\tvars_dict.update(extra_vars)\n\t\t\texcept AttributeError:\n\t\t\t\traise ValueError(\"extra_vars must be a dictionary\")\n\ndef _set_default(key, default_val=None):\n\t\"\"\"\n\tSet and optionally update the default value specified by the user.\n\n\t:param key: Setting key in settings of the app.\n\t:param default_val: Default value to store in the user config.\n\n\tYou can also use ``_set_in_env`` to set individual variables in\n\tthe ``env``"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tassert isinstance(extra_vars, (list, tuple))\n\t\tself.extra_vars = extra_vars\n\t\tself.vars_dict = vars_dict\n\n\tdef get_config_vars(self):\n\t\tself.ensure_config_vars()\n\t\tconfig_vars = self.get_config_vars()\n\t\treturn config_vars\n\n\tdef get_config_path(self):\n\t\t\"\"\"path environment vairable\"\"\"\n\t\tpath = join(self.get_base_path(), CONFIG_HALOEDIR)\n\t\tpath = environ[\"PATH\"] = join"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tif extra_vars is None:\n\t\t\textra_vars = []\n\n\t\tif isinstance(extra_vars, string_types):\n\t\t\textra_vars = [extra_vars, '']\n\n\t\t# Make sure that each element in ``extra_vars`` is a str\n\t\tfor x in extra_vars:\n\t\t\tif not isinstance(x, string_types):\n\t\t\t\tx = str(x)\n\n\t\t\t# Check if variable is already known\n\t\t\tif not hasattr(self, x):\n\t\t\t\tif logger: logger.warn(\"Variable not found: '%s' in %s\", x, self"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\t# FIXME: What if extra_vars is a set?  What if not remove?\n\tfor key in extra_vars:\n\t\tnew_env[key] = combine_vars(vars_dict[key], extra_vars[key])\n\n\ndef remove_empty_list(lst):\n\t\"\"\"\n\tRemove empty elements from a list\n\n\t:param lst: list\n\t\"\"\"\n\treturn filter(bool, lst)\n\n\nclass ConfigLogger(object):\n\t\"\"\"\n\tConfiguration file object, contains all configuration that is parsed and stored in a flat configuration file\n\t\"\"\"\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tfor x in list(vars_dict.values()):\n\t\t\textra_vars.setdefault(x, bld.bldnode)\n\n\tfor x in list(extra_vars):\n\t\tif x in dir(Task.classes):\n\t\t\tcls = getattr(Task.classes, x)\n\t\telse:\n\t\t\tcls = Task.classes[x]\n\t\t# merge them into the first level\n\t\tfor k in bld.node_deps.get(x, []):\n\t\t\tif k incls: \n\t\t\t\tcontinue\n\t\t\ttsk = get_re_tsk(k)\n\t\t\tif not t"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\n\n\tif 'options' not in vars_dict and warn_if_uncommitted_changes():\n\t\tconf_message(\"No config file has been modified since environment was reset\")\n\n\tif 'globals' in globals():\n\t\tset_options_in_env_file(extra_vars)\n\n\tenv = utils.do_dict()\n\tenv = utils.prepend_vars(env, extra_vars=vars_dict, prepend_env=extra_vars)\n\t_cached_env[tool_shed_env_key] = env\n\n\treturn Utils.update_env("}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tdef get_vars():\n\t\t\treturn vars_dict.copy()\n\tenv_vars = get_vars()\n\tenv_vars.update(extra_vars or {})\n\treturn utils.merge_dict(vars_dict, env_vars)\n\n\ndef merge_dicts(*args):\n\t\"\"\"\n\tMerge dicts and expand variables\n\n\t:param *args: List of dicts to merge or expand variables\n\t:return: merged dict\n\t\"\"\"\n\treturn collections.defaultdict(dict, args)\n\ndef get_file_dict(path):\n\t\"\"\"\n\tReturn the"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\ttry:\n\t\t\told_vars = vars_dict.copy()\n\t\texcept AttributeError:\n\t\t\told_vars = vars_dict\n\n\t\tfor key in extra_vars:\n\t\t\tif not key in old_vars:\n\t\t\t\tdel vars_dict[key]\n\n\t\tfor k in extra_vars:\n\t\t\tvalue = extra_vars[k]\n\t\t\tif value is not None:\n\t\t\t\tvars_dict[k] = str(value)\n\n\t\treturn vars_dict\n# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright ("}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\n\n\tif not isinstance(extra_vars, list):\n\t\textra_vars = [extra_vars]\n\n\tlvars = list(set(vars_dict.keys()) - set(extra_vars))\n\tvars_dict = dict(zip(lvars, extra_vars))\n\n\tif extra_vars and isinstance(extra_vars[0], dict):\n\t\tlvars = extra_vars\n\telse:\n\t\tlvars = [extra_vars]\n\n\treturn lvars\n\ndef merge_group(group1, group2):\n\tr_group1, r_"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\t\"\"\"\n\t\tfull_stack = dict_merge(extra_vars, vars_dict)\n\treturn full_stack\n\ndef flatten_dict(nested_dict, joiner=''):\n\t\"\"\"\n\tTake a nested dictionary and return a list of dictionaries\n\t(can be given as a string)\n\n\t:param nested_dict: Nested dictionary to unpack into (i.e. from ``dict``)\n\t:param joiner: Joiner string. e.g. ``['a', 'b']``\n\t:type nested_dict: str | list\n\t\"\"\"\n\tresult = []\n\t"}]}
{"task_id": "6306092f73426c38ae68ad13", "prompt": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t    \n\ttry:\n\t\tdisplay.vvvvv('Starting Ansible playbook on : {0}'.format(ir_plugin.workspace),\n\t\t            host = ir_plugin.hostname)\n\t\tansible_playbook_url = ir_plugin.urls.playbook\n\t\tir_plugin.execute_playbook(playbook_path=playbook_path,\n\t\t                              inventory_file=ir_plugin.inventory)\n\t\tansible_inventory = AnsibleInventory(ir_plugin._connection)\n\t\t\n\t\tif verbose is True:\n\t\t\tdisplay.vvv_info(\"Started Ansible playbook [playbook_url={0}] with args {1}\"."}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\ndef get_plugins_to_run(plugin_type, ansible_playbook):\n\t\"\"\"\n\tThis method gets the list of running plugins and merges them to the same\n\tdict structure by class. So, we run 'ansible-playbook testplugin.yaml' for each\n\tof them and merge it back later.\n\t:param plugin_type: string, one of: \"testplugin\" or \"demo\"\n\t:param ansible_playbook: str of the playbook with configuration (e.g. \"ansible-playbooks\")\n\t:return: a new dictionary with the groups as keys"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\n    args = [ansible_playbook] + ir_workspace.args\n\n\tif verbose:\n\t\tprint (\"Ansible args: \", args)\n\n\t# We use an object to ensure that this process can get started, so we can safely\n\t# pass arguments to plumb_down\n\tplugin = Ansible(args, name=ir_plugin.key, plugin_args=args)\n\tplugin.run()\n\ti = 2\n\twhile not plugin.success(i):\n\t\ti += 1\n\nclass Ansible(object):\n\t# Helper object for using ansible playbooks and Ansible playbooks"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\tir.logger.info('Invoking Ansible with AnsibleCLI')\n\n\t# --help option to view help for this\n\tCLI.do_cli(module_args=dict())\n\n\ndef create_ansible_playbook(ir_workspace, workflow_name, verbose=None, extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\tThe task used can be passed in on the CLI, or it can be passed to the\n\tCLI before it is executed.\n\n\tIf the requested playbook already exists, the playbook will fail"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\n\t\n\tdef new_playbook():\n\t\t\"\"\"\n\t\tCreate an Ansible playbook\n\n\t\t\"\"\"\n\t\tansible_path = os.path.join( ir_workspace.working_dir, ir_plugin.name+'.yml')\n\t\ttry:\n\t\t\tansible_parser = create_parser()\n\t\t\tansible_parser.set_defaults(output_file=ansible_path)\n\t\texcept RuntimeError as e:\n\t\t\traise RuntimeError('Failed to create ansible-playbook {} due to {}'.format(ir_plugin.name, e))\n\t\t\n\t\t# Create Ansible playbook\n\t\tansible_options = ansible_parser.parse"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\t)\n\tif not verbose:\n\t\tprint(\"{plugin} plugin not yet available\".\n\t\t\tformat(plugin=ir_plugin.__name__), file=sys.stderr)\n\treturn run(ir_workspace, ir_plugin, command=playbook_path, extra_vars=extra_vars)\n\ndef ansible_step_plugins(host, start_line, end_line):\n\t\"\"\"\n\tGenerates a CLI with the specified PLUGIN file at the hosts defined\n\tfor all of the task plugins.\n\n\t:param host: (string) path to host file to use for building"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\n    # if verbose is set, do not output to screen as is.\n    # We are only interested in output because we do not want to show\n    # any output except for logging the commands, so we print out what is\n    # happening.\n\n\t# pylint: disable=E0602\n\n\tdef _output(i):\n\t\t\"\"\"\n\t\tOutputs the given command to the screen according to the shell\n\t\tcommand interpreter, via output-capturing.\n\t\t\"\"\"\n\t\tif i:\n\t\t\tsys.stdout.write(i)\n\t\t\tsys.stdout.flush()\n\n\t# we capture any"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\n\tif verbose is None:\n\t\tverbose = conf.get_verbose()\n\n\tansible_args = {}\n\tansible_args.update(extra_vars)\n\tansible_args.update(ir_workspace._parse_cli_vars_files(ir_plugin))\n\tansible_args[\"module_path\"] += \".\"+ir_plugin.get_name()\n\tif verbose:\n\t\tprint \"Parsed Ansible args: \"\n\t\tpprint(ansible_args)\n\n\toutput_dir = os.path.join(settings.ANSIBLE_PLAYBOOK_DIR, ansible_args"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\torchestra.cli.AnsibleRunner.set_default_module()\n\t\torchestra.cli.AnsibleRunner.default(ir_workspace.app)\n\t\torchestra.cli.AnsibleRunner.command_help(ir_plugin, verbose)\n\t\t)\n\"\"\"Base module.\n\nBased partially on Django's own ``setup.py``.\n\"\"\"\n\nfrom distutils import cmd\nfrom distutils.core import setup, Extension\nimport sys\n__version__ = \"0.11.0\"\n\n\nclass MyBuildExtension(Extension):\n    def __init__(self, name,"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\t\"\"\"\n\t\ndef ansible_task(playbook_params_injector, playbook_args, verbose=None, extra_vars=None, ansible_args=None,\n\t               ansible_vars_configuration=None):\n\t\"\"\"\n\tWraps the Task.\n\t\"\"\"\n\treturn AnsibleRunner(\n\t\ttask_path=os.path.join(ir_workspace.get_workspace(), 'tasks', playbook_params_injector.get('playbook_path', '(missing)')),\n\t\tplaybook_params=playbook_params,\n\t\tverbose=verbose,\n\t\textra_vars=extra_vars,"}]}
{"task_id": "6306093273426c38ae68ad15", "prompt": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\t# print \"\\nArgs:\", vars_dict, \"Args Dict: \", vars_dict.keys()\n\t\t# print \"ir_plugin\", ir_plugin\n\t\t# print \"\\nCurrent Workspace:\", ir_workspace\n\t\t# print \"\\nParameters in Ansible is: \", vars_dict.get(\"ansible_parameters\",None)\n\t\t# print \"\\nOutputs\", ir_plugin.get_outputs().keys()\n\t\t# print \"\\nError summary:\"\n\t\t# print ir_plugin.get_error_summary()\n\t\t# print\n\t\t# print \"Output variables\"\n\t\t# print"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\tvars_dict[vars_dict.keys()[0]] = vars_dict\n\n        # parse the command line arguments\n        try:\n        \tcli_args.argv\t\t= cli_args.argv[1:]\n        \tir_plugin.load_parser_results(cli_args)\n\t\tcli_args.argv = cli_args\n\t        # the call to run_ansible on this program is to create a dict of command line arguments\n        \tir_plugin.run_ansible(ir_plugin, cli_args, vars_dict=vars_dict, use_ansible_inventory=False)\n    "}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\n\n\tif vars_dict['cli_args'] is None:\n\t\tcli_args = ['-v']\n\tansible_playbook.run(cli_args, variables=vars_dict)\n\ndef main():\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('-m', '--mode', dest='mode', default='list', type=str, help='The playbook mode on which you want to run', choices=['list', 'playbook'])\n\tparser.add_argument('-r', '--replace-plugins', dest='replace_plugins', help='A comma list of plugins to consider"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\tif vars_dict.get(\"version\") is None:\n\t\t\tenv = os.environ.copy()\n\t\t\tdel env['PATH']\n\t\t\tdel env['PYTHONPATH']\n\t\t\tdel env['JAVA_HOME']\n\t\t\tdel env['JMX_HOME']\n\t\t\timport socket\n\t\t\tif hasattr(socket, 'AF_UNIX'):\n\t\t\t\tos_type = socket.AF_UNIX\n\t\t\telse:\n\t\t\t\tos_type = socket.AF_INET\n\t\t\targs1 = []\n\t\t\tfor v_arg in vars_dict.get(\"args\", []):\n\t\t\t\targs1.append(\""}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\t# if no args just use variables (intended for scripting)\n\t\tvars_dict = vars_dict.copy()\n\t\tvars_dict['ansible_version'] = cli_args.ansible_version\n\t\tvars_dict['ansible_playbook'] = ir_workspace.yaml_to_inventory(True)\n\t\treturn_code, return_stdout, return_stderr, return_msg = _run_ansible(cli_args, vars_dict, ir_plugin)\n\t\tif return_code!= 0 and return_msg!= \"\":\n\t\t\tif ir_plugin.exitCode:\n\t\t\t"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\tif not vars_dict._raw and vars_dict._skipped:\n\t\t\tlogger.debug(\"Skipping vars dict because there is one or more empty vars\")\n\t\t\treturn\n\n\t\tif \"inventory\" not in vars_dict or \"groups\" not in vars_dict:\n\t\t\tvars_dict[\"inventory\"] = dict()\n\t\t# Create the Ansible Inventory\n\t\tmyinventory = AnsibleCollectionConfig(vars_dict)\n\n\t\t# Create the Ansible Group\n\n\t\t# Instantiate the Ansible Group, add Ansible tasks to it\n\t\tgroup = myinventory.add_group('group_all', {}, as_block="}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\treturn _run(\n\t\t\tcli_args=cli_args,\n\t\t\tvars_dict=vars_dict,\n\t\t\tir_workspace=ir_workspace,\n\t\t\tir_plugin=ir_plugin\n\t\t)\n\ndef _run(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param cli_args: the list  of command line arguments\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param ir_workspace: An Infrared Workspace object represents the"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\tcli_command = \"ansible-playbook\"\n\t\tcli_dict, errors = get_args(cli_args, ir_workspace)\t\t\t\t\t\n\t\ttry:\n\t\t\t# run ansible cli\n\t\t\trc = cli.eval(cli_command, globals_=vars_dict, errors=errors)\n\t\texcept SystemExit:\n\t\t\tprint('Unable to execute \"%s\" command' % cli_command)\n\t\t\treturn 1\n\t\texcept:\n\t\t\tlogging.error(\"Unable to update variables of vars from %s: %s\", cli_command, cli_dict)\n\t\t\tlogging.error(\"The last"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\tif ir_workspace.is_enabled:\n\t\t\tir_plugin.enable()\n\t\t\tif not ir_plugin.check_connection():\n\t\t\t\tir_plugin._module_logger.log( \"Unable to execute on {0}. Plugin probably doesn't support connections enabled\"\n\t\t\t\t.format(ir_workspace.get_class_name()))\n\t\t\t\tir_plugin._module_logger.log( \"Please install it before the connection can be established.\")\n\t\t\t\treturn False\n\t\t\t\t\t\t\n\t\t# do some validation of arguments\n\t\targs = vars_dict.get('cli_args',{}).get(cli"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\t\"\"\"\n\t\tret = ansible.CLIRunner(vars_dict, inventory_path='.')\n\t\tif ir_plugin and ir_workspace:\n\t\t\tcmd = 'ansible %s %s' % (ir_plugin.remote_path, ir_workspace)\n\t\t\tret = cli_args.format(cmd=' '.join(cmd))\n\t\treturn ret\n\t\n\ndef run(args={}):\n\tansible.init(__name__, arguments)\n\n\targs_list = ['--help']\n\tif args:\n\t\targs_list = args.split()\n\tansible.CLIRunner.exit_with_command_"}]}
{"task_id": "63060ada73426c38ae68ad31", "prompt": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tparam_values = getattr(values_dict, self.dest_param, None)\n\t\tif not param_values or not isinstance(param_values, (list, tuple)):\n\t\t\traise ValueError(params_name+': Argument should be a list or a tuple. Ex:'+ str(type(param_values)))\n\t\tfor value in param_values:\n\t\t\tif value:\n\t\t\t\tvalues_dict[param_values_name(value)] = value\n\t\tyield dict(zip(self.dest_param_names, values_dict.values()))\n\n\t@classmethod\n\tdef"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tfor arg in values_dict:\n\t\t\ttry:\n\t\t\t\tparser_name, value = arg.split(\"=\")\n\t\t\texcept ValueError:\n\t\t\t\tparser_name, value = arg, None\n\t\t\tself.values_dict_types[parser_name].add(str(value))\n\t\tself.force_cli_args = [arg for arg, in self.values_dict_types.iteritems() \\\n\t\t\t\t\t\t\t  if arg!= []]\n\nclass Shell(Base):\n\t\"\"\"\n\tA container for all sh shell functions\n\t\"\"\"\n\tdef __init__(self):\n\t\tself"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tparam_list = []\n\t\tlist_obj = values_dict.keys()\n\n\t\t# Parse the list of parameters separated by commas (,)\n\t\tfor param in list_obj:\n\t\t\t# Replace commas with =\n\t\t\tsplitparams = param.split(',')\n\n\t\t\t# Get rid of the ; signs:\n\t\t\tsplitparams = [p.strip() for p in splitparams]\n\n\t\t\t# Convert our value to a list:\n\t\t\tvalues_dict[param] = splitparams\n\n\t\t\t#... and assign the value to the proper type/value pairs:\n\t\t\tconverted_list = []\n\t\t\t"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tresult = []\n\t\tif not parse_check_ids(values_dict):\n\t\t\treturn result\n\n\t\tvirsh_cmd = '%s -q' % list2cmdline(values_dict)\n\n\t\t# Convert string to list, split the name at comma and check\n\t\t# if it is more than one entry in list\n\t\tnamev = values_dict['virsh_cmd']\n\t\tnamev = [v for v in namev.splitlines() if len(v)]\n\t\tnames, args = zip(*namev)\n\n\t\tresult_args = []\n\t\tfor name, arg"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tfor param in values_dict.get(\"params\", []):\n\t\t\tparam[\"name\"] = parser_name\n\t\t\tself.__dict__[param[\"name\"]] = param\n\treturn\n\ndef get_pool_details(params):\n\t\"\"\"\n\tGet pool details data from params\n\n\t:param params: Dictionary with the test parameters\n\t:return: Tuple containing (pool name, offset, length)\n\t\"\"\"\n\tlogging.debug(\"Get pool details data\")\n\tpool_name = params['pool_name']\n\tif not pool_name in pools:\n\t\treturn -1\n"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tvalues_dict = values_dict.copy()\n\t\tcommand = self._cmd_name_to_replace(command=parser_name)\n\t\tif ((self._has_value(value_name)\n\t\t\t\tand not self._value_is_list(command, values_dict))\n\t\t\t\tor (not command)):\n\t\t\tcommand = self._cmd_name_to_replace(command=parser_name)\n\t\tif command:\t\n\t\t\tif  hasattr(self, \"_handle_unknown_options\") and \\\n\t\t\t\tself._handle_unknown_options(\n\t\t\t\t\t\tparsed_values"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tcmd_dict = self.default_cmd_dict.copy()\n\t\tif parser_name == 'base':\n\t\t\tcmd_dict['command'] = 'base_command'\n\t\telif parser_name == 'long':\n\t\t\tcmd_dict['command'] = 'base_command'\n\t\t\tcmd_dict['command'] = 'help_command'\n\t\telif parser_name == 'debug':\n\t\t\tcmd_dict['command'] = 'base_command'\n\t\tcmd_dict.update(values_dict)\n\t\tself.__dict_set__('base_command', 'base_"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tif values_dict is None:\n\t\t\tvalues_dict = {}\n\t\tfor v in values_dict.keys():\n\t\t\t# we don't want to take new values_dict again.\n\t\t\tif v in self.available_types:\t# value_dict[k] is a string.\n\t\t\t\tvalues_dict[v] = str(values_dict[v])\n\t\t\telif v in self.allowed_types:\n\t\t\t\tvalues_dict[v] = [str(values_dict[v])] # yes, we will not get back \"true\" or \"false\"\n\t\t\t\tvalues"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\tif values_dict is None:\n\t\t\treturn None\n\t\telif values_dict:\n\t\t\toptions = {}\n\t\t\tfor key, values in values_dict.iteritems():\n\t\t\t\toptions[key] = [str(value) for value in values]\n\t\t\treturn (True, options)\n\t\telse:\n\t\t\treturn (False, 'None')\n\n\n\tdef show_command(self, command):\t\n\t\t\"\"\"\tMethod that display information about a specified command\n\t\t\"\"\"\n\n\t\tif command is None:\n\t\t\treturn (False, 'None')\n\n\t\tlogging.debug(\"Showing command:"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\t\"\"\"\n\t\t# TODO: Fix these special handling\n\t\tif values_dict is None:\n\t\t\tvalues_dict = {}\n\n\t\tfor name, value in values_dict.items():\n\t\t\tif value!= '':\n\t\t\t\tvalues_dict[name] = value\n\n\tdef validate_values_dict(self, values_dict):\n\t\t\"\"\"\n\t\tValidates that all of the passed in values_dict are dictionaries.\n\n\t\tThe values_dict type checks to ensure that the values types match up.\n\t\tIf more than one types are found, a MissingParameterError is thrown.\n\t\tEither way the values"}]}
{"task_id": "63060b1a73426c38ae68ad3e", "prompt": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\tresult = collections.OrderedDict()\n\t\tif not os.path.exists(plugin_dir):\n\t\t\treturn result\t\t# nothing to see here\n\n\t\tresult_key = os.path.join(plugin_dir, \"_\".join([\"_\".join(p.split(\".\")) for p in inspect.getfile(plugin_dir) + \"/defs/\")))\n\t\tplugin_properties = {}\n\t\twith open(os.path.join(plugin_dir, result_key), \"r\") as f:\n\t\t\tline = None\n\t\t\tplugin_line_number = 1\n\t\t\tfor line in"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\tplugins_dir = os.path.join(plugin_dir, 'plugins')\n\t\tproperties_file = os.path.join(plugins_dir, 'properties.json')\n\t\tload_prop_props(properties_file)\n\t\treturn flatten_dict(json.load(open(properties_file)))\n\n\t@staticmethod\n\tdef _get_plugin_spec_data(plugin_dir, target_dir, property_file, type_id):\n\t\t\"\"\"\n\t\tHelper for getting a plugin's specification from the plugin spec\n\t\tdata\n\n\t\t:param plugin_dir: A"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\tspec_dict = {}\n\t\tspec_file_path = os.path.join(plugin_dir,\"plugin.spec.yaml\")\n\t\ttry:\n\t\t\tspec_file = open(spec_file_path,\"r\")\n\t\t\tfor line in spec_file:\n\t\t\t\tif line.strip(\"\\n\").strip(\" \\n\") == \"\": #this is the spec entry\n\t\t\t\t\tcontinue  #skip non-data\n\t\t\t\tspec_name, spec_value_str = line.strip(\"\\n\").split(\"=\",1)\n\t\t\t\tspec_dict[spec_name] = spec_value_"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\tif not os.path.exists(plugin_dir):\n\t\t\treturn {}\n\t\twith open(os.path.join(plugin_dir,'plugin_info.json')) as f:\n\t\t\tfor line in f:\n\t\t\t\tif line.startswith(\"#\"):\n\t\t\t\t\t# Skip commented lines and blank lines\n\t\t\t\t\tcontinue\n\t\t\t\tkey, value = line.split(\":\", 1)\n\t\t\t\tkey = key.strip()\n\t\t\t\tvalue = value.strip()\n\t\t\t\tyield(key, value)\n\n\tdef get_plugin_spec_dict(self, plugin_dir, filter_func = None):"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\tplugin_yaml = os.path.normpath(os.path.join(plugin_dir, '../conf/plugin.yaml'))\n\t\tlogging.debug('Plugin configuration loaded: {}'.format(plugin_yaml))\n\t\ttry:\n\t\t\twith open(plugin_yaml) as config_file:\n\t\t\t\tplugin_spec = yaml.safe_load(config_file)\n\t\texcept IOError as ioerr:\n\t\t\tlogging.error('Error loading plugin configuration: {}'.format(ioerr))\n\t\t\treturn None\n\t\treturn plugin_spec\n\ndef get_plugins_parameters(plugin_dir):\t"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\tplugin_spec = {}\n\t\t# TODO should do a config dump first\n\t\tif plugin_dir is not None:\n\t\t\tfor path in [os.path.abspath(os.path.expanduser(p)) for p in os.listdir(plugin_dir)]:\n\t\t\t\tfilename = os.path.join(plugin_dir, path,'metadata.json')\n\t\t\t\twith open(filename, encoding='utf8') as file:\n\t\t\t\t\tplugin_spec = json.load(file)\n\t\treturn plugin_spec\n\ndef get_plugin(plugin_name, **kwargs):\n\t\"\"\""}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t # TODO: we need this when possible, so all the directories\n\t\t#             #              created are under it\n\t\t #             # /etc/\t\t/usr/lib/\t\t/usr/lib/\t\t/usr/X11R6/\t/usr/lib/\t\t/usr/lib/\n\t\t # /usr/include/\t\t/etc/\t\t/opt/\t\t/usr/include/\t\t/etc/x11/xinit/ \t/usr/include/\n\t\t # /etc/x11/xinit \t/usr/X11R6"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\t# The plugin's dir does not exists yet.\n\t\tif not os.path.isdir(plugin_dir):\n\t\t\treturn None\n\t# Get the plugin's directory.\n\tdir = os.path.dirname(plugin_dir)\n\t# Get the configuration file path.\n\tdir_settings = os.path.join(dir, \"plugin.conf\")\n\tfile = open(dir_settings, 'r')\n\tsettings = dict()\n\tlines = file.readlines()\n\tsettings_lines = []\n\tfor line in lines:\n\t\tif line.startswith(\""}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\n\t\n\tplugin_spec = os.path.basename(plugin_dir)\n\tplugin_properties = extract_plugin_properties(plugin_spec)\n\tplugin_properties['id'] = 'plugin-properties.xml'\n\treturn dict((key.lower(), value) for (key, value) in plugin_properties.iteritems())\n\n#\n# This file is part of the PyMeasure package.\n#\n# Copyright (c) 2013-2021 PyMeasure Developers\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\t\"\"\"\n\tCreates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties\t\"\"\"\n\t\treturn get_flat_dict_from_dir(plugin_dir, key='plugin_property_plugins')\n\n\ndef get_plugin_properties(module, plugin_type):\n\tpcs_directory = plugin_type + \"/\"\n\tplugin_properties = {}\n\tplugin_properties['name'] = module.__name__\n\tfor plugin_dir in sys.path:\n\t\tplugin_path = os.path.join(pcs_directory, plugin_dir)\n\t\tplugin_props = get_plugin_spec_flatten_dict(plugin_path)\n\t\tplugin_properties"}]}
{"task_id": "63060b1b73426c38ae68ad42", "prompt": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tenviron['PATH'] = ':'.join(self.global_dict['PYTHONPATH'].split(':'))\nimport argparse\nimport logging\nimport os\nimport sys\nfrom collections import defaultdict\n\nfrom.. import models\nfrom.. import tasks\nfrom..tasks import app as task_app\n\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass Task(models.Model):\n    \"\"\"Represents a task in the current workspace.\n\n    Attributes:\n        id: Id used for the task, used to identify the task\n        state_json: JSON representation of the task's state.\n        state_data: JSON representation of task's"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\ttry:\n\t\t\tconf = self.get_conf()\n\t\texcept ConfigNotFoundException:\n\t\t\tconf = None\n\t\tif conf:\n\t\t\tself.user.set(conf)\n\t\t\tself.write()\n\t\t\tself.user.load(conf)\n\n\tdef get_conf(self):\n\t\t\"\"\"\n\t\tRetrieve the dict of configuration variables.\t\n\t\t\"\"\"\n\t\tif not self.user:\n\t\t\tself.authenticate()\n\t\treturn self.user.get_conf()\n\n\tdef set_conf(self, path, value):\n\t\t\"\"\"\n\t\tRetrieve the dict of configuration"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tvars = self.envvars\n\n\t\tif not vars.get('INOTIFY') and os.environ.get('CATEMATCH') == 'true':\n\t\t\tself.envvars['INOTIFY'] = \\\n\t\t\t\t'y' if self.is_non_nil_dir(self.environ) else 'n'\n\t\t\tself.envvars['INOTIFY_OPTS'] = ''\n\t\t\tself.envvars['INOTIFY'] = ''\n\n\tdef process_request(self, request):\n\t\t\"\"\"Process a request.\"\"\"\n\t\trequest.add_line('\\n')\n\n\t"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tself.config.PATH = os.path.join(self.config.ROOT_PATH, \"etc\", env_name)\n\t\tenv_plugin.get_config_item(self, \"config\").set(conf_file)\n\t\treturn True\n\n\t##\n\t# Get the path to the default configuration file.\n\t# @param  self  The object pointer\n\t# @param  default_config The default configuration settings\n\t# @param  section  The section the default configuration belongs to (used, by get_config_item).\n\t# @param  required Default value"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tenv_var = 'XDG_CONFIG_HOME'\n\t\ttry:\n\t\t\treturn os.environ[env_var]\n\t\texcept KeyError:\n\t\t\tpass\n\n\tdef load_environment(self):\n\t\t\"\"\"\n\t\tLoads environment variables from ``os.environ.copy()``.\n\t\tIf ``OS_TARGET=1`` or ``OS_TARGET=2`` then asks for a standard install\n\t\tenvironment.\n\t\t\"\"\"\n\t\twith self._lock:\n\t\t\twhile not self._initializing:\n\t\t\t\tself.__initialize()\n\t\t\t\tenv = os.environ.copy()\n\t\t\t\tif"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tself.logger.debug(\"Injecting file(s): '{f}' from template ({t})\".format(f=self.filename, t=self.t))\n\n\t\twith open(self.filename, 'r') as cfh:\n\t\t\tconf = yaml.load(cfh.read())\n\n\t\tpath = os.path.abspath(self.path.strip())\n\n\t\tif os.path.exists(path):\n\t\t\tself.logger.warning(\"File exists: {f}\".format(f=self.filename))\n\t\telif os.access(path, os.X_OK"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tfrom distutils import sysconfig\n\t\timport optparse\n\t\t\n\t\tenv_name = None\n\t\tversion = (0, 0, 1)\n\t\t\n\t\t# Find the correct path\n\t\tprefix = sysconfig.get_config_var('prefix')\n\t\tif prefix:\n\t\t\t# Assume that prefix contains prefix == 'prefix'\n\t\t\tif sysconfig.get_config_var('libdir') is None:\n\t\t\t\traise ValueError('Could not find the library directory')\n\t\t\tprefix = os.path.abspath(os.path.expanduser(prefix))\n\t\n\tdef get_config_vars(self):\n\t\t\"\"\"\n\t\tReturn"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tn = getattr(self, 'n', None) or Options.commands.config\n\t\tif not n: n = getattr(Actions.commands, 'config', None)\n\t\tif not n: n = Options.commands.config = Utils.lget(Config.CONFIG_FILES)\n\t\tif not n: n = Utils.lget(self.style, 'config')\n\t\tif not n: n = Options.commands.config.split()\n\t\tif not n: n = []\n\t\tfor x in n:\n\t\t\ty = x.replace('%%', '%').replace('_', '-')"}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tself._config.envtype = Config.Environment.DEV\n\t\tself._config.environ['HOME'] = (environ.get('HOME') or environ.get('USER') or '') + environ['PATH']\n\n\tdef load(self):\n\t\t\"\"\"\n\t\tRead config and get environment variable values.\n\t\tTo enable reload functionality (like autoloading, in a web server) call read() \n\t\t\"\"\"\n\t\t_app = self._config\n\t\treturn _app.load()\n\n\t@staticmethod\n\tdef init():\n\t\t\"\"\"\n\t\tInitialize variables and get environment variable values.\n\t\t\"\"\""}, {"generated_text": "def inject_config(self):\t\"\"\"\n\tSet the environment variable for config path, if it is undefined.\t\"\"\"\n\t\tif not os.environ.get(\"CONFIG_DIR\"):\n\t\t\tos.environ[\"CONFIG_DIR\"] = tempfile.mkdtemp(\".nc\", dir=self.tmpdir)\n\n\tdef test_setup(self):\n\t\t\"\"\"\n\t\tInstall a fresh netCDF file.\n\t\t\"\"\"\n\t\tprint \"running setup...\"\n\t\tcreate_xml = partial(prepare_ncdump, dump=False)\n\n\t\tprint \"setting up ncdump...\"\n\t\twith self.ncdumper() as nc:\n\t\t\tconfig_xml = setup_config(nc, \"default\")\n\n\t\t\tprint \"waiting before dump!\""}]}
{"task_id": "63060b1b73426c38ae68ad43", "prompt": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tparser = self.setup_cli()\n\n\t\tif not self.args:\n\t\t\treturn parser.print_help(file=self.stdout)\n\n\tdef get_subparser(self, prog_name):\n\t\t\"\"\"\n\t\tFinds the subparser class from the module and returns it.\n\t\t\"\"\"\n\n\t\treturn self.subparser_map.get(prog_name, self.argparse.ArgumentParser)\n\n\tdef extend_ext_parser(self, ext_parser):\t# pragma: no cover\n\t\t\"\"\"\n\t\tImplements the extensions to get runable.\n\t\t\"\"\"\n\t\tfor"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\n\t\n\tif self.subparser is None:\n\t\treturn\n\tdef handle_sub(parser):\n\t\t# create file parser for the subcommand\n\t\tparser_name = os.path.basename(sys.argv[0])\n\t\ttry:\n\t\t\tparser_object = getattr(sys.modules[parser_name], self.subparser)\n\t\t\tparser_object.handle_opt([])\n\t\t\tself.subparser = parser_object.parser_object\n\t\t\n\t\texcept AttributeError:\n\t\t\tpass\t\n\t\t\t#print(\"Subparser:\", parser_name)\n\t\t\n\t\tif parser.usage:\n\t\t\t"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\troot_parser = update_doc_tree_preparser(root_subparsers)\n\t\tself._add_common_options(root_parser)\n\t\troot_parser.set_defaults(func=self.extend_command_func)\n\n\t\tversion_parser = update_doc_tree_postparser(root_add_common_args(self.cmd_args, ['version']))\n\t\tself._add_common_options(version_parser)\n\t\tversion_parser.set_defaults(func=self.extend_command_func)\n\n\tdef _add_common_options"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tif not self.__subparser:\n\t\t\treturn None\n\n\t\tcli_group = root_subparsers.add_parser(self.__name, help=self.__doc__)\n\t\tconf_group = cli_group.add_mutually_exclusive_group()\n\t\tconf_group.add_argument('-h', '--help', action='help', help=\"Show this message and exit\")\n\t\tcli_group.set_defaults(command=self.__cmd_help)\n\t\tconf_group.add_argument('-l', '--list', action='store_true', help=\"show configured keys instead of"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tself.cli_parser = subparsers.add_parser(\n\t\t\t\"extend\",\n\t\t\tadd_help=False,\n\t\t\tprog=\"rkexec\",\n\t\t\tdescription=self.gen_rkexec.__doc__,\n\t\t\tepilog=self.gen_rkexec.__doc__)\n\n\tdef gen_rkexec(self, args):\n\t\t\"\"\"\n\t\tCreates an RPy version of the program that wraps the RPy2 shell\n\t\t\n\t\t:param args: the args of the executable.  \n\t\t\t\n\t\t:return: the RPy2 shell with the script code.\n\t\t:rtype"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tself.__subparser = get_argument_parser(self.__parser,\n\t\t\t\t\t\t\t\t\t\t\t\t'maparser',\n\t\t\t\t\t\t\t\t\t\t\t\t**self.__subparser_args)\n\t\tself.__subparser.add_argument(\n\t\t\t'input_path',\n\t\t\ttype=str,\n\t\t\tdefault='/usr/share/python/apps/app/resources.yaml',\n\t\t\thelp='The input path for the model.')\n\t\tself.__subparser.add_argument(\n\t\t\t'script_args',\n\t\t\ttype=str,\n\t\t\tdefault=\"\",\n\t\t\thelp='Javascript arguments passed on the command line.')"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tsuper(Cli, self).extend_cli(root_subparsers)\n\t\tself.parser.add_argument('-F', '--file', type =argparse.FileType('r'), default=sys.stdout,\n\t\thelp=\"A file to read (optional).\")\n\t\tself.parser.add_argument('-I', '--init-file', type =argparse.FileType('r'), \n\t\tdefault=sys.stdout, nargs = '*', help=\"A file to copy the configuration(s) from.\")\n\t\tself.parser.add_argument('-b', '--bind', type =argparse.FileType('r"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tgroup = root_subparsers.add_parser(self.name, help=self.synopsis,\n\t\t\t\t\t\t\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\t\t\t\t\t\t\tdescription=self.description())\n\t\t\n\t\tgroup.add_argument(\"--include\", default=defaults.paths[0], type=argparse.FileType('r'),\n\t\t            help=\"The list of files/directories to include.\", required=True)\n\n\t\tgroup.add_argument(\"--exclude\", default=defaults.paths[1], type=argparse.FileType('r'),\n\t\t            help=\"The list of files/directories to exclude.\", required=True"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tinit_subparser(self,\n\t\t\t  'cli', {'parent':root_parser,'subparser':subparser},\n\t\t\t  'Extend CLI Options')\n\t\n\tdef create_new_subparser(self, parent, subparser):\n\t\tinit_subparser(self, parent, {'parent':parent}, 'Create new sub-parser')\n\n\tdef extend_subparser(self, parent, arguments):\n\t\tinit_subparser(self, parent, arguments['parent'])\n\nclass MainMenu(object):\n\t\"\"\"\n\tThe main menu object, holds the state, and the parsing"}, {"generated_text": "def extend_cli(self, root_subparsers):\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\t\"\"\"\n\t\tself._subparser.add_argument(\"--all\", action ='store_true', dest = 'all_subparsers', default = False,\n\t\t\thelp = \"do all plugins in all_subparsers\")\n\t\tself._subparser.add_argument(\"-t\", \"--top\", action = \"store_true\", dest = \"top\", default = False,\n\t\t\thelp = \"do top plugins\")\n\t\tself._subparser.set_defaults(func = extend_global)\n\t\t# create args for the command\n\t\t# use 'all' to only load the subcommand as well\n\t\tself"}]}
{"task_id": "62ece4982e6aefcf4aabbd64", "prompt": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t", "suggestions": [{"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tfor i in xrange(len(path)):\n\t\t\tif root[i]!= path[i]:\n\t\t\t\tdel root[i]\n\t\treturn path\nre_header_root = re.compile('^[ \\t]*(?=\\n)|^/\\*.*?\\*/|\\*/', re.M)\ndef parse_header(header):\n\t\"\"\"\n\t>>> parse_header('Content-type: text/html; charset=utf-8')\n\t('Content-type:',...)\n\t>>> parse_header('x-unknown-type; charset=us-ascii')\n\t('x-"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tif root.count(root[0]) == 0:\n\t\t\traise Exception(\"%s is not in file\" % path)\n\t\tif len(root) == 1:\n\t\t\toldval = strip_root(root[0], root[1])\n\t\t\troot.remove(oldval)\n\t\telse:\n\t\t\traise Exception('The file %s does not contain multiple files' % path)\n\tdef copyfiles_recursively(src, dst, force=False, followlinks=False):\n\t\tfor item in os.listdir(src):\n\t\t\ts = os.path.join("}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tparts = root.split(os.sep)\n\t\tfor part in parts[::-1]:\n\t\t\tif part!= '':\n\t\t\t\ttry:\n\t\t\t\t\tsubdir = os.sep.join(parts[0:-1])\n\t\t\t\texcept IndexError:\n\t\t\t\t\tsubdir = ''\n\t\t\t\tpath = os.path.join(root, part)\n\t\t\t\tret = os.remove(path)\n\t\t\t\tif notret:\n\t\t\t\t\tself.log.error('remove_file', 'Failed to remove root \"%s\"'%(path,))\n\t\t\t\t\traise OSError(errno.ENOENT, \"Removing root %s failed\" %path)"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tfor item in os.listdir(path):\n\t\t\tif item == \".\":\n\t\t\t\tos.remove(os.path.join(path, item))\n\t\t\telif os.path.isdir(os.path.join(path, item)):\n\t\t\t\tremove_root(os.path.join(path, item), root)\n\treturn root\n\ndef copytree(src, dst, symlinks=False, ignore=None):\n\tnames = os.listdir(src)\n\tif ignore is not None:\n\t\tignore = ignore(src, names)\n\telse:\n\t\tignore ="}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\twhile root!= \"/\":\n\t\t\tif not root.startswith(\".\"):\n\t\t\t\tbreak\n\t\t\tindex = root.lower().find(\"/\")\n\t\t\tif index == -1:\n\t\t\t\traise ValueError(\"Invalid root path : %s\" % path)\n\t\t\telif index > 0:\n\t\t\t\tname = \" \".join(path[:index + 1])\n\t\t\t\t\n\t\t\t\tif not (name == \"/\" and root == \"\"):\n\t\t\t\t\t# strip the root\n\t\t\t\t\treturn root[1:]\n\t\t\t\telse:\n\t\t\t\t\treturn name\n\t\t\telse:\n\t\t\t\troot = root.replace(path, \"\", 1)\t\t\n\t\t\t# remove hidden"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tfor name in listdir(path):\n\t\t\tpath.remove(path.join(path.join(path.split(path)[0]),''))\n\t\troot += '/'\n\tpath.strip(root + os.sep)\n\ndef file_tree(root_path):\n\t\"\"\"\n\tGenerate and return a xml ElementTree with the contents of  a file in a tree\n\t\n\t:param root_path: The root path where the xml should be generated.\n\t:type root_path: ``string``\n\n\t\"\"\"\n\ttree = {}\n\tfor dirname, dirnames, filenames in os."}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tif root == ENoError:\n\t\t\traise OSError(errno.ENOENT, 'Not empty/non-existent file')\n\t\ttry:\n\t\t\twith open(path, 'rb') as f: \n\t\t\t\treturn f.read()\n\t\texcept IOError as e: \n\t\t\tlogger.warn('remove_file_with_retry: %s', e)\n\t\t\treturn ''\n\t\n\tdef copy(self, path, dest, overwrite=False):\n\t\tif not overwrite and os.path.exists(dest): \n\t\t\traise OSError(errno.EEXIST, 'file already exists: %s' % path"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\tp = path.strip(\"/\")\t\t# remove leading '/' from start\n\t\tif p==\"\":\n\t\t\treturn (False,\"\")\n\t\tif p[0]=='.':\n\t\t\tp = p[1:]\n\t\tsplit = p.split(\"/\")\n\t\troot = path\n\t\tfor s in split:\t\t\t\t\t# remove leading '/' from end\n\t\t\tif not s.startswith(root):\n\t\t\t\troot = s\n\t\tif root == path:\n\t\t\treturn (False,\"\")\n\t\treturn (True, \"\")\n\t\n\tif cmd==\"rm\" or cmd==\"rmdir\":\n\t\tif not path."}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\ttry:\n\t\t\tshutil.rmtree(path)\n\t\texcept Exception as e:\n\t\t\tpathlogger.error('error in %s, error: %s' % (path, e))\n\t\t\traise e\n\n\ttry:\n\t\tos.remove(os.path.realpath(path))\n\texcept Exception as e:\n\t\tpathlogger.error('error in %s, error: %s' % (path, e))\n\t\traise e\n\n\tif os.environ.get('MOUNT', '/')!= path:\n\t\told_cwd = cwd\n\t\tcwd = None\n\telse"}, {"generated_text": "def strip_root(path, root):\t\"\"\"\n\tRemove root from path, throw exception on failure.\t\"\"\"\n\t\ttry:\n\t\t\tos.remove(path)\n\t\texcept Exception, e:\n\t\t\tif raise_exceptions:\n\t\t\t\traise InvalidArchitectureError(\"Unable to delete %s\" % path)\n\t\t\telse:\n\t\t\t\tprint \"Unable to remove %s: %s\" % (path, e)\n\ndef find_arch_dir(mysettings, myarch):\n\tpath = os.path.dirname(os.path.realpath(__file__))\n\treturn os.path.abspath(path)\n\n# Find arch-specific info for the given arch.\ndef find_arch_"}]}
